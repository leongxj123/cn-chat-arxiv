<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;&#21644;&#23618;&#27425;&#20998;&#21106;&#31243;&#24207;&#65292;DSEG-LIME&#25913;&#36827;&#20102;&#22270;&#20687;&#35299;&#37322;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07733</link><description>&lt;p&gt;
DSEG-LIME -- &#36890;&#36807;&#23618;&#27425;&#21270;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;&#25552;&#21319;&#22270;&#20687;&#35299;&#37322;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DSEG-LIME -- Improving Image Explanation by Hierarchical Data-Driven Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07733
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;&#21644;&#23618;&#27425;&#20998;&#21106;&#31243;&#24207;&#65292;DSEG-LIME&#25913;&#36827;&#20102;&#22270;&#20687;&#35299;&#37322;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#25581;&#31034;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;LIME (Local Interpretable Model-agnostic Explanations) &#26159;&#19968;&#20010;&#24191;&#20026;&#20154;&#30693;&#30340;&#29992;&#20110;&#22270;&#20687;&#20998;&#26512;&#30340;XAI&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#22270;&#20687;&#20998;&#21106;&#26469;&#21019;&#24314;&#29305;&#24449;&#20197;&#35782;&#21035;&#30456;&#20851;&#30340;&#20998;&#31867;&#21306;&#22495;&#12290;&#28982;&#32780;&#65292;&#36739;&#24046;&#30340;&#20998;&#21106;&#21487;&#33021;&#20250;&#24433;&#21709;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#24182;&#21066;&#24369;&#21508;&#20010;&#21306;&#22495;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#24433;&#21709;&#25972;&#20307;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DSEG-LIME (Data-Driven Segmentation LIME)&#65292;&#20855;&#26377;: i) &#29992;&#20110;&#29983;&#25104;&#20154;&#31867;&#21487;&#35782;&#21035;&#29305;&#24449;&#30340;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;, &#21644; ii) &#36890;&#36807;&#32452;&#21512;&#23454;&#29616;&#30340;&#23618;&#27425;&#20998;&#21106;&#31243;&#24207;&#12290;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#20351;&#29992;&#26469;&#33258;ImageNet&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#23545;DSEG-LIME&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;-&#36825;&#20123;&#24773;&#26223;&#19981;&#21253;&#21547;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#20998;&#26512;&#21253;&#25324;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;XAI&#25351;&#26631;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#30340;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07733v1 Announce Type: cross  Abstract: Explainable Artificial Intelligence is critical in unraveling decision-making processes in complex machine learning models. LIME (Local Interpretable Model-agnostic Explanations) is a well-known XAI framework for image analysis. It utilizes image segmentation to create features to identify relevant areas for classification. Consequently, poor segmentation can compromise the consistency of the explanation and undermine the importance of the segments, affecting the overall interpretability. Addressing these challenges, we introduce DSEG-LIME (Data-Driven Segmentation LIME), featuring: i) a data-driven segmentation for human-recognized feature generation, and ii) a hierarchical segmentation procedure through composition. We benchmark DSEG-LIME on pre-trained models with images from the ImageNet dataset - scenarios without domain-specific knowledge. The analysis includes a quantitative evaluation using established XAI metrics, complemented
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#38598;&#33976;&#39311;&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#19981;&#24120;&#35265;&#30340;&#23376;&#32452;&#30340;&#26679;&#26412;&#26102;&#65292;&#22914;&#20309;&#30830;&#20445;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#21487;&#20197;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.04676</link><description>&lt;p&gt;
&#24102;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#20998;&#32452;&#20998;&#24067;&#40065;&#26834;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Group Distributionally Robust Dataset Distillation with Risk Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04676
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#38598;&#33976;&#39311;&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#19981;&#24120;&#35265;&#30340;&#23376;&#32452;&#30340;&#26679;&#26412;&#26102;&#65292;&#22914;&#20309;&#30830;&#20445;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#21487;&#20197;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;DD&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#25429;&#25417;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22522;&#26412;&#20449;&#24687;&#26041;&#38754;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#20174;&#32780;&#26041;&#20415;&#20934;&#30830;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#12290;&#20854;&#24212;&#29992;&#28085;&#30422;&#20102;&#36716;&#31227;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#26159;&#23558;&#35757;&#32451;&#25968;&#25454;&#38598;&#35270;&#20026;&#36741;&#21161;&#65292;&#23601;&#20687;&#35757;&#32451;&#38598;&#26159;&#20154;&#21475;&#20998;&#24067;&#30340;&#36817;&#20284;&#26367;&#20195;&#21697;&#19968;&#26679;&#65292;&#32780;&#21518;&#32773;&#25165;&#26159;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;&#20854;&#21463;&#27426;&#36814;&#31243;&#24230;&#24456;&#39640;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#30340;&#19968;&#20010;&#26041;&#38754;&#26159;DD&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#26159;&#36328;&#19981;&#24120;&#35265;&#30340;&#23376;&#32452;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#24403;&#38754;&#23545;&#26469;&#33258;&#32597;&#35265;&#23376;&#32452;&#30340;&#26679;&#26412;&#26102;&#65292;&#25105;&#20204;&#22914;&#20309;&#30830;&#20445;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset distillation (DD) has emerged as a widely adopted technique for crafting a synthetic dataset that captures the essential information of a training dataset, facilitating the training of accurate neural models. Its applications span various domains, including transfer learning, federated learning, and neural architecture search. The most popular methods for constructing the synthetic data rely on matching the convergence properties of training the model with the synthetic dataset and the training dataset. However, targeting the training dataset must be thought of as auxiliary in the same sense that the training set is an approximate substitute for the population distribution, and the latter is the data of interest. Yet despite its popularity, an aspect that remains unexplored is the relationship of DD to its generalization, particularly across uncommon subgroups. That is, how can we ensure that a model trained on the synthetic dataset performs well when faced with samples from re
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#19978;&#34920;&#29616;&#20248;&#20110;&#28145;&#24230;&#32593;&#32476;&#65292;&#24403;&#32473;&#20104;&#36275;&#22815;&#26102;&#38388;&#26102;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22270;&#20687;&#26333;&#20809;&#26102;&#38388;&#30340;&#38480;&#21046;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#38477;&#33267;&#28145;&#24230;&#32593;&#32476;&#30340;&#27700;&#24179;&#65292;&#36825;&#26263;&#31034;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#38656;&#35201;&#39069;&#22806;&#30340;&#24515;&#29702;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#19982;&#32593;&#32476;&#20043;&#38388;&#30340;&#38169;&#35823;&#27169;&#24335;&#20063;&#23384;&#22312;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.03973</link><description>&lt;p&gt;
&#32473;&#20104;&#36275;&#22815;&#26102;&#38388;&#65292;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#19978;&#20987;&#36133;&#20102;&#28145;&#24230;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Humans Beat Deep Networks at Recognizing Objects in Unusual Poses, Given Enough Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03973
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#19978;&#34920;&#29616;&#20248;&#20110;&#28145;&#24230;&#32593;&#32476;&#65292;&#24403;&#32473;&#20104;&#36275;&#22815;&#26102;&#38388;&#26102;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22270;&#20687;&#26333;&#20809;&#26102;&#38388;&#30340;&#38480;&#21046;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#38477;&#33267;&#28145;&#24230;&#32593;&#32476;&#30340;&#27700;&#24179;&#65292;&#36825;&#26263;&#31034;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#38656;&#35201;&#39069;&#22806;&#30340;&#24515;&#29702;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#19982;&#32593;&#32476;&#20043;&#38388;&#30340;&#38169;&#35823;&#27169;&#24335;&#20063;&#23384;&#22312;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#20960;&#20010;&#29289;&#20307;&#35782;&#21035;&#22522;&#20934;&#19978;&#27491;&#22312;&#32553;&#23567;&#19982;&#20154;&#31867;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#22312;&#28041;&#21450;&#20174;&#19981;&#23547;&#24120;&#35270;&#35282;&#35266;&#23519;&#29289;&#20307;&#30340;&#25361;&#25112;&#24615;&#22270;&#20687;&#20013;&#23545;&#36825;&#19968;&#24046;&#36317;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#19982;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#65288;EfficientNet&#12289;SWAG&#12289;ViT&#12289;SWIN&#12289;BEiT&#12289;ConvNext&#65289;&#30456;&#27604;&#65292;&#36825;&#20123;&#32593;&#32476;&#22312;&#27492;&#24773;&#20917;&#19979;&#26222;&#36941;&#33030;&#24369;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#38543;&#30528;&#25105;&#20204;&#38480;&#21046;&#22270;&#20687;&#26333;&#20809;&#26102;&#38388;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#19979;&#38477;&#21040;&#28145;&#24230;&#32593;&#32476;&#30340;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#38656;&#35201;&#39069;&#22806;&#30340;&#24515;&#29702;&#36807;&#31243;&#65288;&#38656;&#35201;&#39069;&#22806;&#30340;&#26102;&#38388;&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20154;&#31867;&#19982;&#32593;&#32476;&#30340;&#38169;&#35823;&#27169;&#24335;&#65292;&#21457;&#29616;&#21363;&#20351;&#22312;&#38480;&#21046;&#26102;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#20154;&#31867;&#19982;&#21069;&#39304;&#28145;&#24230;&#32593;&#32476;&#20063;&#26377;&#19981;&#21516;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#24037;&#20316;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#24102;&#21040;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;&#29702;&#35299;&#22312;&#22806;&#37096;&#24773;&#20917;&#19979;&#21457;&#29983;&#30340;&#24515;&#29702;&#36807;&#31243;&#30340;&#26412;&#36136;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is closing the gap with humans on several object recognition benchmarks. Here we investigate this gap in the context of challenging images where objects are seen from unusual viewpoints. We find that humans excel at recognizing objects in unusual poses, in contrast with state-of-the-art pretrained networks (EfficientNet, SWAG, ViT, SWIN, BEiT, ConvNext) which are systematically brittle in this condition. Remarkably, as we limit image exposure time, human performance degrades to the level of deep networks, suggesting that additional mental processes (requiring additional time) take place when humans identify objects in unusual poses. Finally, our analysis of error patterns of humans vs. networks reveals that even time-limited humans are dissimilar to feed-forward deep networks. We conclude that more work is needed to bring computer vision systems to the level of robustness of the human visual system. Understanding the nature of the mental processes taking place during extr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;DermaMNIST&#21644;Fitzpatrick17k&#30382;&#32932;&#31185;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#38382;&#39064;&#65292;&#23545;&#25968;&#25454;&#37325;&#22797;&#12289;&#25968;&#25454;&#27844;&#28431;&#12289;&#38169;&#35823;&#26631;&#35760;&#21644;&#32570;&#20047;&#27979;&#35797;&#20998;&#21306;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#32416;&#27491;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2401.14497</link><description>&lt;p&gt;
&#30740;&#31350;DermaMNIST&#21644;Fitzpatrick17k&#30382;&#32932;&#31185;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Investigating the Quality of DermaMNIST and Fitzpatrick17k Dermatological Image Datasets. (arXiv:2401.14497v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;DermaMNIST&#21644;Fitzpatrick17k&#30382;&#32932;&#31185;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#38382;&#39064;&#65292;&#23545;&#25968;&#25454;&#37325;&#22797;&#12289;&#25968;&#25454;&#27844;&#28431;&#12289;&#38169;&#35823;&#26631;&#35760;&#21644;&#32570;&#20047;&#27979;&#35797;&#20998;&#21306;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#32416;&#27491;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#30382;&#32932;&#31185;&#20219;&#21153;&#20013;&#21462;&#24471;&#30340;&#26174;&#33879;&#36827;&#23637;&#20351;&#25105;&#20204;&#26356;&#25509;&#36817;&#20110;&#36798;&#21040;&#19982;&#20154;&#31867;&#19987;&#23478;&#30456;&#24403;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22823;&#22411;&#25968;&#25454;&#38598;&#22312;&#21487;&#38752;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24320;&#21457;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#25454;&#36136;&#37327;&#21644;&#20854;&#27491;&#30830;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#22810;&#31181;&#22240;&#32032;&#21487;&#20197;&#24433;&#21709;&#25968;&#25454;&#36136;&#37327;&#65292;&#22914;&#37325;&#22797;&#25968;&#25454;&#30340;&#23384;&#22312;&#65292;&#35757;&#32451;-&#27979;&#35797;&#20998;&#21306;&#30340;&#25968;&#25454;&#27844;&#28431;&#65292;&#38169;&#35823;&#26631;&#35760;&#30340;&#22270;&#20687;&#20197;&#21450;&#32570;&#20047;&#26126;&#30830;&#23450;&#20041;&#30340;&#27979;&#35797;&#20998;&#21306;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#27969;&#34892;&#30340;&#30382;&#32932;&#31185;&#22270;&#20687;&#25968;&#25454;&#38598;DermaMNIST&#21644;Fitzpatrick17k&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#65292;&#27979;&#37327;&#20102;&#36825;&#20123;&#38382;&#39064;&#23545;&#22522;&#20934;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#23545;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#32416;&#27491;&#25514;&#26045;&#12290;&#36890;&#36807;&#20844;&#24320;&#25105;&#20204;&#30340;&#20998;&#26512;&#27969;&#31243;&#21644;&#37197;&#22871;&#20195;&#30721;&#65292;&#30830;&#20445;&#25105;&#20204;&#20998;&#26512;&#30340;&#21487;&#37325;&#22797;&#24615;&#65292;&#25105;&#20204;&#26088;&#22312;&#40723;&#21169;&#31867;&#20284;&#30340;&#25506;&#32034;&#24182;&#20419;&#36827;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable progress of deep learning in dermatological tasks has brought us closer to achieving diagnostic accuracies comparable to those of human experts. However, while large datasets play a crucial role in the development of reliable deep neural network models, the quality of data therein and their correct usage are of paramount importance. Several factors can impact data quality, such as the presence of duplicates, data leakage across train-test partitions, mislabeled images, and the absence of a well-defined test partition. In this paper, we conduct meticulous analyses of two popular dermatological image datasets: DermaMNIST and Fitzpatrick17k, uncovering these data quality issues, measure the effects of these problems on the benchmark results, and propose corrections to the datasets. Besides ensuring the reproducibility of our analysis, by making our analysis pipeline and the accompanying code publicly available, we aim to encourage similar explorations and to facilitate the 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#27700;&#21360;&#25216;&#26415;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28431;&#27934;&#65292;&#24182;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#27700;&#21360;&#26426;&#21046;&#23481;&#26131;&#34987;&#23545;&#25163;&#30772;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.07726</link><description>&lt;p&gt;
&#23545;&#27700;&#21360;&#25216;&#26415;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28431;&#27934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards the Vulnerability of Watermarking Artificial Intelligence Generated Content. (arXiv:2310.07726v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#27700;&#21360;&#25216;&#26415;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28431;&#27934;&#65292;&#24182;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#27700;&#21360;&#26426;&#21046;&#23481;&#26131;&#34987;&#23545;&#25163;&#30772;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#35768;&#22810;&#21830;&#19994;&#26381;&#21153;&#24050;&#32463;&#25512;&#20986;&#12290;&#36825;&#20123;&#26381;&#21153;&#21033;&#29992;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#29992;&#25143;&#29983;&#25104;&#21019;&#24847;&#20869;&#23481;&#65288;&#20363;&#22914;&#36924;&#30495;&#30340;&#22270;&#20687;&#12289;&#27969;&#30021;&#30340;&#21477;&#23376;&#65289;&#12290;&#23545;&#20110;&#27492;&#31867;&#29983;&#25104;&#20869;&#23481;&#30340;&#20351;&#29992;&#38656;&#35201;&#39640;&#24230;&#30417;&#31649;&#65292;&#22240;&#20026;&#26381;&#21153;&#25552;&#20379;&#21830;&#38656;&#35201;&#30830;&#20445;&#29992;&#25143;&#19981;&#36829;&#21453;&#20351;&#29992;&#25919;&#31574;&#65288;&#20363;&#22914;&#28389;&#29992;&#21830;&#19994;&#21270;&#12289;&#29983;&#25104;&#21644;&#20998;&#21457;&#19981;&#23433;&#20840;&#30340;&#20869;&#23481;&#65289;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#27700;&#21360;&#25216;&#26415;&#65292;&#20294;&#26159;&#26412;&#25991;&#34920;&#26126;&#23545;&#25163;&#21487;&#20197;&#36731;&#26131;&#30772;&#35299;&#36825;&#20123;&#27700;&#21360;&#26426;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#21487;&#33021;&#30340;&#25915;&#20987;&#26041;&#24335;&#65306;&#65288;1&#65289;&#27700;&#21360;&#21435;&#38500;&#65306;&#23545;&#25163;&#21487;&#20197;&#36731;&#26494;&#22320;&#20174;&#29983;&#25104;&#20869;&#23481;&#20013;&#21024;&#38500;&#23884;&#20837;&#30340;&#27700;&#21360;&#65292;&#28982;&#21518;&#33258;&#30001;&#20351;&#29992;&#32780;&#19981;&#21463;&#26381;&#21153;&#25552;&#20379;&#21830;&#30340;&#38480;&#21046;&#65307;&#65288;2&#65289;&#27700;&#21360;&#20266;&#36896;&#65306;&#23545;&#25163;&#21487;&#20197;&#21019;&#24314;&#38750;&#27861;&#30340;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence Generated Content (AIGC) is gaining great popularity in social media, with many commercial services available. These services leverage advanced generative models, such as latent diffusion models and large language models, to generate creative content (e.g., realistic images, fluent sentences) for users. The usage of such generated content needs to be highly regulated, as the service providers need to ensure the users do not violate the usage policies (e.g., abuse for commercialization, generating and distributing unsafe content).  Numerous watermarking approaches have been proposed recently. However, in this paper, we show that an adversary can easily break these watermarking mechanisms. Specifically, we consider two possible attacks. (1) Watermark removal: the adversary can easily erase the embedded watermark from the generated content and then use it freely without the regulation of the service provider. (2) Watermark forge: the adversary can create illegal co
&lt;/p&gt;</description></item></channel></rss>