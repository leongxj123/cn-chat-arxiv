<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21457;&#29616;LVLMs&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.18409</link><description>&lt;p&gt;
&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22270;&#20687;&#25512;&#29702;&#21644;&#25551;&#36848;&#30340;&#35748;&#30693;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18409
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21457;&#29616;LVLMs&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(LVLMs)&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#21463;&#21040;&#20840;&#38754;&#30340;&#35748;&#30693;&#33021;&#21147;&#27979;&#35797;&#12290;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#27979;&#35797;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#8220;&#20599;&#39292;&#24178;&#8221;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21033;&#29992;&#20855;&#26377;&#20016;&#23500;&#35821;&#20041;&#30340;&#22270;&#20687;&#35780;&#20272;LVLMs&#30340;&#39640;&#32423;&#35748;&#30693;&#33021;&#21147;&#12290;&#23427;&#23450;&#20041;&#20102;&#20843;&#31181;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21253;&#25324;&#22270;&#20687;&#25551;&#36848;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#12290;&#25105;&#20204;&#23545;&#30693;&#21517;LVLMs&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;LVLMs&#21644;&#20154;&#31867;&#20043;&#38388;&#20173;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18409v1 Announce Type: new  Abstract: Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the "Cookie Theft" task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics. It defines eight reasoning capabilities and consists of an image description task and a visual question answering task. Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.
&lt;/p&gt;</description></item><item><title>&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25104;&#21151;&#22312;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.17767</link><description>&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20351;&#29992;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;
&lt;/p&gt;
&lt;p&gt;
Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17767
&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25104;&#21151;&#22312;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#65288;Stretch RE2&#65289;&#33021;&#22815;&#22312;&#22810;&#26679;&#30340;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25289;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#12290;&#25105;&#20204;&#22312;31&#20010;&#19981;&#21516;&#30340;&#29289;&#20307;&#21644;13&#20010;&#19981;&#21516;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;4&#22825;&#30340;&#23454;&#38469;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#38646;&#20987;&#25171;&#19979;&#65292;&#23545;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#26032;&#39062;&#30340;&#27249;&#26588;&#21644;&#25277;&#23625;&#30340;&#25171;&#24320;&#29575;&#36798;&#21040;61%&#12290;&#23545;&#22833;&#36133;&#27169;&#24335;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#25105;&#20204;&#31995;&#32479;&#38754;&#20020;&#30340;&#26368;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17767v1 Announce Type: cross  Abstract: Pulling open cabinets and drawers presents many difficult technical challenges in perception (inferring articulation parameters for objects from onboard sensors), planning (producing motion plans that conform to tight task constraints), and control (making and maintaining contact while applying forces on the environment). In this work, we build an end-to-end system that enables a commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in diverse previously unseen real world environments. We conduct 4 days of real world testing of this system spanning 31 different objects from across 13 different real world environments. Our system achieves a success rate of 61% on opening novel cabinets and drawers in unseen environments zero-shot. An analysis of the failure modes suggests that errors in perception are the most significant challenge for our system. We will open source code and models for others to replicate and bui
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26631;&#31614;&#22686;&#24378;&#26041;&#27861;&#20013;&#26631;&#31614;&#22686;&#24378;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#20351;&#29992;&#26631;&#31614;&#22686;&#24378;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#21482;&#33021;&#23398;&#20064;&#21040;&#26368;&#23567;&#26041;&#24046;&#29305;&#24449;&#65292;&#32780;&#26631;&#20934;&#35757;&#32451;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#39640;&#26041;&#24046;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#26631;&#31614;&#24179;&#28369;&#21644;Mixup&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25239;&#25200;&#21160;&#21487;&#33021;&#19981;&#22826;&#40065;&#26834;&#12290;</title><link>https://arxiv.org/abs/2402.06855</link><description>&lt;p&gt;
&#26356;&#22909;&#36824;&#26159;&#26356;&#24046;&#65311;&#36890;&#36807;&#26631;&#31614;&#22686;&#24378;&#23398;&#20064;&#26368;&#23567;&#26041;&#24046;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
For Better or For Worse? Learning Minimum Variance Features With Label Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26631;&#31614;&#22686;&#24378;&#26041;&#27861;&#20013;&#26631;&#31614;&#22686;&#24378;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#20351;&#29992;&#26631;&#31614;&#22686;&#24378;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#21482;&#33021;&#23398;&#20064;&#21040;&#26368;&#23567;&#26041;&#24046;&#29305;&#24449;&#65292;&#32780;&#26631;&#20934;&#35757;&#32451;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#39640;&#26041;&#24046;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#26631;&#31614;&#24179;&#28369;&#21644;Mixup&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25239;&#25200;&#21160;&#21487;&#33021;&#19981;&#22826;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#23545;&#20110;&#25104;&#21151;&#22320;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#23376;&#31867;-&#21253;&#25324;&#26631;&#31614;&#24179;&#28369;&#21644;Mixup-&#28041;&#21450;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20462;&#25913;&#36755;&#20837;&#25968;&#25454;&#21644;&#36755;&#20837;&#26631;&#31614;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27492;&#31867;&#26041;&#27861;&#20013;&#26631;&#31614;&#22686;&#24378;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#20351;&#29992;&#26631;&#31614;&#22686;&#24378;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#21482;&#33021;&#23398;&#20064;&#21040;&#26368;&#23567;&#26041;&#24046;&#29305;&#24449;&#65292;&#32780;&#26631;&#20934;&#35757;&#32451;&#65288;&#21253;&#25324;&#26435;&#37325;&#34928;&#20943;&#65289;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#39640;&#26041;&#24046;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#30340;&#19968;&#20010;&#37325;&#35201;&#21518;&#26524;&#26159;&#28040;&#26497;&#30340;&#65306;&#19982;&#26631;&#20934;&#35757;&#32451;&#30456;&#27604;&#65292;&#26631;&#31614;&#24179;&#28369;&#21644;Mixup&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25239;&#25200;&#21160;&#21487;&#33021;&#19981;&#22826;&#40065;&#26834;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#19982;&#23454;&#36341;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation has been pivotal in successfully training deep learning models on classification tasks over the past decade. An important subclass of data augmentation techniques - which includes both label smoothing and Mixup - involves modifying not only the input data but also the input label during model training. In this work, we analyze the role played by the label augmentation aspect of such methods. We prove that linear models on linearly separable data trained with label augmentation learn only the minimum variance features in the data, while standard training (which includes weight decay) can learn higher variance features. An important consequence of our results is negative: label smoothing and Mixup can be less robust to adversarial perturbations of the training data when compared to standard training. We verify that our theory reflects practice via a range of experiments on synthetic data and image classification benchmarks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#23545;&#20110;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26356;&#26377;&#25928;&#30340;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#26041;&#27861;&#65292;&#20026;&#27492;&#35774;&#35745;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#36890;&#36807;&#20351;&#29992;&#35813;&#22522;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#25490;&#29256;&#25915;&#20987;&#23545;LVLM&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.00626</link><description>&lt;p&gt;
Vision-LLMs&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#21487;&#20197;&#33258;&#27450;&#27450;&#20154;
&lt;/p&gt;
&lt;p&gt;
Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00626
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#23545;&#20110;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26356;&#26377;&#25928;&#30340;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#26041;&#27861;&#65292;&#20026;&#27492;&#35774;&#35745;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#36890;&#36807;&#20351;&#29992;&#35813;&#22522;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#25490;&#29256;&#25915;&#20987;&#23545;LVLM&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65307;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#26032;&#31867;&#21035;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;LVLM&#23545;&#20110;&#28041;&#21450;&#23558;&#35823;&#23548;&#24615;&#25991;&#26412;&#21472;&#21152;&#21040;&#22270;&#20687;&#19978;&#30340;&#20174;&#25490;&#29256;&#25915;&#20987;&#30340;&#23481;&#26131;&#21463;&#25915;&#20987;&#24615;&#21364;&#27809;&#26377;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#25490;&#29256;&#25915;&#20987;&#20381;&#36182;&#20110;&#20174;&#39044;&#23450;&#20041;&#31867;&#21035;&#38598;&#21512;&#20013;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#35823;&#23548;&#24615;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#36873;&#25321;&#30340;&#31867;&#21035;&#21487;&#33021;&#19981;&#26159;&#26368;&#26377;&#25928;&#30340;&#25915;&#20987;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#29420;&#29305;&#35774;&#35745;&#30340;&#26032;&#39062;&#22522;&#20934;&#26469;&#27979;&#35797;LVLM&#23545;&#25490;&#29256;&#25915;&#20987;&#30340;&#23481;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#32780;&#26356;&#26377;&#25928;&#30340;&#25490;&#29256;&#25915;&#20987;&#65306;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#31616;&#21333;&#22320;&#25552;&#31034;GPT-4V&#31561;&#27169;&#22411;&#21033;&#29992;&#20854;&#24378;&#22823;&#30340;&#35821;&#35328;&#33021;&#21147;&#25512;&#33616;&#19968;&#31181;&#25490;&#29256;&#25915;&#20987;&#26469;&#20026;&#32473;&#23450;&#30340;&#22270;&#20687;&#29983;&#25104;&#25915;&#20987;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#39062;&#22522;&#20934;&#65292;&#25105;&#20204;&#21457;&#29616;&#25490;&#29256;&#25915;&#20987;&#23545;LVLM&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, significant progress has been made on Large Vision-Language Models (LVLMs); a new class of VL models that make use of large pre-trained language models. Yet, their vulnerability to Typographic attacks, which involve superimposing misleading text onto an image remain unstudied. Furthermore, prior work typographic attacks rely on sampling a random misleading class from a predefined set of classes. However, the random chosen class might not be the most effective attack. To address these issues, we first introduce a novel benchmark uniquely designed to test LVLMs vulnerability to typographic attacks. Furthermore, we introduce a new and more effective typographic attack: Self-Generated typographic attacks. Indeed, our method, given an image, make use of the strong language capabilities of models like GPT-4V by simply prompting them to recommend a typographic attack. Using our novel benchmark, we uncover that typographic attacks represent a significant threat against LVLM(s). Furth
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#32423;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#21516;&#26102;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#21106;&#27169;&#22411;&#21021;&#22987;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.11876</link><description>&lt;p&gt;
&#22810;&#32423;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39044;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-level Asymmetric Contrastive Learning for Medical Image Segmentation Pre-training. (arXiv:2309.11876v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#32423;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#21516;&#26102;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#21106;&#27169;&#22411;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#22270;&#20687;&#32423;&#34920;&#31034;&#30340;&#24378;&#22823;&#25216;&#26415;&#65292;&#20026;&#35299;&#20915;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21644;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#20043;&#38388;&#30340;&#22256;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#20027;&#35201;&#38024;&#23545;&#33258;&#28982;&#22270;&#20687;&#30340;&#19979;&#28216;&#20219;&#21153;&#35774;&#35745;&#65292;&#22240;&#27492;&#24403;&#30452;&#25509;&#24212;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#65288;&#20854;&#19979;&#28216;&#20219;&#21153;&#36890;&#24120;&#26159;&#20998;&#21106;&#65289;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#26159;&#27425;&#20248;&#30340;&#29978;&#33267;&#19981;&#22914;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JCL&#30340;&#26032;&#22411;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#65288;1&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#21516;&#26102;&#22312;&#19968;&#38454;&#27573;&#20869;&#23545;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#21106;&#27169;&#22411;&#21021;&#22987;&#21270;&#12290; &#65288;2&#65289;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#32423;&#23545;&#27604;&#25439;&#22833;&#65292;&#29992;&#20110;&#32771;&#34385;&#29305;&#24449;&#32423;&#21035;&#12289;&#22270;&#20687;&#32423;&#21035;&#21644;&#20687;&#32032;&#32423;&#21035;&#25237;&#24433;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning, which is a powerful technique for learning image-level representations from unlabeled data, leads a promising direction to dealing with the dilemma between large-scale pre-training and limited labeled data. However, most existing contrastive learning strategies are designed mainly for downstream tasks of natural images, therefore they are sub-optimal and even worse than learning from scratch when directly applied to medical images whose downstream tasks are usually segmentation. In this work, we propose a novel asymmetric contrastive learning framework named JCL for medical image segmentation with self-supervised pre-training. Specifically, (1) A novel asymmetric contrastive learning strategy is proposed to pre-train both encoder and decoder simultaneously in one-stage to provide better initialization for segmentation models. (2) A multi-level contrastive loss is designed to take the correspondence among feature-level, image-level and pixel-level projections, resp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MonoDETR&#30340;&#28145;&#24230;&#24341;&#23548;Transformer&#26694;&#26550;&#65292;&#29992;&#20110;&#21333;&#30446;3D&#30446;&#26631;&#26816;&#27979;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#26041;&#27861;&#65292;MonoDETR&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#20449;&#24687;&#26469;&#25351;&#23548;&#25972;&#20010;&#26816;&#27979;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#23545;&#22330;&#26223;&#30340;&#29702;&#35299;&#21644;&#30446;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.13310</link><description>&lt;p&gt;
MonoDETR&#65306;&#28145;&#24230;&#24341;&#23548;&#30340;&#21333;&#30446;3D&#30446;&#26631;&#26816;&#27979;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection. (arXiv:2203.13310v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.13310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MonoDETR&#30340;&#28145;&#24230;&#24341;&#23548;Transformer&#26694;&#26550;&#65292;&#29992;&#20110;&#21333;&#30446;3D&#30446;&#26631;&#26816;&#27979;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#26041;&#27861;&#65292;MonoDETR&#36890;&#36807;&#24341;&#20837;&#28145;&#24230;&#20449;&#24687;&#26469;&#25351;&#23548;&#25972;&#20010;&#26816;&#27979;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#23545;&#22330;&#26223;&#30340;&#29702;&#35299;&#21644;&#30446;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#30446;&#19977;&#32500;&#30446;&#26631;&#26816;&#27979;&#19968;&#30452;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26159;&#26681;&#25454;&#20256;&#32479;&#30340;&#20108;&#32500;&#26816;&#27979;&#22120;&#39318;&#20808;&#23450;&#20301;&#30446;&#26631;&#20013;&#24515;&#65292;&#28982;&#21518;&#36890;&#36807;&#37051;&#36817;&#29305;&#24449;&#39044;&#27979;&#19977;&#32500;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#20351;&#29992;&#23616;&#37096;&#35270;&#35273;&#29305;&#24449;&#26159;&#19981;&#36275;&#20197;&#29702;&#35299;&#22330;&#26223;&#32423;&#21035;&#30340;&#19977;&#32500;&#31354;&#38388;&#32467;&#26500;&#24182;&#24573;&#30053;&#20102;&#36828;&#36317;&#31163;&#30340;&#30446;&#26631;&#28145;&#24230;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#37319;&#29992;&#28145;&#24230;&#24341;&#23548;Transformer&#30340;&#21333;&#30446;&#26816;&#27979;&#26694;&#26550;&#65292;&#31216;&#20026;MonoDETR&#12290;&#25105;&#20204;&#23558;&#22522;&#26412;&#30340;Transformer&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20351;&#20854;&#20855;&#26377;&#28145;&#24230;&#24863;&#30693;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#28145;&#24230;&#32447;&#32034;&#26469;&#25351;&#23548;&#25972;&#20010;&#26816;&#27979;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#25429;&#25417;&#29289;&#20307;&#22806;&#35266;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39044;&#27979;&#21069;&#26223;&#28145;&#24230;&#22270;&#65292;&#24182;&#19987;&#38376;&#35774;&#35745;&#20102;&#19968;&#20010;&#28145;&#24230;&#32534;&#30721;&#22120;&#26469;&#25552;&#21462;&#38750;&#23616;&#37096;&#28145;&#24230;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#19977;&#32500;&#30446;&#26631;&#20505;&#36873;&#29289;&#24418;&#24335;&#21270;&#20026;&#21487;&#23398;&#20064;&#30340;&#26597;&#35810;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#24341;&#23548;&#30340;&#35299;&#30721;&#22120;&#26469;&#36827;&#34892;&#30446;&#26631;-&#22330;&#26223;&#28145;&#24230;&#20132;&#20114;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#27599;&#20010;&#30446;&#26631;&#37117;&#21487;&#20197;&#24471;&#21040;&#26356;&#20840;&#38754;&#30340;&#28145;&#24230;&#24863;&#30693;&#21644;&#26356;&#20934;&#30830;&#30340;&#19977;&#32500;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monocular 3D object detection has long been a challenging task in autonomous driving. Most existing methods follow conventional 2D detectors to first localize object centers, and then predict 3D attributes by neighboring features. However, only using local visual features is insufficient to understand the scene-level 3D spatial structures and ignores the long-range inter-object depth relations. In this paper, we introduce the first DETR framework for Monocular DEtection with a depth-guided TRansformer, named MonoDETR. We modify the vanilla transformer to be depth-aware and guide the whole detection process by contextual depth cues. Specifically, concurrent to the visual encoder that captures object appearances, we introduce to predict a foreground depth map, and specialize a depth encoder to extract non-local depth embeddings. Then, we formulate 3D object candidates as learnable queries and propose a depth-guided decoder to conduct object-scene depth interactions. In this way, each obj
&lt;/p&gt;</description></item></channel></rss>