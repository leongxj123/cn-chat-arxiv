<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>ParFormer&#25552;&#20986;&#20102;&#24182;&#34892;&#23616;&#37096;&#20840;&#23616;&#26631;&#35760;&#28151;&#21512;&#22120;&#21644;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;&#65292;&#20248;&#21270;&#20102;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;CNN&#21644;&#26368;&#20808;&#36827;&#30340;Transformer&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.15004</link><description>&lt;p&gt;
ParFormer&#65306;&#20855;&#26377;&#24182;&#34892;&#23616;&#37096;&#20840;&#23616;&#26631;&#35760;&#28151;&#21512;&#22120;&#21644;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;&#30340;&#35270;&#35273;Transformer&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
ParFormer: Vision Transformer Baseline with Parallel Local Global Token Mixer and Convolution Attention Patch Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15004
&lt;/p&gt;
&lt;p&gt;
ParFormer&#25552;&#20986;&#20102;&#24182;&#34892;&#23616;&#37096;&#20840;&#23616;&#26631;&#35760;&#28151;&#21512;&#22120;&#21644;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;&#65292;&#20248;&#21270;&#20102;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;CNN&#21644;&#26368;&#20808;&#36827;&#30340;Transformer&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ParFormer&#20316;&#20026;&#19968;&#31181;&#22686;&#24378;&#22411;Transformer&#26550;&#26500;&#65292;&#20801;&#35768;&#23558;&#19981;&#21516;&#30340;&#26631;&#35760;&#28151;&#21512;&#22120;&#25972;&#21512;&#21040;&#21333;&#20010;&#38454;&#27573;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#12290;&#21516;&#26102;&#25972;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#25968;&#25454;&#65292;&#23454;&#29616;&#23545;&#30701;&#31243;&#21644;&#38271;&#31243;&#31354;&#38388;&#20851;&#31995;&#30340;&#31934;&#30830;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#20687;&#24179;&#31227;&#31383;&#21475;&#36825;&#26679;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#38500;&#20102;&#24182;&#34892;&#26631;&#35760;&#28151;&#21512;&#22120;&#32534;&#30721;&#22120;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;(CAPE)&#65292;&#20316;&#20026;&#26631;&#20934;&#34917;&#19969;&#23884;&#20837;&#30340;&#22686;&#24378;&#65292;&#36890;&#36807;&#21367;&#31215;&#27880;&#24847;&#21147;&#27169;&#22359;&#25913;&#36827;&#26631;&#35760;&#28151;&#21512;&#22120;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;ParFormer&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29289;&#20307;&#35782;&#21035;&#31561;&#22810;&#20010;&#22797;&#26434;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20110;CNN&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;CAPE&#24050;&#34987;&#35777;&#26126;&#26377;&#30410;&#20110;&#25972;&#20307;MetaFormer&#26550;&#26500;&#65292;&#21363;&#20351;&#20351;&#29992;Id&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15004v1 Announce Type: cross  Abstract: This work presents ParFormer as an enhanced transformer architecture that allows the incorporation of different token mixers into a single stage, hence improving feature extraction capabilities. Integrating both local and global data allows for precise representation of short- and long-range spatial relationships without the need for computationally intensive methods such as shifting windows. Along with the parallel token mixer encoder, We offer the Convolutional Attention Patch Embedding (CAPE) as an enhancement of standard patch embedding to improve token mixer extraction with a convolutional attention module. Our comprehensive evaluation demonstrates that our ParFormer outperforms CNN-based and state-of-the-art transformer-based architectures in image classification and several complex tasks such as object recognition. The proposed CAPE has been demonstrated to benefit the overall MetaFormer architecture, even while utilizing the Id
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#26041;&#27861;$\sigma$-zero&#65292;&#20854;&#21033;&#29992;&#20102;$\ell_0$&#33539;&#25968;&#30340;&#21487;&#24494;&#36817;&#20284;&#21644;&#33258;&#36866;&#24212;&#25237;&#24433;&#36816;&#31639;&#31526;&#65292;&#33021;&#22815;&#22312;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#32422;&#26463;&#19979;&#20248;&#21270;&#65292;&#20174;&#32780;&#35780;&#20272;&#28145;&#24230;&#32593;&#32476;&#23545;&#31232;&#30095;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01879</link><description>&lt;p&gt;
$\sigma$-zero: &#22522;&#20110;&#26799;&#24230;&#30340;$\ell_0$-&#33539;&#25968;&#23545;&#25239;&#26679;&#26412;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
$\sigma$-zero: Gradient-based Optimization of $\ell_0$-norm Adversarial Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01879
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#26041;&#27861;$\sigma$-zero&#65292;&#20854;&#21033;&#29992;&#20102;$\ell_0$&#33539;&#25968;&#30340;&#21487;&#24494;&#36817;&#20284;&#21644;&#33258;&#36866;&#24212;&#25237;&#24433;&#36816;&#31639;&#31526;&#65292;&#33021;&#22815;&#22312;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#32422;&#26463;&#19979;&#20248;&#21270;&#65292;&#20174;&#32780;&#35780;&#20272;&#28145;&#24230;&#32593;&#32476;&#23545;&#31232;&#30095;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#32593;&#32476;&#23545;&#22522;&#20110;&#26799;&#24230;&#25915;&#20987;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#25915;&#20987;&#32771;&#34385;$\ell_2$&#21644;$\ell_\infty$&#33539;&#25968;&#32422;&#26463;&#26469;&#21046;&#36896;&#36755;&#20837;&#25200;&#21160;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#20102;&#31232;&#30095;&#30340;$\ell_1$&#21644;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#12290;&#29305;&#21035;&#26159;&#65292;&#30001;&#20110;&#22312;&#38750;&#20984;&#19988;&#38750;&#21487;&#24494;&#32422;&#26463;&#19978;&#36827;&#34892;&#20248;&#21270;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#26159;&#30740;&#31350;&#26368;&#23569;&#30340;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#25915;&#20987;&#35780;&#20272;&#23545;&#25239;&#40065;&#26834;&#24615;&#21487;&#20197;&#25581;&#31034;&#22312;&#26356;&#20256;&#32479;&#30340;$\ell_2$&#21644;$\ell_\infty$&#33539;&#25968;&#25915;&#20987;&#20013;&#26410;&#33021;&#27979;&#35797;&#20986;&#30340;&#24369;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#65292;&#31216;&#20026;$\sigma$-zero&#65292;&#23427;&#21033;&#29992;&#20102;$\ell_0$&#33539;&#25968;&#30340;&#19968;&#20010;&#29305;&#27530;&#21487;&#24494;&#36817;&#20284;&#26469;&#20419;&#36827;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#24182;&#21033;&#29992;&#33258;&#36866;&#24212;&#25237;&#24433;&#36816;&#31639;&#31526;&#21160;&#24577;&#35843;&#25972;&#25439;&#22833;&#26368;&#23567;&#21270;&#21644;&#25200;&#21160;&#31232;&#30095;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#22312;MNIST&#12289;CIFAR10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Evaluating the adversarial robustness of deep networks to gradient-based attacks is challenging. While most attacks consider $\ell_2$- and $\ell_\infty$-norm constraints to craft input perturbations, only a few investigate sparse $\ell_1$- and $\ell_0$-norm attacks. In particular, $\ell_0$-norm attacks remain the least studied due to the inherent complexity of optimizing over a non-convex and non-differentiable constraint. However, evaluating adversarial robustness under these attacks could reveal weaknesses otherwise left untested with more conventional $\ell_2$- and $\ell_\infty$-norm attacks. In this work, we propose a novel $\ell_0$-norm attack, called $\sigma$-zero, which leverages an ad hoc differentiable approximation of the $\ell_0$ norm to facilitate gradient-based optimization, and an adaptive projection operator to dynamically adjust the trade-off between loss minimization and perturbation sparsity. Extensive evaluations using MNIST, CIFAR10, and ImageNet datasets, involving
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;OCTDL&#30340;&#24320;&#25918;&#33719;&#21462;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;2000&#24352;&#26631;&#35760;&#26377;&#30142;&#30149;&#32452;&#21644;&#35270;&#32593;&#33180;&#30149;&#29702;&#30340;OCT&#22270;&#20687;&#65292;&#26377;&#21161;&#20110;&#35786;&#26029;&#30524;&#37096;&#29366;&#20917;&#12290;</title><link>https://arxiv.org/abs/2312.08255</link><description>&lt;p&gt;
OCTDL&#65306;&#22522;&#20110;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08255
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;OCTDL&#30340;&#24320;&#25918;&#33719;&#21462;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;2000&#24352;&#26631;&#35760;&#26377;&#30142;&#30149;&#32452;&#21644;&#35270;&#32593;&#33180;&#30149;&#29702;&#30340;OCT&#22270;&#20687;&#65292;&#26377;&#21161;&#20110;&#35786;&#26029;&#30524;&#37096;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#25104;&#20687;&#25216;&#26415;&#65292;&#22312;&#30524;&#31185;&#23398;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;OCT&#21487;&#20197;&#21487;&#35270;&#21270;&#35270;&#32593;&#33180;&#23618;&#65292;&#23545;&#26089;&#26399;&#26816;&#27979;&#21644;&#30417;&#27979;&#35270;&#32593;&#33180;&#30142;&#30149;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#25918;&#33719;&#21462;&#30340;OCT&#25968;&#25454;&#38598;&#65288;OCTDL&#65289;&#65292;&#21253;&#25324;&#36229;&#36807;2000&#24352;&#26681;&#25454;&#30142;&#30149;&#32452;&#21644;&#35270;&#32593;&#33180;&#30149;&#29702;&#26631;&#35760;&#30340;OCT&#22270;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#24739;&#26377;&#32769;&#24180;&#24615;&#40644;&#26001;&#21464;&#24615;&#65288;AMD&#65289;&#12289;&#31958;&#23615;&#30149;&#40644;&#26001;&#27700;&#32959;&#65288;DME&#65289;&#12289;&#29627;&#29827;&#20307;&#35270;&#32593;&#33180;&#33180;&#65288;ERM&#65289;&#12289;&#35270;&#32593;&#33180;&#21160;&#33033;&#38381;&#22622;&#65288;RAO&#65289;&#12289;&#35270;&#32593;&#33180;&#38745;&#33033;&#38381;&#22622;&#65288;RVO&#65289;&#21644;&#29627;&#29827;&#20307;&#40644;&#26001;&#30028;&#38754;&#30142;&#30149;&#65288;VID&#65289;&#30340;&#24739;&#32773;&#30340;OCT&#35760;&#24405;&#12290;&#36825;&#20123;&#22270;&#20687;&#26159;&#20351;&#29992;Optovue Avanti RTVue XR&#37319;&#38598;&#30340;&#65292;&#37319;&#29992;&#20102;&#21160;&#24577;&#25195;&#25551;&#38271;&#24230;&#30340;&#20809;&#26629;&#25195;&#25551;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08255v2 Announce Type: replace-cross  Abstract: Optical coherence tomography (OCT) is a non-invasive imaging technique with extensive clinical applications in ophthalmology. OCT enables the visualization of the retinal layers, playing a vital role in the early detection and monitoring of retinal diseases. OCT uses the principle of light wave interference to create detailed images of the retinal microstructures, making it a valuable tool for diagnosing ocular conditions. This work presents an open-access OCT dataset (OCTDL) comprising over 2000 OCT images labeled according to disease group and retinal pathology. The dataset consists of OCT records of patients with Age-related Macular Degeneration (AMD), Diabetic Macular Edema (DME), Epiretinal Membrane (ERM), Retinal Artery Occlusion (RAO), Retinal Vein Occlusion (RVO), and Vitreomacular Interface Disease (VID). The images were acquired with an Optovue Avanti RTVue XR using raster scanning protocols with dynamic scan length a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#21487;&#39640;&#31934;&#24230;&#39044;&#27979;&#19977;&#31181;&#30524;&#30142;&#24739;&#32773;&#30340;&#35270;&#21147;&#21464;&#21270;&#65292;&#24182;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;</title><link>http://arxiv.org/abs/2204.11970</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#23545;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#36827;&#34892;&#35270;&#21147;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Visual Acuity Prediction on Real-Life Patient Data Using a Machine Learning Based Multistage System. (arXiv:2204.11970v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#21487;&#39640;&#31934;&#24230;&#39044;&#27979;&#19977;&#31181;&#30524;&#30142;&#24739;&#32773;&#30340;&#35270;&#21147;&#21464;&#21270;&#65292;&#24182;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#30524;&#31185;&#23398;&#20013;&#30340;&#29627;&#29827;&#20307;&#25163;&#26415;&#33647;&#29289;&#27835;&#30103;&#26159;&#27835;&#30103;&#24180;&#40836;&#30456;&#20851;&#24615;&#40644;&#26001;&#21464;&#24615;&#65288;AMD&#65289;&#12289;&#31958;&#23615;&#30149;&#24615;&#40644;&#26001;&#27700;&#32959;&#65288;DME&#65289;&#21644;&#35270;&#32593;&#33180;&#38745;&#33033;&#38459;&#22622;&#65288;RVO&#65289;&#30456;&#20851;&#30142;&#30149;&#30340;&#19968;&#31181;&#26222;&#36941;&#27835;&#30103;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#21644;&#19981;&#23436;&#25972;&#24615;&#65292;&#24739;&#32773;&#24448;&#24448;&#20250;&#22312;&#22810;&#24180;&#26102;&#38388;&#20869;&#22833;&#21435;&#35270;&#21147;&#65292;&#23613;&#31649;&#25509;&#21463;&#27835;&#30103;&#12290;&#26412;&#25991;&#37319;&#29992;&#22810;&#31181;IT&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#25104;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#34701;&#21512;&#20102;&#24503;&#22269;&#19968;&#23478;&#26368;&#20339;&#21307;&#30103;&#20445;&#20581;&#21307;&#38498;&#30340;&#30524;&#31185;&#37096;&#38376;&#30340;&#19981;&#21516;IT&#31995;&#32479;&#12290;&#32463;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#24739;&#32773;&#35270;&#21147;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20026;&#19977;&#31181;&#30142;&#30149;&#30340;&#39044;&#27979;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20316;&#20026;&#24037;&#20855;&#65292;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
In ophthalmology, intravitreal operative medication therapy (IVOM) is a widespread treatment for diseases related to the age-related macular degeneration (AMD), the diabetic macular edema (DME), as well as the retinal vein occlusion (RVO). However, in real-world settings, patients often suffer from loss of vision on time scales of years despite therapy, whereas the prediction of the visual acuity (VA) and the earliest possible detection of deterioration under real-life conditions is challenging due to heterogeneous and incomplete data. In this contribution, we present a workflow for the development of a research-compatible data corpus fusing different IT systems of the department of ophthalmology of a German maximum care hospital. The extensive data corpus allows predictive statements of the expected progression of a patient and his or her VA in each of the three diseases. We found out for the disease AMD a significant deterioration of the visual acuity over time. Within our proposed m
&lt;/p&gt;</description></item></channel></rss>