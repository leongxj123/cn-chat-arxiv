<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#22823;&#27744;&#21270;&#29305;&#24449;&#22270;&#30340;&#20301;&#31227;&#19981;&#21464;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#22797;&#25968;&#27169;&#30340;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#20301;&#31227;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#29702;&#35770;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.11740</link><description>&lt;p&gt;
&#20851;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#22823;&#27744;&#21270;&#29305;&#24449;&#22270;&#30340;&#20301;&#31227;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Shift Invariance of Max Pooling Feature Maps in Convolutional Neural Networks. (arXiv:2209.11740v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#22823;&#27744;&#21270;&#29305;&#24449;&#22270;&#30340;&#20301;&#31227;&#19981;&#21464;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#22797;&#25968;&#27169;&#30340;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#20301;&#31227;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#29702;&#35770;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#25913;&#21892;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#30340;&#25968;&#23398;&#21487;&#35299;&#37322;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#20854;&#31532;&#19968;&#23618;&#20013;&#20986;&#29616;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#24403;&#22312;&#20687;ImageNet&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#20854;&#31532;&#19968;&#23618;&#24448;&#24448;&#23398;&#20064;&#21040;&#19982;&#26041;&#21521;&#36793;&#36890;&#28388;&#27874;&#22120;&#38750;&#24120;&#30456;&#20284;&#30340;&#21442;&#25968;&#12290;&#20351;&#29992;&#36825;&#26679;&#30340;Gabor&#28388;&#27874;&#22120;&#36827;&#34892;&#23376;&#37319;&#26679;&#21367;&#31215;&#23481;&#26131;&#20986;&#29616;&#28151;&#21472;&#38382;&#39064;&#65292;&#23548;&#33268;&#23545;&#36755;&#20837;&#30340;&#23567;&#20559;&#31227;&#25935;&#24863;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26368;&#22823;&#27744;&#21270;&#31639;&#23376;&#36817;&#20284;&#22797;&#25968;&#27169;&#30340;&#26465;&#20214;&#65292;&#20351;&#20854;&#20960;&#20046;&#20855;&#26377;&#20301;&#31227;&#19981;&#21464;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#23376;&#37319;&#26679;&#21367;&#31215;&#21518;&#26368;&#22823;&#27744;&#21270;&#30340;&#20301;&#31227;&#31283;&#23450;&#24615;&#24230;&#37327;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#28388;&#27874;&#22120;&#30340;&#39057;&#29575;&#21644;&#26041;&#21521;&#22312;&#23454;&#29616;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#36890;&#36807;&#32771;&#34385;&#22522;&#20110;&#21452;&#26641;&#22797;&#23567;&#27874;&#21253;&#21464;&#25442;&#30340;&#30830;&#23450;&#24615;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#21363;&#31163;&#25955;Gabor&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on improving the mathematical interpretability of convolutional neural networks (CNNs) in the context of image classification. Specifically, we tackle the instability issue arising in their first layer, which tends to learn parameters that closely resemble oriented band-pass filters when trained on datasets like ImageNet. Subsampled convolutions with such Gabor-like filters are prone to aliasing, causing sensitivity to small input shifts. In this context, we establish conditions under which the max pooling operator approximates a complex modulus, which is nearly shift invariant. We then derive a measure of shift invariance for subsampled convolutions followed by max pooling. In particular, we highlight the crucial role played by the filter's frequency and orientation in achieving stability. We experimentally validate our theory by considering a deterministic feature extractor based on the dual-tree complex wavelet packet transform, a particular case of discrete Gabor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28023;&#32501;&#27602;&#21270;&#8221;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#35757;&#32451;&#26102;&#27880;&#20837;&#28023;&#32501;&#26679;&#26412;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#27599;&#20010;&#36755;&#20837;&#19978;&#30340;&#33021;&#32791;&#21644;&#24310;&#36831;&#65292;&#24182;&#19988;&#21363;&#20351;&#25915;&#20987;&#32773;&#21482;&#25511;&#21046;&#20102;&#19968;&#20123;&#27169;&#22411;&#26356;&#26032;&#20063;&#21487;&#20197;&#36827;&#34892;&#27492;&#25915;&#20987;&#65292;&#28023;&#32501;&#27602;&#21270;&#20960;&#20046;&#23436;&#20840;&#28040;&#38500;&#20102;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2203.08147</link><description>&lt;p&gt;
&#22522;&#20110;&#28023;&#32501;&#27602;&#21270;&#30340;&#33021;&#32791;&#24310;&#36831;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-Latency Attacks via Sponge Poisoning. (arXiv:2203.08147v4 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.08147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28023;&#32501;&#27602;&#21270;&#8221;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#35757;&#32451;&#26102;&#27880;&#20837;&#28023;&#32501;&#26679;&#26412;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#27599;&#20010;&#36755;&#20837;&#19978;&#30340;&#33021;&#32791;&#21644;&#24310;&#36831;&#65292;&#24182;&#19988;&#21363;&#20351;&#25915;&#20987;&#32773;&#21482;&#25511;&#21046;&#20102;&#19968;&#20123;&#27169;&#22411;&#26356;&#26032;&#20063;&#21487;&#20197;&#36827;&#34892;&#27492;&#25915;&#20987;&#65292;&#28023;&#32501;&#27602;&#21270;&#20960;&#20046;&#23436;&#20840;&#28040;&#38500;&#20102;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#32501;&#26679;&#26412;&#26159;&#22312;&#27979;&#35797;&#26102;&#31934;&#24515;&#20248;&#21270;&#30340;&#36755;&#20837;&#65292;&#21487;&#22312;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#37096;&#32626;&#26102;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#24310;&#36831;&#12290;&#26412;&#25991;&#39318;&#27425;&#35777;&#26126;&#20102;&#28023;&#32501;&#26679;&#26412;&#20063;&#21487;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;&#28023;&#32501;&#27602;&#21270;&#30340;&#25915;&#20987;&#27880;&#20837;&#21040;&#35757;&#32451;&#20013;&#12290;&#35813;&#25915;&#20987;&#20801;&#35768;&#22312;&#27599;&#20010;&#27979;&#35797;&#26102;&#36755;&#20837;&#20013;&#19981;&#21152;&#21306;&#20998;&#22320;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#24310;&#36831;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28023;&#32501;&#27602;&#21270;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#19982;&#20248;&#21270;&#27979;&#35797;&#26102;&#28023;&#32501;&#26679;&#26412;&#30456;&#20851;&#30340;&#38480;&#21046;&#65292;&#24182;&#34920;&#26126;&#21363;&#20351;&#25915;&#20987;&#32773;&#20165;&#25511;&#21046;&#20960;&#20010;&#27169;&#22411;&#26356;&#26032;&#65292;&#20363;&#22914;&#27169;&#22411;&#35757;&#32451;&#34987;&#22806;&#21253;&#32473;&#19981;&#21463;&#20449;&#20219;&#30340;&#31532;&#19977;&#26041;&#25110;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#20998;&#24067;&#24335;&#36827;&#34892;&#65292;&#20063;&#21487;&#20197;&#36827;&#34892;&#36825;&#31181;&#25915;&#20987;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#34920;&#26126;&#28023;&#32501;&#27602;&#21270;&#20960;&#20046;&#23436;&#20840;&#28040;&#38500;&#20102;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#27602;&#21270;&#27169;&#22411;&#30340;&#28608;&#27963;&#65292;&#30830;&#23450;&#20102;&#21738;&#20123;&#35745;&#31639;&#23545;&#23548;&#33268;&#33021;&#37327;&#28040;&#32791;&#21644;&#24310;&#36831;&#22686;&#21152;&#36215;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sponge examples are test-time inputs carefully optimized to increase energy consumption and latency of neural networks when deployed on hardware accelerators. In this work, we are the first to demonstrate that sponge examples can also be injected at training time, via an attack that we call sponge poisoning. This attack allows one to increase the energy consumption and latency of machine-learning models indiscriminately on each test-time input. We present a novel formalization for sponge poisoning, overcoming the limitations related to the optimization of test-time sponge examples, and show that this attack is possible even if the attacker only controls a few model updates; for instance, if model training is outsourced to an untrusted third-party or distributed via federated learning. Our extensive experimental analysis shows that sponge poisoning can almost completely vanish the effect of hardware accelerators. We also analyze the activations of poisoned models, identifying which comp
&lt;/p&gt;</description></item></channel></rss>