<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>Videoshop&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#35270;&#39057;&#32534;&#36753;&#31639;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26412;&#22320;&#21270;&#35821;&#20041;&#32534;&#36753;&#65292;&#20174;&#32780;&#20801;&#35768;&#29992;&#25143;&#23545;&#35270;&#39057;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#65292;&#21462;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14617</link><description>&lt;p&gt;
Videoshop&#65306;&#20855;&#26377;&#22122;&#22768;&#22806;&#25512;&#25193;&#25955;&#21453;&#28436;&#30340;&#26412;&#22320;&#21270;&#35821;&#20041;&#35270;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14617
&lt;/p&gt;
&lt;p&gt;
Videoshop&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#35270;&#39057;&#32534;&#36753;&#31639;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26412;&#22320;&#21270;&#35821;&#20041;&#32534;&#36753;&#65292;&#20174;&#32780;&#20801;&#35768;&#29992;&#25143;&#23545;&#35270;&#39057;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#65292;&#21462;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Videoshop&#65292;&#36825;&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#29992;&#20110;&#26412;&#22320;&#21270;&#35821;&#20041;&#32534;&#36753;&#30340;&#35270;&#39057;&#32534;&#36753;&#31639;&#27861;&#12290;Videoshop&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#20219;&#20309;&#32534;&#36753;&#36719;&#20214;&#65292;&#21253;&#25324;Photoshop&#21644;&#29983;&#25104;&#22635;&#20805;&#65292;&#20462;&#25913;&#31532;&#19968;&#24103;&#65307;&#23427;&#20250;&#33258;&#21160;&#23558;&#36825;&#20123;&#26356;&#25913;&#20256;&#25773;&#21040;&#20854;&#20313;&#24103;&#65292;&#20445;&#25345;&#35821;&#20041;&#12289;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#36816;&#21160;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#36890;&#36807;&#19981;&#31934;&#30830;&#30340;&#25991;&#26412;&#25351;&#20196;&#36827;&#34892;&#32534;&#36753;&#19981;&#21516;&#65292;Videoshop&#20801;&#35768;&#29992;&#25143;&#28155;&#21152;&#25110;&#21024;&#38500;&#23545;&#35937;&#65292;&#35821;&#20041;&#19978;&#26356;&#25913;&#23545;&#35937;&#65292;&#23558;&#32032;&#26448;&#29031;&#29255;&#25554;&#20837;&#35270;&#39057;&#31561;&#65292;&#24182;&#23545;&#20301;&#32622;&#21644;&#22806;&#35266;&#36827;&#34892;&#32454;&#31890;&#24230;&#25511;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#28508;&#22312;&#20540;&#36827;&#34892;&#22122;&#22768;&#22806;&#25512;&#21453;&#28436;&#30340;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#35270;&#39057;&#32534;&#36753;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20174;&#20013;&#25105;&#20204;&#29983;&#25104;&#26681;&#25454;&#32534;&#36753;&#22270;&#20687;&#35843;&#25972;&#30340;&#35270;&#39057;&#12290;Videoshop&#22312;2&#20010;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#20013;&#20351;&#29992;10&#20010;&#35780;&#20272;&#25351;&#26631;&#23545;6&#20010;&#22522;&#32447;&#21462;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14617v1 Announce Type: cross  Abstract: We introduce Videoshop, a training-free video editing algorithm for localized semantic edits. Videoshop allows users to use any editing software, including Photoshop and generative inpainting, to modify the first frame; it automatically propagates those changes, with semantic, spatial, and temporally consistent motion, to the remaining frames. Unlike existing methods that enable edits only through imprecise textual instructions, Videoshop allows users to add or remove objects, semantically change objects, insert stock photos into videos, etc. with fine-grained control over locations and appearance. We achieve this through image-based video editing by inverting latents with noise extrapolation, from which we generate videos conditioned on the edited image. Videoshop produces higher quality edits against 6 baselines on 2 editing benchmarks using 10 evaluation metrics.
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#26080;&#20154;&#26426;&#31454;&#36895;&#20013;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#20811;&#26381;&#26679;&#26412;&#25928;&#29575;&#21644;&#35745;&#31639;&#38656;&#27714;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#26041;&#27861;&#36827;&#34892;&#24615;&#33021;&#21463;&#38480;&#30340;&#33258;&#36866;&#24212;RL&#24494;&#35843;</title><link>https://arxiv.org/abs/2403.12203</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#20223;&#30340;&#22686;&#24378;&#23398;&#20064;&#20026;&#22522;&#20110;&#35270;&#35273;&#30340;&#25935;&#25463;&#39134;&#34892;&#24341;&#23548;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping Reinforcement Learning with Imitation for Vision-Based Agile Flight
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12203
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#26080;&#20154;&#26426;&#31454;&#36895;&#20013;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#20811;&#26381;&#26679;&#26412;&#25928;&#29575;&#21644;&#35745;&#31639;&#38656;&#27714;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#26041;&#27861;&#36827;&#34892;&#24615;&#33021;&#21463;&#38480;&#30340;&#33258;&#36866;&#24212;RL&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#26080;&#20154;&#26426;&#31454;&#36895;&#30340;&#32972;&#26223;&#19979;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26377;&#25928;&#24615;&#21644;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#30340;&#25928;&#29575;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#36755;&#20837;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#30340;&#29366;&#24577;&#20272;&#35745;&#12290;&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#35797;&#38169;&#25552;&#20379;&#20102;&#19968;&#20010;&#23398;&#20064;&#22797;&#26434;&#25511;&#21046;&#22120;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20294;&#38754;&#20020;&#30528;&#26679;&#26412;&#25928;&#29575;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#35270;&#35273;&#36755;&#20837;&#30340;&#32500;&#24230;&#36739;&#39640;&#12290;&#30456;&#21453;&#65292;IL&#22312;&#20174;&#35270;&#35273;&#28436;&#31034;&#20013;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20986;&#25928;&#29575;&#65292;&#20294;&#21463;&#21040;&#28436;&#31034;&#36136;&#37327;&#30340;&#38480;&#21046;&#65292;&#24182;&#38754;&#20020;&#35832;&#22914;&#21327;&#21464;&#37327;&#28418;&#31227;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;RL&#21644;IL&#20248;&#21183;&#30340;&#26032;&#22411;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#20351;&#29992;&#29305;&#26435;&#29366;&#24577;&#20449;&#24687;&#30340;&#24072;&#20613;&#31574;&#30053;&#30340;&#21021;&#22987;&#35757;&#32451;&#65292;&#20351;&#29992;IL&#23558;&#27492;&#31574;&#30053;&#33976;&#39311;&#20026;&#23398;&#29983;&#31574;&#30053;&#65292;&#20197;&#21450;&#24615;&#33021;&#21463;&#38480;&#30340;&#33258;&#36866;&#24212;RL&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12203v1 Announce Type: cross  Abstract: We combine the effectiveness of Reinforcement Learning (RL) and the efficiency of Imitation Learning (IL) in the context of vision-based, autonomous drone racing. We focus on directly processing visual input without explicit state estimation. While RL offers a general framework for learning complex controllers through trial and error, it faces challenges regarding sample efficiency and computational demands due to the high dimensionality of visual inputs. Conversely, IL demonstrates efficiency in learning from visual demonstrations but is limited by the quality of those demonstrations and faces issues like covariate shift. To overcome these limitations, we propose a novel training framework combining RL and IL's advantages. Our framework involves three stages: initial training of a teacher policy using privileged state information, distilling this policy into a student policy using IL, and performance-constrained adaptive RL fine-tunin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#20844;&#24320;&#25968;&#25454;&#38598;&#26500;&#24314;&#29983;&#29289;&#20998;&#31867;&#32676;&#25968;&#25454;&#38598;&#20197;&#21450;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25512;&#23548;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#24182;&#20197;&#33889;&#33796;&#29273;&#26412;&#22320;&#26893;&#29289;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.12072</link><description>&lt;p&gt;
Floralens&#65306;&#19968;&#31181;&#29992;&#20110;&#33889;&#33796;&#29273;&#26412;&#22320;&#26893;&#29289;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Floralens: a Deep Learning Model for the Portuguese Native Flora
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#20844;&#24320;&#25968;&#25454;&#38598;&#26500;&#24314;&#29983;&#29289;&#20998;&#31867;&#32676;&#25968;&#25454;&#38598;&#20197;&#21450;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25512;&#23548;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#24182;&#20197;&#33889;&#33796;&#29273;&#26412;&#22320;&#26893;&#29289;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#35768;&#22810;&#20844;&#27665;&#31185;&#23398;&#24179;&#21488;&#20013;&#23545;&#29983;&#29289;&#29289;&#31181;&#36827;&#34892;&#22522;&#20110;&#22270;&#20687;&#30340;&#35782;&#21035;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#36275;&#22815;&#22823;&#23567;&#21644;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#32593;&#32476;&#20197;&#21450;&#32593;&#32476;&#26550;&#26500;&#30340;&#36873;&#25321;&#26412;&#36523;&#20173;&#28982;&#24456;&#23569;&#26377;&#25991;&#29486;&#35760;&#24405;&#65292;&#22240;&#27492;&#19981;&#23481;&#26131;&#34987;&#22797;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20844;&#24320;&#21487;&#29992;&#30340;&#30740;&#31350;&#32423;&#25968;&#25454;&#38598;&#26500;&#24314;&#29983;&#29289;&#20998;&#31867;&#32676;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#20351;&#29992;&#35895;&#27468;&#30340;AutoML Vision&#20113;&#26381;&#21153;&#25552;&#20379;&#30340;&#29616;&#25104;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#25512;&#23548;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#26159;&#33889;&#33796;&#29273;&#26412;&#22320;&#26893;&#29289;&#65292;&#22522;&#20110;&#30001;&#33889;&#33796;&#29273;&#26893;&#29289;&#23398;&#20250;&#25552;&#20379;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#26469;&#33258;iNaturalist&#12289;Pl@ntNet&#21644;Observation.org&#30340;&#37319;&#38598;&#25968;&#25454;&#36827;&#34892;&#25193;&#23637;&#12290;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#35880;&#24910;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12072v1 Announce Type: cross  Abstract: Machine-learning techniques, namely deep convolutional neural networks, are pivotal for image-based identification of biological species in many Citizen Science platforms. However, the construction of critically sized and sampled datasets to train the networks and the choice of the network architectures itself remains little documented and, therefore, does not lend itself to be easily replicated. In this paper, we develop a streamlined methodology for building datasets for biological taxa from publicly available research-grade datasets and for deriving models from these datasets using off-the-shelf deep convolutional neural networks such as those provided by Google's AutoML Vision cloud service. Our case study is the Portuguese native flora, anchored in a high-quality dataset, provided by the Sociedade Portuguesa de Bot\^anica, scaled up by adding sampled data from iNaturalist, Pl@ntNet, and Observation.org. We find that with a careful
&lt;/p&gt;</description></item><item><title>&#24425;&#34425;&#32593;&#32476;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#36890;&#36807;&#23618;&#20869;&#31070;&#32463;&#20803;&#26435;&#37325;&#20114;&#30456;&#29420;&#31435;&#30340;&#23545;&#40784;&#21644;&#38543;&#26426;&#29305;&#24449;&#26144;&#23556;&#26469;&#36827;&#34892;&#32447;&#24615;&#38477;&#32500;&#21644;&#38750;&#32447;&#24615;&#39640;&#32500;&#23884;&#20837;&#65292;&#22312;ImageNet&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.18512</link><description>&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#40657;&#30418;&#20013;&#30340;&#24425;&#34425;
&lt;/p&gt;
&lt;p&gt;
A Rainbow in Deep Network Black Boxes. (arXiv:2305.18512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18512
&lt;/p&gt;
&lt;p&gt;
&#24425;&#34425;&#32593;&#32476;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#36890;&#36807;&#23618;&#20869;&#31070;&#32463;&#20803;&#26435;&#37325;&#20114;&#30456;&#29420;&#31435;&#30340;&#23545;&#40784;&#21644;&#38543;&#26426;&#29305;&#24449;&#26144;&#23556;&#26469;&#36827;&#34892;&#32447;&#24615;&#38477;&#32500;&#21644;&#38750;&#32447;&#24615;&#39640;&#32500;&#23884;&#20837;&#65292;&#22312;ImageNet&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24425;&#34425;&#32593;&#32476;&#20316;&#20026;&#35757;&#32451;&#22909;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32423;&#32852;&#38543;&#26426;&#29305;&#24449;&#26144;&#23556;&#65292;&#20854;&#26435;&#37325;&#20998;&#24067;&#26159;&#21487;&#20197;&#23398;&#20064;&#30340;&#12290;&#23427;&#20551;&#35774;&#19981;&#21516;&#23618;&#20043;&#38388;&#30340;&#26435;&#37325;&#20381;&#36182;&#24615;&#34987;&#20943;&#23569;&#21040;&#23558;&#36755;&#20837;&#28608;&#27963;&#23545;&#20934;&#30340;&#26059;&#36716;&#12290;&#23618;&#20869;&#30340;&#31070;&#32463;&#20803;&#26435;&#37325;&#22312;&#36825;&#31181;&#23545;&#40784;&#21518;&#26159;&#30456;&#20114;&#29420;&#31435;&#30340;&#12290;&#23427;&#20204;&#30340;&#28608;&#27963;&#23450;&#20041;&#20102;&#22312;&#26080;&#31351;&#23485;&#24230;&#26497;&#38480;&#19979;&#21464;&#24471;&#30830;&#23450;&#30340;&#20869;&#26680;&#12290;&#36825;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;ResNets&#20013;&#36890;&#36807;&#25968;&#23383;&#39564;&#35777;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#23398;&#20064;&#30340;&#26435;&#37325;&#20998;&#24067;&#20855;&#26377;&#20302;&#31209;&#21327;&#26041;&#24046;&#12290;&#22240;&#27492;&#65292;&#24425;&#34425;&#32593;&#32476;&#22312;&#32447;&#24615;&#38477;&#32500;&#21644;&#38750;&#32447;&#24615;&#39640;&#32500;&#23884;&#20837;&#19982;&#30333;&#33394;&#38543;&#26426;&#29305;&#24449;&#20043;&#38388;&#20132;&#26367;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20855;&#26377;&#39640;&#26031;&#26435;&#37325;&#20998;&#24067;&#30340;&#39640;&#26031;&#24425;&#34425;&#32593;&#32476;&#23450;&#20041;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20351;&#29992;&#23567;&#27874;&#25955;&#23556;&#32593;&#32476;&#36827;&#34892;CIFAR-10&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#36827;&#34892;&#20102;&#25968;&#23383;&#39564;&#35777;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;SGD&#26356;&#26032;&#26435;&#37325;&#30340;&#21327;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce rainbow networks as a probabilistic model of trained deep neural networks. The model cascades random feature maps whose weight distributions are learned. It assumes that dependencies between weights at different layers are reduced to rotations which align the input activations. Neuron weights within a layer are independent after this alignment. Their activations define kernels which become deterministic in the infinite-width limit. This is verified numerically for ResNets trained on the ImageNet dataset. We also show that the learned weight distributions have low-rank covariances. Rainbow networks thus alternate between linear dimension reductions and non-linear high-dimensional embeddings with white random features. Gaussian rainbow networks are defined with Gaussian weight distributions. These models are validated numerically on image classification on the CIFAR-10 dataset, with wavelet scattering networks. We further show that during training, SGD updates the weight cov
&lt;/p&gt;</description></item></channel></rss>