<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>BiomedCLIP&#26159;&#19968;&#20010;&#20174;1500&#19975;&#31185;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;&#22823;&#35268;&#27169;&#30340;PMC-15M&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#25968;&#25454;&#38598;&#27604;&#29616;&#26377;&#30340;&#29983;&#29289;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#22823;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#30340;&#26816;&#32034;&#12289;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2303.00915</link><description>&lt;p&gt;
BiomedCLIP&#65306;&#19968;&#31181;&#20174;&#19968;&#21315;&#20116;&#30334;&#19975;&#31185;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. (arXiv:2303.00915v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00915
&lt;/p&gt;
&lt;p&gt;
BiomedCLIP&#26159;&#19968;&#20010;&#20174;1500&#19975;&#31185;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;&#22823;&#35268;&#27169;&#30340;PMC-15M&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#25968;&#25454;&#38598;&#27604;&#29616;&#26377;&#30340;&#29983;&#29289;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#22823;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#30340;&#26816;&#32034;&#12289;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#26412;&#36136;&#19978;&#26159;&#22810;&#27169;&#24577;&#30340;&#65292;&#21253;&#25324;&#29289;&#29702;&#27979;&#37327;&#21644;&#33258;&#28982;&#35821;&#35328;&#21465;&#36848;&#12290;&#19968;&#20010;&#36890;&#29992;&#30340;&#29983;&#29289;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#38656;&#35201;&#21516;&#26102;&#22788;&#29702;&#19981;&#21516;&#30340;&#25968;&#25454;&#27169;&#24577;&#65292;&#21253;&#25324;&#25991;&#26412;&#21644;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#19968;&#20010;&#26377;&#25928;&#30340;&#36890;&#29992;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#20363;&#22914;&#24179;&#34892;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;PMC-15M&#65292;&#27604;&#29616;&#26377;&#30340;&#29983;&#29289;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65288;&#22914;MIMIC-CXR&#65289;&#22823;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#28085;&#30422;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#31867;&#22411;&#12290;PMC-15M&#21253;&#21547;&#20102;&#26469;&#33258;440&#19975;&#31185;&#23398;&#35770;&#25991;&#30340;1500&#19975;&#20010;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12290;&#22522;&#20110;PMC-15M&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;BiomedCLIP&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#33258;&#36866;&#24212;&#65292;&#20197;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#35270;&#35273;-&#35821;&#35328;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#65292;&#20174;&#26816;&#32034;&#21040;&#20998;&#31867;&#21040;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#65288;VQA&#65289;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical data is inherently multimodal, comprising physical measurements and natural language narratives. A generalist biomedical AI model needs to simultaneously process different modalities of data, including text and images. Therefore, training an effective generalist biomedical model requires high-quality multimodal data, such as parallel image-text pairs. Here, we present PMC-15M, a novel dataset that is two orders of magnitude larger than existing biomedical multimodal datasets such as MIMIC-CXR, and spans a diverse range of biomedical image types. PMC-15M contains 15 million biomedical image-text pairs collected from 4.4 million scientific articles. Based on PMC-15M, we have pretrained BiomedCLIP, a multimodal foundation model, with domain-specific adaptations tailored to biomedical vision-language processing. We conducted extensive experiments and ablation studies on standard biomedical imaging tasks from retrieval to classification to visual question-answering (VQA). BiomedC
&lt;/p&gt;</description></item></channel></rss>