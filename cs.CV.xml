<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.12510</link><description>&lt;p&gt;
&#22270;&#20687;&#25805;&#20316;&#30340;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generalized Consistency Trajectory Models for Image Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26080;&#26465;&#20214;&#29983;&#25104;&#20197;&#21450;&#22270;&#20687;&#32534;&#36753;&#21644;&#24674;&#22797;&#31561;&#24212;&#29992;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#22312;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#24615;&#36136;&#65306;&#25193;&#25955;&#23558;&#23558;&#22122;&#22768;&#21040;&#25968;&#25454;&#30340;&#22797;&#26434;&#26144;&#23556;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#21435;&#22122;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#27880;&#20837;&#24341;&#23548;&#39033;&#65292;&#25105;&#20204;&#33021;&#22815;&#23545;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#36845;&#20195;&#36807;&#31243;&#20063;&#24120;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#25968;&#21313;&#27425;&#29978;&#33267;&#25968;&#21315;&#27425;&#20989;&#25968;&#35780;&#20272;&#12290;&#34429;&#28982;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTMs&#65289;&#21487;&#20197;&#22312;&#27010;&#29575;&#27969;ODE&#65288;PFODE&#65289;&#19978;&#20219;&#24847;&#26102;&#38388;&#28857;&#20043;&#38388;&#36827;&#34892;&#36941;&#21382;&#65292;&#24182;&#19988;&#36890;&#36807;&#21333;&#27425;&#20989;&#25968;&#35780;&#20272;&#36827;&#34892;&#24471;&#20998;&#25512;&#23548;&#65292;&#20294;CTMs&#20165;&#20801;&#35768;&#20174;&#39640;&#26031;&#22122;&#22768;&#36716;&#25442;&#20026;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#24191;&#20041;CTMs&#65288;GCTMs&#65289;&#26469;&#21457;&#25381;CTMs&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23454;&#29616;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12510v1 Announce Type: cross  Abstract: Diffusion-based generative models excel in unconditional generation, as well as on applied tasks such as image editing and restoration. The success of diffusion models lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of simple denoising tasks. Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step. However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations. Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data. Thus, this work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbit
&lt;/p&gt;</description></item><item><title>&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26102;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#65292;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;TIVE&#65292;&#26681;&#25454;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#20215;&#20540;&#26469;&#28040;&#38500;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#12290;</title><link>https://arxiv.org/abs/2403.09559</link><description>&lt;p&gt;
&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#23545;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Less is More: Data Value Estimation for Visual Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09559
&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26102;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#65292;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;TIVE&#65292;&#26681;&#25454;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#20215;&#20540;&#26469;&#28040;&#38500;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26159;&#26500;&#24314;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#20851;&#38190;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MLLMs&#20027;&#35201;&#20381;&#36182;&#20110;&#22810;&#20010;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#38598;&#30340;&#28151;&#21512;&#35757;&#32451;&#65288;&#29978;&#33267;&#36229;&#36807;&#19968;&#30334;&#19975;&#26465;&#25351;&#23548;&#65289;&#65292;&#36825;&#21487;&#33021;&#24341;&#20837;&#25968;&#25454;&#20887;&#20313;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#38598;&#20869;&#23384;&#22312;&#26174;&#33879;&#20887;&#20313;&#65292;&#24182;&#26174;&#31034;&#22823;&#22823;&#20943;&#23569;&#20960;&#20010;&#25351;&#23548;&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#29978;&#33267;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;TIVE&#65292;&#20197;&#28040;&#38500;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#12290;TIVE&#39318;&#20808;&#26681;&#25454;&#35745;&#31639;&#30340;&#26799;&#24230;&#20272;&#35745;&#35270;&#35273;&#25351;&#23548;&#30340;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#20215;&#20540;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#20272;&#35745;&#30340;&#20215;&#20540;&#65292;TIVE&#30830;&#23450;&#20102;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#25351;&#23548;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09559v1 Announce Type: new  Abstract: Visual instruction tuning is the key to building multimodal large language models (MLLMs), which greatly improves the reasoning capabilities of large language models (LLMs) in vision scenario. However, existing MLLMs mostly rely on a mixture of multiple highly diverse visual instruction datasets for training (even more than a million instructions), which may introduce data redundancy. To investigate this issue, we conduct a series of empirical studies, which reveal a significant redundancy within the visual instruction datasets, and show that greatly reducing the amount of several instruction dataset even do not affect the performance. Based on the findings, we propose a new data selection approach TIVE, to eliminate redundancy within visual instruction data. TIVE first estimates the task-level and instance-level value of the visual instructions based on computed gradients. Then, according to the estimated values, TIVE determines the tas
&lt;/p&gt;</description></item><item><title>TV-TREES&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19467</link><description>&lt;p&gt;
TV-TREES&#65306;&#29992;&#20110;&#31070;&#32463;&#31526;&#21495;&#35270;&#39057;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;
&lt;/p&gt;
&lt;p&gt;
TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19467
&lt;/p&gt;
&lt;p&gt;
TV-TREES&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#30005;&#35270;&#21098;&#36753;&#31561;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20869;&#23481;&#36827;&#34892;&#38382;&#31572;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#37096;&#20998;&#26159;&#22240;&#20026;&#24403;&#21069;&#30340;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20381;&#36182;&#20110;&#21333;&#27169;&#24577;&#25512;&#29702;&#65292;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TV-TREES&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#12290;TV-TREES&#20316;&#20026;&#19968;&#31181;&#20419;&#36827;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#30340;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#20219;&#21153;&#26469;&#35780;&#20272;&#27492;&#31867;&#26041;&#27861;&#30340;&#25512;&#29702;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#30340;&#12289;&#20855;&#26377;&#26368;&#20808;&#36827;&#38646;-shot&#24615;&#33021;&#30340;&#23436;&#25972;&#35270;&#39057;&#21098;&#36753;&#65292;&#23637;&#31034;&#20102;&#19982;&#40657;&#30418;&#26041;&#27861;&#30456;&#27604;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19467v1 Announce Type: cross  Abstract: It is challenging to perform question-answering over complex, multimodal content such as television clips. This is in part because current video-language models rely on single-modality reasoning, have lowered performance on long inputs, and lack interpetability. We propose TV-TREES, the first multimodal entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by producing trees of entailment relationships between simple premises directly entailed by the videos and higher-level conclusions. We then introduce the task of multimodal entailment tree generation to evaluate the reasoning quality of such methods. Our method's experimental results on the challenging TVQA dataset demonstrate intepretable, state-of-the-art zero-shot performance on full video clips, illustrating a best of both worlds contrast to black-box methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelfFed&#30340;&#33258;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;IoMT&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21294;&#20047;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20004;&#20010;&#38454;&#27573;&#65292;&#36890;&#36807;&#20998;&#25955;&#35757;&#32451;&#21644;&#22686;&#24378;&#24314;&#27169;&#26469;&#20811;&#26381;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#31232;&#32570;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.01514</link><description>&lt;p&gt;
SelfFed: &#33258;&#30417;&#30563;&#30340;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;IoMT&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21294;&#20047;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
SelfFed: Self-supervised Federated Learning for Data Heterogeneity and Label Scarcity in IoMT. (arXiv:2307.01514v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01514
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelfFed&#30340;&#33258;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;IoMT&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21294;&#20047;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20004;&#20010;&#38454;&#27573;&#65292;&#36890;&#36807;&#20998;&#25955;&#35757;&#32451;&#21644;&#22686;&#24378;&#24314;&#27169;&#26469;&#20811;&#26381;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#22312;&#34892;&#19994;&#21644;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21327;&#20316;&#23398;&#20064;&#26410;&#26631;&#35760;&#20294;&#23396;&#31435;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#33258;&#30417;&#30563;&#30340;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#22312;&#26631;&#31614;&#31232;&#32570;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#65288;&#21363;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#65289;&#26041;&#38754;&#23384;&#22312;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#21307;&#30103;&#29289;&#32852;&#32593;&#65288;IoMT&#65289;&#30340;SelfFed&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;SelfFed&#26694;&#26550;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#31532;&#19968;&#20010;&#38454;&#27573;&#26159;&#39044;&#35757;&#32451;&#33539;&#24335;&#65292;&#20351;&#29992;&#22522;&#20110;Swin Transformer&#30340;&#32534;&#30721;&#22120;&#20197;&#20998;&#25955;&#30340;&#26041;&#24335;&#36827;&#34892;&#22686;&#24378;&#24314;&#27169;&#12290;SelfFed&#26694;&#26550;&#30340;&#31532;&#19968;&#20010;&#38454;&#27573;&#26377;&#21161;&#20110;&#20811;&#26381;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#31532;&#20108;&#20010;&#38454;&#27573;&#26159;&#24494;&#35843;&#33539;&#24335;&#65292;&#24341;&#20837;&#23545;&#27604;&#32593;&#32476;&#21644;&#19968;&#31181;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#26032;&#22411;&#32858;&#21512;&#31574;&#30053;&#65292;&#29992;&#20110;&#30446;&#26631;&#20219;&#21153;&#30340;&#20998;&#25955;&#35757;&#32451;&#12290;&#36825;&#20010;&#24494;&#35843;&#38454;&#27573;&#20811;&#26381;&#20102;&#26631;&#31614;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning in federated learning paradigm has been gaining a lot of interest both in industry and research due to the collaborative learning capability on unlabeled yet isolated data. However, self-supervised based federated learning strategies suffer from performance degradation due to label scarcity and diverse data distributions, i.e., data heterogeneity. In this paper, we propose the SelfFed framework for Internet of Medical Things (IoMT). Our proposed SelfFed framework works in two phases. The first phase is the pre-training paradigm that performs augmentive modeling using Swin Transformer based encoder in a decentralized manner. The first phase of SelfFed framework helps to overcome the data heterogeneity issue. The second phase is the fine-tuning paradigm that introduces contrastive network and a novel aggregation strategy that is trained on limited labeled data for a target task in a decentralized manner. This fine-tuning stage overcomes the label scarcity problem
&lt;/p&gt;</description></item><item><title>OpenDriver&#26159;&#19968;&#20221;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#23384;&#22312;&#38382;&#39064;&#30340;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20004;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.04203</link><description>&lt;p&gt;
OpenDriver: &#19968;&#20221;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OpenDriver: an open-road driver state detection dataset. (arXiv:2304.04203v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04203
&lt;/p&gt;
&lt;p&gt;
OpenDriver&#26159;&#19968;&#20221;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#23384;&#22312;&#38382;&#39064;&#30340;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20004;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#31038;&#20250;&#20013;&#65292;&#36947;&#36335;&#23433;&#20840;&#20005;&#37325;&#20381;&#36182;&#20110;&#39550;&#39542;&#21592;&#30340;&#24515;&#29702;&#21644;&#29983;&#29702;&#29366;&#24577;&#12290;&#30130;&#21171;&#12289;&#26127;&#26127;&#27442;&#30561;&#21644;&#21387;&#21147;&#31561;&#36127;&#38754;&#22240;&#32032;&#20250;&#24433;&#21709;&#39550;&#39542;&#21592;&#30340;&#21453;&#24212;&#26102;&#38388;&#21644;&#20915;&#31574;&#33021;&#21147;&#65292;&#23548;&#33268;&#20132;&#36890;&#20107;&#25925;&#30340;&#21457;&#29983;&#29575;&#22686;&#21152;&#12290;&#22312;&#20247;&#22810;&#30340;&#39550;&#39542;&#21592;&#34892;&#20026;&#30417;&#27979;&#30740;&#31350;&#20013;&#65292;&#21487;&#31359;&#25140;&#29983;&#29702;&#27979;&#37327;&#26159;&#19968;&#31181;&#23454;&#26102;&#30417;&#27979;&#39550;&#39542;&#21592;&#29366;&#24577;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#24320;&#25918;&#36947;&#36335;&#22330;&#26223;&#19979;&#65292;&#32570;&#23569;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#65292;&#24050;&#26377;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#20449;&#21495;&#36136;&#37327;&#24046;&#12289;&#26679;&#26412;&#37327;&#23567;&#21644;&#25968;&#25454;&#25910;&#38598;&#26102;&#38388;&#30701;&#31561;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#35774;&#35745;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20004;&#31181;&#39550;&#39542;&#20449;&#21495;&#27169;&#24577;&#65306;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#65292;&#36825;&#20123;&#20449;&#21495;&#26159;&#22312;100&#22810;&#21517;&#39550;&#39542;&#21592;&#36981;&#24490;&#30456;&#21516;&#36335;&#32447;&#34892;&#39542;&#26102;&#35760;&#24405;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern society, road safety relies heavily on the psychological and physiological state of drivers. Negative factors such as fatigue, drowsiness, and stress can impair drivers' reaction time and decision making abilities, leading to an increased incidence of traffic accidents. Among the numerous studies for impaired driving detection, wearable physiological measurement is a real-time approach to monitoring a driver's state. However, currently, there are few driver physiological datasets in open road scenarios and the existing datasets suffer from issues such as poor signal quality, small sample sizes, and short data collection periods. Therefore, in this paper, a large-scale multimodal driving dataset for driver impairment detection and biometric data recognition is designed and described. The dataset contains two modalities of driving signals: six-axis inertial signals and electrocardiogram (ECG) signals, which were recorded while over one hundred drivers were following the same ro
&lt;/p&gt;</description></item><item><title>DualStreamFoveaNet&#26159;&#19968;&#31181;&#20855;&#26377;&#35299;&#21078;&#24847;&#35782;&#30340;&#21452;&#27969;&#34701;&#21512;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#32593;&#33180;&#21644;&#34880;&#31649;&#20998;&#24067;&#36827;&#34892;&#22810;&#32447;&#32034;&#34701;&#21512;&#65292;&#23454;&#29616;&#23545;&#40065;&#26834;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26550;&#26500;&#22312;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06961</link><description>&lt;p&gt;
DualStreamFoveaNet: &#19968;&#31181;&#20855;&#26377;&#35299;&#21078;&#24847;&#35782;&#30340;&#21452;&#27969;&#34701;&#21512;&#26550;&#26500;&#29992;&#20110;&#40065;&#26834;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
DualStreamFoveaNet: A Dual Stream Fusion Architecture with Anatomical Awareness for Robust Fovea Localization. (arXiv:2302.06961v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06961
&lt;/p&gt;
&lt;p&gt;
DualStreamFoveaNet&#26159;&#19968;&#31181;&#20855;&#26377;&#35299;&#21078;&#24847;&#35782;&#30340;&#21452;&#27969;&#34701;&#21512;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#32593;&#33180;&#21644;&#34880;&#31649;&#20998;&#24067;&#36827;&#34892;&#22810;&#32447;&#32034;&#34701;&#21512;&#65292;&#23454;&#29616;&#23545;&#40065;&#26834;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26550;&#26500;&#22312;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#23545;&#20110;&#20998;&#26512;&#35270;&#32593;&#33180;&#30142;&#30149;&#20197;&#39044;&#38450;&#19981;&#21487;&#36870;&#35270;&#21147;&#25439;&#22833;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#34429;&#28982;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20294;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#20013;&#22830;&#20985;&#28857;&#21608;&#22260;&#23616;&#37096;&#35299;&#21078;&#26631;&#35760;&#30340;&#32570;&#22833;&#12289;&#19981;&#33021;&#40065;&#26834;&#22320;&#22788;&#29702;&#30149;&#21464;&#35270;&#32593;&#33180;&#22270;&#20687;&#21644;&#22270;&#20687;&#26465;&#20214;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#31216;&#20026;DualStreamFoveaNet (DSFN)&#29992;&#20110;&#22810;&#32447;&#32034;&#34701;&#21512;&#12290;&#35813;&#26550;&#26500;&#26126;&#30830;&#22320;&#21033;&#29992;&#35270;&#32593;&#33180;&#21644;&#34880;&#31649;&#20998;&#24067;&#26469;&#23454;&#29616;&#38271;&#31243;&#36830;&#25509;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#34701;&#21512;&#65292;&#23454;&#29616;&#40065;&#26834;&#30340;&#20013;&#22830;&#20985;&#28857;&#23450;&#20301;&#12290;&#25105;&#20204;&#22312;&#21452;&#27969;&#32534;&#30721;&#22120;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#31354;&#38388;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#34701;&#21512;&#33258;&#23398;&#20064;&#30340;&#35299;&#21078;&#20449;&#24687;&#65292;&#26356;&#27880;&#37325;&#20998;&#24067;&#22312;&#34880;&#31649;&#27839;&#32447;&#30340;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#20196;&#29260;&#25968;&#37327;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate fovea localization is essential for analyzing retinal diseases to prevent irreversible vision loss. While current deep learning-based methods outperform traditional ones, they still face challenges such as the lack of local anatomical landmarks around the fovea, the inability to robustly handle diseased retinal images, and the variations in image conditions. In this paper, we propose a novel transformer-based architecture called DualStreamFoveaNet (DSFN) for multi-cue fusion. This architecture explicitly incorporates long-range connections and global features using retina and vessel distributions for robust fovea localization. We introduce a spatial attention mechanism in the dual-stream encoder to extract and fuse self-learned anatomical information, focusing more on features distributed along blood vessels and significantly reducing computational costs by decreasing token numbers. Our extensive experiments show that the proposed architecture achieves state-of-the-art perform
&lt;/p&gt;</description></item></channel></rss>