<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#37325;&#26032;&#25506;&#35752;&#20102;&#23494;&#38598;&#36830;&#25509;&#21367;&#31215;&#32593;&#32476;&#65288;DenseNets&#65289;&#65292;&#25581;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;ResNet&#39118;&#26684;&#26550;&#26500;&#30340;&#34987;&#20302;&#20272;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#12289;&#22359;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#24471;DenseNets&#21487;&#20197;&#19982;&#29616;&#20195;&#26550;&#26500;&#31454;&#20105;&#65292;&#24182;&#22312;ImageNet-1K&#19978;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.19588</link><description>&lt;p&gt;
DenseNets&#37325;&#29983;&#65306;&#36229;&#36234;ResNets&#21644;ViTs&#30340;&#33539;&#24335;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#25506;&#35752;&#20102;&#23494;&#38598;&#36830;&#25509;&#21367;&#31215;&#32593;&#32476;&#65288;DenseNets&#65289;&#65292;&#25581;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;ResNet&#39118;&#26684;&#26550;&#26500;&#30340;&#34987;&#20302;&#20272;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#12289;&#22359;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#24471;DenseNets&#21487;&#20197;&#19982;&#29616;&#20195;&#26550;&#26500;&#31454;&#20105;&#65292;&#24182;&#22312;ImageNet-1K&#19978;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22797;&#33487;&#20102;&#23494;&#38598;&#36830;&#25509;&#21367;&#31215;&#32593;&#32476;&#65288;DenseNets&#65289;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30456;&#23545;&#20110;&#20027;&#23548;&#30340;ResNet&#39118;&#26684;&#26550;&#26500;&#34987;&#20302;&#20272;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;DenseNets&#30340;&#28508;&#21147;&#34987;&#24573;&#35270;&#65292;&#26159;&#22240;&#20026;&#26410;&#26366;&#35302;&#21450;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#20256;&#32479;&#35774;&#35745;&#20803;&#32032;&#26410;&#33021;&#23436;&#20840;&#23637;&#29616;&#20854;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36830;&#25509;&#30340;&#23494;&#38598;&#36830;&#25509;&#26159;&#24378;&#22823;&#30340;&#65292;&#34920;&#26126;DenseNets&#21487;&#20197;&#34987;&#37325;&#26032;&#28608;&#27963;&#20197;&#19982;&#29616;&#20195;&#26550;&#26500;&#31454;&#20105;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#25913;&#36827;&#20102;&#27425;&#20248;&#32452;&#20214; - &#26550;&#26500;&#35843;&#25972;&#12289;&#22359;&#37325;&#26032;&#35774;&#35745;&#21644;&#25913;&#36827;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#20197;&#25193;&#23637;DenseNets&#24182;&#25552;&#39640;&#20869;&#23384;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#36830;&#25509;&#24555;&#25463;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#31616;&#21333;&#30340;&#26550;&#26500;&#20803;&#32032;&#65292;&#26368;&#32456;&#36229;&#36234;&#20102;Swin Transformer&#12289;ConvNeXt&#21644;DeiT-III - &#27531;&#24046;&#23398;&#20064;&#35889;&#31995;&#20013;&#30340;&#20851;&#38190;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;ImageNet-1K&#19978;&#23637;&#29616;&#20986;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#31454;&#20105;wi
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19588v1 Announce Type: cross  Abstract: This paper revives Densely Connected Convolutional Networks (DenseNets) and reveals the underrated effectiveness over predominant ResNet-style architectures. We believe DenseNets' potential was overlooked due to untouched training methods and traditional design elements not fully revealing their capabilities. Our pilot study shows dense connections through concatenation are strong, demonstrating that DenseNets can be revitalized to compete with modern architectures. We methodically refine suboptimal components - architectural adjustments, block redesign, and improved training recipes towards widening DenseNets and boosting memory efficiency while keeping concatenation shortcuts. Our models, employing simple architectural elements, ultimately surpass Swin Transformer, ConvNeXt, and DeiT-III - key architectures in the residual learning lineage. Furthermore, our models exhibit near state-of-the-art performance on ImageNet-1K, competing wi
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#26032;&#30340;pose-estimation&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;SSL&#20960;&#20309;&#34920;&#31034;&#65292;&#25552;&#20986;&#26080;&#30417;&#30563;&#36712;&#36857;&#27491;&#21017;&#21270;&#25439;&#22833;&#26469;&#22686;&#24378;SSL&#20960;&#20309;&#34920;&#31034;&#65292;&#22312;&#23039;&#21183;&#20272;&#35745;&#24615;&#33021;&#19978;&#21462;&#24471;10-20%&#30340;&#25552;&#21319;&#65292;&#24182;&#25552;&#39640;&#20102;4%&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14973</link><description>&lt;p&gt;
&#36712;&#36857;&#27491;&#21017;&#21270;&#22686;&#24378;&#33258;&#30417;&#30563;&#20960;&#20309;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Trajectory Regularization Enhances Self-Supervised Geometric Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14973
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#26032;&#30340;pose-estimation&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;SSL&#20960;&#20309;&#34920;&#31034;&#65292;&#25552;&#20986;&#26080;&#30417;&#30563;&#36712;&#36857;&#27491;&#21017;&#21270;&#25439;&#22833;&#26469;&#22686;&#24378;SSL&#20960;&#20309;&#34920;&#31034;&#65292;&#22312;&#23039;&#21183;&#20272;&#35745;&#24615;&#33021;&#19978;&#21462;&#24471;10-20%&#30340;&#25552;&#21319;&#65292;&#24182;&#25552;&#39640;&#20102;4%&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#20027;&#35201;&#20851;&#27880;&#35821;&#20041;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#20960;&#20309;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#20047;&#29992;&#20110;&#35780;&#20272;&#20960;&#20309;&#34920;&#31034;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26041;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#23039;&#21183;&#20272;&#35745;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;SSL&#20960;&#20309;&#34920;&#31034;&#65292;&#35813;&#22522;&#20934;&#35201;&#27714;&#22312;&#27809;&#26377;&#35821;&#20041;&#25110;&#23039;&#21183;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#35821;&#20041;&#21644;&#20960;&#20309;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#29087;&#32451;&#24230;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22686;&#24378;SSL&#20960;&#20309;&#34920;&#31034;&#32780;&#19981;&#29306;&#29298;&#35821;&#20041;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#21033;&#29992;&#20013;&#38388;&#23618;&#34920;&#31034;&#21487;&#20197;&#23558;&#23039;&#21183;&#20272;&#35745;&#24615;&#33021;&#25552;&#39640;10-20&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#36712;&#36857;&#27491;&#21017;&#21270;&#25439;&#22833;&#65292;&#35813;&#25439;&#22833;&#39069;&#22806;&#25552;&#39640;&#20102;4&#65285;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#22312;&#36234;&#30028;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14973v1 Announce Type: cross  Abstract: Self-supervised learning (SSL) has proven effective in learning high-quality representations for various downstream tasks, with a primary focus on semantic tasks. However, its application in geometric tasks remains underexplored, partially due to the absence of a standardized evaluation method for geometric representations. To address this gap, we introduce a new pose-estimation benchmark for assessing SSL geometric representations, which demands training without semantic or pose labels and achieving proficiency in both semantic and geometric downstream tasks. On this benchmark, we study enhancing SSL geometric representations without sacrificing semantic classification accuracy. We find that leveraging mid-layer representations improves pose-estimation performance by 10-20%. Further, we introduce an unsupervised trajectory-regularization loss, which improves performance by an additional 4% and improves generalization ability on out-of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Meta-Prompting for Visual Recognition (MPVR)&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#23569;&#37327;&#20449;&#24687;&#21363;&#21487;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#20013;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.11755</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#25552;&#31034;&#33258;&#21160;&#21270;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#35273;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11755
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Meta-Prompting for Visual Recognition (MPVR)&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#23569;&#37327;&#20449;&#24687;&#21363;&#21487;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#20013;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#31867;&#21035;&#29305;&#23450;&#25552;&#31034;&#30340;&#25552;&#31034;&#38598;&#25104;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#38646;&#26679;&#26412;&#35782;&#21035;&#33021;&#21147;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#20026;&#20102;&#33719;&#24471;&#36825;&#20123;&#31867;&#21035;&#29305;&#23450;&#25552;&#31034;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#24037;&#20026;LLMs&#35774;&#35745;&#25552;&#31034;&#65292;&#20197;&#29983;&#25104;&#19979;&#28216;&#20219;&#21153;&#30340;VLM&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#25163;&#21160;&#32534;&#20889;&#36825;&#20123;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#65292;&#32780;&#19988;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#28085;&#30422;&#19982;&#24863;&#20852;&#36259;&#31867;&#21035;&#30456;&#20851;&#30340;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#21644;&#20219;&#21153;&#29305;&#23450;&#39118;&#26684;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23558;&#20154;&#31867;&#25490;&#38500;&#22312;&#24490;&#29615;&#20043;&#22806;&#65292;&#24182;&#23436;&#20840;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#35270;&#35273;&#35782;&#21035;&#30340;&#20803;&#25552;&#31034;&#65288;MPVR&#65289;&#12290;&#20165;&#20197;&#30446;&#26631;&#20219;&#21153;&#30340;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#24418;&#24335;&#20197;&#21450;&#19968;&#31995;&#21015;&#30456;&#20851;&#31867;&#21035;&#26631;&#31614;&#20316;&#20026;&#36755;&#20837;&#65292;MPVR&#33258;&#21160;&#20135;&#29983;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#31867;&#21035;&#25552;&#31034;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11755v1 Announce Type: cross  Abstract: Prompt ensembling of Large Language Model (LLM) generated category-specific prompts has emerged as an effective method to enhance zero-shot recognition ability of Vision-Language Models (VLMs). To obtain these category-specific prompts, the present methods rely on hand-crafting the prompts to the LLMs for generating VLM prompts for the downstream tasks. However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest. To effectively take humans out of the loop and completely automate the prompt generation process for zero-shot recognition, we propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only minimal information about the target task, in the form of its short natural language description, and a list of associated class labels, MPVR automatically produces a diverse set of cat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Curriculum Learning&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;DeepLens&#35774;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#34920;&#38754;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#22797;&#21512;&#36879;&#38236;&#30340;&#20809;&#23398;&#35774;&#35745;&#65292;&#20811;&#26381;&#20102;&#23545;&#33391;&#22909;&#21021;&#22987;&#35774;&#35745;&#30340;&#38656;&#27714;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#21160;&#35774;&#35745;&#32463;&#20856;&#25104;&#20687;&#36879;&#38236;&#21644;&#22823;&#35270;&#22330;&#24310;&#20280;&#26223;&#28145;&#35745;&#31639;&#36879;&#38236;&#12290;</title><link>https://arxiv.org/abs/2302.01089</link><description>&lt;p&gt;
Curriculum Learning&#29992;&#20110;&#20174;&#22836;&#24320;&#22987;&#30340;&#28145;&#24230;&#23398;&#20064;&#25240;&#23556;&#20809;&#23398;
&lt;/p&gt;
&lt;p&gt;
Curriculum Learning for ab initio Deep Learned Refractive Optics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.01089
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Curriculum Learning&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;DeepLens&#35774;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#34920;&#38754;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#22797;&#21512;&#36879;&#38236;&#30340;&#20809;&#23398;&#35774;&#35745;&#65292;&#20811;&#26381;&#20102;&#23545;&#33391;&#22909;&#21021;&#22987;&#35774;&#35745;&#30340;&#38656;&#27714;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#21160;&#35774;&#35745;&#32463;&#20856;&#25104;&#20687;&#36879;&#38236;&#21644;&#22823;&#35270;&#22330;&#24310;&#20280;&#26223;&#28145;&#35745;&#31639;&#36879;&#38236;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20809;&#23398;&#20248;&#21270;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#20351;&#29992;&#36755;&#20986;&#22270;&#20687;&#20316;&#20026;&#30446;&#26631;&#30340;&#35745;&#31639;&#25104;&#20687;&#31995;&#32479;&#35774;&#35745;&#30340;&#26032;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#23427;&#30446;&#21069;&#34987;&#38480;&#21046;&#20110;&#21333;&#20010;&#20803;&#32032;&#65288;&#22914;&#34893;&#23556;&#20809;&#23398;&#20803;&#20214;&#65288;DOE&#65289;&#25110;&#37329;&#23646;&#36879;&#38236;&#65289;&#26500;&#25104;&#30340;&#31616;&#21333;&#20809;&#23398;&#31995;&#32479;&#65292;&#25110;&#32773;&#20174;&#33391;&#22909;&#30340;&#21021;&#22987;&#35774;&#35745;&#24494;&#35843;&#22797;&#21512;&#36879;&#38236;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Curriculum Learning&#30340;DeepLens&#35774;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#34920;&#38754;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#22797;&#21512;&#36879;&#38236;&#30340;&#20809;&#23398;&#35774;&#35745;&#65292;&#32780;&#26080;&#38656;&#20154;&#20026;&#24178;&#39044;&#65292;&#22240;&#27492;&#20811;&#26381;&#20102;&#23545;&#33391;&#22909;&#21021;&#22987;&#35774;&#35745;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#23436;&#20840;&#33258;&#21160;&#35774;&#35745;&#32463;&#20856;&#25104;&#20687;&#36879;&#38236;&#21644;&#19968;&#20010;&#31867;&#20284;&#25163;&#26426;&#39118;&#26684;&#30340;&#22823;&#35270;&#22330;&#24310;&#20280;&#26223;&#28145;&#35745;&#31639;&#36879;&#38236;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20855;&#26377;&#39640;&#24230;&#38750;&#29699;&#38754;&#26354;&#38754;&#21644;&#30701;&#21518;&#28966;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.01089v3 Announce Type: replace-cross  Abstract: Deep optical optimization has recently emerged as a new paradigm for designing computational imaging systems using only the output image as the objective. However, it has been limited to either simple optical systems consisting of a single element such as a diffractive optical element (DOE) or metalens, or the fine-tuning of compound lenses from good initial designs. Here we present a DeepLens design method based on curriculum learning, which is able to learn optical designs of compound lenses ab initio from randomly initialized surfaces without human intervention, therefore overcoming the need for a good initial design. We demonstrate the effectiveness of our approach by fully automatically designing both classical imaging lenses and a large field-of-view extended depth-of-field computational lens in a cellphone-style form factor, with highly aspheric surfaces and a short back focal length.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#32508;&#21512;&#22686;&#24378;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#21033;&#29992;&#36866;&#24403;&#30340;&#32452;&#21512;&#65292;&#20998;&#26512;&#24182;&#21387;&#32553;&#27169;&#25311;&#24322;&#24120;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#19982;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#20998;&#21106;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;MVTec&#24322;&#24120;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.15068</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#32508;&#21512;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Augmentation Framework for Anomaly Detection. (arXiv:2308.15068v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#32508;&#21512;&#22686;&#24378;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#21033;&#29992;&#36866;&#24403;&#30340;&#32452;&#21512;&#65292;&#20998;&#26512;&#24182;&#21387;&#32553;&#27169;&#25311;&#24322;&#24120;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#19982;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#37319;&#29992;&#20998;&#21106;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;MVTec&#24322;&#24120;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36890;&#24120;&#34987;&#25972;&#21512;&#21040;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#35757;&#32451;&#20013;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22797;&#21046;&#30495;&#23454;&#19990;&#30028;&#30340;&#24322;&#24120;&#25110;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#21040;&#24322;&#24120;&#30340;&#26631;&#20934;&#22312;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#20998;&#24067;&#30340;&#20559;&#24046;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#23545;&#37325;&#26500;&#32593;&#32476;&#35757;&#32451;&#26377;&#36129;&#29486;&#30340;&#27169;&#25311;&#24322;&#24120;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#21387;&#32553;&#25104;&#20960;&#31181;&#26041;&#27861;&#65292;&#20174;&#32780;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#36866;&#24403;&#30340;&#32452;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#23558;&#36825;&#20010;&#26694;&#26550;&#19982;&#22522;&#20110;&#37325;&#26500;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24182;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#21106;&#35757;&#32451;&#31574;&#30053;&#65292;&#26082;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#21448;&#36991;&#20813;&#23545;&#37325;&#26500;&#36807;&#31243;&#24341;&#20837;&#24178;&#25200;&#12290;&#22312;MVTec&#24322;&#24120;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#30446;&#26631;&#30456;&#20851;&#25351;&#26631;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation methods are commonly integrated into the training of anomaly detection models. Previous approaches have primarily focused on replicating real-world anomalies or enhancing diversity, without considering that the standard of anomaly varies across different classes, potentially leading to a biased training distribution.This paper analyzes crucial traits of simulated anomalies that contribute to the training of reconstructive networks and condenses them into several methods, thus creating a comprehensive framework by selectively utilizing appropriate combinations.Furthermore, we integrate this framework with a reconstruction-based approach and concurrently propose a split training strategy that alleviates the issue of overfitting while avoiding introducing interference to the reconstruction process. The evaluations conducted on the MVTec anomaly detection dataset demonstrate that our method outperforms the previous state-of-the-art approach, particularly in terms of objec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoRAPrune&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#30340;&#20540;&#21644;&#26799;&#24230;&#26469;&#35774;&#35745;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18403</link><description>&lt;p&gt;
&#21098;&#26525;&#19982;&#20302;&#31209;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30456;&#36935;
&lt;/p&gt;
&lt;p&gt;
Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning. (arXiv:2305.18403v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoRAPrune&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#30340;&#20540;&#21644;&#26799;&#24230;&#26469;&#35774;&#35745;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LPM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26469;&#20415;&#23452;&#22320;&#24494;&#35843;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#30340;&#37096;&#32626;&#20173;&#28982;&#21463;&#21040;&#24040;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#21046;&#32422;&#12290;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#36890;&#36807;&#21024;&#38500;&#20887;&#20313;&#21442;&#25968;&#26469;&#25552;&#20379;&#27169;&#22411;&#21387;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#21442;&#25968;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LPM&#32780;&#35328;&#65292;&#33719;&#24471;&#26799;&#24230;&#26159;&#35745;&#31639;&#19978;&#31105;&#27490;&#30340;&#65292;&#36825;&#38656;&#35201;&#25506;&#32034;&#26367;&#20195;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;LPM&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#31216;&#20026;LoRAPrune&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#21033;&#29992;&#20102;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#20540;&#21644;&#26799;&#24230;&#65292;&#32780;&#19981;&#26159;&#39044;&#35757;&#32451;&#21442;&#25968;&#30340;&#26799;&#24230;&#36827;&#34892;&#37325;&#35201;&#24615;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#22522;&#20110;&#21098;&#26525;&#20934;&#21017;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#12290;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained models (LPMs), such as LLaMA and ViT-G, have shown exceptional performance across various tasks. Although parameter-efficient fine-tuning (PEFT) has emerged to cheaply fine-tune these large models on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Neural network pruning offers a solution for model compression by removing redundant parameters, but most existing methods rely on computing parameter gradients. However, obtaining the gradients is computationally prohibitive for LPMs, which necessitates the exploration of alternative approaches. To this end, we propose a unified framework for efficient fine-tuning and deployment of LPMs, termed LoRAPrune. We first design a PEFT-aware pruning criterion, which utilizes the values and gradients of Low-Rank Adaption (LoRA), rather than the gradients of pre-trained parameters for importance estimation. We then propose an iterative pruning procedure to remove redundant paramet
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25299;&#23637;&#23545;&#26356;&#22823;&#26356;&#22810;&#20803;&#21270;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#24182;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18381</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#30719;&#30707;&#20013;&#25552;&#28860;&#40644;&#37329;: &#22522;&#20110;&#20851;&#38190;&#26679;&#26412;&#36873;&#25321;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection. (arXiv:2305.18381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18381
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25299;&#23637;&#23545;&#26356;&#22823;&#26356;&#22810;&#20803;&#21270;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#24182;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25928;&#29575;&#23398;&#20064;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25317;&#26377;&#22823;&#37327;&#22810;&#27169;&#22411;&#30340;&#29616;&#22312;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#21487;&#20197;&#25104;&#20026;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#36807;&#31243;&#26412;&#36523;&#20173;&#28982;&#38750;&#24120;&#20302;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#29992;&#20449;&#24687;&#29702;&#35770;&#26469;&#24314;&#27169;&#33976;&#39311;&#38382;&#39064;&#65292;&#35266;&#23519;&#21040;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#23384;&#22312;&#20005;&#37325;&#30340;&#25968;&#25454;&#20887;&#20313;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20197;&#20415;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#26679;&#26412;&#36873;&#25321;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#20248;&#21270;&#36807;&#31243;&#12290;&#36825;&#31181;&#26032;&#31574;&#30053;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#25193;&#22823;&#29616;&#26377;&#31639;&#27861;&#33539;&#22260;&#20197;&#23545;&#26356;&#24222;&#22823;&#21644;&#22810;&#20803;&#21270;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#20363;&#22914;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21482;&#38656;&#35201;0.04&#65285;&#30340;&#35757;&#32451;&#25968;&#25454;&#23601;&#36275;&#20197;&#20445;&#25345;&#21487;&#27604;&#30340;&#33976;&#39311;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#20854;&#36129;&#29486;&#21487;&#33021;&#20026;&#33976;&#39311;&#36807;&#31243;&#30340;&#21160;&#21147;&#23398;&#24320;&#36767;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-efficient learning has drawn significant attention, especially given the current trend of large multi-modal models, where dataset distillation can be an effective solution. However, the dataset distillation process itself is still very inefficient. In this work, we model the distillation problem with reference to information theory. Observing that severe data redundancy exists in dataset distillation, we argue to put more emphasis on the utility of the training samples. We propose a family of methods to exploit the most valuable samples, which is validated by our comprehensive analysis of the optimal data selection. The new strategy significantly reduces the training cost and extends a variety of existing distillation algorithms to larger and more diversified datasets, e.g. in some cases only 0.04% training data is sufficient for comparable distillation performance. Moreover, our strategy consistently enhances the performance, which may open up new analyses on the dynamics of dist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#20998;&#26512;&#20102;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;220&#20010;&#25919;&#27835;&#39046;&#34966;&#30340;YouTube&#35270;&#39057;&#65292;&#24635;&#32467;&#20102;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.09914</link><description>&lt;p&gt;
&#26623;&#23376;&#25919;&#27835;&#30340;&#38754;&#23380;&#65306;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27604;&#36739;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning. (arXiv:2304.09914v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#20998;&#26512;&#20102;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;220&#20010;&#25919;&#27835;&#39046;&#34966;&#30340;YouTube&#35270;&#39057;&#65292;&#24635;&#32467;&#20102;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23186;&#20307;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#25919;&#27835;&#20449;&#24687;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20256;&#25773;&#21644;&#28040;&#36153;&#26041;&#24335;&#65292;&#36825;&#31181;&#36716;&#21464;&#20419;&#20351;&#25919;&#27835;&#20154;&#29289;&#37319;&#21462;&#26032;&#30340;&#31574;&#30053;&#26469;&#25429;&#25417;&#21644;&#20445;&#25345;&#36873;&#27665;&#30340;&#27880;&#24847;&#21147;&#12290;&#36825;&#20123;&#31574;&#30053;&#24448;&#24448;&#20381;&#36182;&#20110;&#24773;&#24863;&#35828;&#26381;&#21644;&#21560;&#24341;&#12290;&#38543;&#30528;&#34394;&#25311;&#31354;&#38388;&#20013;&#35270;&#35273;&#20869;&#23481;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#24456;&#22810;&#25919;&#27835;&#27807;&#36890;&#20063;&#34987;&#26631;&#24535;&#30528;&#21796;&#36215;&#24773;&#24863;&#30340;&#35270;&#39057;&#20869;&#23481;&#21644;&#22270;&#20687;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;&#29616;&#26377;&#35757;&#32451;&#22909;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25552;&#20379;&#30340;Python&#24211;fer&#65292;&#24212;&#29992;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#65292;&#23545;&#25551;&#32472;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;&#25919;&#27835;&#39046;&#34966;&#30340;220&#20010;YouTube&#35270;&#39057;&#26679;&#26412;&#36827;&#34892;&#20998;&#26512;&#12290;&#35813;&#31639;&#27861;&#36820;&#22238;&#24773;&#32490;&#20998;&#25968;&#65292;&#27599;&#19968;&#24103;&#37117;&#20195;&#34920;6&#31181;&#24773;&#32490;&#29366;&#24577;&#65288;&#24868;&#24594;&#65292;&#21388;&#24694;&#65292;&#24656;&#24807;&#65292;&#24555;&#20048;&#65292;&#24754;&#20260;&#21644;&#24778;&#35766;&#65289;&#21644;&#19968;&#20010;&#20013;&#24615;&#34920;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online media has revolutionized the way political information is disseminated and consumed on a global scale, and this shift has compelled political figures to adopt new strategies of capturing and retaining voter attention. These strategies often rely on emotional persuasion and appeal, and as visual content becomes increasingly prevalent in virtual space, much of political communication too has come to be marked by evocative video content and imagery. The present paper offers a novel approach to analyzing material of this kind. We apply a deep-learning-based computer-vision algorithm to a sample of 220 YouTube videos depicting political leaders from 15 different countries, which is based on an existing trained convolutional neural network architecture provided by the Python library fer. The algorithm returns emotion scores representing the relative presence of 6 emotional states (anger, disgust, fear, happiness, sadness, and surprise) and a neutral expression for each frame of the pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;ASR&#25216;&#26415;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#31561;&#25928;&#21442;&#25968;&#36716;&#25442;&#23454;&#29616;&#19981;&#21516;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20043;&#38388;&#30340;&#20114;&#36716;&#12290;&#21644;&#29616;&#26377;&#30340;SRP&#26041;&#27861;&#30456;&#27604;&#65292;ASR&#21487;&#20197;&#25104;&#21151;&#32771;&#34385;&#33258;&#27880;&#24847;&#27169;&#22359;&#65292;&#23454;&#29616;&#25512;&#29702;&#26399;&#38388;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#22312;&#24037;&#19994;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.06345</link><description>&lt;p&gt;
ASR: &#20687;&#27880;&#24847;&#21147;&#19968;&#26679;&#30340;&#32467;&#26500;&#20877;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
ASR: Attention-alike Structural Re-parameterization. (arXiv:2304.06345v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06345
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#30340;ASR&#25216;&#26415;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#31561;&#25928;&#21442;&#25968;&#36716;&#25442;&#23454;&#29616;&#19981;&#21516;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20043;&#38388;&#30340;&#20114;&#36716;&#12290;&#21644;&#29616;&#26377;&#30340;SRP&#26041;&#27861;&#30456;&#27604;&#65292;ASR&#21487;&#20197;&#25104;&#21151;&#32771;&#34385;&#33258;&#27880;&#24847;&#27169;&#22359;&#65292;&#23454;&#29616;&#25512;&#29702;&#26399;&#38388;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#22312;&#24037;&#19994;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#20877;&#21442;&#25968;&#21270;&#65288;SRP&#65289;&#25216;&#26415;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#31561;&#25928;&#21442;&#25968;&#36716;&#25442;&#23454;&#29616;&#19981;&#21516;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20043;&#38388;&#30340;&#20114;&#36716;&#12290;&#35813;&#25216;&#26415;&#20351;&#24471;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36890;&#36807;&#36825;&#20123;&#36716;&#25442;&#20943;&#23569;&#24615;&#33021;&#25552;&#21319;&#30340;&#26032;&#22686;&#20195;&#20215;&#65292;&#20363;&#22914;&#21442;&#25968;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#22240;&#27492;SRP&#22312;&#24037;&#19994;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;SRP&#26041;&#27861;&#24050;&#25104;&#21151;&#32771;&#34385;&#20102;&#35768;&#22810;&#24120;&#29992;&#30340;&#26550;&#26500;&#65292;&#20363;&#22914;&#24402;&#19968;&#21270;&#12289;&#27744;&#21270;&#26041;&#27861;&#12289;&#22810;&#20998;&#25903;&#21367;&#31215;&#31561;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#33258;&#27880;&#24847;&#27169;&#22359;&#30001;&#20110;&#22312;&#25512;&#29702;&#26399;&#38388;&#36890;&#24120;&#20197;&#20056;&#27861;&#26041;&#24335;&#20316;&#29992;&#20110;&#39592;&#24178;&#32593;&#32476;&#24182;&#19988;&#27169;&#22359;&#30340;&#36755;&#20986;&#22312;&#25512;&#29702;&#26102;&#20381;&#36182;&#20110;&#36755;&#20837;&#65292;&#25152;&#20197;&#26080;&#27861;&#30452;&#25509;&#23454;&#29616;SRP&#65292;&#32780;&#36825;&#38480;&#21046;&#20102;SRP&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#32479;&#35745;&#35282;&#24230;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
The structural re-parameterization (SRP) technique is a novel deep learning technique that achieves interconversion between different network architectures through equivalent parameter transformations. This technique enables the mitigation of the extra costs for performance improvement during training, such as parameter size and inference time, through these transformations during inference, and therefore SRP has great potential for industrial and practical applications. The existing SRP methods have successfully considered many commonly used architectures, such as normalizations, pooling methods, multi-branch convolution. However, the widely used self-attention modules cannot be directly implemented by SRP due to these modules usually act on the backbone network in a multiplicative manner and the modules' output is input-dependent during inference, which limits the application scenarios of SRP. In this paper, we conduct extensive experiments from a statistical perspective and discover
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#65292;&#36890;&#36807;&#29983;&#25104;&#25351;&#32441;&#26679;&#26412;&#24182;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#29256;&#26435;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#65292;&#20855;&#26377;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.11760</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#25351;&#32441;&#35782;&#21035;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Fingerprinting Generative Adversarial Networks. (arXiv:2106.11760v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#65292;&#36890;&#36807;&#29983;&#25104;&#25351;&#32441;&#26679;&#26412;&#24182;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#29256;&#26435;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#65292;&#20855;&#26377;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;&#30001;&#20110;&#21830;&#19994;GAN&#30340;&#29983;&#20135;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#20154;&#21147;&#36164;&#28304;&#65292;&#22240;&#27492;&#36843;&#20999;&#38656;&#35201;&#29256;&#26435;&#20445;&#25252;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#12290;&#25105;&#20204;&#31361;&#30772;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#25152;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21019;&#36896;&#24615;&#22320;&#20174;&#30446;&#26631;GAN&#21644;&#20998;&#31867;&#22120;&#26500;&#24314;&#19968;&#20010;&#22797;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#36825;&#20010;&#22797;&#21512;&#27169;&#22411;&#20013;&#20135;&#29983;&#25351;&#32441;&#26679;&#26412;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#29256;&#26435;&#39564;&#35777;&#12290;&#36825;&#31181;&#26041;&#26696;&#21551;&#21457;&#20102;&#19968;&#20123;&#20855;&#20307;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#28385;&#36275;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#25152;&#38656;&#35201;&#30340;&#19981;&#21516;&#23433;&#20840;&#35201;&#27714;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#35777;&#26126;&#35813;&#26041;&#26696;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) have been widely used in various application scenarios. Since the production of a commercial GAN requires substantial computational and human resources, the copyright protection of GANs is urgently needed. In this paper, we present the first fingerprinting scheme for the Intellectual Property (IP) protection of GANs. We break through the stealthiness and robustness bottlenecks suffered by previous fingerprinting methods for classification models being naively transferred to GANs. Specifically, we innovatively construct a composite deep learning model from the target GAN and a classifier. Then we generate fingerprint samples from this composite model, and embed them in the classifier for effective ownership verification. This scheme inspires some concrete methodologies to practically protect the modern GAN models. Theoretical analysis proves that these methods can satisfy different security requirements necessary for IP protection. We also conduct 
&lt;/p&gt;</description></item></channel></rss>