<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#24182;&#23558;&#20854;&#36866;&#24212;&#20026;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#24773;&#32490;&#35782;&#21035;&#20013;&#20851;&#38190;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2011.08388</link><description>&lt;p&gt;
&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#21487;&#35299;&#37322;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#65292;&#24182;&#21033;&#29992;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation based Interpretable Image Emotion Recognition using Facial Expression Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2011.08388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#24182;&#23558;&#20854;&#36866;&#24212;&#20026;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#24773;&#32490;&#35782;&#21035;&#20013;&#20851;&#38190;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#29992;&#20110;&#35782;&#21035;&#21253;&#21547;&#38754;&#37096;&#21644;&#38750;&#38754;&#37096;&#29289;&#20307;&#20197;&#21450;&#38750;&#20154;&#31867;&#32452;&#20214;&#30340;&#36890;&#29992;&#22270;&#20687;&#20013;&#30340;&#24773;&#32490;&#12290;&#23427;&#35299;&#20915;&#20102;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#65288;IER&#65289;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#33391;&#22909;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#65288;FER&#65289;&#31995;&#32479;&#65292;&#23558;&#32473;&#23450;&#30340;&#38754;&#37096;&#22270;&#20687;&#20998;&#31867;&#20026;&#31163;&#25955;&#24773;&#32490;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20687;&#35782;&#21035;&#31995;&#32479;&#65292;&#23558;&#25552;&#20986;&#30340;FER&#31995;&#32479;&#36866;&#24212;&#20110;&#21033;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#35782;&#21035;&#22270;&#20687;&#25152;&#20256;&#36798;&#30340;&#24773;&#32490;&#12290;&#23427;&#23558;&#36890;&#29992;&#22270;&#20687;&#20998;&#31867;&#20026;&#8220;&#24555;&#20048;&#8221;&#65292;&#8220;&#24754;&#20260;&#8221;&#65292;&#8220;&#20167;&#24680;&#8221;&#21644;&#8220;&#24868;&#24594;&#8221;&#31867;&#21035;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#31216;&#20026;&#20998;&#32780;&#27835;&#20043;&#30340;Shap&#65288;DnCShap&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#24773;&#32490;&#35782;&#21035;&#20013;&#39640;&#24230;&#30456;&#20851;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
A domain adaptation technique has been proposed in this paper to identify the emotions in generic images containing facial &amp; non-facial objects and non-human components. It addresses the challenge of the insufficient availability of pre-trained models and well-annotated datasets for image emotion recognition (IER). It starts with proposing a facial emotion recognition (FER) system and then moves on to adapting it for image emotion recognition. First, a deep-learning-based FER system has been proposed that classifies a given facial image into discrete emotion classes. Further, an image recognition system has been proposed that adapts the proposed FER system to recognize the emotions portrayed by images using domain adaptation. It classifies the generic images into 'happy,' 'sad,' 'hate,' and 'anger' classes. A novel interpretability approach, Divide and Conquer based Shap (DnCShap), has also been proposed to interpret the highly relevant visual features for emotion recognition. The prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#22270;&#20687;&#20998;&#31867;&#25506;&#31350;&#20102;&#20154;&#26426;&#24863;&#30693;&#24046;&#24322;&#65292;&#21457;&#29616;&#21363;&#20351;&#20934;&#30830;&#29575;&#30456;&#20284;&#65292;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#31572;&#26696;&#20998;&#24067;&#20063;&#21487;&#33021;&#19981;&#21516;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#20154;&#26426;&#21512;&#20316;&#26469;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.08733</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#26426;&#22120;&#26377;&#30456;&#21516;&#30340;&#30524;&#30555;&#21527;&#65311;&#22522;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#20154;&#26426;&#24863;&#30693;&#24046;&#24322;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do humans and machines have the same eyes? Human-machine perceptual differences on image classification. (arXiv:2304.08733v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#22270;&#20687;&#20998;&#31867;&#25506;&#31350;&#20102;&#20154;&#26426;&#24863;&#30693;&#24046;&#24322;&#65292;&#21457;&#29616;&#21363;&#20351;&#20934;&#30830;&#29575;&#30456;&#20284;&#65292;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#31572;&#26696;&#20998;&#24067;&#20063;&#21487;&#33021;&#19981;&#21516;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#20154;&#26426;&#21512;&#20316;&#26469;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#33391;&#22909;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#27169;&#20223;&#20174;&#35757;&#32451;&#26631;&#31614;&#20013;&#23398;&#21040;&#30340;&#20154;&#31867;&#34892;&#20026;&#26469;&#35299;&#20915;&#35270;&#35273;&#20219;&#21153;&#12290;&#36817;&#26399;&#35270;&#35273;&#30740;&#31350;&#30340;&#22823;&#37096;&#20998;&#21162;&#21147;&#38598;&#20013;&#22312;&#20351;&#29992;&#26631;&#20934;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#27169;&#22411;&#20219;&#21153;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20102;&#35299;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#24863;&#30693;&#24046;&#24322;&#26041;&#38754;&#30340;&#24037;&#20316;&#36824;&#24456;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#20808;&#37327;&#21270;&#24182;&#20998;&#26512;&#20102;&#20004;&#31181;&#26469;&#28304;&#38169;&#35823;&#30340;&#32479;&#35745;&#20998;&#24067;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#38590;&#24230;&#32423;&#21035;&#23545;&#20219;&#21153;&#36827;&#34892;&#25490;&#24207;&#65292;&#25506;&#35752;&#20154;&#31867;&#19982;&#26426;&#22120;&#19987;&#19994;&#30693;&#35782;&#30340;&#24046;&#24322;&#12290;&#21363;&#20351;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#30456;&#20284;&#65292;&#31572;&#26696;&#30340;&#20998;&#24067;&#20063;&#21487;&#33021;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#21033;&#29992;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#24863;&#30693;&#24046;&#24322;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#20102;&#19968;&#31181;&#21518;&#26399;&#20154;&#26426;&#21512;&#20316;&#65292;&#20854;&#34920;&#29616;&#27604;&#21333;&#29420;&#30340;&#20154;&#25110;&#26426;&#22120;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trained computer vision models are assumed to solve vision tasks by imitating human behavior learned from training labels. Most efforts in recent vision research focus on measuring the model task performance using standardized benchmarks. Limited work has been done to understand the perceptual difference between humans and machines. To fill this gap, our study first quantifies and analyzes the statistical distributions of mistakes from the two sources. We then explore human vs. machine expertise after ranking tasks by difficulty levels. Even when humans and machines have similar overall accuracies, the distribution of answers may vary. Leveraging the perceptual difference between humans and machines, we empirically demonstrate a post-hoc human-machine collaboration that outperforms humans or machines alone.
&lt;/p&gt;</description></item></channel></rss>