<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20351;&#29992;&#21463;&#25511;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#28040;&#38500;&#24515;&#33039;&#25104;&#20687;&#20013;&#30340;&#20559;&#24046;&#21644;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19508</link><description>&lt;p&gt;
&#29992;&#21463;&#25511;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#28040;&#38500;&#24515;&#33039;&#25104;&#20687;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Debiasing Cardiac Imaging with Controlled Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19508
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20351;&#29992;&#21463;&#25511;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#28040;&#38500;&#24515;&#33039;&#25104;&#20687;&#20013;&#30340;&#20559;&#24046;&#21644;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#23545;&#22522;&#20110;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#36827;&#34892;&#30142;&#30149;&#35786;&#26029;&#21644;&#39044;&#21518;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#36827;&#23637;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#39640;&#24230;&#19981;&#24179;&#34913;&#21644;&#20559;&#24046;&#30340;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#12289;&#24180;&#40836;&#12289;&#20307;&#37325;&#25351;&#25968;&#21644;&#20581;&#24247;&#29366;&#20917;&#65289;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#20943;&#36731;&#25968;&#25454;&#38598;&#20013;&#22266;&#26377;&#30340;&#19981;&#24179;&#34913;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;ControlNet&#26469;&#20197;&#24739;&#32773;&#20803;&#25968;&#25454;&#21644;&#20351;&#29992;&#22823;&#22411;&#38431;&#21015;&#30740;&#31350;&#65288;&#20855;&#20307;&#26469;&#35828;&#26159;UK Biobank&#65289;&#20013;&#20998;&#21106;&#25513;&#27169;&#23548;&#20986;&#30340;&#24515;&#33039;&#20960;&#20309;&#24418;&#29366;&#20026;&#26465;&#20214;&#29983;&#25104;&#25991;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;&#23450;&#37327;&#25351;&#26631;&#35780;&#20272;&#29983;&#25104;&#22270;&#20687;&#30340;&#36924;&#30495;&#31243;&#24230;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#30340;&#26679;&#26412;&#32416;&#27491;&#20195;&#34920;&#24615;&#19981;&#36275;&#32676;&#20307;&#20869;&#30340;&#19981;&#24179;&#34913;&#26469;&#25913;&#27491;&#20998;&#31867;&#22120;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#31034;&#33539;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19508v1 Announce Type: cross  Abstract: The progress in deep learning solutions for disease diagnosis and prognosis based on cardiac magnetic resonance imaging is hindered by highly imbalanced and biased training data. To address this issue, we propose a method to alleviate imbalances inherent in datasets through the generation of synthetic data based on sensitive attributes such as sex, age, body mass index, and health condition. We adopt ControlNet based on a denoising diffusion probabilistic model to condition on text assembled from patient metadata and cardiac geometry derived from segmentation masks using a large-cohort study, specifically, the UK Biobank. We assess our method by evaluating the realism of the generated images using established quantitative metrics. Furthermore, we conduct a downstream classification task aimed at debiasing a classifier by rectifying imbalances within underrepresented groups through synthetically generated samples. Our experiments demons
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#24314;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#30001;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20551;&#22270;&#20687;&#24402;&#22240;&#20110;&#20854;&#26469;&#28304;&#27169;&#22411;&#65292;&#20174;&#32780;&#35753;&#27169;&#22411;&#25152;&#26377;&#32773;&#23545;&#27169;&#22411;&#30340;&#20219;&#20309;&#28389;&#29992;&#36127;&#36131;&#12290;</title><link>https://arxiv.org/abs/2403.01489</link><description>&lt;p&gt;
&#22522;&#20110;&#37325;&#24314;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20551;&#22270;&#20687;&#28335;&#28304;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Regeneration Based Training-free Attribution of Fake Images Generated by Text-to-Image Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01489
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#24314;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#30001;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#20551;&#22270;&#20687;&#24402;&#22240;&#20110;&#20854;&#26469;&#28304;&#27169;&#22411;&#65292;&#20174;&#32780;&#35753;&#27169;&#22411;&#25152;&#26377;&#32773;&#23545;&#27169;&#22411;&#30340;&#20219;&#20309;&#28389;&#29992;&#36127;&#36131;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26368;&#36817;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#22522;&#20110;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20154;&#20204;&#23545;&#29983;&#25104;&#30340;&#20551;&#22270;&#20687;&#21487;&#33021;&#34987;&#28389;&#29992;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#30001;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#30340;&#20551;&#22270;&#20687;&#24402;&#22240;&#20110;&#20854;&#26469;&#28304;&#27169;&#22411;&#12290;&#32473;&#23450;&#19968;&#20010;&#24453;&#24402;&#22240;&#30340;&#27979;&#35797;&#22270;&#20687;&#65292;&#39318;&#20808;&#25105;&#20204;&#21453;&#21521;&#37325;&#24314;&#22270;&#20687;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#28982;&#21518;&#23558;&#37325;&#24314;&#30340;&#25552;&#31034;&#25918;&#20837;&#19981;&#21516;&#30340;&#20505;&#36873;&#27169;&#22411;&#20013;&#20197;&#20877;&#29616;&#20505;&#36873;&#20551;&#22270;&#20687;&#12290;&#36890;&#36807;&#35745;&#31639;&#21644;&#25490;&#21517;&#27979;&#35797;&#22270;&#20687;&#19982;&#20505;&#36873;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#22270;&#20687;&#30340;&#26469;&#28304;&#12290;&#36825;&#31181;&#28335;&#28304;&#26041;&#27861;&#21487;&#20197;&#35753;&#27169;&#22411;&#25152;&#26377;&#32773;&#23545;&#20854;&#27169;&#22411;&#30340;&#20219;&#20309;&#28389;&#29992;&#36127;&#36131;&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38480;&#21046;&#20505;&#36873;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01489v1 Announce Type: cross  Abstract: Text-to-image generative models have recently garnered significant attention due to their ability to generate images based on prompt descriptions. While these models have shown promising performance, concerns have been raised regarding the potential misuse of the generated fake images. In response to this, we have presented a simple yet effective training-free method to attribute fake images generated by text-to-image models to their source models. Given a test image to be attributed, we first inverse the textual prompt of the image, and then put the reconstructed prompt into different candidate models to regenerate candidate fake images. By calculating and ranking the similarity of the test image and the candidate images, we can determine the source of the image. This attribution allows model owners to be held accountable for any misuse of their models. Note that our approach does not limit the number of candidate text-to-image genera
&lt;/p&gt;</description></item><item><title>AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12226</link><description>&lt;p&gt;
AnyGPT&#65306;&#32479;&#19968;&#30340;&#22810;&#27169;&#24335;&#31163;&#25955;&#24207;&#21015;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12226
&lt;/p&gt;
&lt;p&gt;
AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; AnyGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20219;&#24847;&#22810;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#31163;&#25955;&#34920;&#31034;&#32479;&#19968;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#38899;&#20048;&#12290;AnyGPT &#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#26080;&#38656;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26550;&#26500;&#25110;&#35757;&#32451;&#33539;&#24335;&#36827;&#34892;&#20219;&#20309;&#25913;&#21160;&#12290;&#30456;&#21453;&#65292;&#23427;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#32423;&#39044;&#22788;&#29702;&#65292;&#20419;&#36827;&#20102;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#38598;&#25104;&#21040;LLM&#20013;&#65292;&#31867;&#20284;&#20110;&#26032;&#35821;&#35328;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24335;&#25991;&#26412;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22810;&#27169;&#24335;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#21512;&#25104;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#20219;&#24847;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#25324;108k&#20010;&#22810;&#36718;&#23545;&#35805;&#31034;&#20363;&#65292;&#31934;&#32454;&#22320;&#20132;&#32455;&#21508;&#31181;&#27169;&#24577;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20219;&#24847;&#32452;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AnyGPT&#33021;&#22815;&#20419;&#36827;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#35777;&#25454;&#30340;&#31435;&#22330;&#25277;&#21462;&#32593;&#32476;&#65288;SEN&#65289;&#26469;&#26816;&#27979;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;&#12290;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#35777;&#25454;&#30340;&#31435;&#22330;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20849;&#29616;&#20851;&#31995;&#30340;&#25903;&#25345;-&#21453;&#39539;&#20998;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20844;&#20849;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01766</link><description>&lt;p&gt;
&#25903;&#25345;&#36824;&#26159;&#21453;&#39539;&#65306;&#20998;&#26512;&#35777;&#25454;&#31435;&#22330;&#20197;&#26816;&#27979;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation. (arXiv:2311.01766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#35777;&#25454;&#30340;&#31435;&#22330;&#25277;&#21462;&#32593;&#32476;&#65288;SEN&#65289;&#26469;&#26816;&#27979;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;&#12290;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#35777;&#25454;&#30340;&#31435;&#22330;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20849;&#29616;&#20851;&#31995;&#30340;&#25903;&#25345;-&#21453;&#39539;&#20998;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20844;&#20849;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#35823;&#23548;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#22269;&#23478;&#32423;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#26159;&#21508;&#31181;&#22312;&#32447;&#20260;&#23475;&#30340;&#20027;&#35201;&#26469;&#28304;&#20043;&#19968;&#12290;&#20854;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#35823;&#23548;&#20449;&#24687;&#24418;&#24335;&#26159;&#19978;&#19979;&#25991;&#38169;&#35823;&#65288;OOC&#65289;&#20449;&#24687;&#65292;&#20854;&#20013;&#19981;&#21516;&#30340;&#20449;&#24687;&#34987;&#38169;&#35823;&#22320;&#20851;&#32852;&#36215;&#26469;&#65292;&#20363;&#22914;&#30495;&#23454;&#22270;&#20687;&#19982;&#34394;&#20551;&#30340;&#25991;&#26412;&#26631;&#39064;&#25110;&#35823;&#23548;&#24615;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#23613;&#31649;&#19968;&#20123;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#22806;&#37096;&#35777;&#25454;&#26469;&#25269;&#24481;&#19978;&#19979;&#25991;&#38169;&#35823;&#30340;&#35823;&#23548;&#20449;&#24687;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#21516;&#31435;&#22330;&#30340;&#19981;&#21516;&#35777;&#25454;&#30340;&#20316;&#29992;&#12290;&#21463;&#21040;&#35777;&#25454;&#31435;&#22330;&#20195;&#34920;&#19981;&#21516;&#26816;&#27979;&#32467;&#26524;&#30340;&#20559;&#35265;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#25552;&#21462;&#22810;&#27169;&#24577;&#35777;&#25454;&#30340;&#31435;&#22330;&#30340;&#31435;&#22330;&#25277;&#21462;&#32593;&#32476;&#65288;SEN&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22522;&#20110;&#21629;&#21517;&#23454;&#20307;&#30340;&#20849;&#29616;&#20851;&#31995;&#35745;&#31639;&#30340;&#25903;&#25345;-&#21453;&#39539;&#20998;&#25968;&#21040;&#25991;&#26412;SEN&#20013;&#12290;&#23545;&#20844;&#20849;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mis- and disinformation online have become a major societal problem as major sources of online harms of different kinds. One common form of mis- and disinformation is out-of-context (OOC) information, where different pieces of information are falsely associated, e.g., a real image combined with a false textual caption or a misleading textual description. Although some past studies have attempted to defend against OOC mis- and disinformation through external evidence, they tend to disregard the role of different pieces of evidence with different stances. Motivated by the intuition that the stance of evidence represents a bias towards different detection results, we propose a stance extraction network (SEN) that can extract the stances of different pieces of multi-modal evidence in a unified framework. Moreover, we introduce a support-refutation score calculated based on the co-occurrence relations of named entities into the textual SEN. Extensive experiments on a public large-scale data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#24341;&#20837;&#22122;&#22768;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#22312;&#38477;&#20302;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#26465;&#20214;&#19979;&#25552;&#21319;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.10625</link><description>&lt;p&gt;
&#25506;&#32034;&#23398;&#20064;&#31995;&#32479;&#20013;&#20449;&#24687;&#29109;&#21464;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Influence of Information Entropy Change in Learning Systems. (arXiv:2309.10625v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#24341;&#20837;&#22122;&#22768;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#22312;&#38477;&#20302;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#26465;&#20214;&#19979;&#25552;&#21319;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21521;&#36755;&#20837;/&#38544;&#21547;&#29305;&#24449;&#28155;&#21152;&#22122;&#22768;&#26469;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#29109;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#30340;&#24212;&#29992;&#37325;&#28857;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#65292;&#20294;&#25152;&#25552;&#20986;&#30340;&#29702;&#35770;&#21487;&#20197;&#36827;&#19968;&#27493;&#24212;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#12290;&#22122;&#22768;&#36890;&#24120;&#34987;&#35270;&#20026;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#65289;&#20197;&#21450;&#22270;&#20687;&#20998;&#31867;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#19981;&#21516;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#23475;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#26088;&#22312;&#37325;&#26032;&#24605;&#32771;&#20256;&#32479;&#21629;&#39064;&#26159;&#21542;&#24635;&#26159;&#25104;&#31435;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#25552;&#21319;&#21508;&#31181;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20449;&#24687;&#29109;&#23450;&#20041;&#30340;&#20219;&#21153;&#22797;&#26434;&#24615;&#20943;&#23569;&#26041;&#38754;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#27491;&#22122;&#22768;&#30340;&#22686;&#24378;&#25928;&#26524;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#65289;&#20013;&#23454;&#39564;&#35777;&#26126;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore the influence of entropy change in deep learning systems by adding noise to the inputs/latent features. The applications in this paper focus on deep learning tasks within computer vision, but the proposed theory can be further applied to other fields. Noise is conventionally viewed as a harmful perturbation in various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers (ViTs), as well as different learning tasks like image classification and transfer learning. However, this paper aims to rethink whether the conventional proposition always holds. We demonstrate that specific noise can boost the performance of various deep architectures under certain conditions. We theoretically prove the enhancement gained from positive noise by reducing the task complexity defined by information entropy and experimentally show the significant performance gain in large image datasets, such as the ImageNet. Herein, we use the informat
&lt;/p&gt;</description></item></channel></rss>