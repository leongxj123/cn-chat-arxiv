<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>DeNetDM &#26159;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#19987;&#23478;&#20056;&#31215;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#22312;&#21019;&#24314;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#21518;&#65292;&#23558;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#21435;&#20559;&#35265;&#27169;&#22411;&#65292;&#30456;&#27604;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21462;&#24471;&#26356;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.19863</link><description>&lt;p&gt;
DeNetDM: &#36890;&#36807;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#26469;&#28040;&#38500;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
DeNetDM: Debiasing by Network Depth Modulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19863
&lt;/p&gt;
&lt;p&gt;
DeNetDM &#26159;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#19987;&#23478;&#20056;&#31215;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#22312;&#21019;&#24314;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#21518;&#65292;&#23558;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#21435;&#20559;&#35265;&#27169;&#22411;&#65292;&#30456;&#27604;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21462;&#24471;&#26356;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31070;&#32463;&#32593;&#32476;&#22312;&#20559;&#35265;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#26080;&#24847;&#38388;&#23398;&#20064;&#21040;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#23454;&#29616;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#30446;&#21069;&#35299;&#20915;&#36825;&#31181;&#20559;&#35265;&#30340;&#26041;&#27861;&#36890;&#24120;&#21253;&#25324;&#21033;&#29992;&#20559;&#35265;&#27880;&#37322;&#12289;&#26681;&#25454;&#20266;&#20559;&#35265;&#26631;&#31614;&#36827;&#34892;&#21152;&#26435;&#37325;&#12289;&#25110;&#36890;&#36807;&#22686;&#24378;&#25216;&#26415;&#22686;&#21152;&#20559;&#35265;&#20914;&#31361;&#25968;&#25454;&#28857;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DeNetDM&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35266;&#23519;&#32467;&#26524;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#20248;&#20808;&#23398;&#20064;&#26680;&#24515;&#23646;&#24615;&#65292;&#32780;&#26356;&#28145;&#23618;&#27425;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#33719;&#21462;&#19981;&#21516;&#20449;&#24687;&#26102;&#24378;&#35843;&#20559;&#35265;&#12290;&#25105;&#20204;&#21033;&#29992;&#20174;&#19987;&#23478;&#20056;&#31215;&#20013;&#25512;&#23548;&#20986;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#21019;&#24314;&#20102;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#65292;&#28982;&#21518;&#29992;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#30340;&#21435;&#20559;&#35265;&#27169;&#22411;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19863v1 Announce Type: new  Abstract: When neural networks are trained on biased datasets, they tend to inadvertently learn spurious correlations, leading to challenges in achieving strong generalization and robustness. Current approaches to address such biases typically involve utilizing bias annotations, reweighting based on pseudo-bias labels, or enhancing diversity within bias-conflicting data points through augmentation techniques. We introduce DeNetDM, a novel debiasing method based on the observation that shallow neural networks prioritize learning core attributes, while deeper ones emphasize biases when tasked with acquiring distinct information. Using a training paradigm derived from Product of Experts, we create both biased and debiased branches with deep and shallow architectures and then distill knowledge to produce the target debiased model. Extensive experiments and analyses demonstrate that our approach outperforms current debiasing techniques, achieving a not
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21487;&#24494;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#25910;&#25947;&#24615;&#24046;&#24322;&#12289;$L_1$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#30683;&#30462;&#24615;&#36136;&#20197;&#21450;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#30340;&#19981;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.08426</link><description>&lt;p&gt;
GD&#26080;&#27861;&#32988;&#20219;&#65306;&#38750;&#21487;&#24494;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#19977;&#31181;&#24433;&#21709;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
GD doesn't make the cut: Three ways that non-differentiability affects neural network training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21487;&#24494;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#25910;&#25947;&#24615;&#24046;&#24322;&#12289;$L_1$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#30683;&#30462;&#24615;&#36136;&#20197;&#21450;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#30340;&#19981;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#38750;&#21487;&#24494;&#20989;&#25968;&#65288;NGDMs&#65289;&#21644;&#24212;&#29992;&#20110;&#21487;&#24494;&#20989;&#25968;&#30340;&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#65288;GDs&#65289;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;NGDMs&#30340;&#25910;&#25947;&#24615;&#36136;&#19982;GDs&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#25361;&#25112;&#20102;&#22522;&#20110;$L$-&#20809;&#28369;&#24615;&#30340;&#24191;&#27867;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#25991;&#29486;&#23545;&#38750;&#20809;&#28369;&#31070;&#32463;&#32593;&#32476;&#30340;&#36866;&#29992;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NGDM&#35299;&#20915;$L_1$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#30683;&#30462;&#24615;&#36136;&#65292;&#34920;&#26126;&#22686;&#21152;&#27491;&#21017;&#21270;&#24809;&#32602;&#20250;&#23548;&#33268;NGDMs&#20013;&#26368;&#20248;&#35299;&#30340;$L_1$&#33539;&#25968;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24191;&#27867;&#37319;&#29992;&#30340;&#22522;&#20110;$L_1$&#24809;&#32602;&#30340;&#32593;&#32476;&#20462;&#21098;&#25216;&#26415;&#24182;&#26410;&#20135;&#29983;&#39044;&#26399;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#65288;Edge of Stability&#65289;&#65292;&#25351;&#20986;&#21363;&#20351;&#23545;&#20110;Lipschitz&#36830;&#32493;&#20984;&#21487;&#24494;&#20989;&#25968;&#65292;&#23427;&#20063;&#19981;&#36866;&#29992;&#20110;&#38750;&#20984;&#38750;&#21487;&#24494;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the distinctions between gradient methods applied to non-differentiable functions (NGDMs) and classical gradient descents (GDs) designed for differentiable functions. First, we demonstrate significant differences in the convergence properties of NGDMs compared to GDs, challenging the applicability of the extensive neural network convergence literature based on $L-smoothness$ to non-smooth neural networks. Next, we demonstrate the paradoxical nature of NGDM solutions for $L_{1}$-regularized problems, showing that increasing the regularization penalty leads to an increase in the $L_{1}$ norm of optimal solutions in NGDMs. Consequently, we show that widely adopted $L_{1}$ penalization-based techniques for network pruning do not yield expected results. Finally, we explore the Edge of Stability phenomenon, indicating its inapplicability even to Lipschitz continuous convex differentiable functions, leaving its relevance to non-convex non-differentiable neural networks
&lt;/p&gt;</description></item><item><title>&#32456;&#36523;&#35760;&#24518;&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#21644;&#26816;&#32034;&#26041;&#24335;&#35775;&#38382;&#38271;&#31687;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#65292;&#21033;&#29992;&#38646;-shot&#33021;&#21147;&#36827;&#34892;&#25512;&#29702;&#65292;&#20351;&#29992;&#32622;&#20449;&#24230;&#21644;&#35299;&#37322;&#27169;&#22359;&#20135;&#29983;&#33258;&#20449;&#12289;&#39640;&#36136;&#37327;&#21644;&#21487;&#35299;&#37322;&#30340;&#31572;&#26696;&#65292;&#22312;EgoSchema&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#22312;Ego4D&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#25361;&#25112;&#20013;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;</title><link>https://arxiv.org/abs/2312.05269</link><description>&lt;p&gt;
&#32456;&#36523;&#35760;&#24518;&#65306;&#21033;&#29992;LLMs&#22238;&#31572;&#38271;&#31687;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#20013;&#30340;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05269
&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#35760;&#24518;&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#21644;&#26816;&#32034;&#26041;&#24335;&#35775;&#38382;&#38271;&#31687;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#65292;&#21033;&#29992;&#38646;-shot&#33021;&#21147;&#36827;&#34892;&#25512;&#29702;&#65292;&#20351;&#29992;&#32622;&#20449;&#24230;&#21644;&#35299;&#37322;&#27169;&#22359;&#20135;&#29983;&#33258;&#20449;&#12289;&#39640;&#36136;&#37327;&#21644;&#21487;&#35299;&#37322;&#30340;&#31572;&#26696;&#65292;&#22312;EgoSchema&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#22312;Ego4D&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#25361;&#25112;&#20013;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#32456;&#36523;&#35760;&#24518;(LifelongMemory)&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#21644;&#26816;&#32034;&#26469;&#35775;&#38382;&#38271;&#31687;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#23384;&#20648;&#12290;&#32456;&#36523;&#35760;&#24518;&#29983;&#25104;&#25668;&#20687;&#26426;&#20329;&#25140;&#32773;&#30340;&#31616;&#27905;&#35270;&#39057;&#27963;&#21160;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#33021;&#21147;&#26469;&#25512;&#29702;&#38271;&#31687;&#35270;&#39057;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#32456;&#36523;&#35760;&#24518;&#20351;&#29992;&#32622;&#20449;&#24230;&#21644;&#35299;&#37322;&#27169;&#22359;&#26469;&#20135;&#29983;&#33258;&#20449;&#12289;&#39640;&#36136;&#37327;&#21644;&#21487;&#35299;&#37322;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;EgoSchema&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;Ego4D&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;(NLQ)&#25361;&#25112;&#20013;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Agentic-Learning-AI-Lab/lifelong-memory &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05269v2 Announce Type: replace-cross  Abstract: In this paper we introduce LifelongMemory, a new framework for accessing long-form egocentric videographic memory through natural language question answering and retrieval. LifelongMemory generates concise video activity descriptions of the camera wearer and leverages the zero-shot capabilities of pretrained large language models to perform reasoning over long-form video context. Furthermore, Lifelong Memory uses a confidence and explanation module to produce confident, high-quality, and interpretable answers. Our approach achieves state-of-the-art performance on the EgoSchema benchmark for question answering and is highly competitive on the natural language query (NLQ) challenge of Ego4D. Code is available at https://github.com/Agentic-Learning-AI-Lab/lifelong-memory.
&lt;/p&gt;</description></item><item><title>EMA-Net &#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#36328;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#27169;&#22359;(CTAL)&#65292;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;&#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#36328;&#20219;&#21153;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.11124</link><description>&lt;p&gt;
EMA-Net: &#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#29992;&#20110;&#31264;&#23494;&#22330;&#26223;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
EMA-Net: Efficient Multitask Affinity Learning for Dense Scene Predictions. (arXiv:2401.11124v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11124
&lt;/p&gt;
&lt;p&gt;
EMA-Net &#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#36328;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#27169;&#22359;(CTAL)&#65292;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;&#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#36328;&#20219;&#21153;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#22240;&#20854;&#33021;&#22815;&#32852;&#21512;&#39044;&#27979;&#22810;&#20010;&#20219;&#21153;&#65292;&#22312;&#20351;&#29992;&#27604;&#21333;&#20219;&#21153;&#23398;&#20064;&#26356;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#27599;&#20010;&#20219;&#21153;&#24615;&#33021;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#20197;&#35299;&#30721;&#22120;&#20026;&#37325;&#28857;&#30340;&#26550;&#26500;&#36890;&#36807;&#20351;&#29992;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#30340;&#29305;&#24449;&#26469;&#25913;&#36827;&#22810;&#20219;&#21153;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#25913;&#36827;&#26041;&#27861;&#22312;&#20197;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#24335;&#21516;&#26102;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#20197;&#21450;&#36328;&#20219;&#21153;&#27169;&#24335;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#25928;&#22810;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#32593;&#32476;&#65288;EMA-Net&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#22810;&#20219;&#21153;&#32593;&#32476;&#30340;&#20219;&#21153;&#25913;&#36827;&#33021;&#21147;&#12290;EMA-Net&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#36328;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#65288;CTAL&#65289;&#27169;&#22359;&#24039;&#22937;&#22320;&#25429;&#25417;&#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#36328;&#20219;&#21153;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;CTAL&#30340;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#20854;&#33021;&#22815;&#20197;&#26368;&#36866;&#21512;&#20219;&#21153;&#20146;&#21644;&#30697;&#38453;&#30340;&#26041;&#24335;&#25805;&#32437;&#20219;&#21153;&#20146;&#21644;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multitask learning (MTL) has gained prominence for its ability to jointly predict multiple tasks, achieving better per-task performance while using fewer per-task model parameters than single-task learning. More recently, decoder-focused architectures have considerably improved multitask performance by refining task predictions using the features of other related tasks. However, most of these refinement methods fail to simultaneously capture local and global task-specific representations, as well as cross-task patterns in a parameter-efficient manner. In this paper, we introduce the Efficient Multitask Affinity Learning Network (EMA-Net), which is a lightweight framework that enhances the task refinement capabilities of multitask networks. EMA-Net adeptly captures local, global, and cross-task interactions using our novel Cross-Task Affinity Learning (CTAL) module. The key innovation of CTAL lies in its ability to manipulate task affinity matrices in a manner that is optimally suited t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#38382;&#31572;&#20013;&#31995;&#32479;&#19968;&#33324;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21457;&#29616;&#31616;&#21333;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#22312;&#23454;&#29616;&#31995;&#32479;&#19968;&#33324;&#21270;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36825;&#24847;&#21619;&#30528;&#19981;&#24517;&#25910;&#38598;&#22823;&#37327;&#21644;&#22810;&#26679;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.08798</link><description>&lt;p&gt;
D3: &#25968;&#25454;&#22810;&#26679;&#24615;&#35774;&#35745;&#20026;&#31995;&#32479;&#19968;&#33324;&#21270;&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
D3: Data Diversity Design for Systematic Generalization in Visual Question Answering. (arXiv:2309.08798v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#38382;&#31572;&#20013;&#31995;&#32479;&#19968;&#33324;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21457;&#29616;&#31616;&#21333;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#22312;&#23454;&#29616;&#31995;&#32479;&#19968;&#33324;&#21270;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36825;&#24847;&#21619;&#30528;&#19981;&#24517;&#25910;&#38598;&#22823;&#37327;&#21644;&#22810;&#26679;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#19968;&#33324;&#21270;&#26159;&#26234;&#33021;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#23427;&#25351;&#30340;&#26159;&#36890;&#36807;&#32467;&#21512;&#24050;&#30693;&#30340;&#23376;&#20219;&#21153;&#21644;&#27010;&#24565;&#26469;&#25512;&#24191;&#21040;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#24050;&#32463;&#26174;&#31034;&#24433;&#21709;&#31995;&#32479;&#19968;&#33324;&#21270;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#22810;&#26679;&#24615;&#21487;&#20197;&#20197;&#22810;&#31181;&#26041;&#24335;&#23450;&#20041;&#65292;&#22240;&#20026;&#25968;&#25454;&#20855;&#26377;&#35768;&#22810;&#21464;&#21270;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#26041;&#38754;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#22914;&#20309;&#24433;&#21709;&#31995;&#32479;&#19968;&#33324;&#21270;&#30340;&#26356;&#32454;&#33268;&#30340;&#29702;&#35299;&#23578;&#32570;&#20047;&#12290;&#25105;&#20204;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#38382;&#39064;&#20013;&#25552;&#20379;&#20102;&#26032;&#30340;&#35777;&#25454;&#65292;&#25581;&#31034;&#20102;&#31616;&#21333;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65288;&#21363;&#30001;&#20960;&#20010;&#23376;&#20219;&#21153;&#21644;&#27010;&#24565;&#32452;&#25104;&#30340;&#20219;&#21153;&#65289;&#22312;&#23454;&#29616;&#31995;&#32479;&#19968;&#33324;&#21270;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#36825;&#24847;&#21619;&#30528;&#25910;&#38598;&#22823;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#22797;&#26434;&#20219;&#21153;&#21487;&#33021;&#24182;&#38750;&#24517;&#35201;&#65292;&#36825;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#32467;&#26524;&#19982;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26080;&#20851;&#65292;&#24182;&#36866;&#29992;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#23478;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systematic generalization is a crucial aspect of intelligence, which refers to the ability to generalize to novel tasks by combining known subtasks and concepts. One critical factor that has been shown to influence systematic generalization is the diversity of training data. However, diversity can be defined in various ways, as data have many factors of variation. A more granular understanding of how different aspects of data diversity affect systematic generalization is lacking. We present new evidence in the problem of Visual Question Answering (VQA) that reveals that the diversity of simple tasks (i.e. tasks formed by a few subtasks and concepts) plays a key role in achieving systematic generalization. This implies that it may not be essential to gather a large and varied number of complex tasks, which could be costly to obtain. We demonstrate that this result is independent of the similarity between the training and testing data and applies to well-known families of neural network 
&lt;/p&gt;</description></item></channel></rss>