<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>MiniGPT-5&#20351;&#29992;&#29983;&#25104;&#20973;&#25454;&#20316;&#20026;&#26725;&#26753;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20132;&#38169;&#35270;&#35273;&#19982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#65292;&#24182;&#36890;&#36807;&#29420;&#29305;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#21644;&#26080;&#20998;&#31867;&#22120;&#30340;&#25351;&#23548;&#26469;&#23454;&#29616;&#26080;&#25551;&#36848;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.02239</link><description>&lt;p&gt;
MiniGPT-5: &#36890;&#36807;&#29983;&#25104;&#20973;&#25454;&#23454;&#29616;&#20132;&#38169;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens. (arXiv:2310.02239v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02239
&lt;/p&gt;
&lt;p&gt;
MiniGPT-5&#20351;&#29992;&#29983;&#25104;&#20973;&#25454;&#20316;&#20026;&#26725;&#26753;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20132;&#38169;&#35270;&#35273;&#19982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#65292;&#24182;&#36890;&#36807;&#29420;&#29305;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#21644;&#26080;&#20998;&#31867;&#22120;&#30340;&#25351;&#23548;&#26469;&#23454;&#29616;&#26080;&#25551;&#36848;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#36827;&#23637;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#26080;&#19982;&#20262;&#27604;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21516;&#26102;&#29983;&#25104;&#20855;&#26377;&#36830;&#36143;&#25991;&#26412;&#21465;&#36848;&#30340;&#22270;&#20687;&#20173;&#28982;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#21069;&#27839;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20132;&#38169;&#35270;&#35273;&#19982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#65292;&#20197;"&#29983;&#25104;&#20973;&#25454;"&#30340;&#27010;&#24565;&#20026;&#22522;&#30784;&#65292;&#20316;&#20026;&#21327;&#35843;&#22270;&#20687;&#25991;&#26412;&#36755;&#20986;&#30340;&#26725;&#26753;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29305;&#28857;&#26159;&#29420;&#29305;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#37325;&#28857;&#26159;&#26080;&#25551;&#36848;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#65292;&#35757;&#32451;&#36807;&#31243;&#19981;&#38656;&#35201;&#23545;&#22270;&#20687;&#36827;&#34892;&#20840;&#38754;&#30340;&#25551;&#36848;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26080;&#20998;&#31867;&#22120;&#30340;&#25351;&#23548;&#65292;&#22686;&#24378;&#20102;&#29983;&#25104;&#20973;&#25454;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;MiniGPT-5&#22312;MMDialog&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#22522;&#32447;Divter&#27169;&#22411;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#22987;&#32456;&#25552;&#20379;&#20248;&#36234;&#25110;&#21487;&#27604;&#30340;&#22810;&#27169;&#24577;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have garnered significant attention for their advancements in natural language processing, demonstrating unparalleled prowess in text comprehension and generation. Yet, the simultaneous generation of images with coherent textual narratives remains an evolving frontier. In response, we introduce an innovative interleaved vision-and-language generation technique anchored by the concept of "generative vokens," acting as the bridge for harmonized image-text outputs. Our approach is characterized by a distinctive two-staged training strategy focusing on description-free multimodal generation, where the training requires no comprehensive descriptions of images. To bolster model integrity, classifier-free guidance is incorporated, enhancing the effectiveness of vokens on image generation. Our model, MiniGPT-5, exhibits substantial improvement over the baseline Divter model on the MMDialog dataset and consistently delivers superior or comparable multimodal outputs 
&lt;/p&gt;</description></item></channel></rss>