<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#35299;&#24320;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20013;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#30340;&#20851;&#38190;&#28508;&#21464;&#37327;&#65292;&#36229;&#36234;&#20102;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#22312;&#35299;&#24320;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.00785</link><description>&lt;p&gt;
&#35299;&#24320;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#20043;&#35868;&#65306;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30740;&#31350;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;
&lt;/p&gt;
&lt;p&gt;
Disentangling Hippocampal Shape Variations: A Study of Neurological Disorders Using Graph Variational Autoencoder with Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#35299;&#24320;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20013;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#30340;&#20851;&#38190;&#28508;&#21464;&#37327;&#65292;&#36229;&#36234;&#20102;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#22312;&#35299;&#24320;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#32508;&#21512;&#30740;&#31350;&#65292;&#19987;&#27880;&#20110;&#22312;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#32972;&#26223;&#19979;&#20174;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#65288;DTI&#65289;&#25968;&#25454;&#38598;&#20013;&#35299;&#24320;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#12290;&#20511;&#21161;&#22686;&#24378;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#21306;&#20998;&#20195;&#34920;&#24180;&#40836;&#21644;&#26159;&#21542;&#24739;&#30149;&#30340;&#20004;&#20010;&#19981;&#21516;&#28508;&#21464;&#37327;&#26469;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31995;&#21015;VAE&#26550;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22686;&#24378;&#30340;&#35299;&#24320;&#33021;&#21147;&#12290;&#36825;&#20010;&#35780;&#20272;&#20351;&#29992;&#20102;&#26469;&#33258;DTI&#28023;&#39532;&#25968;&#25454;&#38598;&#30340;&#21512;&#25104;3D&#29615;&#24418;&#32593;&#26684;&#25968;&#25454;&#21644;&#30495;&#23454;&#30340;3D&#28023;&#39532;&#32593;&#26684;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30417;&#30563;&#35299;&#24320;&#27169;&#22411;&#22312;&#35299;&#24320;&#20998;&#25968;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22914;&#23646;&#24615;&#21644;&#24341;&#23548;VAE&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#21306;&#20998;&#19981;&#21516;&#24180;&#40836;&#32452;&#21644;&#30142;&#30149;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00785v1 Announce Type: cross  Abstract: This paper presents a comprehensive study focused on disentangling hippocampal shape variations from diffusion tensor imaging (DTI) datasets within the context of neurological disorders. Leveraging a Graph Variational Autoencoder (VAE) enhanced with Supervised Contrastive Learning, our approach aims to improve interpretability by disentangling two distinct latent variables corresponding to age and the presence of diseases. In our ablation study, we investigate a range of VAE architectures and contrastive loss functions, showcasing the enhanced disentanglement capabilities of our approach. This evaluation uses synthetic 3D torus mesh data and real 3D hippocampal mesh datasets derived from the DTI hippocampal dataset. Our supervised disentanglement model outperforms several state-of-the-art (SOTA) methods like attribute and guided VAEs in terms of disentanglement scores. Our model distinguishes between age groups and disease status in pa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;&#29992;&#20110;&#31435;&#20307;&#35270;&#35273;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#31435;&#20307;&#35270;&#35273;&#25552;&#20379;&#30340;&#39069;&#22806;&#35270;&#35282;&#21644;&#30452;&#25509;&#25512;&#27979;&#29289;&#20307;&#30340;&#36317;&#31163;&#65292;&#35813;&#26041;&#27861;&#22312;6D&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#19988;&#21487;&#36866;&#29992;&#20110;&#20854;&#20182;&#22522;&#20110;&#23494;&#38598;&#29305;&#24449;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05610</link><description>&lt;p&gt;
&#25193;&#23637;&#29992;&#20110;&#31435;&#20307;&#35270;&#35273;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Extending 6D Object Pose Estimators for Stereo Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05610
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;&#29992;&#20110;&#31435;&#20307;&#35270;&#35273;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#31435;&#20307;&#35270;&#35273;&#25552;&#20379;&#30340;&#39069;&#22806;&#35270;&#35282;&#21644;&#30452;&#25509;&#25512;&#27979;&#29289;&#20307;&#30340;&#36317;&#31163;&#65292;&#35813;&#26041;&#27861;&#22312;6D&#23039;&#24577;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#19988;&#21487;&#36866;&#29992;&#20110;&#20854;&#20182;&#22522;&#20110;&#23494;&#38598;&#29305;&#24449;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#12289;&#24555;&#36895;&#12289;&#31283;&#20581;&#22320;&#20272;&#35745;&#29289;&#20307;&#30340;6D&#23039;&#24577;&#20173;&#28982;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30452;&#25509;&#20174;RGB&#22270;&#20687;&#20013;&#20351;&#29992;&#23494;&#38598;&#29305;&#24449;&#22238;&#24402;&#23039;&#24577;&#30340;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#31435;&#20307;&#35270;&#35273;&#25552;&#20379;&#20102;&#23545;&#29289;&#20307;&#30340;&#39069;&#22806;&#35270;&#35282;&#65292;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#23039;&#24577;&#27495;&#20041;&#21644;&#36974;&#25377;&#12290;&#27492;&#22806;&#65292;&#31435;&#20307;&#22270;&#20687;&#21487;&#20197;&#30452;&#25509;&#25512;&#27979;&#29289;&#20307;&#30340;&#36317;&#31163;&#65292;&#32780;&#21333;&#30446;&#35270;&#35273;&#21017;&#38656;&#35201;&#20869;&#32622;&#23545;&#35937;&#23610;&#23544;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#23558;&#26368;&#20808;&#36827;&#30340;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#25193;&#23637;&#21040;&#31435;&#20307;&#35270;&#35273;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#19982;BOP&#20860;&#23481;&#30340;YCB-V&#25968;&#25454;&#38598;&#30340;&#31435;&#20307;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#31435;&#20307;&#35270;&#35273;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;6D&#23039;&#24577;&#20272;&#35745;&#31639;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#20854;&#20182;&#22522;&#20110;&#23494;&#38598;&#29305;&#24449;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the 6D pose of objects accurately, quickly, and robustly remains a difficult task. However, recent methods for directly regressing poses from RGB images using dense features have achieved state-of-the-art results. Stereo vision, which provides an additional perspective on the object, can help reduce pose ambiguity and occlusion. Moreover, stereo can directly infer the distance of an object, while mono-vision requires internalized knowledge of the object's size. To extend the state-of-the-art in 6D object pose estimation to stereo, we created a BOP compatible stereo version of the YCB-V dataset. Our method outperforms state-of-the-art 6D pose estimation algorithms by utilizing stereo vision and can easily be adopted for other dense feature-based algorithms.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#20559;&#24046;&#20851;&#31995;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36965;&#24863;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#36328;&#27169;&#24577;&#21644;&#22330;&#26223;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.07511</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#24046;&#20851;&#31995;&#23398;&#20064;&#23454;&#29616;&#36328;&#27169;&#24577;&#21644;&#22330;&#26223;&#30340;&#32479;&#19968;&#36965;&#24863;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Unified Remote Sensing Anomaly Detector Across Modalities and Scenes via Deviation Relationship Learning. (arXiv:2310.07511v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07511
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20559;&#24046;&#20851;&#31995;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36965;&#24863;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#36328;&#27169;&#24577;&#21644;&#22330;&#26223;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#24322;&#24120;&#26816;&#27979;&#22120;&#21487;&#20197;&#25214;&#21040;&#19982;&#32972;&#26223;&#19981;&#31526;&#30340;&#30446;&#26631;&#20316;&#20026;&#28508;&#22312;&#30446;&#26631;&#12290;&#37492;&#20110;&#22320;&#29699;&#24322;&#24120;&#31867;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#36328;&#27169;&#24577;&#21644;&#22330;&#26223;&#30340;&#32479;&#19968;&#24322;&#24120;&#26816;&#27979;&#22120;&#24212;&#35813;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#65292;&#24182;&#19988;&#23545;&#20110;&#26032;&#30340;&#22320;&#29699;&#35266;&#27979;&#28304;&#21644;&#24322;&#24120;&#31867;&#22411;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#20165;&#38480;&#20110;&#21333;&#19968;&#27169;&#24577;&#21644;&#21333;&#19968;&#22330;&#26223;&#65292;&#22240;&#20026;&#23427;&#20204;&#26088;&#22312;&#23398;&#20064;&#19981;&#26029;&#21464;&#21270;&#30340;&#32972;&#26223;&#20998;&#24067;&#12290;&#22312;&#26222;&#36941;&#30340;&#24322;&#24120;&#20559;&#24046;&#27169;&#24335;&#30340;&#28608;&#21457;&#19979;&#65292;&#21363;&#24322;&#24120;&#23637;&#29616;&#20986;&#19982;&#20854;&#23616;&#37096;&#29615;&#22659;&#30340;&#20559;&#24046;&#29305;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#29305;&#24449;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#22522;&#20110;&#20559;&#24046;&#20851;&#31995;&#30340;&#26080;&#21521;&#21452;&#23618;&#22270;&#65292;&#20854;&#20013;&#24322;&#24120;&#35780;&#20998;&#34987;&#24314;&#27169;&#20026;&#22312;&#32972;&#26223;&#21644;&#27491;&#24120;&#23545;&#35937;&#30340;&#27169;&#24335;&#32473;&#23450;&#19979;&#30340;&#26465;&#20214;&#27010;&#29575;&#12290;&#28982;&#21518;&#65292;&#23398;&#20064;&#30446;&#26631;&#34987;&#34920;&#31034;&#20026;&#19968;&#20010;&#26465;&#20214;&#27010;&#29575;&#25490;&#24207;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23454;&#20363;&#21270;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remote sensing anomaly detector can find the objects deviating from the background as potential targets. Given the diversity in earth anomaly types, a unified anomaly detector across modalities and scenes should be cost-effective and flexible to new earth observation sources and anomaly types. However, the current anomaly detectors are limited to a single modality and single scene, since they aim to learn the varying background distribution. Motivated by the universal anomaly deviation pattern, in that anomalies exhibit deviations from their local context, we exploit this characteristic to build a unified anomaly detector. Firstly, we reformulate the anomaly detection task as an undirected bilayer graph based on the deviation relationship, where the anomaly score is modeled as the conditional probability, given the pattern of the background and normal objects. The learning objective is then expressed as a conditional probability ranking problem. Furthermore, we design an instantiation 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23454;&#29616;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#27169;&#24577;&#25193;&#23637;&#65292;&#24182;&#22312;&#33021;&#28304;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06488</link><description>&lt;p&gt;
SpikeCLIP&#65306;&#19968;&#31181;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network. (arXiv:2310.06488v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23454;&#29616;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#27169;&#24577;&#25193;&#23637;&#65292;&#24182;&#22312;&#33021;&#28304;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#24050;&#32463;&#35777;&#26126;&#20854;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#20013;&#33021;&#22815;&#23454;&#29616;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#33021;&#25928;&#25552;&#39640;&#21644;&#31526;&#21512;&#29983;&#29289;&#21512;&#29702;&#24615;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#21333;&#27169;&#24577;&#30340;SNNs&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30340;&#24773;&#26223;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#21463;&#21040;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#8220;&#23545;&#40784;&#39044;&#35757;&#32451;+&#21452;&#25439;&#22833;&#24494;&#35843;&#8221;&#30340;&#20004;&#27493;&#39588;&#37197;&#26041;&#65292;&#26469;&#35299;&#20915;&#33033;&#20914;&#35745;&#31639;&#32972;&#26223;&#19979;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#24120;&#29992;&#30340;&#29992;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#35780;&#20272;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;SNNs&#21462;&#24471;&#20102;&#19982;&#20854;DNNs&#23545;&#24212;&#29289;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#33021;&#28304;&#28040;&#32791;&#12290;&#27492;&#22806;&#65292;SpikeCLIP&#22312;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#20445;&#25345;&#20102;&#31283;&#23450;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) have demonstrated the capability to achieve comparable performance to deep neural networks (DNNs) in both visual and linguistic domains while offering the advantages of improved energy efficiency and adherence to biological plausibility. However, the extension of such single-modality SNNs into the realm of multimodal scenarios remains an unexplored territory. Drawing inspiration from the concept of contrastive language-image pre-training (CLIP), we introduce a novel framework, named SpikeCLIP, to address the gap between two modalities within the context of spike-based computing through a two-step recipe involving ``Alignment Pre-training + Dual-Loss Fine-tuning". Extensive experiments demonstrate that SNNs achieve comparable results to their DNN counterparts while significantly reducing energy consumption across a variety of datasets commonly used for multimodal model evaluation. Furthermore, SpikeCLIP maintains robust performance in image classification 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21453;&#39304;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#39304;&#26469;&#39537;&#21160;&#37319;&#26679;&#65292;&#23558;&#38745;&#24577;&#25968;&#25454;&#38598;&#22686;&#24378;&#20026;&#21253;&#21547;&#26377;&#29992;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00158</link><description>&lt;p&gt;
&#19981;&#24179;&#34913;&#20998;&#31867;&#20013;&#30340;&#21453;&#39304;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Feedback-guided Data Synthesis for Imbalanced Classification. (arXiv:2310.00158v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21453;&#39304;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#39304;&#26469;&#39537;&#21160;&#37319;&#26679;&#65292;&#23558;&#38745;&#24577;&#25968;&#25454;&#38598;&#22686;&#24378;&#20026;&#21253;&#21547;&#26377;&#29992;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29616;&#29366;&#26159;&#20351;&#29992;&#26469;&#33258;&#38271;&#23614;&#20998;&#24067;&#30340;&#30495;&#23454;&#22270;&#20687;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#36817;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#29992;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#36825;&#20123;&#38745;&#24577;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#25253;&#21578;&#20102;&#36866;&#24230;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20123;&#24615;&#33021;&#25552;&#21319;&#21463;&#21040;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#39304;&#19981;&#36275;&#30340;&#38480;&#21046;&#65292;&#36825;&#23558;&#20419;&#36827;&#29983;&#25104;&#26679;&#26412;&#30340;&#26377;&#29992;&#24615;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#26377;&#29992;&#30340;&#21512;&#25104;&#26679;&#26412;&#22686;&#24378;&#38745;&#24577;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#27425;&#24615;&#21453;&#39304;&#26469;&#39537;&#21160;&#37319;&#26679;&#12290;&#20026;&#20102;&#20351;&#35813;&#26694;&#26550;&#26377;&#25928;&#65292;&#25105;&#20204;&#21457;&#29616;&#26679;&#26412;&#24517;&#39035;&#25509;&#36817;&#25163;&#22836;&#20219;&#21153;&#30340;&#30495;&#23454;&#25968;&#25454;&#25903;&#25345;&#65292;&#24182;&#19988;&#20855;&#26377;&#36275;&#22815;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38271;&#23614;&#25968;&#25454;&#38598;&#65288;ImageNe...&#19978;&#39564;&#35777;&#20102;&#19977;&#20010;&#21453;&#39304;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current status quo in machine learning is to use static datasets of real images for training, which often come from long-tailed distributions. With the recent advances in generative models, researchers have started augmenting these static datasets with synthetic data, reporting moderate performance improvements on classification tasks. We hypothesize that these performance gains are limited by the lack of feedback from the classifier to the generative model, which would promote the usefulness of the generated samples to improve the classifier's performance. In this work, we introduce a framework for augmenting static datasets with useful synthetic samples, which leverages one-shot feedback from the classifier to drive the sampling of the generative model. In order for the framework to be effective, we find that the samples must be close to the support of the real data of the task at hand, and be sufficiently diverse. We validate three feedback criteria on a long-tailed dataset (ImageNe
&lt;/p&gt;</description></item></channel></rss>