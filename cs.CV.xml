<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>LaRE^2 &#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#65288;LaRE&#65289;&#21644;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#27169;&#22359;&#65288;EGRE&#65289;&#23454;&#29616;&#20102;&#23545;&#29305;&#24449;&#30340;&#26377;&#25928;&#25552;&#21462;&#21644;&#22686;&#24378;&#65292;&#20174;&#32780;&#21306;&#20998;&#30495;&#23454;&#21644;&#29983;&#25104;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.17465</link><description>&lt;p&gt;
LaRE^2: &#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#30340;&#25193;&#25955;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17465
&lt;/p&gt;
&lt;p&gt;
LaRE^2 &#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#65288;LaRE&#65289;&#21644;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#27169;&#22359;&#65288;EGRE&#65289;&#23454;&#29616;&#20102;&#23545;&#29305;&#24449;&#30340;&#26377;&#25928;&#25552;&#21462;&#21644;&#22686;&#24378;&#65292;&#20174;&#32780;&#21306;&#20998;&#30495;&#23454;&#21644;&#29983;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17465v1 &#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#20351;&#30495;&#23454;&#22270;&#20687;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30340;&#21306;&#20998;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#23613;&#31649;&#36825;&#19968;&#36827;&#23637;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#26041;&#27861;&#65288;LaRE^2&#65289;&#26469;&#26816;&#27979;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#65288;LaRE&#65289;&#65292;&#20316;&#20026;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#30340;&#31532;&#19968;&#20010;&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#30340;&#29305;&#24449;&#12290;LaRE&#22312;&#29305;&#24449;&#25552;&#21462;&#25928;&#29575;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21306;&#20998;&#30495;&#20551;&#25152;&#38656;&#30340;&#20851;&#38190;&#32447;&#32034;&#12290;&#20026;&#20102;&#21033;&#29992;LaRE&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#27169;&#22359;&#65288;EGRE&#65289;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;LaRE&#24341;&#23548;&#30340;&#26041;&#24335;&#32454;&#21270;&#22270;&#20687;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17465v1 Announce Type: cross  Abstract: The evolution of Diffusion Models has dramatically improved image generation quality, making it increasingly difficult to differentiate between real and generated images. This development, while impressive, also raises significant privacy and security concerns. In response to this, we propose a novel Latent REconstruction error guided feature REfinement method (LaRE^2) for detecting the diffusion-generated images. We come up with the Latent Reconstruction Error (LaRE), the first reconstruction-error based feature in the latent space for generated image detection. LaRE surpasses existing methods in terms of feature extraction efficiency while preserving crucial cues required to differentiate between the real and the fake. To exploit LaRE, we propose an Error-Guided feature REfinement module (EGRE), which can refine the image feature guided by LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an align-then-refine m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21387;&#32553;&#38142;&#8221;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#21270;&#12289;&#21098;&#26525;&#12289;&#25552;&#21069;&#36864;&#20986;&#21644;&#30693;&#35782;&#33976;&#39311;&#31561;&#24120;&#35265;&#25216;&#26415;&#65292;&#23454;&#29616;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21387;&#32553;&#12290;</title><link>https://arxiv.org/abs/2403.17447</link><description>&lt;p&gt;
&#21387;&#32553;&#38142;&#65306;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#32452;&#21512;&#21387;&#32553;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chain of Compression: A Systematic Approach to Combinationally Compress Convolutional Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17447
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21387;&#32553;&#38142;&#8221;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#21270;&#12289;&#21098;&#26525;&#12289;&#25552;&#21069;&#36864;&#20986;&#21644;&#30693;&#35782;&#33976;&#39311;&#31561;&#24120;&#35265;&#25216;&#26415;&#65292;&#23454;&#29616;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#27969;&#34892;&#65292;&#20294;&#23427;&#20204;&#22312;&#35745;&#31639;&#21644;&#23384;&#20648;&#26041;&#38754;&#30340;&#23494;&#38598;&#24615;&#32473;&#36164;&#28304;&#26377;&#38480;&#30340;&#35745;&#31639;&#31995;&#32479;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#38656;&#35201;&#23454;&#26102;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#20943;&#36731;&#36127;&#25285;&#65292;&#27169;&#22411;&#21387;&#32553;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;&#35768;&#22810;&#26041;&#27861;&#65292;&#22914;&#37327;&#21270;&#12289;&#21098;&#26525;&#12289;&#25552;&#21069;&#36864;&#20986;&#21644;&#30693;&#35782;&#33976;&#39311;&#24050;&#32463;&#35777;&#26126;&#20102;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#20013;&#20887;&#20313;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65292;&#21487;&#20197;&#26126;&#26174;&#30475;&#20986;&#65292;&#27599;&#31181;&#26041;&#27861;&#37117;&#21033;&#29992;&#20102;&#20854;&#29420;&#29305;&#30340;&#29305;&#24615;&#26469;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#19988;&#24403;&#23427;&#20204;&#32467;&#21512;&#22312;&#19968;&#36215;&#26102;&#20063;&#21487;&#20197;&#23637;&#29616;&#20986;&#20114;&#34917;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#25506;&#31350;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#20174;&#20114;&#34917;&#29305;&#24615;&#20013;&#33719;&#30410;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21387;&#32553;&#38142;&#65292;&#23427;&#22312;&#32452;&#21512;&#24207;&#21015;&#19978;&#25805;&#20316;&#65292;&#24212;&#29992;&#36825;&#20123;&#24120;&#35265;&#25216;&#26415;&#26469;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17447v1 Announce Type: new  Abstract: Convolutional neural networks (CNNs) have achieved significant popularity, but their computational and memory intensity poses challenges for resource-constrained computing systems, particularly with the prerequisite of real-time performance. To release this burden, model compression has become an important research focus. Many approaches like quantization, pruning, early exit, and knowledge distillation have demonstrated the effect of reducing redundancy in neural networks. Upon closer examination, it becomes apparent that each approach capitalizes on its unique features to compress the neural network, and they can also exhibit complementary behavior when combined. To explore the interactions and reap the benefits from the complementary features, we propose the Chain of Compression, which works on the combinational sequence to apply these common techniques to compress the neural network. Validated on the image-based regression and classi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;MatchSeg&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#21644;&#32852;&#21512;&#27880;&#24847;&#21147;&#27169;&#22359;&#22686;&#24378;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#26377;&#25928;&#23454;&#29616;&#20102;&#25903;&#25345;&#38598;&#21644;&#26597;&#35810;&#38598;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.15901</link><description>&lt;p&gt;
&#36890;&#36807;&#21442;&#32771;&#22270;&#20687;&#21305;&#37197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20998;&#21106;&#65306;MatchSeg
&lt;/p&gt;
&lt;p&gt;
MatchSeg: Towards Better Segmentation via Reference Image Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15901
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;MatchSeg&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#21644;&#32852;&#21512;&#27880;&#24847;&#21147;&#27169;&#22359;&#22686;&#24378;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#26377;&#25928;&#23454;&#29616;&#20102;&#25903;&#25345;&#38598;&#21644;&#26597;&#35810;&#38598;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#32780;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#39640;&#26114;&#19988;&#32791;&#26102;&#12290;Few-shot learning&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;&#25903;&#25345;&#38598;&#65289;&#26469;&#25351;&#23548;&#39044;&#27979;&#26032;&#30340;&#12289;&#26410;&#26631;&#35760;&#22270;&#20687;&#65288;&#31216;&#20026;&#26597;&#35810;&#38598;&#65289;&#30340;&#26631;&#31614;&#65292;&#20174;&#32780;&#20811;&#26381;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#21463;&#21040;&#36825;&#19968;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MatchSeg&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#25112;&#30053;&#24615;&#21442;&#32771;&#22270;&#20687;&#21305;&#37197;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#22312;&#23450;&#20041;&#25903;&#25345;&#38598;&#26102;&#36873;&#25321;&#39640;&#24230;&#30456;&#20851;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#32852;&#21512;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#21152;&#24378;&#25903;&#25345;&#21644;&#26597;&#35810;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20419;&#36827;&#26356;&#26377;&#25928;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15901v1 Announce Type: new  Abstract: Recently, automated medical image segmentation methods based on deep learning have achieved great success. However, they heavily rely on large annotated datasets, which are costly and time-consuming to acquire. Few-shot learning aims to overcome the need for annotated data by using a small labeled dataset, known as a support set, to guide predicting labels for new, unlabeled images, known as the query set. Inspired by this paradigm, we introduce MatchSeg, a novel framework that enhances medical image segmentation through strategic reference image matching. We leverage contrastive language-image pre-training (CLIP) to select highly relevant samples when defining the support set. Additionally, we design a joint attention module to strengthen the interaction between support and query features, facilitating a more effective knowledge transfer between support and query sets. We validated our method across four public datasets. Experimental re
&lt;/p&gt;</description></item><item><title>MathVerse&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14624</link><description>&lt;p&gt;
MathVerse&#65306;&#24744;&#30340;&#22810;&#27169;&#24335;LLM&#26159;&#21542;&#30495;&#27491;&#30475;&#21040;&#20102;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#22270;&#34920;&#65311;
&lt;/p&gt;
&lt;p&gt;
MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14624
&lt;/p&gt;
&lt;p&gt;
MathVerse&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22312;&#35270;&#35273;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#26410;&#20805;&#20998;&#35780;&#20272;&#21644;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#65292;&#23558;&#36807;&#22810;&#30340;&#35270;&#35273;&#20869;&#23481;&#34701;&#20837;&#25991;&#26412;&#38382;&#39064;&#20013;&#65292;&#36825;&#26377;&#21161;&#20110;MLLM&#22312;&#19981;&#30495;&#27491;&#35299;&#37322;&#36755;&#20837;&#22270;&#34920;&#30340;&#24773;&#20917;&#19979;&#25512;&#23548;&#31572;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MathVerse&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#12290;&#25105;&#20204;&#31934;&#24515;&#25910;&#38598;&#20102;2,612&#20010;&#39640;&#36136;&#37327;&#30340;&#22810;&#23398;&#31185;&#25968;&#23398;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#22270;&#34920;&#65292;&#26469;&#28304;&#20110;&#20844;&#24320;&#28192;&#36947;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#38382;&#39064;&#30001;&#20154;&#24037;&#27880;&#37322;&#32773;&#36716;&#21270;&#20026;&#20845;&#20010;&#19981;&#21516;&#29256;&#26412;&#65292;&#27599;&#20010;&#29256;&#26412;&#22312;&#22810;&#27169;&#24335;&#20013;&#25552;&#20379;&#19981;&#21516;&#31243;&#24230;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#20849;&#36129;&#29486;&#20102;15K&#20010;&#27979;&#35797;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;MathVerse&#33021;&#22815;&#21516;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14624v1 Announce Type: cross  Abstract: The remarkable progress of Multi-modal Large Language Models (MLLMs) has garnered unparalleled attention, due to their superior performance in visual contexts. However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood. We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams. To this end, we introduce MathVerse, an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs. We meticulously collect 2,612 high-quality, multi-subject math problems with diagrams from publicly available sources. Each problem is then transformed by human annotators into six distinct versions, each offering varying degrees of information content in multi-modality, contributing to 15K test samples in total. This approach allows MathVerse to co
&lt;/p&gt;</description></item><item><title>&#24352;&#37327;&#21270;&#26159;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21367;&#31215;&#26680;&#26367;&#25442;&#20026;&#32039;&#20945;&#20998;&#35299;&#65292;&#24182;&#30452;&#25509;&#35757;&#32451;&#20998;&#35299;&#22240;&#23376;&#20197;&#20559;&#21521;&#20110;&#20302;&#31209;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#24352;&#37327;&#21270;&#22914;&#20309;&#36890;&#36807;&#35780;&#20272;&#25130;&#26029;&#21367;&#31215;&#26680;&#26469;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14379</link><description>&lt;p&gt;
&#21367;&#31215;&#27169;&#22411;&#30340;&#24352;&#37327;&#32593;&#32476;&#21487;&#21387;&#32553;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tensor network compressibility of convolutional models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14379
&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#21270;&#26159;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21367;&#31215;&#26680;&#26367;&#25442;&#20026;&#32039;&#20945;&#20998;&#35299;&#65292;&#24182;&#30452;&#25509;&#35757;&#32451;&#20998;&#35299;&#22240;&#23376;&#20197;&#20559;&#21521;&#20110;&#20302;&#31209;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#24352;&#37327;&#21270;&#22914;&#20309;&#36890;&#36807;&#35780;&#20272;&#25130;&#26029;&#21367;&#31215;&#26680;&#26469;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#20195;&#34920;&#20102;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20043;&#19968;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#19968;&#33324;&#24773;&#20917;&#19979;&#26356;&#22823;&#30340;CNNs&#36890;&#24120;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#36890;&#36807;&#8220;&#24352;&#37327;&#21270;&#8221;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23567;&#23427;&#20204;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#24352;&#37327;&#21270;&#21253;&#25324;&#23558;&#21367;&#31215;&#26680;&#26367;&#25442;&#20026;&#22914;Tucker&#12289;Canonical Polyadic&#20998;&#35299;&#25110;&#21463;&#37327;&#23376;&#21551;&#21457;&#30340;&#20998;&#35299;&#65288;&#22914;&#30697;&#38453;&#20056;&#31215;&#29366;&#24577;&#65289;&#31561;&#32039;&#20945;&#30340;&#20998;&#35299;&#65292;&#24182;&#30452;&#25509;&#35757;&#32451;&#20998;&#35299;&#20013;&#30340;&#22240;&#23376;&#65292;&#20197;&#20559;&#21521;&#20110;&#20302;&#31209;&#20998;&#35299;&#12290;&#20294;&#20026;&#20160;&#20040;&#24352;&#37327;&#21270;&#20284;&#20046;&#23545;&#20934;&#30830;&#24615;&#27809;&#26377;&#19981;&#21033;&#24433;&#21709;&#65311;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#25130;&#26029;&#23494;&#38598;&#65288;&#38750;&#24352;&#37327;&#21270;&#65289;CNNs&#30340;&#21367;&#31215;&#26680;&#23545;&#20854;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#26469;&#25506;&#35752;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14379v1 Announce Type: cross  Abstract: Convolutional neural networks (CNNs) represent one of the most widely used neural network architectures, showcasing state-of-the-art performance in computer vision tasks. Although larger CNNs generally exhibit higher accuracy, their size can be effectively reduced by "tensorization" while maintaining accuracy. Tensorization consists of replacing the convolution kernels with compact decompositions such as Tucker, Canonical Polyadic decompositions, or quantum-inspired decompositions such as matrix product states, and directly training the factors in the decompositions to bias the learning towards low-rank decompositions. But why doesn't tensorization seem to impact the accuracy adversely? We explore this by assessing how truncating the convolution kernels of dense (untensorized) CNNs impact their accuracy. Specifically, we truncated the kernels of (i) a vanilla four-layer CNN and (ii) ResNet-50 pre-trained for image classification on CIF
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#28304;&#25968;&#25454;&#38598;&#30340;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#25351;&#20986;ImageNet&#39044;&#35757;&#32451;&#27169;&#22411;&#26356;&#23481;&#26131;&#36807;&#25311;&#21512;&#28151;&#26434;&#22240;&#32032;&#65292;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#37325;&#26032;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04484</link><description>&lt;p&gt;
&#26469;&#28304;&#33267;&#20851;&#37325;&#35201;&#65306;&#21307;&#23398;&#25104;&#20687;&#20013;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Source Matters: Source Dataset Impact on Model Robustness in Medical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04484
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#28304;&#25968;&#25454;&#38598;&#30340;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#25351;&#20986;ImageNet&#39044;&#35757;&#32451;&#27169;&#22411;&#26356;&#23481;&#26131;&#36807;&#25311;&#21512;&#28151;&#26434;&#22240;&#32032;&#65292;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#37325;&#26032;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#24050;&#25104;&#20026;&#21307;&#23398;&#25104;&#20687;&#20998;&#31867;&#31639;&#27861;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#36890;&#24120;&#21033;&#29992;ImageNet&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#20174;&#33258;&#28982;&#21040;&#21307;&#23398;&#22270;&#20687;&#30340;&#39046;&#22495;&#36716;&#21464;&#20419;&#20351;&#20102;&#35832;&#22914;RadImageNet &#31561;&#26367;&#20195;&#26041;&#26696;&#30340;&#20986;&#29616;&#65292;&#24448;&#24448;&#23637;&#31034;&#20986;&#21487;&#27604;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36801;&#31227;&#23398;&#20064;&#20013;&#24615;&#33021;&#25552;&#21319;&#26159;&#26469;&#33258;&#20110;&#25913;&#21892;&#30340;&#27867;&#21270;&#36824;&#26159;&#24555;&#25463;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#20844;&#24320;&#30340;&#33016;&#37096;X&#20809;&#29255;&#21644;CT&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#28508;&#22312;&#28151;&#26434;&#22240;&#32032;--&#26080;&#35770;&#26159;&#21512;&#25104;&#30340;&#36824;&#26159;&#20174;&#25968;&#25454;&#20013;&#25277;&#21462;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;ImageNet &#21644; RadImageNet &#23454;&#29616;&#20102;&#21487;&#27604;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#28982;&#32780; ImageNet &#26356;&#23481;&#26131;&#36807;&#25311;&#21512;&#28151;&#26434;&#22240;&#32032;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;ImageNet&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#24320;&#23637;&#31867;&#20284;&#23454;&#39564;&#26469;&#37325;&#26032;&#23457;&#35270;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#23454;&#39564;&#21487;&#22312;https://github.com/DovileDo/source-mat &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04484v1 Announce Type: cross  Abstract: Transfer learning has become an essential part of medical imaging classification algorithms, often leveraging ImageNet weights. However, the domain shift from natural to medical images has prompted alternatives such as RadImageNet, often demonstrating comparable classification performance. However, it remains unclear whether the performance gains from transfer learning stem from improved generalization or shortcut learning. To address this, we investigate potential confounders -- whether synthetic or sampled from the data -- across two publicly available chest X-ray and CT datasets. We show that ImageNet and RadImageNet achieve comparable classification performance, yet ImageNet is much more prone to overfitting to confounders. We recommend that researchers using ImageNet-pretrained models reexamine their model robustness by conducting similar experiments. Our code and experiments are available at https://github.com/DovileDo/source-mat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#29992;&#20110;SAR&#22270;&#20687;&#20013;&#33337;&#21482;&#26816;&#27979;&#30340;Wilcoxon&#38750;&#21442;&#25968;CFAR&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#24050;&#30693;&#26434;&#27874;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#32500;&#25345;&#30446;&#26631;&#26816;&#27979;&#30340;&#24658;&#23450;&#34394;&#35686;&#29575;</title><link>https://arxiv.org/abs/2402.18579</link><description>&lt;p&gt;
SAR&#22270;&#20687;&#20013;&#29992;&#20110;&#33337;&#21482;&#26816;&#27979;&#30340;Wilcoxon&#38750;&#21442;&#25968;&#21270;CFAR&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Wilcoxon Nonparametric CFAR Scheme for Ship Detection in SAR Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18579
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#29992;&#20110;SAR&#22270;&#20687;&#20013;&#33337;&#21482;&#26816;&#27979;&#30340;Wilcoxon&#38750;&#21442;&#25968;CFAR&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#24050;&#30693;&#26434;&#27874;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#32500;&#25345;&#30446;&#26631;&#26816;&#27979;&#30340;&#24658;&#23450;&#34394;&#35686;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#25968;&#34394;&#35686;&#29575;&#65288;CFAR&#65289;&#26816;&#27979;&#31639;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#30446;&#21069;SAR&#22270;&#20687;&#20013;&#26816;&#27979;&#33337;&#21482;&#30446;&#26631;&#65292;&#36825;&#20123;&#31639;&#27861;&#22522;&#20110;&#21508;&#31181;&#32479;&#35745;&#20998;&#24067;&#65292;&#22914;&#39640;&#26031;&#20998;&#24067;&#12289;Gamma&#20998;&#24067;&#12289;Weibull&#20998;&#24067;&#12289;&#23545;&#25968;&#27491;&#24577;&#20998;&#24067;&#12289;G0&#20998;&#24067;&#12289;alpha&#31283;&#23450;&#20998;&#24067;&#31561;&#12290;&#28982;&#32780;&#65292;SAR&#22270;&#20687;&#20013;&#30340;&#26434;&#25955;&#32972;&#26223;&#22797;&#26434;&#22810;&#21464;&#12290;&#24403;&#23454;&#38469;&#26434;&#25955;&#32972;&#26223;&#20559;&#31163;&#20551;&#23450;&#30340;&#32479;&#35745;&#20998;&#24067;&#26102;&#65292;&#21442;&#25968;&#21270;CFAR&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#23558;&#19979;&#38477;&#12290;&#38500;&#20102;&#21442;&#25968;&#21270;CFAR&#26041;&#26696;&#65292;&#36824;&#26377;&#21478;&#19968;&#31867;&#38750;&#21442;&#25968;&#21270;CFAR&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#24050;&#30693;&#26434;&#27874;&#20998;&#24067;&#30340;&#20551;&#35774;&#24773;&#20917;&#19979;&#20445;&#25345;&#30446;&#26631;&#26816;&#27979;&#30340;&#24658;&#23450;&#34394;&#35686;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#29992;&#20110;SAR&#22270;&#20687;&#20013;&#33337;&#21482;&#26816;&#27979;&#30340;Wilcoxon&#38750;&#21442;&#25968;&#21270;CFAR&#26041;&#26696;&#65292;&#24182;&#25512;&#23548;&#20102;Wilcoxon&#38750;&#21442;&#25968;&#26816;&#27979;&#22120;&#30340;&#34394;&#35686;&#29575;&#30340;&#23553;&#38381;&#24418;&#24335;&#20197;&#30830;&#23450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18579v1 Announce Type: cross  Abstract: The parametric constant false alarm rate (CFAR) detection algorithms which are based on various statistical distributions, such as Gaussian, Gamma, Weibull, log-normal, G0 distribution, alpha-stable distribution, etc, are most widely used to detect the ship targets in SAR image at present. However, the clutter background in SAR images is complicated and variable. When the actual clutter background deviates from the assumed statistical distribution, the performance of the parametric CFAR detector will deteriorate. In addition to the parametric CFAR schemes, there is another class of nonparametric CFAR detectors which can maintain a constant false alarm rate for the target detection without the assumption of a known clutter distribution. In this work, the Wilcoxon nonparametric CFAR scheme for ship detection in SAR image is proposed and analyzed, and a closed form of the false alarm rate for the Wilcoxon nonparametric detector to determi
&lt;/p&gt;</description></item><item><title>WeakSAM&#36890;&#36807;&#21033;&#29992;&#39044;&#20808;&#23398;&#20064;&#30340;&#20840;&#29699;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#24369;&#30417;&#30563;&#23545;&#35937;&#26816;&#27979;&#21644;&#20998;&#21106;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;PGT&#29983;&#25104;&#21644;RoI&#20002;&#24323;&#27491;&#21017;&#21270;&#65292;&#26174;&#33879;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.14812</link><description>&lt;p&gt;
WeakSAM: &#20219;&#24847;&#20998;&#21106;&#36935;&#19978;&#24369;&#30417;&#30563;&#23454;&#20363;&#32423;&#21035;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14812
&lt;/p&gt;
&lt;p&gt;
WeakSAM&#36890;&#36807;&#21033;&#29992;&#39044;&#20808;&#23398;&#20064;&#30340;&#20840;&#29699;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#24369;&#30417;&#30563;&#23545;&#35937;&#26816;&#27979;&#21644;&#20998;&#21106;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;PGT&#29983;&#25104;&#21644;RoI&#20002;&#24323;&#27491;&#21017;&#21270;&#65292;&#26174;&#33879;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#30340;&#35270;&#35273;&#35782;&#21035;&#20351;&#29992;&#19981;&#31934;&#30830;&#30340;&#30417;&#30563;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#23427;&#26174;&#33879;&#38477;&#20302;&#20102;&#20154;&#24037;&#26631;&#27880;&#25104;&#26412;&#65292;&#24182;&#19988;&#20256;&#32479;&#19978;&#20381;&#36182;&#22810;&#23454;&#20363;&#23398;&#20064;&#21644;&#20266;&#26631;&#31614;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;WeakSAM&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#21253;&#21547;&#22312;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#39044;&#20808;&#23398;&#20064;&#30340;&#20840;&#29699;&#30693;&#35782;&#65292;&#21363;Segment Anything Model (SAM)&#65292;&#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#29289;&#20307;&#26816;&#27979;&#65288;WSOD&#65289;&#21644;&#20998;&#21106;&#12290;WeakSAM&#36890;&#36807;&#33258;&#36866;&#24212;PGT&#29983;&#25104;&#21644;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;RoI&#65289;&#20002;&#24323;&#27491;&#21017;&#21270;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;WSOD&#37325;&#26032;&#35757;&#32451;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#21363;&#20266;&#26631;&#20934;&#22320;&#38754;&#30495;&#30456;&#65288;PGT&#65289;&#30340;&#19981;&#23436;&#25972;&#24615;&#21644;&#20855;&#26377;&#22024;&#26434;PGT&#23454;&#20363;&#12290;&#23427;&#36824;&#35299;&#20915;&#20102;SAM&#22312;&#33258;&#21160;&#23545;&#35937;&#26816;&#27979;&#21644;&#20998;&#21106;&#26102;&#38656;&#35201;&#25552;&#31034;&#21644;&#31867;&#21035;&#26080;&#24863;&#30693;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;WeakSAM&#22312;WSOD&#21644;WSIS&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14812v1 Announce Type: cross  Abstract: Weakly supervised visual recognition using inexact supervision is a critical yet challenging learning problem. It significantly reduces human labeling costs and traditionally relies on multi-instance learning and pseudo-labeling. This paper introduces WeakSAM and solves the weakly-supervised object detection (WSOD) and segmentation by utilizing the pre-learned world knowledge contained in a vision foundation model, i.e., the Segment Anything Model (SAM). WeakSAM addresses two critical limitations in traditional WSOD retraining, i.e., pseudo ground truth (PGT) incompleteness and noisy PGT instances, through adaptive PGT generation and Region of Interest (RoI) drop regularization. It also addresses the SAM's problems of requiring prompts and category unawareness for automatic object detection and segmentation. Our results indicate that WeakSAM significantly surpasses previous state-of-the-art methods in WSOD and WSIS benchmarks with larg
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#28151;&#21512;&#25512;&#29702;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#12289;&#29702;&#35299;&#35268;&#21017;&#21644;&#27861;&#21017;&#12289;&#25552;&#20379;&#35821;&#22659;&#31561;&#26041;&#24335;&#25552;&#39640;&#33258;&#21160;&#39542;&#39542;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13602</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28151;&#21512;&#25512;&#29702;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13602
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#28151;&#21512;&#25512;&#29702;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#12289;&#29702;&#35299;&#35268;&#21017;&#21644;&#27861;&#21017;&#12289;&#25552;&#20379;&#35821;&#22659;&#31561;&#26041;&#24335;&#25552;&#39640;&#33258;&#21160;&#39542;&#39542;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#29702;&#35299;&#25991;&#26412;&#21644;&#22270;&#20687;&#12289;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#20197;&#21450;&#25191;&#34892;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23558;&#36825;&#31181;&#39640;&#32423;&#25512;&#29702;&#19982;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#30456;&#32467;&#21512;&#20197;&#29992;&#20110;&#21160;&#24577;&#24773;&#20917;&#19979;&#30340;&#20915;&#31574;&#30340;&#27867;&#21270;&#33021;&#21147;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#22312;&#28151;&#21512;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#26041;&#38754;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#24212;&#29992;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#12290;&#25105;&#20204;&#20551;&#35774;LLMs&#30340;&#28151;&#21512;&#25512;&#29702;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#20351;&#23427;&#20204;&#20998;&#26512;&#26816;&#27979;&#21040;&#30340;&#29289;&#20307;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#29702;&#35299;&#39550;&#39542;&#35268;&#23450;&#21644;&#29289;&#29702;&#27861;&#21017;&#65292;&#24182;&#25552;&#20379;&#39069;&#22806;&#30340;&#35821;&#22659;&#26469;&#25913;&#21892;&#33258;&#21160;&#39550;&#39542;&#12290;&#36825;&#35299;&#20915;&#20102;&#22797;&#26434;&#24773;&#26223;&#65292;&#22914;&#20302;&#33021;&#35265;&#24230;&#65288;&#30001;&#20110;&#22825;&#27668;&#26465;&#20214;&#65289;&#19979;&#30340;&#20915;&#31574;&#65292;&#20256;&#32479;&#26041;&#27861;&#21487;&#33021;&#19981;&#36275;&#20197;&#32988;&#20219;&#12290;&#25105;&#20204;&#36890;&#36807;&#20934;&#30830;&#24615;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36825;&#31181;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13602v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have garnered significant attention for their ability to understand text and images, generate human-like text, and perform complex reasoning tasks. However, their ability to generalize this advanced reasoning with a combination of natural language text for decision-making in dynamic situations requires further exploration. In this study, we investigate how well LLMs can adapt and apply a combination of arithmetic and common-sense reasoning, particularly in autonomous driving scenarios. We hypothesize that LLMs hybrid reasoning abilities can improve autonomous driving by enabling them to analyze detected object and sensor data, understand driving regulations and physical laws, and offer additional context. This addresses complex scenarios, like decisions in low visibility (due to weather conditions), where traditional methods might fall short. We evaluated Large Language Models (LLMs) based on accuracy by co
&lt;/p&gt;</description></item><item><title>CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;</title><link>https://arxiv.org/abs/2402.05374</link><description>&lt;p&gt;
CIC&#65306;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CIC: A framework for Culturally-aware Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05374
&lt;/p&gt;
&lt;p&gt;
CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;VLPs&#65289;&#22914;BLIP&#20174;&#22270;&#20687;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#65292;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#25152;&#25551;&#32472;&#30340;&#25991;&#21270;&#20803;&#32032;&#65288;&#20363;&#22914;&#20122;&#27954;&#25991;&#21270;&#32676;&#20307;&#30340;&#20256;&#32479;&#26381;&#35013;&#65289;&#29983;&#25104;&#35814;&#32454;&#25551;&#36848;&#24615;&#23383;&#24149;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;\textbf{&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#65288;CIC&#65289;}&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#20195;&#34920;&#19981;&#21516;&#25991;&#21270;&#30340;&#22270;&#20687;&#20013;&#29983;&#25104;&#23383;&#24149;&#24182;&#25551;&#36848;&#25991;&#21270;&#20803;&#32032;&#12290;&#21463;&#21040;&#23558;&#35270;&#35273;&#27169;&#24577;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#36827;&#34892;&#32452;&#21512;&#30340;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#65288;1&#65289;&#26681;&#25454;&#22270;&#20687;&#20013;&#30340;&#25991;&#21270;&#31867;&#21035;&#29983;&#25104;&#38382;&#39064;&#65292;&#65288;2&#65289;&#21033;&#29992;&#29983;&#25104;&#30340;&#38382;&#39064;&#20174;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20013;&#25552;&#21462;&#25991;&#21270;&#35270;&#35273;&#20803;&#32032;&#65292;&#65288;3&#65289;&#20351;&#29992;&#24102;&#26377;&#25552;&#31034;&#30340;LLMs&#29983;&#25104;&#25991;&#21270;&#24863;&#30693;&#23383;&#24149;&#12290;&#25105;&#20204;&#22312;4&#20010;&#19981;&#21516;&#22823;&#23398;&#30340;45&#21517;&#21442;&#19982;&#32773;&#19978;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Captioning generates descriptive sentences from images using Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved greatly. However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups. In this paper, we propose a new framework, \textbf{Culturally-aware Image Captioning (CIC)}, that generates captions and describes cultural elements extracted from cultural visual elements in images representing cultures. Inspired by methods combining visual modality and Large Language Models (LLMs) through appropriate prompts, our framework (1) generates questions based on cultural categories from images, (2) extracts cultural visual elements from Visual Question Answering (VQA) using generated questions, and (3) generates culturally-aware captions using LLMs with the prompts. Our human evaluation conducted on 45 participants from 4 dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PPR&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#30340;&#24615;&#33021;&#21516;&#26102;&#36991;&#20813;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#38477;&#32423;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#24182;&#36890;&#36807;&#23884;&#20837;&#23545;&#25239;&#25200;&#21160;&#26469;&#22686;&#24378;&#23545;&#25239;&#20154;&#33080;&#26679;&#26412;&#30340;&#36530;&#36991;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08903</link><description>&lt;p&gt;
PPR: &#22312;&#32500;&#25345;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#21516;&#26102;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#23545;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks on Face Recognition Systems. (arXiv:2401.08903v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PPR&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#30340;&#24615;&#33021;&#21516;&#26102;&#36991;&#20813;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#38477;&#32423;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#24182;&#36890;&#36807;&#23884;&#20837;&#23545;&#25239;&#25200;&#21160;&#26469;&#22686;&#24378;&#23545;&#25239;&#20154;&#33080;&#26679;&#26412;&#30340;&#36530;&#36991;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#19978;&#30340;&#23545;&#25239;&#25915;&#20987;&#21487;&#20197;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#21644;&#36530;&#36991;&#25915;&#20987;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#25104;&#21151;&#36827;&#34892;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#24182;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#22312;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#19978;&#25104;&#21151;&#36827;&#34892;&#36530;&#36991;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39044;&#35757;&#32451;&#20462;&#21098;&#24674;&#22797;&#25915;&#20987;&#65288;PPR&#65289;&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#30340;&#24615;&#33021;&#21516;&#26102;&#36991;&#20813;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#38477;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#21487;&#20197;&#23558;&#19968;&#37096;&#20998;&#23545;&#25239;&#25200;&#21160;&#35774;&#20026;&#38646;&#65292;&#24182;&#20542;&#21521;&#20110;&#20445;&#25345;&#25915;&#20987;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#25105;&#20204;&#21487;&#20197;&#20462;&#21098;&#39044;&#35757;&#32451;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#26377;&#36873;&#25321;&#24615;&#22320;&#37322;&#25918;&#26576;&#20123;&#23545;&#25239;&#25200;&#21160;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#23545;&#25239;&#25200;&#21160;&#23884;&#20837;&#21040;&#20462;&#21098;&#21306;&#22495;&#65292;&#20174;&#32780;&#22686;&#24378;&#23545;&#25239;&#20154;&#33080;&#26679;&#26412;&#30340;&#36530;&#36991;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks on Face Recognition (FR) encompass two types: impersonation attacks and evasion attacks. We observe that achieving a successful impersonation attack on FR does not necessarily ensure a successful dodging attack on FR in the black-box setting. Introducing a novel attack method named Pre-training Pruning Restoration Attack (PPR), we aim to enhance the performance of dodging attacks whilst avoiding the degradation of impersonation attacks. Our method employs adversarial example pruning, enabling a portion of adversarial perturbations to be set to zero, while tending to maintain the attack performance. By utilizing adversarial example pruning, we can prune the pre-trained adversarial examples and selectively free up certain adversarial perturbations. Thereafter, we embed adversarial perturbations in the pruned area, which enhances the dodging performance of the adversarial face examples. The effectiveness of our proposed attack method is demonstrated through our experim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;FedED&#65292;&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#31354;&#31867;&#21035;&#33976;&#39311;&#21644;&#36923;&#36753;&#25233;&#21046;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23578;&#26410;&#20805;&#20998;&#35782;&#21035;&#31354;&#31867;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02329</link><description>&lt;p&gt;
&#19981;&#26159;&#25152;&#26377;&#30340;&#23569;&#25968;&#32676;&#20307;&#37117;&#26159;&#24179;&#31561;&#30340;: &#31354;&#31867;&#21035;&#24863;&#30693;&#30340;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Not all Minorities are Equal: Empty-Class-Aware Distillation for Heterogeneous Federated Learning. (arXiv:2401.02329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;FedED&#65292;&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#31354;&#31867;&#21035;&#33976;&#39311;&#21644;&#36923;&#36753;&#25233;&#21046;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23578;&#26410;&#20805;&#20998;&#35782;&#21035;&#31354;&#31867;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24322;&#36136;&#24615;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#34920;&#29616;&#20026;&#23458;&#25143;&#31471;&#20043;&#38388;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#24046;&#24322;&#12290;&#29616;&#26377;&#26041;&#27861;&#24120;&#24120;&#22312;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#31867;&#21035;&#24179;&#34913;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#26412;&#22320;&#31867;&#21035;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#23569;&#25968;&#31867;&#21035;&#20013;&#30001;&#20110;&#36807;&#25311;&#21512;&#26412;&#22320;&#19981;&#24179;&#34913;&#25968;&#25454;&#32780;&#23548;&#33268;&#20934;&#30830;&#24615;&#36739;&#24046;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedED&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#25972;&#21512;&#20102;&#31354;&#31867;&#21035;&#33976;&#39311;&#21644;&#36923;&#36753;&#25233;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31354;&#31867;&#21035;&#33976;&#39311;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#20013;&#20445;&#30041;&#20102;&#19982;&#31354;&#31867;&#21035;&#30456;&#20851;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36923;&#36753;&#25233;&#21046;&#30452;&#25509;&#38459;&#26029;&#20102;&#39044;&#27979;&#32467;&#26524;&#20013;&#23545;&#31354;&#31867;&#21035;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data heterogeneity, characterized by disparities in local data distribution across clients, poses a significant challenge in federated learning. Substantial efforts have been devoted to addressing the heterogeneity in local label distribution. As minority classes suffer from worse accuracy due to overfitting on local imbalanced data, prior methods often incorporate class-balanced learning techniques during local training. Despite the improved mean accuracy across all classes, we observe that empty classes-referring to categories absent from a client's data distribution-are still not well recognized. This paper introduces FedED, a novel approach in heterogeneous federated learning that integrates both empty-class distillation and logit suppression simultaneously. Specifically, empty-class distillation leverages knowledge distillation during local training on each client to retain essential information related to empty classes from the global model. Moreover, logit suppression directly p
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#36127;&#36131;&#20219;&#35780;&#20215;&#26631;&#20934;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2310.15848</link><description>&lt;p&gt;
&#20851;&#20110;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#21644;&#20844;&#24179;&#24615;&#12289;&#38544;&#31169;&#21644;&#27861;&#35268;&#20934;&#21017;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
On Responsible Machine Learning Datasets with Fairness, Privacy, and Regulatory Norms. (arXiv:2310.15848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15848
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#36127;&#36131;&#20219;&#35780;&#20215;&#26631;&#20934;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#36827;&#20837;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#20102;&#24778;&#20154;&#30340;&#25913;&#36827;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#21487;&#20449;&#24615;&#23384;&#22312;&#20005;&#37325;&#25285;&#24551;&#12290;&#31185;&#23398;&#30028;&#33268;&#21147;&#20110;&#24320;&#21457;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#20013;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#20854;&#24320;&#21457;&#36807;&#31243;&#20013;&#20005;&#37325;&#20381;&#36182;&#20351;&#29992;&#30340;&#25968;&#25454;&#12290;&#36825;&#20123;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#26469;&#23398;&#20064;&#34892;&#20026;&#30446;&#26631;&#12290;&#25968;&#25454;&#20013;&#30340;&#20219;&#20309;&#32570;&#38519;&#37117;&#26377;&#21487;&#33021;&#30452;&#25509;&#36716;&#21270;&#20026;&#31639;&#27861;&#30340;&#32570;&#38519;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#36127;&#36131;&#20219;&#35780;&#20215;&#26631;&#20934;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#23545;&#31639;&#27861;&#30340;&#21518;&#26399;&#35780;&#20272;&#20197;&#30830;&#20445;&#20854;&#21487;&#20449;&#24615;&#65292;&#32780;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21333;&#29420;&#32771;&#34385;&#25968;&#25454;&#32452;&#20214;&#20197;&#29702;&#35299;&#20854;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has made its way into various scientific fields, providing astonishing improvements over existing algorithms for a wide variety of tasks. In recent years, there have been severe concerns over the trustworthiness of AI technologies. The scientific community has focused on the development of trustworthy AI algorithms. However, machine and deep learning algorithms, popular in the AI community today, depend heavily on the data used during their development. These learning algorithms identify patterns in the data, learning the behavioral objective. Any flaws in the data have the potential to translate directly into algorithms. In this study, we discuss the importance of Responsible Machine Learning Datasets and propose a framework to evaluate the datasets through a responsible rubric. While existing work focuses on the post-hoc evaluation of algorithms for their trustworthiness, we provide a framework that considers the data component separately to understand it
&lt;/p&gt;</description></item><item><title>CRITERIA&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#31934;&#32454;&#25490;&#21517;&#39044;&#27979;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#24615;&#33021;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.07794</link><description>&lt;p&gt;
CRITERIA&#65306;&#19968;&#31181;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#30340;&#26032;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CRITERIA: a New Benchmarking Paradigm for Evaluating Trajectory Prediction Models for Autonomous Driving. (arXiv:2310.07794v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07794
&lt;/p&gt;
&lt;p&gt;
CRITERIA&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#31934;&#32454;&#25490;&#21517;&#39044;&#27979;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#24615;&#33021;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20934;&#27979;&#35797;&#26159;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#23545;&#20110;&#36739;&#24120;&#35265;&#30340;&#24773;&#20917;&#65288;&#22914;&#24033;&#33322;&#65289;&#23384;&#22312;&#20559;&#24046;&#65292;&#24182;&#36890;&#36807;&#23545;&#25152;&#26377;&#24773;&#20917;&#36827;&#34892;&#24179;&#22343;&#35745;&#31639;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#24230;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#24456;&#23569;&#33021;&#25552;&#20379;&#26377;&#20851;&#27169;&#22411;&#24615;&#33021;&#30340;&#27934;&#23519;&#65292;&#26080;&#35770;&#26159;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#23427;&#20204;&#33021;&#21542;&#33391;&#22909;&#22788;&#29702;&#65292;&#36824;&#26159;&#23427;&#20204;&#30340;&#36755;&#20986;&#26159;&#21542;&#20801;&#35768;&#21644;&#22810;&#26679;&#21270;&#12290;&#34429;&#28982;&#23384;&#22312;&#19968;&#20123;&#29992;&#20110;&#34913;&#37327;&#36712;&#36857;&#21487;&#20801;&#35768;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#34917;&#20805;&#25351;&#26631;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#22914;&#36712;&#36857;&#38271;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65288;CRITERIA&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#36947;&#36335;&#32467;&#26500;&#12289;&#27169;&#22411;&#24615;&#33021;&#21644;&#25968;&#25454;&#29305;&#24615;&#25552;&#21462;&#39550;&#39542;&#22330;&#26223;&#30340;&#26041;&#27861;&#65292;&#20197;&#36827;&#34892;&#31934;&#32454;&#25490;&#21517;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmarking is a common method for evaluating trajectory prediction models for autonomous driving. Existing benchmarks rely on datasets, which are biased towards more common scenarios, such as cruising, and distance-based metrics that are computed by averaging over all scenarios. Following such a regiment provides a little insight into the properties of the models both in terms of how well they can handle different scenarios and how admissible and diverse their outputs are. There exist a number of complementary metrics designed to measure the admissibility and diversity of trajectories, however, they suffer from biases, such as length of trajectories.  In this paper, we propose a new benChmarking paRadIgm for evaluaTing trajEctoRy predIction Approaches (CRITERIA). Particularly, we propose 1) a method for extracting driving scenarios at varying levels of specificity according to the structure of the roads, models' performance, and data properties for fine-grained ranking of prediction 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#26080;&#26799;&#24230;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#30340;&#23545;&#25239;&#34917;&#19969;&#65292;&#25915;&#20987;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.04238</link><description>&lt;p&gt;
&#21306;&#22495;&#38544;&#24418;&#34917;&#19969;&#65306;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#29289;&#20307;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Patch of Invisibility: Naturalistic Black-Box Adversarial Attacks on Object Detectors. (arXiv:2303.04238v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#26080;&#26799;&#24230;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#30340;&#23545;&#25239;&#34917;&#19969;&#65292;&#25915;&#20987;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#36234;&#26469;&#36234;&#24341;&#36215;&#20851;&#27880;&#12290;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#65292;&#21363;&#25152;&#35859;&#30340;&#30333;&#30418;&#25915;&#20987;&#65292;&#22312;&#20854;&#20013;&#25915;&#20987;&#32773;&#21487;&#20197;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#20869;&#37096;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20551;&#35774;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#36890;&#24120;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#30456;&#23545;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26080;&#38656;&#20351;&#29992;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#23398;&#20064;&#22270;&#20687;&#27969;&#24418;&#26469;&#29983;&#25104;&#33258;&#28982;&#30340;&#29289;&#29702;&#23545;&#25239;&#34917;&#19969;&#65292;&#29992;&#20110;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25968;&#23383;&#21644;&#29289;&#29702;&#23618;&#38754;&#19978;&#22343;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks on deep-learning models have been receiving increased attention in recent years. Work in this area has mostly focused on gradient-based techniques, so-called white-box attacks, wherein the attacker has access to the targeted model's internal parameters; such an assumption is usually unrealistic in the real world. Some attacks additionally use the entire pixel space to fool a given model, which is neither practical nor physical (i.e., real-world). On the contrary, we propose herein a gradient-free method that uses the learned image manifold of a pretrained generative adversarial network (GAN) to generate naturalistic physical adversarial patches for object detectors. We show that our proposed method works both digitally and physically.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26032;&#39062;&#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#32954;&#25513;&#27169;&#20449;&#24687;&#26469;&#35782;&#21035;&#26377;&#25928;&#21306;&#22495;&#65292;&#20197;&#23398;&#20064;&#26356;&#26377;&#29992;&#30340;COVID-19&#26816;&#27979;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2211.00313</link><description>&lt;p&gt;
RGMIM: &#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#29992;&#20110;COVID-19&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
RGMIM: Region-Guided Masked Image Modeling for COVID-19 Detection. (arXiv:2211.00313v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26032;&#39062;&#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#32954;&#25513;&#27169;&#20449;&#24687;&#26469;&#35782;&#21035;&#26377;&#25928;&#21306;&#22495;&#65292;&#20197;&#23398;&#20064;&#26356;&#26377;&#29992;&#30340;COVID-19&#26816;&#27979;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#33258;&#30417;&#30563;&#23398;&#20064;&#27491;&#22312;&#24555;&#36895;&#25512;&#36827;&#21307;&#23398;&#39046;&#22495;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#12290;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#25513;&#30422;&#20102;&#19968;&#32452;&#36755;&#20837;&#20687;&#32032;&#24182;&#35797;&#22270;&#39044;&#27979;&#36974;&#30422;&#30340;&#20687;&#32032;&#12290;&#20256;&#32479;&#30340;MIM&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#38543;&#26426;&#25513;&#33180;&#31574;&#30053;&#12290;&#19982;&#26222;&#36890;&#22270;&#20687;&#30456;&#27604;&#65292;&#21307;&#23398;&#22270;&#20687;&#24448;&#24448;&#20855;&#26377;&#29992;&#20110;&#30142;&#30149;&#26816;&#27979;&#30340;&#23567;&#21306;&#22495;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#19987;&#27880;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#33258;&#21160;COVID-19&#35782;&#21035;&#26041;&#38754;&#36827;&#34892;&#35780;&#20272;&#12290;&#26041;&#27861;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65288;RGMIM&#65289;&#29992;&#20110;COVID-19&#26816;&#27979;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25513;&#33180;&#31574;&#30053;&#65292;&#21033;&#29992;&#32954;&#25513;&#27169;&#20449;&#24687;&#26469;&#35782;&#21035;&#26377;&#25928;&#21306;&#22495;&#65292;&#20197;&#23398;&#20064;&#26356;&#26377;&#29992;&#30340;COVID-19&#26816;&#27979;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20116;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65288;MAE&#65292;SKD&#65292;Cross&#65292;BYOL&#21644;SimSiam&#65289;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Self-supervised learning is rapidly advancing computer-aided diagnosis in the medical field. Masked image modeling (MIM) is one of the self-supervised learning methods that masks a subset of input pixels and attempts to predict the masked pixels. Traditional MIM methods often employ a random masking strategy. In comparison to ordinary images, medical images often have a small region of interest for disease detection. Consequently, we focus on fixing the problem in this work, which is evaluated by automatic COVID-19 identification. Methods: In this study, we propose a novel region-guided masked image modeling method (RGMIM) for COVID-19 detection in this paper. In our method, we devise a new masking strategy that employed lung mask information to identify valid regions to learn more useful information for COVID-19 detection. The proposed method was contrasted with five self-supervised learning techniques (MAE, SKD, Cross, BYOL, and, SimSiam). We present a quantitative evaluatio
&lt;/p&gt;</description></item></channel></rss>