<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#20316;&#20026;&#39069;&#22806;&#25104;&#20687;&#25216;&#26415;&#26469;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#21644;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#32467;&#21512;&#30340;&#26041;&#27861;&#25104;&#21151;&#21306;&#20998;&#20102;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;&#21644;&#38750;&#39118;&#38505;&#24739;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.18873</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#32593;&#33180;OCT&#25104;&#20687;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Predicting risk of cardiovascular disease using retinal OCT imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18873
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#20316;&#20026;&#39069;&#22806;&#25104;&#20687;&#25216;&#26415;&#26469;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#21644;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#32467;&#21512;&#30340;&#26041;&#27861;&#25104;&#21151;&#21306;&#20998;&#20102;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;&#21644;&#38750;&#39118;&#38505;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#20316;&#20026;&#19968;&#31181;&#39069;&#22806;&#25104;&#20687;&#25216;&#26415;&#26469;&#39044;&#27979;&#26410;&#26469;&#24515;&#34880;&#31649;&#30142;&#30149;&#65288;CVD&#65289;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#20102;&#39640;&#32500;3D OCT&#22270;&#20687;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#25429;&#25417;&#20102;OCT&#22270;&#20687;&#20013;&#19981;&#21516;&#35270;&#32593;&#33180;&#23618;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#38543;&#21518;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#28508;&#22312;&#29305;&#24449;&#21644;&#21442;&#19982;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#20197;&#21450;&#20020;&#24202;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#20998;&#31867;&#22120;&#65292;&#20197;&#21306;&#20998;&#22788;&#20110;CVD&#20107;&#20214;&#39118;&#38505;&#65288;&#24515;&#26775;&#25110;&#20013;&#39118;&#65289;&#21644;&#38750;CVD&#30149;&#20363;&#30340;&#24739;&#32773;&#12290;&#25105;&#20204;&#30340;&#39044;&#27979;&#27169;&#22411;&#22522;&#20110;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#35780;&#20272;&#20854;&#33021;&#21147;&#26469;&#27491;&#30830;&#35782;&#21035;&#22312;&#22270;&#20687;&#33719;&#21462;&#21518;&#30340;5&#24180;&#20869;&#21487;&#33021;&#24739;&#26377;CVD&#20107;&#20214;&#65288;&#24515;&#26775;&#25110;&#20013;&#39118;&#65289;&#30340;&#20010;&#20307;&#12290;&#25105;&#20204;&#30340;&#33258;&#30417;&#30563;VAE&#29305;&#24449;&#36873;&#25321;&#21644;&#22810;&#27169;&#24577;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18873v1 Announce Type: cross  Abstract: We investigated the potential of optical coherence tomography (OCT) as an additional imaging technique to predict future cardiovascular disease (CVD). We utilised a self-supervised deep learning approach based on Variational Autoencoders (VAE) to learn low-dimensional representations of high-dimensional 3D OCT images and to capture distinct characteristics of different retinal layers within the OCT image. A Random Forest (RF) classifier was subsequently trained using the learned latent features and participant demographic and clinical data, to differentiate between patients at risk of CVD events (MI or stroke) and non-CVD cases. Our predictive model, trained on multimodal data, was assessed based on its ability to correctly identify individuals likely to suffer from a CVD event(MI or stroke), within a 5-year interval after image acquisition. Our self-supervised VAE feature selection and multimodal Random Forest classifier differentiate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#20960;&#31181;&#25968;&#20540;&#19978;&#31283;&#20581;&#30340;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#38381;&#21512;&#24418;&#24335;1D&#23376;&#27169;&#22411;Fisher-Rao&#36317;&#31163;&#30340;&#36890;&#29992;&#19978;&#30028;&#20197;&#21450;&#21462;&#20915;&#20110;&#27979;&#22320;&#32447;&#25110;&#39044;&#27979;&#27979;&#22320;&#32447;&#26159;&#21542;&#38381;&#21512;&#24418;&#24335;&#33719;&#24471;&#30340;&#20960;&#31181;&#36890;&#29992;&#36817;&#20284;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#20445;&#35777;&#36817;&#20284;&#35823;&#24046;&#20219;&#24847;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.10089</link><description>&lt;p&gt;
&#29992;&#20110;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Approximation and bounding techniques for the Fisher-Rao distances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20960;&#31181;&#25968;&#20540;&#19978;&#31283;&#20581;&#30340;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#38381;&#21512;&#24418;&#24335;1D&#23376;&#27169;&#22411;Fisher-Rao&#36317;&#31163;&#30340;&#36890;&#29992;&#19978;&#30028;&#20197;&#21450;&#21462;&#20915;&#20110;&#27979;&#22320;&#32447;&#25110;&#39044;&#27979;&#27979;&#22320;&#32447;&#26159;&#21542;&#38381;&#21512;&#24418;&#24335;&#33719;&#24471;&#30340;&#20960;&#31181;&#36890;&#29992;&#36817;&#20284;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#20445;&#35777;&#36817;&#20284;&#35823;&#24046;&#20219;&#24847;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#27169;&#22411;&#30340;&#20004;&#20010;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;Fisher-Rao&#36317;&#31163;&#34987;&#23450;&#20041;&#20026;Fisher&#20449;&#24687;&#24230;&#37327;&#35825;&#23548;&#30340;Riemannian&#27979;&#22320;&#36317;&#31163;&#12290;&#20026;&#20102;&#20197;&#38381;&#21512;&#24418;&#24335;&#35745;&#31639;Fisher-Rao&#36317;&#31163;&#65292;&#25105;&#20204;&#38656;&#35201;&#65288;1&#65289;&#25512;&#23548;&#20986;Fisher-Rao&#27979;&#22320;&#32447;&#30340;&#20844;&#24335;&#65292;&#20197;&#21450;&#65288;2&#65289;&#27839;&#30528;&#36825;&#20123;&#27979;&#22320;&#32447;&#31215;&#20998;Fisher&#38271;&#24230;&#20803;&#32032;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#25968;&#20540;&#19978;&#31283;&#20581;&#30340;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#23376;&#27169;&#22411;&#30340;&#38381;&#21512;&#24418;&#24335;1D Fisher-Rao&#36317;&#31163;&#25253;&#21578;&#20102;Fisher-Rao&#36317;&#31163;&#30340;&#36890;&#29992;&#19978;&#30028;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20960;&#31181;&#36890;&#29992;&#30340;&#36817;&#20284;&#26041;&#26696;&#65292;&#21462;&#20915;&#20110;Fisher-Rao&#27979;&#22320;&#32447;&#25110;&#39044;&#27979;&#27979;&#22320;&#32447;&#26159;&#21542;&#33021;&#20197;&#38381;&#21512;&#24418;&#24335;&#33719;&#24471;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#22312;&#25552;&#20379;Fisher-Rao&#39044;&#27979;&#27979;&#22320;&#32447;&#21644;&#20005;&#26684;&#30340;&#19979;&#30028;&#21644;&#19978;&#30028;&#26102;&#36817;&#20284;&#20135;&#29983;&#20219;&#24847;&#23567;&#30340;&#38468;&#21152;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10089v1 Announce Type: cross  Abstract: The Fisher-Rao distance between two probability distributions of a statistical model is defined as the Riemannian geodesic distance induced by the Fisher information metric. In order to calculate the Fisher-Rao distance in closed-form, we need (1) to elicit a formula for the Fisher-Rao geodesics, and (2) to integrate the Fisher length element along those geodesics. We consider several numerically robust approximation and bounding techniques for the Fisher-Rao distances: First, we report generic upper bounds on Fisher-Rao distances based on closed-form 1D Fisher-Rao distances of submodels. Second, we describe several generic approximation schemes depending on whether the Fisher-Rao geodesics or pregeodesics are available in closed-form or not. In particular, we obtain a generic method to guarantee an arbitrarily small additive error on the approximation provided that Fisher-Rao pregeodesics and tight lower and upper bounds are available
&lt;/p&gt;</description></item><item><title>Finer&#24037;&#20316;&#25581;&#31034;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#19978;&#30340;&#30701;&#26495;&#65292;&#23588;&#20854;&#26159;&#38590;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#32454;&#33268;&#23646;&#24615;&#35299;&#37322;&#65292;&#23613;&#31649;&#20855;&#26377;&#29983;&#25104;&#39640;&#27700;&#24179;&#22270;&#20687;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16315</link><description>&lt;p&gt;
Finer: &#22312;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30740;&#31350;&#21644;&#22686;&#24378;&#32454;&#31890;&#24230;&#35270;&#35273;&#27010;&#24565;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16315
&lt;/p&gt;
&lt;p&gt;
Finer&#24037;&#20316;&#25581;&#31034;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#19978;&#30340;&#30701;&#26495;&#65292;&#23588;&#20854;&#26159;&#38590;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#32454;&#33268;&#23646;&#24615;&#35299;&#37322;&#65292;&#23613;&#31649;&#20855;&#26377;&#29983;&#25104;&#39640;&#27700;&#24179;&#22270;&#20687;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#27169;&#22411;&#33021;&#22815;&#36731;&#26494;&#29983;&#25104;&#39640;&#27700;&#24179;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#35299;&#37322;&#12290;&#23613;&#31649;&#36825;&#31181;&#33021;&#21147;&#20027;&#35201;&#24402;&#22240;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#21253;&#21547;&#30340;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20845;&#20010;&#19981;&#21516;&#22522;&#20934;&#35774;&#32622;&#19979;&#30340;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#65288;FGVC&#65289;&#19978;&#30340;&#32570;&#38519;&#12290;&#26368;&#36817;&#30340;LVLMs&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#22914;LLaVa-1.5&#65292;InstructBLIP&#21644;GPT-4V&#65292;&#22312;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#20005;&#37325;&#19979;&#38477;&#65292;&#20363;&#22914;&#65292;LLaVA-1.5&#22312;&#26031;&#22374;&#31119;&#29399;&#30340;EM&#24179;&#22343;&#19979;&#38477;&#20102;65.58&#65292;&#32780;&#19988;&#36824;&#38590;&#20197;&#26681;&#25454;&#20986;&#29616;&#22312;&#36755;&#20837;&#22270;&#20687;&#20013;&#30340;&#27010;&#24565;&#29983;&#25104;&#20855;&#26377;&#35814;&#32454;&#23646;&#24615;&#30340;&#20934;&#30830;&#35299;&#37322;&#65292;&#23613;&#31649;&#23427;&#20204;&#26377;&#29983;&#25104;&#25972;&#20307;&#22270;&#20687;&#32423;&#25551;&#36848;&#30340;&#33021;&#21147;&#12290;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#32463;&#36807;&#25351;&#23548;&#35843;&#25972;&#30340;LVLMs&#22312;&#32473;&#23450;&#25991;&#26412;&#26102;&#21576;&#29616;&#20986;&#27169;&#24577;&#24046;&#36317;&#65292;&#26174;&#31034;&#20986;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16315v1 Announce Type: cross  Abstract: Recent advances in instruction-tuned Large Vision-Language Models (LVLMs) have imbued the models with the ability to generate high-level, image-grounded explanations with ease. While such capability is largely attributed to the rich world knowledge contained within the Large Language Models (LLMs), our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings. Most recent state-of-the-art LVLMs like LLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of classification performance, e.g., average drop of 65.58 in EM for Stanford Dogs for LLaVA-1.5, but also struggle to generate an accurate explanation with detailed attributes based on the concept that appears within an input image despite their capability to generate holistic image-level descriptions. In-depth analyses show that instruction-tuned LVLMs exhibit modality gap, showing discrepancy when given tex
&lt;/p&gt;</description></item><item><title>Multi&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#25490;&#34892;&#27036;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23427;&#20860;&#20855;&#20934;&#30830;&#21644;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#24418;&#24335;&#65292;&#25361;&#25112;MLLM&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#21253;&#21547;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03173</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#65306;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#25490;&#34892;&#27036;
&lt;/p&gt;
&lt;p&gt;
Multi: Multimodal Understanding Leaderboard with Text and Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03173
&lt;/p&gt;
&lt;p&gt;
Multi&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#29702;&#35299;&#30340;&#25490;&#34892;&#27036;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23427;&#20860;&#20855;&#20934;&#30830;&#21644;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#24418;&#24335;&#65292;&#25361;&#25112;MLLM&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#21253;&#21547;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#24555;&#36895;&#36827;&#23637;&#24378;&#35843;&#20102;&#21521;&#23398;&#26415;&#30028;&#24341;&#20837;&#20855;&#26377;&#25361;&#25112;&#24615;&#32780;&#21448;&#30495;&#23454;&#30340;&#22522;&#20934;&#30340;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#31616;&#21333;&#30340;&#33258;&#28982;&#22270;&#20687;&#29702;&#35299;&#65292;&#20294;Multi&#25104;&#20026;&#20102;MLLM&#30340;&#23574;&#31471;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#23545;&#29702;&#35299;&#22797;&#26434;&#22270;&#34920;&#21644;&#31185;&#23398;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#21453;&#26144;&#20102;&#24403;&#21069;&#30495;&#23454;&#30340;&#32771;&#35797;&#39118;&#26684;&#65292;&#25552;&#20379;&#22810;&#27169;&#24577;&#30340;&#36755;&#20837;&#65292;&#24182;&#35201;&#27714;&#20934;&#30830;&#25110;&#24320;&#25918;&#24335;&#30340;&#22238;&#31572;&#65292;&#31867;&#20284;&#20110;&#29616;&#23454;&#20013;&#30340;&#23398;&#26657;&#32771;&#35797;&#12290;&#23427;&#36890;&#36807;&#21508;&#31181;&#20219;&#21153;&#25361;&#25112;MLLM&#65292;&#20174;&#20844;&#24335;&#25512;&#23548;&#21040;&#22270;&#20687;&#32454;&#33410;&#20998;&#26512;&#65292;&#20197;&#21450;&#36328;&#27169;&#24577;&#25512;&#29702;&#12290;Multi&#21253;&#25324;&#36229;&#36807;18,000&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#19981;&#21516;&#26684;&#24335;&#30340;&#22522;&#20110;&#31185;&#23398;&#30340;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;Multi-Elite&#65292;&#19968;&#20010;&#21253;&#21547;500&#20010;&#38382;&#39064;&#30340;&#23376;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;MLLM&#30340;&#26497;&#31471;&#24773;&#20917;&#65292;&#20197;&#21450;Multi-Extend&#65292;&#36890;&#36807;&#36229;&#36807;4..&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community. Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions. This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests. It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning. Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats. We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4
&lt;/p&gt;</description></item><item><title>GUPNet++&#26159;&#19968;&#31181;&#36890;&#36807;&#20197;&#27010;&#29575;&#26041;&#24335;&#24314;&#27169;&#20960;&#20309;&#25237;&#24433;&#30340;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#32593;&#32476;&#65292;&#21487;&#20197;&#25552;&#39640;&#21333;&#30446;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#28145;&#24230;&#39044;&#27979;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.15624</link><description>&lt;p&gt;
GUPNet++&#65306;&#29992;&#20110;&#21333;&#30446;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GUPNet++: Geometry Uncertainty Propagation Network for Monocular 3D Object Detection. (arXiv:2310.15624v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15624
&lt;/p&gt;
&lt;p&gt;
GUPNet++&#26159;&#19968;&#31181;&#36890;&#36807;&#20197;&#27010;&#29575;&#26041;&#24335;&#24314;&#27169;&#20960;&#20309;&#25237;&#24433;&#30340;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#32593;&#32476;&#65292;&#21487;&#20197;&#25552;&#39640;&#21333;&#30446;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#28145;&#24230;&#39044;&#27979;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#22312;&#21333;&#30446;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#29289;&#20307;&#30340;&#29289;&#29702;&#23610;&#23544;&#19982;&#22270;&#20687;&#24179;&#38754;&#20013;&#30340;&#20108;&#32500;&#25237;&#24433;&#20043;&#38388;&#30340;&#36879;&#35270;&#25237;&#24433;&#26469;&#20272;&#35745;&#29289;&#20307;&#30340;&#28145;&#24230;&#65292;&#36825;&#21487;&#20197;&#23558;&#25968;&#23398;&#20808;&#39564;&#24341;&#20837;&#28145;&#24230;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25237;&#24433;&#36807;&#31243;&#20063;&#20250;&#24341;&#20837;&#35823;&#24046;&#25918;&#22823;&#65292;&#20272;&#35745;&#39640;&#24230;&#30340;&#35823;&#24046;&#20250;&#34987;&#25918;&#22823;&#24182;&#21453;&#26144;&#21040;&#25237;&#24433;&#30340;&#28145;&#24230;&#20013;&#12290;&#36825;&#23548;&#33268;&#28145;&#24230;&#25512;&#26029;&#19981;&#21487;&#38752;&#65292;&#24182;&#19988;&#24433;&#21709;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#32593;&#32476;(GUPNet++)&#65292;&#36890;&#36807;&#20197;&#27010;&#29575;&#26041;&#24335;&#24314;&#27169;&#20960;&#20309;&#25237;&#24433;&#12290;&#36825;&#30830;&#20445;&#20102;&#28145;&#24230;&#39044;&#27979;&#26159;&#26377;&#30028;&#30340;&#65292;&#24182;&#19982;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#32852;&#12290;&#24341;&#20837;&#36825;&#31181;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#30340;&#24847;&#20041;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;(1)&#12290;&#23427;&#27169;&#25311;&#20102;&#20960;&#20309;&#25237;&#24433;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#20851;&#31995;&#65292;&#25552;&#39640;&#20102;&#31471;&#21040;&#31471;&#27169;&#22411;&#23398;&#20064;&#30340;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometry plays a significant role in monocular 3D object detection. It can be used to estimate object depth by using the perspective projection between object's physical size and 2D projection in the image plane, which can introduce mathematical priors into deep models. However, this projection process also introduces error amplification, where the error of the estimated height is amplified and reflected into the projected depth. It leads to unreliable depth inferences and also impairs training stability. To tackle this problem, we propose a novel Geometry Uncertainty Propagation Network (GUPNet++) by modeling geometry projection in a probabilistic manner. This ensures depth predictions are well-bounded and associated with a reasonable uncertainty. The significance of introducing such geometric uncertainty is two-fold: (1). It models the uncertainty propagation relationship of the geometry projection during training, improving the stability and efficiency of the end-to-end model learni
&lt;/p&gt;</description></item></channel></rss>