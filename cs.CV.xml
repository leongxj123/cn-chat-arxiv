<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#31361;&#20986;&#30340;&#23545;&#35937;&#21306;&#22495;&#36827;&#34892;&#36807;&#37319;&#26679;&#30340;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#22788;&#29702;&#65292;&#20197;&#21450;&#38024;&#23545;&#23545;&#35937;&#21306;&#22495;&#37319;&#26679;&#30340;&#23454;&#20363;&#32423;&#21464;&#24418;&#24341;&#23548;&#65292;&#26377;&#25928;&#20943;&#36731;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.12712</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#20687;&#21464;&#24418;&#35299;&#20915;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Source Scale Bias via Image Warping for Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#31361;&#20986;&#30340;&#23545;&#35937;&#21306;&#22495;&#36827;&#34892;&#36807;&#37319;&#26679;&#30340;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#22788;&#29702;&#65292;&#20197;&#21450;&#38024;&#23545;&#23545;&#35937;&#21306;&#22495;&#37319;&#26679;&#30340;&#23454;&#20363;&#32423;&#21464;&#24418;&#24341;&#23548;&#65292;&#26377;&#25928;&#20943;&#36731;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#65292;&#30001;&#20110;&#30495;&#23454;&#22330;&#26223;&#25968;&#25454;&#38598;&#20013;&#23545;&#35937;&#21644;&#22270;&#20687;&#22823;&#23567;&#20998;&#24067;&#30340;&#19981;&#24179;&#34913;&#65292;&#23610;&#24230;&#20559;&#24046;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#27880;&#20837;&#23610;&#24230;&#19981;&#21464;&#24615;&#20808;&#39564;&#12289;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#25968;&#25454;&#38598;&#22312;&#19981;&#21516;&#23610;&#24230;&#36827;&#34892;&#36807;&#37319;&#26679;&#65292;&#25110;&#32773;&#22312;&#25512;&#26029;&#26102;&#35843;&#25972;&#23610;&#24230;&#12290;&#34429;&#28982;&#36825;&#20123;&#31574;&#30053;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#36731;&#20102;&#23610;&#24230;&#20559;&#24046;&#65292;&#20294;&#23427;&#20204;&#22312;&#36328;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#26102;&#30340;&#36866;&#24212;&#33021;&#21147;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20250;&#22686;&#21152;&#35757;&#32451;&#36807;&#31243;&#30340;&#35745;&#31639;&#36127;&#36733;&#21644;&#25512;&#26029;&#36807;&#31243;&#30340;&#24310;&#36831;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;&#27880;&#24847;&#21147;&#22788;&#29702;&#8212;&#8212;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23601;&#22320;&#25197;&#26354;&#22270;&#20687;&#26469;&#23545;&#31361;&#20986;&#30340;&#23545;&#35937;&#21306;&#22495;&#36827;&#34892;&#36807;&#37319;&#26679;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#25913;&#21464;&#28304;&#23610;&#24230;&#20998;&#24067;&#21487;&#20197;&#25913;&#21892;&#20027;&#24178;&#29305;&#24449;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38754;&#21521;&#23545;&#35937;&#21306;&#22495;&#37319;&#26679;&#30340;&#23454;&#20363;&#32423;&#21464;&#24418;&#24341;&#23548;&#65292;&#20197;&#20943;&#36731;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#23545;&#22320;&#29702;&#12289;&#20809;&#29031;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12712v1 Announce Type: cross  Abstract: In visual recognition, scale bias is a key challenge due to the imbalance of object and image size distribution inherent in real scene datasets. Conventional solutions involve injecting scale invariance priors, oversampling the dataset at different scales during training, or adjusting scale at inference. While these strategies mitigate scale bias to some extent, their ability to adapt across diverse datasets is limited. Besides, they increase computational load during training and latency during inference. In this work, we use adaptive attentional processing -- oversampling salient object regions by warping images in-place during training. Discovering that shifting the source scale distribution improves backbone features, we developed a instance-level warping guidance aimed at object region sampling to mitigate source scale bias in domain adaptation. Our approach improves adaptation across geographies, lighting and weather conditions, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#22270;&#20687;&#32534;&#36753;&#65292;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#39044;&#22791;&#24037;&#20316;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#21644;DDIM&#21453;&#28436;&#65292;&#33719;&#21462;&#32534;&#36753;&#26041;&#21521;&#23884;&#20837;&#65292;&#36827;&#34892;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#65292;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08004</link><description>&lt;p&gt;
Pix2Pix-OnTheFly: &#21033;&#29992;LLMs&#36827;&#34892;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Pix2Pix-OnTheFly: Leveraging LLMs for Instruction-Guided Image Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#22270;&#20687;&#32534;&#36753;&#65292;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#39044;&#22791;&#24037;&#20316;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#21644;DDIM&#21453;&#28436;&#65292;&#33719;&#21462;&#32534;&#36753;&#26041;&#21521;&#23884;&#20837;&#65292;&#36827;&#34892;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#65292;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#26368;&#36817;&#32467;&#21512;&#35821;&#35328;&#22788;&#29702;&#21644;&#22270;&#20687;&#22788;&#29702;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#21644;DDIM&#21453;&#28436;&#65292;&#33719;&#21462;&#32534;&#36753;&#26041;&#21521;&#23884;&#20837;&#65292;&#36827;&#34892;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#65292;&#32780;&#26080;&#38656;&#39044;&#22791;&#24037;&#20316;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08004v1 Announce Type: cross  Abstract: The combination of language processing and image processing keeps attracting increased interest given recent impressive advances that leverage the combined strengths of both domains of research. Among these advances, the task of editing an image on the basis solely of a natural language instruction stands out as a most challenging endeavour. While recent approaches for this task resort, in one way or other, to some form of preliminary preparation, training or fine-tuning, this paper explores a novel approach: We propose a preparation-free method that permits instruction-guided image editing on the fly. This approach is organized along three steps properly orchestrated that resort to image captioning and DDIM inversion, followed by obtaining the edit direction embedding, followed by image editing proper. While dispensing with preliminary preparation, our approach demonstrates to be effective and competitive, outperforming recent, state 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#35780;&#20272;&#20010;&#20307;&#21160;&#20316;&#27169;&#24335;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;3D&#23156;&#20799;&#21160;&#21147;&#23398;&#36827;&#34892;&#24314;&#27169;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.14400</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;3D&#23156;&#20799;&#21160;&#21147;&#23398;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14400
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#35780;&#20272;&#20010;&#20307;&#21160;&#20316;&#27169;&#24335;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;3D&#23156;&#20799;&#21160;&#21147;&#23398;&#36827;&#34892;&#24314;&#27169;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#23156;&#20799;&#31070;&#32463;&#21457;&#32946;&#35780;&#20272;&#26041;&#27861;&#23545;&#20110;&#26089;&#26399;&#21457;&#29616;&#21487;&#33021;&#38656;&#35201;&#21450;&#26102;&#24178;&#39044;&#30340;&#21307;&#23398;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#21457;&#30340;&#36816;&#21160;&#27963;&#21160;&#65292;&#21363;&#8220;&#21160;&#21147;&#23398;&#8221;&#65292;&#34987;&#35777;&#26126;&#21487;&#25552;&#20379;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#39044;&#27979;&#26410;&#26469;&#31070;&#32463;&#21457;&#32946;&#30340;&#26367;&#20195;&#24615;&#27979;&#37327;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#35780;&#20272;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#23450;&#24615;&#21644;&#20027;&#35266;&#30340;&#65292;&#20391;&#37325;&#20110;&#23545;&#36890;&#36807;&#35270;&#35273;&#35782;&#21035;&#30340;&#29305;&#23450;&#24180;&#40836;&#25163;&#21183;&#30340;&#25551;&#36848;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#26681;&#25454;&#25968;&#25454;&#39537;&#21160;&#35780;&#20272;&#20010;&#20307;&#21160;&#20316;&#27169;&#24335;&#26469;&#39044;&#27979;&#23156;&#20799;&#31070;&#32463;&#21457;&#32946;&#25104;&#29087;&#12290;&#25105;&#20204;&#21033;&#29992;&#22788;&#29702;&#36807;&#30340;3D&#23156;&#20799;&#35270;&#39057;&#24405;&#20687;&#36827;&#34892;&#23039;&#21183;&#20272;&#35745;&#65292;&#25552;&#21462;&#35299;&#21078;&#26631;&#24535;&#29289;&#30340;&#26102;&#31354;&#31995;&#21015;&#65292;&#24182;&#24212;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#39044;&#27979;&#23454;&#38469;&#24180;&#40836;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#20110;&#25163;&#21160;&#35774;&#35745;&#29305;&#24449;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#22522;&#32447;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14400v1 Announce Type: cross  Abstract: Reliable methods for the neurodevelopmental assessment of infants are essential for early detection of medical issues that may need prompt interventions. Spontaneous motor activity, or `kinetics', is shown to provide a powerful surrogate measure of upcoming neurodevelopment. However, its assessment is by and large qualitative and subjective, focusing on visually identified, age-specific gestures. Here, we follow an alternative approach, predicting infants' neurodevelopmental maturation based on data-driven evaluation of individual motor patterns. We utilize 3D video recordings of infants processed with pose-estimation to extract spatio-temporal series of anatomical landmarks, and apply adaptive graph convolutional networks to predict the actual age. We show that our data-driven approach achieves improvement over traditional machine learning baselines based on manually engineered features.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22343;&#36136;&#21644;&#24322;&#36136;&#23454;&#20363;&#32423;&#21035;&#32467;&#26500;&#65292;&#26500;&#24314;&#39640;&#36136;&#37327;&#36328;&#27169;&#24577;&#26631;&#31614;&#20851;&#32852;&#30340;&#27169;&#24577;&#32479;&#19968;&#26631;&#31614;&#20256;&#36755;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.00672</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#30340;&#22343;&#36136;&#21644;&#24322;&#36136;&#19968;&#33268;&#26631;&#31614;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised Visible-Infrared Person ReID
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22343;&#36136;&#21644;&#24322;&#36136;&#23454;&#20363;&#32423;&#21035;&#32467;&#26500;&#65292;&#26500;&#24314;&#39640;&#36136;&#37327;&#36328;&#27169;&#24577;&#26631;&#31614;&#20851;&#32852;&#30340;&#27169;&#24577;&#32479;&#19968;&#26631;&#31614;&#20256;&#36755;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#65288;USL-VI-ReID&#65289;&#26088;&#22312;&#26080;&#38656;&#27880;&#37322;&#20174;&#19981;&#21516;&#27169;&#24577;&#20013;&#26816;&#32034;&#30456;&#21516;&#36523;&#20221;&#30340;&#34892;&#20154;&#22270;&#20687;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#24314;&#31435;&#36328;&#27169;&#24577;&#30340;&#20266;&#26631;&#31614;&#20851;&#32852;&#20197;&#24357;&#21512;&#27169;&#24577;&#38388;&#30340;&#24046;&#24322;&#65292;&#20294;&#24573;&#30053;&#20102;&#22312;&#20266;&#26631;&#31614;&#31354;&#38388;&#20013;&#20445;&#25345;&#23454;&#20363;&#32423;&#21035;&#30340;&#22343;&#36136;&#21644;&#24322;&#36136;&#19968;&#33268;&#24615;&#65292;&#23548;&#33268;&#20851;&#32852;&#31895;&#31961;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#24577;&#32479;&#19968;&#26631;&#31614;&#20256;&#36755;&#65288;MULT&#65289;&#27169;&#22359;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22343;&#36136;&#21644;&#24322;&#36136;&#32454;&#31890;&#24230;&#23454;&#20363;&#32423;&#32467;&#26500;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36328;&#27169;&#24577;&#26631;&#31614;&#20851;&#32852;&#12290;&#23427;&#24314;&#27169;&#20102;&#22343;&#36136;&#21644;&#24322;&#36136;&#30340;&#20851;&#32852;&#24615;&#65292;&#21033;&#29992;&#23427;&#20204;&#23450;&#20041;&#20266;&#26631;&#31614;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#28982;&#21518;&#26368;&#23567;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#32500;&#25345;&#20102;&#36328;&#27169;&#24577;&#30340;&#23545;&#40784;&#24182;&#20445;&#25345;&#20102;&#20869;&#37096;&#27169;&#24577;&#32467;&#26500;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#26377;&#19968;&#20010;&#31616;&#21333;&#26131;&#29992;&#30340;&#22312;&#32447;&#20132;&#21449;&#35760;&#24518;&#26631;&#31614;&#24341;&#29992;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to retrieve pedestrian images of the same identity from different modalities without annotations. While prior work focuses on establishing cross-modality pseudo-label associations to bridge the modality-gap, they ignore maintaining the instance-level homogeneous and heterogeneous consistency in pseudo-label space, resulting in coarse associations. In response, we introduce a Modality-Unified Label Transfer (MULT) module that simultaneously accounts for both homogeneous and heterogeneous fine-grained instance-level structures, yielding high-quality cross-modality label associations. It models both homogeneous and heterogeneous affinities, leveraging them to define the inconsistency for the pseudo-labels and then minimize it, leading to pseudo-labels that maintain alignment across modalities and consistency within intra-modality structures. Additionally, a straightforward plug-and-play Online Cross-memory Label Ref
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26435;&#37325;&#22238;&#28378;&#30340;&#24494;&#35843;&#26041;&#27861;OLOR&#65292;&#36890;&#36807;&#32467;&#21512;&#20248;&#21270;&#22120;&#21644;&#26435;&#37325;&#22238;&#28378;&#39033;&#65292;&#35299;&#20915;&#20102;&#23436;&#20840;&#24494;&#35843;&#26041;&#27861;&#20013;&#30340;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#24494;&#35843;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10962</link><description>&lt;p&gt;
&#19968;&#27493;&#23398;&#20064;&#65292;&#19968;&#27493;&#35780;&#23457;
&lt;/p&gt;
&lt;p&gt;
One Step Learning, One Step Review. (arXiv:2401.10962v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26435;&#37325;&#22238;&#28378;&#30340;&#24494;&#35843;&#26041;&#27861;OLOR&#65292;&#36890;&#36807;&#32467;&#21512;&#20248;&#21270;&#22120;&#21644;&#26435;&#37325;&#22238;&#28378;&#39033;&#65292;&#35299;&#20915;&#20102;&#23436;&#20840;&#24494;&#35843;&#26041;&#27861;&#20013;&#30340;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#24494;&#35843;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#35270;&#35273;&#24494;&#35843;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#24403;&#21069;&#20027;&#27969;&#30340;&#26041;&#27861;&#8212;&#8212;&#23436;&#20840;&#24494;&#35843;&#65292;&#23384;&#22312;&#30693;&#35782;&#36951;&#24536;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#21482;&#19987;&#27880;&#20110;&#25311;&#21512;&#19979;&#28216;&#35757;&#32451;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26435;&#37325;&#22238;&#28378;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;OLOR&#65288;&#19968;&#27493;&#23398;&#20064;&#65292;&#19968;&#27493;&#35780;&#23457;&#65289;&#12290;OLOR&#23558;&#24494;&#35843;&#19982;&#20248;&#21270;&#22120;&#30456;&#32467;&#21512;&#65292;&#23558;&#26435;&#37325;&#22238;&#28378;&#39033;&#21152;&#20837;&#21040;&#27599;&#20010;&#27493;&#39588;&#30340;&#26435;&#37325;&#26356;&#26032;&#39033;&#20013;&#12290;&#36825;&#30830;&#20445;&#20102;&#19978;&#28216;&#21644;&#19979;&#28216;&#27169;&#22411;&#30340;&#26435;&#37325;&#33539;&#22260;&#30340;&#19968;&#33268;&#24615;&#65292;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22686;&#24378;&#20102;&#24494;&#35843;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#23618;&#24809;&#32602;&#26041;&#27861;&#65292;&#36890;&#36807; penalty decay &#21644;&#19981;&#21516;&#30340;&#34928;&#20943;&#29575;&#26469;&#35843;&#25972;&#23618;&#30340;&#26435;&#37325;&#22238;&#28378;&#31243;&#24230;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#31561;&#21508;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual fine-tuning has garnered significant attention with the rise of pre-trained vision models. The current prevailing method, full fine-tuning, suffers from the issue of knowledge forgetting as it focuses solely on fitting the downstream training set. In this paper, we propose a novel weight rollback-based fine-tuning method called OLOR (One step Learning, One step Review). OLOR combines fine-tuning with optimizers, incorporating a weight rollback term into the weight update term at each step. This ensures consistency in the weight range of upstream and downstream models, effectively mitigating knowledge forgetting and enhancing fine-tuning performance. In addition, a layer-wise penalty is presented to employ penalty decay and the diversified decay rate to adjust the weight rollback levels of layers for adapting varying downstream tasks. Through extensive experiments on various tasks such as image classification, object detection, semantic segmentation, and instance segmentation, we
&lt;/p&gt;</description></item><item><title>FocDepthFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;LSTM&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#28966;&#28857;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;&#12290;&#36890;&#36807;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;LSTM&#30340;&#38598;&#25104;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#20855;&#26377;&#23545;&#20219;&#24847;&#38271;&#24230;&#22534;&#26632;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11178</link><description>&lt;p&gt;
FocDepthFormer: &#20351;&#29992;LSTM&#30340;Transformer&#29992;&#20110;&#20174;&#28966;&#28857;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
FocDepthFormer: Transformer with LSTM for Depth Estimation from Focus. (arXiv:2310.11178v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11178
&lt;/p&gt;
&lt;p&gt;
FocDepthFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;LSTM&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#28966;&#28857;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;&#12290;&#36890;&#36807;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;LSTM&#30340;&#38598;&#25104;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#20855;&#26377;&#23545;&#20219;&#24847;&#38271;&#24230;&#22534;&#26632;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#28966;&#28857;&#22534;&#26632;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#22534;&#26632;&#20013;&#30340;&#28966;&#28857;/&#31163;&#28966;&#32447;&#32034;&#25512;&#26029;&#28145;&#24230;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#22270;&#20687;&#22534;&#26632;&#19978;&#24212;&#29992;&#20108;&#32500;&#25110;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#26469;&#22788;&#29702;&#27492;&#38382;&#39064;&#65292;&#20197;&#22312;&#22270;&#20687;&#21644;&#22534;&#26632;&#20043;&#38388;&#23398;&#20064;&#29305;&#24449;&#12290;&#30001;&#20110;CNN&#30340;&#23616;&#37096;&#24615;&#36136;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#65292;&#24182;&#19988;&#23427;&#20204;&#34987;&#38480;&#21046;&#22312;&#22788;&#29702;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#20013;&#19968;&#33268;&#30340;&#22266;&#23450;&#25968;&#37327;&#30340;&#22534;&#26632;&#19978;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23545;&#20219;&#24847;&#38271;&#24230;&#22534;&#26632;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#65292;FocDepthFormer&#65292;&#20027;&#35201;&#30001;&#24102;&#26377;LSTM&#27169;&#22359;&#21644;CNN&#35299;&#30721;&#22120;&#30340;Transformer&#32452;&#25104;&#12290;Transformer&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#36890;&#36807;&#38544;&#21547;&#38750;&#23616;&#37096;&#20132;&#21449;&#21442;&#32771;&#33021;&#22815;&#23398;&#20064;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;LSTM&#27169;&#22359;&#34987;&#23398;&#20064;&#29992;&#20110;&#23558;&#34920;&#31034;&#38598;&#25104;&#21040;&#20855;&#26377;&#20219;&#24847;&#22270;&#20687;&#30340;&#22534;&#26632;&#20013;&#12290;&#20026;&#20102;&#30452;&#25509;&#25429;&#33719;&#20302;&#32423;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Depth estimation from focal stacks is a fundamental computer vision problem that aims to infer depth from focus/defocus cues in the image stacks. Most existing methods tackle this problem by applying convolutional neural networks (CNNs) with 2D or 3D convolutions over a set of fixed stack images to learn features across images and stacks. Their performance is restricted due to the local properties of the CNNs, and they are constrained to process a fixed number of stacks consistent in train and inference, limiting the generalization to the arbitrary length of stacks. To handle the above limitations, we develop a novel Transformer-based network, FocDepthFormer, composed mainly of a Transformer with an LSTM module and a CNN decoder. The self-attention in Transformer enables learning more informative features via an implicit non-local cross reference. The LSTM module is learned to integrate the representations across the stack with arbitrary images. To directly capture the low-level featur
&lt;/p&gt;</description></item><item><title>OpenDriver&#26159;&#19968;&#20221;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#23384;&#22312;&#38382;&#39064;&#30340;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20004;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.04203</link><description>&lt;p&gt;
OpenDriver: &#19968;&#20221;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OpenDriver: an open-road driver state detection dataset. (arXiv:2304.04203v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04203
&lt;/p&gt;
&lt;p&gt;
OpenDriver&#26159;&#19968;&#20221;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#23384;&#22312;&#38382;&#39064;&#30340;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20004;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#31038;&#20250;&#20013;&#65292;&#36947;&#36335;&#23433;&#20840;&#20005;&#37325;&#20381;&#36182;&#20110;&#39550;&#39542;&#21592;&#30340;&#24515;&#29702;&#21644;&#29983;&#29702;&#29366;&#24577;&#12290;&#30130;&#21171;&#12289;&#26127;&#26127;&#27442;&#30561;&#21644;&#21387;&#21147;&#31561;&#36127;&#38754;&#22240;&#32032;&#20250;&#24433;&#21709;&#39550;&#39542;&#21592;&#30340;&#21453;&#24212;&#26102;&#38388;&#21644;&#20915;&#31574;&#33021;&#21147;&#65292;&#23548;&#33268;&#20132;&#36890;&#20107;&#25925;&#30340;&#21457;&#29983;&#29575;&#22686;&#21152;&#12290;&#22312;&#20247;&#22810;&#30340;&#39550;&#39542;&#21592;&#34892;&#20026;&#30417;&#27979;&#30740;&#31350;&#20013;&#65292;&#21487;&#31359;&#25140;&#29983;&#29702;&#27979;&#37327;&#26159;&#19968;&#31181;&#23454;&#26102;&#30417;&#27979;&#39550;&#39542;&#21592;&#29366;&#24577;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#24320;&#25918;&#36947;&#36335;&#22330;&#26223;&#19979;&#65292;&#32570;&#23569;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#65292;&#24050;&#26377;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#20449;&#21495;&#36136;&#37327;&#24046;&#12289;&#26679;&#26412;&#37327;&#23567;&#21644;&#25968;&#25454;&#25910;&#38598;&#26102;&#38388;&#30701;&#31561;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#35774;&#35745;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20004;&#31181;&#39550;&#39542;&#20449;&#21495;&#27169;&#24577;&#65306;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#65292;&#36825;&#20123;&#20449;&#21495;&#26159;&#22312;100&#22810;&#21517;&#39550;&#39542;&#21592;&#36981;&#24490;&#30456;&#21516;&#36335;&#32447;&#34892;&#39542;&#26102;&#35760;&#24405;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern society, road safety relies heavily on the psychological and physiological state of drivers. Negative factors such as fatigue, drowsiness, and stress can impair drivers' reaction time and decision making abilities, leading to an increased incidence of traffic accidents. Among the numerous studies for impaired driving detection, wearable physiological measurement is a real-time approach to monitoring a driver's state. However, currently, there are few driver physiological datasets in open road scenarios and the existing datasets suffer from issues such as poor signal quality, small sample sizes, and short data collection periods. Therefore, in this paper, a large-scale multimodal driving dataset for driver impairment detection and biometric data recognition is designed and described. The dataset contains two modalities of driving signals: six-axis inertial signals and electrocardiogram (ECG) signals, which were recorded while over one hundred drivers were following the same ro
&lt;/p&gt;</description></item></channel></rss>