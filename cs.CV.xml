<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Transformer&#20013;&#25552;&#21462;&#22270;&#24418;&#20197;&#29992;&#20110;&#22330;&#26223;&#22270;&#29983;&#25104;&#30340;&#36731;&#37327;&#32423;&#21333;&#38454;&#27573;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25552;&#21462;&#20102;&#20851;&#31995;&#22270;&#12290;</title><link>https://arxiv.org/abs/2404.02072</link><description>&lt;p&gt;
&#20174;Transformer&#20013;&#25552;&#21462;&#22270;&#24418;&#29992;&#20110;&#22330;&#26223;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
EGTR: Extracting Graph from Transformer for Scene Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02072
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Transformer&#20013;&#25552;&#21462;&#22270;&#24418;&#20197;&#29992;&#20110;&#22330;&#26223;&#22270;&#29983;&#25104;&#30340;&#36731;&#37327;&#32423;&#21333;&#38454;&#27573;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25552;&#21462;&#20102;&#20851;&#31995;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;SGG&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#26816;&#27979;&#23545;&#35937;&#24182;&#39044;&#27979;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#21333;&#38454;&#27573;SGG&#27169;&#22411;&#65292;&#23427;&#20174;DETR&#35299;&#30721;&#22120;&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#23618;&#20013;&#23398;&#20064;&#30340;&#21508;&#31181;&#20851;&#31995;&#20013;&#25552;&#21462;&#20851;&#31995;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02072v1 Announce Type: cross  Abstract: Scene Graph Generation (SGG) is a challenging task of detecting objects and predicting relationships between objects. After DETR was developed, one-stage SGG models based on a one-stage object detector have been actively studied. However, complex modeling is used to predict the relationship between objects, and the inherent relationship between object queries learned in the multi-head self-attention of the object detector has been neglected. We propose a lightweight one-stage SGG model that extracts the relation graph from the various relationships learned in the multi-head self-attention layers of the DETR decoder. By fully utilizing the self-attention by-products, the relation graph can be extracted effectively with a shallow relation extraction head. Considering the dependency of the relation extraction task on the object detection task, we propose a novel relation smoothing technique that adjusts the relation label adaptively accor
&lt;/p&gt;</description></item><item><title>LSKNet&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#39592;&#24178;&#65292;&#33021;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#31243;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2403.11735</link><description>&lt;p&gt;
LSKNet&#65306;&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#30340;&#36731;&#37327;&#32423;&#22522;&#30784;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
LSKNet: A Foundation Lightweight Backbone for Remote Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11735
&lt;/p&gt;
&lt;p&gt;
LSKNet&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#39592;&#24178;&#65292;&#33021;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#31243;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#23545;&#19979;&#28216;&#20219;&#21153;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#22823;&#37327;&#30740;&#31350;&#33268;&#21147;&#20110;&#36965;&#24863;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#24573;&#35270;&#20102;&#23884;&#20837;&#22312;&#36965;&#24863;&#22330;&#26223;&#20013;&#30340;&#23453;&#36149;&#20808;&#39564;&#30693;&#35782;&#12290;&#36825;&#20123;&#20808;&#39564;&#30693;&#35782;&#21487;&#33021;&#20250;&#24456;&#26377;&#29992;&#65292;&#22240;&#20026;&#22312;&#27809;&#26377;&#21442;&#32771;&#36275;&#22815;&#38271;&#31243;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#36965;&#24863;&#23545;&#35937;&#21487;&#33021;&#20250;&#34987;&#38169;&#35823;&#35782;&#21035;&#65292;&#32780;&#36825;&#21487;&#20197;&#22240;&#19981;&#21516;&#23545;&#35937;&#32780;&#24322;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#36825;&#20123;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#65288;LSKNet&#65289;&#39592;&#24178;&#32593;&#32476;&#12290;LSKNet&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20808;&#21069;&#23578;&#26410;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#25506;&#32034;&#36807;&#22823;&#22411;&#21644;&#36873;&#25321;&#24615;&#26680;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#27809;&#26377;&#22826;&#22810;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11735v1 Announce Type: cross  Abstract: Remote sensing images pose distinct challenges for downstream tasks due to their inherent complexity. While a considerable amount of research has been dedicated to remote sensing classification, object detection and semantic segmentation, most of these studies have overlooked the valuable prior knowledge embedded within remote sensing scenarios. Such prior knowledge can be useful because remote sensing objects may be mistakenly recognized without referencing a sufficiently long-range context, which can vary for different objects. This paper considers these priors and proposes a lightweight Large Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various objects in remote sensing scenarios. To our knowledge, large and selective kernel mechanisms have not been previously explored in remote sensing images. Without bells and whistles, our lightw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sparse MetA-Tuning&#65288;SMAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28789;&#24863;&#26469;&#33258;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#26041;&#27861;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#22495;&#22806;&#20219;&#21153;&#25935;&#24863;&#24615;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#36716;&#31227;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.08477</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#25554;&#20540;&#19987;&#23478;&#37322;&#25918;&#20803;&#35843;&#25972;&#30340;&#21147;&#37327;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sparse MetA-Tuning&#65288;SMAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28789;&#24863;&#26469;&#33258;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#26041;&#27861;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#22495;&#22806;&#20219;&#21153;&#25935;&#24863;&#24615;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#36716;&#31227;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26234;&#24935;&#24314;&#35758;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#65292;&#26159;&#35270;&#35273;&#36801;&#31227;&#23398;&#20064;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#21462;&#20195;&#20102;&#35832;&#22914;&#20803;&#23398;&#20064;&#20043;&#31867;&#30340;&#20016;&#23500;&#25991;&#29486;&#12290;&#20026;&#20102;&#20860;&#39038;&#20004;&#32773;&#30340;&#21033;&#30410;&#65292;&#20803;&#35843;&#25972;&#24341;&#20837;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#38543;&#21518;&#20248;&#21270;&#38454;&#27573;&#65292;&#20294;&#36804;&#20170;&#21482;&#23637;&#29616;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#65292;&#20851;&#38190;&#22320;&#22312;&#22495;&#22806;&#65288;OOD&#65289;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28789;&#24863;&#26469;&#33258;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#26041;&#27861;&#30340; Sparse MetA-Tuning&#65288;SMAT&#65289;&#26041;&#27861;&#65292;&#23427;&#32463;&#36807;&#35757;&#32451;&#20197;&#33258;&#21160;&#22320;&#20026;&#27599;&#20010;&#20219;&#21153;&#38548;&#31163;&#39044;&#35757;&#32451;&#21442;&#25968;&#23376;&#38598;&#20197;&#36827;&#34892;&#20803;&#35843;&#25972;&#12290;SMAT&#25104;&#21151;&#20811;&#26381;&#20102;OOD&#25935;&#24863;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#22686;&#24378;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#36716;&#31227;&#33021;&#21147;&#30340;&#25215;&#35834;&#12290;&#25105;&#20204;&#22312;Meta-Dataset&#19982;&#39069;&#22806;&#30340;OO&#25361;&#25112;&#32452;&#21512;&#19978;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08477v1 Announce Type: cross  Abstract: Conventional wisdom suggests parameter-efficient fine-tuning of foundation models as the state-of-the-art method for transfer learning in vision, replacing the rich literature of alternatives such as meta-learning. In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-domain (OOD) tasks. In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task. SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient finetuning. We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OO
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;SWAP-Score&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20013;&#27979;&#37327;&#32593;&#32476;&#22312;&#19968;&#25209;&#36755;&#20837;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04161</link><description>&lt;p&gt;
SWAP-NAS: &#36866;&#29992;&#20110;&#36229;&#24555;&#36895;NAS&#30340;&#26679;&#26412;&#32423;&#28608;&#27963;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
SWAP-NAS: Sample-Wise Activation Patterns For Ultra-Fast NAS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;SWAP-Score&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20013;&#27979;&#37327;&#32593;&#32476;&#22312;&#19968;&#25209;&#36755;&#20837;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#65288;&#21363;&#38646;&#25104;&#26412;&#20195;&#29702;&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#36991;&#20813;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#23588;&#20854;&#26159;&#22312;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20013;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#65292;&#27604;&#22914;&#22312;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#26377;&#38480;&#30340;&#20851;&#32852;&#24615;&#21644;&#24046;&#21170;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26679;&#26412;&#32423;&#28608;&#27963;&#27169;&#24335;&#21450;&#20854;&#34893;&#29983;&#29289;SWAP-Score&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#12290;&#23427;&#27979;&#37327;&#20102;&#32593;&#32476;&#22312;&#19968;&#25209;&#36755;&#20837;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;SWAP-Score&#19982;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20013;&#30340;&#30495;&#23454;&#24615;&#33021;&#24378;&#30456;&#20851;&#65292;&#22312;NAS-Bench-101/201/301&#21644;TransNAS-Bench-101&#19978;&#32988;&#36807;&#20102;15&#31181;&#29616;&#26377;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#12290;SWAP-Score&#21487;&#20197;&#36890;&#36807;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#22686;&#24378;&#65292;&#36825;&#22312;&#22522;&#20110;&#21333;&#20803;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#23454;&#29616;&#27169;&#22411;&#22823;&#23567;&#25511;&#21046;&#12290;&#20363;&#22914;&#65292;Spearman&#30340;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04161v1 Announce Type: new  Abstract: Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid resource-intensive neural network training, especially in Neural Architecture Search (NAS). Recent studies show that existing training-free metrics have several limitations, such as limited correlation and poor generalisation across different search spaces and tasks. Hence, we propose Sample-Wise Activation Patterns and its derivative, SWAP-Score, a novel high-performance training-free metric. It measures the expressivity of networks over a batch of input samples. The SWAP-Score is strongly correlated with ground-truth performance across various search spaces and tasks, outperforming 15 existing training-free metrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be further enhanced by regularisation, which leads to even higher correlations in cell-based search space and enables model size control during the search. For example, Spearman's rank
&lt;/p&gt;</description></item><item><title>EFUF&#26159;&#19968;&#31181;&#39640;&#25928;&#31934;&#32454;&#21270;&#21435;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#28040;&#38500;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#65292;&#24182;&#19981;&#38656;&#35201;&#20154;&#24037;&#27880;&#37322;&#37197;&#23545;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.09801</link><description>&lt;p&gt;
EFUF: &#39640;&#25928;&#31934;&#32454;&#21270;&#21435;&#23398;&#20064;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#20943;&#36731;&#24187;&#20687;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09801
&lt;/p&gt;
&lt;p&gt;
EFUF&#26159;&#19968;&#31181;&#39640;&#25928;&#31934;&#32454;&#21270;&#21435;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#28040;&#38500;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#65292;&#24182;&#19981;&#38656;&#35201;&#20154;&#24037;&#27880;&#37322;&#37197;&#23545;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20173;&#20250;&#29983;&#25104;&#21253;&#21547;&#22270;&#20687;&#20013;&#19981;&#23384;&#22312;&#30340;&#29289;&#20307;&#30340;&#25551;&#36848;&#65292;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#29289;&#20307;&#24187;&#35273;&#12290;&#20026;&#20102;&#28040;&#38500;&#24187;&#35273;&#65292;&#29616;&#26377;&#26041;&#27861;&#25163;&#21160;&#27880;&#37322;&#21253;&#21547;&#21644;&#19981;&#21253;&#21547;&#24187;&#35273;&#30340;&#37197;&#23545;&#21709;&#24212;&#65292;&#24182;&#37319;&#29992;&#21508;&#31181;&#23545;&#40784;&#31639;&#27861;&#26469;&#25552;&#39640;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#23545;&#40784;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#20165;&#22312;&#24494;&#35843;&#38454;&#27573;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#36824;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#26469;&#26500;&#24314;&#23545;&#40784;&#31639;&#27861;&#25152;&#38656;&#30340;&#37197;&#23545;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#21435;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31934;&#32454;&#21270;&#21435;&#23398;&#20064;&#26694;&#26550;&#65288;EFUF&#65289;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#37197;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#28040;&#38500;&#24187;&#35273;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#20943;&#23569;&#24187;&#35273;&#21516;&#26102;&#20445;&#30041;&#20934;&#30830;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09801v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we borrow the idea of unlearning and propose an efficient fine-grained unlearning framework (EFUF), which can eliminate hallucinations without the need for paired data. Extensive experiments show that our method consistently reduces hallucinations while preserv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#22270;&#20687;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.04929</link><description>&lt;p&gt;
&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#25193;&#25955;&#24341;&#23548;&#28304;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#22270;&#20687;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;DM-SFDA&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;DM-SFDA&#26041;&#27861;&#21253;&#25324;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#30446;&#26631;&#22270;&#20687;&#30340;&#29305;&#24449;&#26469;&#25351;&#23548;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#28304;&#22495;&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#34987;&#24494;&#35843;&#20197;&#29983;&#25104;&#26368;&#23567;&#21270;&#29109;&#24182;&#26368;&#22823;&#21270;&#39044;&#35757;&#32451;&#28304;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#28304;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#24050;&#24314;&#31435;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#29983;&#25104;&#30340;&#28304;&#22270;&#20687;&#19982;&#30446;&#26631;&#22495;&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;Office-31&#12289;Office-Home&#21644;VisDA&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#12289;&#39046;&#22495;&#29305;&#23450;&#30340;&#22270;&#20687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process. Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model. We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data. We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA. The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images.
&lt;/p&gt;</description></item><item><title>AdaTreeFormer&#26159;&#19968;&#31181;&#20174;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.02956</link><description>&lt;p&gt;
AdaTreeFormer: &#20174;&#19968;&#24352;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#36827;&#34892;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02956
&lt;/p&gt;
&lt;p&gt;
AdaTreeFormer&#26159;&#19968;&#31181;&#20174;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#19968;&#24352;&#33322;&#31354;&#25110;&#21355;&#26143;&#22270;&#20687;&#26469;&#20272;&#35745;&#21644;&#35745;&#25968;&#26641;&#26408;&#23494;&#24230;&#26159;&#25668;&#24433;&#27979;&#37327;&#21644;&#36965;&#24863;&#39046;&#22495;&#20013;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26862;&#26519;&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19981;&#21516;&#22320;&#24418;&#19978;&#21508;&#31181;&#21508;&#26679;&#30340;&#26641;&#26408;&#31181;&#31867;&#20005;&#37325;&#38459;&#30861;&#20102;&#26641;&#26408;&#35745;&#25968;&#27169;&#22411;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#20010;&#20174;&#20855;&#26377;&#36275;&#22815;&#26631;&#27880;&#26641;&#26408;&#30340;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;AdaTreeFormer&#65292;&#21253;&#21547;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#29992;&#20110;&#20174;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20013;&#25552;&#21462;&#31283;&#20581;&#30340;&#29305;&#24449;&#12290;&#23427;&#36824;&#21253;&#25324;&#19977;&#20010;&#23376;&#32593;&#32476;&#65306;&#20004;&#20010;&#29992;&#20110;&#20998;&#21035;&#20174;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#25552;&#21462;&#33258;&#27880;&#24847;&#21147;&#22270;&#65292;&#24182;&#19968;&#20010;&#29992;&#20110;&#25552;&#21462;&#36328;&#39046;&#22495;&#27880;&#24847;&#21147;&#22270;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#36866;&#24212;&#26426;&#21046;&#65292;&#29992;&#20110;&#20174;&#19981;&#21516;&#39046;&#22495;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of estimating and counting tree density using only a single aerial or satellite image is a difficult task in the fields of photogrammetry and remote sensing. However, it plays a crucial role in the management of forests. The huge variety of trees in varied topography severely hinders tree counting models to perform well. The purpose of this paper is to propose a framework that is learnt from the source domain with sufficient labeled trees and is adapted to the target domain with only a limited number of labeled trees. Our method, termed as AdaTreeFormer, contains one shared encoder with a hierarchical feature extraction scheme to extract robust features from the source and target domains. It also consists of three subnets: two for extracting self-domain attention maps from source and target domains respectively and one for extracting cross-domain attention maps. For the latter, an attention-to-adapt mechanism is introduced to distill relevant information from different doma
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#39046;&#22495;&#20135;&#29983;&#20102;&#39072;&#35206;&#24615;&#30340;&#24433;&#21709;&#65292;&#32553;&#23567;&#20102;&#22270;&#20687;&#36136;&#37327;&#19982;&#20154;&#31867;&#24863;&#30693;&#20559;&#22909;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;DM&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#20998;&#26512;&#20102;&#20854;&#29420;&#29305;&#29305;&#28857;&#21644;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#26367;&#20195;&#36755;&#20837;&#39046;&#22495;&#31561;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2401.00736</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#12289;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#21644;&#19968;&#20999;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models, Image Super-Resolution And Everything: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00736
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#39046;&#22495;&#20135;&#29983;&#20102;&#39072;&#35206;&#24615;&#30340;&#24433;&#21709;&#65292;&#32553;&#23567;&#20102;&#22270;&#20687;&#36136;&#37327;&#19982;&#20154;&#31867;&#24863;&#30693;&#20559;&#22909;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;DM&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#20998;&#26512;&#20102;&#20854;&#29420;&#29305;&#29305;&#28857;&#21644;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#26367;&#20195;&#36755;&#20837;&#39046;&#22495;&#31561;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#39046;&#22495;&#20013;&#20135;&#29983;&#20102;&#39072;&#35206;&#24615;&#30340;&#24433;&#21709;&#65292;&#36827;&#19968;&#27493;&#32553;&#23567;&#20102;&#22270;&#20687;&#36136;&#37327;&#19982;&#20154;&#31867;&#24863;&#30693;&#20559;&#22909;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#20204;&#26131;&#20110;&#35757;&#32451;&#65292;&#24182;&#33021;&#29983;&#25104;&#27604;&#20197;&#21069;&#30340;&#29983;&#25104;&#26041;&#27861;&#20135;&#29983;&#30340;&#26679;&#26412;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65306;&#39640;&#35745;&#31639;&#38656;&#27714;&#12289;&#21487;&#27604;&#24615;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12289;&#33394;&#24425;&#20559;&#31227;&#31561;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#22823;&#37327;&#30340;&#20986;&#29256;&#29289;&#65292;&#36827;&#20837;&#36825;&#20010;&#39046;&#22495;&#20196;&#20154;&#38590;&#20197;&#24212;&#23545;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21465;&#36848;&#65292;&#38416;&#26126;&#20102;&#24212;&#29992;&#20110;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;DM&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20221;&#35814;&#32454;&#30340;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#35813;&#39046;&#22495;&#20869;&#19982;&#20854;&#20182;&#32508;&#36848;&#25991;&#31456;&#19981;&#21516;&#30340;&#29420;&#29305;&#29305;&#28857;&#21644;&#26041;&#27861;&#12290;&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#23545;DM&#30340;&#21407;&#21017;&#36827;&#34892;&#20102;&#19968;&#20010;&#36830;&#36143;&#30340;&#29702;&#35299;&#65292;&#24182;&#25506;&#32034;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21253;&#25324;&#26367;&#20195;&#36755;&#20837;&#39046;&#22495;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models (DMs) have disrupted the image Super-Resolution (SR) field and further closed the gap between image quality and human perceptual preferences. They are easy to train and can produce very high-quality samples that exceed the realism of those produced by previous generative methods. Despite their promising results, they also come with new challenges that need further research: high computational demands, comparability, lack of explainability, color shifts, and more. Unfortunately, entry into this field is overwhelming because of the abundance of publications. To address this, we provide a unified recount of the theoretical foundations underlying DMs applied to image SR and offer a detailed analysis that underscores the unique characteristics and methodologies within this domain, distinct from broader existing reviews in the field. This survey articulates a cohesive understanding of DM principles and explores current research avenues, including alternative input domains, c
&lt;/p&gt;</description></item><item><title>EasyEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#25903;&#25345;&#21508;&#31181;&#26368;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#30693;&#21517;&#30340;LLMs&#12290;</title><link>https://arxiv.org/abs/2308.07269</link><description>&lt;p&gt;
EasyEdit&#65306;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.07269
&lt;/p&gt;
&lt;p&gt;
EasyEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#25903;&#25345;&#21508;&#31181;&#26368;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#30693;&#21517;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#36973;&#21463;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#23545;&#26410;&#35265;&#20107;&#20214;&#19981;&#30693;&#24773;&#25110;&#29983;&#25104;&#20855;&#26377;&#19981;&#27491;&#30830;&#20107;&#23454;&#30340;&#25991;&#26412;&#65292;&#21407;&#22240;&#26159;&#25968;&#25454;&#36807;&#26102;/&#22024;&#26434;&#12290;&#20026;&#27492;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#38024;&#23545;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#26088;&#22312;&#24494;&#22937;&#22320;&#27880;&#20837;/&#32534;&#36753;&#26356;&#26032;&#30340;&#30693;&#35782;&#25110;&#35843;&#25972;&#19981;&#33391;&#34892;&#20026;&#65292;&#21516;&#26102;&#23558;&#23545;&#19981;&#30456;&#20851;&#36755;&#20837;&#30340;&#24433;&#21709;&#26368;&#23567;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20197;&#21450;&#20219;&#21153;&#35774;&#32622;&#20013;&#30340;&#21464;&#21270;&#65292;&#31038;&#21306;&#20013;&#27809;&#26377;&#21487;&#29992;&#20110;&#30693;&#35782;&#32534;&#36753;&#30340;&#26631;&#20934;&#23454;&#26045;&#26694;&#26550;&#65292;&#36825;&#22952;&#30861;&#20102;&#20174;&#19994;&#32773;&#23558;&#30693;&#35782;&#32534;&#36753;&#24212;&#29992;&#20110;&#24212;&#29992;&#31243;&#24207;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyEdit&#65292;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;LLMs&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#12290;&#23427;&#25903;&#25345;&#21508;&#31181;&#23574;&#31471;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#35768;&#22810;&#33879;&#21517;&#30340;LLMs&#65292;&#22914;T5&#12289;GPT-J&#12289;LlaMA&#31561;&#12290;&#20174;&#32463;&#39564;&#19978;&#26469;&#30475;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;kno
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.07269v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged -- aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners from applying knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the kno
&lt;/p&gt;</description></item><item><title>&#25581;&#31034;&#20102;&#25345;&#32493;&#23398;&#20064;&#34920;&#31034;&#20013;&#30340;&#30693;&#35782;&#31215;&#32047;&#21644;&#29305;&#24449;&#36951;&#24536;&#38382;&#39064;&#65292;&#34920;&#26126;&#21363;&#20351;&#29305;&#24449;&#36951;&#24536;&#30340;&#32477;&#23545;&#31243;&#24230;&#21487;&#33021;&#36739;&#23567;&#65292;&#26032;&#23398;&#20064;&#30340;&#20449;&#24687;&#22312;&#34920;&#31034;&#23618;&#38754;&#20063;&#38754;&#20020;&#30528;&#20005;&#37325;&#36951;&#24536;&#12290;</title><link>https://arxiv.org/abs/2304.00933</link><description>&lt;p&gt;
&#22312;&#19981;&#26029;&#23398;&#20064;&#30340;&#34920;&#31034;&#20013;&#30340;&#30693;&#35782;&#31215;&#32047;&#19982;&#29305;&#24449;&#36951;&#24536;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Knowledge Accumulation in Continually Learned Representations and the Issue of Feature Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.00933
&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#20102;&#25345;&#32493;&#23398;&#20064;&#34920;&#31034;&#20013;&#30340;&#30693;&#35782;&#31215;&#32047;&#21644;&#29305;&#24449;&#36951;&#24536;&#38382;&#39064;&#65292;&#34920;&#26126;&#21363;&#20351;&#29305;&#24449;&#36951;&#24536;&#30340;&#32477;&#23545;&#31243;&#24230;&#21487;&#33021;&#36739;&#23567;&#65292;&#26032;&#23398;&#20064;&#30340;&#20449;&#24687;&#22312;&#34920;&#31034;&#23618;&#38754;&#20063;&#38754;&#20020;&#30528;&#20005;&#37325;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30740;&#31350;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#8220;&#36755;&#20986;&#32423;&#21035;&#8221;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20294;&#26377;&#20105;&#35758;&#30340;&#26159;&#26159;&#21542;&#22312;&#23398;&#20064;&#30340;&#34920;&#31034;&#32423;&#21035;&#20063;&#23384;&#22312;&#36825;&#31181;&#24773;&#20917;&#12290;&#22810;&#20010;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#34920;&#31034;&#24402;&#22240;&#20026;&#20855;&#26377;&#19968;&#23450;&#31243;&#24230;&#30340;&#22266;&#26377;&#25239;&#36951;&#24536;&#24615; - &#20165;&#20250;&#26368;&#23567;&#31243;&#24230;&#24536;&#35760;&#19988;&#19981;&#20250;&#36951;&#22833;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#24182;&#25193;&#23637;&#20102;&#25581;&#31034;&#36825;&#31181;&#36951;&#24536;&#24046;&#24322;&#30340;&#23454;&#39564;&#65292;&#35828;&#26126;&#20102;&#24433;&#21709;&#25345;&#32493;&#23398;&#20064;&#34920;&#31034;&#36136;&#37327;&#30340;&#20004;&#31181;&#29616;&#35937;&#20849;&#23384;&#65306;&#30693;&#35782;&#31215;&#32047;&#21644;&#29305;&#24449;&#36951;&#24536;&#12290;&#25105;&#20204;&#35880;&#24910;&#32771;&#34385;&#20102;&#36825;&#20004;&#20010;&#26041;&#38754;&#65292;&#34920;&#26126;&#23613;&#31649;&#32477;&#23545;&#20540;&#19978;&#29305;&#24449;&#36951;&#24536;&#21487;&#33021;&#36739;&#23567;&#65292;&#26032;&#23398;&#20064;&#30340;&#20449;&#24687;&#22312;&#34920;&#31034;&#23618;&#38754;&#19982;&#36755;&#20986;&#23618;&#38754;&#19968;&#26679;&#38754;&#20020;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29305;&#24449;&#36951;&#24536;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.00933v3 Announce Type: replace  Abstract: Continual learning research has shown that neural networks suffer from catastrophic forgetting "at the output level", but it is debated whether this is also the case at the level of learned representations. Multiple recent studies ascribe representations a certain level of innate robustness against forgetting - that they only forget minimally and no critical information. We revisit and expand upon the experiments that revealed this difference in forgetting and illustrate the coexistence of two phenomena that affect the quality of continually learned representations: knowledge accumulation and feature forgetting. Carefully taking both aspects into account, we show that, even though it is true that feature forgetting can be small in absolute terms, newly learned information tends to be forgotten just as catastrophically at the level of the representation as it is at the output level. Next we show that this feature forgetting is problem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20854;&#22312;&#29289;&#31181;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00608</link><description>&lt;p&gt;
&#23558;&#19978;&#19979;&#25991;&#24102;&#22238;&#26469;&#65306;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#30456;&#26426;&#38519;&#38449;&#29289;&#31181;&#35782;&#21035;&#20316;&#20026;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bringing Back the Context: Camera Trap Species Identification as Link Prediction on Multimodal Knowledge Graphs. (arXiv:2401.00608v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20854;&#22312;&#29289;&#31181;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#26426;&#38519;&#38449;&#22312;&#21160;&#29289;&#29983;&#24577;&#23398;&#20013;&#26159;&#23453;&#36149;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#29289;&#22810;&#26679;&#24615;&#30417;&#27979;&#21644;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22914;&#22312;&#26032;&#30340;&#26410;&#30693;&#20301;&#32622;&#37096;&#32626;&#26102;&#30340;&#31967;&#31957;&#27867;&#21270;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22270;&#20687;&#33258;&#28982;&#19982;&#21487;&#33021;&#22312;&#19981;&#21516;&#27169;&#24577;&#19979;&#30340;&#24322;&#36136;&#19978;&#19979;&#25991;&#30456;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19982;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30456;&#20851;&#32852;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#25913;&#21892;&#22312;&#30456;&#26426;&#38519;&#38449;&#20013;&#29289;&#31181;&#35782;&#21035;&#36825;&#20010;&#20219;&#21153;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#19968;&#24352;&#37326;&#29983;&#21160;&#29289;&#30340;&#29031;&#29255;&#21487;&#33021;&#19982;&#25293;&#25668;&#22320;&#28857;&#21644;&#26102;&#38388;&#20197;&#21450;&#20851;&#20110;&#21160;&#29289;&#29289;&#31181;&#30340;&#32467;&#26500;&#21270;&#29983;&#29289;&#23398;&#30693;&#35782;&#30456;&#20851;&#32852;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#24573;&#35270;&#36825;&#19968;&#28857;&#65292;&#20294;&#23558;&#36825;&#26679;&#30340;&#19978;&#19979;&#25991;&#24102;&#22238;&#26469;&#21487;&#20197;&#24102;&#26469;&#19968;&#20123;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#22914;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#21644;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#36825;&#26679;&#30340;&#24322;&#36136;&#19978;&#19979;&#25991;&#25972;&#21512;&#21040;&#35270;&#35273;&#39046;&#22495;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Camera traps are valuable tools in animal ecology for biodiversity monitoring and conservation. However, challenges like poor generalization to deployment at new unseen locations limit their practical application. Images are naturally associated with heterogeneous forms of context possibly in different modalities. In this work, we leverage the structured context associated with the camera trap images to improve out-of-distribution generalization for the task of species identification in camera traps. For example, a photo of a wild animal may be associated with information about where and when it was taken, as well as structured biology knowledge about the animal species. While typically overlooked by existing work, bringing back such context offers several potential benefits for better image understanding, such as addressing data scarcity and enhancing generalization. However, effectively integrating such heterogeneous context into the visual domain is a challenging problem. To address
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22330;&#26223;&#22270;&#30693;&#35782;&#25512;&#36827;&#20102;&#25163;&#26415;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#65292;&#35299;&#20915;&#20102;&#25163;&#26415;VQA&#31995;&#32479;&#20013;&#30340;&#38382;&#39064;&#26465;&#20214;&#20559;&#20506;&#21644;&#32570;&#20047;&#22330;&#26223;&#24863;&#30693;&#25512;&#29702;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2312.10251</link><description>&lt;p&gt;
&#36890;&#36807;&#22330;&#26223;&#22270;&#30693;&#35782;&#25512;&#36827;&#22806;&#31185;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Advancing Surgical VQA with Scene Graph Knowledge. (arXiv:2312.10251v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22330;&#26223;&#22270;&#30693;&#35782;&#25512;&#36827;&#20102;&#25163;&#26415;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#65292;&#35299;&#20915;&#20102;&#25163;&#26415;VQA&#31995;&#32479;&#20013;&#30340;&#38382;&#39064;&#26465;&#20214;&#20559;&#20506;&#21644;&#32570;&#20047;&#22330;&#26223;&#24863;&#30693;&#25512;&#29702;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25163;&#26415;&#23460;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#38656;&#35201;&#21019;&#26032;&#30340;&#26415;&#20013;&#25903;&#25345;&#31995;&#32479;&#12290;&#23613;&#31649;&#22806;&#31185;&#25968;&#25454;&#31185;&#23398;&#30340;&#37325;&#28857;&#20027;&#35201;&#22312;&#20110;&#35270;&#39057;&#20998;&#26512;&#65292;&#20294;&#23558;&#22806;&#31185;&#35745;&#31639;&#26426;&#35270;&#35273;&#19982;&#35821;&#35328;&#33021;&#21147;&#30456;&#32467;&#21512;&#25104;&#20026;&#24517;&#35201;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#22330;&#26223;&#22270;&#30693;&#35782;&#25512;&#36827;&#25163;&#26415;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#65292;&#35299;&#20915;&#24403;&#21069;&#25163;&#26415;VQA&#31995;&#32479;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#28040;&#38500;&#25163;&#26415;VQA&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#26465;&#20214;&#20559;&#20506;&#65292;&#20197;&#21450;&#22312;&#25163;&#26415;VQA&#27169;&#22411;&#35774;&#35745;&#20013;&#34701;&#20837;&#22330;&#26223;&#24863;&#30693;&#25512;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25163;&#26415;&#22330;&#26223;&#22270;&#30340;&#25968;&#25454;&#38598;SSG-QA&#65292;&#36890;&#36807;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#20998;&#21106;&#21644;&#26816;&#27979;&#27169;&#22411;&#26469;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#20202;&#22120;&#21644;&#35299;&#21078;&#32467;&#26500;&#30340;&#31354;&#38388;&#21644;&#21160;&#20316;&#20449;&#24687;&#26500;&#24314;&#25163;&#26415;&#22330;&#26223;&#22270;&#12290;&#36825;&#20123;&#22270;&#34987;&#36755;&#20837;&#21040;&#19968;&#20010;&#38382;&#39064;&#24341;&#25806;&#20013;&#65292;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#38382;&#31572;&#23545;&#12290;&#25105;&#20204;&#30340;SSG-QA&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#22797;&#26434;&#12289;&#22810;&#26679;&#21270;&#12289;&#20960;&#20309;&#22522;&#30784;&#12289;&#26080;&#20559;&#20506;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern operating room is becoming increasingly complex, requiring innovative intra-operative support systems. While the focus of surgical data science has largely been on video analysis, integrating surgical computer vision with language capabilities is emerging as a necessity. Our work aims to advance Visual Question Answering (VQA) in the surgical context with scene graph knowledge, addressing two main challenges in the current surgical VQA systems: removing question-condition bias in the surgical VQA dataset and incorporating scene-aware reasoning in the surgical VQA model design. First, we propose a Surgical Scene Graph-based dataset, SSG-QA, generated by employing segmentation and detection models on publicly available datasets. We build surgical scene graphs using spatial and action information of instruments and anatomies. These graphs are fed into a question engine, generating diverse QA pairs. Our SSG-QA dataset provides a more complex, diverse, geometrically grounded, unbiase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE&#26041;&#27861;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;ELBO&#65292;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21644;PatchGAN&#37492;&#21035;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#12290;</title><link>http://arxiv.org/abs/2309.13160</link><description>&lt;p&gt;
GAMIX-VAE: &#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE
&lt;/p&gt;
&lt;p&gt;
GAMIX-VAE: A VAE with Gaussian Mixture Based Posterior. (arXiv:2309.13160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE&#26041;&#27861;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;ELBO&#65292;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21644;PatchGAN&#37492;&#21035;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#29983;&#25104;&#24314;&#27169;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#22522;&#30707;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;VAEs&#30340;&#19968;&#20010;&#32454;&#24494;&#26041;&#38754;&#65292;&#37325;&#28857;&#26159;&#35299;&#37322;KL Divergence&#65292;&#36825;&#26159;Evidence Lower Bound&#65288;ELBO&#65289;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#25511;&#21046;&#20102;&#37325;&#26500;&#20934;&#30830;&#24615;&#21644;&#27491;&#21017;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#34429;&#28982;KL Divergence&#35753;&#28508;&#21464;&#37327;&#20998;&#24067;&#19982;&#20808;&#39564;&#20998;&#24067;&#23545;&#40784;&#65292;&#32473;&#25972;&#20010;&#28508;&#31354;&#38388;&#21152;&#19978;&#32467;&#26500;&#32422;&#26463;&#65292;&#20294;&#21364;&#19981;&#38480;&#21046;&#21508;&#20010;&#21464;&#37327;&#20998;&#24067;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37325;&#26032;&#23450;&#20041;&#20102;&#24102;&#26377;&#39640;&#26031;&#28151;&#21512;&#30340;&#21518;&#39564;&#27010;&#29575;&#30340;ELBO&#65292;&#24341;&#20837;&#20102;&#27491;&#21017;&#21270;&#39033;&#20197;&#38450;&#27490;&#26041;&#24046;&#23849;&#28291;&#65292;&#24182;&#20351;&#29992;PatchGAN&#37492;&#21035;&#22120;&#26469;&#22686;&#24378;&#32441;&#29702;&#36924;&#30495;&#24230;&#12290;&#23454;&#29616;&#32454;&#33410;&#28041;&#21450;Encoder&#21644;Decoder&#30340;ResNetV2&#26550;&#26500;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#30340;&#33021;&#21147;&#65292;&#20026;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational Autoencoders (VAEs) have become a cornerstone in generative modeling and representation learning within machine learning. This paper explores a nuanced aspect of VAEs, focusing on interpreting the Kullback Leibler (KL) Divergence, a critical component within the Evidence Lower Bound (ELBO) that governs the trade-off between reconstruction accuracy and regularization. While the KL Divergence enforces alignment between latent variable distributions and a prior imposing a structure on the overall latent space but leaves individual variable distributions unconstrained. The proposed method redefines the ELBO with a mixture of Gaussians for the posterior probability, introduces a regularization term to prevent variance collapse, and employs a PatchGAN discriminator to enhance texture realism. Implementation details involve ResNetV2 architectures for both the Encoder and Decoder. The experiments demonstrate the ability to generate realistic faces, offering a promising solution for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#65292;&#21457;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#24182;&#25552;&#21319;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20803;&#23398;&#20064;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2309.11433</link><description>&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Systematic Review of Few-Shot Learning in Medical Imaging. (arXiv:2309.11433v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#65292;&#21457;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#24182;&#25552;&#21319;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20803;&#23398;&#20064;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#26631;&#27880;&#30340;&#21307;&#23398;&#24433;&#20687;&#38480;&#21046;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#20943;&#23569;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#22686;&#24378;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#65292;&#23588;&#20854;&#26159;&#22312;&#20803;&#23398;&#20064;&#26041;&#38754;&#12290;&#26412;&#31995;&#32479;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#25628;&#32034;&#20102;&#30456;&#20851;&#25991;&#29486;&#65292;&#24182;&#20174;2018&#24180;&#21040;2023&#24180;&#36873;&#25321;&#20102;80&#31687;&#30456;&#20851;&#25991;&#31456;&#12290;&#25105;&#20204;&#22522;&#20110;&#21307;&#23398;&#32467;&#26524;&#65288;&#22914;&#32959;&#30244;&#20998;&#21106;&#12289;&#30142;&#30149;&#20998;&#31867;&#21644;&#22270;&#20687;&#37197;&#20934;&#65289;&#12289;&#30740;&#31350;&#30340;&#35299;&#21078;&#32467;&#26500;&#65288;&#21363;&#24515;&#33039;&#12289;&#32954;&#31561;&#65289;&#20197;&#21450;&#25152;&#20351;&#29992;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23545;&#36825;&#20123;&#25991;&#31456;&#36827;&#34892;&#20102;&#32858;&#31867;&#12290;&#23545;&#20110;&#27599;&#20010;&#32858;&#31867;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35770;&#25991;&#30340;&#20998;&#24067;&#20197;&#21450;&#26368;&#20808;&#36827;&#27169;&#22411;&#25552;&#20379;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#25152;&#26377;&#30740;&#31350;&#20013;&#30340;&#20849;&#20139;&#36890;&#29992;&#27969;&#31243;&#12290;&#32508;&#36848;&#34920;&#26126;&#23569;&#26679;&#26412;&#23398;&#20064;&#21487;&#20197;&#22312;&#22823;&#22810;&#25968;&#32467;&#26524;&#20013;&#20811;&#26381;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#19988;&#20803;&#23398;&#20064;&#26159;&#19968;&#20010;&#27969;&#34892;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of annotated medical images limits the performance of deep learning models, which usually need large-scale labelled datasets. Few-shot learning techniques can reduce data scarcity issues and enhance medical image analysis, especially with meta-learning. This systematic review gives a comprehensive overview of few-shot learning in medical imaging. We searched the literature systematically and selected 80 relevant articles published from 2018 to 2023. We clustered the articles based on medical outcomes, such as tumour segmentation, disease classification, and image registration; anatomical structure investigated (i.e. heart, lung, etc.); and the meta-learning method used. For each cluster, we examined the papers' distributions and the results provided by the state-of-the-art. In addition, we identified a generic pipeline shared among all the studies. The review shows that few-shot learning can overcome data scarcity in most outcomes and that meta-learning is a popular choice to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21160;&#24577;&#22270;&#65288;UniDG&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#34920;&#31034;&#32593;&#32476;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#24182;&#20445;&#30041;&#27169;&#24577;&#30340;&#29420;&#29305;&#24615;&#12290;&#37319;&#29992;&#21160;&#24577;&#22270;&#32858;&#31867;&#26041;&#27861;&#26500;&#24314;&#21487;&#38752;&#30340;&#20146;&#21644;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#24335;&#30340;&#20851;&#32852;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.14105</link><description>&lt;p&gt;
&#38271;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#24615;&#35282;&#33394;&#20998;&#32452;&#30340;&#32479;&#19968;&#21160;&#24577;&#22270;
&lt;/p&gt;
&lt;p&gt;
Unified and Dynamic Graph for Temporal Character Grouping in Long Videos. (arXiv:2308.14105v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21160;&#24577;&#22270;&#65288;UniDG&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#34920;&#31034;&#32593;&#32476;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#24182;&#20445;&#30041;&#27169;&#24577;&#30340;&#29420;&#29305;&#24615;&#12290;&#37319;&#29992;&#21160;&#24577;&#22270;&#32858;&#31867;&#26041;&#27861;&#26500;&#24314;&#21487;&#38752;&#30340;&#20146;&#21644;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#24335;&#30340;&#20851;&#32852;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#26102;&#38388;&#24615;&#35282;&#33394;&#20998;&#32452;&#26681;&#25454;&#35282;&#33394;&#30340;&#36523;&#20221;&#22312;&#35270;&#39057;&#20013;&#23450;&#20301;&#20986;&#29616;&#30340;&#26102;&#21051;&#12290; &#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20174;&#26080;&#30417;&#30563;&#32858;&#31867;&#21457;&#23637;&#21040;&#22522;&#20110;&#22270;&#30340;&#26377;&#30417;&#30563;&#32858;&#31867;&#12290; &#28982;&#32780;&#65292;&#22270;&#26041;&#27861;&#24314;&#31435;&#22312;&#22266;&#23450;&#30340;&#20146;&#21644;&#22270;&#21069;&#25552;&#19979;&#65292;&#24102;&#26469;&#20102;&#35768;&#22810;&#19981;&#31934;&#30830;&#30340;&#36830;&#25509;&#12290; &#27492;&#22806;&#65292;&#23427;&#20204;&#20351;&#29992;&#21508;&#31181;&#27169;&#22411;&#25552;&#21462;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#23545;&#37096;&#32626;&#19981;&#21451;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24615;&#35282;&#33394;&#20998;&#32452;&#30340;&#32479;&#19968;&#21160;&#24577;&#22270;&#65288;UniDG&#65289;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#34920;&#31034;&#32593;&#32476;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#21516;&#19968;&#31354;&#38388;&#20013;&#22810;&#20010;&#27169;&#24577;&#30340;&#34920;&#31034;&#65292;&#24182;&#21516;&#26102;&#20445;&#30041;&#20102;&#27169;&#24577;&#30340;&#29420;&#29305;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24490;&#29615;&#21305;&#37197;&#31574;&#30053;&#20026;&#27599;&#20010;&#33410;&#28857;&#21160;&#24577;&#26500;&#24314;&#19981;&#21516;&#25968;&#37327;&#30340;&#37051;&#23621;&#65292;&#20197;&#33719;&#24471;&#26356;&#21487;&#38752;&#30340;&#20146;&#21644;&#22270;&#12290; &#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28176;&#36827;&#24335;&#30340;&#20851;&#32852;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Video temporal character grouping locates appearing moments of major characters within a video according to their identities. To this end, recent works have evolved from unsupervised clustering to graph-based supervised clustering. However, graph methods are built upon the premise of fixed affinity graphs, bringing many inexact connections. Besides, they extract multi-modal features with kinds of models, which are unfriendly to deployment. In this paper, we present a unified and dynamic graph (UniDG) framework for temporal character grouping. This is accomplished firstly by a unified representation network that learns representations of multiple modalities within the same space and still preserves the modality's uniqueness simultaneously. Secondly, we present a dynamic graph clustering where the neighbors of different quantities are dynamically constructed for each node via a cyclic matching strategy, leading to a more reliable affinity graph. Thirdly, a progressive association method 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#25913;&#21464;&#25552;&#31034;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20419;&#36827;&#20102;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#32454;&#31890;&#24230;&#21644;&#30446;&#26631;&#21270;&#30340;&#25511;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#29983;&#25104;&#27169;&#22411;&#35270;&#20026;&#36830;&#32493;&#20989;&#25968;&#65292;&#24182;&#22312;&#22270;&#20687;&#31354;&#38388;&#21644;&#25552;&#31034;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#20256;&#36882;&#26799;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.12059</link><description>&lt;p&gt;
&#25805;&#20316;&#31283;&#23450;&#25193;&#25955;&#25552;&#31034;&#30340;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Manipulating Embeddings of Stable Diffusion Prompts. (arXiv:2308.12059v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#25913;&#21464;&#25552;&#31034;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20419;&#36827;&#20102;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#32454;&#31890;&#24230;&#21644;&#30446;&#26631;&#21270;&#30340;&#25511;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#29983;&#25104;&#27169;&#22411;&#35270;&#20026;&#36830;&#32493;&#20989;&#25968;&#65292;&#24182;&#22312;&#22270;&#20687;&#31354;&#38388;&#21644;&#25552;&#31034;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#20256;&#36882;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#12290;&#25913;&#21464;&#25552;&#31034;&#20173;&#28982;&#26159;&#29992;&#25143;&#24819;&#35201;&#25913;&#21464;&#29983;&#25104;&#22270;&#20687;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#37325;&#26032;&#26500;&#24605;&#25552;&#31034;&#26469;&#25913;&#21464;&#22270;&#20687;&#20173;&#28982;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#35797;&#38169;&#36807;&#31243;&#65292;&#36825;&#23548;&#33268;&#20102;&#25552;&#31034;&#24037;&#31243;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#30452;&#25509;&#25913;&#21464;&#25552;&#31034;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#25552;&#31034;&#25991;&#26412;&#12290;&#23427;&#20801;&#35768;&#26356;&#31934;&#32454;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25511;&#21046;&#65292;&#32771;&#34385;&#21040;&#29992;&#25143;&#30340;&#24847;&#22270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#35270;&#20026;&#19968;&#20010;&#36830;&#32493;&#20989;&#25968;&#65292;&#24182;&#22312;&#22270;&#20687;&#31354;&#38388;&#21644;&#25552;&#31034;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#20256;&#36882;&#26799;&#24230;&#12290;&#36890;&#36807;&#35299;&#20915;&#19981;&#21516;&#30340;&#29992;&#25143;&#20132;&#20114;&#38382;&#39064;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#19977;&#20010;&#22330;&#26223;&#20013;&#24212;&#29992;&#36825;&#20010;&#24819;&#27861;&#65306;&#65288;1&#65289;&#20248;&#21270;&#22270;&#20687;&#31354;&#38388;&#20013;&#23450;&#20041;&#30340;&#24230;&#37327;&#65292;&#21487;&#20197;&#27979;&#37327;&#22270;&#20687;&#39118;&#26684;&#31561;&#12290;&#65288;2&#65289;&#24110;&#21161;&#29992;&#25143;&#36827;&#34892;&#21019;&#36896;&#24615;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative text-to-image models such as Stable Diffusion allow users to generate images based on a textual description, the prompt. Changing the prompt is still the primary means for the user to change a generated image as desired. However, changing the image by reformulating the prompt remains a difficult process of trial and error, which has led to the emergence of prompt engineering as a new field of research. We propose and analyze methods to change the embedding of a prompt directly instead of the prompt text. It allows for more fine-grained and targeted control that takes into account user intentions. Our approach treats the generative text-to-image model as a continuous function and passes gradients between the image space and the prompt embedding space. By addressing different user interaction problems, we can apply this idea in three scenarios: (1) Optimization of a metric defined in image space that could measure, for example, image style. (2) Assistance of users in creative 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#33258;&#21160;&#20998;&#31867;&#21644;&#27169;&#25311;&#32676;&#20307;&#32769;&#40736;&#34892;&#20026;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#36328;&#31548;&#20351;&#29992;&#32622;&#25442;&#30697;&#38453;&#21305;&#37197;&#32769;&#40736;&#36523;&#20221;&#65292;&#22312;&#23478;&#40736;&#29615;&#22659;&#19979;&#30740;&#31350;&#32769;&#40736;&#21487;&#20197;&#25429;&#25417;&#21040;&#20010;&#20307;&#34892;&#20026;&#30340;&#26102;&#38388;&#22240;&#32032;&#65292;&#32780;&#19988;&#26080;&#38656;&#20154;&#20026;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2306.03066</link><description>&lt;p&gt;
&#12298;&#40736;&#31867;&#19982;&#37197;&#20598;&#65306;&#21333;&#19968;&#27169;&#22411;&#33258;&#21160;&#23545;&#32676;&#20307;&#20013;&#30340;&#32769;&#40736;&#34892;&#20026;&#36827;&#34892;&#20998;&#31867;&#21644;&#24314;&#27169;&#36328;&#31548;&#12299;
&lt;/p&gt;
&lt;p&gt;
Of Mice and Mates: Automated Classification and Modelling of Mouse Behaviour in Groups using a Single Model across Cages. (arXiv:2306.03066v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#33258;&#21160;&#20998;&#31867;&#21644;&#27169;&#25311;&#32676;&#20307;&#32769;&#40736;&#34892;&#20026;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#36328;&#31548;&#20351;&#29992;&#32622;&#25442;&#30697;&#38453;&#21305;&#37197;&#32769;&#40736;&#36523;&#20221;&#65292;&#22312;&#23478;&#40736;&#29615;&#22659;&#19979;&#30740;&#31350;&#32769;&#40736;&#21487;&#20197;&#25429;&#25417;&#21040;&#20010;&#20307;&#34892;&#20026;&#30340;&#26102;&#38388;&#22240;&#32032;&#65292;&#32780;&#19988;&#26080;&#38656;&#20154;&#20026;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#23454;&#39564;&#36890;&#24120;&#22312;&#19987;&#38376;&#30340;&#31454;&#25216;&#22330;&#20013;&#36827;&#34892;&#65292;&#20294;&#36825;&#21487;&#33021;&#20250;&#28151;&#28102;&#20998;&#26512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24037;&#20855;&#26469;&#30740;&#31350;&#23478;&#40736;&#29615;&#22659;&#20013;&#30340;&#32769;&#40736;&#65292;&#20026;&#29983;&#29289;&#23398;&#23478;&#25552;&#20379;&#20102;&#25429;&#25417;&#20010;&#20307;&#34892;&#20026;&#30340;&#26102;&#38388;&#22240;&#32032;&#21644;&#27169;&#25311;&#26368;&#23567;&#20154;&#20026;&#24178;&#39044;&#19979;&#31548;&#21451;&#20043;&#38388;&#20114;&#21160;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#8220;&#27963;&#21160;&#26631;&#31614;&#27169;&#22359;&#8221;&#65288;ALM&#65289;&#26469;&#33258;&#21160;&#23545;&#32769;&#40736;&#34892;&#20026;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#32676;&#20307;&#34892;&#20026;&#27169;&#22411;&#8221;&#65288;GBM&#65289;&#26469;&#27010;&#25324;&#20182;&#20204;&#22312;&#31548;&#23376;&#20013;&#30340;&#32852;&#21512;&#34892;&#20026;&#65292;&#20351;&#29992;&#32622;&#25442;&#30697;&#38453;&#23558;&#27599;&#20010;&#31548;&#23376;&#20013;&#30340;&#32769;&#40736;&#36523;&#20221;&#19982;&#27169;&#22411;&#21305;&#37197;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#34892;&#20026;&#20998;&#31867;&#22120;&#65288;ABODe&#65289;&#21644;&#34892;&#20026;&#24314;&#27169;&#65288;IMADGE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavioural experiments often happen in specialised arenas, but this may confound the analysis. To address this issue, we provide tools to study mice in the homecage environment, equipping biologists with the possibility to capture the temporal aspect of the individual's behaviour and model the interaction and interdependence between cage-mates with minimal human intervention. We develop the Activity Labelling Module (ALM) to automatically classify mouse behaviour from video, and a novel Group Behaviour Model (GBM) for summarising their joint behaviour across cages, using a permutation matrix to match the mouse identities in each cage to the model. We also release two datasets, ABODe for training behaviour classifiers and IMADGE for modelling behaviour.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#30001;&#20154;&#31867;&#26631;&#27880;&#30340;&#20114;&#34917;&#26631;&#31614;&#65292;&#21019;&#36896;&#20102;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;CLL&#25968;&#25454;&#38598;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#29616;&#23454;&#34920;&#29616;&#19979;CLL&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#23454;&#38469;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2305.08295</link><description>&lt;p&gt;
CLCIFAR&#65306;&#24102;&#20154;&#31867;&#26631;&#27880;&#20114;&#34917;&#26631;&#31614;&#30340;CIFAR&#27966;&#29983;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CLCIFAR: CIFAR-Derived Benchmark Datasets with Human Annotated Complementary Labels. (arXiv:2305.08295v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#30001;&#20154;&#31867;&#26631;&#27880;&#30340;&#20114;&#34917;&#26631;&#31614;&#65292;&#21019;&#36896;&#20102;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;CLL&#25968;&#25454;&#38598;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#29616;&#23454;&#34920;&#29616;&#19979;CLL&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#23454;&#38469;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#65288;CLL&#65289;&#26159;&#19968;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#20165;&#20351;&#29992;&#20114;&#34917;&#26631;&#31614;&#65288;&#26631;&#31034;&#23454;&#20363;&#19981;&#23646;&#20110;&#21738;&#20123;&#31867;&#21035;&#65289;&#26469;&#35757;&#32451;&#22810;&#31867;&#20998;&#31867;&#22120;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;CLL&#31639;&#27861;&#65292;&#20294;&#30001;&#20110;&#20004;&#20010;&#21407;&#22240;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#34920;&#29616;&#20173;&#19981;&#28165;&#26970;&#12290;&#39318;&#20808;&#65292;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#20114;&#34917;&#26631;&#31614;&#29983;&#25104;&#30340;&#20551;&#35774;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#20165;&#38480;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#33719;&#21462;&#26377;&#20851;CLL&#31639;&#27861;&#30340;&#30495;&#23454;&#19990;&#30028;&#34920;&#29616;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21327;&#35758;&#26469;&#25910;&#38598;&#30001;&#20154;&#31867;&#27880;&#37322;&#32773;&#27880;&#37322;&#30340;&#20114;&#34917;&#26631;&#31614;&#12290;&#36825;&#19968;&#21162;&#21147;&#23548;&#33268;&#21019;&#24314;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;CLCIFAR10&#21644;CLCIFAR20&#65292;&#20998;&#21035;&#30001;CIFAR10&#21644;CIFAR100&#27966;&#29983;&#32780;&#26469;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;https://github.com/ntucllab/complementary_cifar&#19978;&#20844;&#24320;&#21457;&#24067;&#65292;&#20195;&#34920;&#20102;&#31532;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;CLL&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#22522;&#20934;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#36739;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24403;&#20351;&#29992;&#20154;&#31867;&#27880;&#37322;&#30340;&#20114;&#34917;&#26631;&#31614;&#26102;&#65292;&#24615;&#33021;&#26377;&#26126;&#26174;&#19979;&#38477;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#20063;&#35266;&#23519;&#21040;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;CLL&#25968;&#25454;&#38598;&#20351;&#24471;&#22312;&#26356;&#25509;&#36817;&#23454;&#38469;&#24212;&#29992;&#26465;&#20214;&#19979;&#35780;&#20272;&#31639;&#27861;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#26356;&#30495;&#23454;&#22320;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complementary-label learning (CLL) is a weakly-supervised learning paradigm that aims to train a multi-class classifier using only complementary labels, which indicate classes to which an instance does not belong. Despite numerous algorithmic proposals for CLL, their practical performance remains unclear for two reasons. Firstly, these algorithms often rely on assumptions about the generation of complementary labels. Secondly, their evaluation has been limited to synthetic datasets. To gain insights into the real-world performance of CLL algorithms, we developed a protocol to collect complementary labels annotated by human annotators. This effort resulted in the creation of two datasets, CLCIFAR10 and CLCIFAR20, derived from CIFAR10 and CIFAR100, respectively. These datasets, publicly released at https://github.com/ntucllab/complementary_cifar, represent the very first real-world CLL datasets. Through extensive benchmark experiments, we discovered a notable decline in performance when 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLIPPINGS&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#35760;&#24405;&#38142;&#25509;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#23545;&#31216;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#22312;&#24230;&#37327;&#31354;&#38388;&#20013;&#23398;&#20064;&#30456;&#36817;&#25110;&#19981;&#21516;&#31867;&#21035;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#65292;&#22914;&#26500;&#24314;&#20840;&#38754;&#30340;&#34917;&#20805;&#19987;&#21033;&#27880;&#20876;&#34920;&#21644;&#35782;&#21035;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#20010;&#20154;&#12290;</title><link>http://arxiv.org/abs/2304.03464</link><description>&lt;p&gt;
&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#36830;&#25509;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Linking Representations with Multimodal Contrastive Learning. (arXiv:2304.03464v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLIPPINGS&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#35760;&#24405;&#38142;&#25509;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#23545;&#31216;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#22312;&#24230;&#37327;&#31354;&#38388;&#20013;&#23398;&#20064;&#30456;&#36817;&#25110;&#19981;&#21516;&#31867;&#21035;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#24212;&#29992;&#22330;&#26223;&#65292;&#22914;&#26500;&#24314;&#20840;&#38754;&#30340;&#34917;&#20805;&#19987;&#21033;&#27880;&#20876;&#34920;&#21644;&#35782;&#21035;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#20010;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#23558;&#21253;&#21547;&#22312;&#21508;&#31181;&#25991;&#26723;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#20998;&#32452;&#25104;&#31867;&#12290;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#19981;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65292;&#20063;&#19981;&#21033;&#29992;&#25991;&#26723;&#22266;&#26377;&#30340;&#22810;&#27169;&#24577;&#24615;&#36136;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35760;&#24405;&#38142;&#25509;&#36890;&#24120;&#34987;&#27010;&#24565;&#21270;&#20026;&#23383;&#31526;&#20018;&#21305;&#37197;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102; CLIPPINGS&#65292;&#19968;&#31181;&#29992;&#20110;&#35760;&#24405;&#38142;&#25509;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#12290;CLIPPINGS &#37319;&#29992;&#31471;&#21040;&#31471;&#35757;&#32451;&#23545;&#31216;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#21452;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#36827;&#34892;&#23545;&#40784;&#65292;&#23398;&#20064;&#19968;&#20010;&#24230;&#37327;&#31354;&#38388;&#65292;&#20854;&#20013;&#32473;&#23450;&#23454;&#20363;&#30340;&#27719;&#24635;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#38752;&#36817;&#21516;&#19968;&#31867;&#20013;&#30340;&#34920;&#31034;&#65292;&#24182;&#36828;&#31163;&#19981;&#21516;&#31867;&#20013;&#30340;&#34920;&#31034;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#31163;&#32447;&#31034;&#20363;&#23884;&#20837;&#32034;&#24341;&#20013;&#26816;&#32034;&#23427;&#20204;&#26368;&#36817;&#30340;&#37051;&#23621;&#25110;&#32858;&#31867;&#23427;&#20204;&#30340;&#34920;&#31034;&#26469;&#38142;&#25509;&#23454;&#20363;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24212;&#29992;&#65306;&#36890;&#36807;&#23558;&#19987;&#21033;&#19982;&#20854;&#23545;&#24212;&#30340;&#30417;&#31649;&#25991;&#20214;&#38142;&#25509;&#26469;&#26500;&#24314;&#20840;&#38754;&#30340;&#34917;&#20805;&#19987;&#21033;&#27880;&#20876;&#34920;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#35782;&#21035;&#20010;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications require grouping instances contained in diverse document datasets into classes. Most widely used methods do not employ deep learning and do not exploit the inherently multimodal nature of documents. Notably, record linkage is typically conceptualized as a string-matching problem. This study develops CLIPPINGS, (Contrastively Linking Pooled Pre-trained Embeddings), a multimodal framework for record linkage. CLIPPINGS employs end-to-end training of symmetric vision and language bi-encoders, aligned through contrastive language-image pre-training, to learn a metric space where the pooled image-text representation for a given instance is close to representations in the same class and distant from representations in different classes. At inference time, instances can be linked by retrieving their nearest neighbor from an offline exemplar embedding index or by clustering their representations. The study examines two challenging applications: constructing comprehensive suppl
&lt;/p&gt;</description></item></channel></rss>