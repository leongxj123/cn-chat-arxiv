<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#23545;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#35814;&#32454;&#35843;&#26597;, &#30528;&#37325;&#20171;&#32461;&#20102;&#20174;&#20256;&#32479;&#29983;&#25104;&#27169;&#22411;&#21040;&#23574;&#31471;Sora&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#24378;&#35843;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#36890;&#29992;&#24615;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.05131</link><description>&lt;p&gt;
Sora&#20316;&#20026;AGI&#19990;&#30028;&#27169;&#22411;&#65311;&#20851;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#23436;&#25972;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Sora as an AGI World Model? A Complete Survey on Text-to-Video Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05131
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#35814;&#32454;&#35843;&#26597;, &#30528;&#37325;&#20171;&#32461;&#20102;&#20174;&#20256;&#32479;&#29983;&#25104;&#27169;&#22411;&#21040;&#23574;&#31471;Sora&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#24378;&#35843;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#36890;&#29992;&#24615;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05131v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#26631;&#24535;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#21457;&#23637;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#21069;&#27839;&#65292;&#25972;&#21512;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#12289;&#35270;&#39057;&#23383;&#24149;&#21644;&#25991;&#26412;&#24341;&#23548;&#32534;&#36753;&#30340;&#36827;&#23637;&#12290;&#26412;&#35843;&#26597;&#23545;&#25991;&#26412;&#21040;&#35270;&#39057;&#25216;&#26415;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#23457;&#35270;&#65292;&#37325;&#28857;&#20851;&#27880;&#20256;&#32479;&#29983;&#25104;&#27169;&#22411;&#21521;&#23574;&#31471;Sora&#27169;&#22411;&#36716;&#21464;&#30340;&#36807;&#31243;&#65292;&#31361;&#20986;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#36890;&#29992;&#24615;&#30340;&#21457;&#23637;&#12290;&#21306;&#21035;&#20110;&#20197;&#24448;&#20316;&#21697;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25216;&#26415;&#26694;&#26550;&#21644;&#28436;&#21270;&#36335;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#35299;&#20915;&#20102;&#20262;&#29702;&#21644;&#25216;&#26415;&#25361;&#25112;&#65292;&#22914;&#26080;&#27861;&#25191;&#34892;&#22810;&#23454;&#20307;&#22788;&#29702;&#12289;&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;&#23398;&#20064;&#12289;&#29702;&#35299;&#29289;&#29702;&#20114;&#21160;&#12289;&#24863;&#30693;&#29289;&#20307;&#32553;&#25918;&#21644;&#27604;&#20363;&#20197;&#21450;&#23545;&#25239;&#29289;&#20307;&#24187;&#35273;&#65292;&#36825;&#20063;&#26159;&#29983;&#25104;&#27169;&#22411;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05131v1 Announce Type: new  Abstract: Text-to-video generation marks a significant frontier in the rapidly evolving domain of generative AI, integrating advancements in text-to-image synthesis, video captioning, and text-guided editing. This survey critically examines the progression of text-to-video technologies, focusing on the shift from traditional generative models to the cutting-edge Sora model, highlighting developments in scalability and generalizability. Distinguishing our analysis from prior works, we offer an in-depth exploration of the technological frameworks and evolutionary pathways of these models. Additionally, we delve into practical applications and address ethical and technological challenges such as the inability to perform multiple entity handling, comprehend causal-effect learning, understand physical interaction, perceive object scaling and proportioning, and combat object hallucination which is also a long-standing problem in generative models. Our c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;MRI&#20998;&#31867;&#20219;&#21153;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XAI&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#27604;&#31616;&#21333;&#27169;&#22411;&#25552;&#20379;&#26356;&#22909;&#30340;&#35299;&#37322;&#65292;&#19988;CNN&#30340;&#35299;&#37322;&#33021;&#21147;&#21462;&#20915;&#20110;&#24213;&#23618;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.12150</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#24433;&#21709;&#22240;&#32032;&#30740;&#31350;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#35299;&#37322;&#24615;&#33021;&#30340;&#22522;&#20934;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Benchmark data to study the influence of pre-training on explanation performance in MR image classification. (arXiv:2306.12150v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;MRI&#20998;&#31867;&#20219;&#21153;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XAI&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#27604;&#31616;&#21333;&#27169;&#22411;&#25552;&#20379;&#26356;&#22909;&#30340;&#35299;&#37322;&#65292;&#19988;CNN&#30340;&#35299;&#37322;&#33021;&#21147;&#21462;&#20915;&#20110;&#24213;&#23618;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#24120;&#24120;&#22312;&#21307;&#23398;&#39044;&#27979;&#20219;&#21153;&#20013;&#34987;&#25104;&#21151;&#22320;&#24212;&#29992;&#65292;&#36890;&#24120;&#19982;&#36801;&#31227;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#26102;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;CNN&#20135;&#29983;&#30340;&#27169;&#22411;&#39640;&#24230;&#22797;&#26434;&#19988;&#36890;&#24120;&#19981;&#25552;&#20379;&#20219;&#20309;&#26377;&#20851;&#20854;&#39044;&#27979;&#26426;&#21046;&#30340;&#20449;&#24687;&#65292;&#36825;&#20419;&#20351;&#20102;&#8220;&#21487;&#35299;&#37322;&#24615;&#8221;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22312;MRI&#20998;&#31867;&#20219;&#21153;&#20013;&#23450;&#37327;&#35780;&#20272;&#35299;&#37322;&#24615;&#33021;&#12290;&#36890;&#36807;&#36825;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#20197;&#20102;&#35299;&#36801;&#31227;&#23398;&#20064;&#23545;&#35299;&#37322;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24212;&#29992;&#20110;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;CNN&#30340;&#27969;&#34892;XAI&#26041;&#27861;&#24182;&#19981;&#19968;&#23450;&#27604;&#31616;&#21333;&#27169;&#22411;&#25552;&#20379;&#26356;&#22909;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;CNN&#25552;&#20379;&#26377;&#24847;&#20041;&#35299;&#37322;&#30340;&#33021;&#21147;&#20005;&#37325;&#20381;&#36182;&#20110;&#24213;&#23618;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks (CNNs) are frequently and successfully used in medical prediction tasks. They are often used in combination with transfer learning, leading to improved performance when training data for the task are scarce. The resulting models are highly complex and typically do not provide any insight into their predictive mechanisms, motivating the field of 'explainable' artificial intelligence (XAI). However, previous studies have rarely quantitatively evaluated the 'explanation performance' of XAI methods against ground-truth data, and transfer learning and its influence on objective measures of explanation performance has not been investigated. Here, we propose a benchmark dataset that allows for quantifying explanation performance in a realistic magnetic resonance imaging (MRI) classification task. We employ this benchmark to understand the influence of transfer learning on the quality of explanations. Experimental results show that popular XAI methods applied to t
&lt;/p&gt;</description></item></channel></rss>