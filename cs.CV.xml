<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>MIMIR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#20013;&#21033;&#29992;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#22686;&#24378;Vision Transformers&#65288;ViTs&#65289;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.04960</link><description>&lt;p&gt;
MIMIR: &#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness. (arXiv:2312.04960v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04960
&lt;/p&gt;
&lt;p&gt;
MIMIR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#20013;&#21033;&#29992;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#22686;&#24378;Vision Transformers&#65288;ViTs&#65289;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;ViTs&#65289;&#30456;&#23545;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;ViTs&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#23545;&#25239;&#24615;&#35757;&#32451;&#26159;&#24314;&#31435;&#24378;&#22823;&#30340;CNN&#27169;&#22411;&#30340;&#26368;&#25104;&#21151;&#26041;&#27861;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;ViTs&#21644;CNNs&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#22914;&#26356;&#22909;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#38450;&#27490;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#21333;&#20010;&#22359;&#19978;&#65292;&#25110;&#20002;&#24323;&#20302;&#27880;&#24847;&#21147;&#30340;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#36981;&#24490;&#20256;&#32479;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#30340;&#35774;&#35745;&#65292;&#38480;&#21046;&#20102;&#23545;ViTs&#30340;&#23545;&#25239;&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;MIMIR&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#20013;&#30340;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#26500;&#24314;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#25509;&#21463;&#23545;&#25239;&#24615;&#20363;&#23376;&#20316;&#20026;&#36755;&#20837;&#65292;&#20294;&#23558;&#24178;&#20928;&#30340;&#20363;&#23376;&#20316;&#20026;&#24314;&#27169;&#30446;&#26631;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20114;&#20449;&#24687;&#65288;MI&#65289;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) achieve superior performance on various tasks compared to convolutional neural networks (CNNs), but ViTs are also vulnerable to adversarial attacks. Adversarial training is one of the most successful methods to build robust CNN models. Thus, recent works explored new methodologies for adversarial training of ViTs based on the differences between ViTs and CNNs, such as better training strategies, preventing attention from focusing on a single block, or discarding low-attention embeddings. However, these methods still follow the design of traditional supervised adversarial training, limiting the potential of adversarial training on ViTs. This paper proposes a novel defense method, MIMIR, which aims to build a different adversarial training methodology by utilizing Masked Image Modeling at pre-training. We create an autoencoder that accepts adversarial examples as input but takes the clean examples as the modeling target. Then, we create a mutual information (MI
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22797;&#26434;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#30340;&#20915;&#31574;&#32452;&#20214;&#65292;&#36890;&#36807;&#35748;&#30693;&#36335;&#24452;&#21644;&#31639;&#27861;&#26469;&#23454;&#29616;&#20840;&#38754;&#25512;&#29702;&#21644;&#21487;&#25191;&#34892;&#39550;&#39542;&#25351;&#20196;&#30340;&#36716;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#33021;&#22815;&#22312;&#21333;&#36710;&#20219;&#21153;&#21644;&#22797;&#26434;&#39550;&#39542;&#34892;&#20026;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36825;&#26159;&#22240;&#20026;&#20854;&#20855;&#26377;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03026</link><description>&lt;p&gt;
LanguageMPC&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#32773;
&lt;/p&gt;
&lt;p&gt;
LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving. (arXiv:2310.03026v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22797;&#26434;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#30340;&#20915;&#31574;&#32452;&#20214;&#65292;&#36890;&#36807;&#35748;&#30693;&#36335;&#24452;&#21644;&#31639;&#27861;&#26469;&#23454;&#29616;&#20840;&#38754;&#25512;&#29702;&#21644;&#21487;&#25191;&#34892;&#39550;&#39542;&#25351;&#20196;&#30340;&#36716;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#33021;&#22815;&#22312;&#21333;&#36710;&#20219;&#21153;&#21644;&#22797;&#26434;&#39550;&#39542;&#34892;&#20026;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36825;&#26159;&#22240;&#20026;&#20854;&#20855;&#26377;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#22312;&#29702;&#35299;&#39640;&#32423;&#20449;&#24687;&#12289;&#25512;&#24191;&#32597;&#35265;&#20107;&#20214;&#21644;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22797;&#26434;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#30340;&#20915;&#31574;&#32452;&#20214;&#65292;&#38656;&#35201;&#20154;&#31867;&#24120;&#35782;&#29702;&#35299;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#35748;&#30693;&#36335;&#24452;&#65292;&#20351;LLMs&#33021;&#22815;&#36827;&#34892;&#20840;&#38754;&#25512;&#29702;&#65292;&#24182;&#24320;&#21457;&#20102;&#23558;LLM&#20915;&#31574;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#39550;&#39542;&#25351;&#20196;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;LLM&#20915;&#31574;&#36890;&#36807;&#24341;&#23548;&#21442;&#25968;&#30697;&#38453;&#36866;&#24212;&#19982;&#20302;&#32423;&#25511;&#21046;&#22120;&#26080;&#32541;&#38598;&#25104;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#21333;&#36710;&#20219;&#21153;&#20013;&#22987;&#32456;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#65292;&#32780;&#19988;&#36824;&#33021;&#22788;&#29702;&#22797;&#26434;&#30340;&#39550;&#39542;&#34892;&#20026;&#65292;&#29978;&#33267;&#22810;&#36710;&#21327;&#35843;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;LLMs&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;LLMs&#20316;&#20026;&#26377;&#25928;&#20915;&#31574;&#32773;&#30340;&#21021;&#27493;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing learning-based autonomous driving (AD) systems face challenges in comprehending high-level information, generalizing to rare events, and providing interpretability. To address these problems, this work employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios that require human commonsense understanding. We devise cognitive pathways to enable comprehensive reasoning with LLMs, and develop algorithms for translating LLM decisions into actionable driving commands. Through this approach, LLM decisions are seamlessly integrated with low-level controllers by guided parameter matrix adaptation. Extensive experiments demonstrate that our proposed method not only consistently surpasses baseline approaches in single-vehicle tasks, but also helps handle complex driving behaviors even multi-vehicle coordination, thanks to the commonsense reasoning capabilities of LLMs. This paper presents an initial step toward leveraging LLMs as effective decision-make
&lt;/p&gt;</description></item><item><title>Awesome-META+&#26159;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#38598;&#25104;&#21644;&#23398;&#20064;&#24179;&#21488;&#65292;&#26088;&#22312;&#25552;&#20379;&#23436;&#25972;&#21487;&#38752;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#24212;&#29992;&#21644;&#38754;&#21521;&#21021;&#23398;&#32773;&#30340;&#23398;&#20064;&#26448;&#26009;&#65292;&#36827;&#32780;&#20419;&#36827;&#20803;&#23398;&#20064;&#30340;&#21457;&#23637;&#24182;&#23558;&#20854;&#20174;&#23567;&#20247;&#39046;&#22495;&#36716;&#21270;&#20026;&#20027;&#27969;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.12921</link><description>&lt;p&gt;
Awesome-META+: &#20803;&#23398;&#20064;&#30740;&#31350;&#19982;&#23398;&#20064;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Awesome-META+: Meta-Learning Research and Learning Platform. (arXiv:2304.12921v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12921
&lt;/p&gt;
&lt;p&gt;
Awesome-META+&#26159;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#38598;&#25104;&#21644;&#23398;&#20064;&#24179;&#21488;&#65292;&#26088;&#22312;&#25552;&#20379;&#23436;&#25972;&#21487;&#38752;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#24212;&#29992;&#21644;&#38754;&#21521;&#21021;&#23398;&#32773;&#30340;&#23398;&#20064;&#26448;&#26009;&#65292;&#36827;&#32780;&#20419;&#36827;&#20803;&#23398;&#20064;&#30340;&#21457;&#23637;&#24182;&#23558;&#20854;&#20174;&#23567;&#20247;&#39046;&#22495;&#36716;&#21270;&#20026;&#20027;&#27969;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#22312;&#32463;&#27982;&#12289;&#20135;&#19994;&#12289;&#25945;&#32946;&#31561;&#21508;&#20010;&#39046;&#22495;&#20135;&#29983;&#20102;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#20294;&#36824;&#23384;&#22312;&#35832;&#22810;&#38480;&#21046;&#12290;&#20803;&#23398;&#20064;&#65292;&#20063;&#31216;&#20026;&#8220;&#23398;&#20064;&#22914;&#20309;&#23398;&#20064;&#8221;&#65292;&#20026;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#31361;&#30772;&#30446;&#21069;&#29942;&#39048;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#20803;&#23398;&#20064;&#36215;&#27493;&#36739;&#26202;&#65292;&#30456;&#27604;CV&#12289;NLP&#31561;&#39046;&#22495;&#65292;&#39033;&#30446;&#25968;&#37327;&#36739;&#23569;&#12290;&#27599;&#27425;&#37096;&#32626;&#37117;&#38656;&#35201;&#22823;&#37327;&#30340;&#32463;&#39564;&#21435;&#37197;&#32622;&#29615;&#22659;&#12289;&#35843;&#35797;&#20195;&#30721;&#29978;&#33267;&#37325;&#20889;&#65292;&#32780;&#19988;&#26694;&#26550;&#20043;&#38388;&#30456;&#23545;&#23396;&#31435;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#38024;&#23545;&#20803;&#23398;&#20064;&#30340;&#19987;&#38376;&#24179;&#21488;&#21644;&#38754;&#21521;&#21021;&#23398;&#32773;&#30340;&#23398;&#20064;&#26448;&#26009;&#30456;&#23545;&#36739;&#23569;&#65292;&#38376;&#27099;&#30456;&#23545;&#36739;&#39640;&#12290;&#22522;&#20110;&#27492;&#65292;Awesome-META+&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#38598;&#25104;&#21644;&#23398;&#20064;&#24179;&#21488;&#65292;&#26088;&#22312;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#24182;&#25552;&#20379;&#23436;&#25972;&#21487;&#38752;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#24212;&#29992;&#21644;&#23398;&#20064;&#24179;&#21488;&#12290;&#35813;&#39033;&#30446;&#26088;&#22312;&#20419;&#36827;&#20803;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#24182;&#23558;&#20854;&#20174;&#19968;&#20010;&#23567;&#20247;&#39046;&#22495;&#36716;&#21270;&#20026;&#19968;&#20010;&#20027;&#27969;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence technology has already had a profound impact in various fields such as economy, industry, and education, but still limited. Meta-learning, also known as "learning to learn", provides an opportunity for general artificial intelligence, which can break through the current AI bottleneck. However, meta learning started late and there are fewer projects compare with CV, NLP etc. Each deployment requires a lot of experience to configure the environment, debug code or even rewrite, and the frameworks are isolated. Moreover, there are currently few platforms that focus exclusively on meta-learning, or provide learning materials for novices, for which the threshold is relatively high. Based on this, Awesome-META+, a meta-learning framework integration and learning platform is proposed to solve the above problems and provide a complete and reliable meta-learning framework application and learning platform. The project aims to promote the development of meta-learning and t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#19979;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#36716;&#25442;&#24605;&#24819;&#21644;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#20307;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03807</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving CNN Training with Transfer Learning. (arXiv:2304.03807v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#19979;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#36716;&#25442;&#24605;&#24819;&#21644;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#20307;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#24050;&#32463;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#21516;&#26102;&#20445;&#25345;&#21516;&#24577;CNN&#35757;&#32451;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#23454;&#29616;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#30340;&#38544;&#31169;&#20445;&#25252;CNN&#35757;&#32451;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#25104;&#21151;&#31361;&#30772;&#36825;&#20010;&#38590;&#39064;&#65292;&#20197;&#21069;&#27809;&#26377;&#20219;&#20309;&#24037;&#20316;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#12290;&#37319;&#29992;&#20102;&#20960;&#31181;&#25216;&#26415;&#65306;&#65288;1&#65289;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#65292;&#21487;&#20197;&#23558;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;&#31616;&#21270;&#20026;&#21516;&#24577;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#29978;&#33267;&#26159;&#22810;&#31867;&#36923;&#36753;&#22238;&#24402;&#65288;MLR&#65289;&#35757;&#32451;&#65307;&#65288;2&#65289;&#36890;&#36807;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#20307;$\texttt{Quadratic Gradient}$&#65292;&#24212;&#29992;&#20110;MLR&#30340;&#22686;&#24378;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65307;&#65288;3&#65289;&#25105;&#20204;&#37319;&#29992;&#25968;&#23398;&#20013;&#30340;&#21464;&#25442;&#24605;&#24819;&#65292;&#23558;&#21152;&#23494;&#22495;&#20013;&#30340;&#36817;&#20284;Softmax&#20989;&#25968;&#36716;&#25442;&#25104;&#24050;&#32463;&#30740;&#31350;&#36807;&#30340;&#36924;&#36817;&#26041;&#27861;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving nerual network inference has been well studied while homomorphic CNN training still remains an open challenging task. In this paper, we present a practical solution to implement privacy-preserving CNN training based on mere Homomorphic Encryption (HE) technique. To our best knowledge, this is the first attempt successfully to crack this nut and no work ever before has achieved this goal. Several techniques combine to make it done: (1) with transfer learning, privacy-preserving CNN training can be reduced to homomorphic neural network training, or even multiclass logistic regression (MLR) training; (2) via a faster gradient variant called $\texttt{Quadratic Gradient}$, an enhanced gradient method for MLR with a state-of-the-art performance in converge speed is applied in this work to achieve high performance; (3) we employ the thought of transformation in mathematics to transform approximating Softmax function in encryption domain to the well-studied approximation of 
&lt;/p&gt;</description></item></channel></rss>