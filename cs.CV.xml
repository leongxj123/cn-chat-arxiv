<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>SMART&#21098;&#26525;&#22120;&#24341;&#20837;&#20102;&#29420;&#31435;&#30340;&#12289;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#25513;&#30721;&#12289;&#21487;&#24494;&#30340;Top k&#36816;&#31639;&#31526;&#21644;&#21160;&#24577;&#28201;&#24230;&#21442;&#25968;&#25216;&#24039;&#65292;&#22312;&#22359;&#21644;&#36755;&#20986;&#36890;&#36947;&#21098;&#26525;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19969</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#20998;&#31163;&#12289;&#21160;&#24577;&#21644;&#21487;&#24494;&#65288;SMART&#65289;&#21098;&#26525;&#22120;
&lt;/p&gt;
&lt;p&gt;
Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output Channel Pruning on Computer Vision Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19969
&lt;/p&gt;
&lt;p&gt;
SMART&#21098;&#26525;&#22120;&#24341;&#20837;&#20102;&#29420;&#31435;&#30340;&#12289;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#25513;&#30721;&#12289;&#21487;&#24494;&#30340;Top k&#36816;&#31639;&#31526;&#21644;&#21160;&#24577;&#28201;&#24230;&#21442;&#25968;&#25216;&#24039;&#65292;&#22312;&#22359;&#21644;&#36755;&#20986;&#36890;&#36947;&#21098;&#26525;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21098;&#26525;&#24050;&#34987;&#35270;&#20026;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#12289;&#25913;&#21892;&#25512;&#29702;&#24310;&#36831;&#20197;&#21450;&#38477;&#20302;DNN&#21152;&#36895;&#22120;&#21151;&#32791;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;&#22312;&#21508;&#31181;&#21098;&#26525;&#25216;&#26415;&#20013;&#65292;&#22359;&#21644;&#36755;&#20986;&#36890;&#36947;&#21098;&#26525;&#22312;&#21152;&#36895;&#30828;&#20214;&#24615;&#33021;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#36890;&#24120;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#31163;&#12289;&#21160;&#24577;&#21644;&#21487;&#24494;&#65288;SMART&#65289;&#21098;&#26525;&#22120;&#12290;&#35813;&#21098;&#26525;&#22120;&#21033;&#29992;&#19968;&#20010;&#29420;&#31435;&#30340;&#12289;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#25513;&#30721;&#36827;&#34892;&#26435;&#37325;&#37325;&#35201;&#24615;&#25490;&#24207;&#65292;&#37319;&#29992;&#21487;&#24494;&#30340;Top k&#36816;&#31639;&#31526;&#26469;&#23454;&#29616;&#30446;&#26631;&#31232;&#30095;&#24615;&#65292;&#24182;&#21033;&#29992;&#21160;&#24577;&#28201;&#24230;&#21442;&#25968;&#25216;&#24039;&#26469;&#36867;&#31163;&#38750;&#31232;&#30095;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;SMART&#21098;&#26525;&#22120;&#22312;&#22359;&#21644;&#36755;&#20986;&#36890;&#36947;&#21098;&#26525;&#30340;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#19978;&#19968;&#30452;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19969v1 Announce Type: cross  Abstract: Deep Neural Network (DNN) pruning has emerged as a key strategy to reduce model size, improve inference latency, and lower power consumption on DNN accelerators. Among various pruning techniques, block and output channel pruning have shown significant potential in accelerating hardware performance. However, their accuracy often requires further improvement. In response to this challenge, we introduce a separate, dynamic and differentiable (SMART) pruner. This pruner stands out by utilizing a separate, learnable probability mask for weight importance ranking, employing a differentiable Top k operator to achieve target sparsity, and leveraging a dynamic temperature parameter trick to escape from non-sparse local minima. In our experiments, the SMART pruner consistently demonstrated its superiority over existing pruning methods across a wide range of tasks and models on block and output channel pruning. Additionally, we extend our testing
&lt;/p&gt;</description></item><item><title>Calib3D&#26159;&#19968;&#20010;&#20174;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;&#22810;&#20010;3D&#22330;&#26223;&#29702;&#35299;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#34429;&#28982;&#20934;&#30830;&#20294;&#19981;&#21487;&#38752;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;&#23433;&#20840;&#20851;&#38190;&#30340;&#32972;&#26223;&#19979;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17010</link><description>&lt;p&gt;
Calib3D&#65306;&#26657;&#20934;&#27169;&#22411;&#20559;&#22909;&#20197;&#23454;&#29616;&#21487;&#38752;&#30340;3D&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Calib3D: Calibrating Model Preferences for Reliable 3D Scene Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17010
&lt;/p&gt;
&lt;p&gt;
Calib3D&#26159;&#19968;&#20010;&#20174;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;&#22810;&#20010;3D&#22330;&#26223;&#29702;&#35299;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#34429;&#28982;&#20934;&#30830;&#20294;&#19981;&#21487;&#38752;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;&#23433;&#20840;&#20851;&#38190;&#30340;&#32972;&#26223;&#19979;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#20851;&#38190;&#30340;3D&#22330;&#26223;&#29702;&#35299;&#20219;&#21153;&#38656;&#35201;&#30340;&#19981;&#20165;&#20165;&#26159;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#36824;&#38656;&#35201;&#26469;&#33258;3D&#24863;&#30693;&#27169;&#22411;&#30340;&#33258;&#20449;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#25512;&#20986;&#20102;Calib3D&#65292;&#36825;&#26159;&#19968;&#39033;&#24320;&#21019;&#24615;&#30340;&#24037;&#20316;&#65292;&#26088;&#22312;&#20174;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#35282;&#24230;&#22522;&#20934;&#21644;&#23457;&#26597;3D&#22330;&#26223;&#29702;&#35299;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;28&#20010;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;10&#20010;&#19981;&#21516;&#30340;3D&#25968;&#25454;&#38598;&#19978;&#65292;&#25581;&#31034;&#20102;&#33021;&#22815;&#22788;&#29702;3D&#22330;&#26223;&#29702;&#35299;&#20013;&#30340;&#35823;&#24046;&#19981;&#30830;&#23450;&#24615;&#21644;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#35265;&#22320;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#29616;&#26377;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#24230;&#27700;&#24179;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745; -- &#36825;&#20010;&#20851;&#38190;&#30340;&#32570;&#38519;&#20005;&#37325;&#25439;&#23475;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#25935;&#24863;&#29615;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#23545;&#20851;&#38190;&#22240;&#32032;&#65288;&#22914;&#32593;&#32476;&#23481;&#37327;&#12289;LiDAR&#34920;&#31034;&#12289;&#20809;&#26629;&#20998;&#36776;&#29575;&#21644;3D&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#25105;&#20204;&#30452;&#25509;&#23558;&#36825;&#20123;&#26041;&#38754;&#19982;&#27169;&#22411;&#26657;&#20934;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17010v1 Announce Type: cross  Abstract: Safety-critical 3D scene understanding tasks necessitate not only accurate but also confident predictions from 3D perception models. This study introduces Calib3D, a pioneering effort to benchmark and scrutinize the reliability of 3D scene understanding models from an uncertainty estimation viewpoint. We comprehensively evaluate 28 state-of-the-art models across 10 diverse 3D datasets, uncovering insightful phenomena that cope with both the aleatoric and epistemic uncertainties in 3D scene understanding. We discover that despite achieving impressive levels of accuracy, existing models frequently fail to provide reliable uncertainty estimates -- a pitfall that critically undermines their applicability in safety-sensitive contexts. Through extensive analysis of key factors such as network capacity, LiDAR representations, rasterization resolutions, and 3D data augmentation techniques, we correlate these aspects directly with the model cal
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#39062;&#30340;Extract and Explore&#65288;EX2&#65289;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20013;&#65292;&#37325;&#35201;&#30340;&#29305;&#24449;&#25551;&#36848;&#21253;&#25324;&#38750;&#35270;&#35273;&#23646;&#24615;&#65292;&#34394;&#20551;&#25551;&#36848;&#24433;&#21709;VLM&#34920;&#31034;&#65292;&#19981;&#21516;&#30340;VLM&#20248;&#20808;&#32771;&#34385;&#19981;&#21516;&#30340;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2403.16442</link><description>&lt;p&gt;
&#22914;&#26524;CLIP&#33021;&#35828;&#35805;: &#36890;&#36807;&#23427;&#20204;&#30340;&#39318;&#36873;&#27010;&#24565;&#25551;&#36848;&#29702;&#35299;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16442
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#39062;&#30340;Extract and Explore&#65288;EX2&#65289;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20013;&#65292;&#37325;&#35201;&#30340;&#29305;&#24449;&#25551;&#36848;&#21253;&#25324;&#38750;&#35270;&#35273;&#23646;&#24615;&#65292;&#34394;&#20551;&#25551;&#36848;&#24433;&#21709;VLM&#34920;&#31034;&#65292;&#19981;&#21516;&#30340;VLM&#20248;&#20808;&#32771;&#34385;&#19981;&#21516;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24120;&#24120;&#20551;&#35774;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#34920;&#31034;&#26159;&#22522;&#20110;&#24418;&#29366;&#31561;&#35270;&#35273;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;VLM&#22312;&#34920;&#31034;&#27010;&#24565;&#26102;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#23558;&#36825;&#20123;&#20449;&#24687;&#20316;&#20026;&#20248;&#20808;&#32771;&#34385;&#23545;&#35937;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Extract and Explore&#65288;EX2&#65289;&#65292;&#29992;&#20110;&#21051;&#30011;VLM&#30340;&#37325;&#35201;&#25991;&#26412;&#29305;&#24449;&#12290;EX2&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23558;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;VLM&#39318;&#36873;&#39033;&#23545;&#40784;&#65292;&#24182;&#29983;&#25104;&#21253;&#21547;VLM&#37325;&#35201;&#29305;&#24449;&#30340;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26816;&#26597;&#36825;&#20123;&#25551;&#36848;&#20197;&#30830;&#23450;&#23545;VLM&#34920;&#31034;&#26377;&#36129;&#29486;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#25552;&#20379;&#20102;&#27809;&#26377;&#24110;&#21161;&#20449;&#24687;&#30340;&#34394;&#20551;&#25551;&#36848;&#65288;&#20363;&#22914;&#65292;&#21333;&#20987;&#25918;&#22823;&#27010;&#24565;&#30340;&#29031;&#29255;&#65289;&#65292;&#20294;&#22312;VLM&#34920;&#31034;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#20449;&#24687;&#20016;&#23500;&#30340;&#25551;&#36848;&#20013;&#65292;VLM&#22312;&#34920;&#31034;&#35270;&#35273;&#27010;&#24565;&#26102;&#26174;&#33879;&#20381;&#36182;&#38750;&#35270;&#35273;&#23646;&#24615;&#65288;&#22914;&#26646;&#24687;&#22320;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19981;&#21516;&#30340;VLM&#20248;&#20808;&#32771;&#34385;&#19981;&#21516;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16442v1 Announce Type: new  Abstract: Recent works often assume that Vision-Language Model (VLM) representations are based on visual attributes like shape. However, it is unclear to what extent VLMs prioritize this information to represent concepts. We propose Extract and Explore (EX2), a novel approach to characterize important textual features for VLMs. EX2 uses reinforcement learning to align a large language model with VLM preferences and generates descriptions that incorporate the important features for the VLM. Then, we inspect the descriptions to identify the features that contribute to VLM representations. We find that spurious descriptions have a major role in VLM representations despite providing no helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly, among informative descriptions, VLMs rely significantly on non-visual attributes like habitat to represent visual concepts. Also, our analysis reveals that different VLMs prioritize differen
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;</title><link>https://arxiv.org/abs/2403.12027</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#21040;&#27934;&#23519;: &#22312;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12027
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21487;&#35270;&#21270;&#20197;&#22270;&#34920;&#24418;&#24335;&#22312;&#25968;&#25454;&#20998;&#26512;&#20013;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#65292;&#25552;&#20379;&#20851;&#38190;&#27934;&#23519;&#24182;&#24110;&#21161;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#38543;&#30528;&#36817;&#24180;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38761;&#21629;&#65292;&#24182;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#20219;&#21153;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#25506;&#35752;&#20102;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12027v1 Announce Type: cross  Abstract: Data visualization in the form of charts plays a pivotal role in data analysis, offering critical insights and aiding in informed decision-making. Automatic chart understanding has witnessed significant advancements with the rise of large foundation models in recent years. Foundation models, such as large language models (LLMs), have revolutionized various natural language processing (NLP) tasks and are increasingly being applied to chart understanding tasks. This survey paper provides a comprehensive overview of the recent developments, challenges, and future directions in chart understanding within the context of these foundation models. The paper begins by defining chart understanding, outlining problem formulations, and discussing fundamental building blocks crucial for studying chart understanding tasks. In the section on tasks and datasets, we explore various tasks within chart understanding and discuss their evaluation metrics a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#24418;&#36716;&#25442;&#22120;&#30340;&#36328;&#22495;&#21644;&#36328;&#32500;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#27491;&#21017;&#21270;&#36793;&#32536;&#37319;&#26679;&#25439;&#22833;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#21644;&#31616;&#21333;&#30340;&#25237;&#24433;&#20989;&#25968;&#65292;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06601</link><description>&lt;p&gt;
&#22270;&#20687;&#21040;&#22270;&#24418;&#21464;&#25442;&#20013;&#30340;&#36328;&#22495;&#21644;&#36328;&#32500;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-domain and Cross-dimension Learning for Image-to-Graph Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06601
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#24418;&#36716;&#25442;&#22120;&#30340;&#36328;&#22495;&#21644;&#36328;&#32500;&#24230;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#27491;&#21017;&#21270;&#36793;&#32536;&#37319;&#26679;&#25439;&#22833;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#21644;&#31616;&#21333;&#30340;&#25237;&#24433;&#20989;&#25968;&#65292;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#30340;&#22270;&#20687;&#21040;&#22270;&#24418;&#36716;&#25442;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#22312;&#21333;&#20010;&#27169;&#22411;&#20013;&#35299;&#20915;&#20102;&#30446;&#26631;&#26816;&#27979;&#21644;&#20851;&#31995;&#39044;&#27979;&#12290;&#30001;&#20110;&#36825;&#20010;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24456;&#38590;&#25214;&#21040;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36825;&#20351;&#24471;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#31181;&#25968;&#25454;&#31232;&#30095;&#24615;&#38656;&#35201;&#24314;&#31435;&#31867;&#20284;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#21040;&#22270;&#24418;&#36716;&#25442;&#22120;&#30340;&#36328;&#22495;&#21644;&#36328;&#32500;&#24230;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;(1) &#27491;&#21017;&#21270;&#36793;&#32536;&#37319;&#26679;&#25439;&#22833;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#37319;&#26679;&#26368;&#20339;&#25968;&#37327;&#30340;&#30446;&#26631;&#20851;&#31995;(&#36793;&#32536;)&#65292;(2) &#19968;&#31181;&#22270;&#20687;&#21040;&#22270;&#24418;&#36716;&#25442;&#22120;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#40784;&#19981;&#21516;&#39046;&#22495;&#30340;&#29305;&#24449;&#65292;&#21644;(3) &#19968;&#31181;&#31616;&#21333;&#30340;&#25237;&#24433;&#20989;&#25968;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#20108;&#32500;&#36755;&#20837;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#19977;&#32500;&#36716;&#25442;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36328;&#22495;&#21644;&#36328;&#32500;&#24230;&#19979;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06601v1 Announce Type: cross  Abstract: Direct image-to-graph transformation is a challenging task that solves object detection and relationship prediction in a single model. Due to the complexity of this task, large training datasets are rare in many domains, which makes the training of large networks challenging. This data sparsity necessitates the establishment of pre-training strategies akin to the state-of-the-art in computer vision. In this work, we introduce a set of methods enabling cross-domain and cross-dimension transfer learning for image-to-graph transformers. We propose (1) a regularized edge sampling loss for sampling the optimal number of object relationships (edges) across domains, (2) a domain adaptation framework for image-to-graph transformers that aligns features from different domains, and (3) a simple projection function that allows us to pretrain 3D transformers on 2D input data. We demonstrate our method's utility in cross-domain and cross-dimension 
&lt;/p&gt;</description></item><item><title>DeiSAM&#25552;&#20986;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#19982;&#21487;&#21306;&#20998;&#36923;&#36753;&#25512;&#29702;&#22120;&#32467;&#21512;&#65292;&#29992;&#20110;&#25351;&#31034;&#25552;&#31034;&#24615;&#20998;&#21106;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#23545;&#35937;&#30340;&#20998;&#21106;</title><link>https://arxiv.org/abs/2402.14123</link><description>&lt;p&gt;
DeiSAM&#65306;&#36890;&#36807;&#25351;&#31034;&#25552;&#31034;&#20998;&#21106;&#20219;&#20309;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
DeiSAM: Segment Anything with Deictic Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14123
&lt;/p&gt;
&lt;p&gt;
DeiSAM&#25552;&#20986;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#19982;&#21487;&#21306;&#20998;&#36923;&#36753;&#25512;&#29702;&#22120;&#32467;&#21512;&#65292;&#29992;&#20110;&#25351;&#31034;&#25552;&#31034;&#24615;&#20998;&#21106;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#23545;&#35937;&#30340;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#38646;-shot&#22270;&#20687;&#20998;&#21106;&#12290;&#20026;&#20102;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#35782;&#21035;&#20855;&#20307;&#23545;&#35937;&#65292;&#20154;&#31867;&#26412;&#33021;&#22320;&#20381;&#36182;&#20110;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#25351;&#31034;&#24615;&#25551;&#36848;&#65292;&#21363;&#26681;&#25454;&#19978;&#19979;&#25991;&#25351;&#31216;&#26576;&#29289;&#65292;&#27604;&#22914;&#8220;&#22312;&#26700;&#23376;&#19978;&#24182;&#22312;&#26479;&#23376;&#21518;&#38754;&#30340;&#29289;&#20307;&#8221;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30001;&#20110;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#32570;&#20047;&#25512;&#29702;&#33021;&#21147;&#65292;&#26080;&#27861;&#21487;&#38752;&#22320;&#35299;&#37322;&#36825;&#31181;&#25351;&#31034;&#24615;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeiSAM&#8212;&#8212;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#19982;&#21487;&#21306;&#20998;&#36923;&#36753;&#25512;&#29702;&#22120;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#25351;&#31034;&#25552;&#31034;&#24615;&#20998;&#21106;&#12290;&#32473;&#23450;&#22797;&#26434;&#30340;&#25991;&#26412;&#20998;&#21106;&#25551;&#36848;&#65292;DeiSAM&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19968;&#38454;&#36923;&#36753;&#35268;&#21017;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#22330;&#26223;&#22270;&#36827;&#34892;&#21487;&#21306;&#20998;&#30340;&#21069;&#21521;&#25512;&#29702;&#12290;&#38543;&#21518;&#65292;DeiSAM&#36890;&#36807;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14123v1 Announce Type: cross  Abstract: Large-scale, pre-trained neural networks have demonstrated strong capabilities in various tasks, including zero-shot image segmentation. To identify concrete objects in complex scenes, humans instinctively rely on deictic descriptions in natural language, i.e., referring to something depending on the context such as "The object that is on the desk and behind the cup.". However, deep learning approaches cannot reliably interpret such deictic representations due to their lack of reasoning capabilities in complex scenarios. To remedy this issue, we propose DeiSAM -- a combination of large pre-trained neural networks with differentiable logic reasoners -- for deictic promptable segmentation. Given a complex, textual segmentation description, DeiSAM leverages Large Language Models (LLMs) to generate first-order logic rules and performs differentiable forward reasoning on generated scene graphs. Subsequently, DeiSAM segments objects by match
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#36890;&#36807;&#26500;&#24314;&#27602;&#32032;-&#28165;&#27905;&#25552;&#31034;&#23545;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807; Proximal Policy Optimization &#35757;&#32451;&#20248;&#21270;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#21508;&#31181; T2I &#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10882</link><description>&lt;p&gt;
&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#29992;&#20110;&#23433;&#20840;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Universal Prompt Optimizer for Safe Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10882
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#36890;&#36807;&#26500;&#24314;&#27602;&#32032;-&#28165;&#27905;&#25552;&#31034;&#23545;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807; Proximal Policy Optimization &#35757;&#32451;&#20248;&#21270;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#21508;&#31181; T2I &#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#22312;&#26681;&#25454;&#25991;&#23383;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#19981;&#23433;&#20840;&#36755;&#20837;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#65292;&#22914;&#33394;&#24773;&#12289;&#39578;&#25200;&#21644;&#38750;&#27861;&#27963;&#21160;&#22270;&#20687;&#12290;&#22522;&#20110;&#22270;&#20687;&#26816;&#26597;&#22120;&#12289;&#27169;&#22411;&#24494;&#35843;&#21644;&#23884;&#20837;&#24335;&#38459;&#27490;&#30340;&#29616;&#26377;&#30740;&#31350;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840; T2I &#29983;&#25104;&#30340;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10882v1 Announce Type: cross  Abstract: Text-to-Image (T2I) models have shown great performance in generating images based on textual prompts. However, these models are vulnerable to unsafe input to generate unsafe content like sexual, harassment and illegal-activity images. Existing studies based on image checker, model fine-tuning and embedding blocking are impractical in real-world applications. Hence, \textit{we propose the first universal prompt optimizer for safe T2I generation in black-box scenario}. We first construct a dataset consisting of toxic-clean prompt pairs by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting toxic prompt to clean prompt while preserving semantic information, we design a novel reward function measuring toxicity and text alignment of generated images and train the optimizer through Proximal Policy Optimization. Experiments show that our approach can effectively reduce the likelihood of various T2I models in generating in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CREMA&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65292;&#24182;&#24341;&#20837;&#26597;&#35810;&#36716;&#25442;&#22120;&#21644;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.05889</link><description>&lt;p&gt;
CREMA: &#36890;&#36807;&#26377;&#25928;&#30340;&#27169;&#22359;&#21270;&#36866;&#24212;&#21644;&#34701;&#21512;&#36827;&#34892;&#22810;&#27169;&#24577;&#32452;&#21512;&#35270;&#39057;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CREMA&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65292;&#24182;&#24341;&#20837;&#26597;&#35810;&#36716;&#25442;&#22120;&#21644;&#34701;&#21512;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#19988;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#32452;&#21512;&#25512;&#29702;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#22788;&#29702;&#22266;&#23450;&#27169;&#24577;&#36755;&#20837;&#24182;&#26356;&#26032;&#35768;&#22810;&#27169;&#22411;&#21442;&#25968;&#65292;&#20173;&#28982;&#23384;&#22312;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;CREMA&#65292;&#19968;&#31181;&#29992;&#20110;&#23558;&#20219;&#20309;&#26032;&#30340;&#27169;&#24577;&#27880;&#20837;&#35270;&#39057;&#25512;&#29702;&#30340;&#39640;&#25928;&#19988;&#27169;&#22359;&#21270;&#30340;&#27169;&#24577;&#34701;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#32473;&#23450;&#30340;&#35270;&#39057;&#20013;&#22686;&#24378;&#22810;&#31181;&#20449;&#24687;&#27169;&#24577;&#65288;&#22914;&#20809;&#27969;&#12289;3D&#28857;&#20113;&#12289;&#38899;&#39057;&#65289;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26597;&#35810;&#36716;&#25442;&#22120;&#65292;&#35813;&#36716;&#25442;&#22120;&#19982;&#27599;&#20010;&#21487;&#20197;&#35775;&#38382;&#30340;&#27169;&#24577;&#30456;&#20851;&#32852;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22359;&#12290;&#23427;&#23558;&#22810;&#31181;&#27169;&#24577;&#29305;&#24449;&#25237;&#24433;&#21040;LLM&#20196;&#29260;&#23884;&#20837;&#31354;&#38388;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#25968;&#25454;&#31867;&#22411;&#20197;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#27169;&#22359;&#65292;&#29992;&#20110;&#21387;&#32553;&#22810;&#27169;&#24577;&#26597;&#35810;&#65292;&#22312;LLM&#20013;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#36827;&#34892;&#34701;&#21512;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite impressive advancements in multimodal compositional reasoning approaches, they are still limited in their flexibility and efficiency by processing fixed modality inputs while updating a lot of model parameters. This paper tackles these critical challenges and proposes CREMA, an efficient and modular modality-fusion framework for injecting any new modality into video reasoning. We first augment multiple informative modalities (such as optical flow, 3D point cloud, audio) from given videos without extra human annotation by leveraging existing pre-trained models. Next, we introduce a query transformer with multiple parameter-efficient modules associated with each accessible modality. It projects diverse modality features to the LLM token embedding space, allowing the model to integrate different data types for response generation. Furthermore, we propose a fusion module designed to compress multimodal queries, maintaining computational efficiency in the LLM while combining additio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;K&#31354;&#38388;&#36827;&#34892;&#22270;&#20687;&#36864;&#21270;&#21644;&#24674;&#22797;&#30340;&#20919;&#25193;&#25955;&#27169;&#22411;&#65292;&#26080;&#38656;&#22122;&#38899;&#65292;&#33021;&#22815;&#20026;&#21152;&#36895;MRI&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#22270;&#20687;</title><link>https://arxiv.org/abs/2311.10162</link><description>&lt;p&gt;
K&#31354;&#38388;&#20919;&#25193;&#25955;&#65306;&#23398;&#20064;&#22312;&#27809;&#26377;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#37325;&#24314;&#21152;&#36895;MRI
&lt;/p&gt;
&lt;p&gt;
K-space Cold Diffusion: Learning to Reconstruct Accelerated MRI without Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10162
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;K&#31354;&#38388;&#36827;&#34892;&#22270;&#20687;&#36864;&#21270;&#21644;&#24674;&#22797;&#30340;&#20919;&#25193;&#25955;&#27169;&#22411;&#65292;&#26080;&#38656;&#22122;&#38899;&#65292;&#33021;&#22815;&#20026;&#21152;&#36895;MRI&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;MRI&#37325;&#24314;&#27169;&#22411;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#12289;&#20462;&#34917;&#12289;&#36229;&#20998;&#36776;&#29575;&#12289;&#22270;&#20687;&#32534;&#36753;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20919;&#25193;&#25955;&#36827;&#19968;&#27493;&#25299;&#23485;&#20102;&#33539;&#22260;&#65292;&#24182;&#32771;&#34385;&#20102;&#22260;&#32469;&#20219;&#24847;&#22270;&#20687;&#21464;&#25442;&#26500;&#24314;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;&#27169;&#31946;&#12289;&#19979;&#37319;&#26679;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;K&#31354;&#38388;&#20013;&#25191;&#34892;&#22270;&#20687;&#36864;&#21270;&#21644;&#24674;&#22797;&#30340;K&#31354;&#38388;&#20919;&#25193;&#25955;&#27169;&#22411;&#65292;&#26080;&#38656;&#39640;&#26031;&#22122;&#22768;&#12290;&#25105;&#20204;&#19982;&#22810;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;MRI&#37325;&#24314;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#22312;&#19968;&#20010;&#30693;&#21517;&#30340;&#22823;&#22411;&#24320;&#28304;MRI&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#39062;&#30340;&#36864;&#21270;&#26041;&#24335;&#21487;&#20197;&#20026;&#21152;&#36895;MRI&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10162v2 Announce Type: replace-cross  Abstract: Deep learning-based MRI reconstruction models have achieved superior performance these days. Most recently, diffusion models have shown remarkable performance in image generation, in-painting, super-resolution, image editing and more. As a generalized diffusion model, cold diffusion further broadens the scope and considers models built around arbitrary image transformations such as blurring, down-sampling, etc. In this paper, we propose a k-space cold diffusion model that performs image degradation and restoration in k-space without the need for Gaussian noise. We provide comparisons with multiple deep learning-based MRI reconstruction models and perform tests on a well-known large open-source MRI dataset. Our results show that this novel way of performing degradation can generate high-quality reconstruction images for accelerated MRI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20026;&#21367;&#31215;&#23618;&#25552;&#20379;&#20855;&#26377;&#30456;&#23545;$n$&#32500;&#31515;&#21345;&#23572;&#22352;&#26631;&#31995;&#30340;&#21333;&#19968;&#36755;&#20837;&#36890;&#36947;&#65292;&#21487;&#20197;&#32531;&#35299;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26080;&#27861;&#37325;&#29616;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;GAN&#21644;VAE&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#38754;&#37096;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.01951</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#33021;&#21542;&#20165;&#29983;&#25104;&#36924;&#30495;&#30340;&#25163;&#37096;&#22270;&#20687;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Generate Realistic Hands Only Using Convolution?. (arXiv:2401.01951v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20026;&#21367;&#31215;&#23618;&#25552;&#20379;&#20855;&#26377;&#30456;&#23545;$n$&#32500;&#31515;&#21345;&#23572;&#22352;&#26631;&#31995;&#30340;&#21333;&#19968;&#36755;&#20837;&#36890;&#36947;&#65292;&#21487;&#20197;&#32531;&#35299;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26080;&#27861;&#37325;&#29616;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;GAN&#21644;VAE&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#38754;&#37096;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#36798;&#21313;&#24180;&#20043;&#20037;&#65292;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#19968;&#30452;&#26080;&#27861;&#37325;&#29616;&#22797;&#26434;&#30340;&#20960;&#20309;&#29305;&#24449;&#65292;&#20363;&#22914;&#20154;&#25163;&#21644;&#25163;&#25351;&#20013;&#25152;&#23384;&#22312;&#30340;&#29305;&#24449;&#65292;&#36825;&#19968;&#38382;&#39064;&#22312;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#19968;&#30452;&#23384;&#22312;&#12290;&#34429;&#28982;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21644;&#22810;&#26679;&#21270;&#35757;&#32451;&#25968;&#25454;&#38598;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#36825;&#20010;&#38382;&#39064;&#22312;&#21508;&#31181;&#27169;&#22411;&#20013;&#20173;&#28982;&#26222;&#36941;&#23384;&#22312;&#65292;&#20174;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#21040;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#36825;&#25351;&#21521;&#20102;&#24213;&#23618;&#32467;&#26500;&#30340;&#26681;&#26412;&#32570;&#38519;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20026;&#21367;&#31215;&#23618;&#25552;&#20379;&#19968;&#20010;&#21333;&#19968;&#36755;&#20837;&#36890;&#36947;&#65292;&#20854;&#20013;&#21253;&#21547;&#30456;&#23545;$n$&#32500;&#31515;&#21345;&#23572;&#22352;&#26631;&#31995;&#65292;&#26469;&#23637;&#31034;&#22914;&#20309;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#26497;&#22823;&#22320;&#25913;&#21892;&#20102;GAN&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#38754;&#37096;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The enduring inability of image generative models to recreate intricate geometric features, such as those present in human hands and fingers has been an ongoing problem in image generation for nearly a decade. While strides have been made by increasing model sizes and diversifying training datasets, this issue remains prevalent across all models, from denoising diffusion models to Generative Adversarial Networks (GAN), pointing to a fundamental shortcoming in the underlying architectures. In this paper, we demonstrate how this problem can be mitigated by augmenting convolution layers geometric capabilities through providing them with a single input channel incorporating the relative $n$-dimensional Cartesian coordinate system. We show that this drastically improves quality of hand and face images generated by GANs and Variational AutoEncoders (VAE).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#36827;&#34892;&#27491;&#35268;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#20174;&#20302;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#20020;&#24202;MRI&#25195;&#25551;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.10968</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#30340;MRI&#22330;&#36716;&#31227;&#37325;&#24314;&#65306;&#36890;&#36807;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#36827;&#34892;&#27491;&#35268;&#21270;
&lt;/p&gt;
&lt;p&gt;
MRI Field-transfer Reconstruction with Limited Data: Regularization by Neural Style Transfer. (arXiv:2308.10968v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#36827;&#34892;&#27491;&#35268;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#20174;&#20302;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#20020;&#24202;MRI&#25195;&#25551;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;MRI&#37325;&#24314;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25253;&#21578;&#30340;&#26041;&#27861;&#37117;&#38656;&#35201;&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#38477;&#22122;&#65288;RED&#65289;&#27491;&#35268;&#21270;&#26159;&#19968;&#31181;&#23558;&#38477;&#22122;&#22120;&#20316;&#20026;&#22270;&#20687;&#37325;&#24314;&#20808;&#39564;&#30340;&#36890;&#29992;&#27969;&#31243;&#12290;RED&#30340;&#28508;&#21147;&#24050;&#32463;&#22312;&#22810;&#20010;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#20219;&#21153;&#65288;&#22914;&#38477;&#22122;&#12289;&#21435;&#27169;&#31946;&#21644;&#36229;&#20998;&#36776;&#29575;&#65289;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;RNST&#65289;&#26041;&#27861;&#36827;&#34892;&#27491;&#35268;&#21270;&#30340;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#21033;&#29992;&#31070;&#32463;&#36716;&#31227;&#21644;&#38477;&#22122;&#24341;&#25806;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;RNST&#33021;&#22815;&#20174;&#26377;&#22122;&#22768;&#30340;&#20302;&#36136;&#37327;&#22270;&#20687;&#20013;&#37325;&#24314;&#20986;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#22270;&#20687;&#39118;&#26684;&#21644;&#26377;&#38480;&#25968;&#25454;&#19981;&#21516;&#12290;&#25105;&#20204;&#20351;&#29992;1.5T&#21644;3T&#30340;&#20020;&#24202;MRI&#25195;&#25551;&#39564;&#35777;&#20102;RNST&#65292;&#24182;&#19988;&#26174;&#31034;RNST&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;RNST&#26694;&#26550;&#22312;MRI&#37325;&#24314;&#21644;&#26377;&#38480;&#25968;&#25454;&#37325;&#24314;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have demonstrated success in MRI reconstruction using deep learning-based models. However, most reported approaches require training on a task-specific, large-scale dataset. Regularization by denoising (RED) is a general pipeline which embeds a denoiser as a prior for image reconstruction. The potential of RED has been demonstrated for multiple image-related tasks such as denoising, deblurring and super-resolution. In this work, we propose a regularization by neural style transfer (RNST) method to further leverage the priors from the neural transfer and denoising engine. This enables RNST to reconstruct a high-quality image from a noisy low-quality image with different image styles and limited data. We validate RNST with clinical MRI scans from 1.5T and 3T and show that RNST can significantly boost image quality. Our results highlight the capability of the RNST framework for MRI reconstruction and the potential for reconstruction tasks with limited data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;API&#30340;&#26041;&#27861;&#29983;&#25104;&#23494;&#20999;&#31867;&#20284;&#20110;&#21407;&#22987;&#31169;&#26377;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#37096;&#32626;&#12290;&#20351;&#29992;Private Evolution&#65288;PE&#65289;&#26694;&#26550;&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#65292;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#36827;&#21270;&#31639;&#27861;&#21644;&#20803;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#26082;&#20026;DP&#21448;&#19982;&#21407;&#22987;&#22270;&#20687;&#22806;&#35266;&#30456;&#20284;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#22312;&#27969;&#34892;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.15560</link><description>&lt;p&gt;
&#22522;&#20110; Foundation Model APIs &#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#65306;&#22270;&#29255;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Synthetic Data via Foundation Model APIs 1: Images. (arXiv:2305.15560v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;API&#30340;&#26041;&#27861;&#29983;&#25104;&#23494;&#20999;&#31867;&#20284;&#20110;&#21407;&#22987;&#31169;&#26377;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#37096;&#32626;&#12290;&#20351;&#29992;Private Evolution&#65288;PE&#65289;&#26694;&#26550;&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#65292;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#36827;&#21270;&#31639;&#27861;&#21644;&#20803;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#26082;&#20026;DP&#21448;&#19982;&#21407;&#22987;&#22270;&#20687;&#22806;&#35266;&#30456;&#20284;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#22312;&#27969;&#34892;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#25968;&#25454;&#39537;&#21160;&#30340;&#19990;&#30028;&#20013;&#65292;&#29983;&#25104;&#23494;&#20999;&#31867;&#20284;&#20110;&#21407;&#22987;&#31169;&#26377;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21512;&#25104;&#25968;&#25454;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21487;&#20943;&#36731;&#38544;&#31169;&#38382;&#39064;&#12290;&#19982;&#24403;&#21069;&#20026;&#27492;&#20219;&#21153;&#35757;&#32451;&#23450;&#21046;&#27169;&#22411;&#30340;&#20570;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;API&#29983;&#25104;DP&#21512;&#25104;&#25968;&#25454;&#65288;DPSDA&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#23558;&#22522;&#30784;&#27169;&#22411;&#35270;&#20026;&#40657;&#30418;&#24182;&#21482;&#21033;&#29992;&#20854;&#25512;&#29702;API&#12290;&#36825;&#20123;&#22522;&#20110;API&#30340;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#26356;&#23481;&#26131;&#37096;&#32626;&#65292;&#22914;&#26368;&#36817; API &#24212;&#29992;&#31243;&#24207;&#30340;&#28608;&#22686;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#12290;&#36825;&#20123;&#26041;&#27861;&#36824;&#21487;&#20197;&#21033;&#29992;&#21487;&#36890;&#36807;&#20854;&#25512;&#29702;API&#35775;&#38382;&#20854;&#26435;&#37325;&#26410;&#21457;&#24067;&#30340;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#27169;&#22411;&#35775;&#38382;&#26356;&#21152;&#20005;&#26684;&#65292;&#36824;&#38656;&#20445;&#25252;API&#25552;&#20379;&#21830;&#30340;&#38544;&#31169;&#65292;&#36825;&#23558;&#24102;&#26469;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026; Private Evolution&#65288;PE&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;API&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#26041;&#38754;&#30340;&#21021;&#22987;&#23454;&#29616;&#12290;PE&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#36827;&#21270;&#31639;&#27861;&#21644;&#20803;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#26082;&#20026;DP&#21448;&#19982;&#21407;&#22987;&#22270;&#20687;&#22806;&#35266;&#30456;&#20284;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#22312;&#27969;&#34892;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#22914;CIFAR-10&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#29992;&#21644;&#38544;&#31169;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;DP&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating differentially private (DP) synthetic data that closely resembles the original private data without leaking sensitive user information is a scalable way to mitigate privacy concerns in the current data-driven world. In contrast to current practices that train customized models for this task, we aim to generate DP Synthetic Data via APIs (DPSDA), where we treat foundation models as blackboxes and only utilize their inference APIs. Such API-based, training-free approaches are easier to deploy as exemplified by the recent surge in the number of API-based apps. These approaches can also leverage the power of large foundation models which are accessible via their inference APIs while the model weights are unreleased. However, this comes with greater challenges due to strictly more restrictive model access and the additional need to protect privacy from the API provider.  In this paper, we present a new framework called Private Evolution (PE) to solve this problem and show its ini
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FPANet&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#21435;&#38500;&#21508;&#31181;&#22823;&#23567;&#30340;&#33707;&#23572;&#32441;&#22270;&#26696;&#26469;&#25913;&#21892;&#24674;&#22797;&#36136;&#37327;&#65292;&#37319;&#29992;&#22810;&#20010;&#36830;&#32493;&#24103;&#25552;&#21462;&#24103;&#19981;&#21464;&#20869;&#23481;&#29305;&#24449;&#65292;&#36755;&#20986;&#26102;&#38388;&#19968;&#33268;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2301.07330</link><description>&lt;p&gt;
FPANet: &#22522;&#20110;&#39057;&#29575;&#30340;&#35270;&#39057;&#21435;&#33707;&#23572;&#32441;&#25216;&#26415;&#65292;&#20351;&#29992;&#24103;&#32423;&#21518;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
FPANet: Frequency-based Video Demoireing using Frame-level Post Alignment. (arXiv:2301.07330v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07330
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FPANet&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#21435;&#38500;&#21508;&#31181;&#22823;&#23567;&#30340;&#33707;&#23572;&#32441;&#22270;&#26696;&#26469;&#25913;&#21892;&#24674;&#22797;&#36136;&#37327;&#65292;&#37319;&#29992;&#22810;&#20010;&#36830;&#32493;&#24103;&#25552;&#21462;&#24103;&#19981;&#21464;&#20869;&#23481;&#29305;&#24449;&#65292;&#36755;&#20986;&#26102;&#38388;&#19968;&#33268;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#21472;&#32593;&#26684;&#27169;&#24335;&#20043;&#38388;&#30340;&#24178;&#25200;&#20250;&#23548;&#33268;&#33707;&#23572;&#32441;&#65292;&#20174;&#32780;&#38477;&#20302;&#26222;&#36890;&#25968;&#30721;&#30456;&#26426;&#25429;&#25417;&#25968;&#23383;&#26174;&#31034;&#23631;&#30340;&#22270;&#20687;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FPANet&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#23398;&#20064;&#39057;&#29575;&#21644;&#31354;&#38388;&#22495;&#20013;&#30340;&#28388;&#27874;&#22120;&#65292;&#36890;&#36807;&#21435;&#38500;&#21508;&#31181;&#22823;&#23567;&#30340;&#33707;&#23572;&#32441;&#22270;&#26696;&#26469;&#25913;&#21892;&#24674;&#22797;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#20351;&#29992;&#22810;&#20010;&#36830;&#32493;&#24103;&#65292;&#23398;&#20064;&#25552;&#21462;&#24103;&#19981;&#21464;&#20869;&#23481;&#29305;&#24449;&#65292;&#24182;&#36755;&#20986;&#26356;&#22909;&#36136;&#37327;&#30340;&#26102;&#38388;&#19968;&#33268;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interference between overlapping gird patterns creates moire patterns, degrading the visual quality of an image that captures a screen of a digital display device by an ordinary digital camera. Removing such moire patterns is challenging due to their complex patterns of diverse sizes and color distortions. Existing approaches mainly focus on filtering out in the spatial domain, failing to remove a large-scale moire pattern. In this paper, we propose a novel model called FPANet that learns filters in both frequency and spatial domains, improving the restoration quality by removing various sizes of moire patterns. To further enhance, our model takes multiple consecutive frames, learning to extract frame-invariant content features and outputting better quality temporally consistent images. We demonstrate the effectiveness of our proposed method with a publicly available large-scale dataset, observing that ours outperforms the state-of-the-art approaches, including ESDNet, VDmoire, MBCNN, 
&lt;/p&gt;</description></item></channel></rss>