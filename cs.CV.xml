<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>CR3DT&#26159;&#19968;&#20010;&#30456;&#26426;&#19982;&#38647;&#36798;&#34701;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#38647;&#36798;&#22312;3D&#26816;&#27979;&#21644;&#36319;&#36394;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#22312;State-of-the-Art&#30456;&#26426;&#26550;&#26500;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.15313</link><description>&lt;p&gt;
CR3DT&#65306;&#30456;&#26426;&#19982;&#38647;&#36798;&#34701;&#21512;&#29992;&#20110;3D&#26816;&#27979;&#21644;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15313
&lt;/p&gt;
&lt;p&gt;
CR3DT&#26159;&#19968;&#20010;&#30456;&#26426;&#19982;&#38647;&#36798;&#34701;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#38647;&#36798;&#22312;3D&#26816;&#27979;&#21644;&#36319;&#36394;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#22312;State-of-the-Art&#30456;&#26426;&#26550;&#26500;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#26816;&#27979;&#21644;&#36319;&#36394;&#21608;&#22260;&#29289;&#20307;&#23545;&#20110;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20809;&#25506;&#27979;&#19982;&#27979;&#36317;&#65288;LiDAR&#65289;&#20256;&#24863;&#22120;&#24050;&#32463;&#25104;&#20026;&#39640;&#24615;&#33021;&#30340;&#22522;&#20934;&#65292;&#20294;&#20165;&#20351;&#29992;&#30456;&#26426;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21560;&#24341;&#21147;&#22312;&#20110;&#20854;&#25104;&#26412;&#25928;&#30410;&#12290;&#23613;&#31649;&#26080;&#32447;&#30005;&#25506;&#27979;&#19982;&#27979;&#36317;&#65288;RADAR&#65289;&#20256;&#24863;&#22120;&#22312;&#27773;&#36710;&#31995;&#32479;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#21644;&#27979;&#37327;&#22122;&#22768;&#30340;&#21407;&#22240;&#65292;&#23427;&#20204;&#22312;3D&#26816;&#27979;&#21644;&#36319;&#36394;&#20013;&#30340;&#28508;&#21147;&#38271;&#26399;&#34987;&#24573;&#35270;&#12290;&#20316;&#20026;&#19968;&#20010;&#26368;&#26032;&#30340;&#21457;&#23637;&#65292;&#30456;&#26426;&#19982;&#38647;&#36798;&#30340;&#32467;&#21512;&#27491;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Camera-RADAR 3D Detection and Tracking (CR3DT)&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;3D&#29289;&#20307;&#26816;&#27979;&#21644;&#22810;&#29289;&#20307;&#36319;&#36394;&#30340;&#30456;&#26426;-&#38647;&#36798;&#34701;&#21512;&#27169;&#22411;&#12290;&#22312;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#21482;&#26377;&#30456;&#26426;&#30340;BEVDet&#26550;&#26500;&#30340;&#22522;&#30784;&#19978;&#65292;CR3DT&#22312;&#26816;&#27979;&#21644;&#36319;&#36394;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#36890;&#36807;&#25972;&#21512;&#38647;&#36798;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15313v1 Announce Type: cross  Abstract: Accurate detection and tracking of surrounding objects is essential to enable self-driving vehicles. While Light Detection and Ranging (LiDAR) sensors have set the benchmark for high performance, the appeal of camera-only solutions lies in their cost-effectiveness. Notably, despite the prevalent use of Radio Detection and Ranging (RADAR) sensors in automotive systems, their potential in 3D detection and tracking has been largely disregarded due to data sparsity and measurement noise. As a recent development, the combination of RADARs and cameras is emerging as a promising solution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a camera-RADAR fusion model for 3D object detection, and Multi-Object Tracking (MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only BEVDet architecture, CR3DT demonstrates substantial improvements in both detection and tracking capabilities, by incorporating the sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#33539;&#24335;&#65292;&#36890;&#36807;&#36873;&#25321;&#19982;&#27979;&#37327;&#26631;&#35782;&#19968;&#33268;&#30340;&#26679;&#26412;&#65292;&#20197;&#21450;&#20174;&#19982;&#27979;&#37327;&#20449;&#21495;&#30456;&#32467;&#21512;&#30340;&#21021;&#22987;&#21270;&#24320;&#22987;&#24674;&#22797;&#36807;&#31243;&#65292;&#23454;&#29616;&#36755;&#20986;&#31283;&#23450;&#24615;&#21644;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.16907</link><description>&lt;p&gt;
&#29992;&#20110;&#22270;&#20687;&#24674;&#22797;&#30340;&#25193;&#25955;&#21518;&#39564;&#36817;&#20284;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Diffusion Posterior Proximal Sampling for Image Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#33539;&#24335;&#65292;&#36890;&#36807;&#36873;&#25321;&#19982;&#27979;&#37327;&#26631;&#35782;&#19968;&#33268;&#30340;&#26679;&#26412;&#65292;&#20197;&#21450;&#20174;&#19982;&#27979;&#37327;&#20449;&#21495;&#30456;&#32467;&#21512;&#30340;&#21021;&#22987;&#21270;&#24320;&#22987;&#24674;&#22797;&#36807;&#31243;&#65292;&#23454;&#29616;&#36755;&#20986;&#31283;&#23450;&#24615;&#21644;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#21151;&#25928;&#12290;&#29616;&#26377;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#31639;&#27861;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#21033;&#29992;&#25968;&#25454;&#20808;&#39564;&#65292;&#20294;&#20173;&#20445;&#30041;&#20102;&#32487;&#25215;&#33258;&#26080;&#26465;&#20214;&#29983;&#25104;&#33539;&#24335;&#30340;&#20803;&#32032;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#33539;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36873;&#25321;&#22312;&#27599;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#19982;&#27979;&#37327;&#26631;&#35782;&#19968;&#33268;&#30340;&#26679;&#26412;&#65292;&#21033;&#29992;&#37319;&#26679;&#36873;&#25321;&#20316;&#20026;&#36755;&#20986;&#31283;&#23450;&#24615;&#21644;&#22686;&#24378;&#30340;&#36884;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#19982;&#27979;&#37327;&#20449;&#21495;&#30456;&#32467;&#21512;&#30340;&#21021;&#22987;&#21270;&#24320;&#22987;&#24674;&#22797;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#38468;&#21152;&#20449;&#24687;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16907v1 Announce Type: cross  Abstract: Diffusion models have demonstrated remarkable efficacy in generating high-quality samples. Existing diffusion-based image restoration algorithms exploit pre-trained diffusion models to leverage data priors, yet they still preserve elements inherited from the unconditional generation paradigm. These strategies initiate the denoising process with pure white noise and incorporate random noise at each generative step, leading to over-smoothed results. In this paper, we introduce a refined paradigm for diffusion-based image restoration. Specifically, we opt for a sample consistent with the measurement identity at each generative step, exploiting the sampling selection as an avenue for output stability and enhancement. Besides, we start the restoration process with an initialization combined with the measurement signal, providing supplementary information to better align the generative process. Extensive experimental results and analyses val
&lt;/p&gt;</description></item><item><title>GAOKAO-MM &#26159;&#22522;&#20110;&#20013;&#22269;&#39640;&#32771;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#65292;&#20026;&#27169;&#22411;&#30340;&#33021;&#21147;&#35774;&#23450;&#20154;&#31867;&#27700;&#24179;&#35201;&#27714;&#65292;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#30446;&#21069;&#30340;LVLMs&#30340;&#20934;&#30830;&#29575;&#26222;&#36941;&#19981;&#36275;50%&#12290;</title><link>https://arxiv.org/abs/2402.15745</link><description>&lt;p&gt;
GAOKAO-MM: &#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#35780;&#20272;&#30340;&#20013;&#22269;&#20154;&#31867;&#27700;&#24179;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15745
&lt;/p&gt;
&lt;p&gt;
GAOKAO-MM &#26159;&#22522;&#20110;&#20013;&#22269;&#39640;&#32771;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#65292;&#20026;&#27169;&#22411;&#30340;&#33021;&#21147;&#35774;&#23450;&#20154;&#31867;&#27700;&#24179;&#35201;&#27714;&#65292;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#30446;&#21069;&#30340;LVLMs&#30340;&#20934;&#30830;&#29575;&#26222;&#36941;&#19981;&#36275;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#24050;&#32463;&#22312;&#22270;&#20687;&#24863;&#30693;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26497;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#22522;&#26412;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#24120;&#35782;&#30693;&#35782;&#65292;&#36825;&#20123;&#26080;&#27861;&#20805;&#20998;&#21453;&#26144;&#20986;LVLMs&#30340;&#20840;&#38754;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GAOKAO-MM&#65292;&#19968;&#20010;&#22522;&#20110;&#20013;&#22269;&#39640;&#32771;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#65292;&#21253;&#25324;8&#20010;&#31185;&#30446;&#21644;12&#31181;&#31867;&#22411;&#30340;&#22270;&#29255;&#65292;&#22914;&#22270;&#34920;&#12289;&#20989;&#25968;&#22270;&#12289;&#22320;&#22270;&#21644;&#29031;&#29255;&#12290;GAOKAO-MM&#26469;&#28304;&#20110;&#20013;&#22269;&#26412;&#22303;&#32972;&#26223;&#65292;&#24182;&#20026;&#27169;&#22411;&#30340;&#33021;&#21147;&#35774;&#23450;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#35201;&#27714;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#29702;&#35299;&#12289;&#30693;&#35782;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;10&#20010;LVLMs&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#20934;&#30830;&#29575;&#37117;&#20302;&#20110;50%&#65292;&#20854;&#20013;GPT-4-Vision&#65288;48.1%&#65289;&#12289;Qwen-VL-Plus&#65288;41.2%&#65289;&#21644;Gemini-Pro-Vision&#65288;35.1%&#65289;&#20301;&#21015;&#21069;&#19977;&#21517;&#12290;&#25105;&#20204;&#30340;&#22810;&#32500;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;LVLMs&#20855;&#26377;&#36866;&#24230;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15745v1 Announce Type: cross  Abstract: The Large Vision-Language Models (LVLMs) have demonstrated great abilities in image perception and language understanding. However, existing multimodal benchmarks focus on primary perception abilities and commonsense knowledge which are insufficient to reflect the comprehensive capabilities of LVLMs. We propose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance Examination (GAOKAO), comprising of 8 subjects and 12 types of images, such as diagrams, function graphs, maps and photos. GAOKAO-MM derives from native Chinese context and sets human-level requirements for the model's abilities, including perception, understanding, knowledge and reasoning. We evaluate 10 LVLMs and find that the accuracies of all of them are lower than 50%, with GPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking in the top three positions. The results of our multi-dimension analysis indicate that LVLMs have moder
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#20013;&#36739;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13699</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#37327;&#23376;&#28857;&#22120;&#20214;&#27979;&#37327;&#20998;&#31867;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Explainable Classification Techniques for Quantum Dot Device Measurements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13699
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#20013;&#36739;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#29702;&#31185;&#23398;&#20013;&#65292;&#23545;&#22270;&#20687;&#25968;&#25454;&#30340;&#31283;&#20581;&#29305;&#24449;&#34920;&#31034;&#38656;&#27714;&#22686;&#21152;&#65306;&#22270;&#20687;&#37319;&#38598;&#65292;&#22312;&#24191;&#20041;&#19978;&#25351;&#20108;&#32500;&#25968;&#25454;&#65292;&#29616;&#22312;&#22312;&#35768;&#22810;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#25105;&#20204;&#22312;&#27492;&#32771;&#34385;&#30340;&#37327;&#23376;&#20449;&#24687;&#31185;&#23398;&#12290;&#34429;&#28982;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#24191;&#27867;&#20351;&#29992;&#20256;&#32479;&#22270;&#20687;&#29305;&#24449;&#65292;&#20294;&#23427;&#20204;&#30340;&#20351;&#29992;&#27491;&#22312;&#36805;&#36895;&#34987;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#25152;&#21462;&#20195;&#65292;&#21518;&#32773;&#24448;&#24448;&#20197;&#29306;&#29298;&#21487;&#35299;&#37322;&#24615;&#20026;&#20195;&#20215;&#25442;&#21462;&#39640;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23637;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#21331;&#36234;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#19981;&#20250;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#31181;&#25216;&#26415;&#24102;&#26469;&#20102;&#23454;&#36136;&#24615;&#30340;&#30410;&#22788;&#65292;&#24403;&#21069;&#21457;&#23637;&#38454;&#27573;&#38656;&#35201;&#20154;&#31867;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13699v1 Announce Type: cross  Abstract: In the physical sciences, there is an increased need for robust feature representations of image data: image acquisition, in the generalized sense of two-dimensional data, is now widespread across a large number of fields, including quantum information science, which we consider here. While traditional image features are widely utilized in such cases, their use is rapidly being supplanted by Neural Network-based techniques that often sacrifice explainability in exchange for high accuracy. To ameliorate this trade-off, we propose a synthetic data-based technique that results in explainable features. We show, using Explainable Boosting Machines (EBMs), that this method offers superior explainability without sacrificing accuracy. Specifically, we show that there is a meaningful benefit to this technique in the context of quantum dot tuning, where human intervention is necessary at the current stage of development.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ColorSwap&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#21305;&#37197;&#29289;&#20307;&#21644;&#39068;&#33394;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#39068;&#33394;&#35789;&#37325;&#26032;&#25490;&#24207;&#20197;&#20462;&#25913;&#19981;&#21516;&#30340;&#23545;&#35937;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#27979;&#35797;&#27169;&#22411;&#22312;&#36825;&#39033;&#20219;&#21153;&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#30446;&#21069;&#30340;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#20173;&#19981;&#22815;&#31283;&#23450;&#65292;&#20294;&#36890;&#36807;&#26356;&#20808;&#36827;&#30340;&#25552;&#31034;&#25216;&#26415;&#21487;&#33021;&#20250;&#26377;&#25152;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2402.04492</link><description>&lt;p&gt;
ColorSwap: &#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#35780;&#20272;&#30340;&#39068;&#33394;&#21644;&#21333;&#35789;&#25490;&#24207;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ColorSwap&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#21305;&#37197;&#29289;&#20307;&#21644;&#39068;&#33394;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#39068;&#33394;&#35789;&#37325;&#26032;&#25490;&#24207;&#20197;&#20462;&#25913;&#19981;&#21516;&#30340;&#23545;&#35937;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#27979;&#35797;&#27169;&#22411;&#22312;&#36825;&#39033;&#20219;&#21153;&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#30446;&#21069;&#30340;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#20173;&#19981;&#22815;&#31283;&#23450;&#65292;&#20294;&#36890;&#36807;&#26356;&#20808;&#36827;&#30340;&#25552;&#31034;&#25216;&#26415;&#21487;&#33021;&#20250;&#26377;&#25152;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ColorSwap&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#25913;&#36827;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#21305;&#37197;&#29289;&#20307;&#21644;&#20854;&#39068;&#33394;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;2000&#20010;&#29420;&#29305;&#30340;&#22270;&#20687;-&#26631;&#39064;&#23545;&#65292;&#20998;&#20026;1000&#20010;&#31034;&#20363;&#12290;&#27599;&#20010;&#31034;&#20363;&#21253;&#25324;&#19968;&#20010;&#26631;&#39064;-&#22270;&#20687;&#23545;&#65292;&#20197;&#21450;&#19968;&#20010;&#8220;&#39068;&#33394;&#20132;&#25442;&#8221;&#23545;&#12290;&#25105;&#20204;&#36981;&#24490;Winoground&#26041;&#26696;&#65306;&#31034;&#20363;&#20013;&#30340;&#20004;&#20010;&#26631;&#39064;&#20855;&#26377;&#30456;&#21516;&#30340;&#21333;&#35789;&#65292;&#20294;&#39068;&#33394;&#21333;&#35789;&#34987;&#37325;&#26032;&#25490;&#21015;&#20197;&#20462;&#25913;&#19981;&#21516;&#30340;&#23545;&#35937;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#26631;&#39064;&#21644;&#22270;&#20687;&#29983;&#25104;&#19982;&#20154;&#31867;&#30340;&#20132;&#20114;&#21019;&#36896;&#32780;&#25104;&#12290;&#25105;&#20204;&#35780;&#20272;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#65288;ITM&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21457;&#29616;&#21363;&#20351;&#26159;&#26368;&#26032;&#30340;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#20173;&#28982;&#19981;&#22815;&#31283;&#20581;&#12290;GPT-4V&#21644;LLaVA&#22312;&#25105;&#20204;&#30340;&#20027;&#35201;VLM&#25351;&#26631;&#19978;&#24471;&#20998;&#20998;&#21035;&#20026;72%&#21644;42%&#65292;&#23613;&#31649;&#23427;&#20204;&#21487;&#33021;&#36890;&#36807;&#26356;&#20808;&#36827;&#30340;&#25552;&#31034;&#25216;&#26415;&#26469;&#25552;&#21319;&#12290;&#22312;&#20027;&#35201;&#30340;ITM&#25351;&#26631;&#19978;&#65292;&#20687;CLIP&#21644;SigLIP&#36825;&#26679;&#30340;&#23545;&#27604;&#27169;&#22411;&#25509;&#36817;&#20110;&#38543;&#26426;&#29468;&#27979;&#65288;&#20998;&#21035;&#20026;12%&#21644;30%&#65289;&#65292;&#23613;&#31649;&#38750;&#23545;&#27604;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the ColorSwap dataset, designed to assess and improve the proficiency of multimodal models in matching objects with their colors. The dataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000 examples. Each example includes a caption-image pair, along with a ``color-swapped'' pair. We follow the Winoground schema: the two captions in an example have the same words, but the color words have been rearranged to modify different objects. The dataset was created through a novel blend of automated caption and image generation with humans in the loop. We evaluate image-text matching (ITM) and visual language models (VLMs) and find that even the latest ones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on our main VLM metric, although they may improve with more advanced prompting techniques. On the main ITM metric, contrastive models such as CLIP and SigLIP perform close to chance (at 12% and 30%, respectively), although the non-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#33322;&#29677;&#28369;&#34892;&#23433;&#20840;&#30340;&#36305;&#36947;&#29289;&#20307;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#35780;&#20272;&#20102;&#35813;&#20998;&#31867;&#22120;&#23545;&#19977;&#31181;&#24120;&#35265;&#22270;&#20687;&#25200;&#21160;&#31867;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21333;&#35843;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.00035</link><description>&lt;p&gt;
&#33322;&#29677;&#28369;&#34892;&#23433;&#20840;&#30340;&#36305;&#36947;&#29289;&#20307;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Robustness Assessment of a Runway Object Classifier for Safe Aircraft Taxiing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#33322;&#29677;&#28369;&#34892;&#23433;&#20840;&#30340;&#36305;&#36947;&#29289;&#20307;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#35780;&#20272;&#20102;&#35813;&#20998;&#31867;&#22120;&#23545;&#19977;&#31181;&#24120;&#35265;&#22270;&#20687;&#25200;&#21160;&#31867;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21333;&#35843;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#35768;&#22810;&#35745;&#31639;&#38382;&#39064;&#19978;&#25104;&#20026;&#20027;&#35201;&#35299;&#20915;&#26041;&#26696;&#65292;&#33322;&#31354;&#19994;&#24076;&#26395;&#25506;&#32034;&#23427;&#20204;&#22312;&#20943;&#36731;&#39134;&#34892;&#21592;&#36127;&#25285;&#21644;&#25913;&#21892;&#36816;&#33829;&#23433;&#20840;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31867;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#20351;&#29992;DNNs&#38656;&#35201;&#36827;&#34892;&#24443;&#24213;&#30340;&#35748;&#35777;&#36807;&#31243;&#12290;&#36825;&#19968;&#38656;&#27714;&#21487;&#20197;&#36890;&#36807;&#24418;&#24335;&#39564;&#35777;&#26469;&#35299;&#20915;&#65292;&#24418;&#24335;&#39564;&#35777;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#65292;&#20363;&#22914;&#35777;&#26126;&#26576;&#20123;&#35823;&#21028;&#30340;&#19981;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;Airbus&#24403;&#21069;&#27491;&#22312;&#24320;&#21457;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;DNN&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#26088;&#22312;&#22312;&#39134;&#26426;&#28369;&#34892;&#38454;&#27573;&#20351;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#26469;&#35780;&#20272;&#36825;&#20010;DNN&#23545;&#19977;&#31181;&#24120;&#35265;&#22270;&#20687;&#25200;&#21160;&#31867;&#22411;&#30340;&#40065;&#26834;&#24615;&#65306;&#22122;&#22768;&#12289;&#20142;&#24230;&#21644;&#23545;&#27604;&#24230;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#37096;&#20998;&#32452;&#21512;&#12290;&#36825;&#20010;&#36807;&#31243;&#28041;&#21450;&#22810;&#27425;&#35843;&#29992;&#24213;&#23618;&#39564;&#35777;&#22120;&#65292;&#36825;&#21487;&#33021;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65307;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21333;&#35843;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks (DNNs) are becoming the prominent solution for many computational problems, the aviation industry seeks to explore their potential in alleviating pilot workload and in improving operational safety. However, the use of DNNs in this type of safety-critical applications requires a thorough certification process. This need can be addressed through formal verification, which provides rigorous assurances -- e.g.,~by proving the absence of certain mispredictions. In this case-study paper, we demonstrate this process using an image-classifier DNN currently under development at Airbus and intended for use during the aircraft taxiing phase. We use formal methods to assess this DNN's robustness to three common image perturbation types: noise, brightness and contrast, and some of their combinations. This process entails multiple invocations of the underlying verifier, which might be computationally expensive; and we therefore propose a method that leverages the monotonicity
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;RetinaNet&#27169;&#22411;&#38024;&#23545;&#37319;&#29992;&#26234;&#33021;&#25163;&#26426;&#26174;&#24494;&#31995;&#32479;&#26816;&#27979;&#36158;&#31532;&#34411;&#21644;&#38544;&#23394;&#23376;&#36827;&#34892;&#26816;&#27979;&#21644;&#35745;&#25968;&#65292;&#24182;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20339;&#25928;&#26524;&#65292;&#20026;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.05339</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#25163;&#26426;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#36158;&#31532;&#34411;&#21644;&#38544;&#23394;&#23376;&#30340;&#26816;&#27979;&#19982;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
Deep-learning assisted detection and quantification of (oo)cysts of Giardia and Cryptosporidium on smartphone microscopy images. (arXiv:2304.05339v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;RetinaNet&#27169;&#22411;&#38024;&#23545;&#37319;&#29992;&#26234;&#33021;&#25163;&#26426;&#26174;&#24494;&#31995;&#32479;&#26816;&#27979;&#36158;&#31532;&#34411;&#21644;&#38544;&#23394;&#23376;&#36827;&#34892;&#26816;&#27979;&#21644;&#35745;&#25968;&#65292;&#24182;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20339;&#25928;&#26524;&#65292;&#20026;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29992;&#21463;&#24494;&#29983;&#29289;&#27745;&#26579;&#30340;&#39135;&#29289;&#21644;&#27700;&#27599;&#24180;&#36896;&#25104;&#25968;&#30334;&#19975;&#20154;&#27515;&#20129;&#12290;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#26174;&#24494;&#31995;&#32479;&#26159;&#19968;&#31181;&#20415;&#25658;&#12289;&#20302;&#25104;&#26412;&#21644;&#27604;&#20256;&#32479;&#30340;&#20142;&#22330;&#26174;&#24494;&#38236;&#26356;&#26131;&#25509;&#36817;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#36158;&#31532;&#34411;&#21644;&#38544;&#23394;&#23376;&#12290;&#28982;&#32780;&#65292;&#26234;&#33021;&#25163;&#26426;&#26174;&#24494;&#38236;&#30340;&#22270;&#20687;&#26377;&#24456;&#22810;&#22122;&#22768;&#65292;&#38656;&#35201;&#22521;&#35757;&#26377;&#32032;&#30340;&#25216;&#26415;&#20154;&#21592;&#36827;&#34892;&#25163;&#21160;&#22218;&#27873;&#35782;&#21035;&#65292;&#32780;&#36825;&#36890;&#24120;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#37319;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23545;&#35937;&#26816;&#27979;&#33258;&#21160;&#26816;&#27979;&#21365;/&#26797;&#29366;&#20307;&#21487;&#33021;&#20026;&#27492;&#38480;&#21046;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#26816;&#27979;&#36158;&#31532;&#34411;&#21644;&#38544;&#23394;&#23376;&#30340;&#25928;&#26524;&#65292;&#25968;&#25454;&#38598;&#21253;&#25324;&#20174;&#34092;&#33756;&#26679;&#21697;&#20013;&#33719;&#21462;&#30340;&#26234;&#33021;&#25163;&#26426;&#21644;&#20142;&#22330;&#26174;&#24494;&#38236;&#22270;&#20687;&#12290;Faster RCNN&#12289;RetinaNet&#21644;You Only Look Once&#65288;YOLOv8s&#65289;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#29992;&#26469;&#25506;&#32034;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#21365;/&#26797;&#29366;&#20307;&#65292;&#20294;RetinaNet&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#20004;&#31181;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#26174;&#24494;&#31995;&#32479;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#33258;&#21160;&#26816;&#27979;&#21644;&#35745;&#25968;&#36158;&#31532;&#34411;&#21644;&#38544;&#23394;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
The consumption of microbial-contaminated food and water is responsible for the deaths of millions of people annually. Smartphone-based microscopy systems are portable, low-cost, and more accessible alternatives for the detection of Giardia and Cryptosporidium than traditional brightfield microscopes. However, the images from smartphone microscopes are noisier and require manual cyst identification by trained technicians, usually unavailable in resource-limited settings. Automatic detection of (oo)cysts using deep-learning-based object detection could offer a solution for this limitation. We evaluate the performance of three state-of-the-art object detectors to detect (oo)cysts of Giardia and Cryptosporidium on a custom dataset that includes both smartphone and brightfield microscopic images from vegetable samples. Faster RCNN, RetinaNet, and you only look once (YOLOv8s) deep-learning models were employed to explore their efficacy and limitations. Our results show that while the deep-l
&lt;/p&gt;</description></item></channel></rss>