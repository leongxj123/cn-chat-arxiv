<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#25968;&#27010;&#24565;&#20462;&#25913;&#28608;&#27963;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#24182;&#25552;&#39640;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11875</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20998;&#25968;&#27010;&#24565;&#65306;&#22686;&#36827;&#28608;&#27963;&#21644;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Fractional Concepts in Neural Networks: Enhancing Activation and Loss Functions. (arXiv:2310.11875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#25968;&#27010;&#24565;&#20462;&#25913;&#28608;&#27963;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#24182;&#25552;&#39640;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#20998;&#25968;&#27010;&#24565;&#20462;&#25913;&#28608;&#27963;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#30830;&#23450;&#35757;&#32451;&#36807;&#31243;&#30340;&#20998;&#25968;&#23548;&#25968;&#38454;&#25968;&#20316;&#20026;&#39069;&#22806;&#30340;&#36229;&#21442;&#25968;&#26469;&#23450;&#20041;&#21644;&#20248;&#21270;&#20854;&#28608;&#27963;&#20989;&#25968;&#12290;&#36825;&#23558;&#20351;&#24471;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#20803;&#33021;&#22815;&#35843;&#25972;&#20854;&#28608;&#27963;&#20989;&#25968;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#24182;&#20943;&#23569;&#36755;&#20986;&#38169;&#35823;&#65292;&#26377;&#21487;&#33021;&#25552;&#39640;&#32593;&#32476;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents a method for using fractional concepts in a neural network to modify the activation and loss functions. The methodology allows the neural network to define and optimize its activation functions by determining the fractional derivative order of the training process as an additional hyperparameter. This will enable neurons in the network to adjust their activation functions to match input data better and reduce output errors, potentially improving the network's overall performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20154;&#30524;&#36861;&#36394;&#38598;&#25104;&#21040;&#35270;&#35273;Transformer&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#22312;&#22810;&#31181;&#39550;&#39542;&#24773;&#20917;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13969</link><description>&lt;p&gt;
&#27880;&#37325;&#27880;&#24847;&#21147;&#65306;&#23558;&#20154;&#30524;&#36861;&#36394;&#38598;&#25104;&#21040;&#35270;&#35273;Transformer&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Fixating on Attention: Integrating Human Eye Tracking into Vision Transformers. (arXiv:2308.13969v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20154;&#30524;&#36861;&#36394;&#38598;&#25104;&#21040;&#35270;&#35273;Transformer&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#22312;&#22810;&#31181;&#39550;&#39542;&#24773;&#20917;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22522;&#20110;Transformer&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22312;&#22810;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36229;&#36234;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#20851;&#38190;&#20219;&#21153;&#65292;&#22914;&#21307;&#23398;&#22270;&#20687;&#35299;&#37322;&#21644;&#33258;&#21160;&#39550;&#39542;&#65292;&#20173;&#28982;&#38656;&#35201;&#20381;&#36182;&#20154;&#31867;&#21028;&#26029;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20154;&#31867;&#35270;&#35273;&#36755;&#20837;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#30524;&#21160;&#20202;&#25910;&#38598;&#21040;&#30340;&#27880;&#35270;&#28857;&#65292;&#38598;&#25104;&#21040;Transformer&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#22312;&#22810;&#31181;&#39550;&#39542;&#24773;&#20917;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#20154;&#31867;&#23454;&#39564;&#23545;&#35937;&#21644;Vision Transformer&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#65292;&#27880;&#35270;&#21306;&#22495;&#22312;&#24038;&#21491;&#39550;&#39542;&#20915;&#31574;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#27604;&#36739;&#20154;&#31867;&#27880;&#35270;&#22270;&#21644;ViT&#27880;&#24847;&#21147;&#26435;&#37325;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21333;&#20010;&#22836;&#37096;&#21644;&#23618;&#20043;&#38388;&#30340;&#37325;&#21472;&#21160;&#24577;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#37325;&#21472;&#21160;&#24577;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#30340;&#20462;&#21098;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#39550;&#39542;&#22330;&#26223;&#20449;&#24687;&#19982;&#27880;&#35270;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#37319;&#29992;&#8220;&#32852;&#21512;&#31354;&#38388;-&#27880;&#35270;&#8221;&#65288;JSF&#65289;&#30340;&#27880;&#24847;&#21147;&#35774;&#32622;&#12290;&#26368;&#21518;
&lt;/p&gt;
&lt;p&gt;
Modern transformer-based models designed for computer vision have outperformed humans across a spectrum of visual tasks. However, critical tasks, such as medical image interpretation or autonomous driving, still require reliance on human judgments. This work demonstrates how human visual input, specifically fixations collected from an eye-tracking device, can be integrated into transformer models to improve accuracy across multiple driving situations and datasets. First, we establish the significance of fixation regions in left-right driving decisions, as observed in both human subjects and a Vision Transformer (ViT). By comparing the similarity between human fixation maps and ViT attention weights, we reveal the dynamics of overlap across individual heads and layers. This overlap is exploited for model pruning without compromising accuracy. Thereafter, we incorporate information from the driving scene with fixation data, employing a "joint space-fixation" (JSF) attention setup. Lastly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21028;&#21035;&#24615;&#20449;&#24687;&#21644;&#33258;&#30001;&#25991;&#26412;&#30340;&#38750;&#20405;&#20837;&#24335;&#24494;&#35843;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#22810;&#26679;&#24615;&#21644;&#39640;&#20934;&#30830;&#29575;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.17155</link><description>&lt;p&gt;
&#22522;&#20110;&#21028;&#21035;&#24615;&#31867;&#26631;&#30340;&#25991;&#26412;&#22270;&#29255;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discriminative Class Tokens for Text-to-Image Diffusion Models. (arXiv:2303.17155v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21028;&#21035;&#24615;&#20449;&#24687;&#21644;&#33258;&#30001;&#25991;&#26412;&#30340;&#38750;&#20405;&#20837;&#24335;&#24494;&#35843;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#22810;&#26679;&#24615;&#21644;&#39640;&#20934;&#30830;&#29575;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#29983;&#25104;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#22270;&#29255;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#27495;&#20041;&#65292;&#29983;&#25104;&#30340;&#22270;&#29255;&#24120;&#24120;&#26080;&#27861;&#25551;&#32472;&#20986;&#24494;&#22937;&#30340;&#32454;&#33410;&#19988;&#26131;&#20110;&#20986;&#38169;&#12290;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#22312;&#26377;&#31867;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#32570;&#28857;&#22312;&#20110;&#65306;&#65288;i&#65289;&#19982;&#29992;&#20110;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#29228;&#21462;&#30340;&#25991;&#26412;-&#22270;&#20687;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#26377;&#31867;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#36739;&#23567;&#65292;&#22240;&#27492;&#29983;&#25104;&#30340;&#22270;&#29255;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20250;&#20005;&#37325;&#21463;&#24433;&#21709;&#65292;&#25110;&#65288;ii&#65289;&#36755;&#20837;&#26159;&#30828;&#32534;&#30721;&#30340;&#26631;&#31614;&#65292;&#32780;&#19981;&#26159;&#33258;&#30001;&#25991;&#26412;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#25511;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#30340;&#24494;&#35843;&#25216;&#26415;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#21028;&#21035;&#24615;&#20449;&#21495;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#26082;&#21457;&#25381;&#20102;&#33258;&#30001;&#25991;&#26412;&#30340;&#34920;&#36798;&#28508;&#21147;&#65292;&#21448;&#33021;&#22815;&#23454;&#29616;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-to-image diffusion models have enabled the generation of diverse and high-quality images. However, generated images often fall short of depicting subtle details and are susceptible to errors due to ambiguity in the input text. One way of alleviating these issues is to train diffusion models on class-labeled datasets. This comes with a downside, doing so limits their expressive power: (i) supervised datasets are generally small compared to large-scale scraped text-image datasets on which text-to-image models are trained, and so the quality and diversity of generated images are severely affected, or (ii) the input is a hard-coded label, as opposed to free-form text, which limits the control over the generated images.  In this work, we propose a non-invasive fine-tuning technique that capitalizes on the expressive potential of free-form text while achieving high accuracy through discriminative signals from a pretrained classifier, which guides the generation. This 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#21270;&#21442;&#25968;&#35009;&#21098;&#31574;&#30053;&#26469;&#23454;&#29616;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;&#65292;&#21487;&#20197;&#21457;&#29616;&#26368;&#20339;&#30340;&#38745;&#24577;&#23376;&#32593;&#32476;&#65292;&#21253;&#21547;&#20108;&#20540;&#38376;&#25511;&#27169;&#22359;&#21644;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#22312;&#24050;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.10798</link><description>&lt;p&gt;
&#36731;&#37327;&#21270;&#21442;&#25968;&#35009;&#21098;&#20197;&#23454;&#29616;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;: &#19968;&#31181;&#20108;&#20540;&#38376;&#25511;&#27169;&#22359;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lightweight Parameter Pruning for Energy-Efficient Deep Learning: A Binarized Gating Module Approach. (arXiv:2302.10798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#21270;&#21442;&#25968;&#35009;&#21098;&#31574;&#30053;&#26469;&#23454;&#29616;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;&#65292;&#21487;&#20197;&#21457;&#29616;&#26368;&#20339;&#30340;&#38745;&#24577;&#23376;&#32593;&#32476;&#65292;&#21253;&#21547;&#20108;&#20540;&#38376;&#25511;&#27169;&#22359;&#21644;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#22312;&#24050;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#22823;&#19988;&#26356;&#21152;&#22797;&#26434;&#65292;&#32511;&#33394;AI&#24050;&#32463;&#24341;&#36215;&#20102;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#37319;&#29992;&#23545;&#32593;&#32476;&#21442;&#25968;&#36827;&#34892;&#35009;&#21098;&#65292;&#20197;&#20943;&#23569;&#35757;&#32451;&#25512;&#26029;&#26102;&#30340;&#35745;&#31639;&#36127;&#33655;&#12290;&#28982;&#32780;&#65292;&#35009;&#21098;&#26041;&#26696;&#36890;&#24120;&#20250;&#23548;&#33268;&#39069;&#22806;&#30340;&#24320;&#38144;&#65292;&#22240;&#20026;&#38656;&#35201;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#21644;&#24494;&#35843;&#25110;&#37325;&#22797;&#35745;&#31639;&#21160;&#24577;&#35009;&#21098;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#35009;&#21098;&#31574;&#30053;&#65292;&#20197;&#23398;&#20064;&#36731;&#37327;&#32423;&#23376;&#32593;&#32476;&#65292;&#26082;&#33021;&#26368;&#23567;&#21270;&#33021;&#32791;&#65292;&#21448;&#33021;&#22312;&#32473;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#19982;&#23436;&#20840;&#21442;&#25968;&#21270;&#30340;&#32593;&#32476;&#20445;&#25345;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35009;&#21098;&#26041;&#26696;&#20197;&#32511;&#33394;&#20026;&#23548;&#21521;&#65292;&#22240;&#20026;&#23427;&#20165;&#38656;&#35201;&#21160;&#24577;&#35009;&#21098;&#26041;&#27861;&#36827;&#34892;&#19968;&#27425;&#35757;&#32451;&#26469;&#21457;&#29616;&#26368;&#20339;&#30340;&#38745;&#24577;&#23376;&#32593;&#32476;&#12290;&#35813;&#26041;&#26696;&#30001;&#19968;&#20010;&#20108;&#36827;&#21046;&#38376;&#25511;&#27169;&#22359;&#21644;&#19968;&#20010;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#32452;&#25104;&#65292;&#20197;&#21457;&#29616;&#20855;&#26377;&#29992;&#25143;&#23450;&#20041;&#31232;&#30095;&#24230;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#31561;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35009;&#21098;&#21644;&#36716;&#25442;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#33021;&#37327;&#28040;&#32791;&#65292;&#21516;&#26102;&#22312;&#24050;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The subject of green AI has been gaining attention within the deep learning community given the recent trend of ever larger and more complex neural network models. Existing solutions for reducing the computational load of training at inference time usually involve pruning the network parameters. Pruning schemes often create extra overhead either by iterative training and fine-tuning for static pruning or repeated computation of a dynamic pruning graph. We propose a new parameter pruning strategy for learning a lighter-weight sub-network that minimizes the energy cost while maintaining comparable performance to the fully parameterised network on given downstream tasks. Our proposed pruning scheme is green-oriented, as it only requires a one-off training to discover the optimal static sub-networks by dynamic pruning methods. The pruning scheme consists of a binary gating module and a novel loss function to uncover sub-networks with user-defined sparsity. Our method enables pruning and tr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Walsh&#31995;&#25968;&#36924;&#36817;&#20915;&#31574;&#36793;&#30028;&#30340;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#28165;&#26224;&#22270;&#20687;&#21644;&#23545;&#25239;&#22270;&#20687;&#20043;&#38388;&#30340;Walsh&#31995;&#25968;&#36924;&#36817;&#24046;&#24322;&#65292;&#23454;&#29616;&#20102;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2211.10227</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#25104;&#36793;&#30028;&#36924;&#36817;&#30340;&#23545;&#25239;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Detection by Approximation of Ensemble Boundary. (arXiv:2211.10227v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Walsh&#31995;&#25968;&#36924;&#36817;&#20915;&#31574;&#36793;&#30028;&#30340;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#28165;&#26224;&#22270;&#20687;&#21644;&#23545;&#25239;&#22270;&#20687;&#20043;&#38388;&#30340;Walsh&#31995;&#25968;&#36924;&#36817;&#24046;&#24322;&#65292;&#23454;&#29616;&#20102;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#38024;&#23545;&#35299;&#20915;&#20004;&#31867;&#27169;&#24335;&#35782;&#21035;&#38382;&#39064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#38598;&#25104;&#12290;&#35813;&#38598;&#25104;&#20351;&#29992;Walsh&#31995;&#25968;&#36827;&#34892;&#32452;&#21512;&#65292;&#33021;&#22815;&#36924;&#36817;&#24067;&#23572;&#20989;&#25968;&#24182;&#25511;&#21046;&#38598;&#25104;&#20915;&#31574;&#36793;&#30028;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#30340;&#20551;&#35774;&#26159;&#39640;&#26354;&#29575;&#30340;&#20915;&#31574;&#36793;&#30028;&#20801;&#35768;&#25214;&#21040;&#23545;&#25239;&#25200;&#21160;&#65292;&#20294;&#20250;&#25913;&#21464;&#20915;&#31574;&#36793;&#30028;&#30340;&#26354;&#29575;&#65292;&#32780;&#19982;&#28165;&#26224;&#22270;&#20687;&#30456;&#27604;&#65292;&#20351;&#29992;Walsh&#31995;&#25968;&#23545;&#20854;&#36827;&#34892;&#36924;&#36817;&#30340;&#26041;&#24335;&#20063;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#36807;&#35266;&#23519;&#28165;&#26224;&#22270;&#20687;&#21644;&#23545;&#25239;&#22270;&#20687;&#20043;&#38388;&#30340;Walsh&#31995;&#25968;&#36924;&#36817;&#24046;&#24322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#36801;&#31227;&#24615;&#21487;&#29992;&#20110;&#26816;&#27979;&#12290;&#27492;&#22806;&#65292;&#36924;&#36817;&#20915;&#31574;&#36793;&#30028;&#21487;&#33021;&#26377;&#21161;&#20110;&#29702;&#35299;DNN&#30340;&#23398;&#20064;&#21644;&#21487;&#36801;&#31227;&#24615;&#29305;&#24615;&#12290;&#23613;&#31649;&#26412;&#25991;&#30340;&#23454;&#39564;&#20351;&#29992;&#22270;&#20687;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20004;&#31867;&#27169;&#24335;&#35782;&#21035;&#38382;&#39064;&#30340;&#38598;&#25104;&#36793;&#30028;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new method of detecting adversarial attacks is proposed for an ensemble of Deep Neural Networks (DNNs) solving two-class pattern recognition problems. The ensemble is combined using Walsh coefficients which are capable of approximating Boolean functions and thereby controlling the complexity of the ensemble decision boundary. The hypothesis in this paper is that decision boundaries with high curvature allow adversarial perturbations to be found, but change the curvature of the decision boundary, which is then approximated in a different way by Walsh coefficients compared to the clean images. By observing the difference in Walsh coefficient approximation between clean and adversarial images, it is shown experimentally that transferability of attack may be used for detection. Furthermore, approximating the decision boundary may aid in understanding the learning and transferability properties of DNNs. While the experiments here use images, the proposed approach of modelling two-class en
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#26041;&#27861;VITO&#24471;&#21040;&#20102;&#20855;&#26377;&#20154;&#31867;&#24863;&#30693;&#29305;&#24449;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#29702;&#35299;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.06433</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#20135;&#29983;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#35270;&#35273;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Self-supervised video pretraining yields human-aligned visual representations. (arXiv:2210.06433v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#26041;&#27861;VITO&#24471;&#21040;&#20102;&#20855;&#26377;&#20154;&#31867;&#24863;&#30693;&#29305;&#24449;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#29702;&#35299;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#35266;&#23519;&#23545;&#35937;&#21644;&#22330;&#26223;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#26041;&#24335;&#23398;&#20064;&#21040;&#20102;&#24378;&#22823;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#38656;&#35201;&#26126;&#30830;&#30340;&#26102;&#38388;&#29702;&#35299;&#30340;&#29305;&#23450;&#20219;&#21153;&#20043;&#22806;&#65292;&#38745;&#24577;&#22270;&#20687;&#39044;&#35757;&#32451;&#20173;&#28982;&#26159;&#23398;&#20064;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#20027;&#27969;&#33539;&#24335;&#12290;&#25105;&#20204;&#23545;&#36825;&#31181;&#19981;&#21305;&#37197;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#24182;&#19988;&#38382;&#26159;&#21542;&#35270;&#39057;&#39044;&#35757;&#32451;&#21487;&#20197;&#20135;&#29983;&#20855;&#26377;&#20154;&#31867;&#24863;&#30693;&#29305;&#24449;&#30340;&#35270;&#35273;&#34920;&#31034;&#65306;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#24615;&#12289;&#23545;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#21644;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31579;&#36873;&#35270;&#39057;&#30340;&#26032;&#39062;&#31243;&#24207;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#23545;&#27604;&#24615;&#26694;&#26550;&#65292;&#20174;&#20854;&#20013;&#30340;&#22797;&#26434;&#36716;&#25442;&#20013;&#23398;&#20064;&#12290;&#36825;&#31181;&#20174;&#35270;&#39057;&#20013;&#25552;&#28860;&#30693;&#35782;&#30340;&#31616;&#21333;&#33539;&#24335;&#34987;&#31216;&#20026;VITO&#65292;&#23427;&#20135;&#29983;&#30340;&#19968;&#33324;&#34920;&#31034;&#22312;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#19978;&#36828;&#36828;&#20248;&#20110;&#20808;&#21069;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#20248;&#20110;&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;VITO&#34920;&#31034;&#23545;&#33258;&#28982;&#21644;&#21512;&#25104;&#24418;&#21464;&#30340;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans learn powerful representations of objects and scenes by observing how they evolve over time. Yet, outside of specific tasks that require explicit temporal understanding, static image pretraining remains the dominant paradigm for learning visual foundation models. We question this mismatch, and ask whether video pretraining can yield visual representations that bear the hallmarks of human perception: generalisation across tasks, robustness to perturbations, and consistency with human judgements. To that end we propose a novel procedure for curating videos, and develop a contrastive framework which learns from the complex transformations therein. This simple paradigm for distilling knowledge from videos, called VITO, yields general representations that far outperform prior video pretraining methods on image understanding tasks, and image pretraining methods on video understanding tasks. Moreover, VITO representations are significantly more robust to natural and synthetic deformati
&lt;/p&gt;</description></item></channel></rss>