<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>SecurePose&#26159;&#19968;&#20010;&#24320;&#28304;&#36719;&#20214;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#23454;&#29616;&#20020;&#24202;&#24405;&#21046;&#30340;&#24739;&#32773;&#35270;&#39057;&#20013;&#30340;&#20154;&#33080;&#27169;&#31946;&#21644;&#21160;&#21147;&#23398;&#29305;&#24449;&#25552;&#21462;&#65292;&#25552;&#39640;&#20102;&#35270;&#39057;&#35780;&#20272;&#21644;&#24739;&#32773;&#38544;&#31169;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14143</link><description>&lt;p&gt;
SecurePose&#65306;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#24405;&#21046;&#30340;&#35270;&#39057;&#20013;&#23454;&#29616;&#33258;&#21160;&#20154;&#33080;&#27169;&#31946;&#21644;&#20154;&#20307;&#36816;&#21160;&#21160;&#21147;&#23398;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
SecurePose: Automated Face Blurring and Human Movement Kinematics Extraction from Videos Recorded in Clinical Settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14143
&lt;/p&gt;
&lt;p&gt;
SecurePose&#26159;&#19968;&#20010;&#24320;&#28304;&#36719;&#20214;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#23454;&#29616;&#20020;&#24202;&#24405;&#21046;&#30340;&#24739;&#32773;&#35270;&#39057;&#20013;&#30340;&#20154;&#33080;&#27169;&#31946;&#21644;&#21160;&#21147;&#23398;&#29305;&#24449;&#25552;&#21462;&#65292;&#25552;&#39640;&#20102;&#35270;&#39057;&#35780;&#20272;&#21644;&#24739;&#32773;&#38544;&#31169;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#38556;&#30861;&#36890;&#24120;&#36890;&#36807;&#19987;&#23478;&#23545;&#20020;&#24202;&#33719;&#21462;&#30340;&#24739;&#32773;&#35270;&#39057;&#36827;&#34892;&#20849;&#35782;&#35780;&#20272;&#26469;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24191;&#27867;&#20998;&#20139;&#24739;&#32773;&#35270;&#39057;&#20250;&#23545;&#24739;&#32773;&#38544;&#31169;&#26500;&#25104;&#39118;&#38505;&#12290;&#20154;&#33080;&#27169;&#31946;&#21487;&#20197;&#29992;&#26469;&#21435;&#26631;&#35782;&#21270;&#35270;&#39057;&#65292;&#20294;&#36825;&#20010;&#36807;&#31243;&#36890;&#24120;&#26159;&#25163;&#21160;&#19988;&#32791;&#26102;&#30340;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#20154;&#33080;&#27169;&#31946;&#25216;&#26415;&#23481;&#26131;&#20986;&#29616;&#36807;&#24230;&#12289;&#19981;&#19968;&#33268;&#25110;&#19981;&#36275;&#30340;&#20154;&#33080;&#27169;&#31946; - &#36825;&#20123;&#37117;&#21487;&#33021;&#23545;&#35270;&#39057;&#35780;&#20272;&#21644;&#24739;&#32773;&#38544;&#31169;&#36896;&#25104;&#28798;&#38590;&#24615;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20123;&#35270;&#39057;&#20013;&#35780;&#20272;&#36816;&#21160;&#38556;&#30861;&#24448;&#24448;&#26159;&#20027;&#35266;&#30340;&#12290;&#25552;&#21462;&#21487;&#37327;&#21270;&#30340;&#21160;&#21147;&#23398;&#29305;&#24449;&#21487;&#20197;&#24110;&#21161;&#20102;&#35299;&#36825;&#20123;&#35270;&#39057;&#20013;&#30340;&#36816;&#21160;&#38556;&#30861;&#35780;&#20272;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#39044;&#27169;&#31946;&#35270;&#39057;&#26102;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;SecurePose&#30340;&#24320;&#28304;&#36719;&#20214;&#65292;&#21487;&#20197;&#22312;&#20020;&#24202;&#24405;&#21046;&#30340;&#24739;&#32773;&#35270;&#39057;&#20013;&#23454;&#29616;&#21487;&#38752;&#30340;&#20154;&#33080;&#27169;&#31946;&#21644;&#33258;&#21160;&#21160;&#21147;&#23398;&#29305;&#24449;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14143v1 Announce Type: cross  Abstract: Movement disorders are typically diagnosed by consensus-based expert evaluation of clinically acquired patient videos. However, such broad sharing of patient videos poses risks to patient privacy. Face blurring can be used to de-identify videos, but this process is often manual and time-consuming. Available automated face blurring techniques are subject to either excessive, inconsistent, or insufficient facial blurring - all of which can be disastrous for video assessment and patient privacy. Furthermore, assessing movement disorders in these videos is often subjective. The extraction of quantifiable kinematic features can help inform movement disorder assessment in these videos, but existing methods to do this are prone to errors if using pre-blurred videos. We have developed an open-source software called SecurePose that can both achieve reliable face blurring and automated kinematic extraction in patient videos recorded in a clinic 
&lt;/p&gt;</description></item><item><title>GINNs&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20960;&#20309;&#20219;&#21153;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#65292;&#37319;&#29992;&#26174;&#24335;&#22810;&#26679;&#24615;&#25439;&#22833;&#20197;&#21450;&#21487;&#24494;&#25439;&#22833;&#26469;&#20943;&#36731;&#27169;&#24577;&#22349;&#32553;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#22797;&#26434;&#24615;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14009</link><description>&lt;p&gt;
&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Geometry-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14009
&lt;/p&gt;
&lt;p&gt;
GINNs&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20960;&#20309;&#20219;&#21153;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#65292;&#37319;&#29992;&#26174;&#24335;&#22810;&#26679;&#24615;&#25439;&#22833;&#20197;&#21450;&#21487;&#24494;&#25439;&#22833;&#26469;&#20943;&#36731;&#27169;&#24577;&#22349;&#32553;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#22797;&#26434;&#24615;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;GINNs&#65289;&#30340;&#27010;&#24565;&#65292;&#28085;&#30422;&#20102;&#65288;i&#65289;&#22312;&#20960;&#20309;&#32422;&#26463;&#19979;&#23398;&#20064;&#65292;&#65288;ii&#65289;&#31070;&#32463;&#22330;&#20316;&#20026;&#21512;&#36866;&#30340;&#34920;&#31034;&#65292;&#65288;iii&#65289;&#29983;&#25104;&#22312;&#20960;&#20309;&#20219;&#21153;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#27424;&#23450;&#31995;&#32479;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GINN&#30340;&#26500;&#24314;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#27492;&#21487;&#20197;&#34987;&#32431;&#32422;&#26463;&#39537;&#21160;&#22320;&#35270;&#20026;&#29983;&#25104;&#24314;&#27169;&#12290;&#25105;&#20204;&#22686;&#21152;&#20102;&#26174;&#24335;&#30340;&#22810;&#26679;&#24615;&#25439;&#22833;&#26469;&#20943;&#36731;&#27169;&#24577;&#22349;&#32553;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#32422;&#26463;&#65292;&#29305;&#21035;&#26159;&#32452;&#20214;&#30340;&#36830;&#36890;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#33707;&#23572;&#26031;&#29702;&#35770;&#23558;&#20854;&#36716;&#21270;&#20026;&#21487;&#24494;&#25439;&#22833;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#26029;&#22686;&#21152;&#22797;&#26434;&#24615;&#30340;&#20108;&#32500;&#21644;&#19977;&#32500;&#22330;&#26223;&#20013;&#65292;GINN&#23398;&#20064;&#33539;&#24335;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14009v1 Announce Type: new  Abstract: We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks. Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints. We add an explicit diversity loss to mitigate mode collapse. We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory. Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity.
&lt;/p&gt;</description></item><item><title>&#22312;DDIM&#26694;&#26550;&#20013;&#20351;&#29992;GMM&#20316;&#20026;&#21453;&#21521;&#36716;&#31227;&#31639;&#23376;&#65292;&#36890;&#36807;&#30697;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#36136;&#37327;&#26356;&#39640;&#30340;&#26679;&#26412;&#12290;&#22312;&#26080;&#26465;&#20214;&#27169;&#22411;&#21644;&#31867;&#26465;&#20214;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;FID&#21644;IS&#25351;&#26631;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.04938</link><description>&lt;p&gt;
&#20351;&#29992;&#30697;&#21305;&#37197;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#25913;&#36827;&#20102;DDIM&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Improved DDIM Sampling with Moment Matching Gaussian Mixtures. (arXiv:2311.04938v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04938
&lt;/p&gt;
&lt;p&gt;
&#22312;DDIM&#26694;&#26550;&#20013;&#20351;&#29992;GMM&#20316;&#20026;&#21453;&#21521;&#36716;&#31227;&#31639;&#23376;&#65292;&#36890;&#36807;&#30697;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#36136;&#37327;&#26356;&#39640;&#30340;&#26679;&#26412;&#12290;&#22312;&#26080;&#26465;&#20214;&#27169;&#22411;&#21644;&#31867;&#26465;&#20214;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;FID&#21644;IS&#25351;&#26631;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#22312;Denoising Diffusion Implicit Models (DDIM)&#26694;&#26550;&#20013;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#20316;&#20026;&#21453;&#21521;&#36716;&#31227;&#31639;&#23376;&#65288;&#20869;&#26680;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#39044;&#35757;&#32451;&#30340;Denoising Diffusion Probabilistic Models (DDPM)&#20013;&#21152;&#36895;&#37319;&#26679;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#20043;&#19968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#32422;&#26463;GMM&#30340;&#21442;&#25968;&#65292;&#21305;&#37197;DDPM&#21069;&#21521;&#36793;&#38469;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#20013;&#24515;&#30697;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#30697;&#21305;&#37197;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#20351;&#29992;&#39640;&#26031;&#26680;&#30340;&#21407;&#22987;DDIM&#30456;&#21516;&#25110;&#26356;&#22909;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;CelebAHQ&#21644;FFHQ&#30340;&#26080;&#26465;&#20214;&#27169;&#22411;&#20197;&#21450;ImageNet&#25968;&#25454;&#38598;&#30340;&#31867;&#26465;&#20214;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#37319;&#26679;&#27493;&#39588;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;GMM&#20869;&#26680;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#36825;&#26159;&#36890;&#36807;FID&#21644;IS&#25351;&#26631;&#34913;&#37327;&#30340;&#12290;&#20363;&#22914;&#65292;&#22312;ImageNet 256x256&#19978;&#65292;&#20351;&#29992;10&#20010;&#37319;&#26679;&#27493;&#39588;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;FID&#20540;&#20026;...
&lt;/p&gt;
&lt;p&gt;
We propose using a Gaussian Mixture Model (GMM) as reverse transition operator (kernel) within the Denoising Diffusion Implicit Models (DDIM) framework, which is one of the most widely used approaches for accelerated sampling from pre-trained Denoising Diffusion Probabilistic Models (DDPM). Specifically we match the first and second order central moments of the DDPM forward marginals by constraining the parameters of the GMM. We see that moment matching is sufficient to obtain samples with equal or better quality than the original DDIM with Gaussian kernels. We provide experimental results with unconditional models trained on CelebAHQ and FFHQ and class-conditional models trained on ImageNet datasets respectively. Our results suggest that using the GMM kernel leads to significant improvements in the quality of the generated samples when the number of sampling steps is small, as measured by FID and IS metrics. For example on ImageNet 256x256, using 10 sampling steps, we achieve a FID of
&lt;/p&gt;</description></item></channel></rss>