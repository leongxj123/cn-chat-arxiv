<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>TraveLER&#26159;&#19968;&#31181;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27839;&#30528;&#35270;&#39057;&#31227;&#21160;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#25910;&#38598;&#20851;&#38190;&#24103;&#30340;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#35270;&#39057;&#38382;&#31572;&#20013;&#20851;&#38190;&#26102;&#38388;&#25139;&#36873;&#25321;&#21644;&#38169;&#35823;&#26102;&#38388;&#25139;&#35843;&#25972;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2404.01476</link><description>&lt;p&gt;
TraveLER&#65306;&#29992;&#20110;&#35270;&#39057;&#38382;&#31572;&#30340;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TraveLER: A Multi-LMM Agent Framework for Video Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01476
&lt;/p&gt;
&lt;p&gt;
TraveLER&#26159;&#19968;&#31181;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27839;&#30528;&#35270;&#39057;&#31227;&#21160;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#25910;&#38598;&#20851;&#38190;&#24103;&#30340;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#35270;&#39057;&#38382;&#31572;&#20013;&#20851;&#38190;&#26102;&#38388;&#25139;&#36873;&#25321;&#21644;&#38169;&#35823;&#26102;&#38388;&#25139;&#35843;&#25972;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#35270;&#39057;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#12289;&#22522;&#20110;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#20197;&#24103;&#20026;&#21333;&#20301;&#36827;&#34892;&#22788;&#29702;&#12290;&#34429;&#28982;&#22522;&#20110;&#22270;&#20687;&#30340;&#35270;&#39057;&#26041;&#27861;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#30340;&#23616;&#38480;&#26159;&#23427;&#20204;&#32463;&#24120;&#24573;&#35270;&#20102;&#22914;&#20309;&#36873;&#25321;&#20851;&#38190;&#26102;&#38388;&#25139;&#65292;&#24182;&#19988;&#26080;&#27861;&#22312;&#30830;&#23450;&#38169;&#35823;&#26102;&#38388;&#25139;&#26102;&#36827;&#34892;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26080;&#27861;&#25552;&#21462;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#32454;&#33410;&#65292;&#32780;&#26159;&#25552;&#20379;&#24103;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#23427;&#27839;&#30528;&#35270;&#39057;&#36827;&#34892;&#31227;&#21160;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#30340;&#26041;&#24335;&#36845;&#20195;&#22320;&#20174;&#20851;&#38190;&#24103;&#25910;&#38598;&#30456;&#20851;&#20449;&#24687;&#65292;&#30452;&#21040;&#33719;&#24471;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TraveLER&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#21046;&#23450;&#8220;&#36941;&#21382;&#8221;&#35270;&#39057;&#35745;&#21010;&#30340;&#27169;&#22411;&#65292;&#35810;&#38382;&#20851;&#20110;&#21333;&#20010;&#24103;&#30340;&#38382;&#39064;&#20197;&#8220;&#23450;&#20301;&#8221;&#24182;&#23384;&#20648;&#20851;&#38190;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01476v1 Announce Type: cross  Abstract: Recently, Large Multimodal Models (LMMs) have made significant progress in video question-answering using a frame-wise approach by leveraging large-scale, image-based pretraining in a zero-shot manner. While image-based methods for videos have shown impressive performance, a current limitation is that they often overlook how key timestamps are selected and cannot adjust when incorrect timestamps are identified. Moreover, they are unable to extract details relevant to the question, instead providing general descriptions of the frame. To overcome this, we design a multi-LMM agent framework that travels along the video, iteratively collecting relevant information from keyframes through interactive question-asking until there is sufficient information to answer the question. Specifically, we propose TraveLER, a model that can create a plan to "Traverse" through the video, ask questions about individual frames to "Locate" and store key info
&lt;/p&gt;</description></item><item><title>UADA3D&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#22788;&#29702;&#31232;&#30095;LiDAR&#25968;&#25454;&#21644;&#22823;&#39046;&#22495;&#24046;&#36317;&#65292;&#24182;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.17633</link><description>&lt;p&gt;
UADA3D&#65306;&#38754;&#21521;&#31232;&#30095;LiDAR&#21644;&#22823;&#39046;&#22495;&#24046;&#36317;&#30340;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17633
&lt;/p&gt;
&lt;p&gt;
UADA3D&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#22788;&#29702;&#31232;&#30095;LiDAR&#25968;&#25454;&#21644;&#22823;&#39046;&#22495;&#24046;&#36317;&#65292;&#24182;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#22312;&#22522;&#20110;LiDAR&#30340;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#36866;&#24212;&#24050;&#24314;&#31435;&#30340;&#39640;&#23494;&#24230;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#26356;&#31232;&#30095;&#30340;&#28857;&#20113;&#65292;&#25429;&#25417;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282;&#30340;&#22330;&#26223;&#65306;&#19981;&#20165;&#26469;&#33258;&#36947;&#36335;&#19978;&#30340;&#36710;&#36742;&#65292;&#36824;&#26469;&#33258;&#20154;&#34892;&#36947;&#19978;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#65292;&#36973;&#36935;&#30528;&#26126;&#26174;&#19981;&#21516;&#30340;&#29615;&#22659;&#26465;&#20214;&#21644;&#20256;&#24863;&#22120;&#37197;&#32622;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;3D&#29289;&#20307;&#26816;&#27979;&#65288;UADA3D&#65289;&#12290;UADA3D&#19981;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#25110;&#24072;&#29983;&#26550;&#26500;&#12290;&#30456;&#21453;&#65292;&#23427;&#20351;&#29992;&#23545;&#25239;&#26041;&#27861;&#30452;&#25509;&#23398;&#20064;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#36866;&#24212;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#39046;&#22495;&#22343;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#26159;&#24320;&#28304;&#30340;&#65292;&#24456;&#24555;&#23558;&#20250;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17633v1 Announce Type: cross  Abstract: In this study, we address a gap in existing unsupervised domain adaptation approaches on LiDAR-based 3D object detection, which have predominantly concentrated on adapting between established, high-density autonomous driving datasets. We focus on sparser point clouds, capturing scenarios from different perspectives: not just from vehicles on the road but also from mobile robots on sidewalks, which encounter significantly different environmental conditions and sensor configurations. We introduce Unsupervised Adversarial Domain Adaptation for 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source models or teacher-student architectures. Instead, it uses an adversarial approach to directly learn domain-invariant features. We demonstrate its efficacy in various adaptation scenarios, showing significant improvements in both self-driving car and mobile robot domains. Our code is open-source and will be available soon.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;G-NoCL&#65292;&#37319;&#29992;&#29983;&#25104;&#25968;&#25454;&#24182;&#21033;&#29992;DIverSity&#21644;COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#36827;&#34892;&#25968;&#25454;&#34701;&#21512;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10853</link><description>&lt;p&gt;
&#21482;&#35828;&#21517;&#31216;&#65306;&#36890;&#36807;&#25968;&#25454;&#29983;&#25104;&#23454;&#29616;&#20165;&#21033;&#29992;&#31867;&#21035;&#21517;&#31216;&#36827;&#34892;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Just Say the Name: Online Continual Learning with Category Names Only via Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;G-NoCL&#65292;&#37319;&#29992;&#29983;&#25104;&#25968;&#25454;&#24182;&#21033;&#29992;DIverSity&#21644;COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#36827;&#34892;&#25968;&#25454;&#34701;&#21512;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#25104;&#26412;&#36807;&#39640;&#65292;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#21463;&#21040;&#22823;&#35268;&#27169;&#32593;&#32476;&#30417;&#30563;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#24314;&#35758;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#21033;&#29992;&#32593;&#32476;&#25235;&#21462;&#30340;&#25968;&#25454;&#65292;&#20294;&#36825;&#24102;&#26469;&#20102;&#35832;&#22914;&#25968;&#25454;&#19981;&#24179;&#34913;&#12289;&#20351;&#29992;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36830;&#32493;&#32593;&#32476;&#30417;&#30563;&#35757;&#32451;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550; - &#20165;&#20351;&#29992;&#21517;&#31216;&#30340;&#29983;&#25104;&#24335;&#36830;&#32493;&#23398;&#20064;&#65288;G-NoCL&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;G-NoCL&#20351;&#29992;&#19968;&#32452;&#29983;&#25104;&#22120;G&#20197;&#21450;&#23398;&#20064;&#32773;&#12290;&#24403;&#36935;&#21040;&#26032;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#31867;&#21035;&#65289;&#26102;&#65292;G-NoCL&#37319;&#29992;&#26032;&#39062;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;&#25216;&#26415;DIverSity and COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#20174;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#26368;&#20248;&#25277;&#26679;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DISCOBER&#22312;G-NoCL&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#28085;&#30422;&#20102;In-Distributi&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10853v1 Announce Type: cross  Abstract: In real-world scenarios, extensive manual annotation for continual learning is impractical due to prohibitive costs. Although prior arts, influenced by large-scale webly supervised training, suggest leveraging web-scraped data in continual learning, this poses challenges such as data imbalance, usage restrictions, and privacy concerns. Addressing the risks of continual webly supervised training, we present an online continual learning framework - Generative Name only Continual Learning (G-NoCL). The proposed G-NoCL uses a set of generators G along with the learner. When encountering new concepts (i.e., classes), G-NoCL employs the novel sample complexity-guided data ensembling technique DIverSity and COmplexity enhancing ensemBlER (DISCOBER) to optimally sample training data from generated data. Through extensive experimentation, we demonstrate superior performance of DISCOBER in G-NoCL online CL benchmarks, covering both In-Distributi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMQ&#30340;&#21019;&#26032;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36880;&#23618;&#20943;&#23567;&#37325;&#26500;&#35823;&#24046;&#26469;&#26377;&#25928;&#38477;&#20302;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#35201;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07134</link><description>&lt;p&gt;
COMQ: &#19968;&#31181;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07134
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMQ&#30340;&#21019;&#26032;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36880;&#23618;&#20943;&#23567;&#37325;&#26500;&#35823;&#24046;&#26469;&#26377;&#25928;&#38477;&#20302;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#35201;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#23558;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#23454;&#29992;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#37096;&#32626;&#26102;&#39640;&#24230;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#27169;&#22411;&#38477;&#33267;&#20302;&#27604;&#29305;&#34920;&#31034;&#32780;&#19981;&#25439;&#23475;&#21407;&#22987;&#20934;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;PTQ&#31639;&#27861;&#31216;&#20026;COMQ&#65292;&#23427;&#36890;&#36807;&#20381;&#27425;&#20943;&#23567;&#36880;&#23618;&#37325;&#26500;&#35823;&#24046;&#26469;&#36827;&#34892;&#22352;&#26631;&#26041;&#21521;&#19978;&#30340;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#25972;&#25968;&#37327;&#21270;&#65292;&#20854;&#20013;&#27599;&#20010;&#37327;&#21270;&#26435;&#37325;&#21487;&#20197;&#20998;&#35299;&#20026;&#19968;&#20010;&#20849;&#20139;&#30340;&#28014;&#28857;&#26631;&#37327;&#21644;&#19968;&#20010;&#25972;&#25968;&#20301;&#32534;&#30721;&#12290;&#22312;&#22266;&#23450;&#23618;&#20869;&#65292;COMQ&#23558;&#25152;&#26377;&#32553;&#25918;&#22240;&#23376;&#21644;&#20301;&#32534;&#30721;&#35270;&#20026;&#37325;&#26500;&#35823;&#24046;&#30340;&#21464;&#37327;&#12290;&#27599;&#27425;&#36845;&#20195;&#37117;&#20250;&#27839;&#30528;&#19968;&#20010;&#22352;&#26631;&#36724;&#25913;&#36827;&#36825;&#20010;&#38169;&#35823;&#65292;&#21516;&#26102;&#20445;&#25345;&#25152;&#26377;&#20854;&#20182;&#21464;&#37327;&#24658;&#23450;&#12290;COMQ&#26131;&#20110;&#20351;&#29992;&#65292;&#26080;&#38656;&#35843;&#25972;&#36229;&#21442;&#25968;&#12290;&#23427;&#21482;&#28041;&#21450;&#28857;&#20056;&#21644;&#22235;&#33293;&#20116;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07134v1 Announce Type: new  Abstract: Post-training quantization (PTQ) has emerged as a practical approach to compress large neural networks, making them highly efficient for deployment. However, effectively reducing these models to their low-bit counterparts without compromising the original accuracy remains a key challenge. In this paper, we propose an innovative PTQ algorithm termed COMQ, which sequentially conducts coordinate-wise minimization of the layer-wise reconstruction errors. We consider the widely used integer quantization, where every quantized weight can be decomposed into a shared floating-point scalar and an integer bit-code. Within a fixed layer, COMQ treats all the scaling factor(s) and bit-codes as the variables of the reconstruction error. Every iteration improves this error along a single coordinate while keeping all other variables constant. COMQ is easy to use and requires no hyper-parameter tuning. It instead involves only dot products and rounding o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#20013;&#38388;&#34920;&#31034;&#30340;&#22270;&#20687;&#26469;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#22797;&#21512;&#25552;&#31034;&#21644;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#30340;&#19968;&#20123;&#37325;&#35201;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.05846</link><description>&lt;p&gt;
&#25193;&#25955;&#38236;&#22836;&#65306;&#35299;&#37322;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#31649;&#36947;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#20013;&#38388;&#34920;&#31034;&#30340;&#22270;&#20687;&#26469;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#22797;&#21512;&#25552;&#31034;&#21644;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#30340;&#19968;&#20123;&#37325;&#35201;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05846v1 &#36890;&#21578;&#31867;&#22411;&#65306;&#36328; &#23384;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;T2I&#65289;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#32534;&#30721;&#22120;&#20135;&#29983;&#25991;&#26412;&#34920;&#31034;&#30340;&#36807;&#31243;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#25955;&#38236;&#22836;&#65292;&#19968;&#31181;&#20998;&#26512; T2I &#27169;&#22411;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20854;&#20013;&#38388;&#34920;&#31034;&#30340;&#22270;&#20687;&#12290;&#20351;&#29992;&#25193;&#25955;&#38236;&#22836;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#26368;&#36817;&#30340; T2I &#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#12290;&#22312;&#25506;&#32034;&#22797;&#21512;&#25552;&#31034;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#25551;&#36848;&#22810;&#20010;&#23545;&#35937;&#30340;&#22797;&#26434;&#22330;&#26223;&#30456;&#23545;&#20110;&#31616;&#21333;&#22330;&#26223;&#26159;&#36880;&#27493;&#19988;&#36739;&#24930;&#22320;&#26500;&#24314;&#30340;&#65307;&#22312;&#25506;&#32034;&#30693;&#35782;&#26816;&#32034;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#34920;&#31034;&#19981;&#24120;&#35265;&#27010;&#24565;&#38656;&#35201;&#27604;&#24120;&#35265;&#27010;&#24565;&#26356;&#22810;&#30340;&#35745;&#31639;&#65292;&#24182;&#19988;&#30693;&#35782;&#26816;&#32034;&#22312;&#23618;&#20043;&#38388;&#26159;&#28176;&#36827;&#30340;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026; T2I &#31649;&#36947;&#20013;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#32452;&#20214;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05846v1 Announce Type: cross  Abstract: Text-to-image diffusion models (T2I) use a latent representation of a text prompt to guide the image generation process. However, the process by which the encoder produces the text representation is unknown. We propose the Diffusion Lens, a method for analyzing the text encoder of T2I models by generating images from its intermediate representations. Using the Diffusion Lens, we perform an extensive analysis of two recent T2I models. Exploring compound prompts, we find that complex scenes describing multiple objects are composed progressively and more slowly compared to simple scenes; Exploring knowledge retrieval, we find that representation of uncommon concepts requires further computation compared to common concepts, and that knowledge retrieval is gradual across layers. Overall, our findings provide valuable insights into the text encoder component in T2I pipelines.
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.18292</link><description>&lt;p&gt;
FSL&#27169;&#22411;&#21487;&#20197;&#22240;&#20026;&#20854;&#20248;&#36234;&#24615;&#24471;&#20998;&#26356;&#39640;
&lt;/p&gt;
&lt;p&gt;
FSL Model can Score Higher as It Is
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18292
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#34987;&#27491;&#30830;&#35782;&#21035;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20542;&#21521;&#20110;&#38754;&#23545;&#38754;&#22320;&#30452;&#35270;&#38754;&#37096;&#35782;&#21035;&#26426;&#65292;&#32780;&#19981;&#26159;&#20391;&#30528;&#38754;&#23545;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#20998;&#31867;&#26412;&#36523;&#23601;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;&#23646;&#20110;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#22312;&#27979;&#35797;&#26399;&#38388;&#23545;&#25197;&#26354;&#21644;&#38750;&#20856;&#22411;&#30340;&#26597;&#35810;&#25110;&#25903;&#25345;&#22270;&#20687;&#20250;&#35753;&#27169;&#22411;&#26356;&#38590;&#27491;&#30830;&#39044;&#27979;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;&#35757;&#32451;&#36807;&#30340;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;FSL&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#20855;&#26377;&#36275;&#22815;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#20855;&#26377;&#23569;&#26679;&#26412;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#25429;&#25417;&#27979;&#35797;&#22270;&#20687;&#30340;&#39118;&#26684;&#25110;&#24418;&#29366;&#65292;&#28982;&#21518;&#35782;&#21035;&#19968;&#20010;&#36866;&#24403;&#30340;&#35757;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18292v1 Announce Type: cross  Abstract: In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable traine
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;&#19968;&#31181;&#37325;&#32622;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;"zapping"&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#25345;&#32493;&#21644;&#36801;&#31227;&#23398;&#20064;&#25928;&#26524;&#65292;&#21516;&#26102;&#20855;&#22791;&#31616;&#21333;&#23454;&#26045;&#21644;&#39640;&#25928;&#35745;&#31639;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.07996</link><description>&lt;p&gt;
&#37325;&#32622;&#24182;&#24536;&#21364;&#65306;&#37325;&#26032;&#23398;&#20064;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#25913;&#21892;&#25345;&#32493;&#21644;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reset It and Forget It: Relearning Last-Layer Weights Improves Continual and Transfer Learning. (arXiv:2310.07996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;&#19968;&#31181;&#37325;&#32622;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;"zapping"&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#25345;&#32493;&#21644;&#36801;&#31227;&#23398;&#20064;&#25928;&#26524;&#65292;&#21516;&#26102;&#20855;&#22791;&#31616;&#21333;&#23454;&#26045;&#21644;&#39640;&#25928;&#35745;&#31639;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26426;&#21046;&#65292;&#33021;&#22815;&#23548;&#33268;&#20855;&#26377;&#26356;&#22909;&#30340;&#25345;&#32493;&#21644;&#36801;&#31227;&#23398;&#20064;&#34920;&#24449;&#12290;&#36825;&#31181;&#26426;&#21046;&#8212;&#8212;&#22312;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#20013;&#21453;&#22797;&#37325;&#32622;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;zapping&#8221;&#8212;&#8212;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#20803;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#22312;&#35768;&#22810;&#19981;&#21516;&#20110;&#20803;&#23398;&#20064;&#21644;&#25345;&#32493;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#20063;&#38750;&#24120;&#36866;&#29992;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#23558;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#36801;&#31227;&#21040;&#19968;&#32452;&#26032;&#30340;&#31867;&#21035;&#65292;&#20165;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;zapping&#36807;&#31243;&#22312;&#26631;&#20934;&#24494;&#35843;&#21644;&#25345;&#32493;&#23398;&#20064;&#35774;&#32622;&#20013;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#36801;&#31227;&#20934;&#30830;&#24615;&#21644;/&#25110;&#26356;&#24555;&#30340;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#23454;&#29616;&#31616;&#21333;&#30340;&#23454;&#26045;&#21644;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20351;&#29992;zapping&#21644;&#39034;&#24207;&#23398;&#20064;&#30340;&#32452;&#21512;&#65292;&#25105;&#20204;&#21487;&#20197;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#30340;&#20803;&#23398;&#20064;&#30456;&#24403;&#30340;&#24615;&#33021;&#32780;&#26080;&#38656;&#26114;&#36149;&#30340;&#39640;&#38454;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work identifies a simple pre-training mechanism that leads to representations exhibiting better continual and transfer learning. This mechanism -- the repeated resetting of weights in the last layer, which we nickname "zapping" -- was originally designed for a meta-continual-learning procedure, yet we show it is surprisingly applicable in many settings beyond both meta-learning and continual learning. In our experiments, we wish to transfer a pre-trained image classifier to a new set of classes, in a few shots. We show that our zapping procedure results in improved transfer accuracy and/or more rapid adaptation in both standard fine-tuning and continual learning settings, while being simple to implement and computationally efficient. In many cases, we achieve performance on par with state of the art meta-learning without needing the expensive higher-order gradients, by using a combination of zapping and sequential learning. An intuitive explanation for the effectiveness of this za
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#32467;&#21512;&#20102;&#22810;&#31181;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#65292;&#24182;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01472</link><description>&lt;p&gt;
&#21453;&#21521;&#31283;&#23450;&#25193;&#25955;&#65306;&#29983;&#25104;&#35813;&#22270;&#20687;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Reverse Stable Diffusion: What prompt was used to generate this image?. (arXiv:2308.01472v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#32467;&#21512;&#20102;&#22810;&#31181;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#65292;&#24182;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65292;&#26368;&#36817;&#21560;&#24341;&#20102;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#20852;&#36259;&#65292;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;&#29983;&#25104;&#36807;&#31243;&#21644;&#22914;&#20309;&#35774;&#35745;&#25552;&#31034;&#20197;&#33719;&#24471;&#25152;&#38656;&#22270;&#20687;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#31995;&#21015;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65288;&#26377;&#21644;&#26080;&#23545;&#25193;&#25955;&#32593;&#32476;&#26435;&#37325;&#36827;&#34892;&#35775;&#38382;&#65289;&#26469;&#22788;&#29702;&#25152;&#25552;&#20986;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#32852;&#21512;&#25552;&#31034;&#22238;&#24402;&#21644;&#22810;&#26631;&#31614;&#35789;&#27719;&#20998;&#31867;&#30446;&#26631;&#65292;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#35838;&#31243;&#23398;&#20064;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#20855;&#26377;&#26356;&#20302;&#26631;&#27880;&#22122;&#22768;&#65288;&#21363;&#26356;&#22909;&#23545;&#40784;&#65289;&#30340;&#22270;&#20687;&#25552;&#31034;&#23545;&#30340;&#23398;&#20064;&#65292;&#24182;&#19988;&#20351;&#29992;&#30456;&#20284;&#24615;&#36827;&#34892;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models such as Stable Diffusion have recently attracted the interest of many researchers, and inverting the diffusion process can play an important role in better understanding the generative process and how to engineer prompts in order to obtain the desired images. To this end, we introduce the new task of predicting the text prompt given an image generated by a generative diffusion model. We combine a series of white-box and black-box models (with and without access to the weights of the diffusion network) to deal with the proposed task. We propose a novel learning framework comprising of a joint prompt regression and multi-label vocabulary classification objective that generates improved prompts. To further improve our method, we employ a curriculum learning procedure that promotes the learning of image-prompt pairs with lower labeling noise (i.e. that are better aligned), and an unsupervised domain-adaptive kernel learning method that uses the similarities b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CASSLE&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20462;&#25913;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26377;&#21521;&#25237;&#24433;&#32593;&#32476;&#65292;&#21033;&#29992;&#22686;&#24378;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06082</link><description>&lt;p&gt;
&#22686;&#24378;&#24863;&#30693;&#30340;&#26377;&#21521;&#25237;&#24433;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Augmentation-aware Self-supervised Learning with Guided Projector. (arXiv:2306.06082v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CASSLE&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20462;&#25913;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26377;&#21521;&#25237;&#24433;&#32593;&#32476;&#65292;&#21033;&#29992;&#22686;&#24378;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#20581;&#22766;&#34920;&#31034;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;SimCLR&#21644;MoCo&#31561;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#23545;&#24212;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#20445;&#25345;&#19981;&#21464;&#65292;&#33021;&#22815;&#36798;&#21040;&#19982;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#19981;&#21464;&#24615;&#21487;&#33021;&#23545;&#35299;&#20915;&#26576;&#20123;&#19979;&#28216;&#20219;&#21153;&#26377;&#23475;&#65292;&#36825;&#20123;&#20219;&#21153;&#20381;&#36182;&#20110;&#21463;&#21040;&#39044;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30340;&#22686;&#24378;&#24433;&#21709;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#39068;&#33394;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20462;&#25913;&#33258;&#30417;&#30563;&#26550;&#26500;&#30340;&#24120;&#35265;&#32452;&#20214;&#20043;&#19968;&#30340;&#26377;&#21521;&#25237;&#24433;&#32593;&#32476;&#65292;&#26469;&#20419;&#36827;&#34920;&#31034;&#31354;&#38388;&#23545;&#36825;&#20123;&#29305;&#24449;&#30340;&#25935;&#24863;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;&#25237;&#24433;&#22120;&#34917;&#20805;&#26377;&#20851;&#24212;&#29992;&#20110;&#22270;&#20687;&#30340;&#22686;&#24378;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35753;&#25237;&#24433;&#22120;&#22312;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#26102;&#21033;&#29992;&#36825;&#31181;&#36741;&#21161;&#25351;&#23548;&#65292;&#29305;&#24449;&#25552;&#21462;&#22120;&#23398;&#20064;&#22312;&#20854;&#34920;&#31034;&#20013;&#20445;&#30041;&#22686;&#24378;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#26377;&#21521;&#25237;&#24433;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;CASSLE&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) is a powerful technique for learning robust representations from unlabeled data. By learning to remain invariant to applied data augmentations, methods such as SimCLR and MoCo are able to reach quality on par with supervised approaches. However, this invariance may be harmful to solving some downstream tasks which depend on traits affected by augmentations used during pretraining, such as color. In this paper, we propose to foster sensitivity to such characteristics in the representation space by modifying the projector network, a common component of self-supervised architectures. Specifically, we supplement the projector with information about augmentations applied to images. In order for the projector to take advantage of this auxiliary guidance when solving the SSL task, the feature extractor learns to preserve the augmentation information in its representations. Our approach, coined Conditional Augmentation-aware Selfsupervised Learning (CASSLE), is d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;AI&#31574;&#23637;&#21644;&#35266;&#20247;&#20114;&#21160;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#26032;&#26500;&#24819;&#20102;&#36203;&#23572;&#36763;&#22522;&#24066;&#33402;&#26415;&#21452;&#24180;&#23637;&#65292;&#20351;&#29992;&#35270;&#35273;-&#25991;&#26412;&#27169;&#22411;&#23558;&#23460;&#20869;&#33402;&#26415;&#21697;&#25918;&#32622;&#22312;&#20844;&#20849;&#31354;&#38388;&#20013;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;360&#33402;&#26415;&#20840;&#26223;&#22270;&#65292;&#20197;&#21019;&#36896;&#20986;&#33402;&#26415;&#21697;&#19982;&#22478;&#24066;&#31354;&#38388;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2306.03753</link><description>&lt;p&gt;
AI&#33402;&#26415;&#31574;&#23637;&#65306;&#37325;&#26032;&#26500;&#24819;&#36203;&#23572;&#36763;&#22522;&#24066;&#33402;&#26415;&#21452;&#24180;&#23637;
&lt;/p&gt;
&lt;p&gt;
AI Art Curation: Re-imagining the city of Helsinki in occasion of its Biennial. (arXiv:2306.03753v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;AI&#31574;&#23637;&#21644;&#35266;&#20247;&#20114;&#21160;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#26032;&#26500;&#24819;&#20102;&#36203;&#23572;&#36763;&#22522;&#24066;&#33402;&#26415;&#21452;&#24180;&#23637;&#65292;&#20351;&#29992;&#35270;&#35273;-&#25991;&#26412;&#27169;&#22411;&#23558;&#23460;&#20869;&#33402;&#26415;&#21697;&#25918;&#32622;&#22312;&#20844;&#20849;&#31354;&#38388;&#20013;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;360&#33402;&#26415;&#20840;&#26223;&#22270;&#65292;&#20197;&#21019;&#36896;&#20986;&#33402;&#26415;&#21697;&#19982;&#22478;&#24066;&#31354;&#38388;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#31574;&#23637;&#23454;&#36341;&#30340;&#29305;&#28857;&#26159;&#20197;&#30693;&#35782;&#30340;&#26041;&#24335;&#23637;&#31034;&#33402;&#26415;&#25910;&#34255;&#21697;&#12290;&#26426;&#22120;&#36807;&#31243;&#30340;&#29305;&#28857;&#26159;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#21644;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#12290;&#26412;&#25991;&#35774;&#24819;&#20102;AI&#31574;&#23637;&#21644;&#35266;&#20247;&#20114;&#21160;&#65292;&#20197;&#25506;&#32034;&#24403;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#31574;&#23637;&#30028;&#30340;&#24433;&#21709;&#12290;&#35813;&#39033;&#30446;&#26159;&#20026;2023&#24180;&#36203;&#23572;&#36763;&#22522;&#33402;&#26415;&#21452;&#24180;&#23637;&#30340;&#22330;&#21512;&#32780;&#24320;&#21457;&#30340;&#65292;&#39064;&#20026;&#8220;&#21487;&#33021;&#20986;&#29616;&#26032;&#30340;&#26041;&#21521;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#36203;&#23572;&#36763;&#22522;&#33402;&#26415;&#21338;&#29289;&#39302;&#65288;HAM&#65289;&#30340;&#34255;&#21697;&#65292;&#36890;&#36807;&#26426;&#22120;&#24863;&#30693;&#30340;&#35270;&#35282;&#37325;&#26032;&#26500;&#24819;&#20102;&#36203;&#23572;&#36763;&#22522;&#24066;&#12290;&#25105;&#20204;&#20351;&#29992;&#35270;&#35273;-&#25991;&#26412;&#27169;&#22411;&#22312;&#20844;&#20849;&#31354;&#38388;&#20013;&#23637;&#31034;&#23460;&#20869;&#33402;&#26415;&#21697;&#65292;&#26681;&#25454;&#30456;&#20284;&#24615;&#35780;&#20998;&#20998;&#37197;&#34394;&#26500;&#30340;&#22352;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#30340;360&#33402;&#26415;&#20840;&#26223;&#22270;&#26469;&#25913;&#21464;&#27599;&#20214;&#33402;&#26415;&#21697;&#22312;&#22478;&#24066;&#20013;&#30340;&#25152;&#22788;&#31354;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#27599;&#20214;&#33402;&#26415;&#21697;&#20301;&#32622;&#30340;360&#20840;&#26223;&#22270;&#30340;&#28145;&#24230;&#20540;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#33402;&#26415;&#21697;&#25552;&#31034;&#26469;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#12290;&#36825;&#20010;&#39033;&#30446;&#30340;&#32467;&#26524;&#23601;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Art curatorial practice is characterized by the presentation of an art collection in a knowledgeable way. Machine processes are characterized by their capacity to manage and analyze large amounts of data. This paper envisages AI curation and audience interaction to explore the implications of contemporary machine learning models for the curatorial world. This project was developed for the occasion of the 2023 Helsinki Art Biennial, entitled New Directions May Emerge. We use the Helsinki Art Museum (HAM) collection to re-imagine the city of Helsinki through the lens of machine perception. We use visual-textual models to place indoor artworks in public spaces, assigning fictional coordinates based on similarity scores. We transform the space that each artwork inhabits in the city by generating synthetic 360 art panoramas. We guide the generation estimating depth values from 360 panoramas at each artwork location, and machine-generated prompts of the artworks. The result of this project i
&lt;/p&gt;</description></item><item><title>&#24425;&#34425;&#32593;&#32476;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#36890;&#36807;&#23618;&#20869;&#31070;&#32463;&#20803;&#26435;&#37325;&#20114;&#30456;&#29420;&#31435;&#30340;&#23545;&#40784;&#21644;&#38543;&#26426;&#29305;&#24449;&#26144;&#23556;&#26469;&#36827;&#34892;&#32447;&#24615;&#38477;&#32500;&#21644;&#38750;&#32447;&#24615;&#39640;&#32500;&#23884;&#20837;&#65292;&#22312;ImageNet&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.18512</link><description>&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#40657;&#30418;&#20013;&#30340;&#24425;&#34425;
&lt;/p&gt;
&lt;p&gt;
A Rainbow in Deep Network Black Boxes. (arXiv:2305.18512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18512
&lt;/p&gt;
&lt;p&gt;
&#24425;&#34425;&#32593;&#32476;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#36890;&#36807;&#23618;&#20869;&#31070;&#32463;&#20803;&#26435;&#37325;&#20114;&#30456;&#29420;&#31435;&#30340;&#23545;&#40784;&#21644;&#38543;&#26426;&#29305;&#24449;&#26144;&#23556;&#26469;&#36827;&#34892;&#32447;&#24615;&#38477;&#32500;&#21644;&#38750;&#32447;&#24615;&#39640;&#32500;&#23884;&#20837;&#65292;&#22312;ImageNet&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24425;&#34425;&#32593;&#32476;&#20316;&#20026;&#35757;&#32451;&#22909;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32423;&#32852;&#38543;&#26426;&#29305;&#24449;&#26144;&#23556;&#65292;&#20854;&#26435;&#37325;&#20998;&#24067;&#26159;&#21487;&#20197;&#23398;&#20064;&#30340;&#12290;&#23427;&#20551;&#35774;&#19981;&#21516;&#23618;&#20043;&#38388;&#30340;&#26435;&#37325;&#20381;&#36182;&#24615;&#34987;&#20943;&#23569;&#21040;&#23558;&#36755;&#20837;&#28608;&#27963;&#23545;&#20934;&#30340;&#26059;&#36716;&#12290;&#23618;&#20869;&#30340;&#31070;&#32463;&#20803;&#26435;&#37325;&#22312;&#36825;&#31181;&#23545;&#40784;&#21518;&#26159;&#30456;&#20114;&#29420;&#31435;&#30340;&#12290;&#23427;&#20204;&#30340;&#28608;&#27963;&#23450;&#20041;&#20102;&#22312;&#26080;&#31351;&#23485;&#24230;&#26497;&#38480;&#19979;&#21464;&#24471;&#30830;&#23450;&#30340;&#20869;&#26680;&#12290;&#36825;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;ResNets&#20013;&#36890;&#36807;&#25968;&#23383;&#39564;&#35777;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#23398;&#20064;&#30340;&#26435;&#37325;&#20998;&#24067;&#20855;&#26377;&#20302;&#31209;&#21327;&#26041;&#24046;&#12290;&#22240;&#27492;&#65292;&#24425;&#34425;&#32593;&#32476;&#22312;&#32447;&#24615;&#38477;&#32500;&#21644;&#38750;&#32447;&#24615;&#39640;&#32500;&#23884;&#20837;&#19982;&#30333;&#33394;&#38543;&#26426;&#29305;&#24449;&#20043;&#38388;&#20132;&#26367;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20855;&#26377;&#39640;&#26031;&#26435;&#37325;&#20998;&#24067;&#30340;&#39640;&#26031;&#24425;&#34425;&#32593;&#32476;&#23450;&#20041;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20351;&#29992;&#23567;&#27874;&#25955;&#23556;&#32593;&#32476;&#36827;&#34892;CIFAR-10&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#36827;&#34892;&#20102;&#25968;&#23383;&#39564;&#35777;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;SGD&#26356;&#26032;&#26435;&#37325;&#30340;&#21327;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce rainbow networks as a probabilistic model of trained deep neural networks. The model cascades random feature maps whose weight distributions are learned. It assumes that dependencies between weights at different layers are reduced to rotations which align the input activations. Neuron weights within a layer are independent after this alignment. Their activations define kernels which become deterministic in the infinite-width limit. This is verified numerically for ResNets trained on the ImageNet dataset. We also show that the learned weight distributions have low-rank covariances. Rainbow networks thus alternate between linear dimension reductions and non-linear high-dimensional embeddings with white random features. Gaussian rainbow networks are defined with Gaussian weight distributions. These models are validated numerically on image classification on the CIFAR-10 dataset, with wavelet scattering networks. We further show that during training, SGD updates the weight cov
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38647;&#36798;&#36870;&#21521;&#20256;&#24863;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#31232;&#30095;&#38647;&#36798;&#26816;&#27979;&#26144;&#23556;&#21040;&#26497;&#22352;&#26631;&#27979;&#37327;&#32593;&#26684;&#65292;&#24182;&#29983;&#25104;&#21160;&#24577;&#32593;&#26684;&#22320;&#22270;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#20960;&#20309;ISM&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20026;&#20174;&#26377;&#38480;&#35270;&#22330;&#30340;&#38647;&#36798;&#20013;&#23398;&#20064;&#26497;&#22352;&#26631;&#26041;&#26696;&#30340;&#21333;&#24103;&#27979;&#37327;&#32593;&#26684;&#30340;&#31532;&#19968;&#20010;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.12409</link><description>&lt;p&gt;
&#21160;&#24577;&#21344;&#25454;&#32593;&#26684;&#22320;&#22270;&#30340;&#28145;&#24230;&#38647;&#36798;&#36870;&#21521;&#20256;&#24863;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Radar Inverse Sensor Models for Dynamic Occupancy Grid Maps. (arXiv:2305.12409v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12409
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38647;&#36798;&#36870;&#21521;&#20256;&#24863;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#31232;&#30095;&#38647;&#36798;&#26816;&#27979;&#26144;&#23556;&#21040;&#26497;&#22352;&#26631;&#27979;&#37327;&#32593;&#26684;&#65292;&#24182;&#29983;&#25104;&#21160;&#24577;&#32593;&#26684;&#22320;&#22270;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#20960;&#20309;ISM&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20026;&#20174;&#26377;&#38480;&#35270;&#22330;&#30340;&#38647;&#36798;&#20013;&#23398;&#20064;&#26497;&#22352;&#26631;&#26041;&#26696;&#30340;&#21333;&#24103;&#27979;&#37327;&#32593;&#26684;&#30340;&#31532;&#19968;&#20010;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#19968;&#20010;&#37325;&#35201;&#27493;&#39588;&#26159;&#22522;&#20110;&#20256;&#24863;&#22120;&#36755;&#20837;&#23545;&#36710;&#36742;&#29615;&#22659;&#36827;&#34892;&#24314;&#27169;&#12290;&#30001;&#20110;&#20854;&#20247;&#25152;&#21608;&#30693;&#30340;&#20248;&#21183;&#65292;&#38647;&#36798;&#25104;&#20026;&#25512;&#26029;&#22260;&#32469;&#36710;&#36742;&#30340;&#32593;&#26684;&#21333;&#20803;&#21344;&#29992;&#29366;&#24577;&#30340;&#27969;&#34892;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#38647;&#36798;&#26816;&#27979;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36870;&#21521;&#20256;&#24863;&#22120;&#27169;&#22411;&#65288;ISM&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#20174;&#31232;&#30095;&#38647;&#36798;&#26816;&#27979;&#21040;&#26497;&#22352;&#26631;&#27979;&#37327;&#32593;&#26684;&#30340;&#26144;&#23556;&#12290;&#25913;&#36827;&#30340;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#27979;&#37327;&#30340;&#32593;&#26684;&#29992;&#20316;&#21442;&#32771;&#12290;&#23398;&#20064;&#21040;&#30340;&#38647;&#36798;&#27979;&#37327;&#32593;&#26684;&#19982;&#38647;&#36798;&#22810;&#26222;&#21202;&#36895;&#24230;&#27979;&#37327;&#30456;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#29992;&#20110;&#29983;&#25104;&#21160;&#24577;&#32593;&#26684;&#22320;&#22270;&#65288;DGM&#65289;&#12290;&#22312;&#23454;&#38469;&#30340;&#39640;&#36895;&#20844;&#36335;&#24773;&#26223;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#20960;&#20309;ISM&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#20174;&#26377;&#38480;&#35270;&#22330;&#65288;FOV&#65289;&#30340;&#38647;&#36798;&#20013;&#23398;&#20064;&#26497;&#22352;&#26631;&#26041;&#26696;&#30340;&#21333;&#24103;&#27979;&#37327;&#32593;&#26684;&#30340;&#26041;&#27861;&#12290;&#23398;&#20064;&#26694;&#26550;&#20351;&#23398;&#20064;&#21040;&#30340;ISM&#21487;&#20197;&#30452;&#25509;&#23884;&#20837;&#21040;&#29616;&#26377;&#30340;&#36125;&#21494;&#26031;&#29366;&#24577;&#20272;&#35745;&#26041;&#26696;&#20013;&#65292;&#20197;&#25552;&#39640;&#29615;&#22659;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To implement autonomous driving, one essential step is to model the vehicle environment based on the sensor inputs. Radars, with their well-known advantages, became a popular option to infer the occupancy state of grid cells surrounding the vehicle. To tackle data sparsity and noise of radar detections, we propose a deep learning-based Inverse Sensor Model (ISM) to learn the mapping from sparse radar detections to polar measurement grids. Improved lidar-based measurement grids are used as reference. The learned radar measurement grids, combined with radar Doppler velocity measurements, are further used to generate a Dynamic Grid Map (DGM). Experiments in real-world highway scenarios show that our approach outperforms the hand-crafted geometric ISMs. In comparison to state-of-the-art deep learning methods, our approach is the first one to learn a single-frame measurement grid in the polar scheme from radars with a limited Field Of View (FOV). The learning framework makes the learned ISM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#20998;&#24067;&#22806;&#26816;&#27979;&#23545;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#21487;&#20197;&#20998;&#35299;&#25104;&#20219;&#21153;&#20869;&#39044;&#27979;&#21644;&#20219;&#21153; ID &#39044;&#27979;&#65292;&#24182;&#19988;&#20219;&#21153; ID &#39044;&#27979;&#19982;&#20998;&#24067;&#22806;&#26816;&#27979;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2304.10038</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#25345;&#32493;&#23398;&#20064;&#65306;&#32479;&#19968;&#26032;&#39062;&#24615;&#26816;&#27979;&#19982;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Open-World Continual Learning: Unifying Novelty Detection and Continual Learning. (arXiv:2304.10038v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#20998;&#24067;&#22806;&#26816;&#27979;&#23545;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#21487;&#20197;&#20998;&#35299;&#25104;&#20219;&#21153;&#20869;&#39044;&#27979;&#21644;&#20219;&#21153; ID &#39044;&#27979;&#65292;&#24182;&#19988;&#20219;&#21153; ID &#39044;&#27979;&#19982;&#20998;&#24067;&#22806;&#26816;&#27979;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528; AI agent &#22312;&#26410;&#30693;&#25110;&#26032;&#22855;&#30340;&#30495;&#23454;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#23427;&#20204;&#38656;&#35201;&#20855;&#22791; (1) &#35748;&#35782;&#24050;&#32463;&#23398;&#20064;&#36807;&#30340;&#29289;&#20307;&#21644;&#26816;&#27979;&#21040;&#20043;&#21069;&#26410;&#35265;&#25110;&#23398;&#20064;&#30340;&#29289;&#20307;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450; (2) &#22686;&#37327;&#22320;&#23398;&#20064;&#26032;&#29289;&#21697;&#65292;&#36880;&#28176;&#21464;&#24471;&#26356;&#26377;&#30693;&#35782;&#21644;&#26356;&#24378;&#22823;&#12290; (1) &#31216;&#20026;&#26032;&#39062;&#24615;&#26816;&#27979;&#25110;&#20998;&#24067;&#22806; (OOD) &#26816;&#27979;&#65292;&#32780; (2) &#31216;&#20026;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064; (CIL)&#65292;&#26159;&#25345;&#32493;&#23398;&#20064; (CL) &#30340;&#19968;&#31181;&#35774;&#32622;&#12290;&#22312;&#29616;&#26377;&#30340;&#30740;&#31350;&#20013;&#65292;OOD &#26816;&#27979;&#21644; CIL &#34987;&#35270;&#20026;&#20004;&#20010;&#23436;&#20840;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102; OOD &#26816;&#27979;&#23454;&#38469;&#19978;&#23545;&#20110; CIL &#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034; CIL &#21487;&#20197;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65306;&#20219;&#21153;&#20869;&#39044;&#27979; (WP) &#21644;&#20219;&#21153; ID &#39044;&#27979;(TP)&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#20102; TP &#19982; OOD &#26816;&#27979;&#30456;&#20851;&#12290;&#20851;&#38190;&#30340;&#29702;&#35770;&#32467;&#26524;&#26159;&#65292;&#26080;&#35770; WP &#21644; OOD &#26816;&#27979;&#65288;&#25110; TP&#65289;&#26159;&#21542;&#30001; CIL &#31639;&#27861;&#26174;&#24335;&#25110;&#38544;&#24335;&#22320;&#23450;&#20041;&#65292;&#22909;&#30340; WP &#21644;&#33391;&#22909;&#30340; OOD &#26816;&#27979;&#25110; TP &#24635;&#26159;&#23384;&#22312;&#23884;&#20837;&#22312;&#20219;&#20309; CIL &#31639;&#27861;&#20013;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI agents are increasingly used in the real open world with unknowns or novelties, they need the ability to (1) recognize objects that (i) they have learned and (ii) detect items that they have not seen or learned before, and (2) learn the new items incrementally to become more and more knowledgeable and powerful. (1) is called novelty detection or out-of-distribution (OOD) detection and (2) is called class incremental learning (CIL), which is a setting of continual learning (CL). In existing research, OOD detection and CIL are regarded as two completely different problems. This paper theoretically proves that OOD detection actually is necessary for CIL. We first show that CIL can be decomposed into two sub-problems: within-task prediction (WP) and task-id prediction (TP). We then prove that TP is correlated with OOD detection. The key theoretical result is that regardless of whether WP and OOD detection (or TP) are defined explicitly or implicitly by a CIL algorithm, good WP and go
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21483;&#20570;Robustmix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#32593;&#32476;&#20197;&#20302;&#39057;&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#26469;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;Imagenet-C&#21644;Stylized Imagenet&#31561;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#29366;&#24577;&#24179;&#22343;&#23792;&#20540;&#35823;&#24046;&#65288;mCE&#65289;&#65292;&#22312;&#36991;&#20813;&#35745;&#31639;&#24320;&#38144;&#21644;&#20808;&#39564;&#30693;&#35782;&#30340;&#22823;&#37327;&#22270;&#20687;&#21464;&#25442;&#30340;&#21516;&#26102;&#23545;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#26032;&#36827;&#23637;&#25552;&#20379;&#20102;&#34917;&#20805;&#12290;</title><link>http://arxiv.org/abs/2304.02847</link><description>&lt;p&gt;
Robustmix&#65306;&#36890;&#36807;&#27491;&#21017;&#21270;&#28145;&#24230;&#32593;&#32476;&#30340;&#39057;&#29575;&#20559;&#24046;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustmix: Improving Robustness by Regularizing the Frequency Bias of Deep Nets. (arXiv:2304.02847v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21483;&#20570;Robustmix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#32593;&#32476;&#20197;&#20302;&#39057;&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#26469;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;Imagenet-C&#21644;Stylized Imagenet&#31561;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#29366;&#24577;&#24179;&#22343;&#23792;&#20540;&#35823;&#24046;&#65288;mCE&#65289;&#65292;&#22312;&#36991;&#20813;&#35745;&#31639;&#24320;&#38144;&#21644;&#20808;&#39564;&#30693;&#35782;&#30340;&#22823;&#37327;&#22270;&#20687;&#21464;&#25442;&#30340;&#21516;&#26102;&#23545;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#26032;&#36827;&#23637;&#25552;&#20379;&#20102;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#22312;&#19968;&#31995;&#21015;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23545;&#20110;&#23545;&#20154;&#31867;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#30340;&#25200;&#21160;&#20173;&#28982;&#24456;&#25935;&#24863;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Robustmix&#30340;Mixup&#26032;&#25193;&#23637;&#65292;&#35813;&#25193;&#23637;&#36890;&#36807;&#27491;&#21017;&#21270;&#32593;&#32476;&#20197;&#22522;&#20110;&#20302;&#39057;&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#27491;&#21017;&#21270;&#25913;&#21892;&#20102;&#22312;&#19968;&#31995;&#21015;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#20363;&#22914;Imagenet-C&#21644;Stylized Imagenet&#12290;&#23427;&#20960;&#20046;&#27809;&#26377;&#35745;&#31639;&#24320;&#38144;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#22823;&#37327;&#22270;&#20687;&#21464;&#25442;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#36827;&#19968;&#27493;&#34917;&#20805;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20351;&#29992;EfficientNet-B8&#27169;&#22411;&#21644;RandAugment&#36798;&#21040;&#20102;44.8&#30340;&#26368;&#26032;&#29366;&#24577;&#24179;&#22343;&#23792;&#20540;&#35823;&#24046;&#65288;mCE&#65289;&#65292;&#30456;&#27604;&#22522;&#32447;&#38477;&#20302;&#20102;16&#20010;mCE&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep networks have achieved impressive results on a range of well-curated benchmark datasets. Surprisingly, their performance remains sensitive to perturbations that have little effect on human performance. In this work, we propose a novel extension of Mixup called Robustmix that regularizes networks to classify based on lower-frequency spatial features. We show that this type of regularization improves robustness on a range of benchmarks such as Imagenet-C and Stylized Imagenet. It adds little computational overhead and, furthermore, does not require a priori knowledge of a large set of image transformations. We find that this approach further complements recent advances in model architecture and data augmentation, attaining a state-of-the-art mCE of 44.8 with an EfficientNet-B8 model and RandAugment, which is a reduction of 16 mCE compared to the baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26080;&#38480;&#32500;&#24230;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#27700;&#24179;&#19978;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#32423;&#25193;&#25955;&#31639;&#27861;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#39640;&#25928;&#22320;&#23398;&#20064;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30456;&#21516;&#25110;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#20135;&#29983;&#27604;&#20256;&#32479;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#24182;&#22788;&#29702;&#30697;&#24418;&#22495;&#12290;</title><link>http://arxiv.org/abs/2303.04772</link><description>&lt;p&gt;
&#22810;&#32423;&#25193;&#25955;&#65306;&#22270;&#20687;&#29983;&#25104;&#30340;&#26080;&#38480;&#32500;&#24230;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation. (arXiv:2303.04772v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26080;&#38480;&#32500;&#24230;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#27700;&#24179;&#19978;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#32423;&#25193;&#25955;&#31639;&#27861;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#39640;&#25928;&#22320;&#23398;&#20064;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30456;&#21516;&#25110;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#20135;&#29983;&#27604;&#20256;&#32479;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#24182;&#22788;&#29702;&#30697;&#24418;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26159;&#36817;&#24180;&#26469;&#22270;&#20687;&#29983;&#25104;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20043;&#19968;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#36890;&#24120;&#22312;&#26377;&#38480;&#32500;&#24230;&#35774;&#32622;&#20013;&#34920;&#36848;&#65292;&#20854;&#20013;&#22270;&#20687;&#34987;&#35270;&#20026;&#20855;&#26377;&#26377;&#38480;&#23610;&#23544;&#30340;&#24352;&#37327;&#12290;&#26412;&#25991;&#22312;&#26080;&#38480;&#32500;&#24230;&#35774;&#32622;&#20013;&#24320;&#21457;&#20102;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21363;&#25105;&#20204;&#23558;&#35757;&#32451;&#25968;&#25454;&#24314;&#27169;&#20026;&#25903;&#25745;&#22312;&#30697;&#24418;&#22495;&#19978;&#30340;&#20989;&#25968;&#12290;&#38500;&#20102;&#36861;&#27714;&#22312;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#29983;&#25104;&#22270;&#20687;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#21019;&#24314;&#19968;&#20010;&#33391;&#22909;&#23450;&#20041;&#30340;&#26080;&#38480;&#32500;&#24230;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#20415;&#21487;&#20197;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#27700;&#24179;&#19978;&#19968;&#33268;&#22320;&#31163;&#25955;&#21270;&#23427;&#12290;&#25105;&#20204;&#24076;&#26395;&#33719;&#24471;&#33021;&#22815;&#27178;&#36328;&#19981;&#21516;&#20998;&#36776;&#29575;&#32423;&#21035;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#39640;&#35757;&#32451;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20811;&#26381;&#24403;&#21069;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#26080;&#38480;&#32500;&#24230;&#35774;&#32622;&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#21069;&#21521;&#36807;&#31243;&#20197;&#30830;&#20445;&#22312;&#26080;&#38480;&#32500;&#24230;&#35774;&#32622;&#20013;&#28508;&#22312;&#20998;&#24067;&#26159;&#33391;&#22909;&#23450;&#20041;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#25193;&#25955;&#31639;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#39640;&#25928;&#22320;&#23398;&#20064;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22810;&#32423;&#27169;&#22411;&#22312;&#30456;&#21516;&#25110;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#20135;&#29983;&#27604;&#20256;&#32479;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#22320;&#29983;&#25104;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#24182;&#22788;&#29702;&#30697;&#24418;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based diffusion models (SBDM) have recently emerged as state-of-the-art approaches for image generation. Existing SBDMs are typically formulated in a finite-dimensional setting, where images are considered as tensors of a finite size. This papers develops SBDMs in the infinite-dimensional setting, that is, we model the training data as functions supported on a rectangular domain. Besides the quest for generating images at ever higher resolution our primary motivation is to create a well-posed infinite-dimensional learning problem so that we can discretize it consistently on multiple resolution levels. We thereby hope to obtain diffusion models that generalize across different resolution levels and improve the efficiency of the training process. We demonstrate how to overcome two shortcomings of current SBDM approaches in the infinite-dimensional setting. First, we modify the forward process to ensure that the latent distribution is well-defined in the infinite-dimensional setting
&lt;/p&gt;</description></item></channel></rss>