<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.12510</link><description>&lt;p&gt;
&#22270;&#20687;&#25805;&#20316;&#30340;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generalized Consistency Trajectory Models for Image Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26080;&#26465;&#20214;&#29983;&#25104;&#20197;&#21450;&#22270;&#20687;&#32534;&#36753;&#21644;&#24674;&#22797;&#31561;&#24212;&#29992;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#22312;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#24615;&#36136;&#65306;&#25193;&#25955;&#23558;&#23558;&#22122;&#22768;&#21040;&#25968;&#25454;&#30340;&#22797;&#26434;&#26144;&#23556;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#21435;&#22122;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#27880;&#20837;&#24341;&#23548;&#39033;&#65292;&#25105;&#20204;&#33021;&#22815;&#23545;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#36845;&#20195;&#36807;&#31243;&#20063;&#24120;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#25968;&#21313;&#27425;&#29978;&#33267;&#25968;&#21315;&#27425;&#20989;&#25968;&#35780;&#20272;&#12290;&#34429;&#28982;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTMs&#65289;&#21487;&#20197;&#22312;&#27010;&#29575;&#27969;ODE&#65288;PFODE&#65289;&#19978;&#20219;&#24847;&#26102;&#38388;&#28857;&#20043;&#38388;&#36827;&#34892;&#36941;&#21382;&#65292;&#24182;&#19988;&#36890;&#36807;&#21333;&#27425;&#20989;&#25968;&#35780;&#20272;&#36827;&#34892;&#24471;&#20998;&#25512;&#23548;&#65292;&#20294;CTMs&#20165;&#20801;&#35768;&#20174;&#39640;&#26031;&#22122;&#22768;&#36716;&#25442;&#20026;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#24191;&#20041;CTMs&#65288;GCTMs&#65289;&#26469;&#21457;&#25381;CTMs&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23454;&#29616;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12510v1 Announce Type: cross  Abstract: Diffusion-based generative models excel in unconditional generation, as well as on applied tasks such as image editing and restoration. The success of diffusion models lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of simple denoising tasks. Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step. However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations. Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data. Thus, this work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbit
&lt;/p&gt;</description></item><item><title>SELFI&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#19982;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#24555;&#36895;&#25913;&#36827;&#65292;&#24182;&#22312;&#36991;&#25758;&#21644;&#31038;&#20132;&#36981;&#20174;&#34892;&#20026;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.00991</link><description>&lt;p&gt;
SELFI: &#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#33258;&#20027;&#33258;&#25105;&#25913;&#36827;&#20197;&#36827;&#34892;&#31038;&#20132;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
SELFI: Autonomous Self-Improvement with Reinforcement Learning for Social Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00991
&lt;/p&gt;
&lt;p&gt;
SELFI&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#19982;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#24555;&#36895;&#25913;&#36827;&#65292;&#24182;&#22312;&#36991;&#25758;&#21644;&#31038;&#20132;&#36981;&#20174;&#34892;&#20026;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#33258;&#25105;&#25913;&#36827;&#30340;&#26426;&#22120;&#20154;&#36890;&#36807;&#19982;&#29615;&#22659;&#20114;&#21160;&#21644;&#32463;&#39564;&#31215;&#32047;&#26469;&#23454;&#29616;&#23558;&#26159;&#26426;&#22120;&#20154;&#31995;&#32479;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25237;&#20837;&#20351;&#29992;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;SELFI&#65292;&#21033;&#29992;&#22312;&#32447;&#26426;&#22120;&#20154;&#32463;&#39564;&#26469;&#24555;&#36895;&#39640;&#25928;&#22320;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;SELFI&#23558;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#20043;&#19978;&#65292;&#20197;&#21457;&#25381;&#36825;&#20004;&#31181;&#23398;&#20064;&#33539;&#24335;&#30340;&#20248;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SELFI&#36890;&#36807;&#23558;&#31163;&#32447;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#23398;&#20064;&#30446;&#26631;&#19982;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#21040;&#30340;Q&#20540;&#30456;&#32467;&#21512;&#65292;&#31283;&#23450;&#20102;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#29616;&#23454;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;SELFI&#65292;&#24182;&#25253;&#21578;&#20102;&#22312;&#36991;&#25758;&#26041;&#38754;&#30340;&#25913;&#21892;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#29992;&#25143;&#30740;&#31350;&#27979;&#37327;&#30340;&#26356;&#20855;&#31038;&#20132;&#36981;&#20174;&#34892;&#20026;&#12290;SELFI&#20351;&#25105;&#20204;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#26377;&#29992;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#20943;&#23569;&#20102;&#39044;&#20808;&#24178;&#39044;&#30340;&#20154;&#21592;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00991v1 Announce Type: cross  Abstract: Autonomous self-improving robots that interact and improve with experience are key to the real-world deployment of robotic systems. In this paper, we propose an online learning method, SELFI, that leverages online robot experience to rapidly fine-tune pre-trained control policies efficiently. SELFI applies online model-free reinforcement learning on top of offline model-based learning to bring out the best parts of both learning paradigms. Specifically, SELFI stabilizes the online learning process by incorporating the same model-based learning objective from offline pre-training into the Q-values learned with online model-free reinforcement learning. We evaluate SELFI in multiple real-world environments and report improvements in terms of collision avoidance, as well as more socially compliant behavior, measured by a human user study. SELFI enables us to quickly learn useful robotic behaviors with less human interventions such as pre-e
&lt;/p&gt;</description></item><item><title>&#23637;&#31034;&#33258;&#21160;&#24494;&#20998;&#22312;&#35745;&#31639;&#21644;&#25511;&#21046;&#38544;&#24335;&#32447;&#24615;&#31639;&#23376;&#39057;&#35889;&#20013;&#30340;&#26377;&#25928;&#24615;&#65307;&#25552;&#20379;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#19968;&#33324;&#21367;&#31215;&#23618;&#30340;&#35009;&#21098;&#26041;&#27861;&#65307;&#30740;&#31350;&#20102;&#25209;&#24402;&#19968;&#21270;&#23618;&#19982;&#21367;&#31215;&#23618;&#32452;&#21512;&#30340;&#25928;&#26524;&#65307;&#36890;&#36807;&#27604;&#36739;&#31639;&#27861;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#31934;&#24230;&#21644;&#24615;&#33021;&#34920;&#26126;&#26356;&#31934;&#30830;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.16017</link><description>&lt;p&gt;
&#38544;&#24335;&#32447;&#24615;&#23618;&#30340;&#39057;&#35889;&#25552;&#21462;&#21644;&#35009;&#21098;
&lt;/p&gt;
&lt;p&gt;
Spectrum Extraction and Clipping for Implicitly Linear Layers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16017
&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#33258;&#21160;&#24494;&#20998;&#22312;&#35745;&#31639;&#21644;&#25511;&#21046;&#38544;&#24335;&#32447;&#24615;&#31639;&#23376;&#39057;&#35889;&#20013;&#30340;&#26377;&#25928;&#24615;&#65307;&#25552;&#20379;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#19968;&#33324;&#21367;&#31215;&#23618;&#30340;&#35009;&#21098;&#26041;&#27861;&#65307;&#30740;&#31350;&#20102;&#25209;&#24402;&#19968;&#21270;&#23618;&#19982;&#21367;&#31215;&#23618;&#32452;&#21512;&#30340;&#25928;&#26524;&#65307;&#36890;&#36807;&#27604;&#36739;&#31639;&#27861;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#31934;&#24230;&#21644;&#24615;&#33021;&#34920;&#26126;&#26356;&#31934;&#30830;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#21160;&#24494;&#20998;&#22312;&#39640;&#25928;&#20934;&#30830;&#35745;&#31639;&#21644;&#25511;&#21046;&#38544;&#24335;&#32447;&#24615;&#31639;&#23376;&#30340;&#39057;&#35889;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#26159;&#19968;&#31867;&#21253;&#25324;&#25152;&#26377;&#26631;&#20934;&#21367;&#31215;&#21644;&#20840;&#36830;&#25509;&#23618;&#30340;&#20016;&#23500;&#23618;&#31867;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#19968;&#33324;&#21367;&#31215;&#23618;&#30340;&#35009;&#21098;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#23548;&#33268;&#20043;&#21069;&#24037;&#20316;&#20013;&#27491;&#30830;&#24615;&#38382;&#39064;&#30340;&#34920;&#31034;&#38480;&#21046;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25209;&#24402;&#19968;&#21270;&#23618;&#19982;&#21367;&#31215;&#23618;&#20018;&#32852;&#26102;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35009;&#21098;&#26041;&#27861;&#22914;&#20309;&#24212;&#29992;&#20110;&#23427;&#20204;&#30340;&#32452;&#21512;&#12290;&#36890;&#36807;&#23545;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#31934;&#24230;&#21644;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#23454;&#39564;&#34920;&#26126;&#23427;&#20204;&#26356;&#31934;&#30830;&#21644;&#39640;&#25928;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#30340;&#20195;&#30721;&#38142;&#25509;https://github.com/Ali-E/FastClip&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16017v1 Announce Type: new  Abstract: We show the effectiveness of automatic differentiation in efficiently and correctly computing and controlling the spectrum of implicitly linear operators, a rich family of layer types including all standard convolutional and dense layers. We provide the first clipping method which is correct for general convolution layers, and illuminate the representational limitation that caused correctness issues in prior work. We study the effect of the batch normalization layers when concatenated with convolutional layers and show how our clipping method can be applied to their composition. By comparing the accuracy and performance of our algorithms to the state-of-the-art methods, using various experiments, we show they are more precise and efficient and lead to better generalization and adversarial robustness. We provide the code for using our methods at https://github.com/Ali-E/FastClip.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#31163;&#25955;&#25193;&#25955;&#32467;&#21512;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26426;&#22120;&#20154;&#35270;&#39057;&#24494;&#35843;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;&#35270;&#39057;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;</title><link>https://arxiv.org/abs/2402.14407</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#36827;&#34892;&#22823;&#35268;&#27169;&#26080;&#21160;&#20316;&#35270;&#39057;&#39044;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14407
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31163;&#25955;&#25193;&#25955;&#32467;&#21512;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26426;&#22120;&#20154;&#35270;&#39057;&#24494;&#35843;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;&#35270;&#39057;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#23436;&#25104;&#22810;&#20010;&#20219;&#21153;&#30340;&#36890;&#29992;&#23454;&#20307;&#20195;&#29702;&#38754;&#20020;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#33258;&#32570;&#20047;&#26377;&#26631;&#35760;&#21160;&#20316;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23384;&#22312;&#22823;&#37327;&#25429;&#25417;&#22797;&#26434;&#20219;&#21153;&#21644;&#19982;&#29289;&#29702;&#19990;&#30028;&#20114;&#21160;&#30340;&#20154;&#31867;&#35270;&#39057;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#26694;&#26550;&#65292;&#21033;&#29992;&#32479;&#19968;&#30340;&#31163;&#25955;&#25193;&#25955;&#23558;&#20154;&#31867;&#35270;&#39057;&#19978;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#23569;&#37327;&#26377;&#26631;&#35760;&#26426;&#22120;&#20154;&#35270;&#39057;&#19978;&#30340;&#31574;&#30053;&#24494;&#35843;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#35270;&#39057;&#21387;&#32553;&#25104;&#32479;&#19968;&#30340;&#35270;&#39057;&#26631;&#35760;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#33945;&#29256;&#26367;&#25442;&#25193;&#25955;&#31574;&#30053;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#26469;&#39044;&#27979;&#28508;&#31354;&#38388;&#20013;&#30340;&#26410;&#26469;&#35270;&#39057;&#26631;&#35760;&#12290;&#22312;&#24494;&#35843;&#38454;&#27573;&#65292;&#25105;&#20204; h
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14407v1 Announce Type: new  Abstract: Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. In this paper, we introduce a novel framework that leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we h
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Switch EMA&#65288;SEMA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#20462;&#25913;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;EMA&#65289;&#21442;&#25968;&#65292;&#21487;&#20197;&#24110;&#21161;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36798;&#21040;&#26356;&#22909;&#30340;&#24179;&#22374;&#24615;&#21644;&#38160;&#24230;&#30340;&#27010;&#25324;&#26368;&#20248;&#35299;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;SEMA&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09240</link><description>&lt;p&gt;
Switch EMA: &#26356;&#22909;&#30340;&#24179;&#22374;&#24615;&#21644;&#38160;&#24230;&#30340;&#20813;&#36153;&#21320;&#39184;
&lt;/p&gt;
&lt;p&gt;
Switch EMA: A Free Lunch for Better Flatness and Sharpness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Switch EMA&#65288;SEMA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#20462;&#25913;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;EMA&#65289;&#21442;&#25968;&#65292;&#21487;&#20197;&#24110;&#21161;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36798;&#21040;&#26356;&#22909;&#30340;&#24179;&#22374;&#24615;&#21644;&#38160;&#24230;&#30340;&#27010;&#25324;&#26368;&#20248;&#35299;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;SEMA&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;EMA&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26435;&#37325;&#24179;&#22343;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20248;&#21270;&#20013;&#23398;&#20064;&#24179;&#22374;&#30340;&#26368;&#20248;&#35299;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27010;&#25324;&#33021;&#21147;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#23613;&#31649;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24179;&#22374;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#24179;&#22343;&#26041;&#27861;&#21487;&#33021;&#38519;&#20837;&#26356;&#24046;&#30340;&#26368;&#32456;&#24615;&#33021;&#25110;&#38656;&#35201;&#39069;&#22806;&#30340;&#27979;&#35797;&#26102;&#38388;&#35745;&#31639;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#20462;&#25913;&#65292;&#21363;&#22312;&#27599;&#20010;&#21608;&#26399;&#21518;&#23558;EMA&#21442;&#25968;&#20999;&#25442;&#22238;&#21407;&#22987;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;EMA&#30340;&#20805;&#20998;&#28508;&#21147;&#65292;&#34987;&#31216;&#20026;Switch EMA&#65288;SEMA&#65289;&#12290;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#26041;&#38754;&#37117;&#35777;&#26126;&#20102;SEMA&#21487;&#20197;&#24110;&#21161;DNNs&#36798;&#21040;&#26356;&#22909;&#30340;&#24179;&#22374;&#24615;&#21644;&#38160;&#24230;&#20043;&#38388;&#24179;&#34913;&#30340;&#27010;&#25324;&#26368;&#20248;&#35299;&#12290;&#20026;&#20102;&#39564;&#35777;SEMA&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#36776;&#21035;&#24615;&#12289;&#29983;&#25104;&#24615;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#20998;&#21106;&#12289;&#22270;&#20687;&#29983;&#25104;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09240v1 Announce Type: new Abstract: Exponential Moving Average (EMA) is a widely used weight averaging (WA) regularization to learn flat optima for better generalizations without extra cost in deep neural network (DNN) optimization. Despite achieving better flatness, existing WA methods might fall into worse final performances or require extra test-time computations. This work unveils the full potential of EMA with a single line of modification, i.e., switching the EMA parameters to the original model after each epoch, dubbed as Switch EMA (SEMA). From both theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to reach generalization optima that better trade-off between flatness and sharpness. To verify the effectiveness of SEMA, we conduct comparison experiments with discriminative, generative, and regression tasks on vision and language datasets, including image classification, self-supervised learning, object detection and segmentation, image generati
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.06165</link><description>&lt;p&gt;
&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#34920;&#31034;&#26469;&#36827;&#34892;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Contrastive Feature Representations for Facial Action Unit Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06165
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#65288;AU&#65289;&#26816;&#27979;&#30340;&#20027;&#35201;&#26041;&#27861;&#28041;&#21450;&#30417;&#30563;&#30340;&#22810;&#26631;&#31614;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24120;&#24120;&#23545;AU&#30340;&#20687;&#32032;&#32423;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#25552;&#20986;&#20102;&#24456;&#22823;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#22122;&#22768;AU&#26631;&#31614;&#65292;&#36825;&#31181;&#20570;&#27861;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#22686;&#24378;&#12290;&#30446;&#26631;&#26159;&#22312;AU&#26816;&#27979;&#39046;&#22495;&#20013;&#25670;&#33073;&#20256;&#32479;&#30340;&#20687;&#32032;&#32423;&#23398;&#20064;&#33539;&#24335;&#65292;&#33719;&#24471;&#21028;&#21035;&#29305;&#24449;&#12290;&#20026;&#20102;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#30417;&#30563;&#20449;&#21495;&#12290;&#36825;&#31181;&#22686;&#24378;&#26159;&#36890;&#36807;&#27491;&#26679;&#26412;&#25277;&#26679;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#27491;&#26679;&#26412;&#23545;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#27599;&#20010;AU&#31867;&#22411;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we empl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26174;&#24335;&#28789;&#25935;&#24230;&#22270;&#30340;&#23637;&#24320;&#24515;&#33039;MRI&#37325;&#24314;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#31639;&#27861;&#23637;&#24320;&#65292;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#20043;&#38388;&#30340;&#25509;&#25910;&#32447;&#22280;&#20851;&#31995;&#26469;&#23454;&#29616;&#21152;&#36895;&#24515;&#33039;MRI&#37325;&#24314;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15608</link><description>&lt;p&gt;
NoSENSE&#65306;&#23398;&#20064;&#26080;&#38656;&#26174;&#24335;&#28789;&#25935;&#24230;&#22270;&#30340;&#23637;&#24320;&#24515;&#33039;MRI&#37325;&#24314;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NoSENSE: Learned unrolled cardiac MRI reconstruction without explicit sensitivity maps. (arXiv:2309.15608v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26174;&#24335;&#28789;&#25935;&#24230;&#22270;&#30340;&#23637;&#24320;&#24515;&#33039;MRI&#37325;&#24314;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#31639;&#27861;&#23637;&#24320;&#65292;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#20043;&#38388;&#30340;&#25509;&#25910;&#32447;&#22280;&#20851;&#31995;&#26469;&#23454;&#29616;&#21152;&#36895;&#24515;&#33039;MRI&#37325;&#24314;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22522;&#20110;&#22810;&#20010;&#25509;&#25910;&#32447;&#22280;&#30340;&#21152;&#36895;&#24515;&#33039;MRI&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#31639;&#27861;&#23637;&#24320;&#12290;&#19982;&#35768;&#22810;&#29616;&#26377;&#30340;&#23398;&#20064;MR&#22270;&#20687;&#37325;&#24314;&#25216;&#26415;&#19981;&#21516;&#65292;&#38656;&#35201;&#23558;&#28789;&#25935;&#24230;&#26144;&#23556;&#65288;CSM&#65289;&#20272;&#35745;&#20316;&#20026;&#19968;&#20010;&#29420;&#31435;&#30340;&#32593;&#32476;&#32452;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#26174;&#24335;&#30340;CSM&#20272;&#35745;&#12290;&#30456;&#21453;&#65292;&#23427;&#38544;&#21547;&#22320;&#25429;&#25417;&#24182;&#23398;&#20064;&#21033;&#29992;&#22270;&#20687;&#20043;&#38388;&#30340;&#25509;&#25910;&#32447;&#22280;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#23398;&#20064;&#22270;&#20687;&#22359;&#21644;k&#31354;&#38388;&#22359;&#32452;&#25104;&#65292;&#20849;&#20139;&#28508;&#22312;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#35843;&#33410;&#21644;&#25509;&#25910;&#32447;&#22280;&#25968;&#25454;&#19968;&#33268;&#24615;&#23454;&#29616;&#23545;&#37319;&#38598;&#21442;&#25968;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;MICCAI STACOM CMRxRecon&#25361;&#25112;&#36187;&#30340;&#24433;&#29255;&#36861;&#36394;&#21644;&#26144;&#23556;&#36861;&#36394;&#39564;&#35777;&#25490;&#34892;&#27036;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#21035;&#22312;PSNR&#20540;&#19978;&#36798;&#21040;&#20102;34.89&#21644;35.56&#65292;SSIM&#20540;&#20998;&#21035;&#20026;0.920&#21644;0.942&#65292;&#22312;&#25776;&#20889;&#26412;&#25991;&#26102;&#20301;&#21015;&#19981;&#21516;&#23567;&#32452;&#31532;4&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel learned image reconstruction method for accelerated cardiac MRI with multiple receiver coils based on deep convolutional neural networks (CNNs) and algorithm unrolling. In contrast to many existing learned MR image reconstruction techniques that necessitate coil-sensitivity map (CSM) estimation as a distinct network component, our proposed approach avoids explicit CSM estimation. Instead, it implicitly captures and learns to exploit the inter-coil relationships of the images. Our method consists of a series of novel learned image and k-space blocks with shared latent information and adaptation to the acquisition parameters by feature-wise modulation (FiLM), as well as coil-wise data-consistency (DC) blocks.  Our method achieved PSNR values of 34.89 and 35.56 and SSIM values of 0.920 and 0.942 in the cine track and mapping track validation leaderboard of the MICCAI STACOM CMRxRecon Challenge, respectively, ranking 4th among different teams at the time of writing.  Cod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#20809;&#27969;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#30456;&#32467;&#21512;&#65292;&#20016;&#23500;&#20102;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#27969;&#31243;&#20013;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#22312;&#25163;&#35821;&#35782;&#21035;&#20219;&#21153;&#20013;&#38477;&#20302;&#20102;WER 0.9&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27979;&#35797;&#38598;&#19978;&#22823;&#22810;&#25968;BLEU&#20998;&#25968;&#32422;0.6&#12290;</title><link>http://arxiv.org/abs/2309.01860</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#65306;&#22686;&#24378;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation. (arXiv:2309.01860v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#20809;&#27969;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#30456;&#32467;&#21512;&#65292;&#20016;&#23500;&#20102;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#27969;&#31243;&#20013;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#22312;&#25163;&#35821;&#35782;&#21035;&#20219;&#21153;&#20013;&#38477;&#20302;&#20102;WER 0.9&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27979;&#35797;&#38598;&#19978;&#22823;&#22810;&#25968;BLEU&#20998;&#25968;&#32422;0.6&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#29992;&#20110;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#19982;&#29616;&#26377;&#30340;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#27969;&#31243;&#30456;&#32467;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#20809;&#27969;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#32467;&#21512;&#65292;&#20197;&#20016;&#23500;&#20855;&#26377;&#19982;&#36816;&#21160;&#30456;&#20851;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;&#35813;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30740;&#31350;&#20102;&#36825;&#31181;&#27169;&#24577;&#21253;&#21547;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#30340;&#25554;&#20214;&#38750;&#24120;&#36731;&#37327;&#32423;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20026;&#26032;&#27169;&#24577;&#21253;&#25324;&#19968;&#20010;&#21333;&#29420;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#25105;&#20204;&#22312;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#20013;&#24212;&#29992;&#20102;&#36825;&#20123;&#25913;&#21464;&#65292;&#25913;&#21892;&#20102;&#27599;&#20010;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;RWTH-PHOENIX-2014&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#24615;&#33021;&#65292;&#29992;&#20110;&#25163;&#35821;&#35782;&#21035;&#65292;&#24182;&#22312;RWTH-PHOENIX-2014T&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#32763;&#35793;&#20219;&#21153;&#12290;&#22312;&#35782;&#21035;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;WER&#38477;&#20302;&#20102;0.9&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22823;&#37096;&#20998;BLEU&#20998;&#25968;&#22312;&#27979;&#35797;&#38598;&#19978;&#25552;&#39640;&#20102;&#32422;0.6&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we devise a mechanism for the addition of multi-modal information with an existing pipeline for continuous sign language recognition and translation. In our procedure, we have incorporated optical flow information with RGB images to enrich the features with movement-related information. This work studies the feasibility of such modality inclusion using a cross-modal encoder. The plugin we have used is very lightweight and doesn't need to include a separate feature extractor for the new modality in an end-to-end manner. We have applied the changes in both sign language recognition and translation, improving the result in each case. We have evaluated the performance on the RWTH-PHOENIX-2014 dataset for sign language recognition and the RWTH-PHOENIX-2014T dataset for translation. On the recognition task, our approach reduced the WER by 0.9, and on the translation task, our approach increased most of the BLEU scores by ~0.6 on the test set.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#22270;&#20687;&#20998;&#31867;&#23545;&#20154;&#31867;&#34880;&#32454;&#32990;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#35782;&#21035;&#65292;&#20026;&#35786;&#26029;&#30142;&#30149;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2308.06300</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#34880;&#32454;&#32990;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Blood Cells Using Deep Learning Models. (arXiv:2308.06300v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06300
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#22270;&#20687;&#20998;&#31867;&#23545;&#20154;&#31867;&#34880;&#32454;&#32990;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#35782;&#21035;&#65292;&#20026;&#35786;&#26029;&#30142;&#30149;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#34880;&#28082;&#20027;&#35201;&#21253;&#25324;&#34880;&#27974;&#12289;&#32418;&#32454;&#32990;&#12289;&#30333;&#32454;&#32990;&#21644;&#34880;&#23567;&#26495;&#12290;&#34880;&#32454;&#32990;&#20026;&#36523;&#20307;&#32454;&#32990;&#25552;&#20379;&#27687;&#27668;&#65292;&#28363;&#20859;&#23427;&#20204;&#65292;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24863;&#26579;&#65292;&#22686;&#24378;&#20813;&#30123;&#21147;&#24182;&#20419;&#36827;&#20957;&#34880;&#12290;&#20154;&#30340;&#20581;&#24247;&#29366;&#20917;&#21487;&#20197;&#20174;&#34880;&#32454;&#32990;&#20013;&#21453;&#26144;&#20986;&#26469;&#12290;&#19968;&#20010;&#20154;&#34987;&#35786;&#26029;&#20986;&#26576;&#31181;&#30142;&#30149;&#30340;&#26426;&#20250;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#20854;&#34880;&#32454;&#32990;&#31867;&#22411;&#21644;&#35745;&#25968;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#34880;&#32454;&#32990;&#20998;&#31867;&#38750;&#24120;&#37325;&#35201;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#30142;&#30149;&#65292;&#21253;&#25324;&#30284;&#30151;&#12289;&#39592;&#39635;&#25439;&#20260;&#12289;&#33391;&#24615;&#32959;&#30244;&#21644;&#23427;&#20204;&#30340;&#29983;&#38271;&#12290;&#36825;&#31181;&#20998;&#31867;&#21487;&#20197;&#24110;&#21161;&#34880;&#28082;&#23398;&#23478;&#21306;&#20998;&#19981;&#21516;&#30340;&#34880;&#32454;&#32990;&#29255;&#27573;&#65292;&#20197;&#20415;&#30830;&#23450;&#30142;&#30149;&#30340;&#21407;&#22240;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#23427;&#23558;&#20154;&#31867;&#34880;&#32454;&#32990;&#65288;&#32418;&#32454;&#32990;&#12289;&#30333;&#32454;&#32990;&#21644;&#34880;&#23567;&#26495;&#65289;&#30340;&#22270;&#20687;&#20998;&#31867;&#20026;&#23427;&#20204;&#30340;&#20122;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23558;&#19981;&#21516;&#30340;CNN&#39044;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#34880;&#32454;&#32990;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human blood mainly comprises plasma, red blood cells, white blood cells, and platelets. The blood cells provide the body's cells oxygen to nourish them, shield them from infections, boost immunity, and aid in clotting. Human health is reflected in blood cells. The chances that a human being can be diagnosed with a disease are significantly influenced by their blood cell type and count. Therefore, blood cell classification is crucial because it helps identify diseases, including cancer, damaged bone marrow, benign tumors, and their growth. This classification allows hematologists to distinguish between different blood cell fragments so that the cause of diseases can be identified. Convolution neural networks are a deep learning technique that classifies images of human blood cells (RBCs, WBCs, and platelets) into their subtypes. For this study, transfer learning is used to apply different CNN pre-trained models, including VGG16, VGG19, ResNet-50, ResNet-101, ResNet-152, InceptionV3 Mobi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#32534;&#30721;&#35270;&#39057;&#25968;&#25454;&#24182;&#23558;&#20854;&#23384;&#20648;&#22312;&#21521;&#37327;&#25968;&#25454;&#24211;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#36827;&#34892;&#35821;&#35328;&#32534;&#30721;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.05822</link><description>&lt;p&gt;
&#32534;&#30721;-&#23384;&#20648;-&#26816;&#32034;&#65306;&#36890;&#36807;&#35821;&#35328;&#32534;&#30721;&#30340;&#33258;&#25105;&#20013;&#24515;&#24863;&#30693;&#22686;&#24378;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Encode-Store-Retrieve: Enhancing Memory Augmentation through Language-Encoded Egocentric Perception. (arXiv:2308.05822v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#32534;&#30721;&#35270;&#39057;&#25968;&#25454;&#24182;&#23558;&#20854;&#23384;&#20648;&#22312;&#21521;&#37327;&#25968;&#25454;&#24211;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#36827;&#34892;&#35821;&#35328;&#32534;&#30721;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20381;&#36182;&#20110;&#33258;&#24049;&#30340;&#35760;&#24518;&#26469;&#32534;&#30721;&#12289;&#23384;&#20648;&#21644;&#26816;&#32034;&#25105;&#20204;&#30340;&#32463;&#21382;&#12290;&#28982;&#32780;&#65292;&#35760;&#24518;&#38388;&#38548;&#26377;&#26102;&#20250;&#21457;&#29983;&#12290;&#23454;&#29616;&#35760;&#24518;&#22686;&#24378;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;&#22686;&#24378;&#29616;&#23454;&#22836;&#25140;&#24335;&#26174;&#31034;&#35774;&#22791;&#26469;&#25429;&#25417;&#21644;&#20445;&#30041;&#33258;&#25105;&#20013;&#24515;&#30340;&#35270;&#39057;&#65292;&#36825;&#31181;&#20570;&#27861;&#36890;&#24120;&#34987;&#31216;&#20026;&#29983;&#27963;&#35760;&#24405;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#25216;&#26415;&#32570;&#20047;&#39640;&#25928;&#32534;&#30721;&#21644;&#23384;&#20648;&#22914;&#27492;&#22823;&#37327;&#30340;&#35270;&#39057;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20174;&#24222;&#22823;&#30340;&#35270;&#39057;&#23384;&#26723;&#20013;&#26816;&#32034;&#29305;&#23450;&#20449;&#24687;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#22797;&#26434;&#20102;&#24555;&#36895;&#35775;&#38382;&#25152;&#38656;&#20869;&#23481;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We depend on our own memory to encode, store, and retrieve our experiences. However, memory lapses can occur. One promising avenue for achieving memory augmentation is through the use of augmented reality head-mounted displays to capture and preserve egocentric videos, a practice commonly referred to as life logging. However, a significant challenge arises from the sheer volume of video data generated through life logging, as the current technology lacks the capability to encode and store such large amounts of data efficiently. Further, retrieving specific information from extensive video archives requires substantial computational power, further complicating the task of quickly accessing desired content. To address these challenges, we propose a memory augmentation system that involves leveraging natural language encoding for video data and storing them in a vector database. This approach harnesses the power of large vision language models to perform the language encoding process. Add
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;CST-YOLO&#31639;&#27861;&#65292;&#22522;&#20110;&#25913;&#36827;&#30340;YOLOv7&#21644;CNN-Swin Transformer&#65292;&#24341;&#20837;&#20102;&#20960;&#20010;&#26377;&#29992;&#30340;&#27169;&#22411;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#34880;&#32454;&#32990;&#26816;&#27979;&#31934;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#19977;&#20010;&#34880;&#32454;&#32990;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.14590</link><description>&lt;p&gt;
CST-YOLO: &#19968;&#31181;&#22522;&#20110;&#25913;&#36827;&#30340;YOLOv7&#21644;CNN-Swin Transformer&#30340;&#34880;&#32454;&#32990;&#26816;&#27979;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CST-YOLO: A Novel Method for Blood Cell Detection Based on Improved YOLOv7 and CNN-Swin Transformer. (arXiv:2306.14590v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;CST-YOLO&#31639;&#27861;&#65292;&#22522;&#20110;&#25913;&#36827;&#30340;YOLOv7&#21644;CNN-Swin Transformer&#65292;&#24341;&#20837;&#20102;&#20960;&#20010;&#26377;&#29992;&#30340;&#27169;&#22411;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#34880;&#32454;&#32990;&#26816;&#27979;&#31934;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#19977;&#20010;&#34880;&#32454;&#32990;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34880;&#32454;&#32990;&#26816;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#20856;&#22411;&#30340;&#23567;&#29289;&#20307;&#26816;&#27979;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;CST-YOLO&#27169;&#22411;&#65292;&#22522;&#20110;YOLOv7&#32467;&#26500;&#24182;&#20351;&#29992;CNN-Swin Transformer&#65288;CST&#65289;&#36827;&#34892;&#22686;&#24378;&#65292;&#36825;&#26159;&#19968;&#31181;CNN-Transformer&#34701;&#21512;&#30340;&#26032;&#23581;&#35797;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19977;&#20010;&#26377;&#29992;&#30340;&#27169;&#22411;&#65306;&#21152;&#26435;&#39640;&#25928;&#23618;&#32858;&#21512;&#32593;&#32476;&#65288;W-ELAN&#65289;&#12289;&#22810;&#23610;&#24230;&#36890;&#36947;&#20998;&#21106;&#65288;MCS&#65289;&#21644;&#32423;&#32852;&#21367;&#31215;&#23618;&#65288;CatConv&#65289;&#65292;&#20197;&#25552;&#39640;&#23567;&#29289;&#20307;&#26816;&#27979;&#31934;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;CST-YOLO&#22312;&#19977;&#20010;&#34880;&#32454;&#32990;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;92.7&#12289;95.6&#21644;91.1 mAP@0.5&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#22914;YOLOv5&#21644;YOLOv7&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/mkang315/CST-YOLO&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blood cell detection is a typical small-scale object detection problem in computer vision. In this paper, we propose a CST-YOLO model for blood cell detection based on YOLOv7 architecture and enhance it with the CNN-Swin Transformer (CST), which is a new attempt at CNN-Transformer fusion. We also introduce three other useful modules: Weighted Efficient Layer Aggregation Networks (W-ELAN), Multiscale Channel Split (MCS), and Concatenate Convolutional Layers (CatConv) in our CST-YOLO to improve small-scale object detection precision. Experimental results show that the proposed CST-YOLO achieves 92.7, 95.6, and 91.1 mAP@0.5 respectively on three blood cell datasets, outperforming state-of-the-art object detectors, e.g., YOLOv5 and YOLOv7. Our code is available at https://github.com/mkang315/CST-YOLO.
&lt;/p&gt;</description></item></channel></rss>