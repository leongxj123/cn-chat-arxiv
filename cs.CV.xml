<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>QKFormer&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#33033;&#20914;&#24418;&#24335;Q-K&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#20998;&#23618;&#32467;&#26500;&#21644;&#34917;&#19969;&#23884;&#20837;&#27169;&#22359;&#65292;&#20197;&#25552;&#39640;&#33033;&#20914;&#21464;&#21387;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16552</link><description>&lt;p&gt;
QKFormer: &#20351;&#29992;Q-K&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#33033;&#20914;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
QKFormer: Hierarchical Spiking Transformer using Q-K Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16552
&lt;/p&gt;
&lt;p&gt;
QKFormer&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#33033;&#20914;&#24418;&#24335;Q-K&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#20998;&#23618;&#32467;&#26500;&#21644;&#34917;&#19969;&#23884;&#20837;&#27169;&#22359;&#65292;&#20197;&#25552;&#39640;&#33033;&#20914;&#21464;&#21387;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#21464;&#21387;&#22120;&#23558;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#21464;&#21387;&#22120;&#26550;&#26500;&#30456;&#32467;&#21512;&#65292;&#30001;&#20110;&#20854;&#33410;&#33021;&#39640;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#29616;&#26377;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#39033;&#21019;&#26032;&#65306;i&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;SNNs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#33033;&#20914;&#24418;&#24335;Q-K&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24615;&#30340;&#20108;&#36827;&#21046;&#21521;&#37327;&#26377;&#25928;&#22320;&#24314;&#27169;&#20196;&#29260;&#25110;&#36890;&#36947;&#32500;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;ii&#65289;&#25105;&#20204;&#23558;&#20855;&#26377;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#30340;&#20998;&#23618;&#32467;&#26500;&#24341;&#20837;&#33033;&#20914;&#21464;&#21387;&#22120;&#65292;&#20174;&#32780;&#33719;&#24471;&#22810;&#23610;&#24230;&#33033;&#20914;&#34920;&#31034;&#65292;&#36825;&#23545;&#22823;&#33041;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#37117;&#26377;&#26174;&#30528;&#22909;&#22788;&#12290;iii&#65289;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#24378;&#22823;&#30340;&#34917;&#19969;&#23884;&#20837;&#27169;&#22359;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#33033;&#20914;&#21464;&#21387;&#22120;&#35774;&#35745;&#30340;&#21464;&#24418;&#24555;&#25463;&#26041;&#24335;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;QKFormer&#65292;&#19968;&#31181;&#20998;&#23618;&#33033;&#20914;&#21464;&#21387;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16552v1 Announce Type: cross  Abstract: Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with Transformer architectures, have attracted significant attention due to their potential for energy efficiency and high performance. However, existing models in this domain still suffer from suboptimal performance. We introduce several innovations to improve the performance: i) We propose a novel spike-form Q-K attention mechanism, tailored for SNNs, which efficiently models the importance of token or channel dimensions through binary vectors with linear complexity. ii) We incorporate the hierarchical structure, which significantly benefits the performance of both the brain and artificial neural networks, into spiking transformers to obtain multi-scale spiking representation. iii) We design a versatile and powerful patch embedding module with a deformed shortcut specifically for spiking transformers. Together, we develop QKFormer, a hierarchical spiking transformer
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#24341;&#23548;&#30340;&#28508;&#22312;&#19968;&#33268;&#24615;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;LCD&#36807;&#31243;&#20013;&#25972;&#21512;&#22870;&#21169;&#27169;&#22411;&#30340;&#21453;&#39304;&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#39640;&#20445;&#30495;&#22270;&#20687;&#29983;&#25104;&#26102;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.11027</link><description>&lt;p&gt;
&#22870;&#21169;&#24341;&#23548;&#30340;&#28508;&#22312;&#19968;&#33268;&#24615;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Reward Guided Latent Consistency Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11027
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#24341;&#23548;&#30340;&#28508;&#22312;&#19968;&#33268;&#24615;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;LCD&#36807;&#31243;&#20013;&#25972;&#21512;&#22870;&#21169;&#27169;&#22411;&#30340;&#21453;&#39304;&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#39640;&#20445;&#30495;&#22270;&#20687;&#29983;&#25104;&#26102;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#19968;&#33268;&#24615;&#33976;&#39311;(LCD)&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#33539;&#24335;&#12290;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;&#25945;&#24072;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#20013;&#33976;&#39311;&#20986;&#28508;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;(LCM)&#65292;LCD&#22312;&#20165;&#38656;2&#21040;4&#20010;&#25512;&#29702;&#27493;&#39588;&#20869;&#20419;&#36827;&#20102;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;LCM&#30340;&#39640;&#25928;&#25512;&#29702;&#26159;&#20197;&#26679;&#26412;&#36136;&#37327;&#20026;&#20195;&#20215;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;LCM&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#26469;&#34917;&#20607;&#36136;&#37327;&#25439;&#22833;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#22870;&#21169;&#24341;&#23548;&#30340;LCD(RG-LCD)&#65292;&#36890;&#36807;&#23558;&#22870;&#21169;&#27169;&#22411;(RM)&#30340;&#21453;&#39304;&#25972;&#21512;&#21040;LCD&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;LCD&#25439;&#22833;&#19982;&#26368;&#22823;&#21270;&#19982;LCM&#21333;&#27493;&#29983;&#25104;&#30456;&#20851;&#32852;&#30340;&#22870;&#21169;&#30340;&#30446;&#26631;&#30456;&#32467;&#21512;&#65292;&#26469;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#65292;&#24403;&#20351;&#29992;&#33391;&#22909;RM&#30340;&#21453;&#39304;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;RG-LCM&#30340;2&#27493;&#29983;&#25104;&#34987;&#20154;&#31867;&#38738;&#30544;&#65292;&#36229;&#36807;&#20102;50&#27493;DDIM&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11027v1 Announce Type: cross  Abstract: Latent Consistency Distillation (LCD) has emerged as a promising paradigm for efficient text-to-image synthesis. By distilling a latent consistency model (LCM) from a pre-trained teacher latent diffusion model (LDM), LCD facilitates the generation of high-fidelity images within merely 2 to 4 inference steps. However, the LCM's efficient inference is obtained at the cost of the sample quality. In this paper, we propose compensating the quality loss by aligning LCM's output with human preference during training. Specifically, we introduce Reward Guided LCD (RG-LCD), which integrates feedback from a reward model (RM) into the LCD process by augmenting the original LCD loss with the objective of maximizing the reward associated with LCM's single-step generation. As validated through human evaluation, when trained with the feedback of a good RM, the 2-step generations from our RG-LCM are favored by humans over the 50-step DDIM samples from 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#30340;&#25299;&#25169;&#20445;&#30495;&#22810;&#31867;&#21035;&#20998;&#21106;&#30340;&#36890;&#29992;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#23558;N&#31867;&#21035;&#20998;&#21106;&#38382;&#39064;&#20998;&#35299;&#20026;N&#20010;&#21333;&#31867;&#21035;&#20998;&#21106;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#39564;&#35777;&#20102;&#22312;&#22235;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2403.11001</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20013;&#25299;&#25169;&#20445;&#30495;&#30340;&#22810;&#31867;&#21035;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Topologically faithful multi-class segmentation in medical images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11001
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#30340;&#25299;&#25169;&#20445;&#30495;&#22810;&#31867;&#21035;&#20998;&#21106;&#30340;&#36890;&#29992;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#23558;N&#31867;&#21035;&#20998;&#21106;&#38382;&#39064;&#20998;&#35299;&#20026;N&#20010;&#21333;&#31867;&#21035;&#20998;&#21106;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#39564;&#35777;&#20102;&#22312;&#22235;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#65292;&#25299;&#25169;&#31934;&#24230;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#23646;&#24615;&#65292;&#23545;&#20110;&#19979;&#28216;&#24212;&#29992;&#22914;&#32593;&#32476;&#20998;&#26512;&#21644;&#34880;&#31649;&#25110;&#32454;&#32990;&#35745;&#25968;&#20013;&#30340;&#27969;&#27169;&#25311;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#37325;&#35201;&#30340;&#26041;&#27861;&#35770;&#36827;&#27493;&#23558;&#20195;&#25968;&#25299;&#25169;&#20013;&#25166;&#23454;&#30340;&#27010;&#24565;&#24102;&#21040;&#20102;&#20108;&#20540;&#20998;&#21106;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#31867;&#21035;&#20998;&#21106;&#22330;&#26223;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#24456;&#23569;&#34987;&#25506;&#32034;&#65292;&#25299;&#25169;&#38169;&#35823;&#24456;&#24120;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25299;&#25169;&#20445;&#30495;&#30340;&#22810;&#31867;&#21035;&#20998;&#21106;&#65292;&#25193;&#23637;&#20102;&#26368;&#36817;&#22522;&#20110;&#25345;&#20037;&#26465;&#30721;&#30340;Betti&#21305;&#37197;&#27010;&#24565;&#12290;&#25105;&#20204;&#23558;N&#31867;&#21035;&#20998;&#21106;&#38382;&#39064;&#25237;&#24433;&#21040;N&#20010;&#21333;&#31867;&#21035;&#20998;&#21106;&#20219;&#21153;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#19968;&#21442;&#25968;&#25345;&#20037;&#21516;&#35843;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21464;&#24471;&#21487;&#34892;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#21253;&#21547;&#39640;&#24230;&#19981;&#21516;&#25299;&#25169;&#29305;&#24449;&#30340;&#22235;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11001v1 Announce Type: cross  Abstract: Topological accuracy in medical image segmentation is a highly important property for downstream applications such as network analysis and flow modeling in vessels or cell counting. Recently, significant methodological advancements have brought well-founded concepts from algebraic topology to binary segmentation. However, these approaches have been underexplored in multi-class segmentation scenarios, where topological errors are common. We propose a general loss function for topologically faithful multi-class segmentation extending the recent Betti matching concept, which is based on induced matchings of persistence barcodes. We project the N-class segmentation problem to N single-class segmentation tasks, which allows us to use 1-parameter persistent homology making training of neural networks computationally feasible. We validate our method on a comprehensive set of four medical datasets with highly variant topological characteristic
&lt;/p&gt;</description></item><item><title>&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26102;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#65292;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;TIVE&#65292;&#26681;&#25454;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#20215;&#20540;&#26469;&#28040;&#38500;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#12290;</title><link>https://arxiv.org/abs/2403.09559</link><description>&lt;p&gt;
&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#23545;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Less is More: Data Value Estimation for Visual Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09559
&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26102;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#65292;&#36890;&#36807;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;TIVE&#65292;&#26681;&#25454;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#20215;&#20540;&#26469;&#28040;&#38500;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26159;&#26500;&#24314;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#20851;&#38190;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MLLMs&#20027;&#35201;&#20381;&#36182;&#20110;&#22810;&#20010;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#38598;&#30340;&#28151;&#21512;&#35757;&#32451;&#65288;&#29978;&#33267;&#36229;&#36807;&#19968;&#30334;&#19975;&#26465;&#25351;&#23548;&#65289;&#65292;&#36825;&#21487;&#33021;&#24341;&#20837;&#25968;&#25454;&#20887;&#20313;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#38598;&#20869;&#23384;&#22312;&#26174;&#33879;&#20887;&#20313;&#65292;&#24182;&#26174;&#31034;&#22823;&#22823;&#20943;&#23569;&#20960;&#20010;&#25351;&#23548;&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#29978;&#33267;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;TIVE&#65292;&#20197;&#28040;&#38500;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#12290;TIVE&#39318;&#20808;&#26681;&#25454;&#35745;&#31639;&#30340;&#26799;&#24230;&#20272;&#35745;&#35270;&#35273;&#25351;&#23548;&#30340;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#20215;&#20540;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#20272;&#35745;&#30340;&#20215;&#20540;&#65292;TIVE&#30830;&#23450;&#20102;&#20219;&#21153;&#32423;&#21644;&#23454;&#20363;&#32423;&#25351;&#23548;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09559v1 Announce Type: new  Abstract: Visual instruction tuning is the key to building multimodal large language models (MLLMs), which greatly improves the reasoning capabilities of large language models (LLMs) in vision scenario. However, existing MLLMs mostly rely on a mixture of multiple highly diverse visual instruction datasets for training (even more than a million instructions), which may introduce data redundancy. To investigate this issue, we conduct a series of empirical studies, which reveal a significant redundancy within the visual instruction datasets, and show that greatly reducing the amount of several instruction dataset even do not affect the performance. Based on the findings, we propose a new data selection approach TIVE, to eliminate redundancy within visual instruction data. TIVE first estimates the task-level and instance-level value of the visual instructions based on computed gradients. Then, according to the estimated values, TIVE determines the tas
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;&#21644;&#23618;&#27425;&#20998;&#21106;&#31243;&#24207;&#65292;DSEG-LIME&#25913;&#36827;&#20102;&#22270;&#20687;&#35299;&#37322;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07733</link><description>&lt;p&gt;
DSEG-LIME -- &#36890;&#36807;&#23618;&#27425;&#21270;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;&#25552;&#21319;&#22270;&#20687;&#35299;&#37322;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DSEG-LIME -- Improving Image Explanation by Hierarchical Data-Driven Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07733
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;&#21644;&#23618;&#27425;&#20998;&#21106;&#31243;&#24207;&#65292;DSEG-LIME&#25913;&#36827;&#20102;&#22270;&#20687;&#35299;&#37322;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#25581;&#31034;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;LIME (Local Interpretable Model-agnostic Explanations) &#26159;&#19968;&#20010;&#24191;&#20026;&#20154;&#30693;&#30340;&#29992;&#20110;&#22270;&#20687;&#20998;&#26512;&#30340;XAI&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#22270;&#20687;&#20998;&#21106;&#26469;&#21019;&#24314;&#29305;&#24449;&#20197;&#35782;&#21035;&#30456;&#20851;&#30340;&#20998;&#31867;&#21306;&#22495;&#12290;&#28982;&#32780;&#65292;&#36739;&#24046;&#30340;&#20998;&#21106;&#21487;&#33021;&#20250;&#24433;&#21709;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#24182;&#21066;&#24369;&#21508;&#20010;&#21306;&#22495;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#24433;&#21709;&#25972;&#20307;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DSEG-LIME (Data-Driven Segmentation LIME)&#65292;&#20855;&#26377;: i) &#29992;&#20110;&#29983;&#25104;&#20154;&#31867;&#21487;&#35782;&#21035;&#29305;&#24449;&#30340;&#25968;&#25454;&#39537;&#21160;&#20998;&#21106;, &#21644; ii) &#36890;&#36807;&#32452;&#21512;&#23454;&#29616;&#30340;&#23618;&#27425;&#20998;&#21106;&#31243;&#24207;&#12290;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#20351;&#29992;&#26469;&#33258;ImageNet&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#23545;DSEG-LIME&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;-&#36825;&#20123;&#24773;&#26223;&#19981;&#21253;&#21547;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#20998;&#26512;&#21253;&#25324;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;XAI&#25351;&#26631;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#30340;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07733v1 Announce Type: cross  Abstract: Explainable Artificial Intelligence is critical in unraveling decision-making processes in complex machine learning models. LIME (Local Interpretable Model-agnostic Explanations) is a well-known XAI framework for image analysis. It utilizes image segmentation to create features to identify relevant areas for classification. Consequently, poor segmentation can compromise the consistency of the explanation and undermine the importance of the segments, affecting the overall interpretability. Addressing these challenges, we introduce DSEG-LIME (Data-Driven Segmentation LIME), featuring: i) a data-driven segmentation for human-recognized feature generation, and ii) a hierarchical segmentation procedure through composition. We benchmark DSEG-LIME on pre-trained models with images from the ImageNet dataset - scenarios without domain-specific knowledge. The analysis includes a quantitative evaluation using established XAI metrics, complemented
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#23545;&#20110;&#29289;&#20307;&#19982;&#32972;&#26223;&#20043;&#38388;&#22810;&#26679;&#21270;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#24341;&#20837;&#19981;&#21516;&#23545;&#35937;&#26041;&#38754;&#21464;&#21270;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.04701</link><description>&lt;p&gt;
ObjectCompose: &#35780;&#20272;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#22312;&#29289;&#20307;&#19982;&#32972;&#26223;&#32452;&#21512;&#21464;&#21270;&#19978;&#30340;&#38887;&#24615;
&lt;/p&gt;
&lt;p&gt;
ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04701
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#23545;&#20110;&#29289;&#20307;&#19982;&#32972;&#26223;&#20043;&#38388;&#22810;&#26679;&#21270;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#20197;&#24341;&#20837;&#19981;&#21516;&#23545;&#35937;&#26041;&#38754;&#21464;&#21270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26368;&#36817;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#35757;&#32451;&#24182;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#20102;&#35299;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#31243;&#24230;&#23545;&#20110;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#24403;&#21069;&#22522;&#20110;&#35270;&#35273;&#30340;&#27169;&#22411;&#38024;&#23545;&#19981;&#21516;&#30340;&#29289;&#20307;&#19982;&#32972;&#26223;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;&#38887;&#24615;&#12290;&#22823;&#22810;&#25968;&#40065;&#26834;&#24615;&#35780;&#20272;&#26041;&#27861;&#24341;&#20837;&#20102;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#35825;&#23548;&#29289;&#20307;&#29305;&#24449;&#65288;&#35270;&#28857;&#12289;&#23610;&#24230;&#12289;&#39068;&#33394;&#65289;&#30340;&#21464;&#21270;&#65292;&#25110;&#32773;&#21033;&#29992;&#22270;&#20687;&#36716;&#25442;&#25216;&#26415;&#65288;&#23545;&#25239;&#24615;&#21464;&#21270;&#12289;&#24120;&#35265;&#30772;&#22351;&#65289;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#27169;&#25311;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#32972;&#26223;&#30340;&#21464;&#21270;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#22312;&#25552;&#20379;&#23545;&#35201;&#36827;&#34892;&#30340;&#26356;&#25913;&#30340;&#25511;&#21046;&#26041;&#38754;&#19981;&#36275;&#65292;&#35201;&#20040;&#25197;&#26354;&#20102;&#29289;&#20307;&#30340;&#35821;&#20041;&#65292;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24341;&#20837;&#21508;&#31181;&#23545;&#35937;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04701v1 Announce Type: cross  Abstract: Given the large-scale multi-modal training of recent vision-based models and their generalization capabilities, understanding the extent of their robustness is critical for their real-world deployment. In this work, we evaluate the resilience of current vision-based models against diverse object-to-background context variations. The majority of robustness evaluation methods have introduced synthetic datasets to induce changes to object characteristics (viewpoints, scale, color) or utilized image transformation techniques (adversarial changes, common corruptions) on real images to simulate shifts in distributions. Recent works have explored leveraging large language models and diffusion models to generate changes in the background. However, these methods either lack in offering control over the changes to be made or distort the object semantics, making them unsuitable for the task. Our method, on the other hand, can induce diverse objec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#22330;&#26223;&#25506;&#32034;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#29983;&#25104;&#20102;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#65292;&#25429;&#25417;&#20102;&#29615;&#22659;&#30340;&#32467;&#26500;</title><link>https://arxiv.org/abs/2402.15487</link><description>&lt;p&gt;
RoboEXP: &#36890;&#36807;&#20132;&#20114;&#24335;&#25506;&#32034;&#23454;&#29616;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#22330;&#26223;&#25506;&#32034;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#29983;&#25104;&#20102;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#65292;&#25429;&#25417;&#20102;&#29615;&#22659;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#38656;&#35201;&#25506;&#32034;&#21608;&#22260;&#29615;&#22659;&#20197;&#36866;&#24212;&#24182;&#24212;&#23545;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20132;&#20114;&#24335;&#22330;&#26223;&#25506;&#32034;&#30340;&#26032;&#20219;&#21153;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#24182;&#29983;&#25104;&#19968;&#20010;&#25429;&#25417;&#22522;&#30784;&#29615;&#22659;&#32467;&#26500;&#30340;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#65288;ACSG&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15487v1 Announce Type: cross  Abstract: Robots need to explore their surroundings to adapt to and tackle tasks in unknown environments. Prior work has proposed building scene graphs of the environment but typically assumes that the environment is static, omitting regions that require active interactions. This severely limits their ability to handle more complex tasks in household and office environments: before setting up a table, robots must explore drawers and cabinets to locate all utensils and condiments. In this work, we introduce the novel task of interactive scene exploration, wherein robots autonomously explore environments and produce an action-conditioned scene graph (ACSG) that captures the structure of the underlying environment. The ACSG accounts for both low-level information, such as geometry and semantics, and high-level information, such as the action-conditioned relationships between different entities in the scene. To this end, we present the Robotic Explo
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#31163;&#25955;&#25193;&#25955;&#32467;&#21512;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26426;&#22120;&#20154;&#35270;&#39057;&#24494;&#35843;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;&#35270;&#39057;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;</title><link>https://arxiv.org/abs/2402.14407</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#36827;&#34892;&#22823;&#35268;&#27169;&#26080;&#21160;&#20316;&#35270;&#39057;&#39044;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14407
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31163;&#25955;&#25193;&#25955;&#32467;&#21512;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26426;&#22120;&#20154;&#35270;&#39057;&#24494;&#35843;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;&#35270;&#39057;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#23436;&#25104;&#22810;&#20010;&#20219;&#21153;&#30340;&#36890;&#29992;&#23454;&#20307;&#20195;&#29702;&#38754;&#20020;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#33258;&#32570;&#20047;&#26377;&#26631;&#35760;&#21160;&#20316;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23384;&#22312;&#22823;&#37327;&#25429;&#25417;&#22797;&#26434;&#20219;&#21153;&#21644;&#19982;&#29289;&#29702;&#19990;&#30028;&#20114;&#21160;&#30340;&#20154;&#31867;&#35270;&#39057;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#26694;&#26550;&#65292;&#21033;&#29992;&#32479;&#19968;&#30340;&#31163;&#25955;&#25193;&#25955;&#23558;&#20154;&#31867;&#35270;&#39057;&#19978;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#23569;&#37327;&#26377;&#26631;&#35760;&#26426;&#22120;&#20154;&#35270;&#39057;&#19978;&#30340;&#31574;&#30053;&#24494;&#35843;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#35270;&#39057;&#21387;&#32553;&#25104;&#32479;&#19968;&#30340;&#35270;&#39057;&#26631;&#35760;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#33945;&#29256;&#26367;&#25442;&#25193;&#25955;&#31574;&#30053;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#26469;&#39044;&#27979;&#28508;&#31354;&#38388;&#20013;&#30340;&#26410;&#26469;&#35270;&#39057;&#26631;&#35760;&#12290;&#22312;&#24494;&#35843;&#38454;&#27573;&#65292;&#25105;&#20204; h
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14407v1 Announce Type: new  Abstract: Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. In this paper, we introduce a novel framework that leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we h
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23553;&#24314;&#23398;&#20064;&#30340;&#35270;&#35273;&#23548;&#33322;&#65292;&#36890;&#36807;&#39640;&#32423;&#31649;&#29702;&#32773;&#12289;&#20013;&#32423;&#31649;&#29702;&#32773;&#21644;&#24037;&#20316;&#20195;&#29702;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#22312;&#19981;&#21516;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#25805;&#20316;&#65292;&#20855;&#26377;&#29420;&#29305;&#27169;&#22359;&#26469;&#23454;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#35760;&#24518;&#20195;&#29702;&#22320;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.12498</link><description>&lt;p&gt;
&#23553;&#24314;&#32593;&#32476;&#29992;&#20110;&#35270;&#35273;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Feudal Networks for Visual Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12498
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23553;&#24314;&#23398;&#20064;&#30340;&#35270;&#35273;&#23548;&#33322;&#65292;&#36890;&#36807;&#39640;&#32423;&#31649;&#29702;&#32773;&#12289;&#20013;&#32423;&#31649;&#29702;&#32773;&#21644;&#24037;&#20316;&#20195;&#29702;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#22312;&#19981;&#21516;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#25805;&#20316;&#65292;&#20855;&#26377;&#29420;&#29305;&#27169;&#22359;&#26469;&#23454;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#35760;&#24518;&#20195;&#29702;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#23548;&#33322;&#36981;&#24490;&#20154;&#31867;&#21487;&#20197;&#22312;&#27809;&#26377;&#35814;&#32454;&#22320;&#22270;&#30340;&#24773;&#20917;&#19979;&#23548;&#33322;&#30340;&#30452;&#35273;&#12290;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#22312;&#24314;&#31435;&#21253;&#21547;&#21487;&#29992;&#20110;&#35268;&#21010;&#30340;&#22270;&#20687;&#33410;&#28857;&#30340;&#25299;&#25169;&#22270;&#30340;&#21516;&#26102;&#36827;&#34892;&#20132;&#20114;&#24335;&#25506;&#32034;&#12290;&#26368;&#36817;&#30340;&#21464;&#20307;&#20174;&#34987;&#21160;&#35270;&#39057;&#20013;&#23398;&#20064;&#65292;&#24182;&#21487;&#20197;&#21033;&#29992;&#22797;&#26434;&#30340;&#31038;&#20132;&#21644;&#35821;&#20041;&#32447;&#32034;&#36827;&#34892;&#23548;&#33322;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#35270;&#39057;&#65292;&#21033;&#29992;&#22823;&#22411;&#22270;&#24182;&#19988;&#30001;&#20110;&#20351;&#29992;&#20102;&#37324;&#31243;&#35745;&#65292;&#22330;&#26223;&#19981;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#23553;&#24314;&#23398;&#20064;&#30340;&#35270;&#35273;&#23548;&#33322;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#30001;&#24037;&#20316;&#20195;&#29702;&#12289;&#20013;&#32423;&#31649;&#29702;&#32773;&#21644;&#39640;&#32423;&#31649;&#29702;&#32773;&#32452;&#25104;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#23553;&#24314;&#23398;&#20064;&#33539;&#24335;&#30340;&#20851;&#38190;&#22312;&#20110;&#65292;&#27599;&#20010;&#32423;&#21035;&#30340;&#20195;&#29702;&#30475;&#21040;&#20219;&#21153;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#36816;&#20316;&#12290;&#22312;&#27492;&#26694;&#26550;&#20013;&#24320;&#21457;&#20102;&#20004;&#20010;&#29420;&#29305;&#30340;&#27169;&#22359;&#12290;&#23545;&#20110;&#39640;&#32423;&#31649;&#29702;&#32773;&#65292;&#25105;&#20204;&#33258;&#30417;&#30563;&#22320;&#23398;&#20064;&#19968;&#20010;&#35760;&#24518;&#20195;&#29702;&#22320;&#22270;&#20197;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12498v1 Announce Type: cross  Abstract: Visual navigation follows the intuition that humans can navigate without detailed maps. A common approach is interactive exploration while building a topological graph with images at nodes that can be used for planning. Recent variations learn from passive videos and can navigate using complex social and semantic cues. However, a significant number of training videos are needed, large graphs are utilized, and scenes are not unseen since odometry is utilized. We introduce a new approach to visual navigation using feudal learning, which employs a hierarchical structure consisting of a worker agent, a mid-level manager, and a high-level manager. Key to the feudal learning paradigm, agents at each level see a different aspect of the task and operate at different spatial and temporal scales. Two unique modules are developed in this framework. For the high- level manager, we learn a memory proxy map in a self supervised manner to record prio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;LM-HT&#27169;&#22411;&#65292;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#22810;&#23618;&#27425;&#38408;&#20540;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#24615;&#33021;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.00411</link><description>&lt;p&gt;
LM-HT SNN: &#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#22810;&#23618;&#27425;&#38408;&#20540;&#27169;&#22411;&#22686;&#24378;SNN&#19982;ANN&#30340;&#24615;&#33021;&#23545;&#24212;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
LM-HT SNN: Enhancing the Performance of SNN to ANN Counterpart through Learnable Multi-hierarchical Threshold Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;LM-HT&#27169;&#22411;&#65292;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#22810;&#23618;&#27425;&#38408;&#20540;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#24615;&#33021;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30456;&#27604;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22240;&#20854;&#26356;&#20855;&#29983;&#29289;&#21551;&#21457;&#21644;&#33021;&#37327;&#25928;&#29575;&#30340;&#20449;&#24687;&#20256;&#36882;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#23398;&#26415;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20043;&#21069;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#23545;SNN&#30340;&#23398;&#20064;&#26799;&#24230;&#21644;&#27169;&#22411;&#32467;&#26500;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#20294;&#22312;&#24615;&#33021;&#26041;&#38754;SNN&#20173;&#28982;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#33853;&#21518;&#20110;ANN&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#22810;&#38408;&#20540;&#27169;&#22411;&#20026;&#36827;&#19968;&#27493;&#22686;&#24378;SNN&#30340;&#23398;&#20064;&#33021;&#21147;&#25552;&#20379;&#20102;&#26356;&#22810;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#25968;&#23398;&#30340;&#35282;&#24230;&#20005;&#26684;&#20998;&#26512;&#20102;&#22810;&#38408;&#20540;&#27169;&#22411;&#12289;&#21407;&#22987;&#33033;&#20914;&#27169;&#22411;&#21644;&#37327;&#21270;ANN&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LM-HT&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#31561;&#36317;&#22810;&#23618;&#27425;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26102;&#38388;&#32500;&#24230;&#19978;&#21160;&#24577;&#35843;&#33410;&#20840;&#23616;&#36755;&#20837;&#30005;&#27969;&#21644;&#33180;&#30005;&#20301;&#27844;&#28431;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;&#22522;&#20110;LM-HT&#27169;&#22411;&#30340;&#30452;&#25509;&#35757;&#32451;&#31639;&#27861;&#21487;&#20197;&#26080;&#32541;&#22320;&#36830;&#25509;&#20004;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to traditional Artificial Neural Network (ANN), Spiking Neural Network (SNN) has garnered widespread academic interest for its intrinsic ability to transmit information in a more biological-inspired and energy-efficient manner. However, despite previous efforts to optimize the learning gradients and model structure of SNNs through various methods, SNNs still lag behind ANNs in terms of performance to some extent. The recently proposed multi-threshold model provides more possibilities for further enhancing the learning capability of SNNs. In this paper, we rigorously analyze the relationship among the multi-threshold model, vanilla spiking model and quantized ANNs from a mathematical perspective, then propose a novel LM-HT model, which is an equidistant multi-hierarchical model that can dynamically regulate the global input current and membrane potential leakage on the time dimension. In addition, we note that the direct training algorithm based on the LM-HT model can seamlessl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLDM&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#20687; Latent Diffusion Model&#65288;LDM&#65289;&#21644;&#35270;&#39057; LDM&#65292;&#22312;&#35270;&#39057;&#32534;&#36753;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#25991;&#26412;&#24341;&#23548;&#30340;&#35270;&#39057;&#32534;&#36753;&#12290;&#36825;&#19968;&#26041;&#27861;&#26082;&#20445;&#25345;&#20102;&#35270;&#39057;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#21448;&#21033;&#29992;&#20102;&#22270;&#20687; LDM &#30340;&#39640;&#20445;&#30495;&#24230;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#24615;&#19982;&#21487;&#26367;&#25442;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16400</link><description>&lt;p&gt;
&#34701;&#21512;&#28508;&#21464;&#25193;&#25955;&#27169;&#22411;&#30340;&#35270;&#39057;&#32534;&#36753;&#65306;&#22810;&#28304;&#28508;&#21464;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fuse Your Latents: Video Editing with Multi-source Latent Diffusion Models. (arXiv:2310.16400v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLDM&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#22270;&#20687; Latent Diffusion Model&#65288;LDM&#65289;&#21644;&#35270;&#39057; LDM&#65292;&#22312;&#35270;&#39057;&#32534;&#36753;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#25991;&#26412;&#24341;&#23548;&#30340;&#35270;&#39057;&#32534;&#36753;&#12290;&#36825;&#19968;&#26041;&#27861;&#26082;&#20445;&#25345;&#20102;&#35270;&#39057;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#21448;&#21033;&#29992;&#20102;&#22270;&#20687; LDM &#30340;&#39640;&#20445;&#30495;&#24230;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#24615;&#19982;&#21487;&#26367;&#25442;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#21464;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#20197;&#20854;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#21512;&#25104;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#35270;&#39057;&#32534;&#36753;&#26041;&#27861;&#23384;&#22312;&#30528;&#39044;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#25110;&#35270;&#39057;&#36880;&#24103;&#37325;&#26032;&#35757;&#32451;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FLDM&#65288;&#34701;&#21512;&#28508;&#21464;&#25193;&#25955;&#27169;&#22411;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#35270;&#39057;LDM&#20013;&#24212;&#29992;&#29616;&#25104;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#26469;&#23454;&#29616;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#32534;&#36753;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FLDM&#22312;&#21435;&#22122;&#36807;&#31243;&#20013;&#34701;&#21512;&#20102;&#22270;&#20687;LDM&#21644;&#35270;&#39057;LDM&#30340;&#28508;&#21464;&#12290;&#36825;&#26679;&#65292;&#21487;&#20197;&#20445;&#25345;&#35270;&#39057;LDM&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#21033;&#29992;&#22270;&#20687;LDM&#30340;&#39640;&#20445;&#30495;&#24230;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#22270;&#20687;LDM&#21644;&#35270;&#39057;LDM&#37117;&#21487;&#20197;&#26367;&#25442;&#65292;&#25152;&#20197;FLDM&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#39640;&#32423;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#22914;InstructPix2Pix&#21644;ControlNet&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;FLDM&#26159;&#31532;&#19968;&#31181;&#23558;&#29616;&#25104;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#24212;&#29992;&#20110;&#35270;&#39057;LDM&#36827;&#34892;&#35270;&#39057;&#32534;&#36753;&#30340;&#26041;&#27861;&#12290;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent Diffusion Models (LDMs) are renowned for their powerful capabilities in image and video synthesis. Yet, video editing methods suffer from insufficient pre-training data or video-by-video re-training cost. In addressing this gap, we propose FLDM (Fused Latent Diffusion Model), a training-free framework to achieve text-guided video editing by applying off-the-shelf image editing methods in video LDMs. Specifically, FLDM fuses latents from an image LDM and an video LDM during the denoising process. In this way, temporal consistency can be kept with video LDM while high-fidelity from the image LDM can also be exploited. Meanwhile, FLDM possesses high flexibility since both image LDM and video LDM can be replaced so advanced image editing methods such as InstructPix2Pix and ControlNet can be exploited. To the best of our knowledge, FLDM is the first method to adapt off-the-shelf image editing methods into video LDMs for video editing. Extensive quantitative and qualitative experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Vision Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;&#12290;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#36827;&#34892;&#32500;&#25252;&#65292;&#36873;&#21462;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#20013;&#30340;&#26368;&#32456;&#32447;&#24615;&#25237;&#24433;&#23618;&#36827;&#34892;ArtiHippo&#30340;&#23454;&#29616;&#21644;&#25104;&#38271;&#12290;</title><link>http://arxiv.org/abs/2303.08250</link><description>&lt;p&gt;
&#22312;&#35270;&#35273;Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#65292;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Grow Artificial Hippocampi in Vision Transformers for Resilient Lifelong Learning. (arXiv:2303.08250v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Vision Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;&#12290;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#36827;&#34892;&#32500;&#25252;&#65292;&#36873;&#21462;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#20013;&#30340;&#26368;&#32456;&#32447;&#24615;&#25237;&#24433;&#23618;&#36827;&#34892;ArtiHippo&#30340;&#23454;&#29616;&#21644;&#25104;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#38656;&#35201;&#25317;&#26377;&#20154;&#31867;&#26234;&#33021;&#30340;&#38887;&#24615;&#65292;&#21363;&#19981;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#31181;&#38887;&#24615;&#19982;&#22823;&#33041;&#20013;&#22797;&#26434;&#30340;&#35760;&#24518;&#26426;&#21046;&#65292;&#23588;&#20854;&#26159;&#28023;&#39532;&#32500;&#25252;&#30340;&#38271;&#26399;&#35760;&#24518;&#65288;LM&#65289;&#32039;&#23494;&#30456;&#20851;&#12290;Transformer&#24050;&#32463;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#8220;&#22823;&#33041;&#8221;&#30340;&#23545;&#24212;&#20307;&#65292;&#20294;LM&#32452;&#20214;&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Vision Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#65288;ArtiHippo&#65289;&#20197;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20840;&#38754;&#28040;&#34701;&#23454;&#39564;&#65292;&#36873;&#23450;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#65288;MHSA&#65289;&#22359;&#20013;&#30340;&#26368;&#32456;&#32447;&#24615;&#25237;&#24433;&#23618;&#26469;&#23454;&#29616;&#21644;&#25104;&#38271;ArtiHippo&#12290;ArtiHippo&#30001;&#19987;&#23478;&#28151;&#21512;&#65288;MoEs&#65289;&#34920;&#31034;&#12290;&#27599;&#20010;&#19987;&#23478;&#32452;&#20214;&#26159;&#32447;&#24615;&#25237;&#24433;&#23618;&#30340;&#29616;&#22330;&#21464;&#20307;&#65292;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#36827;&#34892;&#32500;&#25252;&#65292;&#25628;&#32034;&#31354;&#38388;&#30001;&#22235;&#20010;&#22522;&#26412;&#25104;&#38271;&#25805;&#20316;&#65288;&#36339;&#36807;&#12289;&#37325;&#29992;&#12289;&#36866;&#24212;&#21644;&#26032;&#65289;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning without catastrophic forgetting (i.e., resiliency) possessed by human intelligence is entangled with sophisticated memory mechanisms in the brain, especially the long-term memory (LM) maintained by Hippocampi. To a certain extent, Transformers have emerged as the counterpart ``Brain" of Artificial Intelligence (AI), and yet leave the LM component under-explored for lifelong learning settings. This paper presents a method of learning to grow Artificial Hippocampi (ArtiHippo) in Vision Transformers (ViTs) for resilient lifelong learning. With a comprehensive ablation study, the final linear projection layer in the multi-head self-attention (MHSA) block is selected in realizing and growing ArtiHippo. ArtiHippo is represented by a mixture of experts (MoEs). Each expert component is an on-site variant of the linear projection layer, maintained via neural architecture search (NAS) with the search space defined by four basic growing operations -- skip, reuse, adapt, and new 
&lt;/p&gt;</description></item></channel></rss>