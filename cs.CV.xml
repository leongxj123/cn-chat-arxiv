<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36890;&#36807;&#24341;&#20837;CHAIN&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;GANs&#20013;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00521</link><description>&lt;p&gt;
CHAIN&#65306;&#36890;&#36807;&#21463;&#38480;&#21807;&#19968;&#24615;&#36830;&#32493;&#24615;&#35268;&#33539;&#21270;&#22686;&#24378;&#25968;&#25454;&#39640;&#25928;GANs&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00521
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;CHAIN&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;GANs&#20013;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#26174;&#30528;&#25512;&#21160;&#20102;&#22270;&#20687;&#29983;&#25104;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;GANs&#32463;&#24120;&#38754;&#20020;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#35782;&#21035;Batch Normalization&#65288;BN&#65289;&#20013;&#30340;&#20851;&#38190;&#32570;&#38519;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65306;&#22312;&#20013;&#24515;&#21270;&#21644;&#32553;&#25918;&#27493;&#39588;&#20013;&#26799;&#24230;&#29190;&#28856;&#30340;&#20542;&#21521;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CHAIN&#65288;&#21463;&#38480;&#21807;&#19968;&#24615;&#36830;&#32493;&#24615;&#35268;&#33539;&#21270;&#65289;&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#20013;&#24515;&#21270;&#27493;&#39588;&#26367;&#25442;&#20026;&#38646;&#22343;&#20540;&#27491;&#21017;&#21270;&#65292;&#24182;&#22312;&#32553;&#25918;&#27493;&#39588;&#20013;&#38598;&#25104;&#20102;Lipschitz&#36830;&#32493;&#24615;&#32422;&#26463;&#12290;CHAIN&#36890;&#36807;&#33258;&#36866;&#24212;&#25554;&#20540;&#24402;&#19968;&#21270;&#21644;&#38750;&#24402;&#19968;&#21270;&#29305;&#24449;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;GANs&#30340;&#35757;&#32451;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00521v1 Announce Type: new  Abstract: Generative Adversarial Networks (GANs) significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, GANs often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient GANs. Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lipschitz continuity constraint in the scaling step. CHAIN further enhances GAN training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#65292;&#20351;&#33151;&#24335;&#26426;&#22120;&#20154;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#25805;&#20316;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35757;&#32451;&#21644;Sim2Real&#36716;&#31227;&#23454;&#29616;&#20102;&#22312;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.16967</link><description>&lt;p&gt;
&#29992;&#20110;&#33151;&#24335;&#23450;&#28857;&#26426;&#22120;&#20154;&#36816;&#21160;&#25805;&#20316;&#30340;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Visual Whole-Body Control for Legged Loco-Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16967
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#65292;&#20351;&#33151;&#24335;&#26426;&#22120;&#20154;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#25805;&#20316;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35757;&#32451;&#21644;Sim2Real&#36716;&#31227;&#23454;&#29616;&#20102;&#22312;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#37197;&#22791;&#25163;&#33218;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#36827;&#34892;&#31227;&#21160;&#25805;&#20316;&#30340;&#38382;&#39064;&#65292;&#21363;&#33151;&#24335;&#23450;&#28857;&#25805;&#20316;&#12290;&#23613;&#31649;&#26426;&#22120;&#20154;&#30340;&#33151;&#36890;&#24120;&#29992;&#20110;&#31227;&#21160;&#65292;&#20294;&#36890;&#36807;&#36827;&#34892;&#20840;&#36523;&#25511;&#21046;&#65292;&#21487;&#20197;&#25193;&#22823;&#20854;&#25805;&#20316;&#33021;&#21147;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#20854;&#24037;&#20316;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#20351;&#29992;&#35270;&#35273;&#35266;&#27979;&#33258;&#20027;&#36827;&#34892;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;\ourFull~(\our)&#65292;&#30001;&#19968;&#20010;&#20302;&#32423;&#31574;&#30053;&#21644;&#19968;&#20010;&#39640;&#32423;&#31574;&#30053;&#32452;&#25104;&#12290;&#20302;&#32423;&#31574;&#30053;&#20351;&#29992;&#25152;&#26377;&#33258;&#30001;&#24230;&#26469;&#36319;&#36394;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;&#20301;&#32622;&#65292;&#39640;&#32423;&#31574;&#30053;&#26681;&#25454;&#35270;&#35273;&#36755;&#20837;&#25552;&#20986;&#26411;&#31471;&#25191;&#34892;&#22120;&#20301;&#32622;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#20013;&#35757;&#32451;&#20102;&#20004;&#20010;&#32423;&#21035;&#30340;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#20102;&#20174;Sim&#21040;&#23454;&#29289;&#30340;&#36716;&#31227;&#20197;&#36827;&#34892;&#23454;&#38469;&#26426;&#22120;&#20154;&#37096;&#32626;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#19981;&#21516;&#37197;&#32622;&#19979;&#65288;&#39640;&#24230;&#12289;&#65289;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#65292;&#30456;&#23545;&#22522;&#32447;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16967v1 Announce Type: cross  Abstract: We study the problem of mobile manipulation using legged robots equipped with an arm, namely legged loco-manipulation. The robot legs, while usually utilized for mobility, offer an opportunity to amplify the manipulation capabilities by conducting whole-body control. That is, the robot can control the legs and the arm at the same time to extend its workspace. We propose a framework that can conduct the whole-body control autonomously with visual observations. Our approach, namely \ourFull~(\our), is composed of a low-level policy using all degrees of freedom to track the end-effector manipulator position and a high-level policy proposing the end-effector position based on visual inputs. We train both levels of policies in simulation and perform Sim2Real transfer for real robot deployment. We perform extensive experiments and show significant improvements over baselines in picking up diverse objects in different configurations (heights,
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38750;&#33539;&#20363;&#21270;&#30340;&#24191;&#20041;&#20998;&#26512;&#31867;&#22686;&#37327;&#23398;&#20064;&#65292;&#36890;&#36807;&#37319;&#29992;&#20998;&#26512;&#23398;&#20064;&#24182;&#25552;&#20379;&#20102;&#23545;GCIL&#24773;&#26223;&#30340;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#27169;&#22411;&#24555;&#36895;&#36951;&#24536;&#21644;&#25968;&#25454;&#38544;&#31169;&#20405;&#29359;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15706</link><description>&lt;p&gt;
G-ACIL&#65306;&#38754;&#21521;&#38750;&#33539;&#20363;&#21270;&#30340;&#24191;&#20041;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#20998;&#26512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
G-ACIL: Analytic Learning for Exemplar-Free Generalized Class Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15706
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38750;&#33539;&#20363;&#21270;&#30340;&#24191;&#20041;&#20998;&#26512;&#31867;&#22686;&#37327;&#23398;&#20064;&#65292;&#36890;&#36807;&#37319;&#29992;&#20998;&#26512;&#23398;&#20064;&#24182;&#25552;&#20379;&#20102;&#23545;GCIL&#24773;&#26223;&#30340;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#27169;&#22411;&#24555;&#36895;&#36951;&#24536;&#21644;&#25968;&#25454;&#38544;&#31169;&#20405;&#29359;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;(CIL)&#22312;&#39034;&#24207;&#20219;&#21153;&#19978;&#35757;&#32451;&#32593;&#32476;&#65292;&#27599;&#20010;&#20219;&#21153;&#26377;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20294;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24403;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#24555;&#36895;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#24191;&#20041;CIL(GCIL)&#26088;&#22312;&#35299;&#20915;&#26356;&#25509;&#36817;&#29616;&#23454;&#24773;&#26223;&#19979;&#30340;CIL&#38382;&#39064;&#65292;&#21363;&#26032;&#25968;&#25454;&#20855;&#26377;&#28151;&#21512;&#25968;&#25454;&#31867;&#21035;&#21644;&#26410;&#30693;&#26679;&#26412;&#20998;&#24067;&#22823;&#23567;&#65292;&#23548;&#33268;&#36951;&#24536;&#21152;&#21095;&#12290;&#29616;&#26377;&#30340;&#38024;&#23545;GCIL&#30340;&#23581;&#35797;&#35201;&#20040;&#24615;&#33021;&#19981;&#20339;&#65292;&#35201;&#20040;&#36890;&#36807;&#20445;&#23384;&#21382;&#21490;&#33539;&#20363;&#20405;&#29359;&#25968;&#25454;&#38544;&#31169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38750;&#33539;&#20363;&#21270;&#30340;&#24191;&#20041;&#20998;&#26512;&#31867;&#22686;&#37327;&#23398;&#20064;(G-ACIL)&#12290;G-ACIL&#37319;&#29992;&#20998;&#26512;&#23398;&#20064;(&#19968;&#31181;&#26080;&#26799;&#24230;&#35757;&#32451;&#25216;&#26415;)&#65292;&#24182;&#20026;GCIL&#24773;&#26223;&#25552;&#20379;&#20998;&#26512;&#35299;(&#21363;&#38381;&#21512;&#24418;&#24335;)&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#23558;&#20256;&#20837;&#25968;&#25454;&#20998;&#35299;&#20026;&#26292;&#38706;&#31867;&#21644;&#26410;&#26292;&#38706;&#31867;&#65292;&#23454;&#29616;&#20102;&#22686;&#38271;&#31867;&#20043;&#38388;&#30340;&#31561;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15706v1 Announce Type: new  Abstract: Class incremental learning (CIL) trains a network on sequential tasks with separated categories but suffers from catastrophic forgetting, where models quickly lose previously learned knowledge when acquiring new tasks. The generalized CIL (GCIL) aims to address the CIL problem in a more real-world scenario, where incoming data have mixed data categories and unknown sample size distribution, leading to intensified forgetting. Existing attempts for the GCIL either have poor performance, or invade data privacy by saving historical exemplars. To address this, in this paper, we propose an exemplar-free generalized analytic class incremental learning (G-ACIL). The G-ACIL adopts analytic learning (a gradient-free training technique), and delivers an analytical solution (i.e., closed-form) to the GCIL scenario. This solution is derived via decomposing the incoming data into exposed and unexposed classes, allowing an equivalence between the incre
&lt;/p&gt;</description></item><item><title>AnyV2V&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#31616;&#21270;&#35270;&#39057;&#32534;&#36753;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#20256;&#32479;&#21644;&#26032;&#39062;&#30340;&#32534;&#36753;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.14468</link><description>&lt;p&gt;
AnyV2V&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14468
&lt;/p&gt;
&lt;p&gt;
AnyV2V&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#31616;&#21270;&#35270;&#39057;&#32534;&#36753;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#20256;&#32479;&#21644;&#26032;&#39062;&#30340;&#32534;&#36753;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14468v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#36234; &#25688;&#35201;: &#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#28041;&#21450;&#32534;&#36753;&#28304;&#35270;&#39057;&#20197;&#21450;&#39069;&#22806;&#30340;&#25511;&#21046;&#65288;&#20363;&#22914;&#25991;&#26412;&#25552;&#31034;&#12289;&#20027;&#39064;&#25110;&#39118;&#26684;&#65289;&#65292;&#20197;&#29983;&#25104;&#19982;&#28304;&#35270;&#39057;&#21644;&#25552;&#20379;&#30340;&#25511;&#21046;&#30456;&#21305;&#37197;&#30340;&#26032;&#35270;&#39057;&#12290;&#20256;&#32479;&#26041;&#27861;&#21463;&#38480;&#20110;&#29305;&#23450;&#30340;&#32534;&#36753;&#31867;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#28385;&#36275;&#24191;&#27867;&#29992;&#25143;&#38656;&#27714;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AnyV2V&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20813;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#35270;&#39057;&#32534;&#36753;&#31616;&#21270;&#20026;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#65288;1&#65289;&#21033;&#29992;&#29616;&#25104;&#30340;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#65288;&#20363;&#22914;InstructPix2Pix&#12289;InstantID&#31561;&#65289;&#20462;&#25913;&#31532;&#19968;&#24103;&#65292;&#65288;2&#65289;&#21033;&#29992;&#29616;&#26377;&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;I2VGen-XL&#65289;&#36827;&#34892;DDIM&#36870;&#36716;&#21644;&#29305;&#24449;&#27880;&#20837;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;AnyV2V&#21487;&#20197;&#25554;&#20837;&#20219;&#20309;&#29616;&#26377;&#30340;&#22270;&#20687;&#32534;&#36753;&#24037;&#20855;&#65292;&#20197;&#25903;&#25345;&#24191;&#27867;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#12290;&#38500;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#32534;&#36753;&#26041;&#27861;&#65292;AnyV2V&#36824;&#21487;&#20197;&#25903;&#25345;&#26032;&#39062;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#65292;&#21253;&#25324;&#21442;&#32771;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14468v1 Announce Type: cross  Abstract: Video-to-video editing involves editing a source video along with additional control (such as text prompts, subjects, or styles) to generate a new video that aligns with the source video and the provided control. Traditional methods have been constrained to certain editing types, limiting their ability to meet the wide range of user demands. In this paper, we introduce AnyV2V, a novel training-free framework designed to simplify video editing into two primary steps: (1) employing an off-the-shelf image editing model (e.g. InstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an existing image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion and feature injection. In the first stage, AnyV2V can plug in any existing image editing tools to support an extensive array of video editing tasks. Beyond the traditional prompt-based editing methods, AnyV2V also can support novel video editing tasks, including refe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#35821;&#35328;-Only VLM&#36873;&#25321;&#20013;&#30340;&#20004;&#20010;&#22266;&#26377;&#25361;&#25112;&#65306;&#12300;&#27169;&#24577;&#24046;&#36317;&#12301;&#21644;&#12300;&#33021;&#21147;&#24046;&#36317;&#12301;&#65292;&#24182;&#25552;&#20986;&#20102;VLM&#36873;&#25321;&#20013;&#24357;&#21512;&#36825;&#20004;&#20010;&#24046;&#36317;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.13797</link><description>&lt;p&gt;
&#24357;&#21512;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#21644;&#33021;&#21147;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridge the Modality and Capacity Gaps in Vision-Language Model Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#35821;&#35328;-Only VLM&#36873;&#25321;&#20013;&#30340;&#20004;&#20010;&#22266;&#26377;&#25361;&#25112;&#65306;&#12300;&#27169;&#24577;&#24046;&#36317;&#12301;&#21644;&#12300;&#33021;&#21147;&#24046;&#36317;&#12301;&#65292;&#24182;&#25552;&#20986;&#20102;VLM&#36873;&#25321;&#20013;&#24357;&#21512;&#36825;&#20004;&#20010;&#24046;&#36317;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#36890;&#36807;&#23558;&#22270;&#20687;&#19982;&#25991;&#26412;&#31867;&#21035;&#21517;&#31216;&#37197;&#23545;&#65292;&#22312;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#39044;&#35757;&#32451;&#30340;VLMs&#30340;&#19981;&#26029;&#22686;&#21152;&#20351;&#24471;&#29305;&#23450;&#20219;&#21153;&#30340;VLM&#36873;&#25321;&#26356;&#26377;&#21487;&#33021;&#26631;&#35782;&#20986;&#36866;&#21512;&#30340;VLM&#12290;&#22240;&#27492;&#65292;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#31574;&#30053;&#26159;&#20174;VLM&#21160;&#29289;&#22253;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;VLM&#65292;&#20165;&#20381;&#36182;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#25968;&#25454;&#32780;&#26080;&#38656;&#35775;&#38382;&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#36825;&#31181;&#20165;&#35821;&#35328;VLM&#36873;&#25321;&#20013;&#20004;&#20010;&#22266;&#26377;&#25361;&#25112;&#65306;&#12300;&#27169;&#24577;&#24046;&#36317;&#12301;&#8212;&#8212;VLM&#22312;&#20004;&#20010;&#19981;&#21516;&#27169;&#24577;&#19979;&#30340;&#23884;&#20837;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20351;&#24471;&#25991;&#26412;&#25104;&#20026;&#22270;&#20687;&#30340;&#19968;&#20010;&#19981;&#22826;&#21487;&#38752;&#30340;&#26367;&#20195;&#21697;&#65307;&#12300;&#33021;&#21147;&#24046;&#36317;&#12301;&#8212;&#8212;VLM&#30340;&#25972;&#20307;&#25490;&#21517;&#19982;&#20854;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#25490;&#21517;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#38459;&#30861;&#20102;&#30452;&#25509;&#20174;&#27169;&#22411;&#30340;&#25972;&#20307;&#34920;&#29616;&#26469;&#39044;&#27979;&#20854;&#25968;&#25454;&#38598;&#29305;&#23450;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;VLM&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13797v1 Announce Type: new  Abstract: Vision Language Models (VLMs) excel in zero-shot image classification by pairing images with textual category names. The expanding variety of Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for specific tasks. Thus, a promising zero-shot image classification strategy is selecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely on the text data of the target dataset without access to the dataset's images. In this paper, we analyze two inherent challenges in assessing the ability of a VLM in this Language-Only VLM selection: the "Modality Gap" -- the disparity in VLM's embeddings across two different modalities, making text a less reliable substitute for images; and the "Capability Gap" -- the discrepancy between the VLM's overall ranking and its ranking for target dataset, hindering direct prediction of a model's dataset-specific performance from its general performance. We propose VLM Selectio
&lt;/p&gt;</description></item><item><title>CLIP&#27169;&#22411;&#22312;&#38754;&#23545;&#20998;&#24067;&#36716;&#31227;&#26102;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20316;&#32773;&#35774;&#35745;&#20102;CounterAnimal&#25968;&#25454;&#38598;&#26469;&#25506;&#31350;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11497</link><description>&lt;p&gt;
CLIP&#24635;&#26159;&#27604;ImageNet&#27169;&#22411;&#27867;&#21270;&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do CLIPs Always Generalize Better than ImageNet Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11497
&lt;/p&gt;
&lt;p&gt;
CLIP&#27169;&#22411;&#22312;&#38754;&#23545;&#20998;&#24067;&#36716;&#31227;&#26102;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20316;&#32773;&#35774;&#35745;&#20102;CounterAnimal&#25968;&#25454;&#38598;&#26469;&#25506;&#31350;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;CLIP&#65292;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#12290;CLIP&#23637;&#31034;&#20102;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#65292;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#25991;&#29486;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;CLIP&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#20027;&#35201;&#26159;&#20026;ImageNet&#22522;&#20934;&#32780;&#35774;&#35745;&#30340;&#21464;&#31181;&#65292;&#21487;&#33021;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;CLIP&#22312;LAION&#31561;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#31283;&#20581;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;CounterAnimal&#65292;&#20854;&#20013;&#21253;&#21547;&#21160;&#29289;&#29031;&#29255;&#20013;&#21457;&#29616;&#30340;&#29616;&#23454;&#34394;&#20551;&#29305;&#24449;&#12290;CounterAnimal&#21253;&#25324;a&#65289;&#24120;&#35265;&#32452;&#65306;&#21253;&#25324;&#24120;&#35265;&#32972;&#26223;&#30340;&#21160;&#29289;&#65292;&#24182;&#19988; b) &#23545;&#29031;&#32452;&#65306;&#21253;&#25324;&#22312;&#19981;&#23547;&#24120;&#32972;&#26223;&#19979;&#30340;&#21160;&#29289;&#12290;&#20174;&#24120;&#35265;&#32452;&#21040;&#23545;&#29031;&#32452;&#30340;&#24615;&#33021;&#19979;&#38477;&#37327;&#21270;&#20102;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#65288;&#21363;&#32972;&#26223;&#65289;&#39044;&#27979;&#21160;&#29289;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;LAION&#25110;OpenAI&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;CLIP&#21363;&#27809;&#26377;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11497v1 Announce Type: cross  Abstract: Large vision language models, such as CLIPs, have revolutionized modern machine learning. CLIPs have demonstrated great generalizability under distribution shifts, supported by an increasing body of literature. However, the evaluation datasets for CLIPs are variations primarily designed for ImageNet benchmarks, which may not fully reflect the extent to which CLIPs, e.g., pre-trained on LAION, robust to spurious correlations. To bridge the gap, we collect a real-world dataset called CounterAnimal that contains realistic spurious features found in animal photos. CounterAnimal consists of a) the common group: comprising animals on common backgrounds, and b) the counter group: including animals on unusual backgrounds. The performance drops from the common to counter groups quantify the reliance of models on spurious features (i.e., backgrounds) to predict the animals. We find that CLIPs trained on either LAION or the OpenAI data exhibit no
&lt;/p&gt;</description></item><item><title>LaB-GATr &#26159;&#19968;&#31181;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24207;&#21015;&#21387;&#32553;&#21644;&#25554;&#20540;&#26377;&#25928;&#22320;&#23398;&#20064;&#22823;&#35268;&#27169;&#29983;&#29289;&#21307;&#23398;&#34920;&#38754;&#21644;&#20307;&#31215;&#32593;&#26684;&#65292;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340; GATr &#26041;&#27861;&#24182;&#23562;&#37325;&#20102;&#27431;&#20960;&#37324;&#24471;&#23545;&#31216;&#24615;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07536</link><description>&lt;p&gt;
LaB-GATr&#65306;&#22823;&#35268;&#27169;&#29983;&#29289;&#21307;&#23398;&#34920;&#38754;&#21644;&#20307;&#31215;&#32593;&#26684;&#30340;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
LaB-GATr: geometric algebra transformers for large biomedical surface and volume meshes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07536
&lt;/p&gt;
&lt;p&gt;
LaB-GATr &#26159;&#19968;&#31181;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24207;&#21015;&#21387;&#32553;&#21644;&#25554;&#20540;&#26377;&#25928;&#22320;&#23398;&#20064;&#22823;&#35268;&#27169;&#29983;&#29289;&#21307;&#23398;&#34920;&#38754;&#21644;&#20307;&#31215;&#32593;&#26684;&#65292;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340; GATr &#26041;&#27861;&#24182;&#23562;&#37325;&#20102;&#27431;&#20960;&#37324;&#24471;&#23545;&#31216;&#24615;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35299;&#21078;&#32467;&#26500;&#21487;&#20197;&#29992;&#34920;&#38754;&#25110;&#20307;&#31215;&#32593;&#26684;&#26469;&#25551;&#36848;&#12290;&#26426;&#22120;&#23398;&#20064;&#26159;&#20174;&#36825;&#20123;3D&#27169;&#22411;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#39640;&#20445;&#30495;&#24230;&#30340;&#32593;&#26684;&#36890;&#24120;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#39030;&#28857;&#65292;&#36825;&#22312;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26102;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#24739;&#32773;&#29305;&#24322;&#24615;&#32593;&#26684;&#21487;&#33021;&#27809;&#26377;&#32463;&#20856;&#23545;&#40784;&#65292;&#36825;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LaB-GATr&#65292;&#19968;&#31181;&#20855;&#26377;&#20960;&#20309;&#26631;&#35760;&#21270;&#30340;&#36716;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24207;&#21015;&#21387;&#32553;&#21644;&#25554;&#20540;&#26377;&#25928;&#22320;&#23398;&#20064;&#22823;&#35268;&#27169;&#65288;&#29983;&#29289;&#65289;&#21307;&#23398;&#34920;&#38754;&#21644;&#20307;&#31215;&#32593;&#26684;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;&#65288;GATr&#65289;&#65292;&#22240;&#27492;&#23562;&#37325;&#25152;&#26377;&#27431;&#20960;&#37324;&#24471;&#23545;&#31216;&#24615;&#65292;&#21363;&#26059;&#36716;&#12289;&#24179;&#31227;&#21644;&#21453;&#23556;&#65292;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#24739;&#32773;&#20043;&#38388;&#32463;&#20856;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07536v1 Announce Type: cross  Abstract: Many anatomical structures can be described by surface or volume meshes. Machine learning is a promising tool to extract information from these 3D models. However, high-fidelity meshes often contain hundreds of thousands of vertices, which creates unique challenges in building deep neural network architectures. Furthermore, patient-specific meshes may not be canonically aligned which limits the generalisation of machine learning algorithms. We propose LaB-GATr, a transfomer neural network with geometric tokenisation that can effectively learn with large-scale (bio-)medical surface and volume meshes through sequence compression and interpolation. Our method extends the recently proposed geometric algebra transformer (GATr) and thus respects all Euclidean symmetries, i.e. rotation, translation and reflection, effectively mitigating the problem of canonical alignment between patients. LaB-GATr achieves state-of-the-art results on three ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#22914;&#20309;&#20943;&#23569;CLIP&#23545;&#20266;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#20174;&#32780;&#25552;&#39640;&#32676;&#20307;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#27880;&#37322;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.07241</link><description>&lt;p&gt;
&#26657;&#20934;&#22810;&#27169;&#24577;&#34920;&#31034;&#65306;&#22312;&#19981;&#20351;&#29992;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#36861;&#27714;&#32676;&#20307;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#22914;&#20309;&#20943;&#23569;CLIP&#23545;&#20266;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#20174;&#32780;&#25552;&#39640;&#32676;&#20307;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#27880;&#37322;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07241v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;CLIP&#65292;&#22312;&#22810;&#26679;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#23384;&#22312;&#19968;&#20123;&#30171;&#28857;&#65306;(i) &#30452;&#25509;&#24494;&#35843;&#25972;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#26082;&#26102;&#38388;&#23494;&#38598;&#21448;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#35843;&#25972;&#21518;&#30340;&#27169;&#22411;&#24448;&#24448;&#21464;&#24471;&#39640;&#24230;&#19987;&#19994;&#21270;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#23454;&#29992;&#24615;&#65307;(ii) &#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20998;&#31867;&#22120;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#20110;&#20266;&#29305;&#24449;-&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19982;&#30446;&#26631;&#30456;&#20851;&#30340;&#27169;&#24335;&#65292;&#20294;&#19982;&#30495;&#23454;&#26631;&#31614;&#20989;&#25968;&#26080;&#20851;&#65307;(iii) &#29616;&#26377;&#20851;&#20110;&#20943;&#23569;&#23545;&#20266;&#29305;&#24449;&#20381;&#36182;&#30340;&#30740;&#31350;&#65292;&#20027;&#35201;&#22522;&#20110;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#36825;&#20123;&#29305;&#24449;&#30340;&#20551;&#35774;&#65292;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#24182;&#27809;&#26377;&#25552;&#20379;&#30830;&#20999;&#30340;&#20445;&#35777;&#12290;&#20316;&#20026;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;&#65292;&#26412;&#24037;&#20316;&#20391;&#37325;&#20110;&#25506;&#32034;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;CLIP&#23545;&#20266;&#29305;&#24449;&#20381;&#36182;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07241v1 Announce Type: cross  Abstract: Fine-tuning pre-trained vision-language models, like CLIP, has yielded success on diverse downstream tasks. However, several pain points persist for this paradigm: (i) directly tuning entire pre-trained models becomes both time-intensive and computationally costly. Additionally, these tuned models tend to become highly specialized, limiting their practicality for real-world deployment; (ii) recent studies indicate that pre-trained vision-language classifiers may overly depend on spurious features -- patterns that correlate with the target in training data, but are not related to the true labeling function; and (iii) existing studies on mitigating the reliance on spurious features, largely based on the assumption that we can identify such features, does not provide definitive assurance for real-world applications. As a piloting study, this work focuses on exploring mitigating the reliance on spurious features for CLIP without using any 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Sparse Linear Concept Embeddings&#65288;SpLiCE&#65289;&#65292;&#36890;&#36807;&#23558;CLIP&#34920;&#31034;&#36716;&#25442;&#20026;&#20154;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;CLIP&#23884;&#20837;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.10376</link><description>&lt;p&gt;
&#29992;&#31232;&#30095;&#32447;&#24615;&#27010;&#24565;&#23884;&#20837;&#65288;SpLiCE&#65289;&#35299;&#37322;CLIP
&lt;/p&gt;
&lt;p&gt;
Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Sparse Linear Concept Embeddings&#65288;SpLiCE&#65289;&#65292;&#36890;&#36807;&#23558;CLIP&#34920;&#31034;&#36716;&#25442;&#20026;&#20154;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;CLIP&#23884;&#20837;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#23884;&#20837;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36825;&#20123;&#39640;&#32500;&#31264;&#23494;&#21521;&#37327;&#34920;&#31034;&#24182;&#19981;&#23481;&#26131;&#35299;&#37322;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#36879;&#26126;&#24230;&#30340;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;CLIP&#30340;&#28508;&#22312;&#31354;&#38388;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;&#22240;&#27492;&#21487;&#20197;&#23558;CLIP&#34920;&#31034;&#20998;&#35299;&#20026;&#20854;&#28508;&#22312;&#35821;&#20041;&#32452;&#20214;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#29702;&#35299;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31232;&#30095;&#32447;&#24615;&#27010;&#24565;&#23884;&#20837;&#65288;SpLiCE&#65289;&#65292;&#29992;&#20110;&#23558;CLIP&#34920;&#31034;&#36716;&#25442;&#20026;&#20154;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;SpLiCE&#19981;&#38656;&#35201;&#27010;&#24565;&#26631;&#31614;&#65292;&#24182;&#19988;&#21487;&#20197;&#21518;&#26399;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;SpLiCE&#36755;&#20986;&#30340;&#34920;&#31034;&#21487;&#20197;&#35299;&#37322;&#29978;&#33267;&#21462;&#20195;&#20256;&#32479;&#30340;&#23494;&#38598;CLIP&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10376v1 Announce Type: new  Abstract: CLIP embeddings have demonstrated remarkable performance across a wide range of computer vision tasks. However, these high-dimensional, dense vector representations are not easily interpretable, restricting their usefulness in downstream applications that require transparency. In this work, we empirically show that CLIP's latent space is highly structured, and consequently that CLIP representations can be decomposed into their underlying semantic components. We leverage this understanding to propose a novel method, Sparse Linear Concept Embeddings (SpLiCE), for transforming CLIP representations into sparse linear combinations of human-interpretable concepts. Distinct from previous work, SpLiCE does not require concept labels and can be applied post hoc. Through extensive experimentation with multiple real-world datasets, we validate that the representations output by SpLiCE can explain and even replace traditional dense CLIP representati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;DeSparsify&#65292;&#38024;&#23545;&#20351;&#29992;Token&#31232;&#30095;&#21270;&#26426;&#21046;&#30340;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#31934;&#24515;&#21046;&#20316;&#30340;&#23545;&#25239;&#26679;&#26412;&#27450;&#39575;&#31232;&#30095;&#21270;&#26426;&#21046;&#65292;&#23548;&#33268;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#65292;&#20197;&#27492;&#32791;&#23613;&#25805;&#20316;&#31995;&#32479;&#30340;&#36164;&#28304;&#24182;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02554</link><description>&lt;p&gt;
DeSparsify&#65306;&#23545;&#35270;&#35273;Transformer&#20013;&#30340;Token&#31232;&#30095;&#21270;&#26426;&#21046;&#36827;&#34892;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms in Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;DeSparsify&#65292;&#38024;&#23545;&#20351;&#29992;Token&#31232;&#30095;&#21270;&#26426;&#21046;&#30340;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#31934;&#24515;&#21046;&#20316;&#30340;&#23545;&#25239;&#26679;&#26412;&#27450;&#39575;&#31232;&#30095;&#21270;&#26426;&#21046;&#65292;&#23548;&#33268;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#65292;&#20197;&#27492;&#32791;&#23613;&#25805;&#20316;&#31995;&#32479;&#30340;&#36164;&#28304;&#24182;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20570;&#20986;&#20102;&#24040;&#22823;&#36129;&#29486;&#65292;&#23637;&#29616;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#65288;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#65289;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#39640;&#35745;&#31639;&#35201;&#27714;&#38543;&#20351;&#29992;&#30340;Token&#25968;&#37327;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Token&#31232;&#30095;&#21270;&#25216;&#26415;&#12290;&#36825;&#20123;&#25216;&#26415;&#37319;&#29992;&#20102;&#19968;&#31181;&#20381;&#36182;&#36755;&#20837;&#30340;&#31574;&#30053;&#65292;&#23558;&#26080;&#20851;&#30340;Token&#20174;&#35745;&#31639;&#27969;&#31243;&#20013;&#20002;&#24323;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21160;&#24577;&#24615;&#21644;&#24179;&#22343;&#24773;&#20917;&#20551;&#35774;&#20351;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961; - &#32463;&#36807;&#31934;&#24515;&#21046;&#20316;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#33021;&#22815;&#27450;&#39575;&#31232;&#30095;&#21270;&#26426;&#21046;&#65292;&#23548;&#33268;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#26041;&#27861;DeSparsify&#65292;&#38024;&#23545;&#20351;&#29992;Token&#31232;&#30095;&#21270;&#26426;&#21046;&#30340;&#35270;&#35273;Transformer&#30340;&#21487;&#29992;&#24615;&#12290;&#35813;&#25915;&#20987;&#26088;&#22312;&#32791;&#23613;&#25805;&#20316;&#31995;&#32479;&#30340;&#36164;&#28304;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers have contributed greatly to advancements in the computer vision domain, demonstrating state-of-the-art performance in diverse tasks (e.g., image classification, object detection). However, their high computational requirements grow quadratically with the number of tokens used. Token sparsification techniques have been proposed to address this issue. These techniques employ an input-dependent strategy, in which uninformative tokens are discarded from the computation pipeline, improving the model's efficiency. However, their dynamism and average-case assumption makes them vulnerable to a new threat vector - carefully crafted adversarial examples capable of fooling the sparsification mechanism, resulting in worst-case performance. In this paper, we present DeSparsify, an attack targeting the availability of vision transformers that use token sparsification mechanisms. The attack aims to exhaust the operating system's resources, while maintaining its stealthiness. Our e
&lt;/p&gt;</description></item><item><title>CMMMU&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#20026;&#22635;&#34917;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#35780;&#20272;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31354;&#30333;&#32780;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2401.11944</link><description>&lt;p&gt;
CMMMU&#65306;&#19968;&#20010;&#20013;&#22269;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11944
&lt;/p&gt;
&lt;p&gt;
CMMMU&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#20026;&#22635;&#34917;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#35780;&#20272;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31354;&#30333;&#32780;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;(LMMs)&#30340;&#33021;&#21147;&#19981;&#26029;&#25552;&#21319;&#65292;&#35780;&#20272;LMMs&#30340;&#34920;&#29616;&#26085;&#30410;&#25104;&#20026;&#19968;&#20010;&#36843;&#20999;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#22312;&#35780;&#20272;LMMs&#22312;&#20013;&#25991;&#31561;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26356;&#22823;&#24046;&#36317;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CMMMU&#65292;&#19968;&#20010;&#26032;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;LMMs&#22312;&#38656;&#35201;&#22823;&#23398;&#27700;&#24179;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;CMMMU&#21463;&#21040;&#20102;MMMUs&#30340;&#26631;&#27880;&#21644;&#20998;&#26512;&#27169;&#24335;&#30340;&#21551;&#21457;&#24182;&#20005;&#26684;&#36981;&#24490;&#12290;CMMMU&#21253;&#25324;&#26469;&#33258;&#22823;&#23398;&#32771;&#35797;&#12289;&#27979;&#39564;&#21644;&#25945;&#31185;&#20070;&#30340;1.2&#19975;&#20010;&#25163;&#21160;&#25910;&#38598;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#28085;&#30422;&#20845;&#20010;&#26680;&#24515;&#23398;&#31185;&#65306;&#33402;&#26415;&#19982;&#35774;&#35745;&#12289;&#21830;&#19994;&#12289;&#31185;&#23398;&#12289;&#20581;&#24247;&#19982;&#21307;&#23398;&#12289;&#20154;&#25991;&#31038;&#31185;&#20197;&#21450;&#25216;&#26415;&#19982;&#24037;&#31243;&#65292;&#23601;&#20687;&#20854;&#20249;&#20276;MMMMU&#19968;&#26679;&#12290;&#36825;&#20123;&#38382;&#39064;&#28085;&#30422;30&#20010;&#23398;&#31185;&#65292;&#21253;&#25324;39&#20010;&#39640;&#24230;&#24322;&#36136;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11944v2 Announce Type: replace-cross  Abstract: As the capabilities of large multimodal models (LMMs) continue to advance, evaluating the performance of LMMs emerges as an increasing need. Additionally, there is an even larger gap in evaluating the advanced knowledge and reasoning abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU, a new Chinese Massive Multi-discipline Multimodal Understanding benchmark designed to evaluate LMMs on tasks demanding college-level subject knowledge and deliberate reasoning in a Chinese context. CMMMU is inspired by and strictly follows the annotation and analysis pattern of MMMU.   CMMMU includes 12k manually collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art &amp; Design, Business, Science, Health &amp; Medicine, Humanities &amp; Social Science, and Tech &amp; Engineering, like its companion, MMMU. These questions span 30 subjects and comprise 39 highly heterogeneous image 
&lt;/p&gt;</description></item><item><title>SASSL&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#32806;&#35821;&#20041;&#21644;&#39118;&#26684;&#23646;&#24615;&#65292;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.01187</link><description>&lt;p&gt;
SASSL:&#36890;&#36807;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#22686;&#24378;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer. (arXiv:2312.01187v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01187
&lt;/p&gt;
&lt;p&gt;
SASSL&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#32806;&#35821;&#20041;&#21644;&#39118;&#26684;&#23646;&#24615;&#65292;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20381;&#36182;&#20110;&#25968;&#25454;&#22686;&#24378;&#26469;&#20174;&#26080;&#26631;&#31614;&#22270;&#20687;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#34920;&#24449;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22686;&#24378;&#27969;&#27700;&#32447;&#21253;&#25324;&#20102;&#21508;&#31181;&#21407;&#22987;&#30340;&#36716;&#25442;&#65292;&#20294;&#36890;&#24120;&#24573;&#30053;&#20102;&#33258;&#28982;&#22270;&#20687;&#30340;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#22686;&#24378;&#26679;&#26412;&#21487;&#33021;&#26174;&#31034;&#20986;&#36864;&#21270;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#20302;&#39118;&#26684;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#24433;&#21709;&#21040;&#33258;&#30417;&#30563;&#34920;&#24449;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SASSL&#30340;&#26032;&#22411;&#22686;&#24378;&#25216;&#26415;&#65292;&#23427;&#22522;&#20110;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#12290;&#35813;&#26041;&#27861;&#23558;&#22270;&#20687;&#20013;&#30340;&#35821;&#20041;&#21644;&#39118;&#26684;&#23646;&#24615;&#35299;&#32806;&#65292;&#24182;&#20165;&#23545;&#39118;&#26684;&#24212;&#29992;&#36716;&#25442;&#65292;&#20445;&#25345;&#20869;&#23481;&#65292;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#26679;&#26412;&#65292;&#26356;&#22909;&#22320;&#20445;&#30041;&#23427;&#20204;&#30340;&#35821;&#20041;&#23646;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#24191;&#20026;&#25509;&#21463;&#30340;MoCo v2&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;ImageNet&#19978;&#30340;top-1&#20998;&#31867;&#24615;&#33021;&#25552;&#21319;&#36229;&#36807;2%&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning relies heavily on data augmentation to extract meaningful representations from unlabeled images. While existing state-of-the-art augmentation pipelines incorporate a wide range of primitive transformations, these often disregard natural image structure. Thus, augmented samples can exhibit degraded semantic information and low stylistic diversity, affecting downstream performance of self-supervised representations. To overcome this, we propose SASSL: Style Augmentations for Self Supervised Learning, a novel augmentation technique based on Neural Style Transfer. The method decouples semantic and stylistic attributes in images and applies transformations exclusively to the style while preserving content, generating diverse augmented samples that better retain their semantic properties. Experimental results show our technique achieves a top-1 classification performance improvement of more than 2% on ImageNet compared to the well-established MoCo v2. We also measure
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SegLoc&#65292;&#19968;&#31181;&#29992;&#20110;&#23433;&#20840;&#26816;&#26597;X&#23556;&#32447;&#22270;&#20687;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#26032;&#39062;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#32467;&#21512;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.08421</link><description>&lt;p&gt;
SegLoc: &#26032;&#39062;&#30340;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#29992;&#20110;&#23433;&#20840;&#26816;&#26597;X&#23556;&#32447;&#22270;&#20687;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
SegLoc: Novel Visual Self-supervised Learning Scheme for Dense Prediction Tasks of Security Inspection X-ray Images. (arXiv:2310.08421v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08421
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SegLoc&#65292;&#19968;&#31181;&#29992;&#20110;&#23433;&#20840;&#26816;&#26597;X&#23556;&#32447;&#22270;&#20687;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#26032;&#39062;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#32467;&#21512;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#26174;&#33879;&#36827;&#23637;&#24402;&#21151;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#30340;&#25972;&#21512;&#12290;&#23613;&#31649;&#22312;NLP&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#65292;&#20294;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#30456;&#27604;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#36824;&#19981;&#33021;&#20445;&#25345;&#30456;&#24212;&#30340;&#21457;&#23637;&#12290;&#26368;&#36817;&#65292;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20986;&#36229;&#36234;&#26377;&#30417;&#30563;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#25913;&#36827;&#37117;&#23616;&#38480;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#19988;&#21482;&#26377;&#23569;&#25968;&#24037;&#20316;&#33268;&#21147;&#20110;&#35780;&#20272;&#35745;&#31639;&#26426;&#35270;&#35273;&#23454;&#38469;&#22330;&#26223;&#19979;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#21253;&#21547;&#31867;&#21035;&#20154;&#20687;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#23588;&#20854;&#26159;ImageNet&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23433;&#20840;&#26816;&#26597;X&#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#35821;&#20041;&#20998;&#21106;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#26469;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;SegLoc&#12290;
&lt;/p&gt;
&lt;p&gt;
Lately, remarkable advancements of artificial intelligence have been attributed to the integration of self-supervised learning scheme. Despite impressive achievements within NLP, yet SSL in computer vision has not been able to stay on track comparatively. Recently, integration of contrastive learning on top of existing SSL models has established considerable progress in computer vision through which visual SSL models have outperformed their supervised counterparts. Nevertheless, most of these improvements were limited to classification tasks, and also, few works have been dedicated to evaluation of SSL models in real-world scenarios of computer vision, while the majority of works are centered around datasets containing class-wise portrait images, most notably, ImageNet. Consequently, in this work, we have considered dense prediction task of semantic segmentation in security inspection x-ray images to evaluate our proposed model Segmentation Localization. Based upon the model Instance L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#20998;&#24067;&#22806;&#30340;&#25968;&#25454;&#24182;&#20174;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#22522;&#20110;&#38598;&#25104;&#30340;OOD&#26816;&#27979;&#22120;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15710</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Object Detection in the Open World. (arXiv:2307.15710v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#20998;&#24067;&#22806;&#30340;&#25968;&#25454;&#24182;&#20174;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#22522;&#20110;&#38598;&#25104;&#30340;OOD&#26816;&#27979;&#22120;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#20551;&#35774;&#35757;&#32451;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#26377;&#19968;&#32452;&#22266;&#23450;&#30340;&#31867;&#21035;&#65292;&#21363;&#23646;&#20110;&#20998;&#24067;&#20869;&#65288;ID&#65289;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#26041;&#27861;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#24212;&#29992;&#26102;&#65292;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#22240;&#20026;&#26410;&#26631;&#35760;&#21644;&#27979;&#35797;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#65292;&#21363;&#23646;&#20110;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25506;&#35752;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#26816;&#27979;&#36825;&#20123;OOD&#26679;&#26412;&#65292;&#22914;&#26524;&#21487;&#20197;&#65292;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#20174;&#20013;&#23398;&#20064;&#65311;&#32771;&#34385;&#21040;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24320;&#25918;&#19990;&#30028;&#21322;&#30417;&#30563;&#26816;&#27979;&#26694;&#26550;&#65288;OWSSD&#65289;&#65292;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;OOD&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#20174;ID&#21644;OOD&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#38598;&#25104;&#30340;OOD&#26816;&#27979;&#22120;&#65292;&#30001;&#20165;&#22312;ID&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#36731;&#37327;&#32423;&#33258;&#32534;&#30721;&#22120;&#32593;&#32476;&#32452;&#25104;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;OOD&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches for semi-supervised object detection assume a fixed set of classes present in training and unlabeled datasets, i.e., in-distribution (ID) data. The performance of these techniques significantly degrades when these techniques are deployed in the open-world, due to the fact that the unlabeled and test data may contain objects that were not seen during training, i.e., out-of-distribution (OOD) data. The two key questions that we explore in this paper are: can we detect these OOD samples and if so, can we learn from them? With these considerations in mind, we propose the Open World Semi-supervised Detection framework (OWSSD) that effectively detects OOD data along with a semi-supervised learning pipeline that learns from both ID and OOD data. We introduce an ensemble based OOD detector consisting of lightweight auto-encoder networks trained only on ID data. Through extensive evalulation, we demonstrate that our method performs competitively against state-of-the-art OOD 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#26368;&#20248;&#20256;&#36755;&#26631;&#31614;&#20998;&#37197;(DOTLA)&#26694;&#26550;&#65292;&#20197;&#21516;&#26102;&#23558;&#19968;&#20010;&#27169;&#24577;&#20013;&#29983;&#25104;&#30340;&#26631;&#31614;&#20998;&#37197;&#32473;&#20854;&#23545;&#24212;&#30340;&#27169;&#24577;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;&#12290;&#22312;&#30456;&#24212;&#27169;&#24577;&#20013;&#37051;&#23621;&#26679;&#26412;&#30340;&#25351;&#23548;&#19979;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#37051;&#23621;&#19968;&#33268;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#31934;&#28860;&#21644;&#27491;&#21017;&#21270;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12711</link><description>&lt;p&gt;
&#36890;&#36807;&#37051;&#23621;&#24341;&#23548;&#30340;&#26631;&#31614;&#31934;&#28860;&#21327;&#21516;&#23398;&#20064;&#23454;&#29616;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Visible-Infrared Person ReID by Collaborative Learning with Neighbor-Guided Label Refinement. (arXiv:2305.12711v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#26368;&#20248;&#20256;&#36755;&#26631;&#31614;&#20998;&#37197;(DOTLA)&#26694;&#26550;&#65292;&#20197;&#21516;&#26102;&#23558;&#19968;&#20010;&#27169;&#24577;&#20013;&#29983;&#25104;&#30340;&#26631;&#31614;&#20998;&#37197;&#32473;&#20854;&#23545;&#24212;&#30340;&#27169;&#24577;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;&#12290;&#22312;&#30456;&#24212;&#27169;&#24577;&#20013;&#37051;&#23621;&#26679;&#26412;&#30340;&#25351;&#23548;&#19979;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#37051;&#23621;&#19968;&#33268;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#31934;&#28860;&#21644;&#27491;&#21017;&#21270;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#21487;&#35265;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;(USL-VI-ReID)&#26088;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#36328;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#27169;&#24577;&#19981;&#21464;&#29305;&#24449;&#65292;&#36825;&#22312;&#35270;&#39057;&#30417;&#25511;&#31995;&#32479;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#35299;&#20915;&#36328;&#27169;&#24577;&#25968;&#25454;&#20851;&#32852;&#38382;&#39064;&#23545;&#20110;&#36827;&#19968;&#27493;&#36827;&#34892;&#24322;&#36136;&#32852;&#21512;&#23398;&#20064;&#38750;&#24120;&#20851;&#38190;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#26368;&#20248;&#20256;&#36755;&#26631;&#31614;&#20998;&#37197;(DOTLA)&#26694;&#26550;&#65292;&#21516;&#26102;&#23558;&#19968;&#20010;&#27169;&#24577;&#20013;&#29983;&#25104;&#30340;&#26631;&#31614;&#20998;&#37197;&#32473;&#20854;&#23545;&#24212;&#30340;&#27169;&#24577;&#12290;&#25152;&#25552;&#20986;&#30340;DOTLA&#26426;&#21046;formulate&#20102;&#19968;&#31181;&#30456;&#20114;&#22686;&#24378;&#21644;&#39640;&#25928;&#30340;&#36328;&#27169;&#24577;&#25968;&#25454;&#20851;&#32852;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#19968;&#20123;&#19981;&#36275;&#21644;&#22122;&#22768;&#26631;&#31614;&#20851;&#32852;&#30340;&#21103;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#37051;&#23621;&#19968;&#33268;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#31934;&#28860;&#21644;&#27491;&#21017;&#21270;&#27169;&#22359;&#65292;&#22312;&#30456;&#24212;&#27169;&#24577;&#20013;&#37051;&#23621;&#26679;&#26412;&#30340;&#25351;&#23548;&#19979;&#28040;&#38500;&#30001;&#19981;&#20934;&#30830;&#30340;&#30417;&#30563;&#20449;&#21495;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;USL-VI-ReID&#27169;&#22411;&#19982;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#29978;&#33267;&#19968;&#20123;&#26377;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning visible-infrared person re-identification (USL-VI-ReID) aims at learning modality-invariant features from unlabeled cross-modality dataset, which is crucial for practical applications in video surveillance systems. The key to essentially address the USL-VI-ReID task is to solve the cross-modality data association problem for further heterogeneous joint learning. To address this issue, we propose a Dual Optimal Transport Label Assignment (DOTLA) framework to simultaneously assign the generated labels from one modality to its counterpart modality. The proposed DOTLA mechanism formulates a mutual reinforcement and efficient solution to cross-modality data association, which could effectively reduce the side-effects of some insufficient and noisy label associations. Besides, we further propose a cross-modality neighbor consistency guided label refinement and regularization module, to eliminate the negative effects brought by the inaccurate supervised signals, under th
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21305;&#37197;&#36328;&#27169;&#24577;&#32858;&#31867;&#26469;&#20943;&#23569;&#27169;&#24577;&#24046;&#24322;&#30340;&#21452;&#21521;&#32858;&#31867;&#21305;&#37197;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#27169;&#24577;&#29305;&#23450;&#21644;&#27169;&#24577;&#19981;&#21487;&#30693;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#20849;&#21516;&#23545;&#40784;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.12673</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#21452;&#36793;&#36328;&#27169;&#24577;&#32858;&#31867;&#21305;&#37197;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;&#20809;-&#32418;&#22806;&#20154;&#29289;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Efficient Bilateral Cross-Modality Cluster Matching for Unsupervised Visible-Infrared Person ReID. (arXiv:2305.12673v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12673
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21305;&#37197;&#36328;&#27169;&#24577;&#32858;&#31867;&#26469;&#20943;&#23569;&#27169;&#24577;&#24046;&#24322;&#30340;&#21452;&#21521;&#32858;&#31867;&#21305;&#37197;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#27169;&#24577;&#29305;&#23450;&#21644;&#27169;&#24577;&#19981;&#21487;&#30693;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#20849;&#21516;&#23545;&#40784;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#21487;&#35265;&#20809;-&#32418;&#22806;&#20154;&#29289;&#35782;&#21035;&#65288;USL-VI-ReID&#65289;&#26088;&#22312;&#22312;&#27809;&#26377;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#21305;&#37197;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#34892;&#20154;&#22270;&#20687;&#20013;&#30456;&#21516;&#36523;&#20221;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#38024;&#23545;&#27809;&#26377;&#24456;&#22909;&#25506;&#32034;&#36328;&#27169;&#24577;&#32858;&#31867;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21521;&#32858;&#31867;&#21305;&#37197;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21305;&#37197;&#36328;&#27169;&#24577;&#32858;&#31867;&#26469;&#20943;&#23569;&#27169;&#24577;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20108;&#20998;&#22270;&#20013;&#20248;&#21270;&#26368;&#22823;&#21305;&#37197;&#38382;&#39064;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#23545;&#22810;&#21452;&#36793;&#36328;&#27169;&#24577;&#32858;&#31867;&#21305;&#37197;&#65288;MBCCM&#65289;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#21305;&#37197;&#30340;&#25104;&#23545;&#32858;&#31867;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#21033;&#29992;&#20849;&#20139;&#30340;&#21487;&#35265;&#20809;&#21644;&#32418;&#22806;&#20266;&#26631;&#31614;&#12290;&#22312;&#36825;&#26679;&#30340;&#30417;&#30563;&#20449;&#21495;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#29305;&#23450;&#21644;&#27169;&#24577;&#19981;&#21487;&#30693;&#65288;MSMA&#65289;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#22312;&#32858;&#31867;&#32423;&#21035;&#19978;&#20849;&#21516;&#23545;&#40784;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#36328;&#27169;&#24577;&#30340;&#27169;&#24577;&#29305;&#23450;&#21644;&#27169;&#24577;&#19981;&#21487;&#30693;&#29305;&#24449;&#20063;&#34987;&#32771;&#34385;&#36827;&#21435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to match pedestrian images of the same identity from different modalities without annotations. Existing works mainly focus on alleviating the modality gap by aligning instance-level features of the unlabeled samples. However, the relationships between cross-modality clusters are not well explored. To this end, we propose a novel bilateral cluster matching-based learning framework to reduce the modality gap by matching cross-modality clusters. Specifically, we design a Many-to-many Bilateral Cross-Modality Cluster Matching (MBCCM) algorithm through optimizing the maximum matching problem in a bipartite graph. Then, the matched pairwise clusters utilize shared visible and infrared pseudo-labels during the model training. Under such a supervisory signal, a Modality-Specific and Modality-Agnostic (MSMA) contrastive learning framework is proposed to align features jointly at a cluster-level. Meanwhile, the cross-modal
&lt;/p&gt;</description></item></channel></rss>