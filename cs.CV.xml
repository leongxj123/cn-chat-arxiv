<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#35760;&#24518;&#38382;&#39064;&#12290;&#36890;&#36807;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#31243;&#24230;&#20197;&#21450;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#35760;&#24518;&#30340;&#22240;&#32032;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#21644;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01054</link><description>&lt;p&gt;
&#26080;&#26465;&#20214;&#30340;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#35760;&#24518;&#24739;&#32773;&#24433;&#20687;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Unconditional Latent Diffusion Models Memorize Patient Imaging Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#35760;&#24518;&#38382;&#39064;&#12290;&#36890;&#36807;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#31243;&#24230;&#20197;&#21450;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#35760;&#24518;&#30340;&#22240;&#32032;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#21644;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#30340;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#24212;&#29992;&#26159;&#36890;&#36807;&#25552;&#20986;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#30340;&#26367;&#20195;&#21697;&#26469;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#24320;&#25918;&#25968;&#25454;&#20849;&#20139;&#12290;&#23613;&#31649;&#26377;&#36825;&#20010;&#24212;&#29992;&#30340;&#21069;&#26223;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#24739;&#32773;&#25968;&#25454;&#30340;&#35760;&#24518;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#29983;&#25104;&#24739;&#32773;&#25968;&#25454;&#30340;&#21103;&#26412;&#32780;&#19981;&#26159;&#26032;&#30340;&#21512;&#25104;&#26679;&#26412;&#12290;&#36825;&#30772;&#22351;&#20102;&#20445;&#25252;&#24739;&#32773;&#25968;&#25454;&#30340;&#25972;&#20010;&#30446;&#30340;&#65292;&#29978;&#33267;&#21487;&#33021;&#23548;&#33268;&#24739;&#32773;&#34987;&#37325;&#26032;&#35782;&#21035;&#12290;&#32771;&#34385;&#21040;&#36825;&#20010;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#21307;&#23398;&#24433;&#20687;&#30028;&#20013;&#36825;&#20010;&#38382;&#39064;&#24182;&#27809;&#26377;&#24471;&#21040;&#22826;&#22810;&#20851;&#27880;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#35760;&#24518;&#38382;&#39064;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;2D&#21644;3D&#30340;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#20351;&#29992;CT&#12289;MR&#21644;X&#20809;&#25968;&#25454;&#38598;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#30340;&#29983;&#25104;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#26469;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#34987;&#35760;&#24518;&#30340;&#31243;&#24230;&#65292;&#24182;&#36827;&#19968;&#27493;&#30740;&#31350;&#21487;&#33021;&#23548;&#33268;&#35760;&#24518;&#30340;&#21508;&#31181;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative latent diffusion models hold a wide range of applications in the medical imaging domain. A noteworthy application is privacy-preserved open-data sharing by proposing synthetic data as surrogates of real patient data. Despite the promise, these models are susceptible to patient data memorization, where models generate patient data copies instead of novel synthetic samples. This undermines the whole purpose of preserving patient data and may even result in patient re-identification. Considering the importance of the problem, surprisingly it has received relatively little attention in the medical imaging community. To this end, we assess memorization in latent diffusion models for medical image synthesis. We train 2D and 3D latent diffusion models on CT, MR, and X-ray datasets for synthetic data generation. Afterwards, we examine the amount of training data memorized utilizing self-supervised models and further investigate various factors that can possibly lead to memorization 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26597;&#25214;&#34920;&#36827;&#34892;&#39640;&#25928;&#36793;&#32536;&#22270;&#20687;&#25512;&#26029;&#30340;ICELUT&#31639;&#27861;&#65292;&#26080;&#38656;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#38477;&#20302;&#30828;&#20214;&#25512;&#26029;&#26102;&#38388;&#21644;&#21151;&#32791;&#30340;&#21516;&#26102;&#23454;&#29616;&#36817;&#20046;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19238</link><description>&lt;p&gt;
&#39640;&#25928;&#22270;&#20687;&#20462;&#39280;&#30340;&#26597;&#25214;&#34920;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Taming Lookup Tables for Efficient Image Retouching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19238
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26597;&#25214;&#34920;&#36827;&#34892;&#39640;&#25928;&#36793;&#32536;&#22270;&#20687;&#25512;&#26029;&#30340;ICELUT&#31639;&#27861;&#65292;&#26080;&#38656;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#38477;&#20302;&#30828;&#20214;&#25512;&#26029;&#26102;&#38388;&#21644;&#21151;&#32791;&#30340;&#21516;&#26102;&#23454;&#29616;&#36817;&#20046;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#28165;&#23631;&#24149;&#22312;&#31471;&#35774;&#22791;(&#22914;&#32456;&#31471;&#29992;&#25143;&#30456;&#26426;&#12289;&#26234;&#33021;&#25163;&#26426;&#21644;&#30005;&#35270;)&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#25512;&#21160;&#20102;&#22270;&#20687;&#22686;&#24378;&#38656;&#27714;&#30340;&#26174;&#30528;&#22686;&#38271;&#12290;&#29616;&#26377;&#30340;&#22686;&#24378;&#27169;&#22411;&#36890;&#24120;&#22312;&#20248;&#21270;&#39640;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20943;&#23569;&#30828;&#20214;&#25512;&#26029;&#26102;&#38388;&#21644;&#21151;&#32791;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#21463;&#38480;&#30340;&#31471;&#35774;&#22791;&#32780;&#35328;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20687;&#39068;&#33394;&#22686;&#24378;&#26597;&#25214;&#34920;(ICELUT)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#26597;&#25214;&#34920;&#36827;&#34892;&#26497;&#20854;&#39640;&#25928;&#30340;&#36793;&#32536;&#25512;&#26029;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36880;&#28857;(1x1)&#21367;&#31215;&#26469;&#25552;&#21462;&#39068;&#33394;&#20449;&#24687;&#65292;&#21516;&#26102;&#20351;&#29992;&#20998;&#21106;&#20840;&#36830;&#25509;&#23618;&#26469;&#34701;&#20837;&#20840;&#23616;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#36825;&#20004;&#20010;&#32452;&#20214;&#37117;&#26080;&#32541;&#36716;&#25442;&#20026;&#26597;&#25214;&#34920;&#65292;&#20197;&#20415;&#36827;&#34892;&#30828;&#20214;&#26080;&#20851;&#30340;&#37096;&#32626;&#12290;ICELUT&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21151;&#32791;&#26497;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19238v1 Announce Type: cross  Abstract: The widespread use of high-definition screens in edge devices, such as end-user cameras, smartphones, and televisions, is spurring a significant demand for image enhancement. Existing enhancement models often optimize for high performance while falling short of reducing hardware inference time and power consumption, especially on edge devices with constrained computing and storage resources. To this end, we propose Image Color Enhancement Lookup Table (ICELUT) that adopts LUTs for extremely efficient edge inference, without any convolutional neural network (CNN). During training, we leverage pointwise (1x1) convolution to extract color information, alongside a split fully connected layer to incorporate global information. Both components are then seamlessly converted into LUTs for hardware-agnostic deployment. ICELUT achieves near-state-of-the-art performance and remarkably low power consumption. We observe that the pointwise network s
&lt;/p&gt;</description></item><item><title>PathoTune&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#22810;&#27169;&#24577;&#25552;&#31034;&#24494;&#35843;&#65292;&#23558;&#30149;&#29702;&#29978;&#33267;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#39640;&#25928;&#22320;&#35843;&#25972;&#21040;&#30149;&#29702;&#29305;&#23450;&#20219;&#21153;&#65292;&#20174;&#32780;&#32531;&#35299;&#22522;&#30784;-&#20219;&#21153;&#24046;&#36317;&#21644;&#20219;&#21153;-&#23454;&#20363;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.16497</link><description>&lt;p&gt;
PathoTune: &#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#35843;&#25972;&#33267;&#30149;&#29702;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
PathoTune: Adapting Visual Foundation Model to Pathological Specialists
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16497
&lt;/p&gt;
&lt;p&gt;
PathoTune&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#22810;&#27169;&#24577;&#25552;&#31034;&#24494;&#35843;&#65292;&#23558;&#30149;&#29702;&#29978;&#33267;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#39640;&#25928;&#22320;&#35843;&#25972;&#21040;&#30149;&#29702;&#29305;&#23450;&#20219;&#21153;&#65292;&#20174;&#32780;&#32531;&#35299;&#22522;&#30784;-&#20219;&#21153;&#24046;&#36317;&#21644;&#20219;&#21153;-&#23454;&#20363;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#22270;&#20687;&#29702;&#35299;&#36208;&#21521;&#39044;&#35757;&#32451;&#24494;&#35843;&#30340;&#26102;&#20195;&#30340;&#21516;&#26102;&#65292;&#30149;&#29702;&#24433;&#20687;&#30340;&#30740;&#31350;&#20063;&#22312;&#19981;&#26029;&#21457;&#23637;&#12290;&#23613;&#31649;&#20027;&#35201;&#20851;&#27880;&#39044;&#35757;&#32451;&#30149;&#29702;&#22522;&#30784;&#27169;&#22411;&#65292;&#20294;&#22914;&#20309;&#23558;&#22522;&#30784;&#27169;&#22411;&#35843;&#25972;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#20026;&#20102;&#19979;&#28216;&#35843;&#25972;&#65292;&#25105;&#20204;&#25552;&#20986;&#23384;&#22312;&#20004;&#20010;&#22495;&#24046;&#36317;&#65292;&#21363;&#22522;&#30784;-&#20219;&#21153;&#24046;&#36317;&#21644;&#20219;&#21153;-&#23454;&#20363;&#24046;&#36317;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; PathoTune&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#22810;&#27169;&#24577;&#25552;&#31034;&#24494;&#35843;&#65292;&#39640;&#25928;&#22320;&#23558;&#30149;&#29702;&#29978;&#33267;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#35843;&#25972;&#21040;&#30149;&#29702;&#29305;&#23450;&#20219;&#21153;&#30340;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#35270;&#35273;&#25552;&#31034;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#25991;&#26412;&#25552;&#31034;&#26469;&#35782;&#21035;&#20219;&#21153;&#30456;&#20851;&#29305;&#24449;&#65292;&#20197;&#21450;&#23454;&#20363;&#29305;&#23450;&#30340;&#35270;&#35273;&#25552;&#31034;&#26469;&#32534;&#30721;&#21333;&#20010;&#30149;&#29702;&#22270;&#20687;&#29305;&#24449;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20197;&#34917;&#19969;&#32423;&#21035;&#21644;WSI&#32423;&#21035;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#21333;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16497v1 Announce Type: cross  Abstract: As natural image understanding moves towards the pretrain-finetune era, research in pathology imaging is concurrently evolving. Despite the predominant focus on pretraining pathological foundation models, how to adapt foundation models to downstream tasks is little explored. For downstream adaptation, we propose the existence of two domain gaps, i.e., the Foundation-Task Gap and the Task-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework designed to efficiently adapt pathological or even visual foundation models to pathology-specific tasks via multi-modal prompt tuning. The proposed framework leverages Task-specific Visual Prompts and Task-specific Textual Prompts to identify task-relevant features, along with Instance-specific Visual Prompts for encoding single pathological image features. Results across multiple datasets at both patch-level and WSI-level demonstrate its superior performance over single-modality
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#35760;&#24518;&#32593;&#32476;&#21644;&#23616;&#37096;&#26597;&#35810;&#20989;&#25968;&#65292;&#36825;&#39033;&#24037;&#20316;&#33268;&#21147;&#20110;&#22312;&#36830;&#32493;&#26816;&#27979;&#20013;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#35299;&#20915;&#20102;&#25345;&#32493;&#26816;&#27979;&#20013;&#30340;&#32972;&#26223;&#36140;&#20302;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14797</link><description>&lt;p&gt;
&#36890;&#36807;&#35760;&#24518;&#32593;&#32476;&#22312;&#36830;&#32493;&#26816;&#27979;&#20013;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Preventing Catastrophic Forgetting through Memory Networks in Continuous Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14797
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#35760;&#24518;&#32593;&#32476;&#21644;&#23616;&#37096;&#26597;&#35810;&#20989;&#25968;&#65292;&#36825;&#39033;&#24037;&#20316;&#33268;&#21147;&#20110;&#22312;&#36830;&#32493;&#26816;&#27979;&#20013;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#35299;&#20915;&#20102;&#25345;&#32493;&#26816;&#27979;&#20013;&#30340;&#32972;&#26223;&#36140;&#20302;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#39044;&#35757;&#32451;&#26550;&#26500;&#22312;&#25345;&#32493;&#23545;&#26032;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#26102;&#24456;&#38590;&#20445;&#30041;&#20808;&#21069;&#30340;&#20449;&#24687;&#12290;&#23613;&#31649;&#22312;&#25345;&#32493;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#38024;&#23545;&#22797;&#26434;&#35270;&#35273;&#20219;&#21153;&#65288;&#22914;&#26816;&#27979;&#25110;&#20998;&#21106;&#65289;&#35774;&#35745;&#30340;&#31995;&#32479;&#20173;&#28982;&#38590;&#20197;&#33719;&#24471;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#26816;&#27979;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#20197;&#20351;&#39044;&#35757;&#32451;&#30340;DETR&#39118;&#26684;&#26816;&#27979;&#22120;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#20808;&#21069;&#20219;&#21153;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23616;&#37096;&#26597;&#35810;&#20989;&#25968;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20174;&#35760;&#24518;&#21333;&#20803;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25345;&#32493;&#26816;&#27979;&#20013;&#19968;&#20010;&#31216;&#20026;&#32972;&#26223;&#36140;&#20302;&#30340;&#22522;&#26412;&#25361;&#25112;&#12290;&#24403;&#26469;&#33258;&#20808;&#21069;&#20219;&#21153;&#30340;&#23545;&#35937;&#31867;&#21035;&#22312;&#26410;&#26469;&#20219;&#21153;&#20013;&#37325;&#26032;&#20986;&#29616;&#26102;&#65292;&#21487;&#33021;&#27809;&#26377;&#26631;&#31614;&#65292;&#23548;&#33268;&#23427;&#20204;&#34987;&#38544;&#24335;&#35270;&#20026;&#32972;&#26223;&#12290;&#36825;&#26159;&#25345;&#32493;&#26816;&#27979;&#25110;&#20998;&#21106;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14797v1 Announce Type: cross  Abstract: Modern pre-trained architectures struggle to retain previous information while undergoing continuous fine-tuning on new tasks. Despite notable progress in continual classification, systems designed for complex vision tasks such as detection or segmentation still struggle to attain satisfactory performance. In this work, we introduce a memory-based detection transformer architecture to adapt a pre-trained DETR-style detector to new tasks while preserving knowledge from previous tasks. We propose a novel localized query function for efficient information retrieval from memory units, aiming to minimize forgetting. Furthermore, we identify a fundamental challenge in continual detection referred to as background relegation. This arises when object categories from earlier tasks reappear in future tasks, potentially without labels, leading them to be implicitly treated as background. This is an inevitable issue in continual detection or segme
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#25552;&#31034;&#21040;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#25928;&#26524;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.12326</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#23398;&#20064;&#25552;&#31034;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Removing Undesirable Concepts in Text-to-Image Generative Models with Learnable Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12326
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#25552;&#31034;&#21040;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#25928;&#26524;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#35270;&#35273;&#19978;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20869;&#23481;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26410;&#32463;&#31579;&#36873;&#30340;&#20114;&#32852;&#32593;&#25968;&#25454;&#19978;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#23398;&#20064;&#21644;&#38543;&#21518;&#20256;&#25773;&#19981;&#33391;&#27010;&#24565;&#65288;&#22914;&#21463;&#29256;&#26435;&#20445;&#25252;&#25110;&#19981;&#36947;&#24503;&#20869;&#23481;&#65289;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#25552;&#31034;&#32467;&#21512;&#21040;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#65292;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;&#12290;&#36825;&#21487;&#23398;&#20064;&#25552;&#31034;&#20805;&#24403;&#38468;&#21152;&#20869;&#23384;&#65292;&#23558;&#19981;&#33391;&#27010;&#24565;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#20854;&#20013;&#65292;&#24182;&#20943;&#23569;&#36825;&#20123;&#27010;&#24565;&#23545;&#27169;&#22411;&#21442;&#25968;&#21644;&#30456;&#24212;&#25991;&#26412;&#36755;&#20837;&#30340;&#20381;&#36182;&#12290;&#30001;&#20110;&#36825;&#31181;&#30693;&#35782;&#36716;&#31227;&#21040;&#25552;&#31034;&#20013;&#65292;&#28040;&#38500;&#36825;&#20123;&#19981;&#33391;&#27010;&#24565;&#26356;&#21152;&#31283;&#23450;&#65292;&#24182;&#23545;&#20854;&#20182;&#27010;&#24565;&#24433;&#21709;&#26368;&#23567;&#12290;&#25105;&#20204;&#22312;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12326v1 Announce Type: new  Abstract: Generative models have demonstrated remarkable potential in generating visually impressive content from textual descriptions. However, training these models on unfiltered internet data poses the risk of learning and subsequently propagating undesirable concepts, such as copyrighted or unethical content. In this paper, we propose a novel method to remove undesirable concepts from text-to-image generative models by incorporating a learnable prompt into the cross-attention module. This learnable prompt acts as additional memory to transfer the knowledge of undesirable concepts into it and reduce the dependency of these concepts on the model parameters and corresponding textual inputs. Because of this knowledge transfer into the prompt, erasing these undesirable concepts is more stable and has minimal negative impact on other concepts. We demonstrate the effectiveness of our method on the Stable Diffusion model, showcasing its superiority ov
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#35270;&#39057;&#32534;&#36753;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#21407;&#22987;&#35270;&#39057;&#21644;&#32534;&#36753;&#35270;&#39057;&#30340;&#26102;&#31354;&#33258;&#30456;&#20284;&#24615;&#65292;&#22312;&#20998;&#25968;&#33976;&#39311;&#36807;&#31243;&#20013;&#35299;&#20915;&#20102;&#26032;&#20869;&#23481;&#24341;&#20837;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#32467;&#26500;&#21644;&#36816;&#21160;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12002</link><description>&lt;p&gt;
DreamMotion&#65306;&#29992;&#20110;&#38646;&#26679;&#26412;&#35270;&#39057;&#32534;&#36753;&#30340;&#26102;&#31354;&#33258;&#30456;&#20284;&#24615;&#20998;&#25968;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
DreamMotion: Space-Time Self-Similarity Score Distillation for Zero-Shot Video Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12002
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#35270;&#39057;&#32534;&#36753;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21305;&#37197;&#21407;&#22987;&#35270;&#39057;&#21644;&#32534;&#36753;&#35270;&#39057;&#30340;&#26102;&#31354;&#33258;&#30456;&#20284;&#24615;&#65292;&#22312;&#20998;&#25968;&#33976;&#39311;&#36807;&#31243;&#20013;&#35299;&#20915;&#20102;&#26032;&#20869;&#23481;&#24341;&#20837;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#32467;&#26500;&#21644;&#36816;&#21160;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12002v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22522;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#25193;&#25955;&#24335;&#35270;&#39057;&#32534;&#36753;&#22312;&#22270;&#20687;&#32534;&#36753;&#25991;&#29486;&#20013;&#26174;&#29616;&#20102;&#19968;&#39033;&#29420;&#29305;&#25361;&#25112;&#65306;&#24314;&#31435;&#30495;&#23454;&#19990;&#30028;&#36816;&#21160;&#12290;&#19982;&#29616;&#26377;&#30340;&#35270;&#39057;&#32534;&#36753;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#19987;&#27880;&#20110;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65292;&#20197;&#35268;&#36991;&#26631;&#20934;&#30340;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#65292;&#24182;&#20174;&#24050;&#23637;&#29616;&#33258;&#28982;&#36816;&#21160;&#30340;&#35270;&#39057;&#20013;&#21551;&#21160;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#34429;&#28982;&#35270;&#39057;&#20998;&#25968;&#33976;&#39311;&#21487;&#20197;&#26377;&#25928;&#22320;&#24341;&#20837;&#30446;&#26631;&#25991;&#26412;&#25351;&#31034;&#30340;&#26032;&#20869;&#23481;&#65292;&#20294;&#20063;&#21487;&#33021;&#23548;&#33268;&#26174;&#33879;&#30340;&#32467;&#26500;&#21644;&#36816;&#21160;&#20559;&#24046;&#12290;&#20026;&#20102;&#25269;&#28040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#20998;&#25968;&#33976;&#39311;&#36807;&#31243;&#20013;&#21305;&#37197;&#21407;&#22987;&#35270;&#39057;&#21644;&#32534;&#36753;&#35270;&#39057;&#30340;&#26102;&#31354;&#33258;&#30456;&#20284;&#24615;&#12290;&#30001;&#20110;&#20351;&#29992;&#20102;&#20998;&#25968;&#33976;&#39311;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#21487;&#24212;&#29992;&#20110;&#32423;&#32852;&#21644;&#38750;&#32423;&#32852;&#35270;&#39057;&#25193;&#25955;&#26694;&#26550;&#12290;&#36890;&#36807;&#19982;&#39046;&#20808;&#26041;&#27861;&#30340;&#24191;&#27867;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#35270;&#39057;&#32534;&#36753;&#20013;&#30340;&#21331;&#36234;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12002v1 Announce Type: cross  Abstract: Text-driven diffusion-based video editing presents a unique challenge not encountered in image editing literature: establishing real-world motion. Unlike existing video editing approaches, here we focus on score distillation sampling to circumvent the standard reverse diffusion process and initiate optimization from videos that already exhibit natural motion. Our analysis reveals that while video score distillation can effectively introduce new content indicated by target text, it can also cause significant structure and motion deviation. To counteract this, we propose to match space-time self-similarities of the original video and the edited video during the score distillation. Thanks to the use of score distillation, our approach is model-agnostic, which can be applied for both cascaded and non-cascaded video diffusion frameworks. Through extensive comparisons with leading methods, our approach demonstrates its superiority in alterin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11299</link><description>&lt;p&gt;
SQ-LLaVA&#65306;&#33258;&#38382;&#33258;&#31572;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#22312;&#32463;&#36807;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#21518;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26174;&#30528;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#40511;&#27807;&#25104;&#20026;&#25972;&#20010;&#32593;&#32476;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#25913;&#21892;&#36328;&#27169;&#24577;&#23545;&#40784;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#32771;&#34385;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#35270;&#35273;&#20219;&#21153;&#33539;&#22260;&#30340;&#26356;&#22810;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#65292;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#29992;&#20110;&#38382;&#31572;&#65292;&#20294;&#36825;&#31181;&#25805;&#20316;&#25104;&#26412;&#36739;&#39640;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#21253;&#21547;&#22823;&#37327;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20294;&#36825;&#19968;&#26041;&#38754;&#19968;&#30452;&#40092;&#26377;&#20154;&#25506;&#32034;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#21033;&#29992;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20869;&#37096;&#34987;&#24573;&#35270;&#30340;&#19978;&#19979;&#25991;&#65292;&#35757;&#32451;&#27169;&#22411;&#33258;&#25105;&#35757;&#32451;'&#23398;&#20064;'&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65306;&#33258;&#38382;&#33258;&#31572;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#21161;&#25163;&#12290;SQ-LLaVA&#22312;&#29983;&#25104;&#28789;&#27963;&#19988;&#26377;&#24847;&#20041;&#30340;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11299v1 Announce Type: cross  Abstract: Recent advancements in the vision-language model have shown notable generalization in vision-language tasks after visual instruction tuning. However, bridging the gap between the pre-trained vision encoder and the large language models becomes the whole network's bottleneck. To improve cross-modality alignment, existing works usually consider more visual instruction data covering a broader range of vision tasks to fine-tune the model for question-answering, which are costly to obtain. However, the image contains rich contextual information that has been largely under-explored. This paper first attempts to harness this overlooked context within visual instruction data, training the model to self-supervised `learning' how to ask high-quality questions. In this way, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible and meaningful image-
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#20840;&#38754;&#30740;&#31350;&#20219;&#21153;&#38590;&#24230;&#65292;&#21457;&#29616;&#36739;&#26089;&#26102;&#38388;&#27493;&#38271;&#30340;&#21435;&#22122;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21435;&#22122;&#20219;&#21153;&#38590;&#24230;&#30340;&#28176;&#36827;&#24335;&#35838;&#31243;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10348</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#22122;&#20219;&#21153;&#38590;&#24230;&#30340;&#28176;&#36827;&#24335;&#35838;&#31243;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Denoising Task Difficulty-based Curriculum for Training Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10348
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#20840;&#38754;&#30740;&#31350;&#20219;&#21153;&#38590;&#24230;&#65292;&#21457;&#29616;&#36739;&#26089;&#26102;&#38388;&#27493;&#38271;&#30340;&#21435;&#22122;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21435;&#22122;&#20219;&#21153;&#38590;&#24230;&#30340;&#28176;&#36827;&#24335;&#35838;&#31243;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#25104;&#20026;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#23545;&#21508;&#20010;&#26102;&#38388;&#27493;&#38271;&#21644;&#22122;&#22768;&#27700;&#24179;&#20043;&#38388;&#30340;&#21435;&#22122;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20851;&#20110;&#21435;&#22122;&#20219;&#21153;&#30340;&#30456;&#23545;&#38590;&#24230;&#20173;&#23384;&#22312;&#20105;&#35758;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#20219;&#21153;&#38590;&#24230;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#25910;&#25947;&#34892;&#20026;&#21644;&#26102;&#38388;&#27493;&#38271;&#38388;&#36830;&#32493;&#27010;&#29575;&#20998;&#24067;&#30340;&#30456;&#23545;&#29109;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#26174;&#31034;&#65292;&#36739;&#26089;&#26102;&#38388;&#27493;&#38271;&#30340;&#21435;&#22122;&#23384;&#22312;&#25910;&#25947;&#32531;&#24930;&#21644;&#36739;&#39640;&#30340;&#30456;&#23545;&#29109;&#65292;&#34920;&#26126;&#22312;&#36825;&#20123;&#36739;&#20302;&#26102;&#38388;&#27493;&#38271;&#19978;&#20219;&#21153;&#38590;&#24230;&#22686;&#21152;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#30001;&#26131;&#21040;&#38590;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#20511;&#37492;&#28176;&#36827;&#24335;&#23398;&#20064;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10348v1 Announce Type: cross  Abstract: Diffusion-based generative models have emerged as powerful tools in the realm of generative modeling. Despite extensive research on denoising across various timesteps and noise levels, a conflict persists regarding the relative difficulties of the denoising tasks. While various studies argue that lower timesteps present more challenging tasks, others contend that higher timesteps are more difficult. To address this conflict, our study undertakes a comprehensive examination of task difficulties, focusing on convergence behavior and changes in relative entropy between consecutive probability distributions across timesteps. Our observational study reveals that denoising at earlier timesteps poses challenges characterized by slower convergence and higher relative entropy, indicating increased task difficulty at these lower timesteps. Building on these observations, we introduce an easy-to-hard learning scheme, drawing from curriculum learn
&lt;/p&gt;</description></item><item><title>eCLIP&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#19987;&#23478;&#27880;&#37322;&#21644;&#28151;&#21512;&#22686;&#24378;&#26469;&#24212;&#23545;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#21644;&#27169;&#24577;&#24046;&#36317;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;</title><link>https://arxiv.org/abs/2403.10153</link><description>&lt;p&gt;
&#25913;&#36827;&#21307;&#23398;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#19982;&#19987;&#23478;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
Improving Medical Multi-modal Contrastive Learning with Expert Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10153
&lt;/p&gt;
&lt;p&gt;
eCLIP&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#19987;&#23478;&#27880;&#37322;&#21644;&#28151;&#21512;&#22686;&#24378;&#26469;&#24212;&#23545;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#21644;&#27169;&#24577;&#24046;&#36317;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;CLIP&#27169;&#22411;&#8212;&#8212;eCLIP&#65292;&#23427;&#38598;&#25104;&#20102;&#25918;&#23556;&#31185;&#21307;&#29983;&#30524;&#29699;&#27880;&#35270;&#28909;&#22270;&#24418;&#24335;&#30340;&#19987;&#23478;&#27880;&#37322;&#12290;&#23427;&#35299;&#20915;&#20102;&#23545;&#27604;&#22810;&#27169;&#24577;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#25968;&#25454;&#31232;&#32570;&#21644;&#8220;&#27169;&#24577;&#24046;&#36317;&#8221;&#8212;&#8212;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#23384;&#22312;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#38477;&#20302;&#20102;&#34920;&#31034;&#30340;&#36136;&#37327;&#24182;&#38459;&#30861;&#20102;&#36328;&#27169;&#24577;&#20114;&#25805;&#20316;&#24615;&#12290;eCLIP&#38598;&#25104;&#20102;&#19968;&#20010;&#28909;&#22270;&#22788;&#29702;&#22120;&#65292;&#24182;&#21033;&#29992;&#28151;&#21512;&#22686;&#24378;&#26469;&#26377;&#25928;&#21033;&#29992;&#31232;&#32570;&#30340;&#19987;&#23478;&#27880;&#37322;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;eCLIP&#35774;&#35745;&#20026;&#36890;&#29992;&#30340;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#24418;&#24335;&#30340;CLIP&#21464;&#20307;&#65292;&#26080;&#38656;&#20462;&#25913;&#26680;&#24515;&#26550;&#26500;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#20219;&#21153;&#30340;&#35814;&#32454;&#35780;&#20272;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#25512;&#26029;&#12289;&#32447;&#24615;&#25506;&#38024;&#12289;&#36328;&#27169;&#24577;&#26816;&#32034;&#20197;&#21450;&#20351;&#29992;&#20923;&#32467;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#65292;eCLIP&#23637;&#31034;&#20102;&#20854;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10153v1 Announce Type: cross  Abstract: We introduce eCLIP, an enhanced version of the CLIP model that integrates expert annotations in the form of radiologist eye-gaze heatmaps. It tackles key challenges in contrastive multi-modal medical imaging analysis, notably data scarcity and the "modality gap" -- a significant disparity between image and text embeddings that diminishes the quality of representations and hampers cross-modal interoperability. eCLIP integrates a heatmap processor and leverages mixup augmentation to efficiently utilize the scarce expert annotations, thus boosting the model's learning effectiveness. eCLIP is designed to be generally applicable to any variant of CLIP without requiring any modifications of the core architecture. Through detailed evaluations across several tasks, including zero-shot inference, linear probing, cross-modal retrieval, and Retrieval Augmented Generation (RAG) of radiology reports using a frozen Large Language Model, eCLIP showca
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#26377;&#26465;&#20214;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#26080;&#30417;&#30563;&#20248;&#21270;&#35270;&#32593;&#33180;&#20551;&#20307;&#21050;&#28608;&#65292;&#25552;&#39640;&#20102;&#30005;&#26497;&#38453;&#21015;&#30340;&#21050;&#28608;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.04884</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#26465;&#20214;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#35270;&#32593;&#33180;&#20551;&#20307;&#21050;&#28608;
&lt;/p&gt;
&lt;p&gt;
Optimizing Retinal Prosthetic Stimuli with Conditional Invertible Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04884
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26377;&#26465;&#20214;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#26080;&#30417;&#30563;&#20248;&#21270;&#35270;&#32593;&#33180;&#20551;&#20307;&#21050;&#28608;&#65292;&#25552;&#39640;&#20102;&#30005;&#26497;&#38453;&#21015;&#30340;&#21050;&#28608;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#26893;&#20837;&#30340;&#35270;&#32593;&#33180;&#20551;&#20307;&#20026;&#36890;&#36807;&#32469;&#36807;&#35270;&#32593;&#33180;&#20013;&#25439;&#22351;&#30340;&#20809;&#24863;&#21463;&#32454;&#32990;&#24182;&#30452;&#25509;&#21050;&#28608;&#21097;&#20313;&#21151;&#33021;&#24615;&#35270;&#32593;&#33180;&#32454;&#32990;&#26469;&#24674;&#22797;&#37096;&#20998;&#35270;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25668;&#20687;&#22836;&#21644;&#35270;&#32593;&#33180;&#32454;&#32990;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#36755;&#36890;&#24120;&#21463;&#38480;&#20110;&#30005;&#26497;&#38453;&#21015;&#30340;&#20302;&#20998;&#36776;&#29575;&#21644;&#23545;&#19981;&#21516;&#33410;&#32454;&#32990;&#31867;&#22411;&#30340;&#29305;&#24322;&#24615;&#19981;&#36275;&#65292;&#23548;&#33268;&#21050;&#28608;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#26377;&#26465;&#20214;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20248;&#21270;&#35270;&#32593;&#33180;&#20551;&#20307;&#21050;&#28608;&#12290;&#36825;&#20123;&#32593;&#32476;&#30340;&#21487;&#36870;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#23427;&#20204;&#29992;&#20316;&#35270;&#35273;&#31995;&#32479;&#30340;&#35745;&#31639;&#27169;&#22411;&#30340;&#26367;&#20195;&#65292;&#24182;&#23558;&#36755;&#20837;&#25668;&#20687;&#22836;&#20449;&#21495;&#32534;&#30721;&#20026;&#30005;&#26497;&#38453;&#21015;&#19978;&#20248;&#21270;&#30340;&#30005;&#21050;&#28608;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#22914;&#31616;&#21333;&#30340;&#38477;&#37319;&#26679;&#12289;&#32447;&#24615;&#27169;&#22411;&#21644;&#21069;&#39304;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04884v1 Announce Type: cross  Abstract: Implantable retinal prostheses offer a promising solution to restore partial vision by circumventing damaged photoreceptor cells in the retina and directly stimulating the remaining functional retinal cells. However, the information transmission between the camera and retinal cells is often limited by the low resolution of the electrode array and the lack of specificity for different ganglion cell types, resulting in suboptimal stimulations. In this work, we propose to utilize normalizing flow-based conditional invertible neural networks to optimize retinal implant stimulation in an unsupervised manner. The invertibility of these networks allows us to use them as a surrogate for the computational model of the visual system, while also encoding input camera signals into optimized electrical stimuli on the electrode array. Compared to other methods, such as trivial downsampling, linear models, and feed-forward convolutional neural networ
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoAT&#30340;Chain-of-Action-Thought&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#20808;&#21069;&#21160;&#20316;&#25551;&#36848;&#12289;&#24403;&#21069;&#23631;&#24149;&#24773;&#20917;&#20197;&#21450;&#26410;&#26469;&#21160;&#20316;&#24605;&#32771;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26234;&#33021;&#25163;&#26426;GUI&#20195;&#29702;&#30340;&#20219;&#21153;&#25191;&#34892;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.02713</link><description>&lt;p&gt;
Android&#22312;&#21160;&#29289;&#22253;&#20013;: GUI&#20195;&#29702;&#30340;&#21160;&#20316;&#24605;&#32500;&#38142;
&lt;/p&gt;
&lt;p&gt;
Android in the Zoo: Chain-of-Action-Thought for GUI Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoAT&#30340;Chain-of-Action-Thought&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#20808;&#21069;&#21160;&#20316;&#25551;&#36848;&#12289;&#24403;&#21069;&#23631;&#24149;&#24773;&#20917;&#20197;&#21450;&#26410;&#26469;&#21160;&#20316;&#24605;&#32771;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26234;&#33021;&#25163;&#26426;GUI&#20195;&#29702;&#30340;&#20219;&#21153;&#25191;&#34892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23548;&#33268;&#26234;&#33021;&#25163;&#26426;&#19978;&#30340;&#22823;&#37327;&#33258;&#20027;GUI&#20195;&#29702;&#28608;&#22686;&#65292;&#36825;&#20123;&#20195;&#29702;&#36890;&#36807;&#39044;&#27979;API&#30340;&#19968;&#31995;&#21015;&#21160;&#20316;&#26469;&#23436;&#25104;&#30001;&#33258;&#28982;&#35821;&#35328;&#35302;&#21457;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#35813;&#20219;&#21153;&#39640;&#24230;&#20381;&#36182;&#20110;&#36807;&#21435;&#30340;&#21160;&#20316;&#21644;&#35270;&#35273;&#35266;&#23519;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#24456;&#23569;&#32771;&#34385;&#20013;&#38388;&#25130;&#22270;&#21644;&#23631;&#24149;&#25805;&#20316;&#20256;&#36882;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#20316;&#24605;&#32500;&#38142;&#65288;CoAT&#65289;&#65292;&#23427;&#32771;&#34385;&#20102;&#20808;&#21069;&#21160;&#20316;&#30340;&#25551;&#36848;&#12289;&#24403;&#21069;&#23631;&#24149;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#20998;&#26512;&#24212;&#24403;&#25191;&#34892;&#30340;&#21160;&#20316;&#20197;&#21450;&#36873;&#25321;&#30340;&#21160;&#20316;&#24102;&#26469;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20351;&#29992;&#29616;&#25104;LLM&#36827;&#34892;&#38646;&#27425;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;CoAT&#30456;&#27604;&#20110;&#26631;&#20934;&#19978;&#19979;&#25991;&#24314;&#27169;&#26174;&#33879;&#25552;&#39640;&#20102;&#30446;&#26631;&#30340;&#23436;&#25104;&#24773;&#20917;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20419;&#36827;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Android-In-The-Zoo&#65288;AitZ&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;18,643&#20010;&#23631;&#24149;&#21160;&#20316;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02713v1 Announce Type: new  Abstract: Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API. Even though the task highly relies on past actions and visual observations, existing studies typical consider little semantic information carried out by intermediate screenshots and screen operations. To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action. We demonstrate that, in a zero-shot setting upon an off-the-shell LLM, CoAT significantly improves the goal progress compared to standard context modeling. To further facilitate the research in this line, we construct a benchmark Android-In-The-Zoo (AitZ), which contains 18,643 screen-action pa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#27010;&#29575;&#27010;&#24565;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;ProTIP&#30340;&#39640;&#25928;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#20854;&#32479;&#35745;&#20445;&#35777;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#36807;&#31243;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#23545;&#25239;&#24615;&#26679;&#26412;&#21028;&#26029;&#22256;&#38590;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.15429</link><description>&lt;p&gt;
ProTIP&#65306;&#38024;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#25239;&#38543;&#26426;&#25200;&#21160;&#30340;&#27010;&#29575;&#40065;&#26834;&#24615;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#27010;&#29575;&#27010;&#24565;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;ProTIP&#30340;&#39640;&#25928;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#20854;&#32479;&#35745;&#20445;&#35777;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#36807;&#31243;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#23545;&#25239;&#24615;&#26679;&#26412;&#21028;&#26029;&#22256;&#38590;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#23637;&#29616;&#20102;&#22312;&#31616;&#21333;&#25991;&#26412;&#25551;&#36848;&#22522;&#30784;&#19978;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#21360;&#35937;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19982;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#19968;&#26679;&#65292;DMs&#23384;&#22312;&#32570;&#20047;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#35780;&#20272;T2I DMs&#30340;&#40065;&#26834;&#24615;&#26102;&#65292;&#23384;&#22312;&#20197;&#20108;&#20803;&#25110;&#26368;&#22351;&#24773;&#20917;&#38382;&#39064;&#35299;&#26041;&#38754;&#30340;&#23581;&#35797;&#65292;&#20294;&#26080;&#27861;&#22238;&#31572;&#27169;&#22411;&#22312;&#23384;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#65288;AE&#65289;&#26102;&#30340;&#24635;&#20307;&#40065;&#26834;&#24615;&#22914;&#20309;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#24341;&#20837;&#20102;T2I DMs&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#65307;&#28982;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;ProTIP&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#35780;&#20272;&#12290;&#20027;&#35201;&#25361;&#25112;&#28304;&#33258;&#65306;i&#65289;&#29983;&#25104;&#36807;&#31243;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65307;&#21644;ii&#65289;&#30830;&#23450;&#25200;&#21160;&#36755;&#20837;&#26159;&#21542;&#20026;AE&#28041;&#21450;&#27604;&#36739;&#20004;&#20010;&#36755;&#20986;&#20998;&#24067;&#65292;&#36825;&#19982;&#20854;&#20182;DL&#20219;&#21153;&#65288;&#22914;&#20998;&#31867;&#65289;&#19981;&#21516;&#65292;&#20854;&#20013;AE&#26159;&#22312;&#26631;&#31614;&#38169;&#35823;&#39044;&#27979;&#26102;&#34987;&#35782;&#21035;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15429v1 Announce Type: cross  Abstract: Text-to-Image (T2I) Diffusion Models (DMs) have shown impressive abilities in generating high-quality images based on simple text descriptions. However, as is common with many Deep Learning (DL) models, DMs are subject to a lack of robustness. While there are attempts to evaluate the robustness of T2I DMs as a binary or worst-case problem, they cannot answer how robust in general the model is whenever an adversarial example (AE) can be found. In this study, we first introduce a probabilistic notion of T2I DMs' robustness; and then establish an efficient framework, ProTIP, to evaluate it with statistical guarantees. The main challenges stem from: i) the high computational cost of the generation process; and ii) determining if a perturbed input is an AE involves comparing two output distributions, which is fundamentally harder compared to other DL tasks like classification where an AE is identified upon misprediction of labels. To tackle
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#38454;&#23545;&#27604;&#23398;&#20064;&#65288;MCL&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#25233;&#21046;&#38382;&#39064;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#23398;&#20064;&#20840;&#38754;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.11816</link><description>&lt;p&gt;
&#36991;&#20813;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#25233;&#21046;&#65306;&#23398;&#20064;&#20197;&#21069;&#26410;&#26366;&#23398;&#21040;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11816
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#38454;&#23545;&#27604;&#23398;&#20064;&#65288;MCL&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#25233;&#21046;&#38382;&#39064;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#23398;&#20064;&#20840;&#38754;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#33719;&#21462;&#39640;&#36136;&#37327;&#34920;&#31034;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22312;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;&#65288;&#22914;SimCLR&#12289;CLIP&#20013;&#65289;&#20013;&#21457;&#29616;&#20102;&#29305;&#24449;&#25233;&#21046;&#65306;&#22312;&#21333;&#20010;&#31471;&#21040;&#31471;&#35757;&#32451;&#38454;&#27573;&#65292;&#23545;&#27604;&#27169;&#22411;&#20165;&#25429;&#33719;&#23545;&#27604;&#35266;&#28857;&#20043;&#38388;&#30340;&#19968;&#37096;&#20998;&#20849;&#20139;&#20449;&#24687;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#28508;&#22312;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#20855;&#26377;&#29305;&#24449;&#25233;&#21046;&#65292;&#23545;&#27604;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#23398;&#20064;&#36275;&#22815;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#20943;&#36731;&#29305;&#24449;&#25233;&#21046;&#38382;&#39064;&#24182;&#30830;&#20445;&#23545;&#27604;&#27169;&#22411;&#23398;&#20064;&#20840;&#38754;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#23545;&#27604;&#23398;&#20064;&#65288;MCL&#65289;&#26694;&#26550;&#12290;&#19982;&#36890;&#24120;&#20250;&#23548;&#33268;&#29305;&#24449;&#25233;&#21046;&#30340;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;&#19981;&#21516;&#65292;MCL&#36880;&#28176;&#23398;&#20064;&#20197;&#21069;&#26410;&#25506;&#32034;&#36807;&#30340;&#26032;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#25345;&#24050;&#32463;&#23398;&#21040;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11816v1 Announce Type: cross  Abstract: Self-Supervised contrastive learning has emerged as a powerful method for obtaining high-quality representations from unlabeled data. However, feature suppression has recently been identified in standard contrastive learning ($e.g.$, SimCLR, CLIP): in a single end-to-end training stage, the contrastive model captures only parts of the shared information across contrasting views, while ignore the other potentially useful information. With feature suppression, contrastive models often fail to learn sufficient representations capable for various downstream tasks. To mitigate the feature suppression problem and ensure the contrastive model to learn comprehensive representations, we develop a novel Multistage Contrastive Learning (MCL) framework. Unlike standard contrastive learning that often result in feature suppression, MCL progressively learn new features that have not been explored in the previous stage, while maintaining the well-lea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SpecFormer&#65292;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#22855;&#24322;&#20540;&#24809;&#32602;&#26469;&#22686;&#24378;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38887;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;Lipschitz&#36793;&#30028;&#21644;&#26368;&#22823;&#22855;&#24322;&#20540;&#24809;&#32602;&#26041;&#27861;&#65288;MSVP&#65289;&#65292;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;&#27880;&#24847;&#21147;&#26435;&#37325;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.03317</link><description>&lt;p&gt;
SpecFormer&#65306;&#36890;&#36807;&#26368;&#22823;&#22855;&#24322;&#20540;&#24809;&#32602;&#26469;&#20445;&#25252;&#35270;&#35273;Transformer&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
SpecFormer: Guarding Vision Transformer Robustness via Maximum Singular Value Penalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SpecFormer&#65292;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#22855;&#24322;&#20540;&#24809;&#32602;&#26469;&#22686;&#24378;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38887;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;Lipschitz&#36793;&#30028;&#21644;&#26368;&#22823;&#22855;&#24322;&#20540;&#24809;&#32602;&#26041;&#27861;&#65288;MSVP&#65289;&#65292;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;&#27880;&#24847;&#21147;&#26435;&#37325;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#22240;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#25104;&#20026;&#24191;&#27867;&#20351;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#39318;&#36873;&#12290;&#28982;&#32780;&#65292;&#20854;&#24191;&#27867;&#24212;&#29992;&#24341;&#36215;&#20102;&#23545;&#38754;&#23545;&#24694;&#24847;&#25915;&#20987;&#26102;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#32463;&#39564;&#35843;&#25972;&#65292;&#32570;&#20047;&#26126;&#30830;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;SpecFormer&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#35813;&#26041;&#27861;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22686;&#24378;ViTs&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38887;&#24615;&#65292;&#24182;&#24471;&#21040;&#20102;&#20180;&#32454;&#25512;&#23548;&#30340;&#29702;&#35770;&#20445;&#35777;&#30340;&#25903;&#25345;&#12290;&#25105;&#20204;&#20026;&#33258;&#27880;&#24847;&#23618;&#24314;&#31435;&#20102;&#26412;&#22320;Lipschitz&#36793;&#30028;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26368;&#22823;&#22855;&#24322;&#20540;&#24809;&#32602;&#65288;MSVP&#65289;&#65292;&#20197;&#31934;&#30830;&#25511;&#21046;&#36825;&#20123;&#36793;&#30028;&#12290;&#25105;&#20204;&#20351;&#29992;&#24130;&#36845;&#20195;&#26041;&#27861;&#23558;MSVP&#26080;&#32541;&#38598;&#25104;&#21040;ViTs&#30340;&#27880;&#24847;&#21147;&#23618;&#20013;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#20462;&#25913;&#21518;&#30340;&#27169;&#22411;SpecFormer&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;&#27880;&#24847;&#21147;&#26435;&#37325;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#65292;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have gained prominence as a preferred choice for a wide range of computer vision tasks due to their exceptional performance. However, their widespread adoption has raised concerns about security in the face of malicious attacks. Most existing methods rely on empirical adjustments during the training process, lacking a clear theoretical foundation. In this study, we address this gap by introducing SpecFormer, specifically designed to enhance ViTs' resilience against adversarial attacks, with support from carefully derived theoretical guarantees. We establish local Lipschitz bounds for the self-attention layer and introduce a novel approach, Maximum Singular Value Penalization (MSVP), to attain precise control over these bounds. We seamlessly integrate MSVP into ViTs' attention layers, using the power iteration method for enhanced computational efficiency. The modified model, SpecFormer, effectively reduces the spectral norms of attention weight matrices, there
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;DeCoF&#26159;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#28040;&#38500;&#31354;&#38388;&#20266;&#24433;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02085</link><description>&lt;p&gt;
DeCoF:&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#36827;&#34892;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DeCoF: Generated Video Detection via Frame Consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02085
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;DeCoF&#26159;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#28040;&#38500;&#31354;&#38388;&#20266;&#24433;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#20135;&#29983;&#30340;&#35270;&#39057;&#36136;&#37327;&#19981;&#26029;&#25552;&#39640;&#65292;&#36825;&#23548;&#33268;&#31038;&#20250;&#38754;&#20020;&#26032;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#20351;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#25104;&#20026;&#32039;&#36843;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;&#20026;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#21512;&#20316;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#26126;&#30830;&#29992;&#20110;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#20026;&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#20197;&#35780;&#20272;&#21644;&#25913;&#36827;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#31934;&#24515;&#35774;&#35745;&#30340;&#25506;&#27979;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#20266;&#24433;&#22312;&#24320;&#21457;&#29983;&#25104;&#35270;&#39057;&#30340;&#36890;&#29992;&#21644;&#31283;&#20581;&#26816;&#27979;&#22120;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#35270;&#39057;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26816;&#27979;&#27169;&#22411;&#65288;DeCoF&#65289;&#65292;&#23427;&#28040;&#38500;&#20102;&#31354;&#38388;&#20266;&#24433;&#22312;&#36890;&#29992;&#29305;&#24449;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;DeCoF&#22312;&#26816;&#27979;&#26410;&#35265;&#36807;&#30340;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#30340;&#35270;&#39057;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#19988;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The escalating quality of video generated by advanced video generation methods leads to new security challenges in society, which makes generated video detection an urgent research priority.To foster collaborative research in this area, we construct the first open-source dataset explicitly for generated video detection, providing a valuable resource for the community to benchmark and improve detection methodologies. Through a series of carefully designed probe experiments, our study explores the significance of temporal and spatial artifacts in developing general and robust detectors for generated video. Based on the principle of video frame consistency, we introduce a simple yet effective detection model (DeCoF) that eliminates the impact of spatial artifacts during generalizing feature learning. Our extensive experiments demonstrate the efficacy of DeCoF in detecting videos produced by unseen video generation models and confirm its powerful generalization capabilities across several 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24212;&#29992;&#28145;&#24230;&#32858;&#31867;&#31639;&#27861;&#23545;&#25968;&#25454;&#31435;&#26041;&#20307;&#20687;&#32032;&#30340;&#20809;&#35889;&#23646;&#24615;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#21487;&#20197;&#23454;&#29616;&#25968;&#25454;&#31435;&#26041;&#20307;&#30340;&#22270;&#20687;&#20998;&#21106;&#21644;&#32479;&#35745;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2401.17695</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#20809;&#35889;&#32858;&#31867;&#23454;&#29616;&#25968;&#25454;&#31435;&#26041;&#20307;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Datacube segmentation via Deep Spectral Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17695
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24212;&#29992;&#28145;&#24230;&#32858;&#31867;&#31639;&#27861;&#23545;&#25968;&#25454;&#31435;&#26041;&#20307;&#20687;&#32032;&#30340;&#20809;&#35889;&#23646;&#24615;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#21487;&#20197;&#23454;&#29616;&#25968;&#25454;&#31435;&#26041;&#20307;&#30340;&#22270;&#20687;&#20998;&#21106;&#21644;&#32479;&#35745;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#23637;&#35270;&#35273;&#25216;&#26415;&#22312;&#29289;&#29702;&#23398;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#28982;&#32780;&#65292;&#30001;&#27492;&#31867;&#20998;&#26512;&#20135;&#29983;&#30340;&#25968;&#25454;&#31435;&#26041;&#20307;&#22312;&#35299;&#37322;&#19978;&#24448;&#24448;&#20855;&#26377;&#25361;&#25112;&#65292;&#22240;&#20026;&#24456;&#38590;&#20174;&#32452;&#25104;&#25968;&#25454;&#31435;&#26041;&#20307;&#30340;&#20809;&#35889;&#20013;&#36776;&#21035;&#20986;&#30456;&#20851;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#31435;&#26041;&#20307;&#20809;&#35889;&#30340;&#24040;&#22823;&#32500;&#24230;&#23545;&#20110;&#32479;&#35745;&#35299;&#37322;&#26469;&#35828;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65307;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#21253;&#21547;&#20102;&#22823;&#37327;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#21033;&#29992;&#65292;&#20197;&#25551;&#32472;&#20986;&#25152;&#30740;&#31350;&#26696;&#20363;&#30340;&#19968;&#20123;&#22522;&#26412;&#29305;&#24615;&#65292;&#20363;&#22914;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#36866;&#24403;&#23450;&#20041;&#30340;&#20302;&#32500;&#23884;&#20837;&#31354;&#38388;&#20013;&#23545;&#25968;&#25454;&#31435;&#26041;&#20307;&#20809;&#35889;&#36827;&#34892;&#65288;&#28145;&#24230;&#65289;&#32858;&#31867;&#26469;&#33719;&#24471;&#22270;&#20687;&#20998;&#21106;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#32534;&#30721;&#31354;&#38388;&#20013;&#24212;&#29992;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#23545;&#25968;&#25454;&#31435;&#26041;&#20307;&#20687;&#32032;&#30340;&#20809;&#35889;&#23646;&#24615;&#36827;&#34892;&#28145;&#24230;&#32858;&#31867;&#12290;&#36890;&#36807;&#19987;&#38376;&#35757;&#32451;&#30340;&#32479;&#35745;&#32500;&#24230;&#32553;&#20943;&#22120;&#36827;&#34892;&#32479;&#35745;&#32500;&#24230;&#32553;&#20943;
&lt;/p&gt;
&lt;p&gt;
Extended Vision techniques are ubiquitous in physics. However, the data cubes steaming from such analysis often pose a challenge in their interpretation, due to the intrinsic difficulty in discerning the relevant information from the spectra composing the data cube.   Furthermore, the huge dimensionality of data cube spectra poses a complex task in its statistical interpretation; nevertheless, this complexity contains a massive amount of statistical information that can be exploited in an unsupervised manner to outline some essential properties of the case study at hand, e.g.~it is possible to obtain an image segmentation via (deep) clustering of data-cube's spectra, performed in a suitably defined low-dimensional embedding space.   To tackle this topic, we explore the possibility of applying unsupervised clustering methods in encoded space, i.e. perform deep clustering on the spectral properties of datacube pixels. A statistical dimensional reduction is performed by an ad hoc trained 
&lt;/p&gt;</description></item><item><title>LatentEditor &#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#23454;&#29616;&#23545;&#31070;&#32463;&#22330;&#36827;&#34892;&#31934;&#30830;&#21644;&#23616;&#37096;&#21463;&#25511;&#32534;&#36753;&#65292;&#23558;&#30495;&#23454;&#22330;&#26223;&#23884;&#20837;&#28508;&#22312;&#31354;&#38388;&#65292;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#26356;&#20855;&#36866;&#24212;&#24615;&#30340;&#32534;&#36753;NeRF&#39592;&#24178;&#12290;&#24341;&#20837;&#20102;&#22686;&#37327;&#20998;&#25968;&#21644;&#20687;&#32032;&#32423;&#35780;&#20998;&#26041;&#27861;&#20197;&#25552;&#39640;&#32534;&#36753;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2312.09313</link><description>&lt;p&gt;
LatentEditor: &#25991;&#26412;&#39537;&#21160;&#30340;&#19977;&#32500;&#22330;&#26223;&#23616;&#37096;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
LatentEditor: Text Driven Local Editing of 3D Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09313
&lt;/p&gt;
&lt;p&gt;
LatentEditor &#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#23454;&#29616;&#23545;&#31070;&#32463;&#22330;&#36827;&#34892;&#31934;&#30830;&#21644;&#23616;&#37096;&#21463;&#25511;&#32534;&#36753;&#65292;&#23558;&#30495;&#23454;&#22330;&#26223;&#23884;&#20837;&#28508;&#22312;&#31354;&#38388;&#65292;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#26356;&#20855;&#36866;&#24212;&#24615;&#30340;&#32534;&#36753;NeRF&#39592;&#24178;&#12290;&#24341;&#20837;&#20102;&#22686;&#37327;&#20998;&#25968;&#21644;&#20687;&#32032;&#32423;&#35780;&#20998;&#26041;&#27861;&#20197;&#25552;&#39640;&#32534;&#36753;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#22330;&#22312;&#35270;&#22270;&#21512;&#25104;&#21644;&#22330;&#26223;&#37325;&#24314;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#38544;&#21547;&#22320;&#20174;&#22810;&#35270;&#22270;&#36755;&#20837;&#32534;&#30721;&#20960;&#20309;&#21644;&#32441;&#29702;&#20449;&#24687;&#65292;&#32534;&#36753;&#23427;&#20204;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;\textsc{LatentEditor}&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#36171;&#20104;&#29992;&#25143;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#25191;&#34892;&#31070;&#32463;&#22330;&#30340;&#31934;&#30830;&#21644;&#23616;&#37096;&#21463;&#25511;&#32534;&#36753;&#30340;&#33021;&#21147;&#12290;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#23884;&#20837;&#28508;&#22312;&#31354;&#38388;&#65292;&#20174;&#32780;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#23545;NeRF&#39592;&#24178;&#36827;&#34892;&#26356;&#24555;&#26356;&#20855;&#36866;&#24212;&#24615;&#30340;&#32534;&#36753;&#12290;&#20026;&#20102;&#22686;&#24378;&#32534;&#36753;&#31934;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22686;&#37327;&#20998;&#25968;&#26469;&#35745;&#31639;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;2D&#25513;&#30721;&#65292;&#20316;&#20026;&#23616;&#37096;&#20462;&#25913;&#30340;&#25351;&#21335;&#65292;&#21516;&#26102;&#20445;&#30041;&#19981;&#30456;&#20851;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#20687;&#32032;&#32423;&#35780;&#20998;&#26041;&#27861;&#21033;&#29992;&#20102;InstructPix2Pix (IP2P)&#30340;&#33021;&#21147;&#65292;&#20197;&#36776;&#21035; IP2 &#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09313v3 Announce Type: replace-cross  Abstract: While neural fields have made significant strides in view synthesis and scene reconstruction, editing them poses a formidable challenge due to their implicit encoding of geometry and texture information from multi-view inputs. In this paper, we introduce \textsc{LatentEditor}, an innovative framework designed to empower users with the ability to perform precise and locally controlled editing of neural fields using text prompts. Leveraging denoising diffusion models, we successfully embed real-world scenes into the latent space, resulting in a faster and more adaptable NeRF backbone for editing compared to traditional methods. To enhance editing precision, we introduce a delta score to calculate the 2D mask in the latent space that serves as a guide for local modifications while preserving irrelevant regions. Our novel pixel-level scoring approach harnesses the power of InstructPix2Pix (IP2P) to discern the disparity between IP2
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20855;&#26377;&#21487;&#23398;&#20064;&#24352;&#37327;&#26680;&#33539;&#25968;&#30340;&#26032;&#22411;&#24352;&#37327;&#24674;&#22797;&#27169;&#22411;&#65292;&#24341;&#20837;&#20132;&#26367;&#36817;&#31471;&#20056;&#23376;&#26041;&#27861;&#65288;APMM&#65289;&#20248;&#21270;&#31639;&#27861;&#65292;&#35299;&#20915;&#22788;&#29702;&#38750;&#20809;&#28369;&#21464;&#21270;&#30340;&#24352;&#37327;&#25968;&#25454;&#25361;&#25112;</title><link>https://arxiv.org/abs/2311.13958</link><description>&lt;p&gt;
&#22788;&#29702;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#20013;&#30340;&#38750;&#20809;&#28369;&#25361;&#25112;&#65306;&#22810;&#30446;&#26631;&#24352;&#37327;&#24674;&#22797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Handling The Non-Smooth Challenge in Tensor SVD: A Multi-Objective Tensor Recovery Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13958
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20855;&#26377;&#21487;&#23398;&#20064;&#24352;&#37327;&#26680;&#33539;&#25968;&#30340;&#26032;&#22411;&#24352;&#37327;&#24674;&#22797;&#27169;&#22411;&#65292;&#24341;&#20837;&#20132;&#26367;&#36817;&#31471;&#20056;&#23376;&#26041;&#27861;&#65288;APMM&#65289;&#20248;&#21270;&#31639;&#27861;&#65292;&#35299;&#20915;&#22788;&#29702;&#38750;&#20809;&#28369;&#21464;&#21270;&#30340;&#24352;&#37327;&#25968;&#25454;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35768;&#22810;&#22522;&#20110;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#30340;&#24352;&#37327;&#24674;&#22797;&#26041;&#27861;&#22312;&#22788;&#29702;&#35270;&#35273;&#25968;&#25454;&#65288;&#22914;&#24425;&#33394;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#26174;&#31034;&#20986;&#38750;&#20809;&#28369;&#21464;&#21270;&#30340;&#24352;&#37327;&#25968;&#25454;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20250;&#36973;&#21463;&#20005;&#37325;&#30340;&#24615;&#33021;&#36864;&#21270;&#12290;&#34429;&#28982;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#32463;&#24120;&#35266;&#23519;&#21040;&#36825;&#31181;&#24773;&#20917;&#65292;&#20294;&#20256;&#32479;&#30340;&#22522;&#20110;t-SVD&#30340;&#26041;&#27861;&#21364;&#24573;&#35270;&#20102;&#36825;&#19968;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24352;&#37327;&#24674;&#22797;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#21487;&#23398;&#20064;&#30340;&#24352;&#37327;&#26680;&#33539;&#25968;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#26367;&#36817;&#31471;&#20056;&#23376;&#26041;&#27861;&#65288;APMM&#65289;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#36845;&#20195;&#22320;&#35299;&#20915;&#25552;&#20986;&#30340;&#24352;&#37327;&#34917;&#20840;&#27169;&#22411;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;APMM&#25910;&#25947;&#21040;&#20248;&#21270;&#38382;&#39064;&#30340;Karush-Kuhn-Tucker&#65288;KKT&#65289;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;APMM&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#24352;&#37327;&#24674;&#22797;&#26694;&#26550;&#65292;&#20197;&#26377;&#25928;&#25506;&#32034;&#21327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13958v2 Announce Type: replace-cross  Abstract: Recently, numerous tensor singular value decomposition (t-SVD)-based tensor recovery methods have shown promise in processing visual data, such as color images and videos. However, these methods often suffer from severe performance degradation when confronted with tensor data exhibiting non-smooth changes. It has been commonly observed in real-world scenarios but ignored by the traditional t-SVD-based methods. In this work, we introduce a novel tensor recovery model with a learnable tensor nuclear norm to address such a challenge. We develop a new optimization algorithm named the Alternating Proximal Multiplier Method (APMM) to iteratively solve the proposed tensor completion model. Theoretical analysis demonstrates the convergence of the proposed APMM to the Karush-Kuhn-Tucker (KKT) point of the optimization problem. In addition, we propose a multi-objective tensor recovery framework based on APMM to efficiently explore the co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10712</link><description>&lt;p&gt;
Q&amp;A&#25552;&#31034;&#65306;&#36890;&#36807;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#25552;&#31034;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#28385;&#36275;&#23545;&#22810;&#26679;&#19990;&#30028;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge. (arXiv:2401.10712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#65292;&#22238;&#31572;&#38656;&#35201;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#21644;&#19990;&#30028;&#30693;&#35782;&#30340;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;AI&#27169;&#22411;&#37197;&#22791;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#31867;&#30340;&#35748;&#30693;&#26041;&#26696;&#23578;&#26410;&#31995;&#32479;&#22320;&#34987;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30456;&#20449;&#65292;&#22914;&#26524;&#25105;&#20204;&#33021;&#23613;&#21487;&#33021;&#25910;&#38598;&#32473;&#23450;&#22270;&#20687;&#20013;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#25105;&#20204;&#23558;&#33021;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#22270;&#20687;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#38382;&#39064;&#65292;&#26356;&#23481;&#26131;&#22238;&#24518;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#26368;&#32456;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;&#20687;&#20013;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#36825;&#20123;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#24182;&#23558;&#23427;&#20204;&#20316;&#20026;&#25552;&#31034;&#21457;&#36865;&#21040;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22270;&#20687;-&#31572;&#26696;&#23545;&#21644;&#30456;&#24212;&#30340;&#38382;&#39064;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35757;&#32451;&#19968;&#20010;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&amp;A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question gener
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25991;&#26412;&#36164;&#28304;&#65292;&#22522;&#20110;135&#20010;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#36523;&#20221;&#32676;&#20307;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65288;T2I&#65289;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#30340;&#22320;&#29702;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21051;&#26495;&#23646;&#24615;&#22312;&#22270;&#20687;&#20013;&#30340;&#23384;&#22312;&#21487;&#33021;&#24615;&#26159;&#21051;&#26495;&#23646;&#24615;&#30340;&#19977;&#20493;&#12290;</title><link>http://arxiv.org/abs/2401.06310</link><description>&lt;p&gt;
&#36229;&#36234;&#34920;&#38754;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#35270;&#35273;&#21051;&#26495;&#21360;&#35937;&#30340;&#20840;&#29699;&#35268;&#27169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation. (arXiv:2401.06310v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25991;&#26412;&#36164;&#28304;&#65292;&#22522;&#20110;135&#20010;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#36523;&#20221;&#32676;&#20307;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65288;T2I&#65289;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#30340;&#22320;&#29702;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21051;&#26495;&#23646;&#24615;&#22312;&#22270;&#20687;&#20013;&#30340;&#23384;&#22312;&#21487;&#33021;&#24615;&#26159;&#21051;&#26495;&#23646;&#24615;&#30340;&#19977;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#24050;&#32463;&#24378;&#35843;&#20102;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65288;T2I&#65289;&#27169;&#22411;&#29983;&#25104;&#30340;&#20154;&#29289;&#24418;&#35937;&#20013;&#23384;&#22312;&#30340;&#19981;&#21516;&#36523;&#20221;&#32676;&#20307;&#30340;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#38480;&#21046;&#65292;&#21253;&#25324;&#22312;&#35780;&#20272;&#20013;&#23545;&#20840;&#29699;&#36523;&#20221;&#32676;&#20307;&#30340;&#35206;&#30422;&#29575;&#26126;&#26174;&#19981;&#36275;&#65292;&#20197;&#21450;&#30456;&#20851;&#21051;&#26495;&#21360;&#35937;&#30340;&#33539;&#22260;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36890;&#24120;&#32570;&#20047;&#23545;&#26412;&#36136;&#19978;&#26159;&#35270;&#35273;&#21051;&#26495;&#21360;&#35937;&#65288;&#22914;&#8220;&#30246;&#24369;&#8221;&#25110;&#8220;&#22696;&#35199;&#21733;&#33609;&#24125;&#8221;&#65289;&#21644;&#25991;&#21270;&#30456;&#20851;&#30340;&#21051;&#26495;&#21360;&#35937;&#65288;&#22914;&#8220;&#21560;&#24341;&#20154;&#8221;&#25110;&#8220;&#24656;&#24598;&#20998;&#23376;&#8221;&#65289;&#20043;&#38388;&#30340;&#37325;&#35201;&#21306;&#21035;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25991;&#26412;&#36164;&#28304;&#26469;&#23558;&#25105;&#20204;&#23545;&#26469;&#33258;T2I&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#19982;&#22320;&#29702;&#25991;&#21270;&#30456;&#20851;&#30340;&#21051;&#26495;&#21360;&#35937;&#30340;&#35780;&#20272;&#36827;&#34892;&#22522;&#30784;&#32465;&#23450;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#21051;&#26495;&#21360;&#35937;&#22522;&#20934;&#26469;&#35782;&#21035;&#21644;&#35780;&#20272;&#20840;&#29699;&#33539;&#22260;&#20869;&#28041;&#21450;135&#20010;&#22522;&#20110;&#22269;&#31821;&#30340;&#36523;&#20221;&#32676;&#20307;&#30340;&#35270;&#35273;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#22270;&#20687;&#20013;&#23384;&#22312;&#21051;&#26495;&#21360;&#35937;&#30340;&#21487;&#33021;&#24615;&#26159;&#21051;&#26495;&#23646;&#24615;&#30340;&#19977;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have highlighted the issue of stereotypical depictions for people of different identity groups in Text-to-Image (T2I) model generations. However, these existing approaches have several key limitations, including a noticeable lack of coverage of global identity groups in their evaluation, and the range of their associated stereotypes. Additionally, they often lack a critical distinction between inherently visual stereotypes, such as `underweight' or `sombrero', and culturally dependent stereotypes like `attractive' or `terrorist'. In this work, we address these limitations with a multifaceted approach that leverages existing textual resources to ground our evaluation of geo-cultural stereotypes in the generated images from T2I models. We employ existing stereotype benchmarks to identify and evaluate visual stereotypes at a global scale, spanning 135 nationality-based identity groups. We demonstrate that stereotypical attributes are thrice as likely to be present in images
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#37197;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#32473;&#21069;&#26223;&#32534;&#36753;&#21306;&#22495;&#26469;&#23454;&#29616;&#35270;&#39057;&#32534;&#36753;&#30340;&#39640;&#25928;&#29575;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.05735</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#23454;&#29616;&#39640;&#25928;&#35270;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Diffusion for Efficient Video Editing. (arXiv:2401.05735v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#37197;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#32473;&#21069;&#26223;&#32534;&#36753;&#21306;&#22495;&#26469;&#23454;&#29616;&#35270;&#39057;&#32534;&#36753;&#30340;&#39640;&#25928;&#29575;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#39057;&#32534;&#36753;&#24050;&#32463;&#36798;&#21040;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#32534;&#36753;&#25552;&#31034;&#26469;&#36716;&#25442;&#35270;&#39057;&#30340;&#20840;&#23616;&#39118;&#26684;&#12289;&#23616;&#37096;&#32467;&#26500;&#21644;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#29983;&#25104;&#20855;&#26377;&#26102;&#24207;&#19968;&#33268;&#24615;&#30340;&#24103;&#65292;&#21487;&#33021;&#28041;&#21450;&#25193;&#25955;&#21453;&#28436;&#21644;/&#25110;&#36328;&#24103;&#27880;&#24847;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#20302;&#25928;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20462;&#25913;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36895;&#24230;&#21516;&#26102;&#20445;&#25345;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#65288;OCD&#65289;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#36164;&#28304;&#26356;&#22810;&#22320;&#20998;&#37197;&#32473;&#23545;&#24863;&#30693;&#36136;&#37327;&#26356;&#37325;&#35201;&#30340;&#21069;&#26223;&#32534;&#36753;&#21306;&#22495;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;&#24310;&#36831;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#26032;&#30340;&#25552;&#26696;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65306;i&#65289;&#38754;&#21521;&#23545;&#35937;&#30340;&#37319;&#26679;&#65292;&#23558;&#29992;&#20110;&#26174;&#33879;&#21306;&#22495;&#25110;&#32972;&#26223;&#30340;&#25193;&#25955;&#27493;&#39588;&#19982;&#29992;&#20110;&#21069;&#26223;&#30340;&#25193;&#25955;&#27493;&#39588;&#20998;&#31163;&#24320;&#26469;&#65292;&#23558;&#22823;&#37096;&#20998;&#27169;&#22411;&#23481;&#37327;&#20998;&#37197;&#32473;&#21069;&#32773;&#65307;ii&#65289;&#38754;&#21521;&#23545;&#35937;&#30340;3D&#20196;&#29260;&#21512;&#24182;&#65292;&#29992;&#20110;&#25913;&#21892;&#21069;&#26223;&#21644;&#32972;&#26223;&#20043;&#38388;&#30340;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based video editing have reached impressive quality and can transform either the global style, local structure, and attributes of given video inputs, following textual edit prompts. However, such solutions typically incur heavy memory and computational costs to generate temporally-coherent frames, either in the form of diffusion inversion and/or cross-frame attention. In this paper, we conduct an analysis of such inefficiencies, and suggest simple yet effective modifications that allow significant speed-ups whilst maintaining quality. Moreover, we introduce Object-Centric Diffusion, coined as OCD, to further reduce latency by allocating computations more towards foreground edited regions that are arguably more important for perceptual quality. We achieve this by two novel proposals: i) Object-Centric Sampling, decoupling the diffusion steps spent on salient regions or background, allocating most of the model capacity to the former, and ii) Object-Centric 3D Token Merging, whi
&lt;/p&gt;</description></item><item><title>DiagrammerGPT&#26159;&#19968;&#20010;&#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#30340;&#26694;&#26550;&#65292;&#22635;&#34917;&#20102;T2I&#27169;&#22411;&#22312;&#22270;&#34920;&#29983;&#25104;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.12128</link><description>&lt;p&gt;
DiagrammerGPT: &#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;
&lt;/p&gt;
&lt;p&gt;
DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning. (arXiv:2310.12128v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12128
&lt;/p&gt;
&lt;p&gt;
DiagrammerGPT&#26159;&#19968;&#20010;&#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#30340;&#26694;&#26550;&#65292;&#22635;&#34917;&#20102;T2I&#27169;&#22411;&#22312;&#22270;&#34920;&#29983;&#25104;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#20351;&#29992;T2I&#27169;&#22411;&#29983;&#25104;&#22270;&#34920;&#26041;&#38754;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#22270;&#34920;&#26159;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#20016;&#23500;&#21644;&#31354;&#38388;&#22797;&#26434;&#30340;&#21487;&#35270;&#21270;&#26469;&#35299;&#37322;&#20449;&#24687;&#30340;&#31526;&#21495;/&#31034;&#24847;&#24615;&#34920;&#31034;&#65288;&#20363;&#22914;&#65292;&#19968;&#31181;&#23494;&#38598;&#30340;&#30456;&#20851;&#23545;&#35937;&#12289;&#25991;&#26412;&#26631;&#31614;&#12289;&#26041;&#21521;&#31661;&#22836;&#12289;&#36830;&#25509;&#32447;&#31561;&#32452;&#21512;&#65289;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;T2I&#27169;&#22411;&#22312;&#29983;&#25104;&#22270;&#34920;&#26102;&#32463;&#24120;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35768;&#22810;&#23545;&#35937;&#36890;&#36807;&#22797;&#26434;&#30340;&#20851;&#31995;&#65288;&#22914;&#31661;&#22836;/&#32447;&#65289;&#23494;&#38598;&#36830;&#25509;&#26102;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#23545;&#35937;&#24067;&#23616;&#25511;&#21046;&#65292;&#24182;&#19988;&#32463;&#24120;&#19981;&#33021;&#28210;&#26579;&#20986;&#21487;&#29702;&#35299;&#30340;&#25991;&#26412;&#26631;&#31614;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiagrammerGPT&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#25991;&#26412;&#21040;&#22270;&#34920;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;LLM&#65288;&#22914;GPT-4&#65289;&#30340;&#24067;&#23616;&#24341;&#23548;&#33021;&#21147;&#26469;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;LLM&#29983;&#25104;&#21644;&#36845;&#20195;&#25913;&#36827;&#8220;&#22270;&#34920;&#35268;&#21010;&#8221;&#65288;&#22312;&#19968;&#20010;&#35268;&#21010;&#26041;&#26696;&#20013;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image (T2I) generation has seen significant growth over the past few years. Despite this, there has been little work on generating diagrams with T2I models. A diagram is a symbolic/schematic representation that explains information using structurally rich and spatially complex visualizations (e.g., a dense combination of related objects, text labels, directional arrows, connection lines, etc.). Existing state-of-the-art T2I models often fail at diagram generation because they lack fine-grained object layout control when many objects are densely connected via complex relations such as arrows/lines and also often fail to render comprehensible text labels. To address this gap, we present DiagrammerGPT, a novel two-stage text-to-diagram generation framework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4) to generate more accurate open-domain, open-platform diagrams. In the first stage, we use LLMs to generate and iteratively refine 'diagram plans' (in a planne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#23454;&#29616;&#19968;&#33268;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#26469;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#12290;</title><link>http://arxiv.org/abs/2309.15091</link><description>&lt;p&gt;
VideoDirectorGPT: &#36890;&#36807;LLM&#24341;&#23548;&#30340;&#35268;&#21010;&#23454;&#29616;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning. (arXiv:2309.15091v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#23454;&#29616;&#19968;&#33268;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#26469;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#24037;&#20316;&#38598;&#20013;&#22312;&#29983;&#25104;&#21333;&#20010;&#20107;&#20214;&#21644;&#21333;&#19968;&#32972;&#26223;&#30340;&#30701;&#35270;&#39057;&#29255;&#27573;&#65288;&#21363;&#21333;&#22330;&#26223;&#35270;&#39057;&#65289;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#29983;&#25104;&#24067;&#23616;&#21644;&#25511;&#21046;&#19979;&#28216;&#35270;&#35273;&#27169;&#22359;&#65288;&#22914;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65289;&#30340;&#31243;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;&#36825;&#20123;LLMs&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#36827;&#34892;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#21333;&#20010;&#25991;&#26412;&#25552;&#31034;&#36755;&#20837;&#25105;&#20204;&#30340;&#35270;&#39057;&#35268;&#21010;&#22120;LLM&#65288;GPT-4&#65289;&#20013;&#65292;&#23558;&#20854;&#25193;&#23637;&#20026;&#8220;&#35270;&#39057;&#35745;&#21010;&#8221;&#65292;&#20854;&#20013;&#21253;&#25324;&#29983;&#25104;&#22330;&#26223;&#25551;&#36848;&#12289;&#23454;&#20307;&#21450;&#20854;&#24067;&#23616;&#12289;&#27599;&#20010;&#22330;&#26223;&#30340;&#32972;&#26223;&#20197;&#21450;&#20445;&#25345;&#19968;&#33268;&#24615;&#31561;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although recent text-to-video (T2V) generation methods have seen significant advancements, most of these works focus on producing short video clips of a single event with a single background (i.e., single-scene videos). Meanwhile, recent large language models (LLMs) have demonstrated their capability in generating layouts and programs to control downstream visual modules such as image generation models. This raises an important question: can we leverage the knowledge embedded in these LLMs for temporally consistent long video generation? In this paper, we propose VideoDirectorGPT, a novel framework for consistent multi-scene video generation that uses the knowledge of LLMs for video content planning and grounded video generation. Specifically, given a single text prompt, we first ask our video planner LLM (GPT-4) to expand it into a 'video plan', which involves generating the scene descriptions, the entities with their respective layouts, the background for each scene, and consistency 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20013;&#21516;&#26102;&#22788;&#29702;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#21033;&#29992;&#25345;&#32493;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#21518;&#32493;&#20219;&#21153;&#20013;&#30340;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.12112</link><description>&lt;p&gt;
&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Generalized Continual Category Discovery. (arXiv:2308.12112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20013;&#21516;&#26102;&#22788;&#29702;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#21033;&#29992;&#25345;&#32493;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#21518;&#32493;&#20219;&#21153;&#20013;&#30340;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#25512;&#21160;&#30528;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#30340;&#26497;&#38480;&#65292;&#20854;&#20013;&#19968;&#20010;&#26234;&#33021;&#20307;&#26399;&#26395;&#23398;&#20064;&#26032;&#30340;&#26631;&#35760;&#20219;&#21153;&#32780;&#19981;&#20250;&#24536;&#35760;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35774;&#32622;&#19982;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#19981;&#22826;&#21563;&#21512;&#65292;&#20854;&#20013;&#23398;&#20064;&#26234;&#33021;&#20307;&#21487;&#20197;&#35775;&#38382;&#22823;&#37327;&#30340;&#26080;&#26631;&#35760;&#25968;&#25454;&#65292;&#21253;&#25324;&#20840;&#26032;&#65288;&#23436;&#20840;&#26080;&#26631;&#35760;&#65289;&#31867;&#21035;&#21644;&#24050;&#30693;&#31867;&#21035;&#30340;&#31034;&#20363;&#12290;&#21463;&#21040;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#25918;&#26494;&#36825;&#20010;&#20551;&#35774;&#12290;&#30830;&#20999;&#22320;&#35828;&#65292;&#22312;&#20219;&#20309;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20801;&#35768;&#23384;&#22312;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#24517;&#39035;&#20351;&#29992;&#25345;&#32493;&#29256;&#26412;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#35774;&#32622;&#20026;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#12290;&#23427;&#32479;&#19968;&#20102;CL&#21644;GCD&#65292;&#24357;&#21512;&#20102;&#21512;&#25104;&#22522;&#20934;&#21644;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#26080;&#27861;&#20174;&#21518;&#32493;&#20219;&#21153;&#20013;&#31215;&#32047;&#30693;&#35782;&#65292;&#20854;&#20013;&#21253;&#21547;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of Continual Learning (CL) methods push the limit of supervised learning settings, where an agent is expected to learn new labeled tasks and not forget previous knowledge. However, these settings are not well aligned with real-life scenarios, where a learning agent has access to a vast amount of unlabeled data encompassing both novel (entirely unlabeled) classes and examples from known classes. Drawing inspiration from Generalized Category Discovery (GCD), we introduce a novel framework that relaxes this assumption. Precisely, in any task, we allow for the existence of novel and known classes, and one must use continual version of unsupervised learning methods to discover them. We call this setting Generalized Continual Category Discovery (GCCD). It unifies CL and GCD, bridging the gap between synthetic benchmarks and real-life scenarios. With a series of experiments, we present that existing methods fail to accumulate knowledge from subsequent tasks in which unlabeled samples of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26080;&#38480;&#21046;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;AdvDiff&#12290;&#36890;&#36807;&#35774;&#35745;&#20004;&#31181;&#26032;&#30340;&#23545;&#25239;&#24341;&#23548;&#25216;&#26415;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#23545;&#25239;&#37319;&#26679;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.12499</link><description>&lt;p&gt;
AdvDiff:&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26080;&#38480;&#21046;&#30340;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models. (arXiv:2307.12499v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26080;&#38480;&#21046;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;AdvDiff&#12290;&#36890;&#36807;&#35774;&#35745;&#20004;&#31181;&#26032;&#30340;&#23545;&#25239;&#24341;&#23548;&#25216;&#26415;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#23545;&#25239;&#37319;&#26679;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38480;&#21046;&#30340;&#23545;&#25239;&#25915;&#20987;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#23427;&#20204;&#23545;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#36896;&#25104;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#32469;&#36807;&#38450;&#24481;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#25915;&#20987;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#36825;&#20123;&#32593;&#32476;&#22312;&#29702;&#35770;&#19978;&#26080;&#27861;&#35777;&#26126;&#65292;&#22240;&#27492;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#65289;&#19978;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#30446;&#26631;&#29983;&#25104;&#30340;&#20363;&#23376;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;AdvDiff&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26080;&#38480;&#21046;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#26032;&#30340;&#23545;&#25239;&#24341;&#23548;&#25216;&#26415;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#23545;&#25239;&#37319;&#26679;&#12290;&#36825;&#20004;&#31181;&#25216;&#26415;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#30446;&#26631;&#20998;&#31867;&#22120;&#26799;&#24230;&#38598;&#25104;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#38750;&#24120;&#26377;&#25928;&#21644;&#31283;&#23450;&#12290;&#22312;MNIST&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AdvDiff&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unrestricted adversarial attacks present a serious threat to deep learning models and adversarial defense techniques. They pose severe security problems for deep learning applications because they can effectively bypass defense mechanisms. However, previous attack methods often utilize Generative Adversarial Networks (GANs), which are not theoretically provable and thus generate unrealistic examples by incorporating adversarial objectives, especially for large-scale datasets like ImageNet. In this paper, we propose a new method, called AdvDiff, to generate unrestricted adversarial examples with diffusion models. We design two novel adversarial guidance techniques to conduct adversarial sampling in the reverse generation process of diffusion models. These two techniques are effective and stable to generate high-quality, realistic adversarial examples by integrating gradients of the target classifier interpretably. Experimental results on MNIST and ImageNet datasets demonstrate that AdvD
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10864</link><description>&lt;p&gt;
&#23558;&#27880;&#24847;&#21147;&#20998;&#21106;&#19982;&#32465;&#23450;&#29992;&#20110;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing. (arXiv:2307.10864v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#65292;&#23637;&#31034;&#20102;&#39640;&#24230;&#36924;&#30495;&#30340;&#21387;&#20498;&#24615;&#32467;&#26524;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#23436;&#20840;&#20381;&#29031;&#36755;&#20837;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#8212;&#8212;&#20851;&#27880;&#19982;&#28608;&#21457;&#65292;&#24341;&#20837;&#20102;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#65288;GSN&#65289;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#22312;&#25512;&#26029;&#26102;&#20248;&#21270;&#36328;&#27880;&#24847;&#21147;&#20197;&#26356;&#22909;&#22320;&#34701;&#20837;&#35821;&#20041;&#12290;&#23427;&#22312;&#29983;&#25104;&#31616;&#21333;&#25552;&#31034;&#65292;&#22914;&#8220;&#19968;&#21482;&#29483;&#21644;&#19968;&#21482;&#29399;&#8221;&#65292;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#25552;&#31034;&#20197;&#21450;&#35299;&#20915;&#19981;&#36866;&#24403;&#30340;&#23646;&#24615;&#32465;&#23450;&#38382;&#39064;&#26041;&#38754;&#30340;&#21151;&#25928;&#26377;&#25152;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#22797;&#26434;&#25552;&#31034;&#25110;&#28041;&#21450;&#22810;&#20010;&#23454;&#20307;&#30340;&#22330;&#26223;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#25913;&#36827;&#30340;&#23646;&#24615;&#32465;&#23450;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#21106;&#19982;&#32465;&#23450;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;GSN&#25439;&#22833;&#30446;&#26631;&#65306;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#20002;&#22833;&#21644;&#19968;&#31181;&#32465;&#23450;&#20002;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#23558;&#35821;&#20041;&#32435;&#20837;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#29305;&#28857;&#19978;&#33073;&#39062;&#32780;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend &amp; Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide &amp; Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to fa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32622;&#25442;&#31561;&#21464;&#39592;&#24178;&#21644;&#32622;&#25442;&#19981;&#21464;&#20840;&#23616;&#27744;&#21270;&#22312;&#28857;&#20113;&#20998;&#31867;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#20351;&#29992;&#22797;&#26434;&#27744;&#21270;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31616;&#21333;&#39592;&#24178;&#30340;&#24615;&#33021;&#65292;&#20294;&#21363;&#20351;&#26159;&#22797;&#26434;&#30340;&#39592;&#24178;&#20063;&#21487;&#20197;&#21463;&#30410;&#20110;&#26356;&#22797;&#26434;&#30340;&#12289;&#26126;&#30830;&#32534;&#30721;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#32622;&#25442;&#19981;&#21464;&#27744;&#21270;&#26159;&#33719;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2306.05553</link><description>&lt;p&gt;
&#31561;&#21464;&#23618;&#19982;&#19981;&#21464;&#23618;&#30340;&#23545;&#27604;&#65306;&#28857;&#20113;&#20998;&#31867;&#20013;&#39592;&#24178;&#32593;&#32476;&#21644;&#27744;&#21270;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Equivariant vs. Invariant Layers: A Comparison of Backbone and Pooling for Point Cloud Classification. (arXiv:2306.05553v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32622;&#25442;&#31561;&#21464;&#39592;&#24178;&#21644;&#32622;&#25442;&#19981;&#21464;&#20840;&#23616;&#27744;&#21270;&#22312;&#28857;&#20113;&#20998;&#31867;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#20351;&#29992;&#22797;&#26434;&#27744;&#21270;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31616;&#21333;&#39592;&#24178;&#30340;&#24615;&#33021;&#65292;&#20294;&#21363;&#20351;&#26159;&#22797;&#26434;&#30340;&#39592;&#24178;&#20063;&#21487;&#20197;&#21463;&#30410;&#20110;&#26356;&#22797;&#26434;&#30340;&#12289;&#26126;&#30830;&#32534;&#30721;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#32622;&#25442;&#19981;&#21464;&#27744;&#21270;&#26159;&#33719;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#28857;&#20113;&#31561;&#38598;&#21512;&#32467;&#26500;&#25968;&#25454;&#24050;&#21463;&#21040;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#36890;&#36807;&#25972;&#21512;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#20026;&#35774;&#35745;&#26377;&#25928;&#30340;&#28857;&#20113;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#34013;&#26412;&#12290;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#26159;&#32622;&#25442;&#19981;&#21464;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#30001;&#32622;&#25442;&#31561;&#21464;&#39592;&#24178;&#12289;&#32622;&#25442;&#19981;&#21464;&#20840;&#23616;&#27744;&#21270;&#21644;&#22238;&#24402;/&#20998;&#31867;&#22836;&#32452;&#25104;&#12290;&#23613;&#31649;&#29616;&#26377;&#25991;&#29486;&#20391;&#37325;&#20110;&#25913;&#21892;&#32622;&#25442;&#31561;&#21464;&#39592;&#24178;&#65292;&#20294;&#20840;&#23616;&#27744;&#21270;&#30340;&#24433;&#21709;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32622;&#25442;&#31561;&#21464;&#39592;&#24178;&#21644;&#32622;&#25442;&#19981;&#21464;&#20840;&#23616;&#27744;&#21270;&#22312;&#19977;&#20010;&#22522;&#20934;&#28857;&#20113;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;1&#65289;&#35832;&#22914;&#22522;&#20110;&#20256;&#36755;&#25110;&#27880;&#24847;&#21147;&#30340;&#22797;&#26434;&#27744;&#21270;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31616;&#21333;&#39592;&#24178;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#39592;&#24178;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#25910;&#30410;&#20250;&#20943;&#24369;&#12290;2&#65289;&#29978;&#33267;&#22797;&#26434;&#30340;&#39592;&#24178;&#20063;&#21487;&#20197;&#21463;&#30410;&#20110;&#26356;&#22797;&#26434;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#26126;&#30830;&#22320;&#32534;&#30721;&#32622;&#25442;&#19981;&#21464;&#24615;&#12290;3&#65289;&#20351;&#29992;&#32622;&#25442;&#19981;&#21464;&#27744;&#21270;&#23545;&#20110;&#22312;&#28857;&#20113;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from set-structured data, such as point clouds, has gained significant attention from the community. Geometric deep learning provides a blueprint for designing effective set neural networks by incorporating permutation symmetry. Of our interest are permutation invariant networks, which are composed of a permutation equivariant backbone, permutation invariant global pooling, and regression/classification head. While existing literature has focused on improving permutation equivariant backbones, the impact of global pooling is often overlooked. In this paper, we examine the interplay between permutation equivariant backbones and permutation invariant global pooling on three benchmark point cloud classification datasets. Our findings reveal that: 1) complex pooling methods, such as transport-based or attention-based poolings, can significantly boost the performance of simple backbones, but the benefits diminish for more complex backbones, 2) even complex backbones can benefit fro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#35299;&#20915;&#26041;&#26696;&#65306;&#19968;&#31181;&#28789;&#27963;&#30340;&#20998;&#24067;&#23545;&#40784;&#26041;&#27861;&#65292;&#19968;&#31181;&#36719;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#21644;&#19968;&#31181;&#25193;&#20805;&#26410;&#26631;&#35760;&#38598;&#30340;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.04621</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#23545;&#40784;&#12289;&#25552;&#28860;&#21644;&#25193;&#20805;&#25152;&#26377;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Align, Distill, and Augment Everything All at Once for Imbalanced Semi-Supervised Learning. (arXiv:2306.04621v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#35299;&#20915;&#26041;&#26696;&#65306;&#19968;&#31181;&#28789;&#27963;&#30340;&#20998;&#24067;&#23545;&#40784;&#26041;&#27861;&#65292;&#19968;&#31181;&#36719;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#21644;&#19968;&#31181;&#25193;&#20805;&#26410;&#26631;&#35760;&#38598;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#26102;&#65292;&#38656;&#38754;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#24050;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#36793;&#32536;&#20998;&#24067;&#30340;&#21306;&#21035;&#65292;&#21069;&#32773;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#19988;&#21487;&#33021;&#19982;&#21518;&#32773;&#19981;&#21516;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20123;&#37325;&#22823;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#26159;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36991;&#20813;&#20351;&#20266;&#26631;&#31614;&#23545;&#30446;&#26631;&#20998;&#24067;&#30340;&#20559;&#20506;&#65292;&#22914;&#24050;&#26631;&#35760;&#25968;&#25454;&#25110;&#24179;&#34913;&#20998;&#24067;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#26159;&#30830;&#20445;&#25512;&#29702;&#26102;&#30340;&#24179;&#34913;&#26410;&#26631;&#35760;&#20998;&#24067;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#36890;&#36807;&#28789;&#27963;&#30340;&#20998;&#24067;&#23545;&#40784;&#65292;&#36880;&#28176;&#23558;&#20998;&#31867;&#22120;&#20174;&#21160;&#24577;&#20272;&#35745;&#30340;&#26410;&#26631;&#35760;&#20808;&#39564;&#20998;&#24067;&#23545;&#40784;&#21040;&#24179;&#34913;&#20998;&#24067;&#65307;&#21033;&#29992;&#34987;&#22522;&#20110;&#38408;&#20540;&#30340;&#26041;&#27861;&#33293;&#24323;&#30340;&#20302;&#32622;&#20449;&#24230;&#20266;&#26631;&#31614;&#30340;&#36719;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#65307;&#20197;&#21450;&#19968;&#31181;&#23558;&#26631;&#35760;&#37096;&#20998;&#30340;&#36755;&#20837;&#25968;&#25454;&#25193;&#23637;&#21040;&#26410;&#26631;&#35760;&#38598;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing the class imbalance in long-tailed semi-supervised learning (SSL) poses a few significant challenges stemming from differences between the marginal distributions of unlabeled data and the labeled data, as the former is often unknown and potentially distinct from the latter. The first challenge is to avoid biasing the pseudo-labels towards an incorrect distribution, such as that of the labeled data or a balanced distribution, during training. However, we still wish to ensure a balanced unlabeled distribution during inference, which is the second challenge. To address both of these challenges, we propose a three-faceted solution: a flexible distribution alignment that progressively aligns the classifier from a dynamically estimated unlabeled prior towards a balanced distribution, a soft consistency regularization that exploits underconfident pseudo-labels discarded by threshold-based methods, and a schema for expanding the unlabeled set with input data from the labeled partiti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#38271;&#26102;&#38388;&#20869;&#30340;&#36816;&#21160;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.00416</link><description>&lt;p&gt;
&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Controllable Motion Diffusion Model. (arXiv:2306.00416v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00416
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#38271;&#26102;&#38388;&#20869;&#30340;&#36816;&#21160;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#21160;&#30011;&#20013;&#65292;&#20026;&#34394;&#25311;&#35282;&#33394;&#29983;&#25104;&#36924;&#30495;&#19988;&#21487;&#25511;&#30340;&#36816;&#21160;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20174;&#22270;&#20687;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22823;&#22810;&#38480;&#20110;&#31163;&#32447;&#24212;&#29992;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#21516;&#26102;&#29983;&#25104;&#25152;&#26377;&#27493;&#39588;&#30340;&#24207;&#21015;&#32423;&#29983;&#25104;&#12290;&#20026;&#20102;&#33021;&#22815;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20197;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#20026;&#22522;&#30784;&#65292;&#36880;&#27493;&#29983;&#25104;&#36816;&#21160;&#24207;&#21015;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#20351;&#29992;&#26631;&#20934;DDPM&#31639;&#27861;&#32780;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#20135;&#29983;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#36816;&#21160;&#25511;&#21046;&#19979;&#38271;&#26102;&#38388;&#20869;&#30340;&#39640;&#20445;&#30495;&#24230;&#36816;&#21160;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating realistic and controllable motions for virtual characters is a challenging task in computer animation, and its implications extend to games, simulations, and virtual reality. Recent studies have drawn inspiration from the success of diffusion models in image generation, demonstrating the potential for addressing this task. However, the majority of these studies have been limited to offline applications that target at sequence-level generation that generates all steps simultaneously. To enable real-time motion synthesis with diffusion models in response to time-varying control signals, we propose the framework of the Controllable Motion Diffusion Model (COMODO). Our framework begins with an auto-regressive motion diffusion model (A-MDM), which generates motion sequences step by step. In this way, simply using the standard DDPM algorithm without any additional complexity, our framework is able to generate high-fidelity motion sequences over extended periods with different type
&lt;/p&gt;</description></item><item><title>MaskedKD&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36974;&#34109;&#22270;&#20687;&#22359;&#26469;&#26174;&#33879;&#38477;&#20302;Vision Transformer (ViT)&#33976;&#39311;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#24433;&#21709;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10494</link><description>&lt;p&gt;
MaskedKD&#65306;&#20351;&#29992;&#36974;&#34109;&#22270;&#20687;&#30340;&#39640;&#25928;Vision Transformer&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MaskedKD: Efficient Distillation of Vision Transformers with Masked Images. (arXiv:2302.10494v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10494
&lt;/p&gt;
&lt;p&gt;
MaskedKD&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36974;&#34109;&#22270;&#20687;&#22359;&#26469;&#26174;&#33879;&#38477;&#20302;Vision Transformer (ViT)&#33976;&#39311;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#24433;&#21709;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#23545;&#20110;&#35757;&#32451;&#36731;&#37327;&#32423;&#27169;&#22411;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20250;&#22312;&#35757;&#32451;&#25104;&#26412;&#20013;&#24341;&#20837;&#22823;&#37327;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#22240;&#20026;&#35813;&#26041;&#27861;&#38656;&#35201;&#22312;&#35757;&#32451;&#26679;&#26412;&#19978;&#33719;&#21462;&#25945;&#24072;&#30417;&#30563;&#12290;&#24403;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;Vision Transformer&#65288;ViTs&#65289;&#31561;&#25945;&#24072;&#27169;&#22411;&#26102;&#65292;&#36825;&#31181;&#38468;&#21152;&#25104;&#26412;&#8212;&#8212;&#33976;&#39311;&#25104;&#26412;&#8212;&#8212;&#26368;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MaskedKD&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#30528;&#38477;&#20302;&#33976;&#39311;ViTs&#30340;&#25104;&#26412;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MaskedKD&#36890;&#36807;&#36974;&#34109;&#19968;&#37096;&#20998;&#36755;&#20837;&#21040;&#25945;&#24072;&#27169;&#22411;&#30340;&#22270;&#20687;&#22359;&#20196;&#25945;&#24072;&#27169;&#22411;&#30340;&#25512;&#29702;&#25104;&#26412;&#20943;&#23569;&#65292;&#22240;&#27492;&#21487;&#20197;&#36339;&#36807;&#22788;&#29702;&#36825;&#20123;&#22359;&#25152;&#38656;&#30340;&#35745;&#31639;&#12290;&#25152;&#36873;&#30340;&#36974;&#32617;&#20301;&#32622;&#26088;&#22312;&#38450;&#27490;&#23631;&#34109;&#23398;&#29983;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#30340;&#22270;&#20687;&#30340;&#26680;&#24515;&#29305;&#24449;&#12290;&#35813;&#36974;&#32617;&#36873;&#25321;&#26426;&#21046;&#22522;&#20110;&#23398;&#29983;&#27169;&#22411;&#30340;&#26576;&#20123;&#27880;&#24847;&#21147;&#20998;&#25968;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is an effective method for training lightweight models, but it introduces a significant amount of computational overhead to the training cost, as the method requires acquiring teacher supervisions on training samples. This additional cost -- called distillation cost -- is most pronounced when we employ large-scale teacher models such as vision transformers (ViTs). We present MaskedKD, a simple yet effective strategy that can significantly reduce the cost of distilling ViTs without sacrificing the prediction accuracy of the student model. Specifically, MaskedKD diminishes the cost of running teacher at inference by masking a fraction of image patch tokens fed to the teacher, and therefore skipping the computations required to process those patches. The mask locations are selected to prevent masking away the core features of an image that the student model uses for prediction. This mask selection mechanism operates based on some attention score of the student model
&lt;/p&gt;</description></item></channel></rss>