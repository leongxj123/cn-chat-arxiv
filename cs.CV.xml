<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27169;&#22411;&#38598;&#25104;&#35780;&#20272;&#26041;&#27861;&#65292;&#20316;&#32773;&#25581;&#31034;&#20102;&#29616;&#26377;&#38598;&#25104;&#26041;&#27861;&#30340;&#20851;&#38190;&#32570;&#38519;&#65292;&#25552;&#20986;&#20102;&#25552;&#39640;&#20256;&#32479;&#27169;&#22411;&#38598;&#25104;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#22810;&#26679;&#24615;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.16260</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#22810;&#29702;&#35299;&#38598;&#25104;&#23454;&#29616;&#36234;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16260
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27169;&#22411;&#38598;&#25104;&#35780;&#20272;&#26041;&#27861;&#65292;&#20316;&#32773;&#25581;&#31034;&#20102;&#29616;&#26377;&#38598;&#25104;&#26041;&#27861;&#30340;&#20851;&#38190;&#32570;&#38519;&#65292;&#25552;&#20986;&#20102;&#25552;&#39640;&#20256;&#32479;&#27169;&#22411;&#38598;&#25104;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#22810;&#26679;&#24615;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#36234;&#30028;&#65288;OOD&#65289;&#29305;&#24449;&#34920;&#31034;&#39046;&#22495;&#35268;&#27169;&#23545;&#27169;&#22411;&#22312;OOD&#26816;&#27979;&#20013;&#25928;&#26524;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#37319;&#29992;&#27169;&#22411;&#38598;&#25104;&#20316;&#20026;&#22686;&#24378;&#36825;&#19968;&#29305;&#24449;&#34920;&#31034;&#39046;&#22495;&#30340;&#31361;&#20986;&#31574;&#30053;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#26399;&#30340;&#27169;&#22411;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27169;&#22411;&#38598;&#25104;&#35780;&#20272;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#25439;&#22833;&#30406;/&#38556;&#30861;&#21487;&#35270;&#21270;&#21644;&#33258;&#32806;&#21512;&#25351;&#25968;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#38598;&#25104;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#38519;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#21253;&#21547;&#21487;&#36827;&#34892;&#20223;&#23556;&#21464;&#25442;&#30340;&#26435;&#37325;&#65292;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#21487;&#21464;&#24615;&#65292;&#20174;&#32780;&#26410;&#33021;&#23454;&#29616;&#29305;&#24449;&#34920;&#31034;&#20013;&#25152;&#38656;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16260v1 Announce Type: cross  Abstract: Recent research underscores the pivotal role of the Out-of-Distribution (OOD) feature representation field scale in determining the efficacy of models in OOD detection. Consequently, the adoption of model ensembles has emerged as a prominent strategy to augment this feature representation field, capitalizing on anticipated model diversity.   However, our introduction of novel qualitative and quantitative model ensemble evaluation methods, specifically Loss Basin/Barrier Visualization and the Self-Coupling Index, reveals a critical drawback in existing ensemble methods. We find that these methods incorporate weights that are affine-transformable, exhibiting limited variability and thus failing to achieve the desired diversity in feature representation.   To address this limitation, we elevate the dimensions of traditional model ensembles, incorporating various factors such as different weight initializations, data holdout, etc., into di
&lt;/p&gt;</description></item><item><title>&#36229;&#21442;&#25968;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#34987;&#24378;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28041;&#21450;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#35780;&#20272;&#38454;&#27573;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;</title><link>https://arxiv.org/abs/2403.09066</link><description>&lt;p&gt;
Continual Learning&#20013;&#30340;&#36229;&#21442;&#25968;&#65306;&#29616;&#23454;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Hyperparameters in Continual Learning: a Reality Check
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09066
&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#34987;&#24378;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28041;&#21450;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#35780;&#20272;&#38454;&#27573;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#31639;&#27861;&#26088;&#22312;&#22312;CL&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#32531;&#35299;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#35843;&#25972;&#27599;&#31181;&#31639;&#27861;&#30340;&#36866;&#24403;&#36229;&#21442;&#25968;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#26412;&#25991;&#20027;&#24352;&#29616;&#34892;&#30340;&#35780;&#20272;&#21327;&#35758;&#26082;&#19981;&#20999;&#23454;&#38469;&#65292;&#20063;&#26080;&#27861;&#26377;&#25928;&#35780;&#20272;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09066v1 Announce Type: new  Abstract: Various algorithms for continual learning (CL) have been designed with the goal of effectively alleviating the trade-off between stability and plasticity during the CL process. To achieve this goal, tuning appropriate hyperparameters for each algorithm is essential. As an evaluation protocol, it has been common practice to train a CL algorithm using diverse hyperparameter values on a CL scenario constructed with a benchmark dataset. Subsequently, the best performance attained with the optimal hyperparameter value serves as the criterion for evaluating the CL algorithm. In this paper, we contend that this evaluation protocol is not only impractical but also incapable of effectively assessing the CL capability of a CL algorithm. Returning to the fundamental principles of model evaluation in machine learning, we propose an evaluation protocol that involves Hyperparameter Tuning and Evaluation phases. Those phases consist of different datase
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#36125;&#21494;&#26031;&#38647;&#36798;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20248;&#36125;&#21494;&#26031;&#34701;&#21512;&#26469;&#26377;&#25928;&#22320;&#27719;&#24635;&#22810;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#27010;&#29575;&#21521;&#37327;&#65292;&#20197;&#25913;&#36827;&#26080;&#20154;&#26426;&#38647;&#36798;&#25130;&#38754;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.17987</link><description>&lt;p&gt;
&#22810;&#24577;&#38647;&#36798;&#23545;&#31354;&#20013;&#39134;&#34892;&#22120;&#38647;&#36798;&#25130;&#38754;&#35782;&#21035;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multistatic-Radar RCS-Signature Recognition of Aerial Vehicles: A Bayesian Fusion Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17987
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#36125;&#21494;&#26031;&#38647;&#36798;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20248;&#36125;&#21494;&#26031;&#34701;&#21512;&#26469;&#26377;&#25928;&#22320;&#27719;&#24635;&#22810;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#27010;&#29575;&#21521;&#37327;&#65292;&#20197;&#25913;&#36827;&#26080;&#20154;&#26426;&#38647;&#36798;&#25130;&#38754;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17987v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#26080;&#20154;&#26426;&#30340;&#38647;&#36798;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;&#65288;RATR&#65289;&#28041;&#21450;&#21457;&#23556;&#30005;&#30913;&#27874;&#24182;&#23545;&#25509;&#25910;&#21040;&#30340;&#38647;&#36798;&#22238;&#27874;&#25191;&#34892;&#30446;&#26631;&#31867;&#22411;&#35782;&#21035;&#65292;&#23545;&#22269;&#38450;&#21644;&#33322;&#31354;&#33322;&#22825;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#22810;&#24577;&#38647;&#36798;&#37197;&#32622;&#22312;RATR&#20013;&#20248;&#20110;&#21333;&#24577;&#38647;&#36798;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22810;&#24577;&#38647;&#36798;&#37197;&#32622;&#20013;&#30340;&#34701;&#21512;&#26041;&#27861;&#36890;&#24120;&#20197;&#27010;&#29575;&#26041;&#24335;&#27425;&#20248;&#22320;&#32452;&#21512;&#26469;&#33258;&#21508;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#21521;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#36125;&#21494;&#26031;RATR&#26694;&#26550;&#65292;&#37319;&#29992;&#26368;&#20248;&#36125;&#21494;&#26031;&#34701;&#21512;&#65288;OBF&#65289;&#26469;&#32858;&#21512;&#26469;&#33258;&#22810;&#20010;&#38647;&#36798;&#30340;&#20998;&#31867;&#27010;&#29575;&#21521;&#37327;&#12290;OBF&#22522;&#20110;&#26399;&#26395;0-1&#25439;&#22833;&#65292;&#26681;&#25454;&#22810;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#21382;&#21490;&#35266;&#27979;&#26356;&#26032;&#30446;&#26631;&#26080;&#20154;&#26426;&#31867;&#22411;&#30340;&#36882;&#24402;&#36125;&#21494;&#26031;&#20998;&#31867;&#65288;RBC&#65289;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#30340;&#38543;&#26426;&#34892;&#36208;&#36712;&#36857;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#20849;&#28041;&#21450;&#19971;&#31181;&#26426;&#21160;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17987v1 Announce Type: cross  Abstract: Radar Automated Target Recognition (RATR) for Unmanned Aerial Vehicles (UAVs) involves transmitting Electromagnetic Waves (EMWs) and performing target type recognition on the received radar echo, crucial for defense and aerospace applications. Previous studies highlighted the advantages of multistatic radar configurations over monostatic ones in RATR. However, fusion methods in multistatic radar configurations often suboptimally combine classification vectors from individual radars probabilistically. To address this, we propose a fully Bayesian RATR framework employing Optimal Bayesian Fusion (OBF) to aggregate classification probability vectors from multiple radars. OBF, based on expected 0-1 loss, updates a Recursive Bayesian Classification (RBC) posterior distribution for target UAV type, conditioned on historical observations across multiple time steps. We evaluate the approach using simulated random walk trajectories for seven dro
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22823;&#37327;&#30340;&#21307;&#30103;&#25968;&#25454;&#26679;&#26412;&#12289;&#22522;&#20934;&#26041;&#27861;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17542</link><description>&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#21512;&#21307;&#23398;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data-Effective Learning: A Comprehensive Medical Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17542
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22823;&#37327;&#30340;&#21307;&#30103;&#25968;&#25454;&#26679;&#26412;&#12289;&#22522;&#20934;&#26041;&#27861;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#26088;&#22312;&#20197;&#26368;&#26377;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#25968;&#25454;&#26469;&#35757;&#32451;AI&#27169;&#22411;&#65292;&#20854;&#28041;&#21450;&#20851;&#27880;&#25968;&#25454;&#36136;&#37327;&#32780;&#38750;&#25968;&#37327;&#30340;&#31574;&#30053;&#65292;&#30830;&#20445;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#20855;&#26377;&#39640;&#20449;&#24687;&#20215;&#20540;&#12290;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#22312;&#21152;&#24555;AI&#35757;&#32451;&#12289;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#33410;&#30465;&#25968;&#25454;&#23384;&#20648;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#22312;&#36817;&#24180;&#26469;&#21307;&#23398;&#25968;&#25454;&#30340;&#25968;&#37327;&#36229;&#20986;&#20102;&#35768;&#22810;&#20154;&#30340;&#39044;&#26399;&#26102;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#20934;&#21644;&#32508;&#21512;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30740;&#31350;&#36824;&#19981;&#22815;&#28145;&#20837;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;31&#20010;&#21307;&#30103;&#20013;&#24515;&#25968;&#30334;&#19975;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;(DataDEL)&#65292;&#29992;&#20110;&#27604;&#36739;&#30340;&#22522;&#20934;&#26041;&#27861;(MedDEL)&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#23458;&#35266;&#34913;&#37327;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#24615;&#33021;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;(NormDEL)&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#23454;&#39564;&#21644;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22522;&#20934;&#22312;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-effective learning aims to use data in the most impactful way to train AI models, which involves strategies that focus on data quality rather than quantity, ensuring the data used for training has high informational value. Data-effective learning plays a profound role in accelerating AI training, reducing computational costs, and saving data storage, which is very important as the volume of medical data in recent years has grown beyond many people's expectations. However, due to the lack of standards and comprehensive benchmark, research on medical data-effective learning is poorly studied. To address this gap, our paper introduces a comprehensive benchmark specifically for evaluating data-effective learning in the medical field. This benchmark includes a dataset with millions of data samples from 31 medical centers (DataDEL), a baseline method for comparison (MedDEL), and a new evaluation metric (NormDEL) to objectively measure data-effective learning performance. Our extensive e
&lt;/p&gt;</description></item><item><title>MIMIR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#20013;&#21033;&#29992;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#22686;&#24378;Vision Transformers&#65288;ViTs&#65289;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.04960</link><description>&lt;p&gt;
MIMIR: &#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness. (arXiv:2312.04960v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04960
&lt;/p&gt;
&lt;p&gt;
MIMIR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#20013;&#21033;&#29992;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#22686;&#24378;Vision Transformers&#65288;ViTs&#65289;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;ViTs&#65289;&#30456;&#23545;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;ViTs&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#23545;&#25239;&#24615;&#35757;&#32451;&#26159;&#24314;&#31435;&#24378;&#22823;&#30340;CNN&#27169;&#22411;&#30340;&#26368;&#25104;&#21151;&#26041;&#27861;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;ViTs&#21644;CNNs&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#22914;&#26356;&#22909;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#38450;&#27490;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#21333;&#20010;&#22359;&#19978;&#65292;&#25110;&#20002;&#24323;&#20302;&#27880;&#24847;&#21147;&#30340;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#36981;&#24490;&#20256;&#32479;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#30340;&#35774;&#35745;&#65292;&#38480;&#21046;&#20102;&#23545;ViTs&#30340;&#23545;&#25239;&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;MIMIR&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#20013;&#30340;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#26500;&#24314;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#25509;&#21463;&#23545;&#25239;&#24615;&#20363;&#23376;&#20316;&#20026;&#36755;&#20837;&#65292;&#20294;&#23558;&#24178;&#20928;&#30340;&#20363;&#23376;&#20316;&#20026;&#24314;&#27169;&#30446;&#26631;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20114;&#20449;&#24687;&#65288;MI&#65289;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) achieve superior performance on various tasks compared to convolutional neural networks (CNNs), but ViTs are also vulnerable to adversarial attacks. Adversarial training is one of the most successful methods to build robust CNN models. Thus, recent works explored new methodologies for adversarial training of ViTs based on the differences between ViTs and CNNs, such as better training strategies, preventing attention from focusing on a single block, or discarding low-attention embeddings. However, these methods still follow the design of traditional supervised adversarial training, limiting the potential of adversarial training on ViTs. This paper proposes a novel defense method, MIMIR, which aims to build a different adversarial training methodology by utilizing Masked Image Modeling at pre-training. We create an autoencoder that accepts adversarial examples as input but takes the clean examples as the modeling target. Then, we create a mutual information (MI
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#38271;&#26102;&#38388;&#20869;&#30340;&#36816;&#21160;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.00416</link><description>&lt;p&gt;
&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Controllable Motion Diffusion Model. (arXiv:2306.00416v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00416
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#38271;&#26102;&#38388;&#20869;&#30340;&#36816;&#21160;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#21160;&#30011;&#20013;&#65292;&#20026;&#34394;&#25311;&#35282;&#33394;&#29983;&#25104;&#36924;&#30495;&#19988;&#21487;&#25511;&#30340;&#36816;&#21160;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20174;&#22270;&#20687;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22823;&#22810;&#38480;&#20110;&#31163;&#32447;&#24212;&#29992;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#21516;&#26102;&#29983;&#25104;&#25152;&#26377;&#27493;&#39588;&#30340;&#24207;&#21015;&#32423;&#29983;&#25104;&#12290;&#20026;&#20102;&#33021;&#22815;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20197;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#20026;&#22522;&#30784;&#65292;&#36880;&#27493;&#29983;&#25104;&#36816;&#21160;&#24207;&#21015;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#20351;&#29992;&#26631;&#20934;DDPM&#31639;&#27861;&#32780;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#20135;&#29983;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#36816;&#21160;&#25511;&#21046;&#19979;&#38271;&#26102;&#38388;&#20869;&#30340;&#39640;&#20445;&#30495;&#24230;&#36816;&#21160;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating realistic and controllable motions for virtual characters is a challenging task in computer animation, and its implications extend to games, simulations, and virtual reality. Recent studies have drawn inspiration from the success of diffusion models in image generation, demonstrating the potential for addressing this task. However, the majority of these studies have been limited to offline applications that target at sequence-level generation that generates all steps simultaneously. To enable real-time motion synthesis with diffusion models in response to time-varying control signals, we propose the framework of the Controllable Motion Diffusion Model (COMODO). Our framework begins with an auto-regressive motion diffusion model (A-MDM), which generates motion sequences step by step. In this way, simply using the standard DDPM algorithm without any additional complexity, our framework is able to generate high-fidelity motion sequences over extended periods with different type
&lt;/p&gt;</description></item></channel></rss>