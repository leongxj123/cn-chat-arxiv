<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>TorchCP&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;Python&#24037;&#20855;&#21253;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#29616;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#22810;&#31181;&#24037;&#20855;&#65292;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;En_Tdlr: TorchCP is a Python toolbox built on PyTorch for conformal prediction research on deep learning models, providing various implementations for posthoc and training methods for classification and regression tasks, including multi-dimension output.</title><link>https://arxiv.org/abs/2402.12683</link><description>&lt;p&gt;
TorchCP&#65306;&#22522;&#20110;PyTorch&#30340;&#19968;&#31181;&#36866;&#29992;&#20110;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
TorchCP: A Library for Conformal Prediction based on PyTorch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12683
&lt;/p&gt;
&lt;p&gt;
TorchCP&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;Python&#24037;&#20855;&#21253;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#29616;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#22810;&#31181;&#24037;&#20855;&#65292;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;En_Tdlr: TorchCP is a Python toolbox built on PyTorch for conformal prediction research on deep learning models, providing various implementations for posthoc and training methods for classification and regression tasks, including multi-dimension output.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TorchCP&#26159;&#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#30340;Python&#24037;&#20855;&#21253;&#12290;&#23427;&#21253;&#21547;&#20102;&#29992;&#20110;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#21508;&#31181;&#23454;&#29616;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65288;&#21253;&#25324;&#22810;&#32500;&#36755;&#20986;&#65289;&#12290;TorchCP&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#24182;&#21033;&#29992;&#30697;&#38453;&#35745;&#31639;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#31616;&#27905;&#39640;&#25928;&#30340;&#25512;&#29702;&#23454;&#29616;&#12290;&#35813;&#20195;&#30721;&#37319;&#29992;LGPL&#35768;&#21487;&#35777;&#65292;&#24182;&#22312;$\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this https URL}}$&#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12683v1 Announce Type: new  Abstract: TorchCP is a Python toolbox for conformal prediction research on deep learning models. It contains various implementations for posthoc and training methods for classification and regression tasks (including multi-dimension output). TorchCP is built on PyTorch (Paszke et al., 2019) and leverages the advantages of matrix computation to provide concise and efficient inference implementations. The code is licensed under the LGPL license and is open-sourced at $\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this https URL}}$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22836;&#37096;&#38598;&#21644;&#23614;&#37096;&#38598;&#30340;&#23398;&#20064;&#35270;&#20026;&#20004;&#20010;&#29420;&#31435;&#36830;&#32493;&#30340;&#27493;&#39588;&#65292;&#24182;&#21033;&#29992;&#23450;&#29702;&#35777;&#26126;&#25345;&#32493;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#26356;&#26032;&#23398;&#20064;&#32773;&#30340;&#26435;&#37325;&#20197;&#23398;&#20064;&#23614;&#37096;&#65292;&#21516;&#26102;&#19981;&#20250;&#24536;&#35760;&#22836;&#37096;&#12290;</title><link>http://arxiv.org/abs/2306.13275</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#33021;&#25913;&#36827;&#38271;&#23614;&#35782;&#21035;&#21527;&#65311;&#36208;&#21521;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Can Continual Learning Improve Long-Tailed Recognition? Toward a Unified Framework. (arXiv:2306.13275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22836;&#37096;&#38598;&#21644;&#23614;&#37096;&#38598;&#30340;&#23398;&#20064;&#35270;&#20026;&#20004;&#20010;&#29420;&#31435;&#36830;&#32493;&#30340;&#27493;&#39588;&#65292;&#24182;&#21033;&#29992;&#23450;&#29702;&#35777;&#26126;&#25345;&#32493;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#26356;&#26032;&#23398;&#20064;&#32773;&#30340;&#26435;&#37325;&#20197;&#23398;&#20064;&#23614;&#37096;&#65292;&#21516;&#26102;&#19981;&#20250;&#24536;&#35760;&#22836;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#26679;&#26412;&#25968;&#37327;&#26497;&#24230;&#22833;&#34913;&#20250;&#20986;&#29616;&#38271;&#23614;&#35782;&#21035;&#65288;LTR&#65289;&#38382;&#39064;&#12290;LTR&#26041;&#27861;&#26088;&#22312;&#20934;&#30830;&#22320;&#23398;&#20064;&#21253;&#21547;&#19968;&#20010;&#36739;&#22823;&#8220;&#22836;&#8221;&#38598;&#21644;&#19968;&#20010;&#36739;&#23567;&#8220;&#23614;&#8221;&#38598;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#20551;&#35774;&#25439;&#22833;&#20989;&#25968;&#26159;&#24378;&#20984;&#30340;&#65292;&#37027;&#20040;&#23436;&#25972;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#23398;&#20064;&#32773;&#30340;&#26435;&#37325;&#22312;&#21516;&#19968;&#20010;&#23398;&#20064;&#32773;&#20005;&#26684;&#35757;&#32451;&#22836;&#38598;&#26102;&#30340;&#26435;&#37325;&#19978;&#38480;&#20043;&#20869;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22768;&#31216;&#23558;&#22836;&#38598;&#21644;&#23614;&#38598;&#30340;&#23398;&#20064;&#35270;&#20026;&#20004;&#20010;&#29420;&#31435;&#30340;&#36830;&#32493;&#27493;&#39588;&#65292;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#26356;&#26032;&#23398;&#20064;&#32773;&#30340;&#26435;&#37325;&#20197;&#23398;&#20064;&#23614;&#37096;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#22836;&#37096;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#29609;&#20855;MNIST-LT&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#26631;&#20934;LTR&#22522;&#20934;&#65288;CIFAR100-LT&#21644;CIFAR10-L&#65289;&#30340;&#22810;&#20010;&#19981;&#24179;&#34913;&#21464;&#20307;&#19978;&#35780;&#20272;&#20102;&#20960;&#31181;CL&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Long-Tailed Recognition (LTR) problem emerges in the context of learning from highly imbalanced datasets, in which the number of samples among different classes is heavily skewed. LTR methods aim to accurately learn a dataset comprising both a larger Head set and a smaller Tail set. We propose a theorem where under the assumption of strong convexity of the loss function, the weights of a learner trained on the full dataset are within an upper bound of the weights of the same learner trained strictly on the Head. Next, we assert that by treating the learning of the Head and Tail as two separate and sequential steps, Continual Learning (CL) methods can effectively update the weights of the learner to learn the Tail without forgetting the Head. First, we validate our theoretical findings with various experiments on the toy MNIST-LT dataset. We then evaluate the efficacy of several CL strategies on multiple imbalanced variations of two standard LTR benchmarks (CIFAR100-LT and CIFAR10-L
&lt;/p&gt;</description></item></channel></rss>