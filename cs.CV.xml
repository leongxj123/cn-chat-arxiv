<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#24335;&#28857;&#20113;&#34920;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#36830;&#32493;&#20843;&#21449;&#26641;&#27010;&#29575;&#22330;&#21644;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32593;&#26684;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#28210;&#26579;&#21644;&#20445;&#30041;&#32454;&#33268;&#20960;&#20309;&#32454;&#33410;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#20960;&#20010;&#24120;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.16862</link><description>&lt;p&gt;
INPC&#65306;&#29992;&#20110;&#36752;&#23556;&#22330;&#28210;&#26579;&#30340;&#38544;&#24335;&#31070;&#32463;&#28857;&#20113;
&lt;/p&gt;
&lt;p&gt;
INPC: Implicit Neural Point Clouds for Radiance Field Rendering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16862
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#24335;&#28857;&#20113;&#34920;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#36830;&#32493;&#20843;&#21449;&#26641;&#27010;&#29575;&#22330;&#21644;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32593;&#26684;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#28210;&#26579;&#21644;&#20445;&#30041;&#32454;&#33268;&#20960;&#20309;&#32454;&#33410;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#20960;&#20010;&#24120;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#24314;&#21644;&#21512;&#25104;&#26080;&#36793;&#30028;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;&#19982;&#20197;&#24448;&#20351;&#29992;&#20307;&#31215;&#22330;&#12289;&#22522;&#20110;&#32593;&#26684;&#30340;&#27169;&#22411;&#25110;&#31163;&#25955;&#28857;&#20113;&#20195;&#29702;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#22330;&#26223;&#34920;&#31034;&#65292;&#23427;&#22312;&#36830;&#32493;&#20843;&#21449;&#26641;&#27010;&#29575;&#22330;&#21644;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32593;&#26684;&#20013;&#38544;&#21547;&#22320;&#32534;&#30721;&#28857;&#20113;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#20004;&#20010;&#19990;&#30028;&#30340;&#20248;&#21183;&#65292;&#20445;&#30041;&#20102;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#26377;&#21033;&#30340;&#34892;&#20026;&#65306;&#25105;&#20204;&#30340;&#26032;&#39062;&#38544;&#24335;&#28857;&#20113;&#34920;&#31034;&#21644;&#21487;&#24494;&#30340;&#21452;&#32447;&#24615;&#20809;&#26629;&#21270;&#22120;&#23454;&#29616;&#20102;&#24555;&#36895;&#28210;&#26579;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#32454;&#24494;&#30340;&#20960;&#20309;&#32454;&#33410;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#20687;&#32467;&#26500;&#36816;&#21160;&#28857;&#20113;&#36825;&#26679;&#30340;&#21021;&#22987;&#20808;&#39564;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#24120;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#24555;&#36895;&#25512;&#29702;&#65292;&#21487;&#20132;&#20114;&#24103;&#36895;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#21462;&#26174;&#24335;&#28857;&#20113;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16862v1 Announce Type: cross  Abstract: We introduce a new approach for reconstruction and novel-view synthesis of unbounded real-world scenes. In contrast to previous methods using either volumetric fields, grid-based models, or discrete point cloud proxies, we propose a hybrid scene representation, which implicitly encodes a point cloud in a continuous octree-based probability field and a multi-resolution hash grid. In doing so, we combine the benefits of both worlds by retaining favorable behavior during optimization: Our novel implicit point cloud representation and differentiable bilinear rasterizer enable fast rendering while preserving fine geometric detail without depending on initial priors like structure-from-motion point clouds. Our method achieves state-of-the-art image quality on several common benchmark datasets. Furthermore, we achieve fast inference at interactive frame rates, and can extract explicit point clouds to further enhance performance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#29420;&#31435;&#20110;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#31283;&#20581;&#21453;&#21521;&#36807;&#31243;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#26377;&#25928;&#22788;&#29702;&#23545;&#25239;&#20928;&#21270;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#25439;&#22833;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16067</link><description>&lt;p&gt;
&#38024;&#23545;&#23545;&#25239;&#20928;&#21270;&#30340;&#24378;&#22823;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust Diffusion Models for Adversarial Purification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16067
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#29420;&#31435;&#20110;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#31283;&#20581;&#21453;&#21521;&#36807;&#31243;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#26377;&#25928;&#22788;&#29702;&#23545;&#25239;&#20928;&#21270;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#25439;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#30340;&#23545;&#25239;&#20928;&#21270;&#65288;AP&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#26368;&#26377;&#21147;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#30053;&#20102;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#23545;&#23545;&#25239;&#25915;&#20987;&#24182;&#19981;&#31283;&#20581;&#36825;&#19968;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#25193;&#25955;&#36807;&#31243;&#24456;&#23481;&#26131;&#30772;&#22351;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#21453;&#21521;&#36807;&#31243;&#21518;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#20294;&#19982;&#21407;&#22987;&#36755;&#20837;&#22270;&#20687;&#23436;&#20840;&#19981;&#21516;&#65292;&#23548;&#33268;&#26631;&#20934;&#31934;&#24230;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#24819;&#27861;&#26159;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#28982;&#32780;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#31105;&#27490;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#23545;&#25239;&#24341;&#23548;&#30340;&#31283;&#20581;&#21453;&#21521;&#36807;&#31243;&#65292;&#23427;&#29420;&#31435;&#20110;&#32473;&#23450;&#30340;&#39044;&#35757;&#32451;DMs&#65292;&#24182;&#19988;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;DMs&#12290;&#36825;&#31181;&#24378;&#22823;&#30340;&#24341;&#23548;&#19981;&#20165;&#21487;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#20928;&#21270;&#31034;&#20363;&#20445;&#30041;&#26356;&#22810;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#36824;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16067v1 Announce Type: cross  Abstract: Diffusion models (DMs) based adversarial purification (AP) has shown to be the most powerful alternative to adversarial training (AT). However, these methods neglect the fact that pre-trained diffusion models themselves are not robust to adversarial attacks as well. Additionally, the diffusion process can easily destroy semantic information and generate a high quality image but totally different from the original input image after the reverse process, leading to degraded standard accuracy. To overcome these issues, a natural idea is to harness adversarial training strategy to retrain or fine-tune the pre-trained diffusion model, which is computationally prohibitive. We propose a novel robust reverse process with adversarial guidance, which is independent of given pre-trained DMs and avoids retraining or fine-tuning the DMs. This robust guidance can not only ensure to generate purified examples retaining more semantic content but also m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#20445;&#25345;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#32676;&#23545;&#31216;&#24615;&#31561;&#39069;&#22806;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#23545;&#31216;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.19369</link><description>&lt;p&gt;
&#32467;&#26500;&#20445;&#25345;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Structure Preserving Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19369
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#20445;&#25345;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#32676;&#23545;&#31216;&#24615;&#31561;&#39069;&#22806;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#23545;&#31216;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#20027;&#35201;&#30340;&#20998;&#24067;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#32467;&#26500;&#20445;&#25345;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#31867;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#39069;&#22806;&#32467;&#26500;&#65288;&#22914;&#32676;&#23545;&#31216;&#24615;&#65289;&#30340;&#20998;&#24067;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#36890;&#36807;&#21046;&#23450;&#25193;&#25955;&#36716;&#25442;&#27493;&#39588;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#29702;&#35770;&#26465;&#20214;&#12290;&#38500;&#20102;&#23454;&#29616;&#31561;&#21464;&#25968;&#25454;&#37319;&#26679;&#36712;&#36857;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#23545;&#31216;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#26469;&#35828;&#26126;&#36825;&#20123;&#32467;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#22266;&#26377;&#23545;&#31216;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#35777;&#30740;&#31350;&#39564;&#35777;&#25152;&#24320;&#21457;&#30340;&#27169;&#22411;&#31526;&#21512;&#25552;&#20986;&#30340;&#29702;&#35770;&#65292;&#24182;&#22312;&#26679;&#26412;&#22343;&#31561;&#24615;&#26041;&#38754;&#33021;&#22815;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#25552;&#20986;&#30340;&#27169;&#22411;&#23454;&#29616;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#31561;&#21464;&#22270;&#20687;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19369v1 Announce Type: new  Abstract: Diffusion models have become the leading distribution-learning method in recent years. Herein, we introduce structure-preserving diffusion processes, a family of diffusion processes for learning distributions that possess additional structure, such as group symmetries, by developing theoretical conditions under which the diffusion transition steps preserve said symmetry. While also enabling equivariant data sampling trajectories, we exemplify these results by developing a collection of different symmetry equivariant diffusion models capable of learning distributions that are inherently symmetric. Empirical studies, over both synthetic and real-world datasets, are used to validate the developed models adhere to the proposed theory and are capable of achieving improved performance over existing methods in terms of sample equality. We also show how the proposed models can be used to achieve theoretically guaranteed equivariant image noise r
&lt;/p&gt;</description></item><item><title>&#20851;&#32852;&#21464;&#25442;&#22120;&#65288;AiT&#65289;&#26159;&#19968;&#31181;&#37319;&#29992;&#20302;&#31209;&#26174;&#24335;&#35760;&#24518;&#21644;&#20851;&#32852;&#35760;&#24518;&#30340;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#27169;&#22359;&#29305;&#21270;&#21644;&#27880;&#24847;&#21147;&#29942;&#39048;&#30340;&#24418;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.12862</link><description>&lt;p&gt;
&#20851;&#32852;&#21464;&#25442;&#22120;&#26159;&#19968;&#31181;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Associative Transformer Is A Sparse Representation Learner. (arXiv:2309.12862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12862
&lt;/p&gt;
&lt;p&gt;
&#20851;&#32852;&#21464;&#25442;&#22120;&#65288;AiT&#65289;&#26159;&#19968;&#31181;&#37319;&#29992;&#20302;&#31209;&#26174;&#24335;&#35760;&#24518;&#21644;&#20851;&#32852;&#35760;&#24518;&#30340;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#27169;&#22359;&#29305;&#21270;&#21644;&#27880;&#24847;&#21147;&#29942;&#39048;&#30340;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#20013;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#22522;&#20110;&#31232;&#30095;&#20132;&#20114;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36825;&#31181;&#26426;&#21046;&#19982;&#29983;&#29289;&#21407;&#29702;&#26356;&#20026;&#25509;&#36817;&#12290;&#21253;&#25324;Set Transformer&#21644;Perceiver&#22312;&#20869;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19982;&#26377;&#38480;&#33021;&#21147;&#30340;&#28508;&#22312;&#31354;&#38388;&#30456;&#32467;&#21512;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22522;&#20110;&#26368;&#36817;&#23545;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#29702;&#35770;&#21644;&#20851;&#32852;&#35760;&#24518;&#30340;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#32852;&#21464;&#25442;&#22120;&#65288;AiT&#65289;&#12290;AiT&#24341;&#20837;&#20102;&#20302;&#31209;&#26174;&#24335;&#35760;&#24518;&#65292;&#26082;&#21487;&#20197;&#20316;&#20026;&#20808;&#39564;&#26469;&#25351;&#23548;&#20849;&#20139;&#24037;&#20316;&#31354;&#38388;&#30340;&#29942;&#39048;&#27880;&#24847;&#21147;&#65292;&#21448;&#21487;&#20197;&#20316;&#20026;&#20851;&#32852;&#35760;&#24518;&#30340;&#21560;&#24341;&#23376;&#12290;&#36890;&#36807;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#36825;&#20123;&#20808;&#39564;&#33258;&#28982;&#22320;&#21457;&#23637;&#20986;&#27169;&#22359;&#30340;&#29305;&#21270;&#65292;&#27599;&#20010;&#27169;&#22359;&#23545;&#24418;&#25104;&#27880;&#24847;&#21147;&#29942;&#39048;&#30340;&#24402;&#32435;&#20559;&#22909;&#26377;&#25152;&#36129;&#29486;&#12290;&#29942;&#39048;&#21487;&#20197;&#20419;&#36827;&#36755;&#20837;&#20043;&#38388;&#20026;&#23558;&#20449;&#24687;&#20889;&#20837;&#20869;&#23384;&#32780;&#36827;&#34892;&#31454;&#20105;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;AiT&#26159;&#19968;&#31181;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging from the monolithic pairwise attention mechanism in conventional Transformer models, there is a growing interest in leveraging sparse interactions that align more closely with biological principles. Approaches including the Set Transformer and the Perceiver employ cross-attention consolidated with a latent space that forms an attention bottleneck with limited capacity. Building upon recent neuroscience studies of Global Workspace Theory and associative memory, we propose the Associative Transformer (AiT). AiT induces low-rank explicit memory that serves as both priors to guide bottleneck attention in the shared workspace and attractors within associative memory of a Hopfield network. Through joint end-to-end training, these priors naturally develop module specialization, each contributing a distinct inductive bias to form attention bottlenecks. A bottleneck can foster competition among inputs for writing information into the memory. We show that AiT is a sparse representation 
&lt;/p&gt;</description></item></channel></rss>