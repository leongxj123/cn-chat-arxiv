<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#32763;&#35793;&#22270;&#20687;&#20197;&#20351;&#20854;&#20855;&#26377;&#25991;&#21270;&#30456;&#20851;&#24615;&#65292;&#26500;&#24314;&#20102;&#35780;&#20272;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#22810;&#26041;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#21457;&#29616;&#30446;&#21069;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.01247</link><description>&lt;p&gt;
&#22270;&#29255;&#34429;&#28982;&#20195;&#34920;&#21315;&#35328;&#19975;&#35821;&#65292;&#20294;&#27599;&#20010;&#20154;&#37117;&#33021;&#21548;&#25026;&#21527;&#65311;&#20851;&#20110;&#32763;&#35793;&#20855;&#26377;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
An image speaks a thousand words, but can everyone listen? On translating images for cultural relevance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01247
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#32763;&#35793;&#22270;&#20687;&#20197;&#20351;&#20854;&#20855;&#26377;&#25991;&#21270;&#30456;&#20851;&#24615;&#65292;&#26500;&#24314;&#20102;&#35780;&#20272;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#22810;&#26041;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#21457;&#29616;&#30446;&#21069;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#20852;&#36215;&#65292;&#20154;&#31867;&#32763;&#35793;&#36234;&#26469;&#36234;&#22810;&#22320;&#20851;&#27880;&#20110;&#25991;&#21270;&#36866;&#24212;&#65292;&#19981;&#20165;&#38480;&#20110;&#25991;&#23383;&#65292;&#36824;&#21253;&#25324;&#22270;&#29255;&#31561;&#20854;&#20182;&#24418;&#24335;&#65292;&#20197;&#20256;&#36798;&#30456;&#21516;&#30340;&#21547;&#20041;&#12290;&#34429;&#28982;&#26377;&#20960;&#31181;&#24212;&#29992;&#23558;&#21463;&#30410;&#20110;&#36825;&#19968;&#28857;&#65292;&#20294;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20173;&#28982;&#23616;&#38480;&#20110;&#22788;&#29702;&#35821;&#35328;&#30340;&#21475;&#22836;&#21644;&#25991;&#23383;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#32763;&#35793;&#22270;&#20687;&#20197;&#20351;&#20854;&#20855;&#26377;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#31532;&#19968;&#27493;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19977;&#20010;&#21253;&#21547;&#26368;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#30340;&#27969;&#27700;&#32447;&#26469;&#23436;&#25104;&#36825;&#39033;&#20219;&#21153;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65306;i&#65289;&#27010;&#24565;&#65306;&#21253;&#25324;600&#24352;&#36328;&#25991;&#21270;&#36830;&#36143;&#30340;&#22270;&#20687;&#65292;&#27599;&#24352;&#22270;&#20687;&#19987;&#27880;&#20110;&#21333;&#20010;&#27010;&#24565;&#65292;ii&#65289;&#24212;&#29992;&#65306;&#21253;&#25324;&#20174;&#23454;&#38469;&#24212;&#29992;&#20013;&#31579;&#36873;&#20986;&#30340;100&#24352;&#22270;&#20687;&#12290;&#25105;&#20204;&#23545;&#32763;&#35793;&#21518;&#30340;&#22270;&#20687;&#36827;&#34892;&#20102;&#22810;&#26041;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#20854;&#25991;&#21270;&#30456;&#20851;&#24615;&#21644;&#21547;&#20041;&#20445;&#30041;&#12290;&#25105;&#20204;&#21457;&#29616;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#22312;&#36825;&#39033;&#20219;&#21153;&#19978;&#22833;&#36133;&#20102;&#65292;&#20294;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01247v1 Announce Type: new  Abstract: Given the rise of multimedia content, human translators increasingly focus on culturally adapting not only words but also other modalities such as images to convey the same meaning. While several applications stand to benefit from this, machine translation systems remain confined to dealing with language in speech and text. In this work, we take a first step towards translating images to make them culturally relevant. First, we build three pipelines comprising state-of-the-art generative models to do the task. Next, we build a two-part evaluation dataset: i) concept: comprising 600 images that are cross-culturally coherent, focusing on a single concept per image, and ii) application: comprising 100 images curated from real-world applications. We conduct a multi-faceted human evaluation of translated images to assess for cultural relevance and meaning preservation. We find that as of today, image-editing models fail at this task, but can 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MedCLIP-SAM&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;CLIP&#21644;SAM&#27169;&#22411;&#65292;&#23454;&#29616;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20020;&#24202;&#22270;&#20687;&#20998;&#21106;</title><link>https://arxiv.org/abs/2403.20253</link><description>&lt;p&gt;
MedCLIP-SAM&#65306;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#36827;&#34892;&#26725;&#25509;&#65292;&#23454;&#29616;&#36890;&#29992;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20253
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MedCLIP-SAM&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;CLIP&#21644;SAM&#27169;&#22411;&#65292;&#23454;&#29616;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20020;&#24202;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20013;&#35299;&#21078;&#32467;&#26500;&#21644;&#30149;&#21464;&#30340;&#20998;&#21106;&#22312;&#29616;&#20195;&#20020;&#24202;&#35786;&#26029;&#12289;&#30142;&#30149;&#30740;&#31350;&#21644;&#27835;&#30103;&#35268;&#21010;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;MedCLIP-SAM&#65292;&#32467;&#21512;&#20102;CLIP&#21644;SAM&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20020;&#24202;&#25195;&#25551;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20253v1 Announce Type: cross  Abstract: Medical image segmentation of anatomical structures and pathology is crucial in modern clinical diagnosis, disease study, and treatment planning. To date, great progress has been made in deep learning-based segmentation techniques, but most methods still lack data efficiency, generalizability, and interactability. Consequently, the development of new, precise segmentation methods that demand fewer labeled datasets is of utmost importance in medical image analysis. Recently, the emergence of foundation models, such as CLIP and Segment-Anything-Model (SAM), with comprehensive cross-domain representation opened the door for interactive and universal image segmentation. However, exploration of these models for data-efficient medical image segmentation is still limited, but is highly necessary. In this paper, we propose a novel framework, called MedCLIP-SAM that combines CLIP and SAM models to generate segmentation of clinical scans using t
&lt;/p&gt;</description></item><item><title>MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.19888</link><description>&lt;p&gt;
MambaMixer&#65306;&#20855;&#26377;&#21452;&#37325;&#26631;&#35760;&#21644;&#36890;&#36947;&#36873;&#25321;&#30340;&#39640;&#25928;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19888
&lt;/p&gt;
&lt;p&gt;
MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;Transformers&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#24615;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#22823;&#35268;&#27169;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26550;&#26500;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#23637;&#29616;&#20986;&#36755;&#20837;&#22823;&#23567;&#30340;&#20108;&#27425;&#26102;&#38388;&#21644;&#31354;&#38388;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#23581;&#35797;&#20026;&#22810;&#32500;&#25968;&#25454;&#35774;&#35745;&#39640;&#25928;&#26377;&#25928;&#30340;&#26550;&#26500;&#20027;&#24178;&#65292;&#20363;&#22914;&#22270;&#20687;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#35201;&#20040;&#26159;&#25968;&#25454;&#29420;&#31435;&#30340;&#65292;&#35201;&#20040;&#26080;&#27861;&#20801;&#35768;&#36328;&#32500;&#24230;&#21644;&#20869;&#37096;&#32500;&#24230;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#26368;&#36817;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#39640;&#25928;&#30828;&#20214;&#24863;&#30693;&#23454;&#29616;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#23637;&#29616;&#20986;&#20102;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#21463;&#21040;SSMs&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MambaMixer&#65292;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#36328;&#26631;&#35760;&#21644;&#36890;&#36947;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19888v1 Announce Type: cross  Abstract: Recent advances in deep learning have mainly relied on Transformers due to their data dependency and ability to learn at scale. The attention module in these architectures, however, exhibits quadratic time and space in input size, limiting their scalability for long-sequence modeling. Despite recent attempts to design efficient and effective architecture backbone for multi-dimensional data, such as images and multivariate time series, existing models are either data independent, or fail to allow inter- and intra-dimension communication. Recently, State Space Models (SSMs), and more specifically Selective State Space Models, with efficient hardware-aware implementation, have shown promising potential for long sequence modeling. Motivated by the success of SSMs, we present MambaMixer, a new architecture with data-dependent weights that uses a dual selection mechanism across tokens and channels, called Selective Token and Channel Mixer. M
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;MatchSeg&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#21644;&#32852;&#21512;&#27880;&#24847;&#21147;&#27169;&#22359;&#22686;&#24378;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#26377;&#25928;&#23454;&#29616;&#20102;&#25903;&#25345;&#38598;&#21644;&#26597;&#35810;&#38598;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.15901</link><description>&lt;p&gt;
&#36890;&#36807;&#21442;&#32771;&#22270;&#20687;&#21305;&#37197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20998;&#21106;&#65306;MatchSeg
&lt;/p&gt;
&lt;p&gt;
MatchSeg: Towards Better Segmentation via Reference Image Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15901
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;MatchSeg&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#21644;&#32852;&#21512;&#27880;&#24847;&#21147;&#27169;&#22359;&#22686;&#24378;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#26377;&#25928;&#23454;&#29616;&#20102;&#25903;&#25345;&#38598;&#21644;&#26597;&#35810;&#38598;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#32780;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#39640;&#26114;&#19988;&#32791;&#26102;&#12290;Few-shot learning&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;&#25903;&#25345;&#38598;&#65289;&#26469;&#25351;&#23548;&#39044;&#27979;&#26032;&#30340;&#12289;&#26410;&#26631;&#35760;&#22270;&#20687;&#65288;&#31216;&#20026;&#26597;&#35810;&#38598;&#65289;&#30340;&#26631;&#31614;&#65292;&#20174;&#32780;&#20811;&#26381;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#21463;&#21040;&#36825;&#19968;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MatchSeg&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#25112;&#30053;&#24615;&#21442;&#32771;&#22270;&#20687;&#21305;&#37197;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#22312;&#23450;&#20041;&#25903;&#25345;&#38598;&#26102;&#36873;&#25321;&#39640;&#24230;&#30456;&#20851;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#32852;&#21512;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#21152;&#24378;&#25903;&#25345;&#21644;&#26597;&#35810;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20419;&#36827;&#26356;&#26377;&#25928;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15901v1 Announce Type: new  Abstract: Recently, automated medical image segmentation methods based on deep learning have achieved great success. However, they heavily rely on large annotated datasets, which are costly and time-consuming to acquire. Few-shot learning aims to overcome the need for annotated data by using a small labeled dataset, known as a support set, to guide predicting labels for new, unlabeled images, known as the query set. Inspired by this paradigm, we introduce MatchSeg, a novel framework that enhances medical image segmentation through strategic reference image matching. We leverage contrastive language-image pre-training (CLIP) to select highly relevant samples when defining the support set. Additionally, we design a joint attention module to strengthen the interaction between support and query features, facilitating a more effective knowledge transfer between support and query sets. We validated our method across four public datasets. Experimental re
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;$\text{CardioVAE}_\text{X,G}$&#65289;&#65292;&#23558;&#20302;&#25104;&#26412;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#24418;&#24335;&#25972;&#21512;&#36215;&#26469;&#65292;&#24182;&#23454;&#29616;&#20102;&#20849;&#20139;&#29305;&#24449;&#21644;&#29420;&#29305;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.13658</link><description>&lt;p&gt;
&#29992;&#20110;&#20302;&#25104;&#26412;&#24515;&#33039;&#34880;&#28082;&#21160;&#21147;&#23398;&#19981;&#31283;&#23450;&#24615;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics Instability Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13658
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;$\text{CardioVAE}_\text{X,G}$&#65289;&#65292;&#23558;&#20302;&#25104;&#26412;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#24418;&#24335;&#25972;&#21512;&#36215;&#26469;&#65292;&#24182;&#23454;&#29616;&#20102;&#20849;&#20139;&#29305;&#24449;&#21644;&#29420;&#29305;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#38750;&#20405;&#20837;&#24615;&#26816;&#27979;&#24515;&#33039;&#34880;&#28082;&#21160;&#21147;&#23398;&#19981;&#31283;&#23450;&#24615;&#65288;CHDI&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#21333;&#19968;&#25968;&#25454;&#24418;&#24335;&#65292;&#22914;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;&#26631;&#35760;&#30340;&#24739;&#32773;&#25968;&#25454;&#37327;&#26377;&#38480;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#25928;&#26524;&#19981;&#20339;&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#30340;&#24120;&#35265;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#26041;&#27861;&#26469;&#30740;&#31350;CHDI&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#26114;&#36149;&#30340;&#25968;&#25454;&#24418;&#24335;&#65292;&#22914;&#24515;&#33039;MRI&#21644;&#24515;&#33039;&#36229;&#22768;&#22270;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;$\text{CardioVAE}_\text{X,G}$&#65289;&#26469;&#25972;&#21512;&#20302;&#25104;&#26412;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#24418;&#24335;&#65292;&#24182;&#22312;&#22823;&#22411;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;$\text{CardioVAE}_\text{X,G}$&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#27969;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#23398;&#20064;&#20849;&#20139;&#29305;&#24449;&#21644;&#21508;&#25968;&#25454;&#24418;&#24335;&#29420;&#26377;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;fi
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13658v1 Announce Type: new  Abstract: Recent advancements in non-invasive detection of cardiac hemodynamic instability (CHDI) primarily focus on applying machine learning techniques to a single data modality, e.g. cardiac magnetic resonance imaging (MRI). Despite their potential, these approaches often fall short especially when the size of labeled patient data is limited, a common challenge in the medical domain. Furthermore, only a few studies have explored multimodal methods to study CHDI, which mostly rely on costly modalities such as cardiac MRI and echocardiogram. In response to these limitations, we propose a novel multimodal variational autoencoder ($\text{CardioVAE}_\text{X,G}$) to integrate low-cost chest X-ray (CXR) and electrocardiogram (ECG) modalities with pre-training on a large unlabeled dataset. Specifically, $\text{CardioVAE}_\text{X,G}$ introduces a novel tri-stream pre-training strategy to learn both shared and modality-specific features, thus enabling fi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21644;&#21435;&#22122;&#25193;&#25955;&#26469;&#20934;&#30830;&#25429;&#25417;&#35299;&#21078;&#26641;&#20960;&#20309;&#21644;&#25299;&#25169;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.08974</link><description>&lt;p&gt;
&#29992;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#31070;&#32463;&#22330;&#34920;&#31034;&#35299;&#21078;&#26641;
&lt;/p&gt;
&lt;p&gt;
Representing Anatomical Trees by Denoising Diffusion of Implicit Neural Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08974
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21644;&#21435;&#22122;&#25193;&#25955;&#26469;&#20934;&#30830;&#25429;&#25417;&#35299;&#21078;&#26641;&#20960;&#20309;&#21644;&#25299;&#25169;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#21078;&#26641;&#22312;&#20020;&#24202;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35299;&#21078;&#26641;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#20960;&#20309;&#24418;&#29366;&#22810;&#26679;&#19988;&#22797;&#26434;&#65292;&#20934;&#30830;&#34920;&#31034;&#35299;&#21078;&#26641;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INRs&#65289;&#26469;&#34920;&#31034;&#35299;&#21078;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#36890;&#36807;&#22312;INR&#31354;&#38388;&#20013;&#36827;&#34892;&#21435;&#22122;&#25193;&#25955;&#26469;&#25429;&#25417;&#19968;&#32452;&#26641;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#21487;&#20197;&#22312;&#20219;&#20309;&#25152;&#38656;&#20998;&#36776;&#29575;&#19979;&#20934;&#30830;&#25429;&#25417;&#35299;&#21078;&#26641;&#30340;&#22797;&#26434;&#20960;&#20309;&#21644;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08974v1 Announce Type: cross  Abstract: Anatomical trees play a central role in clinical diagnosis and treatment planning. However, accurately representing anatomical trees is challenging due to their varying and complex topology and geometry. Traditional methods for representing tree structures, captured using medical imaging, while invaluable for visualizing vascular and bronchial networks, exhibit drawbacks in terms of limited resolution, flexibility, and efficiency. Recently, implicit neural representations (INRs) have emerged as a powerful tool for representing shapes accurately and efficiently. We propose a novel approach for representing anatomical trees using INR, while also capturing the distribution of a set of trees via denoising diffusion in the space of INRs. We accurately capture the intricate geometries and topologies of anatomical trees at any desired resolution. Through extensive qualitative and quantitative evaluation, we demonstrate high-fidelity tree reco
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.18213</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#21487;&#24494;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-objective Differentiable Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18213
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#20013;&#30340;Pareto&#21069;&#27839;&#36718;&#24275;&#21078;&#26512;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#22312;&#20687;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36825;&#26679;&#30340;&#26114;&#36149;&#30446;&#26631;&#20013;&#12290; &#30456;&#23545;&#20110;&#20256;&#32479;&#30340;NAS&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#24182;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#36229;&#32593;&#32476;&#21442;&#25968;&#21270;&#36328;&#22810;&#20010;&#35774;&#22791;&#21644;&#22810;&#20010;&#30446;&#26631;&#30340;&#32852;&#21512;&#26550;&#26500;&#20998;&#24067;&#65292;&#36229;&#32593;&#32476;&#21487;&#20197;&#26681;&#25454;&#30828;&#20214;&#29305;&#24449;&#21644;&#20559;&#22909;&#21521;&#37327;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#23454;&#29616;&#38646;&#27425;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18213v1 Announce Type: new  Abstract: Pareto front profiling in multi-objective optimization (MOO), i.e. finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives like neural network training. Typically, in MOO neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a search for each constraint. In this work, we propose a novel NAS algorithm that encodes user preferences for the trade-off between performance and hardware metrics, and yields representative and diverse architectures across multiple devices in just one search run. To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot t
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#35780;&#20272;&#20010;&#20307;&#21160;&#20316;&#27169;&#24335;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;3D&#23156;&#20799;&#21160;&#21147;&#23398;&#36827;&#34892;&#24314;&#27169;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.14400</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;3D&#23156;&#20799;&#21160;&#21147;&#23398;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14400
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#35780;&#20272;&#20010;&#20307;&#21160;&#20316;&#27169;&#24335;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;3D&#23156;&#20799;&#21160;&#21147;&#23398;&#36827;&#34892;&#24314;&#27169;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#23156;&#20799;&#31070;&#32463;&#21457;&#32946;&#35780;&#20272;&#26041;&#27861;&#23545;&#20110;&#26089;&#26399;&#21457;&#29616;&#21487;&#33021;&#38656;&#35201;&#21450;&#26102;&#24178;&#39044;&#30340;&#21307;&#23398;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#21457;&#30340;&#36816;&#21160;&#27963;&#21160;&#65292;&#21363;&#8220;&#21160;&#21147;&#23398;&#8221;&#65292;&#34987;&#35777;&#26126;&#21487;&#25552;&#20379;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#39044;&#27979;&#26410;&#26469;&#31070;&#32463;&#21457;&#32946;&#30340;&#26367;&#20195;&#24615;&#27979;&#37327;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#35780;&#20272;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#23450;&#24615;&#21644;&#20027;&#35266;&#30340;&#65292;&#20391;&#37325;&#20110;&#23545;&#36890;&#36807;&#35270;&#35273;&#35782;&#21035;&#30340;&#29305;&#23450;&#24180;&#40836;&#25163;&#21183;&#30340;&#25551;&#36848;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#26681;&#25454;&#25968;&#25454;&#39537;&#21160;&#35780;&#20272;&#20010;&#20307;&#21160;&#20316;&#27169;&#24335;&#26469;&#39044;&#27979;&#23156;&#20799;&#31070;&#32463;&#21457;&#32946;&#25104;&#29087;&#12290;&#25105;&#20204;&#21033;&#29992;&#22788;&#29702;&#36807;&#30340;3D&#23156;&#20799;&#35270;&#39057;&#24405;&#20687;&#36827;&#34892;&#23039;&#21183;&#20272;&#35745;&#65292;&#25552;&#21462;&#35299;&#21078;&#26631;&#24535;&#29289;&#30340;&#26102;&#31354;&#31995;&#21015;&#65292;&#24182;&#24212;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#39044;&#27979;&#23454;&#38469;&#24180;&#40836;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#20110;&#25163;&#21160;&#35774;&#35745;&#29305;&#24449;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#22522;&#32447;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14400v1 Announce Type: cross  Abstract: Reliable methods for the neurodevelopmental assessment of infants are essential for early detection of medical issues that may need prompt interventions. Spontaneous motor activity, or `kinetics', is shown to provide a powerful surrogate measure of upcoming neurodevelopment. However, its assessment is by and large qualitative and subjective, focusing on visually identified, age-specific gestures. Here, we follow an alternative approach, predicting infants' neurodevelopmental maturation based on data-driven evaluation of individual motor patterns. We utilize 3D video recordings of infants processed with pose-estimation to extract spatio-temporal series of anatomical landmarks, and apply adaptive graph convolutional networks to predict the actual age. We show that our data-driven approach achieves improvement over traditional machine learning baselines based on manually engineered features.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#65288;PST&#65289;&#26694;&#26550;&#65292;&#22312;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20013;&#25506;&#31350;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35780;&#20272;&#20102;DALLE-3&#22312;&#24615;&#21035;&#32844;&#19994;&#21644;&#32452;&#32455;&#26435;&#21147;&#26041;&#38754;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.11089</link><description>&lt;p&gt;
&#30007;&#24615;CEO&#21644;&#22899;&#24615;&#21161;&#29702;&#65306;&#36890;&#36807;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#25506;&#31350;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
The Male CEO and the Female Assistant: Probing Gender Biases in Text-To-Image Models Through Paired Stereotype Test
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11089
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#65288;PST&#65289;&#26694;&#26550;&#65292;&#22312;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20013;&#25506;&#31350;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35780;&#20272;&#20102;DALLE-3&#22312;&#24615;&#21035;&#32844;&#19994;&#21644;&#32452;&#32455;&#26435;&#21147;&#26041;&#38754;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#65288;&#22914;DALLE-3&#65289;&#23637;&#31034;&#20102;&#22312;&#26032;&#24212;&#29992;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20063;&#38754;&#20020;&#21069;&#25152;&#26410;&#26377;&#30340;&#20844;&#24179;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#21333;&#20154;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;T2I&#27169;&#22411;&#24212;&#29992;&#21487;&#33021;&#38656;&#35201;&#21516;&#26102;&#25551;&#32472;&#20004;&#20010;&#25110;&#26356;&#22810;&#20154;&#12290;&#35813;&#35774;&#23450;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#20173;&#26410;&#34987;&#25506;&#31350;&#65292;&#23548;&#33268;&#20351;&#29992;&#20013;&#30340;&#20844;&#24179;&#30456;&#20851;&#39118;&#38505;&#12290;&#20026;&#20102;&#30740;&#31350;T2I&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#22522;&#26412;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#65288;PST&#65289;&#20559;&#35265;&#35780;&#20272;&#26694;&#26550;&#12290;PST&#20419;&#20351;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#22270;&#20687;&#20013;&#30340;&#20004;&#20010;&#20010;&#20307;&#65292;&#29992;&#19982;&#30456;&#21453;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#30456;&#20851;&#32852;&#30340;&#20004;&#20010;&#31038;&#20250;&#36523;&#20221;&#26469;&#25551;&#36848;&#20182;&#20204;&#12290;&#36890;&#36807;&#29983;&#25104;&#30340;&#22270;&#20687;&#36981;&#20174;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#30340;&#31243;&#24230;&#26469;&#34913;&#37327;&#20559;&#35265;&#12290;&#21033;&#29992;PST&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#35780;&#20272;DALLE-3&#65306;&#24615;&#21035;&#32844;&#19994;&#20013;&#30340;&#20559;&#35265;&#21644;&#32452;&#32455;&#26435;&#21147;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11089v1 Announce Type: cross  Abstract: Recent large-scale Text-To-Image (T2I) models such as DALLE-3 demonstrate great potential in new applications, but also face unprecedented fairness challenges. Prior studies revealed gender biases in single-person image generation, but T2I model applications might require portraying two or more people simultaneously. Potential biases in this setting remain unexplored, leading to fairness-related risks in usage. To study these underlying facets of gender biases in T2I models, we propose a novel Paired Stereotype Test (PST) bias evaluation framework. PST prompts the model to generate two individuals in the same image. They are described with two social identities that are stereotypically associated with the opposite gender. Biases can then be measured by the level of conformation to gender stereotypes in generated images. Using PST, we evaluate DALLE-3 from 2 perspectives: biases in gendered occupation and biases in organizational power.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20998;&#21106;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#35299;&#21078;&#21487;&#25511;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#27169;&#28040;&#34701;&#35757;&#32451;&#31639;&#27861;&#23454;&#29616;&#23545;&#35299;&#21078;&#32422;&#26463;&#30340;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#32593;&#32476;&#23545;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05210</link><description>&lt;p&gt;
&#37319;&#29992;&#20998;&#21106;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#35299;&#21078;&#21487;&#25511;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20998;&#21106;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#35299;&#21078;&#21487;&#25511;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#27169;&#28040;&#34701;&#35757;&#32451;&#31639;&#27861;&#23454;&#29616;&#23545;&#35299;&#21078;&#32422;&#26463;&#30340;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#32593;&#32476;&#23545;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#23454;&#29616;&#20102;&#38750;&#24120;&#39640;&#36136;&#37327;&#30340;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#65292;&#21487;&#20197;&#36890;&#36807;&#20026;&#23567;&#22411;&#25110;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#25552;&#20379;&#34917;&#20805;&#65292;&#20174;&#32780;&#24110;&#21161;&#20943;&#36731;&#33719;&#21462;&#21644;&#27880;&#37322;&#26032;&#22270;&#20687;&#30340;&#36153;&#29992;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#22270;&#20687;&#26102;&#38754;&#20020;&#30528;&#20840;&#23616;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#21078;&#21487;&#25511;&#30340;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#20013;&#36981;&#24490;&#22810;&#31867;&#35299;&#21078;&#20998;&#21106;&#25513;&#27169;&#65292;&#24182;&#37319;&#29992;&#38543;&#26426;&#25513;&#27169;&#28040;&#34701;&#35757;&#32451;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#25152;&#36873;&#35299;&#21078;&#32422;&#26463;&#30340;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#20801;&#35768;&#20854;&#20182;&#35299;&#21078;&#21306;&#22495;&#30340;&#28789;&#27963;&#24615;&#12290;&#36825;&#20063;&#25913;&#21892;&#20102;&#32593;&#32476;&#22312;&#23436;&#20840;&#26080;&#26465;&#20214;&#65288;&#26080;&#32422;&#26463;&#29983;&#25104;&#65289;&#24773;&#20917;&#19979;&#23545;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#20083;&#33146;MRI&#21644;&#33145;&#37096;/&#39048;&#37096;&#21040;&#30406;&#33108;CT&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#22312;&#35299;&#21078;&#30495;&#23454;&#24615;&#21644;&#36755;&#20837;&#25513;&#27169;&#20445;&#30495;&#24230;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have enabled remarkably high-quality medical image generation, which can help mitigate the expenses of acquiring and annotating new images by supplementing small or imbalanced datasets, along with other applications. However, these are hampered by the challenge of enforcing global anatomical realism in generated images. To this end, we propose a diffusion model for anatomically-controlled medical image generation. Our model follows a multi-class anatomical segmentation mask at each sampling step and incorporates a \textit{random mask ablation} training algorithm, to enable conditioning on a selected combination of anatomical constraints while allowing flexibility in other anatomical areas. This also improves the network's learning of anatomical realism for the completely unconditional (unconstrained generation) case. Comparative evaluation on breast MRI and abdominal/neck-to-pelvis CT datasets demonstrates superior anatomical realism and input mask faithfulness over st
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#30690;&#37327;&#65292;&#24182;&#20351;&#29992;&#21333;&#20010;&#22270;&#21367;&#31215;&#23618;&#36827;&#34892;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00564</link><description>&lt;p&gt;
&#19968;&#27425;&#22270;&#21367;&#31215;&#23601;&#22815;&#20102;&#65306;&#39640;&#25928;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Single Graph Convolution Is All You Need: Efficient Grayscale Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#30690;&#37327;&#65292;&#24182;&#20351;&#29992;&#21333;&#20010;&#22270;&#21367;&#31215;&#23618;&#36827;&#34892;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#22120;&#36890;&#24120;&#20381;&#36182;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26469;&#23436;&#25104;&#20219;&#21153;&#65292;&#32780;CNN&#30456;&#27604;&#20110;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#26356;&#21152;&#24222;&#22823;&#65292;&#36825;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#24102;&#26469;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#36866;&#29992;&#20110;RGB&#21644;&#28784;&#24230;&#25968;&#25454;&#38598;&#65292;&#20294;&#20165;&#20165;&#20351;&#29992;&#28784;&#24230;&#22270;&#20687;&#30340;&#20998;&#31867;&#22120;&#30456;&#23545;&#36739;&#23569;&#35265;&#12290;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;(SAR)&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;(ATR)&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#20687;&#30340;&#30690;&#37327;&#21270;&#35270;&#22270;&#30340;&#26032;&#22411;&#28784;&#24230;(&#21333;&#36890;&#36947;)&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#30690;&#37327;&#65292;&#24182;&#23558;&#38382;&#39064;&#35774;&#32622;&#20026;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;MLP&#30340;&#36731;&#37327;&#32423;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25209;&#27425;&#32423;&#21035;&#20351;&#29992;&#21333;&#20010;&#22270;&#21367;&#31215;&#23618;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#24182;&#20943;&#23567;&#24615;&#33021;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#23450;&#21046;&#30340;&#20934;&#30830;&#29575;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image classifiers often rely on convolutional neural networks (CNN) for their tasks, which are inherently more heavyweight than multilayer perceptrons (MLPs), which can be problematic in real-time applications. Additionally, many image classification models work on both RGB and grayscale datasets. Classifiers that operate solely on grayscale images are much less common. Grayscale image classification has diverse applications, including but not limited to medical image classification and synthetic aperture radar (SAR) automatic target recognition (ATR). Thus, we present a novel grayscale (single channel) image classification approach using a vectorized view of images. We exploit the lightweightness of MLPs by viewing images as a vector and reducing our problem setting to the grayscale image classification setting. We find that using a single graph convolutional layer batch-wise increases accuracy and reduces variance in the performance of our model. Moreover, we develop a customized acc
&lt;/p&gt;</description></item><item><title>MapGPT&#24341;&#20837;&#20102;&#22312;&#32447;&#35821;&#35328;&#24418;&#25104;&#30340;&#22320;&#22270;&#65292;&#24110;&#21161;GPT&#29702;&#35299;&#25972;&#20307;&#29615;&#22659;&#65292;&#25552;&#20986;&#33258;&#36866;&#24212;&#35268;&#21010;&#26426;&#21046;&#20197;&#21327;&#21161;&#20195;&#29702;&#25191;&#34892;&#22810;&#27493;&#36335;&#24452;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2401.07314</link><description>&lt;p&gt;
MapGPT&#65306;&#20855;&#26377;&#33258;&#36866;&#24212;&#36335;&#24452;&#35268;&#21010;&#30340;&#22320;&#22270;&#24341;&#23548;&#25552;&#31034;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07314
&lt;/p&gt;
&lt;p&gt;
MapGPT&#24341;&#20837;&#20102;&#22312;&#32447;&#35821;&#35328;&#24418;&#25104;&#30340;&#22320;&#22270;&#65292;&#24110;&#21161;GPT&#29702;&#35299;&#25972;&#20307;&#29615;&#22659;&#65292;&#25552;&#20986;&#33258;&#36866;&#24212;&#35268;&#21010;&#26426;&#21046;&#20197;&#21327;&#21161;&#20195;&#29702;&#25191;&#34892;&#22810;&#27493;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;GPT&#20316;&#20026;&#22823;&#33041;&#30340;&#20307;&#39564;&#20195;&#29702;&#34920;&#29616;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#38750;&#20961;&#20915;&#31574;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#38646;-shot&#20195;&#29702;&#21482;&#20419;&#20351;GPT-4&#22312;&#23616;&#37096;&#29615;&#22659;&#20013;&#36873;&#25321;&#28508;&#22312;&#20301;&#32622;&#65292;&#32780;&#27809;&#26377;&#20026;&#20195;&#29702;&#26500;&#24314;&#19968;&#20010;&#26377;&#25928;&#30340;&#8220;&#20840;&#23616;&#35270;&#22270;&#8221;&#26469;&#29702;&#35299;&#25972;&#20307;&#29615;&#22659;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22320;&#22270;&#24341;&#23548;&#30340;&#22522;&#20110;GPT&#30340;&#20195;&#29702;&#65292;&#21517;&#20026;MapGPT&#65292;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#32447;&#35821;&#35328;&#24418;&#25104;&#30340;&#22320;&#22270;&#26469;&#40723;&#21169;&#20840;&#23616;&#25506;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22312;&#32447;&#22320;&#22270;&#65292;&#24182;&#23558;&#20854;&#21512;&#24182;&#21040;&#21253;&#21547;&#33410;&#28857;&#20449;&#24687;&#21644;&#25299;&#25169;&#20851;&#31995;&#30340;&#25552;&#31034;&#20013;&#65292;&#20197;&#24110;&#21161;GPT&#29702;&#35299;&#31354;&#38388;&#29615;&#22659;&#12290;&#20174;&#36825;&#19968;&#35774;&#35745;&#20013;&#33719;&#30410;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35268;&#21010;&#26426;&#21046;&#65292;&#20197;&#24110;&#21161;&#20195;&#29702;&#26681;&#25454;&#22320;&#22270;&#25191;&#34892;&#22810;&#27493;&#35268;&#21010;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#22810;&#20010;&#20505;&#36873;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07314v2 Announce Type: replace  Abstract: Embodied agents equipped with GPT as their brain have exhibited extraordinary decision-making and generalization abilities across various tasks. However, existing zero-shot agents for vision-and-language navigation (VLN) only prompt the GPT-4 to select potential locations within localized environments, without constructing an effective "global-view" for the agent to understand the overall environment. In this work, we present a novel map-guided GPT-based agent, dubbed MapGPT, which introduces an online linguistic-formed map to encourage the global exploration. Specifically, we build an online map and incorporate it into the prompts that include node information and topological relationships, to help GPT understand the spatial environment. Benefiting from this design, we further propose an adaptive planning mechanism to assist the agent in performing multi-step path planning based on a map, systematically exploring multiple candidate 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25193;&#23637;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#33267;&#38477;&#27700;&#36229;&#20998;&#36776;&#29575;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30830;&#23450;&#24615;&#38477;&#23610;&#24230;&#22120;&#21644;&#26242;&#26102;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#25429;&#25417;&#22122;&#22768;&#29305;&#24449;&#21644;&#39640;&#39057;&#29575;&#27169;&#24335;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.06071</link><description>&lt;p&gt;
&#20855;&#26377;&#26102;&#31354;&#35270;&#39057;&#25193;&#25955;&#30340;&#38477;&#27700;&#38477;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
Precipitation Downscaling with Spatiotemporal Video Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06071
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#23637;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#33267;&#38477;&#27700;&#36229;&#20998;&#36776;&#29575;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30830;&#23450;&#24615;&#38477;&#23610;&#24230;&#22120;&#21644;&#26242;&#26102;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#25429;&#25417;&#22122;&#22768;&#29305;&#24449;&#21644;&#39640;&#39057;&#29575;&#27169;&#24335;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27668;&#20505;&#31185;&#23398;&#21644;&#27668;&#35937;&#23398;&#39046;&#22495;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#23616;&#37096;&#38477;&#27700;&#65288;&#38632;&#38634;&#65289;&#39044;&#27979;&#21463;&#21040;&#22522;&#20110;&#27169;&#25311;&#26041;&#27861;&#30340;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#12290;&#32479;&#35745;&#38477;&#23610;&#24230;&#65292;&#25110;&#32773;&#31216;&#20026;&#36229;&#20998;&#36776;&#29575;&#65292;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#20854;&#20013;&#20302;&#20998;&#36776;&#29575;&#39044;&#27979;&#36890;&#36807;&#32479;&#35745;&#26041;&#27861;&#24471;&#21040;&#25913;&#36827;&#12290;&#19982;&#20256;&#32479;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19981;&#21516;&#65292;&#22825;&#27668;&#21644;&#27668;&#20505;&#24212;&#29992;&#38656;&#35201;&#25429;&#25417;&#32473;&#23450;&#20302;&#20998;&#36776;&#29575;&#27169;&#24335;&#30340;&#39640;&#20998;&#36776;&#29575;&#30340;&#20934;&#30830;&#26465;&#20214;&#20998;&#24067;&#65292;&#20197;&#30830;&#20445;&#21487;&#38752;&#30340;&#38598;&#21512;&#24179;&#22343;&#21644;&#26497;&#31471;&#20107;&#20214;&#65288;&#22914;&#26292;&#38632;&#65289;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;&#26412;&#30740;&#31350;&#23558;&#26368;&#26032;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#25193;&#23637;&#21040;&#38477;&#27700;&#36229;&#20998;&#36776;&#29575;&#65292;&#20351;&#29992;&#30830;&#23450;&#24615;&#38477;&#23610;&#24230;&#22120;&#65292;&#28982;&#21518;&#26159;&#26242;&#26102;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#25429;&#25417;&#22122;&#22768;&#29305;&#24449;&#21644;&#39640;&#39057;&#29575;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;FV3GFS&#36755;&#20986;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#24050;&#24314;&#31435;&#30340;&#22823;&#35268;&#27169;&#20840;&#29699;&#22823;&#27668;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06071v2 Announce Type: replace-cross  Abstract: In climate science and meteorology, high-resolution local precipitation (rain and snowfall) predictions are limited by the computational costs of simulation-based methods. Statistical downscaling, or super-resolution, is a common workaround where a low-resolution prediction is improved using statistical approaches. Unlike traditional computer vision tasks, weather and climate applications require capturing the accurate conditional distribution of high-resolution given low-resolution patterns to assure reliable ensemble averages and unbiased estimates of extreme events, such as heavy rain. This work extends recent video diffusion models to precipitation super-resolution, employing a deterministic downscaler followed by a temporally-conditioned diffusion model to capture noise characteristics and high-frequency patterns. We test our approach on FV3GFS output, an established large-scale global atmosphere model, and compare it agai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;3D&#33041;&#34880;&#31649;&#27880;&#24847;&#21147;UNet&#26041;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#25552;&#21462;&#33041;&#34880;&#31649;&#22270;&#20687;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#28145;&#24230;&#30417;&#30563;UNet&#26469;&#25913;&#21892;&#33041;&#34880;&#31649;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#65292;&#26377;&#21161;&#20110;&#39044;&#38450;&#20013;&#39118;&#12290;</title><link>https://arxiv.org/abs/2311.10224</link><description>&lt;p&gt;
CV-Attention UNet: Attention-based UNet for 3D Cerebrovascular Segmentation of Enhanced TOF-MRA Images
&lt;/p&gt;
&lt;p&gt;
CV-Attention UNet: Attention-based UNet for 3D Cerebrovascular Segmentation of Enhanced TOF-MRA Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;3D&#33041;&#34880;&#31649;&#27880;&#24847;&#21147;UNet&#26041;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#25552;&#21462;&#33041;&#34880;&#31649;&#22270;&#20687;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#28145;&#24230;&#30417;&#30563;UNet&#26469;&#25913;&#21892;&#33041;&#34880;&#31649;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#65292;&#26377;&#21161;&#20110;&#39044;&#38450;&#20013;&#39118;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#35786;&#26029;&#33041;&#34880;&#31649;&#30142;&#30149;&#65292;&#26102;&#38388;&#39134;&#34892;&#30913;&#20849;&#25391;&#34880;&#31649;&#36896;&#24433;&#22270;&#65288;TOF-MRA&#65289;&#36890;&#24120;&#26159;&#36890;&#36807;&#30446;&#35270;&#35780;&#20272;&#65292;&#36825;&#23548;&#33268;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;3D&#33041;&#34880;&#31649;&#27880;&#24847;&#21147;UNet&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;CV-Attention UNet&#65292;&#29992;&#20110;&#31934;&#30830;&#25552;&#21462;&#33041;&#34880;&#31649;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#38543;&#21518;&#37319;&#29992;&#28145;&#24230;&#30417;&#30563;UNet&#26469;&#25913;&#21892;&#33041;&#34880;&#31649;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#39044;&#38450;&#20013;&#39118;&#12290;&#20026;&#20102;&#32467;&#21512;&#20302;&#35821;&#20041;&#21644;&#39640;&#35821;&#20041;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10224v2 Announce Type: replace-cross  Abstract: Due to the lack of automated methods, to diagnose cerebrovascular disease, time-of-flight magnetic resonance angiography (TOF-MRA) is assessed visually, making it time-consuming. The commonly used encoder-decoder architectures for cerebrovascular segmentation utilize redundant features, eventually leading to the extraction of low-level features multiple times. Additionally, convolutional neural networks (CNNs) suffer from performance degradation when the batch size is small, and deeper networks experience the vanishing gradient problem. Methods: In this paper, we attempt to solve these limitations and propose the 3D cerebrovascular attention UNet method, named CV-AttentionUNet, for precise extraction of brain vessel images. We proposed a sequence of preprocessing techniques followed by deeply supervised UNet to improve the accuracy of segmentation of the brain vessels leading to a stroke. To combine the low and high semantics, 
&lt;/p&gt;</description></item><item><title>M2CURL&#26159;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20174;&#35270;&#35302;&#35273;&#25968;&#25454;&#20013;&#23398;&#20064;&#20986;&#39640;&#25928;&#30340;&#34920;&#31034;&#65292;&#24182;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2401.17032</link><description>&lt;p&gt;
M2CURL: &#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#24378;&#21270;&#23398;&#20064;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
M2CURL: Sample-Efficient Multimodal Reinforcement Learning via Self-Supervised Representation Learning for Robotic Manipulation. (arXiv:2401.17032v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17032
&lt;/p&gt;
&lt;p&gt;
M2CURL&#26159;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20174;&#35270;&#35302;&#35273;&#25968;&#25454;&#20013;&#23398;&#20064;&#20986;&#39640;&#25928;&#30340;&#34920;&#31034;&#65292;&#24182;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26041;&#38754;&#20043;&#19968;&#26159;&#26377;&#25928;&#22320;&#25972;&#21512;&#19981;&#21516;&#30340;&#35266;&#27979;&#27169;&#24577;&#12290;&#20174;&#36825;&#20123;&#27169;&#24577;&#20013;&#24471;&#21040;&#31283;&#20581;&#20934;&#30830;&#30340;&#34920;&#31034;&#23545;&#20110;&#25552;&#21319;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#35270;&#35302;&#35273;&#25968;&#25454;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#23398;&#20064;&#34920;&#31034;&#38754;&#20020;&#30528;&#37325;&#35201;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#25968;&#25454;&#30340;&#39640;&#32500;&#24230;&#21644;&#23558;&#35270;&#35302;&#35273;&#36755;&#20837;&#19982;&#21160;&#24577;&#29615;&#22659;&#21644;&#20219;&#21153;&#30446;&#26631;&#36827;&#34892;&#30456;&#20851;&#24615;&#20998;&#26512;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#23545;&#27604;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65288;M2CURL&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#23398;&#20064;&#20986;&#39640;&#25928;&#30340;&#34920;&#31034;&#24182;&#21152;&#36895;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26080;&#20851;&#65292;&#22240;&#27492;&#21487;&#20197;&#19982;&#20219;&#20309;&#21487;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#25972;&#21512;&#12290;&#25105;&#20204;&#22312;Tactile Gym 2&#27169;&#25311;&#22120;&#19978;&#35780;&#20272;&#20102;M2CURL&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most critical aspects of multimodal Reinforcement Learning (RL) is the effective integration of different observation modalities. Having robust and accurate representations derived from these modalities is key to enhancing the robustness and sample efficiency of RL algorithms. However, learning representations in RL settings for visuotactile data poses significant challenges, particularly due to the high dimensionality of the data and the complexity involved in correlating visual and tactile inputs with the dynamic environment and task objectives. To address these challenges, we propose Multimodal Contrastive Unsupervised Reinforcement Learning (M2CURL). Our approach employs a novel multimodal self-supervised learning technique that learns efficient representations and contributes to faster convergence of RL algorithms. Our method is agnostic to the RL algorithm, thus enabling its integration with any available RL algorithm. We evaluate M2CURL on the Tactile Gym 2 simulator 
&lt;/p&gt;</description></item><item><title>MISS&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#24494;&#35843;&#26041;&#27861;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#25105;&#20204;&#25226;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20316;&#20026;&#19968;&#20010;&#29983;&#25104;&#24335;&#20219;&#21153;&#22788;&#29702;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20351;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21333;&#27169;&#24577;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36716;&#25442;&#21644;&#23383;&#24149;&#26041;&#27861;&#23454;&#29616;&#29305;&#24449;&#31354;&#38388;&#30340;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.05163</link><description>&lt;p&gt;
MISS&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MISS: A Generative Pretraining and Finetuning Approach for Med-VQA. (arXiv:2401.05163v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05163
&lt;/p&gt;
&lt;p&gt;
MISS&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#24494;&#35843;&#26041;&#27861;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#25105;&#20204;&#25226;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20316;&#20026;&#19968;&#20010;&#29983;&#25104;&#24335;&#20219;&#21153;&#22788;&#29702;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20351;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21333;&#27169;&#24577;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36716;&#25442;&#21644;&#23383;&#24149;&#26041;&#27861;&#23454;&#29616;&#29305;&#24449;&#31354;&#38388;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20854;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22810;&#25968;&#26041;&#27861;&#23558;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#35270;&#20026;&#19968;&#20010;&#38590;&#20197;&#36716;&#31227;&#21040;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#30340;&#31572;&#26696;&#20998;&#31867;&#20219;&#21153;&#12290;&#21478;&#22806;&#65292;&#30001;&#20110;&#21307;&#23398;&#22270;&#20687;&#30340;&#38544;&#31169;&#24615;&#21644;&#26114;&#36149;&#30340;&#27880;&#37322;&#36807;&#31243;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#21307;&#23398;&#22270;&#25991;&#23545;&#25968;&#25454;&#38598;&#20005;&#37325;&#32570;&#20047;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#65288;MISS&#65289;&#26694;&#26550;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#35270;&#20026;&#19968;&#39033;&#29983;&#25104;&#24335;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20351;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25193;&#23637;&#21333;&#27169;&#24577;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36716;&#25442;&#21644;&#23383;&#24149;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#29305;&#24449;&#31354;&#38388;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical visual question answering (VQA) is a challenging multimodal task, where Vision-Language Pre-training (VLP) models can effectively improve the generalization performance. However, most methods in the medical field treat VQA as an answer classification task which is difficult to transfer to practical application scenarios. Additionally, due to the privacy of medical images and the expensive annotation process, large-scale medical image-text pairs datasets for pretraining are severely lacking. In this paper, we propose a large-scale MultI-task Self-Supervised learning based framework (MISS) for medical VQA tasks. Unlike existing methods, we treat medical VQA as a generative task. We unify the text encoder and multimodal encoder and align image-text features through multi-task learning. Furthermore, we propose a Transfer-and-Caption method that extends the feature space of single-modal image datasets using large language models (LLMs), enabling those traditional medical vision fiel
&lt;/p&gt;</description></item><item><title>RSAdapter&#26159;&#19968;&#31181;&#38024;&#23545;&#36965;&#24863;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#36890;&#36807;&#24182;&#34892;&#36866;&#37197;&#22120;&#21644;&#32447;&#24615;&#36716;&#25442;&#23618;&#30340;&#35774;&#35745;&#65292;&#25552;&#39640;&#20102;&#36816;&#34892;&#26102;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.13120</link><description>&lt;p&gt;
RSAdapter: &#36866;&#24212;&#36965;&#24863;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RSAdapter: Adapting Multimodal Models for Remote Sensing Visual Question Answering. (arXiv:2310.13120v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13120
&lt;/p&gt;
&lt;p&gt;
RSAdapter&#26159;&#19968;&#31181;&#38024;&#23545;&#36965;&#24863;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#36890;&#36807;&#24182;&#34892;&#36866;&#37197;&#22120;&#21644;&#32447;&#24615;&#36716;&#25442;&#23618;&#30340;&#35774;&#35745;&#65292;&#25552;&#39640;&#20102;&#36816;&#34892;&#26102;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22522;&#20110;Transformer&#30340;&#22810;&#27169;&#24577;&#26550;&#26500;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#22270;&#20687;&#25551;&#36848;&#12289;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#21644;&#22270;&#20687;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36965;&#24863;VQA&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#36164;&#28304;&#23494;&#38598;&#22411;&#25216;&#26415;&#65292;&#22914;&#23545;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#24494;&#35843;&#25110;&#20174;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#25552;&#21462;&#22270;&#20687;-&#25991;&#26412;&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#35299;&#30721;&#22120;&#36827;&#34892;&#27169;&#24577;&#34701;&#21512;&#12290;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24403;&#25968;&#37327;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RSAdapter&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#36816;&#34892;&#26102;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;RSAdapter&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#24182;&#34892;&#36866;&#37197;&#22120;&#21644;&#25554;&#20837;&#22312;&#36866;&#37197;&#22120;&#30340;&#27599;&#20010;&#20840;&#36830;&#25509;&#65288;FC&#65289;&#23618;&#21518;&#30340;&#39069;&#22806;&#32447;&#24615;&#36716;&#25442;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, with the rapid advancement of transformer models, transformer-based multimodal architectures have found wide application in various downstream tasks, including but not limited to Image Captioning, Visual Question Answering (VQA), and Image-Text Generation. However, contemporary approaches to Remote Sensing (RS) VQA often involve resource-intensive techniques, such as full fine-tuning of large models or the extraction of image-text features from pre-trained multimodal models, followed by modality fusion using decoders. These approaches demand significant computational resources and time, and a considerable number of trainable parameters are introduced. To address these challenges, we introduce a novel method known as RSAdapter, which prioritizes runtime and parameter efficiency. RSAdapter comprises two key components: the Parallel Adapter and an additional linear transformation layer inserted after each fully connected (FC) layer within the Adapter. This approach not on
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;&#30340;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;ACE&#25968;&#25454;&#38598;&#20197;&#21450;&#30456;&#24212;&#30340;&#20195;&#30721;&#12290;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#22270;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#19981;&#21516;&#25968;&#25454;&#22495;&#20013;&#30340;&#20154;&#31867;&#21160;&#20316;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.06219</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#38142;&#25509;&#39044;&#27979;&#22312;&#29983;&#27963;&#26041;&#24335;vlog&#20013;&#30340;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;
&lt;/p&gt;
&lt;p&gt;
Human Action Co-occurrence in Lifestyle Vlogs using Graph Link Prediction. (arXiv:2309.06219v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06219
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;&#30340;&#20219;&#21153;&#65292;&#24182;&#21019;&#24314;&#20102;ACE&#25968;&#25454;&#38598;&#20197;&#21450;&#30456;&#24212;&#30340;&#20195;&#30721;&#12290;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#22270;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#25429;&#25417;&#19981;&#21516;&#25968;&#25454;&#22495;&#20013;&#30340;&#20154;&#31867;&#21160;&#20316;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21160;&#20316;&#20849;&#29616;&#30340;&#20219;&#21153;&#65292;&#21363;&#30830;&#23450;&#20004;&#20010;&#20154;&#31867;&#21160;&#20316;&#26159;&#21542;&#21487;&#20197;&#22312;&#21516;&#19968;&#26102;&#38388;&#38388;&#38548;&#20869;&#20849;&#29616;&#12290;&#25105;&#20204;&#21019;&#24314;&#24182;&#20844;&#24320;&#20102;ACE&#65288;Action Co-occurrencE&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#32422;12k&#20010;&#20849;&#29616;&#30340;&#35270;&#35273;&#21160;&#20316;&#23545;&#21644;&#23427;&#20204;&#23545;&#24212;&#30340;&#35270;&#39057;&#29255;&#27573;&#32452;&#25104;&#30340;&#22823;&#22411;&#22270;&#24418;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#26469;&#33258;&#21160;&#25512;&#26029;&#20004;&#20010;&#21160;&#20316;&#26159;&#21542;&#20849;&#29616;&#30340;&#22270;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#24418;&#29305;&#21035;&#36866;&#21512;&#25429;&#25417;&#20154;&#31867;&#21160;&#20316;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#25152;&#23398;&#20064;&#30340;&#22270;&#24418;&#34920;&#31034;&#23545;&#20110;&#25105;&#20204;&#30340;&#20219;&#21153;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#22495;&#20013;&#25429;&#25417;&#21040;&#26032;&#39062;&#32780;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;ACE&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;https://github.com/MichiganNLP/vlog_action_co-occurrence&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the task of automatic human action co-occurrence identification, i.e., determine whether two human actions can co-occur in the same interval of time. We create and make publicly available the ACE (Action Co-occurrencE) dataset, consisting of a large graph of ~12k co-occurring pairs of visual actions and their corresponding video clips. We describe graph link prediction models that leverage visual and textual information to automatically infer if two actions are co-occurring. We show that graphs are particularly well suited to capture relations between human actions, and the learned graph representations are effective for our task and capture novel and relevant information across different data domains. The ACE dataset and the code introduced in this paper are publicly available at https://github.com/MichiganNLP/vlog_action_co-occurrence.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2308.07706</link><description>&lt;p&gt;
&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#25506;&#32034;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models. (arXiv:2308.07706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21508;&#31181;&#20020;&#24202;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#25972;&#21512;&#25991;&#26412;&#25351;&#23548;&#20197;&#22686;&#24378;&#35270;&#35273;&#29305;&#24449;&#20173;&#28982;&#26159;&#19968;&#20010;&#36827;&#23637;&#26377;&#38480;&#30340;&#39046;&#22495;&#12290;&#29616;&#26377;&#21033;&#29992;&#25991;&#26412;&#25351;&#23548;&#30340;&#20998;&#21106;&#27169;&#22411;&#20027;&#35201;&#22312;&#24320;&#25918;&#39046;&#22495;&#22270;&#20687;&#19978;&#35757;&#32451;&#65292;&#36825;&#24341;&#21457;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#30452;&#25509;&#24212;&#29992;&#30340;&#38590;&#39064;&#65292;&#38656;&#35201;&#25163;&#21160;&#20171;&#20837;&#25110;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20174;&#22270;&#20687;&#25551;&#36848;&#21644;&#22270;&#20687;&#20013;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#24471;&#33021;&#22815;&#23545;&#22810;&#26679;&#21270;&#30340;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#12290;&#35813;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#20197;&#35780;&#20272;&#20854;&#20174;&#24320;&#25918;&#39046;&#22495;&#21521;&#21307;&#23398;&#39046;&#22495;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#20013;&#20197;&#21069;&#26410;&#35265;&#22270;&#20687;&#30340;&#22270;&#20687;&#25551;&#36848;&#24341;&#20837;&#20102;&#21464;&#21270;&#65292;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical Image Segmentation is crucial in various clinical applications within the medical domain. While state-of-the-art segmentation models have proven effective, integrating textual guidance to enhance visual features for this task remains an area with limited progress. Existing segmentation models that utilize textual guidance are primarily trained on open-domain images, raising concerns about their direct applicability in the medical domain without manual intervention or fine-tuning.  To address these challenges, we propose using multimodal vision-language models for capturing semantic information from image descriptions and images, enabling the segmentation of diverse medical images. This study comprehensively evaluates existing vision language models across multiple datasets to assess their transferability from the open domain to the medical field. Furthermore, we introduce variations of image descriptions for previously unseen images in the dataset, revealing notable variations 
&lt;/p&gt;</description></item><item><title>mBLIP&#26159;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;Vision-LLM&#65292;&#36890;&#36807;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#20363;&#30340;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26041;&#24335;&#33719;&#24471;&#12290;</title><link>http://arxiv.org/abs/2307.06930</link><description>&lt;p&gt;
mBLIP: &#22810;&#35821;&#35328;&#35270;&#35273;-LLM&#30340;&#39640;&#25928;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs. (arXiv:2307.06930v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06930
&lt;/p&gt;
&lt;p&gt;
mBLIP&#26159;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;Vision-LLM&#65292;&#36890;&#36807;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#20363;&#30340;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26041;&#24335;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22359;&#21270;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;Vision-LLM&#65289;&#23558;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#19982;&#65288;&#39044;&#35757;&#32451;&#30340;&#65289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#40784;&#65292;&#26159;&#19968;&#31181;&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#30340;&#36873;&#25321;&#65292;&#21487;&#20197;&#20195;&#26367;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#32780;&#21518;&#32773;&#23545;&#20110;&#22823;&#22810;&#25968;&#20154;&#26469;&#35828;&#25104;&#26412;&#22826;&#39640;&#12290; Vision-LLM&#23558;LLM&#20107;&#21518;&#26465;&#20214;&#21270;&#20026;&#8220;&#29702;&#35299;&#8221;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#36755;&#20986;&#12290;&#38543;&#30528;&#29616;&#25104;&#30340;&#39640;&#36136;&#37327;&#33521;&#25991;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#20197;&#21450;&#21333;&#35821;&#33521;&#35821;LLM&#30340;&#20016;&#23500;&#24615;&#65292;&#30740;&#31350;&#37325;&#28857;&#24050;&#32463;&#25918;&#22312;&#20165;&#33521;&#25991;&#30340;Vision-LLM&#19978;&#12290;&#32780;&#22810;&#35821;&#35328;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#20027;&#35201;&#36890;&#36807;&#26114;&#36149;&#30340;&#31471;&#21040;&#31471;&#39044;&#35757;&#32451;&#33719;&#24471;&#65292;&#36825;&#23548;&#33268;&#20102;&#30456;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#26377;&#38480;&#30340;&#22810;&#35821;&#35328;&#22270;&#20687;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#34917;&#20805;&#20102;&#20165;&#26377;&#25991;&#26412;&#30340;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;mBLIP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#35821;&#35328;Vision-LLM&#65292;&#25105;&#20204;&#20197;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26041;&#24335;&#33719;&#24471;&#65292;&#20165;&#20351;&#29992;&#20960;&#30334;&#19975;&#20010;&#35757;&#32451;&#26679;&#20363;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modular vision-language models (Vision-LLMs) align pretrained image encoders with (pretrained) large language models (LLMs), representing a computationally much more efficient alternative to end-to-end training of large vision-language models from scratch, which is prohibitively expensive for most. Vision-LLMs instead post-hoc condition LLMs to `understand' the output of an image encoder. With the abundance of readily available high-quality English image-text data as well as monolingual English LLMs, the research focus has been on English-only Vision-LLMs. Multilingual vision-language models are still predominantly obtained via expensive end-to-end pretraining, resulting in comparatively smaller models, trained on limited multilingual image data supplemented with text-only multilingual corpora. In this work, we present mBLIP, the first multilingual Vision-LLM, which we obtain in a computationally efficient manner -- on consumer hardware using only a few million training examples -- by 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoRAPrune&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#30340;&#20540;&#21644;&#26799;&#24230;&#26469;&#35774;&#35745;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18403</link><description>&lt;p&gt;
&#21098;&#26525;&#19982;&#20302;&#31209;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30456;&#36935;
&lt;/p&gt;
&lt;p&gt;
Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning. (arXiv:2305.18403v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoRAPrune&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#30340;&#20540;&#21644;&#26799;&#24230;&#26469;&#35774;&#35745;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LPM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26469;&#20415;&#23452;&#22320;&#24494;&#35843;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#30340;&#37096;&#32626;&#20173;&#28982;&#21463;&#21040;&#24040;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#21046;&#32422;&#12290;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#36890;&#36807;&#21024;&#38500;&#20887;&#20313;&#21442;&#25968;&#26469;&#25552;&#20379;&#27169;&#22411;&#21387;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#21442;&#25968;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LPM&#32780;&#35328;&#65292;&#33719;&#24471;&#26799;&#24230;&#26159;&#35745;&#31639;&#19978;&#31105;&#27490;&#30340;&#65292;&#36825;&#38656;&#35201;&#25506;&#32034;&#26367;&#20195;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;LPM&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#31216;&#20026;LoRAPrune&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#21033;&#29992;&#20102;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#20540;&#21644;&#26799;&#24230;&#65292;&#32780;&#19981;&#26159;&#39044;&#35757;&#32451;&#21442;&#25968;&#30340;&#26799;&#24230;&#36827;&#34892;&#37325;&#35201;&#24615;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#22522;&#20110;&#21098;&#26525;&#20934;&#21017;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#12290;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained models (LPMs), such as LLaMA and ViT-G, have shown exceptional performance across various tasks. Although parameter-efficient fine-tuning (PEFT) has emerged to cheaply fine-tune these large models on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Neural network pruning offers a solution for model compression by removing redundant parameters, but most existing methods rely on computing parameter gradients. However, obtaining the gradients is computationally prohibitive for LPMs, which necessitates the exploration of alternative approaches. To this end, we propose a unified framework for efficient fine-tuning and deployment of LPMs, termed LoRAPrune. We first design a PEFT-aware pruning criterion, which utilizes the values and gradients of Low-Rank Adaption (LoRA), rather than the gradients of pre-trained parameters for importance estimation. We then propose an iterative pruning procedure to remove redundant paramet
&lt;/p&gt;</description></item></channel></rss>