<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#30340;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23450;&#21046;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21644;&#24341;&#20837;&#22810;&#27169;&#24577;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#22810;&#27169;&#24577;&#24322;&#24120;&#26816;&#27979;&#21644;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.11083</link><description>&lt;p&gt;
&#20026;&#22810;&#27169;&#24577;&#24322;&#24120;&#26816;&#27979;&#21644;&#25512;&#29702;&#23450;&#21046;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11083
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#30340;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23450;&#21046;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21644;&#24341;&#20837;&#22810;&#27169;&#24577;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#22810;&#27169;&#24577;&#24322;&#24120;&#26816;&#27979;&#21644;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#21508;&#31181;&#24037;&#19994;&#22330;&#26223;&#20013;&#21313;&#20998;&#37325;&#35201;&#65292;&#21253;&#25324;&#29983;&#20135;&#32447;&#19978;&#24322;&#24120;&#27169;&#24335;&#30340;&#35782;&#21035;&#21644;&#29992;&#20110;&#36136;&#37327;&#25511;&#21046;&#30340;&#21046;&#36896;&#32570;&#38519;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#30340;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#25317;&#26377;&#24191;&#27867;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#30340;&#36890;&#29992;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23450;&#21046;&#20026;&#24322;&#24120;&#26816;&#27979;&#22120;&#21644;&#25512;&#29702;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#25552;&#31034;&#31574;&#30053;&#65292;&#23558;&#39046;&#22495;&#19987;&#23478;&#30340;&#39046;&#22495;&#30693;&#35782;&#20316;&#20026;&#26465;&#20214;&#24341;&#23548;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#22810;&#27169;&#24577;&#25552;&#31034;&#31867;&#22411;&#65292;&#21253;&#25324;&#20219;&#21153;&#25551;&#36848;&#12289;&#31867;&#21035;&#19978;&#19979;&#25991;&#12289;&#27491;&#24120;&#35268;&#21017;&#21644;&#21442;&#32771;&#22270;&#20687;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#34920;&#31034;&#32479;&#19968;&#20026;2D&#22270;&#20687;&#26684;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11083v1 Announce Type: cross  Abstract: Anomaly detection is vital in various industrial scenarios, including the identification of unusual patterns in production lines and the detection of manufacturing defects for quality control. Existing techniques tend to be specialized in individual scenarios and lack generalization capacities. In this study, we aim to develop a generic anomaly detection model applicable across multiple scenarios. To achieve this, we customize generic visual-language foundation models that possess extensive knowledge and robust reasoning abilities into anomaly detectors and reasoners. Specifically, we introduce a multi-modal prompting strategy that incorporates domain knowledge from experts as conditions to guide the models. Our approach considers multi-modal prompt types, including task descriptions, class context, normality rules, and reference images. In addition, we unify the input representation of multi-modality into a 2D image format, enabling m
&lt;/p&gt;</description></item><item><title>RadCLIP&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#36328;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#20197;&#25913;&#36827;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#65292;&#21253;&#21547;&#38024;&#23545;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#23450;&#21046;&#30340;&#26032;&#39062;3D&#20999;&#29255;&#27744;&#21270;&#26426;&#21046;&#65292;&#24182;&#20351;&#29992;&#20016;&#23500;&#22810;&#26679;&#30340;&#25918;&#23556;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.09948</link><description>&lt;p&gt;
RadCLIP: &#36890;&#36807;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#22686;&#24378;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
RadCLIP: Enhancing Radiologic Image Analysis through Contrastive Language-Image Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09948
&lt;/p&gt;
&lt;p&gt;
RadCLIP&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#36328;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#20197;&#25913;&#36827;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#65292;&#21253;&#21547;&#38024;&#23545;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#23450;&#21046;&#30340;&#26032;&#39062;3D&#20999;&#29255;&#27744;&#21270;&#26426;&#21046;&#65292;&#24182;&#20351;&#29992;&#20016;&#23500;&#22810;&#26679;&#30340;&#25918;&#23556;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09948v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: &#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#19982;&#25918;&#23556;&#23398;&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#21307;&#23398;&#35786;&#26029;&#39046;&#22495;&#30340;&#21464;&#38761;&#26102;&#20195;&#12290;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#24050;&#34987;&#37319;&#29992;&#26469;&#22686;&#24378;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#25918;&#23556;&#23398;&#22270;&#20687;&#30340;&#29420;&#29305;&#22797;&#26434;&#24615;&#65292;&#21253;&#25324;&#23545;2D&#21644;3D&#25918;&#23556;&#23398;&#25968;&#25454;&#30340;&#35299;&#35835;&#65292;&#24102;&#26469;&#20102;&#29616;&#26377;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#24212;&#23545;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#36890;&#29992;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#21307;&#23398;&#25104;&#20687;&#25152;&#38656;&#30340;&#35786;&#26029;&#31934;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RadCLIP&#65306;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#36328;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#26469;&#25913;&#36827;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;RadCLIP&#21253;&#21547;&#19968;&#31181;&#26032;&#39062;&#30340;3D&#20999;&#29255;&#27744;&#21270;&#26426;&#21046;&#65292;&#19987;&#20026;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#23450;&#21046;&#65292;&#20351;&#29992;&#20102;&#20016;&#23500;&#22810;&#26679;&#30340;&#25918;&#23556;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;RadCLIP&#33021;&#26377;&#25928;&#22320;&#23545;&#40784;&#25918;&#23556;&#23398;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09948v1 Announce Type: cross  Abstract: The integration of artificial intelligence (AI) with radiology has marked a transformative era in medical diagnostics. Vision foundation models have been adopted to enhance radiologic imaging analysis. However, the distinct complexities of radiological imaging, including the interpretation of 2D and 3D radiological data, pose unique challenges that existing models, trained on general non-medical images, fail to address adequately. To bridge this gap and capitalize on the diagnostic precision required in medical imaging, we introduce RadCLIP: a pioneering cross-modal foundational model that harnesses Contrastive Language-Image Pre-training (CLIP) to refine radiologic image analysis. RadCLIP incorporates a novel 3D slice pooling mechanism tailored for volumetric image analysis and is trained using a comprehensive and diverse dataset of radiologic image-text pairs. Our evaluations demonstrate that RadCLIP effectively aligns radiological i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;CPR&#65289;&#65292;&#26088;&#22312;&#32852;&#21512;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#30446;&#26631;&#20154;&#21592;&#26816;&#32034;&#65292;&#24341;&#20837;&#38646;&#26679;&#26412;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;ZS-CPR&#65289;&#35299;&#20915;&#20102;CPR&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;Word4Per&#12290;</title><link>https://arxiv.org/abs/2311.16515</link><description>&lt;p&gt;
Word4Per: Zero-shot&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Word4Per: Zero-shot Composed Person Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16515
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;CPR&#65289;&#65292;&#26088;&#22312;&#32852;&#21512;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#30446;&#26631;&#20154;&#21592;&#26816;&#32034;&#65292;&#24341;&#20837;&#38646;&#26679;&#26412;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;ZS-CPR&#65289;&#35299;&#20915;&#20102;CPR&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;Word4Per&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#29305;&#23450;&#20154;&#21592;&#20855;&#26377;&#26497;&#22823;&#30340;&#31038;&#20250;&#25928;&#30410;&#21644;&#23433;&#20840;&#20215;&#20540;&#65292;&#36890;&#24120;&#28041;&#21450;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#32467;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;CPR&#65289;&#65292;&#26088;&#22312;&#32852;&#21512;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#30446;&#26631;&#20154;&#21592;&#26816;&#32034;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;CPR&#38656;&#35201;&#26114;&#36149;&#30340;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#32780;&#30446;&#21069;&#27809;&#26377;&#21487;&#29992;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#38646;&#26679;&#26412;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;ZS-CPR&#65289;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#39046;&#22495;&#30456;&#20851;&#25968;&#25454;&#35299;&#20915;&#20102;CPR&#38382;&#39064;&#32780;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#27880;&#37322;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#23398;&#20064;ZS-CPR&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;Word4Per&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25991;&#26412;&#21453;&#36716;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16515v2 Announce Type: replace-cross  Abstract: Searching for specific person has great social benefits and security value, and it often involves a combination of visual and textual information. Conventional person retrieval methods, whether image-based or text-based, usually fall short in effectively harnessing both types of information, leading to the loss of accuracy. In this paper, a whole new task called Composed Person Retrieval (CPR) is proposed to jointly utilize both image and text information for target person retrieval. However, the supervised CPR requires very costly manual annotation dataset, while there are currently no available resources. To mitigate this issue, we firstly introduce the Zero-shot Composed Person Retrieval (ZS-CPR), which leverages existing domain-related data to resolve the CPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we propose a two-stage learning framework, Word4Per, where a lightweight Textual Inversion Netw
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;&#25216;&#26415;&#65292;&#20351;&#29992;&#31169;&#38053;&#38145;&#27169;&#22359;&#20445;&#25252;&#20219;&#24847;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#21487;&#30830;&#20445;&#26080;&#27861;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#37325;&#24314;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.04095</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#38053;&#38145;&#27169;&#22359;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Gradient Leakage Defense with Key-Lock Module for Federated Learning. (arXiv:2305.04095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;&#25216;&#26415;&#65292;&#20351;&#29992;&#31169;&#38053;&#38145;&#27169;&#22359;&#20445;&#25252;&#20219;&#24847;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#21487;&#30830;&#20445;&#26080;&#27861;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#37325;&#24314;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#31169;&#26377;&#25968;&#25454;&#20445;&#25345;&#26412;&#22320;&#65292;&#20801;&#35768;&#23433;&#20840;&#35745;&#31639;&#21644;&#26412;&#22320;&#27169;&#22411;&#26799;&#24230;&#19982;&#31532;&#19977;&#26041;&#21442;&#25968;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#20132;&#25442;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#20849;&#20139;&#30340;&#26799;&#24230;&#21487;&#33021;&#20250;&#21361;&#21450;&#38544;&#31169;&#24182;&#24674;&#22797;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#21644;&#23545;&#26799;&#24230;&#27844;&#28431;&#38382;&#39064;&#30340;&#26032;&#35270;&#35282;&#12290;&#36825;&#20123;&#29702;&#35770;&#24037;&#20316;&#23548;&#33268;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;&#25216;&#26415;&#65292;&#20351;&#29992;&#31169;&#38053;&#38145;&#27169;&#22359;&#20445;&#25252;&#20219;&#24847;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#12290;&#21482;&#26377;&#38145;&#23450;&#30340;&#26799;&#24230;&#34987;&#20256;&#36755;&#21040;&#21442;&#25968;&#26381;&#21153;&#22120;&#36827;&#34892;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23398;&#20064;&#26041;&#27861;&#23545;&#26799;&#24230;&#27844;&#38706;&#25915;&#20987;&#20855;&#26377;&#25269;&#25239;&#21147;&#65292;&#24182;&#19988;&#25152;&#35774;&#35745;&#21644;&#35757;&#32451;&#30340;&#23494;&#38053;&#38145;&#27169;&#22359;&#21487;&#20197;&#30830;&#20445;&#65292;&#27809;&#26377;&#23494;&#38053;&#38145;&#27169;&#22359;&#30340;&#31169;&#26377;&#20449;&#24687;&#65306;a) &#26080;&#27861;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#37325;&#24314;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a widely adopted privacy-preserving machine learning approach where private data remains local, enabling secure computations and the exchange of local model gradients between local clients and third-party parameter servers. However, recent findings reveal that privacy may be compromised and sensitive information potentially recovered from shared gradients. In this study, we offer detailed analysis and a novel perspective on understanding the gradient leakage problem. These theoretical works lead to a new gradient leakage defense technique that secures arbitrary model architectures using a private key-lock module. Only the locked gradient is transmitted to the parameter server for global model aggregation. Our proposed learning method is resistant to gradient leakage attacks, and the key-lock module is designed and trained to ensure that, without the private information of the key-lock module: a) reconstructing private training data from the shared gradient is
&lt;/p&gt;</description></item></channel></rss>