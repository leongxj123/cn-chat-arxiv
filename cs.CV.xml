<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ADAPT&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;prompt&#35843;&#20248;&#33539;&#24335;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#65292;&#22686;&#24378;&#35270;&#35273;Transformer&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13196</link><description>&lt;p&gt;
&#20351;Prompt&#35843;&#20248;&#35270;&#35273;Transformer&#26356;&#20026;&#20581;&#22766;&#30340;ADAPT
&lt;/p&gt;
&lt;p&gt;
ADAPT to Robustify Prompt Tuning Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ADAPT&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;prompt&#35843;&#20248;&#33539;&#24335;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#65292;&#22686;&#24378;&#35270;&#35273;Transformer&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35270;&#35273;Transformer&#65292;&#24050;&#30693;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#35768;&#22810;&#29616;&#26377;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#22914;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#20381;&#36182;&#20110;&#23545;&#25972;&#20010;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#24494;&#35843;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#38656;&#35201;&#20026;&#27599;&#20010;&#20219;&#21153;&#23384;&#20648;&#25972;&#20010;&#27169;&#22411;&#30340;&#21103;&#26412;&#65292;&#32780;&#27169;&#22411;&#21487;&#33021;&#21253;&#21547;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21442;&#25968;&#39640;&#25928;&#30340;prompt&#35843;&#20248;&#34987;&#29992;&#26469;&#36866;&#24212;&#22823;&#22411;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#65292;&#26080;&#38656;&#20445;&#23384;&#22823;&#22411;&#21103;&#26412;&#12290;&#26412;&#25991;&#20174;&#31283;&#20581;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#23545;&#35270;&#35273;Transformer&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#21442;&#25968;&#39640;&#25928;prompt&#35843;&#20248;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20043;&#21069;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#22312;&#24212;&#29992;&#21040;prompt&#35843;&#20248;&#33539;&#24335;&#26102;&#65292;&#23384;&#22312;&#26799;&#24230;&#27169;&#31946;&#24182;&#23481;&#26131;&#21463;&#21040;&#33258;&#36866;&#24212;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ADAPT&#65292;&#19968;&#31181;&#22312;prompt&#35843;&#20248;&#33539;&#24335;&#20013;&#25191;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13196v1 Announce Type: new  Abstract: The performance of deep models, including Vision Transformers, is known to be vulnerable to adversarial attacks. Many existing defenses against these attacks, such as adversarial training, rely on full-model fine-tuning to induce robustness in the models. These defenses require storing a copy of the entire model, that can have billions of parameters, for each task. At the same time, parameter-efficient prompt tuning is used to adapt large transformer-based models to downstream tasks without the need to save large copies. In this paper, we examine parameter-efficient prompt tuning of Vision Transformers for downstream tasks under the lens of robustness. We show that previous adversarial defense methods, when applied to the prompt tuning paradigm, suffer from gradient obfuscation and are vulnerable to adaptive attacks. We introduce ADAPT, a novel framework for performing adaptive adversarial training in the prompt tuning paradigm. Our meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22270;&#20687;&#20998;&#31867;&#20013;OOD&#40065;&#26834;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#25193;&#23637;&#30340;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;von Mises-Fisher&#26680;&#26469;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;OOD&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07277</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22270;&#20687;&#20998;&#31867;&#20013;OOD&#40065;&#26834;&#24615;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Approach to OOD Robustness in Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22270;&#20687;&#20998;&#31867;&#20013;OOD&#40065;&#26834;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#25193;&#23637;&#30340;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;von Mises-Fisher&#26680;&#26469;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;OOD&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#26159;&#30830;&#20445;&#31639;&#27861;&#23545;&#22270;&#20687;&#39046;&#22495;&#30340;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#22788;&#29702;&#27492;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#20294;&#27809;&#26377;&#27880;&#37322;&#30340;&#22270;&#20687;&#12290;&#22312;&#38754;&#20020;&#30495;&#23454;&#19990;&#30028;&#30340;&#22495;&#20043;&#22806;&#65288;OOD&#65289;&#24178;&#25200;&#21644;&#36974;&#25377;&#30340;OOD-CV&#22522;&#20934;&#25361;&#25112;&#30340;&#28608;&#21169;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#23454;&#29616;&#29289;&#20307;&#20998;&#31867;&#30340;OOD&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#24050;&#34987;&#35777;&#26126;&#22312;&#36974;&#25377;&#24773;&#20917;&#19979;&#20855;&#26377;&#40065;&#26834;&#24615;&#20294;&#22312;OOD&#25968;&#25454;&#27979;&#35797;&#26102;&#20005;&#37325;&#38477;&#32423;&#30340;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;&#65288;CompNets&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;CompNets&#21253;&#21547;&#30340;&#22312;von Mises-Fisher&#65288;vMF&#65289;&#26680;&#34920;&#31034;&#30340;&#29305;&#24449;&#21521;&#37327;&#19978;&#23450;&#20041;&#30340;&#29983;&#25104;&#22836;&#65292;&#36825;&#20123;&#26680;&#22823;&#33268;&#23545;&#24212;&#20110;&#23545;&#35937;&#37096;&#20998;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#22495;&#20043;&#38388;&#30340;&#26576;&#20123;vMF&#26680;&#26159;&#30456;&#20284;&#30340;&#65292;&#32780;&#21478;&#19968;&#20123;&#21017;&#19981;&#26159;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#19968;&#20010;transiti
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07277v1 Announce Type: cross  Abstract: An important and unsolved problem in computer vision is to ensure that the algorithms are robust to changes in image domains. We address this problem in the scenario where we have access to images from the target domains but no annotations. Motivated by the challenges of the OOD-CV benchmark where we encounter real world Out-of-Domain (OOD) nuisances and occlusion, we introduce a novel Bayesian approach to OOD robustness for object classification. Our work extends Compositional Neural Networks (CompNets), which have been shown to be robust to occlusion but degrade badly when tested on OOD data. We exploit the fact that CompNets contain a generative head defined over feature vectors represented by von Mises-Fisher (vMF) kernels, which correspond roughly to object parts, and can be learned without supervision. We obverse that some vMF kernels are similar between different domains, while others are not. This enables us to learn a transiti
&lt;/p&gt;</description></item></channel></rss>