<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;$\text{CardioVAE}_\text{X,G}$&#65289;&#65292;&#23558;&#20302;&#25104;&#26412;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#24418;&#24335;&#25972;&#21512;&#36215;&#26469;&#65292;&#24182;&#23454;&#29616;&#20102;&#20849;&#20139;&#29305;&#24449;&#21644;&#29420;&#29305;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.13658</link><description>&lt;p&gt;
&#29992;&#20110;&#20302;&#25104;&#26412;&#24515;&#33039;&#34880;&#28082;&#21160;&#21147;&#23398;&#19981;&#31283;&#23450;&#24615;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics Instability Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13658
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;$\text{CardioVAE}_\text{X,G}$&#65289;&#65292;&#23558;&#20302;&#25104;&#26412;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#24418;&#24335;&#25972;&#21512;&#36215;&#26469;&#65292;&#24182;&#23454;&#29616;&#20102;&#20849;&#20139;&#29305;&#24449;&#21644;&#29420;&#29305;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#38750;&#20405;&#20837;&#24615;&#26816;&#27979;&#24515;&#33039;&#34880;&#28082;&#21160;&#21147;&#23398;&#19981;&#31283;&#23450;&#24615;&#65288;CHDI&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#21333;&#19968;&#25968;&#25454;&#24418;&#24335;&#65292;&#22914;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;&#26631;&#35760;&#30340;&#24739;&#32773;&#25968;&#25454;&#37327;&#26377;&#38480;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#25928;&#26524;&#19981;&#20339;&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#30340;&#24120;&#35265;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#26041;&#27861;&#26469;&#30740;&#31350;CHDI&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#26114;&#36149;&#30340;&#25968;&#25454;&#24418;&#24335;&#65292;&#22914;&#24515;&#33039;MRI&#21644;&#24515;&#33039;&#36229;&#22768;&#22270;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;$\text{CardioVAE}_\text{X,G}$&#65289;&#26469;&#25972;&#21512;&#20302;&#25104;&#26412;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#24418;&#24335;&#65292;&#24182;&#22312;&#22823;&#22411;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;$\text{CardioVAE}_\text{X,G}$&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#27969;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#23398;&#20064;&#20849;&#20139;&#29305;&#24449;&#21644;&#21508;&#25968;&#25454;&#24418;&#24335;&#29420;&#26377;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;fi
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13658v1 Announce Type: new  Abstract: Recent advancements in non-invasive detection of cardiac hemodynamic instability (CHDI) primarily focus on applying machine learning techniques to a single data modality, e.g. cardiac magnetic resonance imaging (MRI). Despite their potential, these approaches often fall short especially when the size of labeled patient data is limited, a common challenge in the medical domain. Furthermore, only a few studies have explored multimodal methods to study CHDI, which mostly rely on costly modalities such as cardiac MRI and echocardiogram. In response to these limitations, we propose a novel multimodal variational autoencoder ($\text{CardioVAE}_\text{X,G}$) to integrate low-cost chest X-ray (CXR) and electrocardiogram (ECG) modalities with pre-training on a large unlabeled dataset. Specifically, $\text{CardioVAE}_\text{X,G}$ introduces a novel tri-stream pre-training strategy to learn both shared and modality-specific features, thus enabling fi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32479;&#19968;&#24322;&#24120;&#26816;&#27979;&#30340;Hierarchical Gaussian mixture normalizing flow (HGAD)&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#39640;&#26031;&#28151;&#21512;&#24314;&#27169;&#26469;&#25552;&#21319;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.13349</link><description>&lt;p&gt;
&#20998;&#23618;&#39640;&#26031;&#28151;&#21512;&#27491;&#35268;&#21270;&#27969;&#24314;&#27169;&#29992;&#20110;&#32479;&#19968;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13349
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32479;&#19968;&#24322;&#24120;&#26816;&#27979;&#30340;Hierarchical Gaussian mixture normalizing flow (HGAD)&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#39640;&#26031;&#28151;&#21512;&#24314;&#27169;&#26469;&#25552;&#21319;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#19968;&#24322;&#24120;&#26816;&#27979;&#26159;&#24322;&#24120;&#26816;&#27979;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#20854;&#20013;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#31867;&#21035;&#30340;&#27491;&#24120;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#30446;&#26631;&#26159;&#26816;&#27979;&#36825;&#20123;&#31867;&#21035;&#20013;&#30340;&#24322;&#24120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#39640;&#26031;&#28151;&#21512;&#27491;&#35268;&#21270;&#27969;&#24314;&#27169;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;HGAD&#65292;&#29992;&#20110;&#23436;&#25104;&#32479;&#19968;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;HGAD&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#36328;&#31867;&#21035;&#39640;&#26031;&#28151;&#21512;&#24314;&#27169;&#21644;&#31867;&#20869;&#28151;&#21512;&#31867;&#20013;&#24515;&#23398;&#20064;&#12290;&#19982;&#20808;&#21069;&#22522;&#20110;NF&#30340;AD&#26041;&#27861;&#30456;&#27604;&#65292;&#20998;&#23618;&#39640;&#26031;&#28151;&#21512;&#24314;&#27169;&#26041;&#27861;&#21487;&#20197;&#20026;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#24102;&#26469;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13349v1 Announce Type: new  Abstract: Unified anomaly detection (AD) is one of the most challenges for anomaly detection, where one unified model is trained with normal samples from multiple classes with the objective to detect anomalies in these classes. For such a challenging task, popular normalizing flow (NF) based AD methods may fall into a "homogeneous mapping" issue,where the NF-based AD models are biased to generate similar latent representations for both normal and abnormal features, and thereby lead to a high missing rate of anomalies. In this paper, we propose a novel Hierarchical Gaussian mixture normalizing flow modeling method for accomplishing unified Anomaly Detection, which we call HGAD. Our HGAD consists of two key components: inter-class Gaussian mixture modeling and intra-class mixed class centers learning. Compared to the previous NF-based AD methods, the hierarchical Gaussian mixture modeling approach can bring stronger representation capability to the 
&lt;/p&gt;</description></item><item><title>DexCap&#26159;&#19968;&#20010;&#21487;&#31227;&#26893;&#30340;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#65292;&#32467;&#21512;DexIL&#31639;&#27861;&#20174;&#20154;&#31867;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#20013;&#35757;&#32451;&#26426;&#22120;&#20154;&#25216;&#33021;&#65292;&#20855;&#26377;&#31934;&#30830;&#36861;&#36394;&#21644;&#22797;&#21046;&#20154;&#31867;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.07788</link><description>&lt;p&gt;
DexCap&#65306;&#29992;&#20110;&#28789;&#24039;&#25805;&#20316;&#30340;&#21487;&#25193;&#23637;&#21644;&#21487;&#31227;&#26893;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#25910;&#38598;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07788
&lt;/p&gt;
&lt;p&gt;
DexCap&#26159;&#19968;&#20010;&#21487;&#31227;&#26893;&#30340;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#65292;&#32467;&#21512;DexIL&#31639;&#27861;&#20174;&#20154;&#31867;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#20013;&#35757;&#32451;&#26426;&#22120;&#20154;&#25216;&#33021;&#65292;&#20855;&#26377;&#31934;&#30830;&#36861;&#36394;&#21644;&#22797;&#21046;&#20154;&#31867;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#26159;&#20026;&#26426;&#22120;&#20154;&#36171;&#20104;&#31867;&#20154;&#28789;&#24039;&#22312;&#29616;&#23454;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#36884;&#24452;&#65292;&#28982;&#32780;&#65292;&#29616;&#23384;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#30340;&#21487;&#31227;&#26893;&#24615;&#20197;&#21450;&#23558;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#36716;&#21270;&#20026;&#26377;&#25928;&#25511;&#21046;&#31574;&#30053;&#30340;&#22256;&#38590;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DexCap&#65292;&#19968;&#20010;&#20415;&#25658;&#24335;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#65292;&#20197;&#21450;DexIL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#20223;&#31639;&#27861;&#65292;&#21487;&#30452;&#25509;&#20174;&#20154;&#31867;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#35757;&#32451;&#28789;&#24039;&#26426;&#22120;&#20154;&#25216;&#33021;&#12290;DexCap&#22522;&#20110;SLAM&#21644;&#30005;&#30913;&#22330;&#20197;&#21450;&#29615;&#22659;&#30340;3D&#35266;&#23519;&#65292;&#25552;&#20379;&#20102;&#23545;&#25163;&#33109;&#21644;&#25163;&#25351;&#36816;&#21160;&#30340;&#31934;&#30830;&#12289;&#25239;&#36974;&#25377;&#30340;&#36319;&#36394;&#12290;&#21033;&#29992;&#36825;&#19968;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;DexIL&#37319;&#29992;&#36870;&#36816;&#21160;&#23398;&#21644;&#22522;&#20110;&#28857;&#20113;&#30340;&#27169;&#20223;&#23398;&#20064;&#26469;&#22797;&#21046;&#20154;&#31867;&#21160;&#20316;&#19982;&#26426;&#22120;&#20154;&#25163;&#12290;&#38500;&#20102;&#20174;&#20154;&#31867;&#36816;&#21160;&#20013;&#23398;&#20064;&#22806;&#65292;DexCap&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;op
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07788v1 Announce Type: cross  Abstract: Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the difficulty of translating mocap data into effective control policies. To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to replicate human actions with robot hands. Beyond learning from human motion, DexCap also offers an op
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22810;&#27169;&#24577;&#23545;&#20107;&#23454;&#25512;&#29702;&#24773;&#24863;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#20928;&#21270;&#21644;&#32531;&#35299;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#25552;&#39640;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05023</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#24046;&#20928;&#21270;&#23454;&#29616;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Multimodal Sentiment Analysis Debiasing via Bias Purification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05023
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22810;&#27169;&#24577;&#23545;&#20107;&#23454;&#25512;&#29702;&#24773;&#24863;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#20928;&#21270;&#21644;&#32531;&#35299;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#25552;&#39640;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65288;MSA&#65289;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#65288;&#22914;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#38899;&#39057;&#65289;&#30340;&#19982;&#24773;&#24863;&#30456;&#20851;&#32447;&#32034;&#26469;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;MSA&#20219;&#21153;&#26222;&#36941;&#21463;&#21040;&#26410;&#32463;&#35745;&#21010;&#30340;&#25968;&#25454;&#38598;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22810;&#27169;&#24577;&#35805;&#35821;&#32423;&#26631;&#31614;&#20559;&#35265;&#21644;&#21333;&#35789;&#32423;&#19978;&#19979;&#25991;&#20559;&#35265;&#12290;&#36825;&#20123;&#26377;&#23475;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#35823;&#23548;&#27169;&#22411;&#19987;&#27880;&#20110;&#32479;&#35745;&#25463;&#24452;&#21644;&#38169;&#35823;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#32780;&#38750;&#20256;&#32479;&#20284;&#28982;&#24615;&#30340;&#22810;&#27169;&#24577;&#23545;&#20107;&#23454;&#25512;&#29702;&#24773;&#24863;&#65288;MCIS&#65289;&#20998;&#26512;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#19968;&#20010;&#22240;&#26524;&#22270;&#26469;&#21457;&#29616;&#24050;&#35757;&#32451;&#30340;&#22522;&#20934;&#27169;&#22411;&#20013;&#30340;&#26377;&#23475;&#20559;&#35265;&#12290;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;&#32473;&#23450;&#19968;&#20010;&#20107;&#23454;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;MCIS&#24819;&#35937;&#20004;&#31181;&#23545;&#20107;&#23454;&#24773;&#24418;&#65292;&#20197;&#20928;&#21270;&#21644;&#32531;&#35299;&#36825;&#20123;&#20559;&#35265;&#12290;&#28982;&#21518;&#65292;MCIS&#21487;&#20197;&#20174;&#20559;&#24046;&#20013;&#20570;&#20986;&#19981;&#24102;&#20559;&#35265;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05023v1 Announce Type: new  Abstract: Multimodal Sentiment Analysis (MSA) aims to understand human intentions by integrating emotion-related clues from diverse modalities, such as visual, language, and audio. Unfortunately, the current MSA task invariably suffers from unplanned dataset biases, particularly multimodal utterance-level label bias and word-level context bias. These harmful biases potentially mislead models to focus on statistical shortcuts and spurious correlations, causing severe performance bottlenecks. To alleviate these issues, we present a Multimodal Counterfactual Inference Sentiment (MCIS) analysis framework based on causality rather than conventional likelihood. Concretely, we first formulate a causal graph to discover harmful biases from already-trained vanilla models. In the inference phase, given a factual multimodal input, MCIS imagines two counterfactual scenarios to purify and mitigate these biases. Then, MCIS can make unbiased decisions from biase
&lt;/p&gt;</description></item><item><title>U$^2$MRPD&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#24341;&#23548;&#65292;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#27424;&#37319;&#26679;MRI&#37325;&#24314;&#65292;&#33021;&#22815;&#25903;&#25345;&#22270;&#20687;&#29305;&#23450;&#30340;MRI&#37325;&#24314;&#65292;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#30417;&#30563;&#21644;MRI&#25193;&#25955;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10609</link><description>&lt;p&gt;
U$^2$MRPD: &#36890;&#36807;&#22823;&#22411;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#24341;&#23548;&#30340;&#26080;&#30417;&#30563;MRI&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
U$^2$MRPD: Unsupervised undersampled MRI reconstruction by prompting a large latent diffusion model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10609
&lt;/p&gt;
&lt;p&gt;
U$^2$MRPD&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#24341;&#23548;&#65292;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#27424;&#37319;&#26679;MRI&#37325;&#24314;&#65292;&#33021;&#22815;&#25903;&#25345;&#22270;&#20687;&#29305;&#23450;&#30340;MRI&#37325;&#24314;&#65292;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#30417;&#30563;&#21644;MRI&#25193;&#25955;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10609v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LLDM)&#20013;&#34164;&#21547;&#30528;&#20016;&#23500;&#32780;&#20551;&#35774;&#19978;&#26222;&#36941;&#36866;&#29992;&#20110;&#33258;&#28982;&#21644;&#21307;&#23398;&#22270;&#20687;&#30340;&#38544;&#21547;&#35270;&#35273;&#30693;&#35782;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;U$^2$MRPD&#65289;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#27424;&#37319;&#26679;MRI&#37325;&#24314;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#12289;&#30417;&#30563;&#30340;&#27424;&#37319;&#26679;MRI&#37325;&#24314;&#32593;&#32476;&#36890;&#24120;&#20855;&#26377;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#36866;&#24212;&#24615;&#65292;&#19981;&#36275;&#20197;&#24212;&#23545;&#21508;&#31181;&#25968;&#25454;&#37319;&#38598;&#22330;&#26223;&#65307;&#28982;&#32780;&#65292;U$^2$MRPD&#36890;&#36807;&#20351;&#29992;&#37327;&#36523;&#23450;&#21046;&#30340;MRSampler&#65292;&#25903;&#25345;&#22270;&#20687;&#29305;&#23450;&#30340;MRI&#37325;&#24314;&#65292;&#35813;MRSampler&#36866;&#29992;&#20110;&#22797;&#20540;MRI&#22270;&#20687;&#12290;&#36890;&#36807;&#20219;&#20309;&#21333;&#19968;&#26469;&#28304;&#25110;&#22810;&#28304;MRI&#25968;&#25454;&#38598;&#65292;U$^2$MRPD&#30340;&#24615;&#33021;&#36824;&#21487;&#20197;&#36890;&#36807;MRAdapter&#36827;&#34892;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#22270;&#20687;&#20808;&#39564;&#19981;&#21464;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;U$^2$MRPD&#23454;&#29616;&#20102;&#19982;&#30417;&#30563;&#21644;MRI&#25193;&#25955;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10609v1 Announce Type: cross  Abstract: Implicit visual knowledge in a large latent diffusion model (LLDM) pre-trained on natural images is rich and hypothetically universal to natural and medical images. To test this hypothesis, we introduce a novel framework for Unsupervised Undersampled MRI Reconstruction by Prompting a pre-trained large latent Diffusion model ( U$^2$MRPD). Existing data-driven, supervised undersampled MRI reconstruction networks are typically of limited generalizability and adaptability toward diverse data acquisition scenarios; yet U$^2$MRPD supports image-specific MRI reconstruction by prompting an LLDM with an MRSampler tailored for complex-valued MRI images. With any single-source or diverse-source MRI dataset, U$^2$MRPD's performance is further boosted by an MRAdapter while keeping the generative image priors intact. Experiments on multiple datasets show that U$^2$MRPD achieves comparable or better performance than supervised and MRI diffusion metho
&lt;/p&gt;</description></item><item><title>ScreenAI&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#29702;&#35299;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#20462;&#34917;&#31574;&#30053;&#21644;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#20197;&#21450;&#38024;&#23545;UI&#20803;&#32032;&#30340;&#23631;&#24149;&#27880;&#35299;&#20219;&#21153;&#30340;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04615</link><description>&lt;p&gt;
ScreenAI: &#29992;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#29702;&#35299;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ScreenAI: A Vision-Language Model for UI and Infographics Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04615
&lt;/p&gt;
&lt;p&gt;
ScreenAI&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#29702;&#35299;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#20462;&#34917;&#31574;&#30053;&#21644;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#20197;&#21450;&#38024;&#23545;UI&#20803;&#32032;&#30340;&#23631;&#24149;&#27880;&#35299;&#20219;&#21153;&#30340;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23631;&#24149;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#21644;&#20449;&#24687;&#22270;&#34920;&#22312;&#20154;&#31867;&#27807;&#36890;&#21644;&#20154;&#26426;&#20132;&#20114;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#19988;&#20849;&#20139;&#30456;&#20284;&#30340;&#35270;&#35273;&#35821;&#35328;&#21644;&#35774;&#35745;&#21407;&#21017;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ScreenAI&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#29702;&#35299;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25913;&#36827;&#20102;PaLI&#26550;&#26500;&#65292;&#37319;&#29992;&#20102;pix2struct&#30340;&#28789;&#27963;&#20462;&#34917;&#31574;&#30053;&#65292;&#24182;&#32463;&#36807;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#12290;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#26680;&#24515;&#26159;&#19968;&#39033;&#26032;&#39062;&#30340;&#23631;&#24149;&#27880;&#35299;&#20219;&#21153;&#65292;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;UI&#20803;&#32032;&#30340;&#31867;&#22411;&#21644;&#20301;&#32622;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25991;&#26412;&#27880;&#35299;&#26469;&#25551;&#36848;&#23631;&#24149;&#65292;&#24182;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#38382;&#31572;&#65288;QA&#65289;&#65292;UI&#23548;&#33322;&#21644;&#25688;&#35201;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#20197;&#23637;&#31034;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#22312;&#20165;&#26377;5B&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;ScreenAI&#22312;&#22522;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#30340;&#20219;&#21153;&#65288;&#22810;&#39029;&#25991;&#26723;VQA&#65292;WebSRC&#65292;MoTIF&#21644;Widget&#23383;&#24149;&#65289;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#24182;&#19988;&#36798;&#21040;&#20102;&#26368;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Screen user interfaces (UIs) and infographics, sharing similar visual language and design principles, play important roles in human communication and human-machine interaction. We introduce ScreenAI, a vision-language model that specializes in UI and infographics understanding. Our model improves upon the PaLI architecture with the flexible patching strategy of pix2struct and is trained on a unique mixture of datasets. At the heart of this mixture is a novel screen annotation task in which the model has to identify the type and location of UI elements. We use these text annotations to describe screens to Large Language Models and automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale. We run ablation studies to demonstrate the impact of these design choices. At only 5B parameters, ScreenAI achieves new state-of-the-artresults on UI- and infographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and Widget Captioning), and new best-in
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MT-HCCAR&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20113;&#23646;&#24615;&#26816;&#32034;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#20113;&#23646;&#24615;&#26816;&#32034;&#20219;&#21153;&#20043;&#38388;&#30340;&#23618;&#32423;&#20851;&#31995;&#65292;&#24182;&#20855;&#26377;&#23545;&#19981;&#21516;&#20256;&#24863;&#22120;&#25968;&#25454;&#38598;&#20855;&#26377;&#20581;&#22766;&#27867;&#21270;&#33021;&#21147;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.16520</link><description>&lt;p&gt;
MT-HCCAR: &#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#19982;&#23618;&#32423;&#20998;&#31867;&#30340;&#27880;&#24847;&#21147;&#22238;&#24402;&#29992;&#20110;&#20113;&#23646;&#24615;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
MT-HCCAR: Multi-Task Deep Learning with Hierarchical Classification and Attention-based Regression for Cloud Property Retrieval. (arXiv:2401.16520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16520
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MT-HCCAR&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20113;&#23646;&#24615;&#26816;&#32034;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#20113;&#23646;&#24615;&#26816;&#32034;&#20219;&#21153;&#20043;&#38388;&#30340;&#23618;&#32423;&#20851;&#31995;&#65292;&#24182;&#20855;&#26377;&#23545;&#19981;&#21516;&#20256;&#24863;&#22120;&#25968;&#25454;&#38598;&#20855;&#26377;&#20581;&#22766;&#27867;&#21270;&#33021;&#21147;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#26377;&#25928;&#30340;&#20113;&#23646;&#24615;&#26816;&#32034;&#21253;&#25324;&#20113;&#36974;&#34109;&#12289;&#20113;&#30456;&#20998;&#31867;&#21644;&#20113;&#20809;&#23398;&#21402;&#24230;&#65288;COT&#65289;&#39044;&#27979;&#20173;&#28982;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#20256;&#24863;&#22120;&#20202;&#22120;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#29420;&#29305;&#30340;&#20809;&#35889;&#29305;&#24449;&#12290;&#26368;&#36817;&#65292;&#22312;&#22320;&#29699;&#31185;&#23398;&#30740;&#31350;&#20013;&#37319;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#21355;&#26143;&#25968;&#25454;&#38598;&#30340;&#20809;&#35889;&#35266;&#27979;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#32771;&#34385;&#26816;&#32034;&#20219;&#21153;&#20043;&#38388;&#23618;&#32423;&#20851;&#31995;&#30340;&#21019;&#26032;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#29616;&#26377;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#20809;&#35889;&#22810;&#26679;&#24615;&#65292;&#24320;&#21457;&#20855;&#26377;&#23545;&#19981;&#21516;&#20256;&#24863;&#22120;&#25968;&#25454;&#38598;&#20855;&#26377;&#20581;&#22766;&#27867;&#21270;&#33021;&#21147;&#30340;&#27169;&#22411;&#26159;&#24517;&#35201;&#30340;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#30446;&#21069;&#32570;&#20047;&#35299;&#20915;&#22810;&#26679;&#25968;&#25454;&#38598;&#19979;&#36873;&#25321;&#26368;&#20248;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;MT-HCCAR&#65292;&#36825;&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of Earth science, effective cloud property retrieval, encompassing cloud masking, cloud phase classification, and cloud optical thickness (COT) prediction, remains pivotal. Traditional methodologies necessitate distinct models for each sensor instrument due to their unique spectral characteristics. Recent strides in Earth Science research have embraced machine learning and deep learning techniques to extract features from satellite datasets' spectral observations. However, prevailing approaches lack novel architectures accounting for hierarchical relationships among retrieval tasks. Moreover, considering the spectral diversity among existing sensors, the development of models with robust generalization capabilities over different sensor datasets is imperative. Surprisingly, there is a dearth of methodologies addressing the selection of an optimal model for diverse datasets. In response, this paper introduces MT-HCCAR, an end-to-end deep learning model employing multi-task 
&lt;/p&gt;</description></item><item><title>PhotoBot&#26159;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#21644;&#26426;&#22120;&#20154;&#25668;&#24433;&#24072;&#30456;&#20114;&#20316;&#29992;&#30340;&#33258;&#21160;&#21270;&#29031;&#29255;&#33719;&#21462;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#29289;&#20307;&#26816;&#27979;&#22120;&#26469;&#25552;&#20379;&#25668;&#24433;&#24314;&#35758;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#21464;&#25442;&#22120;&#35745;&#31639;&#30456;&#26426;&#30340;&#23039;&#24577;&#35843;&#25972;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#29031;&#29255;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2401.11061</link><description>&lt;p&gt;
PhotoBot&#65306;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#30340;&#21442;&#32771;&#20114;&#21160;&#25668;&#24433;
&lt;/p&gt;
&lt;p&gt;
PhotoBot: Reference-Guided Interactive Photography via Natural Language. (arXiv:2401.11061v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11061
&lt;/p&gt;
&lt;p&gt;
PhotoBot&#26159;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#21644;&#26426;&#22120;&#20154;&#25668;&#24433;&#24072;&#30456;&#20114;&#20316;&#29992;&#30340;&#33258;&#21160;&#21270;&#29031;&#29255;&#33719;&#21462;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#29289;&#20307;&#26816;&#27979;&#22120;&#26469;&#25552;&#20379;&#25668;&#24433;&#24314;&#35758;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#21464;&#25442;&#22120;&#35745;&#31639;&#30456;&#26426;&#30340;&#23039;&#24577;&#35843;&#25972;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#29031;&#29255;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PhotoBot&#30340;&#26694;&#26550;&#65292;&#23427;&#22522;&#20110;&#39640;&#32423;&#20154;&#31867;&#35821;&#35328;&#24341;&#23548;&#21644;&#26426;&#22120;&#20154;&#25668;&#24433;&#24072;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#30340;&#29031;&#29255;&#33719;&#21462;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#20174;&#31574;&#23637;&#30011;&#24266;&#20013;&#26816;&#32034;&#21040;&#30340;&#21442;&#32771;&#22270;&#29255;&#21521;&#29992;&#25143;&#20256;&#36798;&#25668;&#24433;&#24314;&#35758;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#21644;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#23545;&#21442;&#32771;&#22270;&#29255;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#22522;&#20110;&#29992;&#25143;&#35821;&#35328;&#26597;&#35810;&#30340;&#25991;&#26412;&#25512;&#29702;&#26816;&#32034;&#30456;&#20851;&#30340;&#21442;&#32771;&#22270;&#29255;&#12290;&#20026;&#20102;&#23545;&#24212;&#21442;&#32771;&#22270;&#29255;&#21644;&#35266;&#23519;&#21040;&#30340;&#22330;&#26223;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#26174;&#33879;&#19981;&#21516;&#30340;&#22270;&#20687;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#36879;&#35270;n-&#28857;&#65288;PnP&#65289;&#38382;&#39064;&#26469;&#35745;&#31639;RGB-D&#30456;&#26426;&#30340;&#23039;&#24577;&#35843;&#25972;&#12290;&#25105;&#20204;&#22312;&#37197;&#22791;&#26377;&#25163;&#33109;&#30456;&#26426;&#30340;&#30495;&#23454;&#26426;&#26800;&#25163;&#33218;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;PhotoBot&#25293;&#25668;&#30340;&#29031;&#29255;&#20855;&#26377;&#33391;&#22909;&#30340;&#36136;&#37327;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce PhotoBot, a framework for automated photo acquisition based on an interplay between high-level human language guidance and a robot photographer. We propose to communicate photography suggestions to the user via a reference picture that is retrieved from a curated gallery. We exploit a visual language model (VLM) and an object detector to characterize reference pictures via textual descriptions and use a large language model (LLM) to retrieve relevant reference pictures based on a user's language query through text-based reasoning. To correspond the reference picture and the observed scene, we exploit pre-trained features from a vision transformer capable of capturing semantic similarity across significantly varying images. Using these features, we compute pose adjustments for an RGB-D camera by solving a Perspective-n-Point (PnP) problem. We demonstrate our approach on a real-world manipulator equipped with a wrist camera. Our user studies show that photos taken by PhotoBo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#65288;LAACA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#33402;&#26415;&#21697;&#20813;&#21463;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;NST&#65289;&#30340;&#28389;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#19981;&#21487;&#23519;&#35273;&#30340;&#24773;&#20917;&#19979;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#65292;&#20135;&#29983;&#23545;NST&#20855;&#26377;&#24178;&#25200;&#20316;&#29992;&#30340;&#25200;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.09673</link><description>&lt;p&gt;
&#20351;&#29992;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#23545;&#33402;&#26415;&#21697;&#36827;&#34892;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#30340;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Artwork Protection Against Neural Style Transfer Using Locally Adaptive Adversarial Color Attack. (arXiv:2401.09673v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#65288;LAACA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#33402;&#26415;&#21697;&#20813;&#21463;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;NST&#65289;&#30340;&#28389;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#19981;&#21487;&#23519;&#35273;&#30340;&#24773;&#20917;&#19979;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#65292;&#20135;&#29983;&#23545;NST&#20855;&#26377;&#24178;&#25200;&#20316;&#29992;&#30340;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;NST&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#20219;&#24847;&#39118;&#26684;&#30340;&#26032;&#22270;&#20687;&#12290;&#36825;&#20010;&#36807;&#31243;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#39118;&#26684;&#22270;&#20687;&#30340;&#32654;&#23398;&#20803;&#32032;&#19982;&#20869;&#23481;&#22270;&#20687;&#30340;&#32467;&#26500;&#22240;&#32032;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#24418;&#25104;&#19968;&#20010;&#21644;&#35856;&#25972;&#21512;&#30340;&#35270;&#35273;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26410;&#32463;&#25480;&#26435;&#30340;NST&#21487;&#33021;&#20250;&#28389;&#29992;&#33402;&#26415;&#21697;&#12290;&#36825;&#31181;&#28389;&#29992;&#24341;&#36215;&#20102;&#20851;&#20110;&#33402;&#26415;&#23478;&#26435;&#21033;&#30340;&#31038;&#20250;&#25216;&#26415;&#38382;&#39064;&#65292;&#24182;&#20419;&#20351;&#24320;&#21457;&#25216;&#26415;&#26041;&#27861;&#26469;&#31215;&#26497;&#20445;&#25252;&#21407;&#22987;&#21019;&#20316;&#12290;&#23545;&#25239;&#24615;&#25915;&#20987;&#20027;&#35201;&#22312;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#20013;&#36827;&#34892;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#36825;&#19968;&#25216;&#26415;&#24341;&#20837;&#21040;&#20445;&#25252;&#33402;&#26415;&#23478;&#30693;&#35782;&#20135;&#26435;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#65288;LAACA&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20197;&#23545;&#20154;&#30524;&#19981;&#21487;&#23519;&#35273;&#20294;&#23545;NST&#20135;&#29983;&#24178;&#25200;&#30340;&#26041;&#24335;&#20462;&#25913;&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#38024;&#23545;&#39640;&#39057;&#20869;&#23481;&#20016;&#23500;&#21306;&#22495;&#30340;&#25200;&#21160;&#65292;&#36825;&#20123;&#25200;&#21160;&#30001;&#20013;&#38388;&#29305;&#24449;&#30340;&#30772;&#22351;&#20135;&#29983;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#21644;&#29992;&#25143;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural style transfer (NST) is widely adopted in computer vision to generate new images with arbitrary styles. This process leverages neural networks to merge aesthetic elements of a style image with the structural aspects of a content image into a harmoniously integrated visual result. However, unauthorized NST can exploit artwork. Such misuse raises socio-technical concerns regarding artists' rights and motivates the development of technical approaches for the proactive protection of original creations. Adversarial attack is a concept primarily explored in machine learning security. Our work introduces this technique to protect artists' intellectual property. In this paper Locally Adaptive Adversarial Color Attack (LAACA), a method for altering images in a manner imperceptible to the human eyes but disruptive to NST. Specifically, we design perturbations targeting image areas rich in high-frequency content, generated by disrupting intermediate features. Our experiments and user study
&lt;/p&gt;</description></item><item><title>FUTURE-AI&#26159;&#31532;&#19968;&#20010;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2309.12325</link><description>&lt;p&gt;
FUTURE-AI&#65306;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;&#21644;&#21487;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;&#30340;&#22269;&#38469;&#20849;&#35782;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare. (arXiv:2309.12325v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12325
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI&#26159;&#31532;&#19968;&#20010;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;AI&#25216;&#26415;&#22312;&#29616;&#23454;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#37096;&#32626;&#21644;&#37319;&#29992;&#20173;&#21463;&#38480;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#21307;&#30103;AI&#30340;&#25216;&#26415;&#12289;&#20020;&#24202;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#39118;&#38505;&#25552;&#20986;&#20102;&#20851;&#27880;&#12290;&#20026;&#20102;&#22686;&#21152;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37319;&#29992;&#65292;&#21307;&#30103;AI&#24037;&#20855;&#24517;&#39035;&#24471;&#21040;&#24739;&#32773;&#12289;&#20020;&#24202;&#21307;&#29983;&#12289;&#20581;&#24247;&#32452;&#32455;&#21644;&#24403;&#23616;&#30340;&#20449;&#20219;&#21644;&#25509;&#21463;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;FUTURE-AI&#25351;&#21335;&#20316;&#20026;&#31532;&#19968;&#20010;&#29992;&#20110;&#25351;&#23548;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#12290;FUTURE-AI&#32852;&#30431;&#25104;&#31435;&#20110;2021&#24180;&#65292;&#30446;&#21069;&#21253;&#25324;&#26469;&#33258;51&#20010;&#22269;&#23478;&#30340;118&#20301;&#36328;&#23398;&#31185;&#19987;&#23478;&#65292;&#20195;&#34920;&#20102;&#25152;&#26377;&#22823;&#27954;&#65292;&#21253;&#25324;AI&#31185;&#23398;&#23478;&#12289;&#20020;&#24202;&#21307;&#29983;&#12289;&#20262;&#29702;&#23398;&#23478;&#21644;&#31038;&#20250;&#31185;&#23398;&#23478;&#12290;&#22312;&#20026;&#26399;&#20004;&#24180;&#30340;&#26102;&#38388;&#37324;&#65292;&#32852;&#30431;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#23450;&#20041;&#20102;&#21487;&#20449;AI&#30340;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20854;&#20013;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
Despite major advances in artificial intelligence (AI) for medicine and healthcare, the deployment and adoption of AI technologies remain limited in real-world clinical practice. In recent years, concerns have been raised about the technical, clinical, ethical and legal risks associated with medical AI. To increase real world adoption, it is essential that medical AI tools are trusted and accepted by patients, clinicians, health organisations and authorities. This work describes the FUTURE-AI guideline as the first international consensus framework for guiding the development and deployment of trustworthy AI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and currently comprises 118 inter-disciplinary experts from 51 countries representing all continents, including AI scientists, clinicians, ethicists, and social scientists. Over a two-year period, the consortium defined guiding principles and best practices for trustworthy AI through an iterative process comprising a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#29109;&#30340;&#25439;&#22833;&#39033;&#65292;&#36890;&#36807;&#27979;&#37327;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#25968;&#25454;&#26102;&#30340;&#29109;&#21464;&#21270;&#65292;&#25351;&#23548;&#31070;&#32463;&#32593;&#32476;&#20197;&#26356;&#24555;&#36895;&#30340;&#25910;&#25947;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#23398;&#20064;&#20016;&#23500;&#30340;&#28508;&#22312;&#25968;&#25454;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.14938</link><description>&lt;p&gt;
&#22522;&#20110;&#29109;&#30340;&#25351;&#23548;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#25910;&#25947;&#21644;&#25913;&#21892;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Entropy-based Guidance of Deep Neural Networks for Accelerated Convergence and Improved Performance. (arXiv:2308.14938v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#29109;&#30340;&#25439;&#22833;&#39033;&#65292;&#36890;&#36807;&#27979;&#37327;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#25968;&#25454;&#26102;&#30340;&#29109;&#21464;&#21270;&#65292;&#25351;&#23548;&#31070;&#32463;&#32593;&#32476;&#20197;&#26356;&#24555;&#36895;&#30340;&#25910;&#25947;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#23398;&#20064;&#20016;&#23500;&#30340;&#28508;&#22312;&#25968;&#25454;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26497;&#22823;&#22320;&#22686;&#21152;&#20102;&#25105;&#20204;&#20174;&#22823;&#35268;&#27169;&#12289;&#39640;&#32500;&#24230;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#36328;&#36234;&#26080;&#25968;&#23398;&#31185;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20915;&#31574;&#19981;&#26131;&#35299;&#37322;&#65292;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#24314;&#31435;&#21644;&#35757;&#32451;&#23427;&#20204;&#26159;&#19981;&#30830;&#23450;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#32473;&#36825;&#20123;&#21162;&#21147;&#22686;&#21152;&#32467;&#26500;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#25968;&#23398;&#32467;&#26524;&#65292;&#20197;&#39640;&#25928;&#22320;&#27979;&#37327;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#25968;&#25454;&#26102;&#30340;&#29109;&#21464;&#21270;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#29109;&#30340;&#25439;&#22833;&#39033;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22270;&#20687;&#21387;&#32553;&#21644;&#22270;&#20687;&#20998;&#31867;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#25439;&#22833;&#39033;&#25351;&#23548;&#31070;&#32463;&#32593;&#32476;&#20197;&#26356;&#23569;&#30340;&#32500;&#24230;&#23398;&#20064;&#20016;&#23500;&#30340;&#28508;&#22312;&#25968;&#25454;&#34920;&#31034;&#65292;&#25910;&#25947;&#20110;&#26356;&#23569;&#30340;&#35757;&#32451;&#36718;&#27425;&#65292;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#27979;&#35797;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have dramatically increased our capacity to learn from large, high-dimensional datasets across innumerable disciplines. However, their decisions are not easily interpretable, their computational costs are high, and building and training them are uncertain processes. To add structure to these efforts, we derive new mathematical results to efficiently measure the changes in entropy as fully-connected and convolutional neural networks process data, and introduce entropy-based loss terms. Experiments in image compression and image classification on benchmark datasets demonstrate these losses guide neural networks to learn rich latent data representations in fewer dimensions, converge in fewer training epochs, and achieve better test metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;LEGO&#65292;&#36890;&#36807;&#38598;&#25104;&#22270;&#20248;&#21270;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#20013;&#30340;&#25968;&#25454;&#20851;&#32852;&#24615;&#33021;&#12290;&#20351;&#29992;LiDAR&#21333;&#29420;&#36827;&#34892;&#36319;&#36394;&#30340;LEGO&#26041;&#27861;&#22312;KITTI&#30446;&#26631;&#36319;&#36394;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09908</link><description>&lt;p&gt;
LEGO: &#23545;&#20110;&#22522;&#20110;&#28857;&#20113;&#30340;&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;
&lt;/p&gt;
&lt;p&gt;
LEGO: Learning and Graph-Optimized Modular Tracker for Online Multi-Object Tracking with Point Clouds. (arXiv:2308.09908v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;LEGO&#65292;&#36890;&#36807;&#38598;&#25104;&#22270;&#20248;&#21270;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#20013;&#30340;&#25968;&#25454;&#20851;&#32852;&#24615;&#33021;&#12290;&#20351;&#29992;LiDAR&#21333;&#29420;&#36827;&#34892;&#36319;&#36394;&#30340;LEGO&#26041;&#27861;&#22312;KITTI&#30446;&#26631;&#36319;&#36394;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MOT&#65289;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#36319;&#36394;-&#26816;&#27979;&#26041;&#27861;&#65292;&#25968;&#25454;&#20851;&#32852;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#65288;LEGO&#65289;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#20851;&#32852;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;LEGO&#36319;&#36394;&#22120;&#38598;&#25104;&#20102;&#22270;&#20248;&#21270;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#21046;&#23450;&#20851;&#32852;&#35780;&#20998;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20934;&#30830;&#39640;&#25928;&#30340;&#30446;&#26631;&#21305;&#37197;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#29366;&#24577;&#26356;&#26032;&#36807;&#31243;&#65292;&#26412;&#25991;&#36824;&#28155;&#21152;&#20102;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65292;&#36890;&#36807;&#23558;&#23545;&#35937;&#29366;&#24577;&#30340;&#26102;&#38388;&#36830;&#36143;&#24615;&#32435;&#20837;&#36319;&#36394;&#20013;&#65292;&#30830;&#20445;&#19968;&#33268;&#30340;&#36319;&#36394;&#12290;&#19982;&#20854;&#20182;&#22312;&#32447;&#36319;&#36394;&#26041;&#27861;&#65288;&#21253;&#25324;&#22522;&#20110;LiDAR&#21644;&#22522;&#20110;LiDAR-&#30456;&#26426;&#34701;&#21512;&#30340;&#26041;&#27861;&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20165;&#21033;&#29992;LiDAR&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#24615;&#33021;&#12290;&#22312;&#25552;&#20132;&#32467;&#26524;&#33267;KITTI&#30446;&#26631;&#36319;&#36394;&#35780;&#20272;&#25490;&#34892;&#27036;&#26102;&#65292;LEGO&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online multi-object tracking (MOT) plays a pivotal role in autonomous systems. The state-of-the-art approaches usually employ a tracking-by-detection method, and data association plays a critical role. This paper proposes a learning and graph-optimized (LEGO) modular tracker to improve data association performance in the existing literature. The proposed LEGO tracker integrates graph optimization and self-attention mechanisms, which efficiently formulate the association score map, facilitating the accurate and efficient matching of objects across time frames. To further enhance the state update process, the Kalman filter is added to ensure consistent tracking by incorporating temporal coherence in the object states. Our proposed method utilizing LiDAR alone has shown exceptional performance compared to other online tracking approaches, including LiDAR-based and LiDAR-camera fusion-based methods. LEGO ranked 1st at the time of submitting results to KITTI object tracking evaluation ranki
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#8212;&#8212;&#36234;&#30028;&#36951;&#24536;&#65292;&#24403;&#32473;&#23450;&#31867;&#21035;&#24341;&#20837;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#23427;&#20250;&#26174;&#30528;&#21066;&#24369;&#35813;&#31867;&#21035;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.00427</link><description>&lt;p&gt;
&#38024;&#23545;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#30340;&#36807;&#24230;&#36951;&#24536;&#65306;&#36830;&#32493;&#23398;&#20064;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution forgetting: vulnerability of continual learning to intra-class distribution shift. (arXiv:2306.00427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00427
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#8212;&#8212;&#36234;&#30028;&#36951;&#24536;&#65292;&#24403;&#32473;&#23450;&#31867;&#21035;&#24341;&#20837;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#23427;&#20250;&#26174;&#30528;&#21066;&#24369;&#35813;&#31867;&#21035;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26159;&#35753;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#24037;&#20316;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#22312;&#32852;&#21512;&#23398;&#20064;&#20013;&#65292;&#20154;&#20204;&#24050;&#32463;&#30693;&#36947;&#30001;&#24847;&#22270;&#25915;&#20987;&#25110;&#29615;&#22659;&#25200;&#21160;&#24341;&#36215;&#30340;&#36234;&#30028;&#38382;&#39064;&#20005;&#37325;&#24433;&#21709;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#36830;&#32493;&#23398;&#20064;&#35774;&#32622;&#20013;&#30001;&#36234;&#30028;&#38382;&#39064;&#24341;&#36215;&#30340;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#36234;&#30028;&#36951;&#24536;&#65288;OODF&#65289;&#12290;&#22312;&#36830;&#32493;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#38024;&#23545;&#32473;&#23450;&#31867;&#21035;&#65292;&#24341;&#20837;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#26174;&#30528;&#21066;&#24369;&#20102;&#21518;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#35813;&#31867;&#21035;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#31181;&#29616;&#35937;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#32780;&#35328;&#26159;&#29305;&#27530;&#30340;&#65292;&#22240;&#20026;&#21516;&#26679;&#32423;&#21035;&#30340;&#20998;&#24067;&#36716;&#31227;&#21482;&#26377;&#24494;&#19981;&#36275;&#36947;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) is an important technique to allow artificial neural networks to work in open environments. CL enables a system to learn new tasks without severe interference to its performance on old tasks, i.e., overcome the problems of catastrophic forgetting. In joint learning, it is well known that the out-of-distribution (OOD) problem caused by intentional attacks or environmental perturbations will severely impair the ability of networks to generalize. In this work, we reported a special form of catastrophic forgetting raised by the OOD problem in continual learning settings, and we named it out-of-distribution forgetting (OODF). In continual image classification tasks, we found that for a given category, introducing an intra-class distribution shift significantly impaired the recognition accuracy of CL methods for that category during subsequent learning. Interestingly, this phenomenon is special for CL as the same level of distribution shift had only negligible effects
&lt;/p&gt;</description></item></channel></rss>