<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#35268;&#21010;&#12289;&#20223;&#30495;&#21644;&#20851;&#38190;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#35201;&#36129;&#29486;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#32763;&#35793;&#33021;&#21147;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#39550;&#39542;&#22330;&#26223;&#21019;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01105</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey for Foundation Models in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#35268;&#21010;&#12289;&#20223;&#30495;&#21644;&#20851;&#38190;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#35201;&#36129;&#29486;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#32763;&#35793;&#33021;&#21147;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#39550;&#39542;&#22330;&#26223;&#21019;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21457;&#29983;&#20102;&#38761;&#21629;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#23545;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#23637;&#31034;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#20316;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#35268;&#21010;&#21644;&#20223;&#30495;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#22312;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#20851;&#38190;&#20219;&#21153;&#20013;&#24471;&#21040;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#20197;&#21450;&#20026;&#20223;&#30495;&#21644;&#27979;&#35797;&#21019;&#24314;&#36924;&#30495;&#30340;&#39550;&#39542;&#22330;&#26223;&#12290;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#25972;&#21512;&#22810;&#26679;&#30340;&#36755;&#20837;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#65292;&#23545;&#20110;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20998;&#31867;&#65292;&#26681;&#25454;&#27169;&#24577;&#21644;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#21151;&#33021;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of foundation models has revolutionized the fields of natural language processing and computer vision, paving the way for their application in autonomous driving (AD). This survey presents a comprehensive review of more than 40 research papers, demonstrating the role of foundation models in enhancing AD. Large language models contribute to planning and simulation in AD, particularly through their proficiency in reasoning, code generation and translation. In parallel, vision foundation models are increasingly adapted for critical tasks such as 3D object detection and tracking, as well as creating realistic driving scenarios for simulation and testing. Multi-modal foundation models, integrating diverse inputs, exhibit exceptional visual understanding and spatial reasoning, crucial for end-to-end AD. This survey not only provides a structured taxonomy, categorizing foundation models based on their modalities and functionalities within the AD domain but also delves into the meth
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;AVicuna&#65292;&#29983;&#25104;&#20102;PU-VALOR&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#38899;&#39057;-&#35270;&#35273;&#26102;&#38388;&#25351;&#20195;&#23545;&#35805;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#32570;&#20047;&#20934;&#30830;&#26102;&#38388;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21644;&#25972;&#21512;&#22797;&#26434;&#26102;&#38388;&#32447;&#32034;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.16276</link><description>&lt;p&gt;
AVicuna&#65306;&#20855;&#26377;&#20132;&#38169;&#22120;&#21644;&#19978;&#19979;&#25991;&#36793;&#30028;&#23545;&#40784;&#30340;&#38899;&#39057;-&#35270;&#35273;LLM&#29992;&#20110;&#26102;&#38388;&#25351;&#20195;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary Alignment for Temporal Referential Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16276
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;AVicuna&#65292;&#29983;&#25104;&#20102;PU-VALOR&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#38899;&#39057;-&#35270;&#35273;&#26102;&#38388;&#25351;&#20195;&#23545;&#35805;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#32570;&#20047;&#20934;&#30830;&#26102;&#38388;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21644;&#25972;&#21512;&#22797;&#26434;&#26102;&#38388;&#32447;&#32034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#20132;&#27969;&#20013;&#65292;&#20154;&#31867;&#32463;&#24120;&#20351;&#29992;&#35821;&#38899;&#21644;&#25163;&#21183;&#26469;&#25351;&#20195;&#29305;&#23450;&#21306;&#22495;&#25110;&#23545;&#35937;&#65292;&#36825;&#20010;&#36807;&#31243;&#31216;&#20026;&#25351;&#20195;&#23545;&#35805;&#65288;RD&#65289;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25110;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#38745;&#24577;&#29615;&#22659;&#20013;&#35843;&#26597;&#20102;RD&#65292;&#20294;&#22312;&#38899;&#39057;-&#35270;&#35273;&#23186;&#20307;&#20013;&#25506;&#32034;&#26102;&#38388;&#25351;&#20195;&#23545;&#35805;&#65288;TRD&#65289;&#20173;&#28982;&#26377;&#38480;&#12290;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#38459;&#30861;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#65306;&#65288;1&#65289;&#32570;&#20047;&#20855;&#26377;&#31934;&#30830;&#26102;&#38388;&#27880;&#37322;&#30340;&#20840;&#38754;&#26410;&#20462;&#21098;&#38899;&#39057;-&#35270;&#35273;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#65288;2&#65289;&#38656;&#35201;&#26377;&#25928;&#25972;&#21512;&#22797;&#26434;&#30340;&#26102;&#38388;&#21548;&#35273;&#21644;&#35270;&#35273;&#32447;&#32034;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29983;&#25104;PU-VALOR&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;114,000&#20010;&#26410;&#20462;&#21098;&#35270;&#39057;&#30340;&#24191;&#27867;&#38899;&#39057;-&#35270;&#35273;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;&#20102;AVicuna&#65292;&#20855;&#26377;&#38899;&#39057;-&#35270;&#35273;&#20196;&#29260;&#20132;&#38169;&#22120;&#65288;AVTI&#65289;&#65292;&#30830;&#20445;&#20102;&#26102;&#38388;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16276v1 Announce Type: cross  Abstract: In everyday communication, humans frequently use speech and gestures to refer to specific areas or objects, a process known as Referential Dialogue (RD). While prior studies have investigated RD through Large Language Models (LLMs) or Large Multimodal Models (LMMs) in static contexts, the exploration of Temporal Referential Dialogue (TRD) within audio-visual media remains limited. Two primary challenges hinder progress in this field: (1) the absence of comprehensive, untrimmed audio-visual video datasets with precise temporal annotations, and (2) the need for methods to integrate complex temporal auditory and visual cues effectively. To address these challenges, we introduce a novel framework to generate PU-VALOR, an extensive audio-visual dataset comprising over 114,000 untrimmed videos with accurate temporal demarcations. We also present AVicuna, featuring an Audio-Visual Tokens Interleaver (AVTI) that ensures the temporal alignment 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;Intrinsic Vision-Language Hallucination&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65292;&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#20135;&#29983;&#30340;&#21407;&#22240;&#21644;&#21453;&#26144;&#12290;</title><link>https://arxiv.org/abs/2403.11116</link><description>&lt;p&gt;
&#21338;&#22763;&#35770;&#25991;&#65306;&#19968;&#20010;&#25552;&#31034;&#30340;&#35270;&#35273;&#24187;&#35273;&#35780;&#20272;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PhD: A Prompted Visual Hallucination Evaluation Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;Intrinsic Vision-Language Hallucination&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65292;&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#20135;&#29983;&#30340;&#21407;&#22240;&#21644;&#21453;&#26144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#22686;&#38271;&#25512;&#21160;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#21457;&#23637;&#12290;&#22312;LLMs&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#24187;&#35273;&#25361;&#25112;&#20063;&#20986;&#29616;&#22312;LVLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;LVLM&#20013;&#30340;&#23545;&#35937;&#24187;&#35273;&#19978;&#65292;&#24573;&#30053;&#20102;LVLM&#24187;&#35273;&#30340;&#22810;&#26679;&#21270;&#31867;&#22411;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22266;&#26377;&#35270;&#35273;&#35821;&#35328;&#24187;&#35273;&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#65292;&#23545;&#23548;&#33268;&#24187;&#35273;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;IVL-Hallu&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#23427;&#20204;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65306;&#65288;a&#65289;&#23545;&#35937;&#24187;&#35273;&#65292;&#30001;&#20110;&#23545;&#35937;&#30340;&#35823;&#35782;&#21035;&#32780;&#20135;&#29983;&#65292;&#65288;b&#65289;&#23646;&#24615;&#24187;&#35273;&#65292;&#30001;&#20110;&#23646;&#24615;&#30340;&#35823;&#35782;&#21035;&#32780;&#24341;&#36215;&#65292;&#65288;c&#65289;&#22810;&#27169;&#24577;&#20914;&#31361;&#24187;&#35273;&#65292;&#28304;&#33258;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#20043;&#38388;&#30340;&#30683;&#30462;&#65292;&#20197;&#21450;&#65288;d&#65289;&#21453;&#24120;&#35782;&#24187;&#35273;&#65292;&#30001;&#20110;&#23545;&#31435;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11116v1 Announce Type: cross  Abstract: The rapid growth of Large Language Models (LLMs) has driven the development of Large Vision-Language Models (LVLMs). The challenge of hallucination, prevalent in LLMs, also emerges in LVLMs. However, most existing efforts mainly focus on object hallucination in LVLM, ignoring diverse types of LVLM hallucinations. In this study, we delve into the Intrinsic Vision-Language Hallucination (IVL-Hallu) issue, thoroughly analyzing different types of IVL-Hallu on their causes and reflections. Specifically, we propose several novel IVL-Hallu tasks and categorize them into four types: (a) object hallucination, which arises from the misidentification of objects, (b) attribute hallucination, which is caused by the misidentification of attributes, (c) multi-modal conflicting hallucination, which derives from the contradictions between textual and visual information, and (d) counter-common-sense hallucination, which owes to the contradictions betwee
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DiffuMatting&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#25216;&#26415;&#23454;&#29616;&#20102;&#8220;&#25248;&#22270;&#20219;&#20309;&#29289;&#20307;&#8221;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#27880;&#37322;&#65292;&#21516;&#26102;&#20860;&#23481;&#31038;&#21306;LoRAs&#25110;&#21508;&#31181;&#26465;&#20214;&#25511;&#21046;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.06168</link><description>&lt;p&gt;
DiffuMatting&#65306;&#20351;&#29992;Matting&#32423;&#21035;&#26631;&#27880;&#21512;&#25104;&#20219;&#24847;&#23545;&#35937;
&lt;/p&gt;
&lt;p&gt;
DiffuMatting: Synthesizing Arbitrary Objects with Matting-level Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06168
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DiffuMatting&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#25216;&#26415;&#23454;&#29616;&#20102;&#8220;&#25248;&#22270;&#20219;&#20309;&#29289;&#20307;&#8221;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#27880;&#37322;&#65292;&#21516;&#26102;&#20860;&#23481;&#31038;&#21306;LoRAs&#25110;&#21508;&#31181;&#26465;&#20214;&#25511;&#21046;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33719;&#21462;&#39640;&#24230;&#20934;&#30830;&#25110;&#25248;&#22270;&#27880;&#37322;&#30340;&#22256;&#38590;&#21644;&#21171;&#21160;&#23494;&#38598;&#24615;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#39640;&#24230;&#20934;&#30830;&#26631;&#31614;&#25968;&#37327;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;DiffuMatting&#65292;&#23427;&#32487;&#25215;&#20102;&#25193;&#25955;&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#36171;&#20104;&#20102;&#8220;&#25248;&#22270;&#20219;&#20309;&#29289;&#20307;&#8221;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;DiffuMatting&#21487;&#20197;&#65306;1&#65289;&#20316;&#20026;&#19968;&#20010;&#20855;&#26377;&#39640;&#20934;&#30830;&#24230;&#27880;&#37322;&#30340;&#20219;&#24847;&#25248;&#22270;&#24037;&#21378;&#65307;2&#65289;&#19982;&#31038;&#21306;LoRAs&#25110;&#21508;&#31181;&#26465;&#20214;&#25511;&#21046;&#26041;&#27861;&#20860;&#23481;&#65292;&#20197;&#23454;&#29616;&#31038;&#21306;&#21451;&#22909;&#30340;&#33402;&#26415;&#35774;&#35745;&#21644;&#21487;&#25511;&#29983;&#25104;&#12290;&#20855;&#20307;&#22320;&#65292;&#21463;&#32511;&#24149;&#25248;&#20687;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#25945;&#25480;&#25193;&#25955;&#27169;&#22411;&#22312;&#22266;&#23450;&#30340;&#32511;&#24149;&#30011;&#24067;&#19978;&#32472;&#30011;&#12290;&#20026;&#27492;&#65292;&#25910;&#38598;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#32511;&#24149;&#25968;&#25454;&#38598;&#65288;Green100K&#65289;&#20316;&#20026;DiffuMatting&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32511;&#33394;&#32972;&#26223;&#25511;&#21046;&#25439;&#22833;&#65292;&#20197;&#20445;&#25345;&#30011;&#24067;&#20026;&#32431;&#32511;&#33394;&#20197;&#36827;&#34892;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06168v1 Announce Type: cross  Abstract: Due to the difficulty and labor-consuming nature of getting highly accurate or matting annotations, there only exists a limited amount of highly accurate labels available to the public. To tackle this challenge, we propose a DiffuMatting which inherits the strong Everything generation ability of diffusion and endows the power of "matting anything". Our DiffuMatting can 1). act as an anything matting factory with high accurate annotations 2). be well-compatible with community LoRAs or various conditional control approaches to achieve the community-friendly art design and controllable generation. Specifically, inspired by green-screen-matting, we aim to teach the diffusion model to paint on a fixed green screen canvas. To this end, a large-scale greenscreen dataset (Green100K) is collected as a training dataset for DiffuMatting. Secondly, a green background control loss is proposed to keep the drawing board as a pure green color to disti
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LadaGAN&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#65292;&#36890;&#36807;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09596</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#21147;Transformer&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Efficient generative adversarial networks using linear additive-attention Transformers. (arXiv:2401.09596v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LadaGAN&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#65292;&#36890;&#36807;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#31561;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#35745;&#31639;&#22797;&#26434;&#30340;&#26550;&#26500;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30740;&#31350;&#23454;&#39564;&#23460;&#21644;&#36164;&#28304;&#20805;&#36275;&#30340;&#20844;&#21496;&#20013;&#30340;&#37319;&#29992;&#21644;&#20351;&#29992;&#65292;&#21516;&#26102;&#20063;&#26497;&#22823;&#22320;&#22686;&#21152;&#20102;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#30340;&#30899;&#36275;&#36857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LadaGAN&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#24314;&#31435;&#22312;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#19978;&#12290;&#35813;&#22359;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#19968;&#20010;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#65292;&#23427;&#27599;&#20010;&#22836;&#37096;&#35745;&#31639;&#19968;&#20010;&#27880;&#24847;&#21521;&#37327;&#65292;&#32780;&#19981;&#26159;&#20108;&#27425;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#22312;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#20013;&#37117;&#37319;&#29992;&#20102;Ladaformer&#65292;&#36825;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#20811;&#26381;&#20102;Transformer GAN&#32463;&#24120;&#20986;&#29616;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#12290;LadaGAN&#19968;&#30452;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;GANs&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the capacity of deep generative models for image generation, such as Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has dramatically improved in recent years, much of their success can be attributed to computationally expensive architectures. This has limited their adoption and use to research laboratories and companies with large resources, while significantly raising the carbon footprint for training, fine-tuning, and inference. In this work, we present LadaGAN, an efficient generative adversarial network that is built upon a novel Transformer block named Ladaformer. The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention. We employ Ladaformer in both the generator and discriminator, which reduces the computational complexity and overcomes the training instabilities often associated with Transformer GANs. LadaGAN consistently outperforms exist
&lt;/p&gt;</description></item><item><title>Kosmos-2.5&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25991;&#23398;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#26426;&#22120;&#38405;&#35835;&#25991;&#26412;&#23494;&#38598;&#22411;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#31354;&#38388;&#24863;&#30340;&#25991;&#26412;&#22359;&#21644;&#32467;&#26500;&#21270;&#25991;&#26412;&#36755;&#20986;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#25552;&#31034;&#19979;&#20219;&#20309;&#25991;&#26412;&#23494;&#38598;&#22411;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#25193;&#23637;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.11419</link><description>&lt;p&gt;
Kosmos-2.5: &#19968;&#20010;&#22810;&#27169;&#24577;&#25991;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Kosmos-2.5: A Multimodal Literate Model. (arXiv:2309.11419v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11419
&lt;/p&gt;
&lt;p&gt;
Kosmos-2.5&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25991;&#23398;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#26426;&#22120;&#38405;&#35835;&#25991;&#26412;&#23494;&#38598;&#22411;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#31354;&#38388;&#24863;&#30340;&#25991;&#26412;&#22359;&#21644;&#32467;&#26500;&#21270;&#25991;&#26412;&#36755;&#20986;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#25552;&#31034;&#19979;&#20219;&#20309;&#25991;&#26412;&#23494;&#38598;&#22411;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#25193;&#23637;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Kosmos-2.5&#65292;&#19968;&#20010;&#29992;&#20110;&#23545;&#25991;&#26412;&#23494;&#38598;&#22411;&#22270;&#20687;&#36827;&#34892;&#26426;&#22120;&#38405;&#35835;&#30340;&#22810;&#27169;&#24577;&#25991;&#23398;&#27169;&#22411;&#12290;Kosmos-2.5&#22312;&#20004;&#20010;&#19981;&#21516;&#20294;&#30456;&#20114;&#21512;&#20316;&#30340;&#36716;&#24405;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65306;(1) &#29983;&#25104;&#20855;&#26377;&#31354;&#38388;&#24863;&#30340;&#25991;&#26412;&#22359;&#65292;&#20854;&#20013;&#27599;&#20010;&#25991;&#26412;&#22359;&#37117;&#34987;&#36171;&#20104;&#20854;&#22312;&#22270;&#20687;&#20013;&#30340;&#31354;&#38388;&#22352;&#26631;&#65292;&#20197;&#21450;(2) &#29983;&#25104;&#20197;Markdown&#26684;&#24335;&#25429;&#25417;&#26679;&#24335;&#21644;&#32467;&#26500;&#30340;&#32467;&#26500;&#21270;&#25991;&#26412;&#36755;&#20986;&#12290;&#36825;&#31181;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#25991;&#23398;&#33021;&#21147;&#26159;&#36890;&#36807;&#20849;&#20139;&#30340;Transformer&#26550;&#26500;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#21644;&#28789;&#27963;&#30340;&#25991;&#26412;&#34920;&#31034;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#22312;&#31471;&#21040;&#31471;&#30340;&#25991;&#26723;&#32423;&#25991;&#26412;&#35782;&#21035;&#21644;&#22270;&#20687;&#21040;Markdown&#25991;&#26412;&#29983;&#25104;&#19978;&#35780;&#20272;&#20102;Kosmos-2.5&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#36731;&#26494;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#25552;&#31034;&#30340;&#20219;&#20309;&#25991;&#26412;&#23494;&#38598;&#22411;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#65292;&#20351;&#20854;&#25104;&#20026;&#28041;&#21450;&#25991;&#26412;&#20016;&#23500;&#22270;&#20687;&#30340;&#23454;&#38469;&#24212;&#29992;&#30340;&#36890;&#29992;&#24037;&#20855;&#12290;&#36825;&#39033;&#24037;&#20316;&#36824;&#20026;&#26410;&#26469;&#30340;&#25193;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Kosmos-2.5, a multimodal literate model for machine reading of text-intensive images. Pre-trained on large-scale text-intensive images, Kosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned its spatial coordinates within the image, and (2) producing structured text output that captures styles and structures into the markdown format. This unified multimodal literate capability is achieved through a shared Transformer architecture, task-specific prompts, and flexible text representations. We evaluate Kosmos-2.5 on end-to-end document-level text recognition and image-to-markdown text generation. Furthermore, the model can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images. This work also paves the way for the future scaling o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;MSc-iNCD&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#30340;&#23398;&#20064;&#20013;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.15975</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#20855;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery. (arXiv:2303.15975v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;MSc-iNCD&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#30340;&#23398;&#20064;&#20013;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#21629;&#38271;&#23398;&#20064;&#32773;&#20013;&#65292;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#36830;&#32493;&#22320;&#21457;&#29616;&#26032;&#27010;&#24565;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26399;&#26395;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#36825;&#31867;&#38382;&#39064;&#22312;&#38750;&#24120;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#37096;&#20998;&#35299;&#20915;&#65292;&#20854;&#20013;&#35201;&#20040;&#20026;&#21457;&#29616;&#26032;&#27010;&#24565;&#25552;&#20379;&#26377;&#26631;&#21495;&#30340;&#25968;&#25454;&#65288;&#20363;&#22914; NCD&#65289;&#65292;&#35201;&#20040;&#23398;&#20064;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#22686;&#37327;&#27493;&#39588;&#20013;&#21457;&#29983;&#65288;&#20363;&#22914;&#31867; iNCD&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026; MSc-iNCD&#65292;&#20854;&#20013;&#23398;&#20064;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#65292;&#24182;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#22522;&#32447;&#65292;&#19981;&#20165;&#22312;&#36739;&#38271;&#30340;&#23398;&#20064;&#24773;&#22659;&#19979;&#20855;&#26377;&#24377;&#24615;&#65292;&#32780;&#19988;&#19982;&#22797;&#26434;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#32447;&#30340;&#26377;&#25928;&#24615;&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#22522;&#20934;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering novel concepts from unlabelled data and in a continuous manner is an important desideratum of lifelong learners. In the literature such problems have been partially addressed under very restricted settings, where either access to labelled data is provided for discovering novel concepts (e.g., NCD) or learning occurs for a limited number of incremental steps (e.g., class-iNCD). In this work we challenge the status quo and propose a more challenging and practical learning paradigm called MSc-iNCD, where learning occurs continuously and unsupervisedly, while exploiting the rich priors from large-scale pre-trained models. To this end, we propose simple baselines that are not only resilient under longer learning scenarios, but are surprisingly strong when compared with sophisticated state-of-the-art methods. We conduct extensive empirical evaluation on a multitude of benchmarks and show the effectiveness of our proposed baselines, which significantly raises the bar.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#27169;&#22411;&#21270;&#25277;&#35937;&#30446;&#26631;&#65292;&#20197;&#38477;&#20302;&#34892;&#21160;&#39044;&#27979;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#39044;&#27979;&#27169;&#22411;&#12290;&#20351;&#29992;&#35270;&#35273;&#34920;&#24449;&#26469;&#25551;&#36848;&#21160;&#20316;&#21644;&#30446;&#26631;&#20449;&#24687;&#65292;&#24182;&#35774;&#35745;&#25277;&#35937;&#30446;&#26631;&#20026;&#19968;&#20010;&#20998;&#24067;&#12290;&#35813;&#27169;&#22411;&#21487;&#22312;Epic-Kitchen&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.05044</link><description>&lt;p&gt;
&#24314;&#27169;&#25277;&#35937;&#30446;&#26631;&#39044;&#27979;&#19979;&#19968;&#27493;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Predicting the Next Action by Modeling the Abstract Goal. (arXiv:2209.05044v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05044
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#27169;&#22411;&#21270;&#25277;&#35937;&#30446;&#26631;&#65292;&#20197;&#38477;&#20302;&#34892;&#21160;&#39044;&#27979;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#39044;&#27979;&#27169;&#22411;&#12290;&#20351;&#29992;&#35270;&#35273;&#34920;&#24449;&#26469;&#25551;&#36848;&#21160;&#20316;&#21644;&#30446;&#26631;&#20449;&#24687;&#65292;&#24182;&#35774;&#35745;&#25277;&#35937;&#30446;&#26631;&#20026;&#19968;&#20010;&#20998;&#24067;&#12290;&#35813;&#27169;&#22411;&#21487;&#22312;Epic-Kitchen&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20154;&#31867;&#21160;&#20316;&#30340;&#38382;&#39064;&#20855;&#26377;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#26159;&#65292;&#22914;&#26524;&#25105;&#20204;&#26377;&#20851;&#20110;&#21160;&#20316;&#23454;&#29616;&#30446;&#26631;&#30340;&#24863;&#30693;&#65292;&#21487;&#20197;&#38477;&#20302;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#21160;&#39044;&#27979;&#27169;&#22411;&#65292;&#21033;&#29992;&#30446;&#26631;&#20449;&#24687;&#26469;&#20943;&#23569;&#26410;&#26469;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#35270;&#35273;&#34920;&#24449;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#21160;&#20316;&#21644;&#30446;&#26631;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#27492;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#25277;&#35937;&#30446;&#26631;&#30340;&#26032;&#27010;&#24565;&#65292;&#20854;&#21462;&#20915;&#20110;&#35266;&#23519;&#21040;&#30340;&#35270;&#35273;&#29305;&#24449;&#24207;&#21015;&#65292;&#29992;&#20110;&#34892;&#21160;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#25277;&#35937;&#30446;&#26631;&#35774;&#35745;&#20026;&#19968;&#20010;&#20998;&#24067;&#65292;&#20854;&#21442;&#25968;&#26159;&#20351;&#29992;&#21464;&#20998;&#36882;&#24402;&#32593;&#32476;&#20272;&#35745;&#30340;&#12290;&#25105;&#20204;&#23545;&#19979;&#19968;&#20010;&#21160;&#20316;&#36827;&#34892;&#22810;&#27425;&#37319;&#26679;&#65292;&#24182;&#24341;&#20837;&#30446;&#26631;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#30830;&#23450;&#20174;&#25277;&#35937;&#30446;&#26631;&#24471;&#20986;&#30340;&#26368;&#20339;&#20505;&#36873;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;Epic-Kitchen&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of anticipating human actions is an inherently uncertain one. However, we can reduce this uncertainty if we have a sense of the goal that the actor is trying to achieve. Here, we present an action anticipation model that leverages goal information for the purpose of reducing the uncertainty in future predictions. Since we do not possess goal information or the observed actions during inference, we resort to visual representation to encapsulate information about both actions and goals. Through this, we derive a novel concept called abstract goal which is conditioned on observed sequences of visual features for action anticipation. We design the abstract goal as a distribution whose parameters are estimated using a variational recurrent network. We sample multiple candidates for the next action and introduce a goal consistency measure to determine the best candidate that follows from the abstract goal. Our method obtains impressive results on the very challenging Epic-Kitchen
&lt;/p&gt;</description></item></channel></rss>