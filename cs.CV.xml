<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#30340;&#28151;&#21512;SNN-ANN&#32593;&#32476;&#65292;&#21253;&#25324;&#20102;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26725;&#25509;&#27169;&#22359;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#31232;&#30095;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10173</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#30340;&#28151;&#21512;SNN-ANN&#32593;&#32476;&#65292;&#20855;&#26377;&#31354;&#38388;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Hybrid SNN-ANN Network for Event-based Object Detection with Spatial and Temporal Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10173
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#30340;&#28151;&#21512;SNN-ANN&#32593;&#32476;&#65292;&#21253;&#25324;&#20102;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26725;&#25509;&#27169;&#22359;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#31232;&#30095;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#25552;&#20379;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#21160;&#24577;&#33539;&#22260;&#65292;&#20960;&#20046;&#27809;&#26377;&#36816;&#21160;&#27169;&#31946;&#65292;&#38750;&#24120;&#36866;&#21512;&#23545;&#35937;&#26816;&#27979;&#20219;&#21153;&#12290;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19982;&#20107;&#20214;&#39537;&#21160;&#24863;&#30693;&#25968;&#25454;&#22825;&#29983;&#21305;&#37197;&#65292;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#33021;&#22815;&#23454;&#29616;&#36229;&#20302;&#21151;&#32791;&#21644;&#20302;&#24310;&#36831;&#25512;&#26029;&#65292;&#32780;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#21017;&#23637;&#31034;&#20986;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#20855;&#26377;&#26356;&#22909;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#28151;&#21512;SNN-ANN&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#21033;&#29992;SNN&#21644;ANN&#20307;&#31995;&#32467;&#26500;&#30340;&#20248;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;&#27880;&#24847;&#21147;&#30340;SNN-ANN&#39592;&#24178;&#32593;&#32476;&#65292;&#29992;&#20110;&#20351;&#29992;&#20107;&#20214;&#30456;&#26426;&#36827;&#34892;&#23545;&#35937;&#26816;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;SNN-ANN&#26725;&#25509;&#27169;&#22359;&#65292;&#20174;SNN&#23618;&#20013;&#25429;&#25417;&#31232;&#30095;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#23494;&#38598;&#29305;&#24449;&#22270;&#65292;&#20379;&#39592;&#24178;&#32593;&#32476;&#30340;ANN&#37096;&#20998;&#20351;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10173v1 Announce Type: cross  Abstract: Event cameras offer high temporal resolution and dynamic range with minimal motion blur, making them promising for object detection tasks. While Spiking Neural Networks (SNNs) are a natural match for event-based sensory data and enable ultra-energy efficient and low latency inference on neuromorphic hardware, Artificial Neural Networks (ANNs) tend to display more stable training dynamics and faster convergence resulting in greater task performance. Hybrid SNN-ANN approaches are a promising alternative, enabling to leverage the strengths of both SNN and ANN architectures. In this work, we introduce the first Hybrid Attention-based SNN-ANN backbone for object detection using event cameras. We propose a novel Attention-based SNN-ANN bridge module to capture sparse spatial and temporal relations from the SNN layer and convert them into dense feature maps for the ANN part of the backbone. Experimental results demonstrate that our proposed m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#32422;&#26463;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#32465;&#23450;&#26368;&#23567;&#21644;&#26368;&#22823;&#25928;&#29992;&#20540;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#28857;&#24182;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.16442</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#37197;&#23545;&#27425;&#27169;&#27169;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;&#22823;&#20110;&#20869;&#23384;&#30340;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Distributed Larger-Than-Memory Subset Selection With Pairwise Submodular Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#32422;&#26463;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#32465;&#23450;&#26368;&#23567;&#21644;&#26368;&#22823;&#25928;&#29992;&#20540;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#28857;&#24182;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23398;&#20064;&#38382;&#39064;&#21462;&#20915;&#20110;&#23376;&#38598;&#36873;&#25321;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#30830;&#23450;&#19968;&#32452;&#37325;&#35201;&#21644;&#20195;&#34920;&#24615;&#30340;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21487;&#35777;&#20272;&#35745;&#36817;&#20284;&#20445;&#35777;&#30340;&#26032;&#39062;&#20998;&#24067;&#24335;&#32422;&#26463;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#32465;&#23450;&#26368;&#23567;&#21644;&#26368;&#22823;&#25928;&#29992;&#20540;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#28857;&#24182;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16442v1 Announce Type: cross  Abstract: Many learning problems hinge on the fundamental problem of subset selection, i.e., identifying a subset of important and representative points. For example, selecting the most significant samples in ML training cannot only reduce training costs but also enhance model quality. Submodularity, a discrete analogue of convexity, is commonly used for solving subset selection problems. However, existing algorithms for optimizing submodular functions are sequential, and the prior distributed methods require at least one central machine to fit the target subset. In this paper, we relax the requirement of having a central machine for the target subset by proposing a novel distributed bounding algorithm with provable approximation guarantees. The algorithm iteratively bounds the minimum and maximum utility values to select high quality points and discard the unimportant ones. When bounding does not find the complete subset, we use a multi-round, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#32467;&#21512;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#23545;&#35805;&#20013;&#36880;&#27493;&#29702;&#35299;&#23545;&#35805;&#21382;&#21490;&#21644;&#21560;&#25910;&#35270;&#39057;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07259</link><description>&lt;p&gt;
&#25581;&#31034;&#38544;&#34255;&#30340;&#32852;&#31995;&#65306;&#29992;&#20110;&#35270;&#39057;&#23545;&#35805;&#30340;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Uncovering Hidden Connections: Iterative Tracking and Reasoning for Video-grounded Dialog. (arXiv:2310.07259v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#32467;&#21512;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#23545;&#35805;&#20013;&#36880;&#27493;&#29702;&#35299;&#23545;&#35805;&#21382;&#21490;&#21644;&#21560;&#25910;&#35270;&#39057;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#35270;&#35273;&#38382;&#31572;&#30456;&#27604;&#65292;&#35270;&#39057;&#23545;&#35805;&#38656;&#35201;&#23545;&#23545;&#35805;&#21382;&#21490;&#21644;&#35270;&#39057;&#20869;&#23481;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#65292;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#38754;&#20020;&#36880;&#27493;&#29702;&#35299;&#22797;&#26434;&#30340;&#23545;&#35805;&#21382;&#21490;&#21644;&#21560;&#25910;&#35270;&#39057;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#23558;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#22120;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#20197;&#36335;&#24452;&#36319;&#36394;&#21644;&#32858;&#21512;&#26426;&#21046;&#20026;&#26680;&#24515;&#65292;&#33021;&#22815;&#20174;&#23545;&#35805;&#21382;&#21490;&#20013;&#33719;&#21462;&#37325;&#35201;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#20197;&#35299;&#37322;&#25152;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21033;&#29992;&#36845;&#20195;&#25512;&#29702;&#32593;&#32476;&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#21644;&#24378;&#35843;&#20851;&#38190;&#35270;&#35273;&#26631;&#35760;&#65292;&#22686;&#24378;&#23545;&#35270;&#35273;&#29702;&#35299;&#30340;&#28145;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;GPT-&#27169;&#22411;&#23558;&#36825;&#20123;&#20016;&#23500;&#30340;&#20449;&#24687;&#32508;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrast to conventional visual question answering, video-grounded dialog necessitates a profound understanding of both dialog history and video content for accurate response generation. Despite commendable strides made by existing methodologies, they often grapple with the challenges of incrementally understanding intricate dialog histories and assimilating video information. In response to this gap, we present an iterative tracking and reasoning strategy that amalgamates a textual encoder, a visual encoder, and a generator. At its core, our textual encoder is fortified with a path tracking and aggregation mechanism, adept at gleaning nuances from dialog history that are pivotal to deciphering the posed questions. Concurrently, our visual encoder harnesses an iterative reasoning network, meticulously crafted to distill and emphasize critical visual markers from videos, enhancing the depth of visual comprehension. Culminating this enriched information, we employ the pre-trained GPT-
&lt;/p&gt;</description></item><item><title>RelationMatch&#26159;&#19968;&#31181;&#21033;&#29992;&#30697;&#38453;&#20132;&#21449;&#29109;&#65288;MCE&#65289;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21305;&#37197;&#25209;&#20869;&#20851;&#31995;&#65292;&#26377;&#25928;&#25552;&#39640;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10397</link><description>&lt;p&gt;
RelationMatch&#65306;&#29992;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#25209;&#20869;&#20851;&#31995;&#21305;&#37197;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
RelationMatch: Matching In-batch Relationships for Semi-supervised Learning. (arXiv:2305.10397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10397
&lt;/p&gt;
&lt;p&gt;
RelationMatch&#26159;&#19968;&#31181;&#21033;&#29992;&#30697;&#38453;&#20132;&#21449;&#29109;&#65288;MCE&#65289;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21305;&#37197;&#25209;&#20869;&#20851;&#31995;&#65292;&#26377;&#25928;&#25552;&#39640;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#65292;&#24050;&#32463;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31639;&#27861;&#36890;&#24120;&#38598;&#20013;&#22312;&#26469;&#33258;&#30456;&#21516;&#26469;&#28304;&#30340;&#25104;&#23545;&#25968;&#25454;&#28857;&#30340;&#39044;&#27979;&#23545;&#20934;&#19978;&#65292;&#24182;&#24573;&#30053;&#20102;&#27599;&#20010;&#25209;&#27425;&#20869;&#30340;&#28857;&#38388;&#20851;&#31995;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;RelationMatch&#65292;&#23427;&#21033;&#29992;&#19968;&#31181;&#30697;&#38453;&#20132;&#21449;&#29109;&#65288;MCE&#65289;&#25439;&#22833;&#20989;&#25968;&#26469;&#21457;&#25496;&#25209;&#20869;&#20851;&#31995;&#12290;&#36890;&#36807;&#24212;&#29992;MCE&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22914;FixMatch&#21644;FlexMatch&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20165;&#20351;&#29992;40&#20010;&#26631;&#31614;&#30340;STL-10&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#30456;&#23545;&#20110;FlexMatch&#26377;15.21&#65285;&#30340;&#26174;&#33879;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;MCE&#24212;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning has achieved notable success by leveraging very few labeled data and exploiting the wealth of information derived from unlabeled data. However, existing algorithms usually focus on aligning predictions on paired data points augmented from an identical source, and overlook the inter-point relationships within each batch. This paper introduces a novel method, RelationMatch, which exploits in-batch relationships with a matrix cross-entropy (MCE) loss function. Through the application of MCE, our proposed method consistently surpasses the performance of established state-of-the-art methods, such as FixMatch and FlexMatch, across a variety of vision datasets. Notably, we observed a substantial enhancement of 15.21% in accuracy over FlexMatch on the STL-10 dataset using only 40 labels. Moreover, we apply MCE to supervised learning scenarios, and observe consistent improvements as well.
&lt;/p&gt;</description></item></channel></rss>