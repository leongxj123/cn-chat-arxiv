<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>UDEEP&#26159;&#19968;&#20010;&#22522;&#20110;&#36793;&#32536;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#24179;&#21488;&#65292;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#20837;&#20405;&#20449;&#21495;&#40857;&#34430;&#21644;&#24223;&#24323;&#22609;&#26009;&#23545;&#27700;&#29983;&#29983;&#24577;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.06157</link><description>&lt;p&gt;
UDEEP: &#22522;&#20110;&#36793;&#32536;&#30340;&#27700;&#19979;&#20449;&#21495;&#40857;&#34430;&#21644;&#22609;&#26009;&#26816;&#27979;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;
&lt;/p&gt;
&lt;p&gt;
UDEEP: Edge-based Computer Vision for In-Situ Underwater Crayfish and Plastic Detection. (arXiv:2401.06157v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06157
&lt;/p&gt;
&lt;p&gt;
UDEEP&#26159;&#19968;&#20010;&#22522;&#20110;&#36793;&#32536;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#24179;&#21488;&#65292;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#20837;&#20405;&#20449;&#21495;&#40857;&#34430;&#21644;&#24223;&#24323;&#22609;&#26009;&#23545;&#27700;&#29983;&#29983;&#24577;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20837;&#20405;&#30340;&#20449;&#21495;&#40857;&#34430;&#23545;&#29983;&#24577;&#31995;&#32479;&#36896;&#25104;&#20102;&#19981;&#21033;&#24433;&#21709;&#12290;&#23427;&#20204;&#20256;&#25773;&#20102;&#23545;&#33521;&#22269;&#21807;&#19968;&#30340;&#26412;&#22320;&#30333;&#29226;&#40857;&#34430;&#33268;&#21629;&#30340;&#30495;&#33740;&#22411;&#40857;&#34430;&#30239;&#30123;&#30149;(Aphanomyces astaci)&#12290;&#20837;&#20405;&#30340;&#20449;&#21495;&#40857;&#34430;&#24191;&#27867;&#25366;&#25496;&#27934;&#31348;&#65292;&#30772;&#22351;&#26646;&#24687;&#22320;&#65292;&#20405;&#34432;&#27827;&#23736;&#24182;&#23545;&#27700;&#36136;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#21516;&#26102;&#31454;&#20105;&#26412;&#22320;&#29289;&#31181;&#30340;&#36164;&#28304;&#24182;&#23548;&#33268;&#26412;&#22320;&#31181;&#32676;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#27745;&#26579;&#20063;&#20351;&#30333;&#29226;&#40857;&#34430;&#26356;&#21152;&#23481;&#26131;&#21463;&#21040;&#25439;&#23475;&#65292;&#20854;&#31181;&#32676;&#22312;&#33521;&#22269;&#26576;&#20123;&#22320;&#21306;&#19979;&#38477;&#36229;&#36807;90&#65285;&#65292;&#20351;&#20854;&#26497;&#26131;&#28626;&#20020;&#28781;&#32477;&#12290;&#20026;&#20102;&#20445;&#25252;&#27700;&#29983;&#29983;&#24577;&#31995;&#32479;&#65292;&#35299;&#20915;&#20837;&#20405;&#29289;&#31181;&#21644;&#24223;&#24323;&#22609;&#26009;&#23545;&#33521;&#22269;&#27827;&#27969;&#29983;&#24577;&#31995;&#32479;&#30340;&#25361;&#25112;&#33267;&#20851;&#37325;&#35201;&#12290;UDEEP&#24179;&#21488;&#21487;&#20197;&#36890;&#36807;&#23454;&#26102;&#20998;&#31867;&#20449;&#21495;&#40857;&#34430;&#21644;&#22609;&#26009;&#30862;&#29255;&#65292;&#20805;&#24403;&#29615;&#22659;&#30417;&#27979;&#30340;&#20851;&#38190;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invasive signal crayfish have a detrimental impact on ecosystems. They spread the fungal-type crayfish plague disease (Aphanomyces astaci) that is lethal to the native white clawed crayfish, the only native crayfish species in Britain. Invasive signal crayfish extensively burrow, causing habitat destruction, erosion of river banks and adverse changes in water quality, while also competing with native species for resources and leading to declines in native populations. Moreover, pollution exacerbates the vulnerability of White-clawed crayfish, with their populations declining by over 90% in certain English counties, making them highly susceptible to extinction. To safeguard aquatic ecosystems, it is imperative to address the challenges posed by invasive species and discarded plastics in the United Kingdom's river ecosystem's. The UDEEP platform can play a crucial role in environmental monitoring by performing on-the-fly classification of Signal crayfish and plastic debris while leveragi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeltaSpace&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#29992;&#20110;&#28789;&#27963;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#12290;&#22312;DeltaSpace&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;DeltaEdit&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#23558;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#24182;&#20174;CLIP&#39044;&#27979;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#28789;&#27963;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.08785</link><description>&lt;p&gt;
DeltaSpace:&#19968;&#31181;&#29992;&#20110;&#28789;&#27963;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#30340;&#35821;&#20041;&#23545;&#40784;&#29305;&#24449;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided Image Editing. (arXiv:2310.08785v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeltaSpace&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#29992;&#20110;&#28789;&#27963;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#12290;&#22312;DeltaSpace&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;DeltaEdit&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#23558;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#24182;&#20174;CLIP&#39044;&#27979;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#28789;&#27963;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#38754;&#20020;&#30528;&#35757;&#32451;&#21644;&#25512;&#29702;&#28789;&#27963;&#24615;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#35768;&#22810;&#25991;&#29486;&#36890;&#36807;&#25910;&#38598;&#22823;&#37327;&#26631;&#27880;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#26469;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#25991;&#26412;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#26082;&#26114;&#36149;&#21448;&#20302;&#25928;&#12290;&#28982;&#21518;&#65292;&#19968;&#20123;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#20986;&#29616;&#20102;&#65292;&#20197;&#36991;&#20813;&#25968;&#25454;&#25910;&#38598;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#22522;&#20110;&#27599;&#20010;&#25991;&#26412;&#25552;&#31034;&#30340;&#20248;&#21270;&#25110;&#25512;&#29702;&#26102;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35843;&#26597;&#21644;&#30830;&#23450;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#31354;&#38388;&#65292;&#31216;&#20026;CLIP DeltaSpace&#65292;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#65292;&#20004;&#20010;&#22270;&#20687;&#30340;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#19982;&#20854;&#23545;&#24212;&#30340;&#25991;&#26412;&#25551;&#36848;&#30340;CLIP&#25991;&#26412;&#29305;&#24449;&#24046;&#24322;&#22312;&#35821;&#20041;&#19978;&#26159;&#23545;&#40784;&#30340;&#12290;&#22522;&#20110;DeltaSpace&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DeltaEdit&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#23558;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#24182;&#20174;CLIP&#39044;&#27979;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-guided image editing faces significant challenges to training and inference flexibility. Much literature collects large amounts of annotated image-text pairs to train text-conditioned generative models from scratch, which is expensive and not efficient. After that, some approaches that leverage pre-trained vision-language models are put forward to avoid data collection, but they are also limited by either per text-prompt optimization or inference-time hyper-parameters tuning. To address these issues, we investigate and identify a specific space, referred to as CLIP DeltaSpace, where the CLIP visual feature difference of two images is semantically aligned with the CLIP textual feature difference of their corresponding text descriptions. Based on DeltaSpace, we propose a novel framework called DeltaEdit, which maps the CLIP visual feature differences to the latent space directions of a generative model during the training phase, and predicts the latent space directions from the CLIP
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992;MildTriple Loss&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#24182;&#27169;&#25311;&#36328;&#27169;&#24577;&#20154;&#31867;&#21160;&#20316;&#24207;&#21015;&#19982;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.04195</link><description>&lt;p&gt;
MildTriple Loss&#27169;&#22411;&#19979;&#30340;&#36816;&#21160;&#21644;&#25991;&#26412;&#36328;&#27169;&#24577;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Retrieval for Motion and Text via MildTriple Loss. (arXiv:2305.04195v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#27169;&#22411;&#65292;&#20351;&#29992;MildTriple Loss&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#24182;&#27169;&#25311;&#36328;&#27169;&#24577;&#20154;&#31867;&#21160;&#20316;&#24207;&#21015;&#19982;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#26816;&#32034;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#30740;&#31350;&#35838;&#39064;&#65292;&#38543;&#30528;&#22270;&#20687;&#25991;&#26412;&#21644;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;&#23613;&#31649;&#22312;&#34394;&#25311;&#29616;&#23454;&#31561;&#24191;&#27867;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#20294;&#20154;&#31867;&#21160;&#20316;&#24207;&#21015;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#26816;&#32034;&#23578;&#26410;&#24341;&#36215;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#36825;&#20010;&#20219;&#21153;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#23545;&#20004;&#31181;&#35821;&#35328;&#30340;&#20849;&#21516;&#24314;&#27169;&#65292;&#35201;&#27714;&#20174;&#25991;&#26412;&#20013;&#29702;&#35299;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20449;&#24687;&#65292;&#24182;&#20174;&#19977;&#32500;&#20154;&#20307;&#36816;&#21160;&#24207;&#21015;&#20013;&#23398;&#20064;&#34892;&#20026;&#29305;&#24449;&#12290;&#20197;&#24448;&#30340;&#36816;&#21160;&#25968;&#25454;&#24314;&#27169;&#20027;&#35201;&#20381;&#36182;&#20110;&#33258;&#22238;&#24402;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36825;&#21487;&#33021;&#20250;&#36951;&#24536;&#20197;&#21069;&#30340;&#20449;&#24687;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#36816;&#21160;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#20174;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#20013;&#23398;&#20064;&#34920;&#31034;&#24182;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
Cross-modal retrieval has become a prominent research topic in computer vision and natural language processing with advances made in image-text and video-text retrieval technologies. However, cross-modal retrieval between human motion sequences and text has not garnered sufficient attention despite the extensive application value it holds, such as aiding virtual reality applications in better understanding users' actions and language. This task presents several challenges, including joint modeling of the two modalities, demanding the understanding of person-centered information from text, and learning behavior features from 3D human motion sequences. Previous work on motion data modeling mainly relied on autoregressive feature extractors that may forget previous information, while we propose an innovative model that includes simple yet powerful transformer-based motion and text encoders, which can learn representations from the two different modalities and capture long-term dependencie
&lt;/p&gt;</description></item></channel></rss>