<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BehAVE&#30340;&#35270;&#39057;&#29702;&#35299;&#26694;&#26550;&#65292;&#20511;&#21161;&#29616;&#26377;&#30340;&#21830;&#19994;&#35270;&#39057;&#28216;&#25103;&#23454;&#29616;&#39046;&#22495;&#38543;&#26426;&#21270;&#65292;&#26080;&#38656;&#20223;&#30495;&#22120;&#30340;&#25903;&#25345;&#12290;&#36890;&#36807;&#21033;&#29992;&#28216;&#25103;&#20013;&#20016;&#23500;&#30340;&#35270;&#35273;&#22810;&#26679;&#24615;&#36827;&#34892;&#38543;&#26426;&#21270;&#65292;&#20197;&#21450;&#36890;&#36807;&#29609;&#23478;&#34892;&#20026;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#25351;&#23548;&#20855;&#26377;&#30456;&#20284;&#20869;&#23481;&#30340;&#35270;&#39057;&#30340;&#23545;&#40784;&#65292;BehAVE&#22312;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#38754;&#23637;&#29616;&#20102;&#40065;&#26834;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01335</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#39057;&#28216;&#25103;&#23454;&#29616;&#26080;&#20223;&#30495;&#22120;&#35270;&#35273;&#39046;&#22495;&#38543;&#26426;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simulator-Free Visual Domain Randomization via Video Games
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BehAVE&#30340;&#35270;&#39057;&#29702;&#35299;&#26694;&#26550;&#65292;&#20511;&#21161;&#29616;&#26377;&#30340;&#21830;&#19994;&#35270;&#39057;&#28216;&#25103;&#23454;&#29616;&#39046;&#22495;&#38543;&#26426;&#21270;&#65292;&#26080;&#38656;&#20223;&#30495;&#22120;&#30340;&#25903;&#25345;&#12290;&#36890;&#36807;&#21033;&#29992;&#28216;&#25103;&#20013;&#20016;&#23500;&#30340;&#35270;&#35273;&#22810;&#26679;&#24615;&#36827;&#34892;&#38543;&#26426;&#21270;&#65292;&#20197;&#21450;&#36890;&#36807;&#29609;&#23478;&#34892;&#20026;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#25351;&#23548;&#20855;&#26377;&#30456;&#20284;&#20869;&#23481;&#30340;&#35270;&#39057;&#30340;&#23545;&#40784;&#65292;BehAVE&#22312;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#38754;&#23637;&#29616;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#38543;&#26426;&#21270;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#65292;&#29992;&#20110;&#25552;&#39640;&#35270;&#35273;&#27169;&#22411;&#22312;&#35270;&#35273;&#19978;&#25130;&#28982;&#19981;&#21516;&#20294;&#20869;&#23481;&#30456;&#20284;&#30340;&#39046;&#22495;&#20013;&#30340;&#20256;&#36882;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#37327;&#20381;&#36182;&#20110;&#35843;&#25972;&#22797;&#26434;&#21644;&#19987;&#38376;&#30340;&#20223;&#30495;&#24341;&#25806;&#65292;&#36825;&#20123;&#24341;&#25806;&#30340;&#26500;&#24314;&#24456;&#22256;&#38590;&#65292;&#36827;&#32780;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#21487;&#34892;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BehAVE&#65292;&#19968;&#31181;&#35270;&#39057;&#29702;&#35299;&#26694;&#26550;&#65292;&#23427;&#29420;&#29305;&#22320;&#21033;&#29992;&#29616;&#26377;&#30340;&#21830;&#19994;&#35270;&#39057;&#28216;&#25103;&#26469;&#23454;&#29616;&#39046;&#22495;&#38543;&#26426;&#21270;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#23427;&#20204;&#30340;&#20223;&#30495;&#24341;&#25806;&#12290;&#22312;BehAVE&#19979;&#65292;(1) &#35270;&#39057;&#28216;&#25103;&#22266;&#26377;&#30340;&#20016;&#23500;&#35270;&#35273;&#22810;&#26679;&#24615;&#25104;&#20026;&#38543;&#26426;&#21270;&#30340;&#26469;&#28304;&#65292;(2) &#29609;&#23478;&#34892;&#20026; - &#36890;&#36807;&#21160;&#20316;&#30340;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#35821;&#20041;&#34920;&#31034; - &#24341;&#23548;&#20855;&#26377;&#30456;&#20284;&#20869;&#23481;&#30340;&#35270;&#39057;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#35270;&#39057;&#21644;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#19978;&#27979;&#35797;&#20102;BehAVE&#65292;&#24182;&#25253;&#21578;&#20102;&#23427;&#22312;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain randomization is an effective computer vision technique for improving transferability of vision models across visually distinct domains exhibiting similar content. Existing approaches, however, rely extensively on tweaking complex and specialized simulation engines that are difficult to construct, subsequently affecting their feasibility and scalability. This paper introduces BehAVE, a video understanding framework that uniquely leverages the plethora of existing commercial video games for domain randomization, without requiring access to their simulation engines. Under BehAVE (1) the inherent rich visual diversity of video games acts as the source of randomization and (2) player behavior -- represented semantically via textual descriptions of actions -- guides the *alignment* of videos with similar content. We test BehAVE on 25 games of the first-person shooter (FPS) genre across various video and text foundation models and we report its robustness for domain randomization. Beh
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;C-Flat&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24179;&#22374;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#21487;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#31616;&#21270;&#20102;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.00986</link><description>&lt;p&gt;
&#36890;&#36807;C-Flat&#20351;&#25345;&#32493;&#23398;&#20064;&#26356;&#24378;&#22823;
&lt;/p&gt;
&lt;p&gt;
Make Continual Learning Stronger via C-Flat
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00986
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;C-Flat&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24179;&#22374;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#21487;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#31616;&#21270;&#20102;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#20013;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#23545;&#20110;&#22788;&#29702;&#36830;&#32493;&#21040;&#36798;&#20219;&#21153;&#30340;&#21160;&#24577;&#26356;&#26032;&#30693;&#35782;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20026;&#20102;&#35299;&#20915;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#25935;&#24863;&#24615;-&#31283;&#23450;&#24615;&#22256;&#22659;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26435;&#37325;&#25439;&#22833;&#26223;&#35266;&#30340;&#38497;&#23789;&#24230;&#65292;&#23547;&#25214;&#20301;&#20110;&#20855;&#26377;&#32479;&#19968;&#20302;&#25439;&#22833;&#25110;&#24179;&#31283;&#26799;&#24230;&#30340;&#37051;&#22495;&#20013;&#30340;&#24179;&#22374;&#26368;&#23567;&#20540;&#65292;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#35757;&#32451;&#26041;&#24335;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#20248;&#21270;&#22120;&#22914;SGD&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#20316;&#21697;&#35752;&#35770;&#20102;&#36825;&#31181;&#35757;&#32451;&#26041;&#24335;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#35777;&#26126;&#29305;&#23450;&#35774;&#35745;&#30340;&#38646;&#38454;&#38497;&#23789;&#24230;&#20248;&#21270;&#22120;&#21487;&#20197;&#25552;&#21319;&#25345;&#32493;&#23398;&#20064;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Continual Flatness&#65288;C-Flat&#65289;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20026;&#25345;&#32493;&#23398;&#20064;&#23450;&#21046;&#30340;&#26356;&#24179;&#22374;&#30340;&#25439;&#22833;&#26223;&#35266;&#12290;C-Flat&#21482;&#38656;&#19968;&#34892;&#20195;&#30721;&#21363;&#21487;&#36731;&#26494;&#35843;&#29992;&#65292;&#24182;&#21487;&#19982;&#20219;&#20309;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#25554;&#25773;&#12290;C-Flat&#24212;&#29992;&#20110;&#25152;&#26377;&#25345;&#32493;&#23398;&#20064;&#31867;&#21035;&#30340;&#19968;&#33324;&#26694;&#26550;&#65292;&#24182;&#19982;&#25439;&#22833;&#26368;&#23567;&#21270;&#20248;&#21270;&#22120;&#36827;&#34892;&#20102;&#24443;&#24213;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00986v1 Announce Type: new  Abstract: Model generalization ability upon incrementally acquiring dynamically updating knowledge from sequentially arriving tasks is crucial to tackle the sensitivity-stability dilemma in Continual Learning (CL). Weight loss landscape sharpness minimization seeking for flat minima lying in neighborhoods with uniform low loss or smooth gradient is proven to be a strong training regime improving model generalization compared with loss minimization based optimizer like SGD. Yet only a few works have discussed this training regime for CL, proving that dedicated designed zeroth-order sharpness optimizer can improve CL performance. In this work, we propose a Continual Flatness (C-Flat) method featuring a flatter loss landscape tailored for CL. C-Flat could be easily called with only one line of code and is plug-and-play to any CL methods. A general framework of C-Flat applied to all CL categories and a thorough comparison with loss minima optimizer an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#32463;&#20856;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#25506;&#31350;&#65292;&#25581;&#31034;&#20102;&#20998;&#21106;TTA&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;&#32463;&#20856;TTA&#31574;&#30053;&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#24182;&#19981;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.05341</link><description>&lt;p&gt;
&#23545;&#35821;&#20041;&#20998;&#21106;&#20013;&#32463;&#20856;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#30340;&#25209;&#21028;&#24615;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Critical Look at Classic Test-Time Adaptation Methods in Semantic Segmentation. (arXiv:2310.05341v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05341
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#32463;&#20856;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#25506;&#31350;&#65292;&#25581;&#31034;&#20102;&#20998;&#21106;TTA&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;&#32463;&#20856;TTA&#31574;&#30053;&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#24182;&#19981;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;TTA&#65289;&#26088;&#22312;&#23558;&#26368;&#21021;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36866;&#24212;&#20110;&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#21487;&#33021;&#20998;&#24067;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;TTA&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#23545;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;TTA&#25506;&#32034;&#38750;&#24120;&#26377;&#38480;&#12290;&#36825;&#31181;&#23545;&#20998;&#31867;&#30340;&#31361;&#20986;&#37325;&#35270;&#21487;&#33021;&#23548;&#33268;&#35768;&#22810;&#26032;&#25163;&#21644;&#24037;&#31243;&#24072;&#38169;&#35823;&#22320;&#35748;&#20026;&#20026;&#20998;&#31867;&#35774;&#35745;&#30340;&#32463;&#20856;TTA&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#20998;&#21106;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20551;&#35774;&#20173;&#26410;&#32463;&#39564;&#35777;&#65292;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#20998;&#21106;TTA&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#30830;&#23450;&#32463;&#20856;TTA&#31574;&#30053;&#26159;&#21542;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#36825;&#19968;&#20219;&#21153;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#32467;&#26524;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#24120;&#29992;&#20110;&#20998;&#31867;TTA&#30340;&#32463;&#20856;&#25209;&#24402;&#19968;&#21270;&#26356;&#26032;&#31574;&#30053;&#21482;&#33021;&#24102;&#26469;&#36731;&#24494;&#30340;&#24615;&#33021;&#25913;&#21892;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#20250;&#23545;&#32467;&#26524;&#20135;&#29983;&#36870;&#21521;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-time adaptation (TTA) aims to adapt a model, initially trained on training data, to potential distribution shifts in the test data. Most existing TTA studies, however, focus on classification tasks, leaving a notable gap in the exploration of TTA for semantic segmentation. This pronounced emphasis on classification might lead numerous newcomers and engineers to mistakenly assume that classic TTA methods designed for classification can be directly applied to segmentation. Nonetheless, this assumption remains unverified, posing an open question. To address this, we conduct a systematic, empirical study to disclose the unique challenges of segmentation TTA, and to determine whether classic TTA strategies can effectively address this task. Our comprehensive results have led to three key observations. First, the classic batch norm updating strategy, commonly used in classification TTA, only brings slight performance improvement, and in some cases it might even adversely affect the resu
&lt;/p&gt;</description></item></channel></rss>