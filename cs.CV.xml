<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>SugarcaneNet2024&#26159;&#36890;&#36807;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24555;&#36895;&#20934;&#30830;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.18870</link><description>&lt;p&gt;
SugarcaneNet2024: LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
SugarcaneNet2024: An Optimized Weighted Average Ensemble Approach of LASSO Regularized Pre-trained Models for Sugarcane Disease Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18870
&lt;/p&gt;
&lt;p&gt;
SugarcaneNet2024&#26159;&#36890;&#36807;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24555;&#36895;&#20934;&#30830;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29976;&#34071;&#20316;&#20026;&#19990;&#30028;&#31958;&#19994;&#30340;&#20851;&#38190;&#20316;&#29289;&#65292;&#23481;&#26131;&#21463;&#22810;&#31181;&#30149;&#23475;&#20405;&#23475;&#65292;&#36825;&#20123;&#30149;&#23475;&#23545;&#20854;&#20135;&#37327;&#21644;&#36136;&#37327;&#37117;&#26377;&#37325;&#22823;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#26377;&#25928;&#31649;&#29702;&#21644;&#23454;&#26045;&#39044;&#38450;&#25514;&#26045;&#65292;&#24517;&#39035;&#21450;&#26102;&#20934;&#30830;&#22320;&#26816;&#27979;&#30149;&#23475;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SugarcaneNet2024&#30340;&#29420;&#29305;&#27169;&#22411;&#65292;&#36890;&#36807;&#21494;&#29255;&#22270;&#20687;&#22788;&#29702;&#65292;&#33021;&#22815;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#33258;&#21160;&#24555;&#36895;&#26816;&#27979;&#29976;&#34071;&#30149;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#27719;&#24635;&#20102;&#19971;&#20010;&#23450;&#21046;&#30340;&#12289;&#32463;&#36807;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;&#65292;&#29305;&#21035;&#26159;InceptionV3&#12289;InceptionResNetV2&#12289;DenseNet201&#12289;DenseNet169&#12289;Xception&#21644;ResNet152V2&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#22312;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#24213;&#37096;&#28155;&#21152;&#20102;&#19977;&#23618;&#26356;&#23494;&#38598;&#23618;&#65292;&#20855;&#26377;0.0001&#30340;LASSO&#27491;&#21017;&#21270;&#65292;&#19977;&#20010;30%&#30340;dropout&#23618;&#21644;&#19977;&#20010;&#21551;&#29992;renorm&#30340;&#25209;&#37327;&#24402;&#19968;&#21270;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18870v1 Announce Type: cross  Abstract: Sugarcane, a key crop for the world's sugar industry, is prone to several diseases that have a substantial negative influence on both its yield and quality. To effectively manage and implement preventative initiatives, diseases must be detected promptly and accurately. In this study, we present a unique model called sugarcaneNet2024 that outperforms previous methods for automatically and quickly detecting sugarcane disease through leaf image processing. Our proposed model consolidates an optimized weighted average ensemble of seven customized and LASSO-regularized pre-trained models, particularly InceptionV3, InceptionResNetV2, DenseNet201, DenseNet169, Xception, and ResNet152V2. Initially, we added three more dense layers with 0.0001 LASSO regularization, three 30% dropout layers, and three batch normalizations with renorm enabled at the bottom of these pre-trained models to improve the performance. The accuracy of sugarcane leaf dise
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#33041;&#21330;&#20013;&#20998;&#21106;&#19978;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#26159;&#21542;&#38656;&#35201;&#39640;&#32423;&#21035;&#35774;&#35745;&#26469;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.17177</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#33041;&#21330;&#20013;&#20998;&#21106;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Brain Stroke Segmentation Using Deep Learning Models: A Comparative Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#33041;&#21330;&#20013;&#20998;&#21106;&#19978;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#26159;&#21542;&#38656;&#35201;&#39640;&#32423;&#21035;&#35774;&#35745;&#26469;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#21330;&#20013;&#20998;&#21106;&#22312;&#33041;&#21330;&#20013;&#24739;&#32773;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36890;&#36807;&#25552;&#20379;&#21463;&#24433;&#21709;&#33041;&#21306;&#22495;&#30340;&#31354;&#38388;&#20449;&#24687;&#21644;&#21463;&#25439;&#31243;&#24230;&#12290;&#20934;&#30830;&#20998;&#21106;&#33041;&#21330;&#20013;&#30149;&#21464;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#25163;&#24037;&#25216;&#26415;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#26368;&#36817;&#65292;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#22411;&#24050;&#34987;&#24341;&#20837;&#29992;&#20110;&#19968;&#33324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23637;&#31034;&#20986;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#26102;&#36229;&#36234;&#35768;&#22810;&#26368;&#20808;&#36827;&#32593;&#32476;&#30340;&#26377;&#21069;&#26223;&#32467;&#26524;&#12290;&#38543;&#30528;&#35270;&#35273;Transformer&#30340;&#20986;&#29616;&#65292;&#24050;&#32463;&#22522;&#20110;&#23427;&#20204;&#24341;&#20837;&#20102;&#20960;&#31181;&#27169;&#22411;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21017;&#26088;&#22312;&#35774;&#35745;&#22522;&#20110;&#20256;&#32479;&#21367;&#31215;&#23618;&#26469;&#25552;&#21462;&#20687;Transformer&#36825;&#26679;&#30340;&#38271;&#31243;&#20381;&#36182;&#30340;&#26356;&#22909;&#27169;&#22359;&#12290;&#26159;&#21542;&#23545;&#25152;&#26377;&#20998;&#21106;&#26696;&#20363;&#37117;&#38656;&#35201;&#36825;&#26679;&#39640;&#32423;&#21035;&#30340;&#35774;&#35745;&#26469;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#35299;&#31572;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#22235;&#31181;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17177v1 Announce Type: cross  Abstract: Stroke segmentation plays a crucial role in the diagnosis and treatment of stroke patients by providing spatial information about affected brain regions and the extent of damage. Segmenting stroke lesions accurately is a challenging task, given that conventional manual techniques are time consuming and prone to errors. Recently, advanced deep models have been introduced for general medical image segmentation, demonstrating promising results that surpass many state of the art networks when evaluated on specific datasets. With the advent of the vision Transformers, several models have been introduced based on them, while others have aimed to design better modules based on traditional convolutional layers to extract long-range dependencies like Transformers. The question of whether such high-level designs are necessary for all segmentation cases to achieve the best results remains unanswered. In this study, we selected four types of deep 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;Intrinsic Vision-Language Hallucination&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65292;&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#20135;&#29983;&#30340;&#21407;&#22240;&#21644;&#21453;&#26144;&#12290;</title><link>https://arxiv.org/abs/2403.11116</link><description>&lt;p&gt;
&#21338;&#22763;&#35770;&#25991;&#65306;&#19968;&#20010;&#25552;&#31034;&#30340;&#35270;&#35273;&#24187;&#35273;&#35780;&#20272;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PhD: A Prompted Visual Hallucination Evaluation Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;Intrinsic Vision-Language Hallucination&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65292;&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#20135;&#29983;&#30340;&#21407;&#22240;&#21644;&#21453;&#26144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#22686;&#38271;&#25512;&#21160;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#21457;&#23637;&#12290;&#22312;LLMs&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#24187;&#35273;&#25361;&#25112;&#20063;&#20986;&#29616;&#22312;LVLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;LVLM&#20013;&#30340;&#23545;&#35937;&#24187;&#35273;&#19978;&#65292;&#24573;&#30053;&#20102;LVLM&#24187;&#35273;&#30340;&#22810;&#26679;&#21270;&#31867;&#22411;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22266;&#26377;&#35270;&#35273;&#35821;&#35328;&#24187;&#35273;&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#65292;&#23545;&#23548;&#33268;&#24187;&#35273;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;IVL-Hallu&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#23427;&#20204;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65306;&#65288;a&#65289;&#23545;&#35937;&#24187;&#35273;&#65292;&#30001;&#20110;&#23545;&#35937;&#30340;&#35823;&#35782;&#21035;&#32780;&#20135;&#29983;&#65292;&#65288;b&#65289;&#23646;&#24615;&#24187;&#35273;&#65292;&#30001;&#20110;&#23646;&#24615;&#30340;&#35823;&#35782;&#21035;&#32780;&#24341;&#36215;&#65292;&#65288;c&#65289;&#22810;&#27169;&#24577;&#20914;&#31361;&#24187;&#35273;&#65292;&#28304;&#33258;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#20043;&#38388;&#30340;&#30683;&#30462;&#65292;&#20197;&#21450;&#65288;d&#65289;&#21453;&#24120;&#35782;&#24187;&#35273;&#65292;&#30001;&#20110;&#23545;&#31435;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11116v1 Announce Type: cross  Abstract: The rapid growth of Large Language Models (LLMs) has driven the development of Large Vision-Language Models (LVLMs). The challenge of hallucination, prevalent in LLMs, also emerges in LVLMs. However, most existing efforts mainly focus on object hallucination in LVLM, ignoring diverse types of LVLM hallucinations. In this study, we delve into the Intrinsic Vision-Language Hallucination (IVL-Hallu) issue, thoroughly analyzing different types of IVL-Hallu on their causes and reflections. Specifically, we propose several novel IVL-Hallu tasks and categorize them into four types: (a) object hallucination, which arises from the misidentification of objects, (b) attribute hallucination, which is caused by the misidentification of attributes, (c) multi-modal conflicting hallucination, which derives from the contradictions between textual and visual information, and (d) counter-common-sense hallucination, which owes to the contradictions betwee
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#26032;&#25351;&#26631;&#23545;&#25239;&#36229;&#20307;&#31215;&#26469;&#20840;&#38754;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#31181;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#37319;&#29992;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05100</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#25239;&#30028;&#38480;&#65306;&#36890;&#36807;&#23545;&#25239;&#36229;&#20307;&#31215;&#37327;&#21270;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Adversarial Frontier: Quantifying Robustness via Adversarial Hypervolume
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05100
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#26032;&#25351;&#26631;&#23545;&#25239;&#36229;&#20307;&#31215;&#26469;&#20840;&#38754;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#31181;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#37319;&#29992;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38754;&#20020;&#26085;&#30410;&#20005;&#37325;&#30340;&#23545;&#25239;&#25915;&#20987;&#23041;&#32961;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#23545;&#40065;&#26834;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#20256;&#32479;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#20381;&#36182;&#20110;&#23545;&#25239;&#20934;&#30830;&#24615;&#65292;&#35813;&#25351;&#26631;&#34913;&#37327;&#27169;&#22411;&#22312;&#29305;&#23450;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#21333;&#19968;&#25351;&#26631;&#24182;&#19981;&#33021;&#23436;&#20840;&#27010;&#25324;&#27169;&#22411;&#23545;&#19981;&#21516;&#31243;&#24230;&#25200;&#21160;&#30340;&#25972;&#20307;&#38887;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#23545;&#25239;&#36229;&#20307;&#31215;&#65292;&#20174;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#35282;&#24230;&#32508;&#21512;&#35780;&#20272;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#25351;&#26631;&#20801;&#35768;&#28145;&#20837;&#27604;&#36739;&#38450;&#24481;&#26426;&#21046;&#65292;&#24182;&#25215;&#35748;&#20102;&#36739;&#24369;&#30340;&#38450;&#24481;&#31574;&#30053;&#25152;&#24102;&#26469;&#30340;&#40065;&#26834;&#24615;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#22343;&#21248;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05100v1 Announce Type: cross  Abstract: The escalating threat of adversarial attacks on deep learning models, particularly in security-critical fields, has underscored the need for robust deep learning systems. Conventional robustness evaluations have relied on adversarial accuracy, which measures a model's performance under a specific perturbation intensity. However, this singular metric does not fully encapsulate the overall resilience of a model against varying degrees of perturbation. To address this gap, we propose a new metric termed adversarial hypervolume, assessing the robustness of deep learning models comprehensively over a range of perturbation intensities from a multi-objective optimization standpoint. This metric allows for an in-depth comparison of defense mechanisms and recognizes the trivial improvements in robustness afforded by less potent defensive strategies. Additionally, we adopt a novel training algorithm that enhances adversarial robustness uniformly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#33539;&#30068;&#35770;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23436;&#20840;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#22810;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.02598</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#20010;&#21327;&#21464;&#37327;&#36716;&#31227;&#21644;&#19981;&#24179;&#34913;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Pooling Image Datasets With Multiple Covariate Shift and Imbalance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#33539;&#30068;&#35770;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23436;&#20840;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#22810;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23398;&#31185;&#20013;&#24120;&#35265;&#23567;&#26679;&#26412;&#22823;&#23567;&#65292;&#36825;&#38656;&#35201;&#36328;&#22810;&#20010;&#26426;&#26500;&#27719;&#24635;&#22823;&#33268;&#30456;&#20284;&#30340;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#22270;&#20687;&#19982;&#30142;&#30149;&#32467;&#26524;&#20043;&#38388;&#30340;&#24369;&#20294;&#30456;&#20851;&#20851;&#32852;&#12290;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#20307;&#29616;&#20986;&#21327;&#21464;&#37327;&#65288;&#21363;&#27425;&#35201;&#30340;&#38750;&#25104;&#20687;&#25968;&#25454;&#65289;&#30340;&#36716;&#31227;/&#19981;&#24179;&#34913;&#12290;&#22312;&#26631;&#20934;&#32479;&#35745;&#20998;&#26512;&#20013;&#25511;&#21046;&#36825;&#20123;&#26080;&#29992;&#21464;&#37327;&#26159;&#24120;&#35265;&#30340;&#65292;&#20294;&#36825;&#20123;&#24605;&#24819;&#24182;&#19981;&#30452;&#25509;&#36866;&#29992;&#20110;&#21442;&#25968;&#36807;&#22810;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#20174;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24847;&#20041;&#30340;&#36215;&#28857;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#24211;&#20165;&#38480;&#20110;&#19968;&#27425;&#32771;&#34385;&#20960;&#20010;&#21327;&#21464;&#37327;&#30340;&#36716;&#31227;/&#19981;&#24179;&#34913;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#33539;&#30068;&#35770;&#30340;&#35282;&#24230;&#30475;&#24453;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23436;&#20840;&#36991;&#20813;&#20102;&#21407;&#26412;&#38656;&#35201;&#22797;&#26434;&#30340;&#22810;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02598v1 Announce Type: new  Abstract: Small sample sizes are common in many disciplines, which necessitates pooling roughly similar datasets across multiple institutions to study weak but relevant associations between images and disease outcomes. Such data often manifest shift/imbalance in covariates (i.e., secondary non-imaging data). Controlling for such nuisance variables is common within standard statistical analysis, but the ideas do not directly apply to overparameterized models. Consequently, recent work has shown how strategies from invariant representation learning provides a meaningful starting point, but the current repertoire of methods is limited to accounting for shifts/imbalances in just a couple of covariates at a time. In this paper, we show how viewing this problem from the perspective of Category theory provides a simple and effective solution that completely avoids elaborate multi-stage training pipelines that would otherwise be needed. We show the effect
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;&#19982;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#39044;&#27979;&#22810;&#27169;&#24577;&#33829;&#38144;&#27963;&#21160;&#25928;&#26524;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26089;&#26399;&#26816;&#27979;&#21487;&#33021;&#20855;&#26377;&#35828;&#26381;&#21147;&#30340;&#22810;&#27169;&#24577;&#27963;&#21160;&#24182;&#35780;&#20272;&#21644;&#22686;&#24378;&#33829;&#38144;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03607</link><description>&lt;p&gt;
&#25552;&#39640;&#22810;&#27169;&#24577;&#33829;&#38144;&#30340;&#19978;&#19979;&#25991;&#19968;&#33268;&#24615;&#65306;&#30693;&#35782;&#22522;&#30784;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Contextual Congruence Across Modalities for Effective Multimodal Marketing using Knowledge-infused Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;&#19982;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#39044;&#27979;&#22810;&#27169;&#24577;&#33829;&#38144;&#27963;&#21160;&#25928;&#26524;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26089;&#26399;&#26816;&#27979;&#21487;&#33021;&#20855;&#26377;&#35828;&#26381;&#21147;&#30340;&#22810;&#27169;&#24577;&#27963;&#21160;&#24182;&#35780;&#20272;&#21644;&#22686;&#24378;&#33829;&#38144;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#35774;&#22791;&#30340;&#26222;&#21450;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#32447;&#20307;&#39564;&#22810;&#27169;&#24577;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#35270;&#35273;&#27169;&#22411;&#65288;LVM&#65289;&#20173;&#28982;&#21463;&#21040;&#25429;&#25417;&#36328;&#27169;&#24577;&#35821;&#20041;&#20851;&#31995;&#30340;&#25972;&#20307;&#24847;&#20041;&#30340;&#38480;&#21046;&#12290;&#32570;&#20047;&#26126;&#30830;&#30340;&#24120;&#35782;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#20316;&#20026;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#65289;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20165;&#36890;&#36807;&#25429;&#25417;&#24222;&#22823;&#30340;&#35821;&#26009;&#24211;&#20013;&#30340;&#39640;&#32423;&#27169;&#24335;&#26469;&#23398;&#20064;&#38544;&#24335;&#34920;&#31034;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#37325;&#35201;&#30340;&#19978;&#19979;&#25991;&#36328;&#27169;&#24577;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#26174;&#24335;&#30340;&#24120;&#35782;&#30693;&#35782;&#20197;&#30693;&#35782;&#22270;&#35889;&#30340;&#24418;&#24335;&#19982;&#22823;&#22411;&#30340;VLM&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21363;&#39044;&#27979;&#22810;&#27169;&#24577;&#33829;&#38144;&#27963;&#21160;&#30340;&#26377;&#25928;&#24615;&#12290;&#34429;&#28982;&#33829;&#38144;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#35828;&#26381;&#21147;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#26089;&#26399;&#21457;&#29616;&#21487;&#33021;&#20855;&#26377;&#35828;&#26381;&#21147;&#30340;&#22810;&#27169;&#24577;&#27963;&#21160;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#35780;&#20272;&#21644;&#22686;&#24378;&#33829;&#38144;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of smart devices with the ability to capture moments in multiple modalities has enabled users to experience multimodal information online. However, large Language (LLMs) and Vision models (LVMs) are still limited in capturing holistic meaning with cross-modal semantic relationships. Without explicit, common sense knowledge (e.g., as a knowledge graph), Visual Language Models (VLMs) only learn implicit representations by capturing high-level patterns in vast corpora, missing essential contextual cross-modal cues. In this work, we design a framework to couple explicit commonsense knowledge in the form of knowledge graphs with large VLMs to improve the performance of a downstream task, predicting the effectiveness of multi-modal marketing campaigns. While the marketing application provides a compelling metric for assessing our methods, our approach enables the early detection of likely persuasive multi-modal campaigns and the assessment and augmentation of marketing theory.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LadaGAN&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#65292;&#36890;&#36807;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09596</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#21147;Transformer&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Efficient generative adversarial networks using linear additive-attention Transformers. (arXiv:2401.09596v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LadaGAN&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#65292;&#36890;&#36807;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#31561;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#35745;&#31639;&#22797;&#26434;&#30340;&#26550;&#26500;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30740;&#31350;&#23454;&#39564;&#23460;&#21644;&#36164;&#28304;&#20805;&#36275;&#30340;&#20844;&#21496;&#20013;&#30340;&#37319;&#29992;&#21644;&#20351;&#29992;&#65292;&#21516;&#26102;&#20063;&#26497;&#22823;&#22320;&#22686;&#21152;&#20102;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#30340;&#30899;&#36275;&#36857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LadaGAN&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#24314;&#31435;&#22312;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#19978;&#12290;&#35813;&#22359;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#19968;&#20010;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#65292;&#23427;&#27599;&#20010;&#22836;&#37096;&#35745;&#31639;&#19968;&#20010;&#27880;&#24847;&#21521;&#37327;&#65292;&#32780;&#19981;&#26159;&#20108;&#27425;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#22312;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#20013;&#37117;&#37319;&#29992;&#20102;Ladaformer&#65292;&#36825;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#20811;&#26381;&#20102;Transformer GAN&#32463;&#24120;&#20986;&#29616;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#12290;LadaGAN&#19968;&#30452;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;GANs&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the capacity of deep generative models for image generation, such as Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has dramatically improved in recent years, much of their success can be attributed to computationally expensive architectures. This has limited their adoption and use to research laboratories and companies with large resources, while significantly raising the carbon footprint for training, fine-tuning, and inference. In this work, we present LadaGAN, an efficient generative adversarial network that is built upon a novel Transformer block named Ladaformer. The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention. We employ Ladaformer in both the generator and discriminator, which reduces the computational complexity and overcomes the training instabilities often associated with Transformer GANs. LadaGAN consistently outperforms exist
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01286</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#19982;&#20154;&#31867;&#20132;&#27969;&#32039;&#23494;&#30456;&#20284;&#30340;&#25991;&#26412;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26174;&#33879;&#35745;&#31639;&#38656;&#27714;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#21442;&#25968;&#21270;&#36896;&#25104;&#30340;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#20110;&#19990;&#30028;&#30340;&#21160;&#24577;&#24615;&#65292;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;LLM&#20197;&#20462;&#27491;&#36807;&#26102;&#30340;&#20449;&#24687;&#25110;&#38598;&#25104;&#26032;&#30693;&#35782;&#65292;&#20174;&#32780;&#30830;&#20445;&#20854;&#25345;&#32493;&#30340;&#30456;&#20851;&#24615;&#12290;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#22312;&#35757;&#32451;&#21518;&#36827;&#34892;&#25345;&#32493;&#30340;&#27169;&#22411;&#35843;&#25972;&#65292;&#20197;&#35299;&#20915;&#32570;&#38519;&#25110;&#19981;&#33391;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;LLM&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#39640;&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#26377;&#25928;&#22320;&#20462;&#25913;LLM&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#22312;&#21508;&#31181;&#36755;&#20837;&#20013;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#39318;&#20808;&#23450;&#20041;&#20102;&#30693;&#35782;&#32534;&#36753;&#30340;&#30446;&#26631;&#21644;&#25361;&#25112;&#65292;&#28982;&#21518;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24212;&#29992;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#37319;&#26679;/&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;&#21435;&#22122;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36890;&#29992;&#21435;&#22122;&#32593;&#32476;&#22312;&#19981;&#21516;&#22122;&#22768;&#20998;&#24067;&#19979;&#34920;&#29616;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.20064</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#35757;&#32451;&#31574;&#30053;&#29992;&#20110;&#30450;&#30446;&#30340;&#22810;&#20998;&#24067;&#22122;&#22768;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
A Scalable Training Strategy for Blind Multi-Distribution Noise Removal. (arXiv:2310.20064v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20064
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#37319;&#26679;/&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;&#21435;&#22122;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36890;&#29992;&#21435;&#22122;&#32593;&#32476;&#22312;&#19981;&#21516;&#22122;&#22768;&#20998;&#24067;&#19979;&#34920;&#29616;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#26159;&#24320;&#21457;&#36890;&#29992;&#30340;&#21435;&#22122;&#21644;&#21435;&#20266;&#24433;&#32593;&#32476;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#22266;&#23450;&#30340;&#32593;&#32476;&#26435;&#37325;&#65292;&#19968;&#20010;&#20219;&#21153;&#65288;&#20363;&#22914;&#21435;&#38500;&#27850;&#26494;&#22122;&#22768;&#65289;&#30340;&#19987;&#38376;&#21270;&#19982;&#21478;&#19968;&#20010;&#20219;&#21153;&#65288;&#20363;&#22914;&#21435;&#38500;&#26001;&#28857;&#22122;&#22768;&#65289;&#30340;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#22825;&#28982;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#32500;&#24230;&#30340;&#35781;&#21650;&#65292;&#35757;&#32451;&#36825;&#26679;&#30340;&#32593;&#32476;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65306;&#38543;&#30528;&#35268;&#26684;&#31354;&#38388;&#30340;&#32500;&#24230;&#22686;&#21152;&#65288;&#21363;&#38656;&#35201;&#25551;&#36848;&#22122;&#22768;&#20998;&#24067;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#65289;&#65292;&#38656;&#35201;&#35757;&#32451;&#30340;&#21807;&#19968;&#35268;&#26684;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#22343;&#21248;&#37319;&#26679;&#36825;&#20010;&#31354;&#38388;&#20250;&#23548;&#33268;&#32593;&#32476;&#22312;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#35268;&#26684;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#38382;&#39064;&#35268;&#26684;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#21363;&#20351;&#22823;&#35823;&#24046;&#20063;&#23545;&#24635;&#20307;&#22343;&#26041;&#35823;&#24046;&#30340;&#24433;&#21709;&#24456;&#23567;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#37319;&#26679;/&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;&#21435;&#22122;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25913;&#36827;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances, developing general-purpose universal denoising and artifact-removal networks remains largely an open problem: Given fixed network weights, one inherently trades-off specialization at one task (e.g.,~removing Poisson noise) for performance at another (e.g.,~removing speckle noise). In addition, training such a network is challenging due to the curse of dimensionality: As one increases the dimensions of the specification-space (i.e.,~the number of parameters needed to describe the noise distribution) the number of unique specifications one needs to train for grows exponentially. Uniformly sampling this space will result in a network that does well at very challenging problem specifications but poorly at easy problem specifications, where even large errors will have a small effect on the overall mean squared error.  In this work we propose training denoising networks using an adaptive-sampling/active-learning strategy. Our work improves upon a recently proposed un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#32467;&#21512;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#23545;&#35805;&#20013;&#36880;&#27493;&#29702;&#35299;&#23545;&#35805;&#21382;&#21490;&#21644;&#21560;&#25910;&#35270;&#39057;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07259</link><description>&lt;p&gt;
&#25581;&#31034;&#38544;&#34255;&#30340;&#32852;&#31995;&#65306;&#29992;&#20110;&#35270;&#39057;&#23545;&#35805;&#30340;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Uncovering Hidden Connections: Iterative Tracking and Reasoning for Video-grounded Dialog. (arXiv:2310.07259v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#32467;&#21512;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#23545;&#35805;&#20013;&#36880;&#27493;&#29702;&#35299;&#23545;&#35805;&#21382;&#21490;&#21644;&#21560;&#25910;&#35270;&#39057;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#35270;&#35273;&#38382;&#31572;&#30456;&#27604;&#65292;&#35270;&#39057;&#23545;&#35805;&#38656;&#35201;&#23545;&#23545;&#35805;&#21382;&#21490;&#21644;&#35270;&#39057;&#20869;&#23481;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#65292;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#38754;&#20020;&#36880;&#27493;&#29702;&#35299;&#22797;&#26434;&#30340;&#23545;&#35805;&#21382;&#21490;&#21644;&#21560;&#25910;&#35270;&#39057;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#36319;&#36394;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#23558;&#25991;&#26412;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#22120;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#20197;&#36335;&#24452;&#36319;&#36394;&#21644;&#32858;&#21512;&#26426;&#21046;&#20026;&#26680;&#24515;&#65292;&#33021;&#22815;&#20174;&#23545;&#35805;&#21382;&#21490;&#20013;&#33719;&#21462;&#37325;&#35201;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#20197;&#35299;&#37322;&#25152;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21033;&#29992;&#36845;&#20195;&#25512;&#29702;&#32593;&#32476;&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#21644;&#24378;&#35843;&#20851;&#38190;&#35270;&#35273;&#26631;&#35760;&#65292;&#22686;&#24378;&#23545;&#35270;&#35273;&#29702;&#35299;&#30340;&#28145;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;GPT-&#27169;&#22411;&#23558;&#36825;&#20123;&#20016;&#23500;&#30340;&#20449;&#24687;&#32508;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrast to conventional visual question answering, video-grounded dialog necessitates a profound understanding of both dialog history and video content for accurate response generation. Despite commendable strides made by existing methodologies, they often grapple with the challenges of incrementally understanding intricate dialog histories and assimilating video information. In response to this gap, we present an iterative tracking and reasoning strategy that amalgamates a textual encoder, a visual encoder, and a generator. At its core, our textual encoder is fortified with a path tracking and aggregation mechanism, adept at gleaning nuances from dialog history that are pivotal to deciphering the posed questions. Concurrently, our visual encoder harnesses an iterative reasoning network, meticulously crafted to distill and emphasize critical visual markers from videos, enhancing the depth of visual comprehension. Culminating this enriched information, we employ the pre-trained GPT-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24494;&#34920;&#24773;&#35270;&#39057;&#25968;&#25454;&#38598;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24085;&#37329;&#26862;&#30149;&#31579;&#26597;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24494;&#31505;&#35270;&#39057;&#20013;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;89.7%&#30340;&#20934;&#30830;&#24615;&#21644;89.3%&#30340;AUROC&#20540;&#65292;&#21516;&#26102;&#22312;&#20154;&#32676;&#23376;&#32452;&#19978;&#27809;&#26377;&#26816;&#27979;&#21040;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2308.02588</link><description>&lt;p&gt;
&#29992;&#24494;&#31505;&#25581;&#31034;&#24085;&#37329;&#26862;&#30149;&#65306;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31579;&#26597;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Unmasking Parkinson's Disease with Smile: An AI-enabled Screening Framework. (arXiv:2308.02588v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24494;&#34920;&#24773;&#35270;&#39057;&#25968;&#25454;&#38598;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24085;&#37329;&#26862;&#30149;&#31579;&#26597;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24494;&#31505;&#35270;&#39057;&#20013;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;89.7%&#30340;&#20934;&#30830;&#24615;&#21644;89.3%&#30340;AUROC&#20540;&#65292;&#21516;&#26102;&#22312;&#20154;&#32676;&#23376;&#32452;&#19978;&#27809;&#26377;&#26816;&#27979;&#21040;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#30446;&#21069;&#32570;&#20047;&#21487;&#38752;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#26377;&#38480;&#30340;&#20020;&#24202;&#25252;&#29702;&#36164;&#28304;&#65292;&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#30340;&#35786;&#26029;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;&#24494;&#34920;&#24773;&#30340;&#26368;&#22823;&#35270;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;PD&#31579;&#26597;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;1,059&#21517;&#29420;&#31435;&#21442;&#19982;&#32773;&#30340;3,871&#20010;&#35270;&#39057;&#65292;&#20854;&#20013;&#21253;&#25324;256&#21517;&#33258;&#25253;PD&#24739;&#32773;&#12290;&#36825;&#20123;&#24405;&#20687;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#65292;&#21253;&#25324;&#22810;&#20010;&#22269;&#23478;&#30340;&#21442;&#19982;&#32773;&#23478;&#20013;&#12289;&#19968;&#23478;&#35786;&#25152;&#21644;&#19968;&#20010;&#32654;&#22269;&#30340;PD&#25252;&#29702;&#26426;&#26500;&#12290;&#36890;&#36807;&#21033;&#29992;&#38754;&#37096;&#26631;&#24535;&#21644;&#34892;&#21160;&#21333;&#20301;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#19982;PD&#30340;&#19968;&#20010;&#20027;&#35201;&#30151;&#29366;Hypomimia&#65288;&#38754;&#37096;&#34920;&#24773;&#20943;&#23569;&#65289;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#22312;&#36825;&#20123;&#29305;&#24449;&#19978;&#35757;&#32451;&#30340;&#19968;&#32452;AI&#27169;&#22411;&#22312;&#20445;&#30041;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;89.7%&#30340;&#20934;&#30830;&#24615;&#21644;89.3%&#30340;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24615;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUROC&#65289;&#65292;&#24182;&#19988;&#22312;&#24615;&#21035;&#21644;&#31181;&#26063;&#31561;&#20154;&#32676;&#23376;&#32452;&#19978;&#26080;&#21487;&#26816;&#27979;&#30340;&#20559;&#35265;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#65292;&#20165;&#36890;&#36807;&#24494;&#31505;&#35270;&#39057;&#20013;&#30340;&#29305;&#24449;&#23601;&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#21644;AUROC&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkinson's disease (PD) diagnosis remains challenging due to lacking a reliable biomarker and limited access to clinical care. In this study, we present an analysis of the largest video dataset containing micro-expressions to screen for PD. We collected 3,871 videos from 1,059 unique participants, including 256 self-reported PD patients. The recordings are from diverse sources encompassing participants' homes across multiple countries, a clinic, and a PD care facility in the US. Leveraging facial landmarks and action units, we extracted features relevant to Hypomimia, a prominent symptom of PD characterized by reduced facial expressions. An ensemble of AI models trained on these features achieved an accuracy of 89.7% and an Area Under the Receiver Operating Characteristic (AUROC) of 89.3% while being free from detectable bias across population subgroups based on sex and ethnicity on held-out data. Further analysis reveals that features from the smiling videos alone lead to comparable 
&lt;/p&gt;</description></item><item><title>&#36951;&#24536;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#19981;&#20165;&#38480;&#20110;&#36830;&#32493;&#23398;&#20064;&#39046;&#22495;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#19982;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#36951;&#24536;&#19981;&#24635;&#26159;&#26377;&#23475;&#30340;&#65292;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#20013;&#12290;</title><link>http://arxiv.org/abs/2307.09218</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#36951;&#24536;&#29616;&#35937;&#30340;&#20840;&#38754;&#35843;&#26597;&#65306;&#36229;&#36234;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning. (arXiv:2307.09218v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09218
&lt;/p&gt;
&lt;p&gt;
&#36951;&#24536;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#19981;&#20165;&#38480;&#20110;&#36830;&#32493;&#23398;&#20064;&#39046;&#22495;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#19982;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#36951;&#24536;&#19981;&#24635;&#26159;&#26377;&#23475;&#30340;&#65292;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36951;&#24536;&#25351;&#30340;&#26159;&#20808;&#21069;&#33719;&#21462;&#30340;&#20449;&#24687;&#25110;&#30693;&#35782;&#30340;&#20007;&#22833;&#25110;&#24694;&#21270;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20851;&#20110;&#36951;&#24536;&#30340;&#35843;&#26597;&#20027;&#35201;&#38598;&#20013;&#22312;&#36830;&#32493;&#23398;&#20064;&#26041;&#38754;&#65292;&#20294;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#36951;&#24536;&#26159;&#19968;&#31181;&#26222;&#36941;&#29616;&#35937;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#20013;&#35266;&#23519;&#21040;&#12290;&#36951;&#24536;&#22312;&#30740;&#31350;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26469;&#65292;&#20363;&#22914;&#30001;&#20110;&#29983;&#25104;&#22120;&#28418;&#31227;&#32780;&#22312;&#29983;&#25104;&#27169;&#22411;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26469;&#65292;&#20197;&#21450;&#30001;&#20110;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#32780;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26469;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#28041;&#21450;&#21040;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#22312;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#21516;&#26102;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36830;&#32493;&#23398;&#20064;&#35843;&#26597;&#37117;&#40664;&#35748;&#35748;&#20026;&#36951;&#24536;&#24635;&#26159;&#26377;&#23475;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#35748;&#20026;&#36951;&#24536;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#20363;&#22914;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#12290;&#36890;&#36807;&#22312;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#25506;&#35752;&#36951;&#24536;&#29616;&#35937;&#65292;
&lt;/p&gt;
&lt;p&gt;
Forgetting refers to the loss or deterioration of previously acquired information or knowledge. While the existing surveys on forgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research domains within deep learning. Forgetting manifests in research fields such as generative models due to generator shifts, and federated learning due to heterogeneous data distributions across clients. Addressing forgetting encompasses several challenges, including balancing the retention of old task knowledge with fast learning of new tasks, managing task interference with conflicting goals, and preventing privacy leakage, etc. Moreover, most existing surveys on continual learning implicitly assume that forgetting is always harmful. In contrast, our survey argues that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios. By exploring forgetting in a broader context
&lt;/p&gt;</description></item></channel></rss>