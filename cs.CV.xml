<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#20351;&#29992;&#21387;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#20195;&#26367;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#20687;&#24674;&#22797;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.17744</link><description>&lt;p&gt;
&#20351;&#29992;&#21387;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Variational Bayes image restoration with compressive autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17744
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21387;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#20195;&#26367;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#20687;&#24674;&#22797;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#38382;&#39064;&#30340;&#27491;&#21017;&#21270;&#22312;&#35745;&#31639;&#25104;&#20687;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26377;&#25928;&#22270;&#20687;&#34920;&#31034;&#30340;&#33021;&#21147;&#24050;&#34987;&#21033;&#29992;&#26469;&#35774;&#35745;&#24378;&#22823;&#30340;&#25968;&#25454;&#39537;&#21160;&#27491;&#21017;&#21270;&#22120;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20351;&#29992;&#21387;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#12290;&#36825;&#20123;&#32593;&#32476;&#21487;&#20197;&#34987;&#30475;&#20316;&#20855;&#26377;&#28789;&#27963;&#28508;&#22312;&#20808;&#39564;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#27604;&#36215;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#26356;&#23567;&#26356;&#23481;&#26131;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17744v2 Announce Type: replace-cross  Abstract: Regularization of inverse problems is of paramount importance in computational imaging. The ability of neural networks to learn efficient image representations has been recently exploited to design powerful data-driven regularizers. While state-of-the-art plug-and-play methods rely on an implicit regularization provided by neural denoisers, alternative Bayesian approaches consider Maximum A Posteriori (MAP) estimation in the latent space of a generative model, thus with an explicit regularization. However, state-of-the-art deep generative models require a huge amount of training data compared to denoisers. Besides, their complexity hampers the optimization involved in latent MAP derivation. In this work, we first propose to use compressive autoencoders instead. These networks, which can be seen as variational autoencoders with a flexible latent prior, are smaller and easier to train than state-of-the-art generative models. As a
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#20102;&#23545;&#20110;&#35757;&#32451;&#33391;&#22909;&#30340;AI&#27169;&#22411;&#65292;&#22914;&#26524;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#65292;&#23558;&#20986;&#29616;&#31232;&#30095;&#20132;&#20114;&#27010;&#24565;&#65292;&#36825;&#20123;&#27010;&#24565;&#33021;&#22815;&#25551;&#36848;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#25512;&#29702;&#20998;&#25968;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.01939</link><description>&lt;p&gt;
&#35777;&#26126;AI&#27169;&#22411;&#20013;&#31232;&#30095;&#31526;&#21495;&#27010;&#24565;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Where We Have Arrived in Proving the Emergence of Sparse Symbolic Concepts in AI Models. (arXiv:2305.01939v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01939
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#20102;&#23545;&#20110;&#35757;&#32451;&#33391;&#22909;&#30340;AI&#27169;&#22411;&#65292;&#22914;&#26524;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#65292;&#23558;&#20986;&#29616;&#31232;&#30095;&#20132;&#20114;&#27010;&#24565;&#65292;&#36825;&#20123;&#27010;&#24565;&#33021;&#22815;&#25551;&#36848;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#25512;&#29702;&#20998;&#25968;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35777;&#26126;&#35757;&#32451;&#33391;&#22909;&#30340;AI&#27169;&#22411;&#20013;&#20986;&#29616;&#31526;&#21495;&#27010;&#24565;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#65288;1&#65289;&#27169;&#22411;&#36755;&#20986;&#30456;&#23545;&#20110;&#36755;&#20837;&#21464;&#37327;&#30340;&#39640;&#38454;&#23548;&#25968;&#22343;&#20026;&#38646;&#65292;&#65288;2&#65289;AI&#27169;&#22411;&#21487;&#29992;&#20110;&#36974;&#25377;&#26679;&#26412;&#19988;&#36755;&#20837;&#26679;&#26412;&#36739;&#23569;&#36974;&#25377;&#26102;&#20250;&#20135;&#29983;&#26356;&#39640;&#30340;&#32622;&#20449;&#24230;&#65292;&#65288;3&#65289;AI&#27169;&#22411;&#22312;&#36974;&#25377;&#26679;&#26412;&#19978;&#30340;&#32622;&#20449;&#24230;&#24182;&#19981;&#20250;&#26174;&#33879;&#38477;&#20302;&#65292;&#21017;AI&#27169;&#22411;&#23558;&#32534;&#30721;&#31232;&#30095;&#20132;&#20114;&#27010;&#24565;&#12290;&#27599;&#20010;&#20132;&#20114;&#27010;&#24565;&#34920;&#31034;&#29305;&#23450;&#19968;&#32452;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#25512;&#29702;&#20998;&#25968;&#20135;&#29983;&#19968;&#23450;&#30340;&#25968;&#20540;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#20998;&#25968;&#24635;&#26159;&#21487;&#20197;&#34920;&#31034;&#20026;&#25152;&#26377;&#20132;&#20114;&#27010;&#24565;&#30340;&#20132;&#20114;&#25928;&#24212;&#20043;&#21644;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#24076;&#26395;&#35777;&#26126;&#20986;&#29616;&#31526;&#21495;&#27010;&#24565;&#30340;&#26465;&#20214;&#38750;&#24120;&#26222;&#36941;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#20110;&#22823;&#22810;&#25968;AI&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#24120;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#30340;&#20132;&#20114;&#27010;&#24565;&#26469;&#27169;&#25311;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to prove the emergence of symbolic concepts in well-trained AI models. We prove that if (1) the high-order derivatives of the model output w.r.t. the input variables are all zero, (2) the AI model can be used on occluded samples and will yield higher confidence when the input sample is less occluded, and (3) the confidence of the AI model does not significantly degrade on occluded samples, then the AI model will encode sparse interactive concepts. Each interactive concept represents an interaction between a specific set of input variables, and has a certain numerical effect on the inference score of the model. Specifically, it is proved that the inference score of the model can always be represented as the sum of the interaction effects of all interactive concepts. In fact, we hope to prove that conditions for the emergence of symbolic concepts are quite common. It means that for most AI models, we can usually use a small number of interactive concepts to mimic the mode
&lt;/p&gt;</description></item></channel></rss>