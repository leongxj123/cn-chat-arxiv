<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>InVA&#26159;&#19968;&#31181;&#32508;&#21512;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#20013;&#19981;&#21516;&#26469;&#28304;&#30340;&#22810;&#20010;&#22270;&#20687;&#26469;&#36827;&#34892;&#39044;&#27979;&#25512;&#29702;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;VAE&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02734</link><description>&lt;p&gt;
InVA: &#32508;&#21512;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#22810;&#27169;&#24577;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#30340;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02734
&lt;/p&gt;
&lt;p&gt;
InVA&#26159;&#19968;&#31181;&#32508;&#21512;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#20013;&#19981;&#21516;&#26469;&#28304;&#30340;&#22810;&#20010;&#22270;&#20687;&#26469;&#36827;&#34892;&#39044;&#27979;&#25512;&#29702;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;VAE&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25506;&#32034;&#22810;&#20010;&#26469;&#33258;&#19981;&#21516;&#25104;&#20687;&#27169;&#24335;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#32852;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#23613;&#31649;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#25991;&#29486;&#30740;&#31350;&#22522;&#20110;&#22810;&#20010;&#22270;&#20687;&#26469;&#25512;&#26029;&#22270;&#20687;&#30340;&#39044;&#27979;&#25512;&#29702;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#25928;&#20511;&#29992;&#22810;&#20010;&#25104;&#20687;&#27169;&#24335;&#20043;&#38388;&#30340;&#20449;&#24687;&#26469;&#39044;&#27979;&#22270;&#20687;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#26412;&#25991;&#24314;&#31435;&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#30340;&#25991;&#29486;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#32508;&#21512;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;InVA&#65289;&#26041;&#27861;&#65292;&#23427;&#20174;&#19981;&#21516;&#26469;&#28304;&#33719;&#24471;&#30340;&#22810;&#20010;&#22270;&#20687;&#20013;&#20511;&#29992;&#20449;&#24687;&#26469;&#32472;&#21046;&#22270;&#20687;&#30340;&#39044;&#27979;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25429;&#25417;&#20102;&#32467;&#26524;&#22270;&#20687;&#19982;&#36755;&#20837;&#22270;&#20687;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#32852;&#65292;&#24182;&#20801;&#35768;&#24555;&#36895;&#35745;&#31639;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;InVA&#30456;&#23545;&#20110;&#36890;&#24120;&#19981;&#20801;&#35768;&#20511;&#29992;&#36755;&#20837;&#22270;&#20687;&#20043;&#38388;&#20449;&#24687;&#30340;VAE&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a significant interest in exploring non-linear associations among multiple images derived from diverse imaging modalities. While there is a growing literature on image-on-image regression to delineate predictive inference of an image based on multiple images, existing approaches have limitations in efficiently borrowing information between multiple imaging modalities in the prediction of an image. Building on the literature of Variational Auto Encoders (VAEs), this article proposes a novel approach, referred to as Integrative Variational Autoencoder (\texttt{InVA}) method, which borrows information from multiple images obtained from different sources to draw predictive inference of an image. The proposed approach captures complex non-linear association between the outcome image and input images, while allowing rapid computation. Numerical results demonstrate substantial advantages of \texttt{InVA} over VAEs, which typically do not allow borrowing information between input imag
&lt;/p&gt;</description></item><item><title>TSGCNeXt&#26159;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#38271;&#26399;&#23398;&#20064;&#28508;&#21147;&#65292;&#23427;&#37319;&#29992;&#21160;&#38745;&#24577;&#22810;&#22270;&#21367;&#31215;&#26469;&#27719;&#38598;&#22810;&#20010;&#29420;&#31435;&#25299;&#25169;&#22270;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#19968;&#20010;&#22270;&#24418;&#21367;&#31215;&#35757;&#32451;&#21152;&#36895;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.11631</link><description>&lt;p&gt;
TSGCNeXt&#65306;&#20855;&#22791;&#38271;&#26399;&#23398;&#20064;&#28508;&#21147;&#30340;&#39640;&#25928;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#21160;&#38745;&#24577;&#22810;&#22270;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
TSGCNeXt: Dynamic-Static Multi-Graph Convolution for Efficient Skeleton-Based Action Recognition with Long-term Learning Potential. (arXiv:2304.11631v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11631
&lt;/p&gt;
&lt;p&gt;
TSGCNeXt&#26159;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#38271;&#26399;&#23398;&#20064;&#28508;&#21147;&#65292;&#23427;&#37319;&#29992;&#21160;&#38745;&#24577;&#22810;&#22270;&#21367;&#31215;&#26469;&#27719;&#38598;&#22810;&#20010;&#29420;&#31435;&#25299;&#25169;&#22270;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#26500;&#24314;&#20102;&#19968;&#20010;&#22270;&#24418;&#21367;&#31215;&#35757;&#32451;&#21152;&#36895;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#21457;&#23637;&#65292;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#22312;&#20154;&#31867;&#21160;&#20316;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#36235;&#21521;&#20110;&#26500;&#24314;&#20855;&#26377;&#20887;&#20313;&#35757;&#32451;&#30340;&#22797;&#26434;&#23398;&#20064;&#26426;&#21046;&#65292;&#24182;&#23384;&#22312;&#38271;&#26102;&#38388;&#24207;&#21015;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Temporal-Spatio Graph ConvNeXt&#65288;TSGCNeXt&#65289;&#26469;&#25506;&#32034;&#38271;&#26102;&#38388;&#39592;&#39612;&#24207;&#21015;&#30340;&#39640;&#25928;&#23398;&#20064;&#26426;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22270;&#24418;&#23398;&#20064;&#26426;&#21046;&#65292;&#21160;&#38745;&#20998;&#31163;&#22810;&#22270;&#21367;&#31215;&#65288;DS-SMG&#65289;&#65292;&#20197;&#27719;&#38598;&#22810;&#20010;&#29420;&#31435;&#25299;&#25169;&#22270;&#30340;&#29305;&#24449;&#24182;&#36991;&#20813;&#33410;&#28857;&#20449;&#24687;&#22312;&#21160;&#24577;&#21367;&#31215;&#26399;&#38388;&#34987;&#24573;&#30053;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22270;&#24418;&#21367;&#31215;&#35757;&#32451;&#21152;&#36895;&#26426;&#21046;&#65292;&#20197;55.08&#65285;&#30340;&#36895;&#24230;&#25552;&#39640;&#21160;&#24577;&#22270;&#24418;&#23398;&#20064;&#30340;&#21453;&#21521;&#20256;&#25773;&#35745;&#31639;&#36895;&#24230;&#12290;&#26368;&#21518;&#65292;TSGCNeXt&#36890;&#36807;&#19977;&#20010;&#26102;&#31354;&#23398;&#20064;&#27169;&#22359;&#37325;&#26032;&#26500;&#24314;&#20102;GCN&#30340;&#25972;&#20307;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#39640;&#25928;&#30340;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skeleton-based action recognition has achieved remarkable results in human action recognition with the development of graph convolutional networks (GCNs). However, the recent works tend to construct complex learning mechanisms with redundant training and exist a bottleneck for long time-series. To solve these problems, we propose the Temporal-Spatio Graph ConvNeXt (TSGCNeXt) to explore efficient learning mechanism of long temporal skeleton sequences. Firstly, a new graph learning mechanism with simple structure, Dynamic-Static Separate Multi-graph Convolution (DS-SMG) is proposed to aggregate features of multiple independent topological graphs and avoid the node information being ignored during dynamic convolution. Next, we construct a graph convolution training acceleration mechanism to optimize the back-propagation computing of dynamic graph learning with 55.08\% speed-up. Finally, the TSGCNeXt restructure the overall structure of GCN with three Spatio-temporal learning modules,effic
&lt;/p&gt;</description></item></channel></rss>