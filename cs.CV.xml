<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36890;&#36807;&#20351;&#29992;Masked AutoEncoders&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;NeRF-MAE&#29992;&#20110;&#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#65292;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;&#23494;&#38598;&#36755;&#20837;&#65292;&#20197;&#20135;&#29983;&#26377;&#25928;&#30340;&#19977;&#32500;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2404.01300</link><description>&lt;p&gt;
NeRF-MAE: &#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;Masked AutoEncoders
&lt;/p&gt;
&lt;p&gt;
NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;Masked AutoEncoders&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;NeRF-MAE&#29992;&#20110;&#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#65292;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;&#23494;&#38598;&#36755;&#20837;&#65292;&#20197;&#20135;&#29983;&#26377;&#25928;&#30340;&#19977;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31070;&#32463;&#22330;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#33021;&#22815;&#29702;&#35299;&#19977;&#32500;&#35270;&#35273;&#19990;&#30028;&#65292;&#22914;&#25512;&#26029;&#35821;&#20041;&#12289;&#20960;&#20309;&#21644;&#21160;&#24577;&#31561;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#22330;&#22312;&#20174;&#20108;&#32500;&#22270;&#20687;&#20013;&#23494;&#38598;&#34920;&#31034;&#19977;&#32500;&#22330;&#26223;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20855;&#20307;&#20351;&#29992;Masked AutoEncoders&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#23558;transformers&#25193;&#23637;&#21040;&#26032;&#25968;&#25454;&#27169;&#24577;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#25104;&#21151;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#26469;&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#12290;&#25105;&#20204;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;transformer&#30340;&#23494;&#38598;&#36755;&#20837;&#65292;&#19982;&#20854;&#20182;&#19977;&#32500;&#34920;&#31034;&#65288;&#22914;&#28857;&#20113;&#65289;&#36827;&#34892;&#23545;&#27604;&#65292;&#20854;&#20449;&#24687;&#23494;&#24230;&#21487;&#33021;&#19981;&#22343;&#21248;&#65292;&#32780;&#34920;&#31034;&#26159;&#19981;&#35268;&#21017;&#30340;&#12290;&#30001;&#20110;&#23558;masked autoencoders&#24212;&#29992;&#20110;&#31867;&#20284;NeRF&#36825;&#26679;&#30340;&#38544;&#24335;&#34920;&#31034;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#36873;&#25321;&#25552;&#21462;&#19968;&#20010;&#26174;&#24335;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01300v1 Announce Type: cross  Abstract: Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRF's volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit repres
&lt;/p&gt;</description></item><item><title>NeuroPictor&#36890;&#36807;&#30452;&#25509;&#35843;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;fMRI&#21040;&#22270;&#20687;&#30340;&#37325;&#24314;&#65292;&#22312;&#22810;&#20010;&#20010;&#20307;&#39044;&#35757;&#32451;&#21644;&#22810;&#23618;&#27425;&#30340;&#24341;&#23548;&#26465;&#20214;&#19979;&#65292;&#23454;&#29616;&#20102;&#26356;&#35814;&#32454;&#30340;&#22270;&#20687;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.18211</link><description>&lt;p&gt;
NeuroPictor: &#36890;&#36807;&#22810;&#20010;&#20010;&#20307;&#30340;&#39044;&#35757;&#32451;&#21644;&#22810;&#23618;&#35843;&#21046;&#20248;&#21270;fMRI&#21040;&#22270;&#20687;&#30340;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual Pretraining and Multi-level Modulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18211
&lt;/p&gt;
&lt;p&gt;
NeuroPictor&#36890;&#36807;&#30452;&#25509;&#35843;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;fMRI&#21040;&#22270;&#20687;&#30340;&#37325;&#24314;&#65292;&#22312;&#22810;&#20010;&#20010;&#20307;&#39044;&#35757;&#32451;&#21644;&#22810;&#23618;&#27425;&#30340;&#24341;&#23548;&#26465;&#20214;&#19979;&#65292;&#23454;&#29616;&#20102;&#26356;&#35814;&#32454;&#30340;&#22270;&#20687;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;fMRI&#21040;&#22270;&#20687;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;fMRI&#20449;&#21495;&#19982;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#29305;&#23450;&#26465;&#20214;&#20851;&#32852;&#36215;&#26469;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#30452;&#25509;&#35843;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23558;fMRI&#21040;&#22270;&#20687;&#36807;&#31243;&#20998;&#20026;&#19977;&#20010;&#27493;&#39588;&#65306;i) fMRI&#26657;&#20934;&#32534;&#30721;&#65292;&#29992;&#20110;&#22788;&#29702;&#20849;&#20139;&#28508;&#22312;&#31354;&#38388;&#30340;&#22810;&#20010;&#20307;&#39044;&#35757;&#32451;&#65292;&#20197;&#26368;&#23567;&#21270;&#20010;&#20307;&#24046;&#24322;&#24182;&#23454;&#29616;&#21518;&#32493;&#30340;&#36328;&#20027;&#20307;&#35757;&#32451;&#65307;ii) fMRI&#21040;&#22270;&#20687;&#36328;&#20010;&#20307;&#39044;&#35757;&#32451;&#65292;&#24863;&#30693;&#22320;&#23398;&#20064;&#22914;&#20309;&#24341;&#23548;&#19981;&#21516;&#20010;&#20307;&#20043;&#38388;&#39640;&#20302;&#23618;&#27425;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#65307;iii) fMRI&#21040;&#22270;&#20687;&#21333;&#20010;&#20307;&#32454;&#21270;&#65292;&#31867;&#20284;&#20110;&#27493;&#39588;ii&#65292;&#20294;&#20391;&#37325;&#20110;&#36866;&#24212;&#29305;&#23450;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18211v1 Announce Type: cross  Abstract: Recent fMRI-to-image approaches mainly focused on associating fMRI signals with specific conditions of pre-trained diffusion models. These approaches, while producing high-quality images, capture only a limited aspect of the complex information in fMRI signals and offer little detailed control over image creation. In contrast, this paper proposes to directly modulate the generation process of diffusion models using fMRI signals. Our approach, NeuroPictor, divides the fMRI-to-image process into three steps: i) fMRI calibrated-encoding, to tackle multi-individual pre-training for a shared latent space to minimize individual difference and enable the subsequent cross-subject training; ii) fMRI-to-image cross-subject pre-training, perceptually learning to guide diffusion model with high- and low-level conditions across different individuals; iii) fMRI-to-image single-subject refining, similar with step ii but focus on adapting to particula
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#37324;&#31243;&#30865;&#25968;&#25454;&#38598;BIMCV-R&#65292;&#21253;&#21547;8,069&#20010;3D CT&#20307;&#31215;&#21644;&#20854;&#25918;&#23556;&#23398;&#25253;&#21578;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#26816;&#32034;&#31574;&#30053;MedFinder&#65292;&#20026;3D&#21307;&#23398;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#39046;&#22495;&#25552;&#20379;&#20102;&#37325;&#35201;&#36129;&#29486;</title><link>https://arxiv.org/abs/2403.15992</link><description>&lt;p&gt;
BIMCV-R&#65306;&#29992;&#20110;3D CT&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#30340;&#37324;&#31243;&#30865;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15992
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#37324;&#31243;&#30865;&#25968;&#25454;&#38598;BIMCV-R&#65292;&#21253;&#21547;8,069&#20010;3D CT&#20307;&#31215;&#21644;&#20854;&#25918;&#23556;&#23398;&#25253;&#21578;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#26816;&#32034;&#31574;&#30053;MedFinder&#65292;&#20026;3D&#21307;&#23398;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#39046;&#22495;&#25552;&#20379;&#20102;&#37325;&#35201;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15992v1 &#21457;&#24067;&#31867;&#22411;: &#36328;&#36234;  &#25688;&#35201;: &#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#19982;&#21307;&#30103;&#20445;&#20581;&#30340;&#34701;&#21512;&#19981;&#26029;&#22686;&#21152;&#20102;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#30340;&#24037;&#20316;&#37327;&#12290;&#20026;&#20102;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#22312;&#35786;&#26029;&#36807;&#31243;&#20013;&#65292;&#20943;&#36731;&#20854;&#24037;&#20316;&#37327;&#65292;&#24320;&#21457;&#19968;&#20010;&#21487;&#38752;&#30340;&#26816;&#32034;&#30456;&#20284;&#30149;&#20363;&#30740;&#31350;&#30340;&#31995;&#32479;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#36825;&#19968;&#27010;&#24565;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#30446;&#21069;3D&#21307;&#23398;&#25991;&#26412;&#22270;&#20687;&#26816;&#32034;&#39046;&#22495;&#21463;&#38480;&#20110;&#32570;&#20047;&#20581;&#20840;&#30340;&#35780;&#20272;&#22522;&#20934;&#21644;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;BIMCV-R&#65288;&#27492;&#25968;&#25454;&#38598;&#23558;&#22312;&#25509;&#21463;&#21518;&#21457;&#24067;&#12290;&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;8,069&#20010;3D CT&#20307;&#31215;&#30340;&#24191;&#27867;&#25910;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;200&#19975;&#24352;&#20999;&#29255;&#65292;&#20197;&#21450;&#23427;&#20204;&#21508;&#33258;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#22312;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25299;&#23637;&#20102;&#19968;&#31181;&#26816;&#32034;&#31574;&#30053;&#65292;MedFinder&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#21452;&#27969;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#22823;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15992v1 Announce Type: cross  Abstract: The burgeoning integration of 3D medical imaging into healthcare has led to a substantial increase in the workload of medical professionals. To assist clinicians in their diagnostic processes and alleviate their workload, the development of a robust system for retrieving similar case studies presents a viable solution. While the concept holds great promise, the field of 3D medical text-image retrieval is currently limited by the absence of robust evaluation benchmarks and curated datasets. To remedy this, our study presents a groundbreaking dataset, BIMCV-R (This dataset will be released upon acceptance.), which includes an extensive collection of 8,069 3D CT volumes, encompassing over 2 million slices, paired with their respective radiological reports. Expanding upon the foundational work of our dataset, we craft a retrieval strategy, MedFinder. This approach employs a dual-stream network architecture, harnessing the potential of larg
&lt;/p&gt;</description></item><item><title>&#22686;&#21152;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#20294;&#20165;&#22312;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#36317;&#31163;&#36739;&#23567;&#26102;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2403.13808</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Pretraining Data Diversity for Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13808
&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#20294;&#20165;&#22312;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#36317;&#31163;&#36739;&#23567;&#26102;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#26356;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#26159;&#21807;&#19968;&#26679;&#26412;&#25968;&#37327;&#65292;&#22312;&#22266;&#23450;&#30340;&#35745;&#31639;&#39044;&#31639;&#19979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#19968;&#33268;&#34920;&#26126;&#65292;&#22686;&#21152;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;SSL&#24615;&#33021;&#65292;&#23613;&#31649;&#21482;&#26377;&#24403;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#36317;&#31163;&#24456;&#23567;&#30340;&#26102;&#20505;&#25165;&#26159;&#22914;&#27492;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#36890;&#36807;&#32593;&#32476;&#29228;&#34411;&#25110;&#25193;&#25955;&#29983;&#25104;&#30340;&#25968;&#25454;&#31561;&#26041;&#24335;&#23454;&#29616;&#20102;&#24322;&#24120;&#22823;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#20998;&#24067;&#36716;&#31227;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28085;&#30422;&#20102;&#19971;&#31181;SSL&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#35832;&#22914;ImageNet&#21644;YFCC100M&#31561;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24635;&#35745;&#36229;&#36807;200&#20010;GPU&#22825;&#12290;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#23558;&#22312;https://github.com/hammoudhasan/DiversitySSL &#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13808v1 Announce Type: cross  Abstract: We explore the impact of training with more diverse datasets, characterized by the number of unique samples, on the performance of self-supervised learning (SSL) under a fixed computational budget. Our findings consistently demonstrate that increasing pretraining data diversity enhances SSL performance, albeit only when the distribution distance to the downstream data is minimal. Notably, even with an exceptionally large pretraining data diversity achieved through methods like web crawling or diffusion-generated data, among other ways, the distribution shift remains a challenge. Our experiments are comprehensive with seven SSL methods using large-scale datasets such as ImageNet and YFCC100M amounting to over 200 GPU days. Code and trained models will be available at https://github.com/hammoudhasan/DiversitySSL .
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22810;&#35270;&#35282;&#25968;&#25454;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#28155;&#21152;&#27700;&#21360;&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#24481;&#23545;&#28304;&#27169;&#22411;&#21151;&#33021;&#30340;&#31363;&#21462;&#25915;&#20987;</title><link>https://arxiv.org/abs/2403.10663</link><description>&lt;p&gt;
&#19981;&#20165;&#25913;&#21464;&#26631;&#31614;&#65292;&#23398;&#20064;&#29305;&#24449;&#65306;&#20351;&#29992;&#22810;&#35270;&#35282;&#25968;&#25454;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#28155;&#21152;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10663
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22810;&#35270;&#35282;&#25968;&#25454;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#28155;&#21152;&#27700;&#21360;&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#24481;&#23545;&#28304;&#27169;&#22411;&#21151;&#33021;&#30340;&#31363;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#65288;MLaaS&#65289;&#24179;&#21488;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27700;&#21360;&#25216;&#26415;&#12290;&#36825;&#20123;&#26041;&#27861;&#29992;&#20110;&#39564;&#35777;&#30446;&#26631;DNN&#27169;&#22411;&#30340;&#25152;&#26377;&#26435;&#20197;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#12290;&#26412;&#25991;&#39318;&#20808;&#20174;&#29305;&#24449;&#23398;&#20064;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35302;&#21457;&#38598;&#30340;&#27700;&#21360;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#36873;&#25321;&#23637;&#31034;&#22810;&#20010;&#29305;&#24449;&#30340;&#25968;&#25454;&#65292;&#20063;&#34987;&#31216;&#20026;$\textit{&#22810;&#35270;&#35282;&#25968;&#25454;}$&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#38450;&#24481;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10663v1 Announce Type: cross  Abstract: With the increasing prevalence of Machine Learning as a Service (MLaaS) platforms, there is a growing focus on deep neural network (DNN) watermarking techniques. These methods are used to facilitate the verification of ownership for a target DNN model to protect intellectual property. One of the most widely employed watermarking techniques involves embedding a trigger set into the source model. Unfortunately, existing methodologies based on trigger sets are still susceptible to functionality-stealing attacks, potentially enabling adversaries to steal the functionality of the source model without a reliable means of verifying ownership. In this paper, we first introduce a novel perspective on trigger set-based watermarking methods from a feature learning perspective. Specifically, we demonstrate that by selecting data exhibiting multiple features, also referred to as $\textit{multi-view data}$, it becomes feasible to effectively defend 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65292;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.09918</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Attention-based Class-Conditioned Alignment for Multi-Source Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09918
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65292;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#65288;OD&#65289;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#33268;&#21147;&#20110;&#36890;&#36807;&#20419;&#36827;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#29305;&#24449;&#23545;&#40784;&#26469;&#32531;&#35299;&#20998;&#24067;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#20801;&#35768;&#21033;&#29992;&#22810;&#20010;&#24102;&#27880;&#37322;&#30340;&#28304;&#25968;&#25454;&#38598;&#21644;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#26469;&#25552;&#39640;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;OD MSDA&#26041;&#27861;&#20197;&#19968;&#31181;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#26041;&#24335;&#25191;&#34892;&#29305;&#24449;&#23545;&#40784;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#25353;&#31867;&#21035;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#22024;&#26434;&#30340;&#20266;&#26631;&#31614;&#32780;&#23548;&#33268;&#38169;&#35823;&#31215;&#32047;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;MSDA&#65292;&#35813;&#26041;&#26696;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09918v1 Announce Type: cross  Abstract: Domain adaptation methods for object detection (OD) strive to mitigate the impact of distribution shifts by promoting feature alignment across source and target domains. Multi-source domain adaptation (MSDA) allows leveraging multiple annotated source datasets, and unlabeled target data to improve the accuracy and robustness of the detection model. Most state-of-the-art MSDA methods for OD perform feature alignment in a class-agnostic manner. This is challenging since the objects have unique modal information due to variations in object appearance across domains. A recent prototype-based approach proposed a class-wise alignment, yet it suffers from error accumulation due to noisy pseudo-labels which can negatively affect adaptation with imbalanced data. To overcome these limitations, we propose an attention-based class-conditioned alignment scheme for MSDA that aligns instances of each object category across domains. In particular, an 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#20449;&#21495;&#20256;&#25773;&#30340;&#20844;&#24335;&#65292;&#25552;&#20986;&#20102;DeepScaleLM&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#28145;&#23618;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#27973;&#23618;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.09635</link><description>&lt;p&gt;
Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models
&lt;/p&gt;
&lt;p&gt;
Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09635
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#20449;&#21495;&#20256;&#25773;&#30340;&#20844;&#24335;&#65292;&#25552;&#20986;&#20102;DeepScaleLM&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#28145;&#23618;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#27973;&#23618;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;transformer&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#28145;&#24230;&#26041;&#38754;&#20173;&#28982;&#24456;&#38590;&#25193;&#23637;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#21069;&#21521;&#21644;&#21453;&#21521;&#20449;&#21495;&#30697;&#30340;&#20844;&#24335;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#29992;&#20110;&#29702;&#35299;&#21644;&#32531;&#35299;&#19982;&#39640;&#27880;&#24847;&#21147;&#20998;&#25968;&#30456;&#20851;&#30340;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#12289;&#31209;&#22349;&#32553;&#21644;&#19981;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;DeepScaleLM&#65292;&#19968;&#31181;&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#36890;&#36807;&#35813;&#26041;&#26696;&#33021;&#22815;&#22312;&#27169;&#22411;&#20013;&#20445;&#25345;&#21333;&#20301;&#36755;&#20986;/&#26799;&#24230;&#30697;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#20855;&#26377;100&#22810;&#23618;&#30340;&#38750;&#24120;&#28145;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;transformer&#27169;&#22411;&#21487;&#20197;&#26356;&#28145; - &#25105;&#20204;&#30340;&#28145;&#23618;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#35821;&#38899;&#32763;&#35793;&#21644;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#21253;&#25324;&#20165;&#32534;&#30721;&#22120;&#12289;&#20165;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#20307;&#65292;&#36866;&#29992;&#20110;Pre-LN&#21644;Post-LN transformers&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09635v1 Announce Type: cross  Abstract: In spite of their huge success, transformer models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 100s of layers. We find that transformer models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, Speech Translation, and Image Classification, across Encoder-only, Decoder-only and Encoder-Decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes. These imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22914;&#20309;&#20419;&#36827;&#26377;&#25928;&#30340;&#24494;&#35843;&#65292;&#21516;&#26102;&#25351;&#20986;&#36739;&#20302;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#22987;&#32456;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18286</link><description>&lt;p&gt;
&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65306;&#36808;&#21521;&#39640;&#32423;&#22270;&#20687;&#20998;&#26512;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning in Electron Microscopy: Towards a Foundation Model for Advanced Image Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22914;&#20309;&#20419;&#36827;&#26377;&#25928;&#30340;&#24494;&#35843;&#65292;&#21516;&#26102;&#25351;&#20986;&#36739;&#20302;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#22987;&#32456;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20174;&#26080;&#26631;&#31614;&#30340;&#30005;&#23376;&#26174;&#24494;&#38236;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#36808;&#20986;&#20102;&#26500;&#24314;&#35813;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#30340;&#19968;&#27493;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22914;&#20309;&#20419;&#36827;&#26377;&#25928;&#30340;&#24494;&#35843;&#65292;&#20197;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#20998;&#21106;&#12289;&#21435;&#22122;&#12289;&#22122;&#22768;&#19982;&#32972;&#26223;&#21435;&#38500;&#20197;&#21450;&#36229;&#20998;&#36776;&#29575;&#12290;&#36890;&#36807;&#23454;&#39564;&#19981;&#21516;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#24863;&#21463;&#37326;&#22823;&#23567;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#26174;&#33879;&#30340;&#29616;&#35937;&#65292;&#21363;&#24494;&#35843;&#36807;&#30340;&#36739;&#20302;&#22797;&#26434;&#24230;&#27169;&#22411;&#22987;&#32456;&#32988;&#36807;&#20855;&#26377;&#38543;&#26426;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#26356;&#22797;&#26434;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#30005;&#23376;&#26174;&#24494;&#38236;&#32972;&#26223;&#19979;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#22810;&#25165;&#22810;&#33402;&#65292;&#20351;&#24471;&#24555;&#36895;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#20652;&#21270;&#21058;&#65292;&#29305;&#21035;&#22312;&#26377;&#38480;&#30340;&#27880;&#37322;&#25968;&#25454;&#21487;&#29992;&#26102;&#21644; ef
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18286v1 Announce Type: cross  Abstract: In this work, we explore the potential of self-supervised learning from unlabeled electron microscopy datasets, taking a step toward building a foundation model in this field. We show how self-supervised pretraining facilitates efficient fine-tuning for a spectrum of downstream tasks, including semantic segmentation, denoising, noise &amp; background removal, and super-resolution. Experimentation with varying model complexities and receptive field sizes reveals the remarkable phenomenon that fine-tuned models of lower complexity consistently outperform more complex models with random weight initialization. We demonstrate the versatility of self-supervised pretraining across various downstream tasks in the context of electron microscopy, allowing faster convergence and better performance. We conclude that self-supervised pretraining serves as a powerful catalyst, being especially advantageous when limited annotated data are available and ef
&lt;/p&gt;</description></item><item><title>V-IRL&#26159;&#19968;&#20010;&#24179;&#21488;&#65292;&#21487;&#20197;&#35753;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#19982;&#29616;&#23454;&#19990;&#30028;&#36827;&#34892;&#20114;&#21160;&#65292;&#26088;&#22312;&#23558;&#25968;&#23383;&#21644;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#65292;&#24182;&#24320;&#21457;&#20986;&#20855;&#26377;&#20016;&#23500;&#24863;&#30693;&#12289;&#20915;&#31574;&#21644;&#19982;&#30495;&#23454;&#25968;&#25454;&#20114;&#21160;&#33021;&#21147;&#30340;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.03310</link><description>&lt;p&gt;
V-IRL: &#23558;&#34394;&#25311;&#26234;&#33021;&#19982;&#29616;&#23454;&#29983;&#27963;&#32852;&#31995;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
V-IRL: Grounding Virtual Intelligence in Real Life
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03310
&lt;/p&gt;
&lt;p&gt;
V-IRL&#26159;&#19968;&#20010;&#24179;&#21488;&#65292;&#21487;&#20197;&#35753;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#19982;&#29616;&#23454;&#19990;&#30028;&#36827;&#34892;&#20114;&#21160;&#65292;&#26088;&#22312;&#23558;&#25968;&#23383;&#21644;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#65292;&#24182;&#24320;&#21457;&#20986;&#20855;&#26377;&#20016;&#23500;&#24863;&#30693;&#12289;&#20915;&#31574;&#21644;&#19982;&#30495;&#23454;&#25968;&#25454;&#20114;&#21160;&#33021;&#21147;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#29983;&#27963;&#22312;&#22320;&#29699;&#19978;&#65292;&#32780;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#25152;&#21019;&#36896;&#30340;&#25968;&#23383;&#39046;&#22495;&#20043;&#38388;&#23384;&#22312;&#30528;&#24863;&#23448;&#24046;&#36317;&#12290;&#20026;&#20102;&#24320;&#21457;&#20986;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#28789;&#27963;&#24863;&#30693;&#12289;&#24605;&#32771;&#21644;&#34892;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#24517;&#39035;&#24357;&#21512;&#25968;&#23383;&#21644;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#30340;&#36924;&#30495;&#24046;&#36317;&#12290;&#25105;&#20204;&#22914;&#20309;&#22312;&#19968;&#20010;&#20687;&#25105;&#20204;&#25152;&#23621;&#20303;&#30340;&#19990;&#30028;&#20013;&#19968;&#26679;&#20016;&#23500;&#22810;&#26679;&#30340;&#29615;&#22659;&#20013;&#20307;&#29616;&#20195;&#29702;&#65292;&#32780;&#19981;&#21463;&#30495;&#23454;&#30828;&#20214;&#21644;&#25511;&#21046;&#25152;&#26045;&#21152;&#30340;&#32422;&#26463;&#65311;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;V-IRL: &#19968;&#31181;&#24179;&#21488;&#65292;&#21487;&#20197;&#20351;&#20195;&#29702;&#22312;&#34394;&#25311;&#32780;&#36924;&#30495;&#30340;&#29615;&#22659;&#20013;&#19982;&#29616;&#23454;&#19990;&#30028;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#24179;&#21488;&#26082;&#26159;&#19968;&#20010;&#24320;&#21457;&#20195;&#29702;&#23436;&#25104;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#30340;&#28216;&#20048;&#22330;&#65292;&#21448;&#26159;&#19968;&#20010;&#24191;&#38420;&#30340;&#27979;&#35797;&#22522;&#22320;&#65292;&#29992;&#20110;&#34913;&#37327;&#22312;&#24863;&#30693;&#12289;&#20915;&#31574;&#21644;&#19982;&#20840;&#29699;&#30495;&#23454;&#25968;&#25454;&#30340;&#20114;&#21160;&#33021;&#21147;&#31561;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created. To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds. How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control? Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment. Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe.
&lt;/p&gt;</description></item><item><title>SynthCLIP&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#22270;&#29255;&#21644;&#26631;&#39064;&#25968;&#25454;&#38598;&#65292;&#22312;&#24615;&#33021;&#19978;&#21487;&#20197;&#19982;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;</title><link>https://arxiv.org/abs/2402.01832</link><description>&lt;p&gt;
SynthCLIP: &#25105;&#20204;&#20934;&#22791;&#22909;&#24320;&#22987;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#35757;&#32451;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01832
&lt;/p&gt;
&lt;p&gt;
SynthCLIP&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#22270;&#29255;&#21644;&#26631;&#39064;&#25968;&#25454;&#38598;&#65292;&#22312;&#24615;&#33021;&#19978;&#21487;&#20197;&#19982;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SynthCLIP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#35757;&#32451;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#19982;&#20043;&#21069;&#20381;&#36182;&#30495;&#23454;&#25968;&#25454;&#30340;&#26041;&#27861;&#26377;&#30528;&#26174;&#33879;&#21306;&#21035;&#12290;&#20511;&#21161;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#32593;&#32476;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#20219;&#24847;&#35268;&#27169;&#30340;&#22270;&#20687;&#21644;&#30456;&#24212;&#30340;&#26631;&#39064;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#26080;&#38656;&#20154;&#20026;&#24178;&#39044;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#65292;SynthCLIP&#23454;&#29616;&#20102;&#19982;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;SynthCI-30M&#65292;&#19968;&#20010;&#32431;&#31929;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;3000&#19975;&#24352;&#24102;&#26631;&#39064;&#30340;&#22270;&#29255;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#24050;&#32463;&#22312;https://github.com/hammoudhasan/SynthCLIP&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SynthCLIP, a novel framework for training CLIP models with entirely synthetic text-image pairs, significantly departing from previous methods relying on real data. Leveraging recent text-to-image (TTI) generative networks and large language models (LLM), we are able to generate synthetic datasets of images and corresponding captions at any scale, with no human intervention. With training at scale, SynthCLIP achieves performance comparable to CLIP models trained on real datasets. We also introduce SynthCI-30M, a purely synthetic dataset comprising 30 million captioned images. Our code, trained models, and generated data are released at https://github.com/hammoudhasan/SynthCLIP
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#24120;&#35782;&#25512;&#29702;&#26469;&#24314;&#27169;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#65292;&#36890;&#36807;&#25193;&#23637;&#21040;Deepfake Detection VQA&#20219;&#21153;&#26469;&#27169;&#25311;&#20154;&#31867;&#30452;&#35273;&#65292;&#35299;&#37322;&#26631;&#35760;&#22270;&#20687;&#20026;&#30495;&#23454;&#25110;&#20266;&#36896;&#30340;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2402.00126</link><description>&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#24120;&#35782;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Common Sense Reasoning for Deep Fake Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00126
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#24120;&#35782;&#25512;&#29702;&#26469;&#24314;&#27169;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#65292;&#36890;&#36807;&#25193;&#23637;&#21040;Deepfake Detection VQA&#20219;&#21153;&#26469;&#27169;&#25311;&#20154;&#31867;&#30452;&#35273;&#65292;&#35299;&#37322;&#26631;&#35760;&#22270;&#20687;&#20026;&#30495;&#23454;&#25110;&#20266;&#36896;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#29305;&#24449;&#36827;&#34892;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20108;&#20998;&#31867;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#22312;&#30417;&#30563;&#35757;&#32451;&#19979;&#25552;&#21462;&#20102;&#21487;&#33021;&#30340;&#20266;&#36896;&#29305;&#24449;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#34920;&#31034;&#19981;&#33258;&#28982;&#30340;&#8220;&#38750;&#29289;&#29702;&#8221;&#35821;&#20041;&#38754;&#37096;&#23646;&#24615; - &#27169;&#31946;&#30340;&#21457;&#38469;&#32447;&#12289;&#21452;&#30473;&#27611;&#12289;&#20725;&#30828;&#30340;&#30643;&#23380;&#25110;&#19981;&#33258;&#28982;&#30340;&#30382;&#32932;&#30528;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#38754;&#37096;&#23646;&#24615;&#36890;&#24120;&#36890;&#36807;&#24120;&#35782;&#25512;&#29702;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#23481;&#26131;&#24863;&#30693;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26174;&#33879;&#24615;&#22270;&#25552;&#20379;&#35270;&#35273;&#35299;&#37322;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21487;&#33021;&#24456;&#38590;&#34987;&#20154;&#31867;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#24120;&#35782;&#25512;&#29702;&#26469;&#24314;&#27169;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#65292;&#24182;&#23558;&#20854;&#25193;&#23637;&#21040;Deepfake Detection VQA&#65288;DD-VQA&#65289;&#20219;&#21153;&#65292;&#30446;&#30340;&#26159;&#27169;&#25311;&#20154;&#31867;&#30452;&#35273;&#26469;&#35299;&#37322;&#26631;&#35760;&#22270;&#20687;&#20026;&#30495;&#23454;&#25110;&#20266;&#36896;&#30340;&#21407;&#22240;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#19982;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30456;&#20851;&#30340;&#38382;&#39064;&#25552;&#20379;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art approaches rely on image-based features extracted via neural networks for the deepfake detection binary classification. While these approaches trained in the supervised sense extract likely fake features, they may fall short in representing unnatural `non-physical' semantic facial attributes -- blurry hairlines, double eyebrows, rigid eye pupils, or unnatural skin shading. However, such facial attributes are generally easily perceived by humans via common sense reasoning. Furthermore, image-based feature extraction methods that provide visual explanation via saliency maps can be hard to be interpreted by humans. To address these challenges, we propose the use of common sense reasoning to model deepfake detection, and extend it to the Deepfake Detection VQA (DD-VQA) task with the aim to model human intuition in explaining the reason behind labeling an image as either real or fake. To this end, we introduce a new dataset that provides answers to the questions related to 
&lt;/p&gt;</description></item><item><title>Receler&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#27010;&#24565;&#25830;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#27233;&#30382;&#25830;&#23454;&#29616;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#24565;&#25830;&#38500;&#65292;&#20855;&#22791;&#40065;&#26834;&#24615;&#21644;&#23616;&#37096;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.17717</link><description>&lt;p&gt;
Receler: &#36890;&#36807;&#36731;&#37327;&#32423;&#27233;&#30382;&#25830;&#21487;&#38752;&#22320;&#25830;&#38500;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via Lightweight Erasers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17717
&lt;/p&gt;
&lt;p&gt;
Receler&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#27010;&#24565;&#25830;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#27233;&#30382;&#25830;&#23454;&#29616;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#24565;&#25830;&#38500;&#65292;&#20855;&#22791;&#40065;&#26834;&#24615;&#21644;&#23616;&#37096;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#27010;&#24565;&#25830;&#38500;&#26088;&#22312;&#31105;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19982;&#30446;&#26631;&#27010;&#24565;&#30456;&#20851;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#38752;&#30340;&#27010;&#24565;&#25830;&#38500;&#65292;&#24076;&#26395;&#20855;&#22791;&#40065;&#26834;&#24615;&#21644;&#23616;&#37096;&#24615;&#30340;&#23646;&#24615;&#12290;&#21069;&#32773;&#38459;&#27490;&#27169;&#22411;&#20026;&#20219;&#20309;&#37322;&#20041;&#25110;&#23398;&#20064;&#25552;&#31034;&#29983;&#25104;&#19982;&#30446;&#26631;&#27010;&#24565;&#30456;&#20851;&#30340;&#22270;&#20687;&#65292;&#32780;&#21518;&#32773;&#20445;&#25345;&#20854;&#29983;&#25104;&#20855;&#26377;&#38750;&#30446;&#26631;&#27010;&#24565;&#30340;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#36731;&#37327;&#32423;&#27233;&#30382;&#25830;&#65288;Receler&#65289;&#26469;&#23454;&#29616;&#21487;&#38752;&#30340;&#27010;&#24565;&#25830;&#38500;&#12290;&#23427;&#23398;&#20064;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#27233;&#30382;&#25830;&#26469;&#36827;&#34892;&#27010;&#24565;&#25830;&#38500;&#65292;&#21516;&#26102;&#36890;&#36807;&#25552;&#20986;&#30340;&#27010;&#24565;&#23450;&#20301;&#27491;&#21017;&#21270;&#21644;&#23545;&#25239;&#25552;&#31034;&#23398;&#20064;&#26041;&#26696;&#28385;&#36275;&#19978;&#36848;&#29702;&#24819;&#29305;&#24615;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#27010;&#24565;&#30340;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#20102;Receler&#30456;&#23545;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;&#25509;&#21463;&#21518;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17717v2 Announce Type: replace-cross  Abstract: Concept erasure in text-to-image diffusion models aims to disable pre-trained diffusion models from generating images related to a target concept. To perform reliable concept erasure, the properties of robustness and locality are desirable. The former refrains the model from producing images associated with the target concept for any paraphrased or learned prompts, while the latter preserves its ability in generating images with non-target concepts. In this paper, we propose Reliable Concept Erasing via Lightweight Erasers (Receler). It learns a lightweight Eraser to perform concept erasing while satisfying the above desirable properties by proposed concept-localized regularization and adversarial prompt learning schemes. Comprehensive experiments with various concepts verify the superiority of Receler over previous methods. Our code will be available upon acceptance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24615;&#25512;&#29702;&#21644;&#38382;&#39064;&#29983;&#25104;&#65292;&#23558;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMM)&#36171;&#20104;&#20102;&#26174;&#24615;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25512;&#29702;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10005</link><description>&lt;p&gt;
&#20197;&#26174;&#24615;&#25512;&#29702;&#38142;&#21644;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#25512;&#36827;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation. (arXiv:2401.10005v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24615;&#25512;&#29702;&#21644;&#38382;&#39064;&#29983;&#25104;&#65292;&#23558;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMM)&#36171;&#20104;&#20102;&#26174;&#24615;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25512;&#29702;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#33021;&#22815;&#35299;&#37322;&#21644;&#25512;&#29702;&#35270;&#35273;&#20869;&#23481;&#30340;&#26234;&#33021;&#31995;&#32479;&#38656;&#27714;&#36234;&#26469;&#36234;&#39640;&#65292;&#38656;&#35201;&#24320;&#21457;&#19981;&#20165;&#20934;&#30830;&#32780;&#19988;&#20855;&#26377;&#26174;&#24615;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#26174;&#24615;&#25512;&#29702;&#33021;&#21147;&#36171;&#20104;LMMs&#65292;&#22522;&#20110;&#35270;&#35273;&#20869;&#23481;&#21644;&#25991;&#26412;&#25351;&#23548;&#36827;&#34892;&#26174;&#24615;&#25512;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21487;&#20197;&#25552;&#38382;&#20197;&#33719;&#21462;&#24517;&#35201;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#25512;&#29702;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#30340;&#24320;&#21457;&#65292;&#26088;&#22312;&#20419;&#36827;&#24605;&#32500;&#38142;&#25512;&#29702;&#19982;&#25552;&#38382;&#26426;&#21046;&#30340;&#32467;&#21512;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#24230;&#20855;&#26377;&#21306;&#22495;&#24847;&#35782;&#30340;LMM&#65292;&#20197;&#35299;&#20915;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#30340;&#22797;&#26434;&#38656;&#27714;&#12290;&#35813;&#27169;&#22411;&#32463;&#21382;&#20102;&#19977;&#20010;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#39318;&#20808;&#26159;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#65292;&#25509;&#19979;&#26469;&#26159;&#36890;&#36807;&#26174;&#24335;&#25512;&#29702;&#30340;&#38382;&#39064;&#29983;&#25104;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing demand for intelligent systems capable of interpreting and reasoning about visual content requires the development of Large Multi-Modal Models (LMMs) that are not only accurate but also have explicit reasoning capabilities. This paper presents a novel approach to imbue an LMM with the ability to conduct explicit reasoning based on visual content and textual instructions. We introduce a system that can ask a question to acquire necessary knowledge, thereby enhancing the robustness and explicability of the reasoning process. Our method comprises the development of a novel dataset generated by a Large Language Model (LLM), designed to promote chain-of-thought reasoning combined with a question-asking mechanism. We designed an LMM, which has high capabilities on region awareness to address the intricate requirements of image-text alignment. The model undergoes a three-stage training phase, starting with large-scale image-text alignment using a large-scale datasets, followed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#37327;&#21270;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20869;&#23384;&#39640;&#25928;&#20010;&#24615;&#21270;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#31574;&#30053;&#26469;&#35299;&#20915;&#22522;&#32447;&#27169;&#22411;&#20013;&#20027;&#39064;&#21644;&#25552;&#31034;&#36136;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.04339</link><description>&lt;p&gt;
&#20351;&#29992;&#37327;&#21270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20869;&#23384;&#39640;&#25928;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Memory-Efficient Personalization using Quantized Diffusion Model. (arXiv:2401.04339v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#37327;&#21270;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20869;&#23384;&#39640;&#25928;&#20010;&#24615;&#21270;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#31574;&#30053;&#26469;&#35299;&#20915;&#22522;&#32447;&#27169;&#22411;&#20013;&#20027;&#39064;&#21644;&#25552;&#31034;&#36136;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20159;&#32423;&#21442;&#25968;&#25193;&#25955;&#27169;&#22411;&#65288;&#22914;Stable Diffusion XL&#12289;Imagen&#21644;Dall-E3&#65289;&#30340;&#23835;&#36215;&#26174;&#33879;&#25512;&#21160;&#20102;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36164;&#28304;&#38656;&#27714;&#39640;&#21644;&#25512;&#29702;&#36895;&#24230;&#24930;&#65292;&#23427;&#20204;&#30340;&#22823;&#35268;&#27169;&#24615;&#36136;&#22312;&#24494;&#35843;&#21644;&#37096;&#32626;&#20013;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#23545;&#37327;&#21270;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#30456;&#23545;&#26410;&#24320;&#21457;&#20294;&#26377;&#21069;&#26223;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#21046;&#19977;&#20010;&#27169;&#22411;&#65288;&#29992;&#20110;&#24494;&#35843;&#37327;&#21270;&#21442;&#25968;&#30340;PEQA&#65292;&#29992;&#20110;&#21518;&#26399;&#37327;&#21270;&#30340;Q-Diffusion&#21644;&#20010;&#24615;&#21270;&#30340;DreamBooth&#65289;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22522;&#32447;&#27169;&#22411;&#20013;&#20027;&#39064;&#21644;&#25552;&#31034;&#36136;&#37327;&#20043;&#38388;&#30340;&#26126;&#26174;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#25193;&#25955;&#27169;&#22411;&#20013;&#19981;&#21516;&#26102;&#38388;&#27493;&#38271;&#30340;&#19981;&#21516;&#35282;&#33394;&#65306;S1&#22312;&#36873;&#25321;&#30340;&#26102;&#38388;&#38388;&#38548;&#20869;&#20165;&#20248;&#21270;&#19968;&#32452;&#24494;&#35843;&#21442;&#25968;&#65292;S2&#21019;&#24314;&#22810;&#20010;&#24494;&#35843;&#21442;&#25968;&#32452;&#65292;&#27599;&#20010;&#32452;&#19987;&#38376;&#29992;&#20110;&#19981;&#21516;&#30340;&#26102;&#38388;&#27493;&#38271;&#38388;&#38548;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of billion-parameter diffusion models like Stable Diffusion XL, Imagen, and Dall-E3 markedly advances the field of generative AI. However, their large-scale nature poses challenges in fine-tuning and deployment due to high resource demands and slow inference speed. This paper ventures into the relatively unexplored yet promising realm of fine-tuning quantized diffusion models. We establish a strong baseline by customizing three models: PEQA for fine-tuning quantization parameters, Q-Diffusion for post-training quantization, and DreamBooth for personalization. Our analysis reveals a notable trade-off between subject and prompt fidelity within the baseline model. To address these issues, we introduce two strategies, inspired by the distinct roles of different timesteps in diffusion models: S1 optimizing a single set of fine-tuning parameters exclusively at selected intervals, and S2 creating multiple fine-tuning parameter sets, each specialized for different timestep intervals. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#26080;&#30417;&#30563;&#24494;&#35843;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#26410;&#30693;&#31867;&#21035;&#21644;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;UEO&#30340;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#25552;&#39640;&#23545;&#36229;&#20986;&#20998;&#24067;&#26679;&#26412;&#30340;&#26816;&#27979;&#33021;&#21147;&#21644;&#39044;&#23450;&#20041;&#31867;&#21035;&#23454;&#20363;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12919</link><description>&lt;p&gt;
&#29992;CLIP&#23454;&#29616;&#30495;&#23454;&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Towards Realistic Unsupervised Fine-tuning with CLIP. (arXiv:2308.12919v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#26080;&#30417;&#30563;&#24494;&#35843;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#26410;&#30693;&#31867;&#21035;&#21644;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;UEO&#30340;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#25552;&#39640;&#23545;&#36229;&#20986;&#20998;&#24067;&#26679;&#26412;&#30340;&#26816;&#27979;&#33021;&#21147;&#21644;&#39044;&#23450;&#20041;&#31867;&#21035;&#23454;&#20363;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22914;CLIP&#30340;&#20986;&#29616;&#25512;&#21160;&#20102;&#20154;&#20204;&#22312;&#19979;&#28216;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;&#12290;&#23613;&#31649;&#19968;&#20123;&#20043;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;CLIP&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#20381;&#36182;&#20110;&#19982;&#30495;&#23454;&#26631;&#31614;&#30456;&#20851;&#30340;&#31867;&#21517;&#31561;&#20808;&#39564;&#30693;&#35782;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#30495;&#23454;&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;&#24773;&#26223;&#65292;&#20551;&#35774;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#26469;&#33258;&#26410;&#30693;&#31867;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#39044;&#23450;&#20041;&#31867;&#26631;&#31614;&#30340;&#35782;&#21035;&#20043;&#22806;&#65292;&#21516;&#26102;&#25552;&#39640;&#23545;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;Universal Entropy Optimization (UEO)&#12290;UEO&#21033;&#29992;&#26679;&#26412;&#32423;&#32622;&#20449;&#24230;&#65292;&#20197;&#36817;&#20284;&#26041;&#24335;&#26368;&#23567;&#21270;&#32622;&#20449;&#23454;&#20363;&#30340;&#26465;&#20214;&#29109;&#24182;&#26368;&#22823;&#21270;&#36793;&#32536;&#29109;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of vision-language models (VLMs), such as CLIP, has spurred a significant research effort towards their application for downstream supervised learning tasks. Although some previous studies have explored the unsupervised fine-tuning of CLIP, they often rely on prior knowledge in the form of class names associated with ground truth labels. In this paper, we delve into a realistic unsupervised fine-tuning scenario by assuming that the unlabeled data might contain out-of-distribution samples from unknown classes. Furthermore, we emphasize the importance of simultaneously enhancing out-of-distribution detection capabilities alongside the recognition of instances associated with predefined class labels.  To tackle this problem, we present a simple, efficient, and effective fine-tuning approach called Universal Entropy Optimization (UEO). UEO leverages sample-level confidence to approximately minimize the conditional entropy of confident instances and maximize the marginal entro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#21644;&#20004;&#31181;logit&#35843;&#25972;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#23614;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;CIFAR&#21644;ImageNet&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10648</link><description>&lt;p&gt;
&#23545;&#39640;&#38271;&#23614;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;logit&#36827;&#34892;&#39640;&#26031;&#24418;&#24335;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Adjusting Logit in Gaussian Form for Long-Tailed Visual Recognition. (arXiv:2305.10648v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#21644;&#20004;&#31181;logit&#35843;&#25972;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#23614;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;CIFAR&#21644;ImageNet&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#24448;&#24448;&#20855;&#26377;&#38271;&#23614;&#20998;&#24067;&#12290;&#23545;&#20110;&#36825;&#31181;&#25968;&#25454;&#65292;&#30001;&#20110;&#38590;&#20197;&#27491;&#30830;&#20998;&#31867;&#23614;&#37096;&#31867;&#21035;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#24050;&#26377;&#19968;&#20123;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#20998;&#31867;&#22120;&#20559;&#24046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21069;&#25552;&#26159;&#29992;&#38271;&#23614;&#25968;&#25454;&#33719;&#24471;&#30340;&#29305;&#24449;&#36275;&#22815;&#20195;&#34920;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#30452;&#25509;&#22312;&#38271;&#23614;&#25968;&#25454;&#19978;&#35757;&#32451;&#20250;&#23548;&#33268;&#19981;&#22343;&#21248;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22836;&#31867;&#30340;&#23884;&#20837;&#31354;&#38388;&#20005;&#37325;&#21387;&#32553;&#23614;&#31867;&#65292;&#36825;&#23545;&#20110;&#21518;&#32493;&#30340;&#20998;&#31867;&#22120;&#23398;&#20064;&#26159;&#19981;&#21033;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20174;&#29305;&#24449;&#27700;&#24179;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#38271;&#23614;&#35270;&#35273;&#35782;&#21035;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#29305;&#24449;&#22686;&#24378;&#26469;&#24179;&#34913;&#23884;&#20837;&#20998;&#24067;&#12290;&#19981;&#21516;&#31867;&#21035;&#30340;&#29305;&#24449;&#20197;&#39640;&#26031;&#24418;&#24335;&#20855;&#26377;&#19981;&#21516;&#25391;&#24133;&#30340;&#25200;&#21160;&#12290;&#22522;&#20110;&#36825;&#20123;&#25200;&#21160;&#30340;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;logit&#35843;&#25972;&#26041;&#27861;&#26469;&#25552;&#39640;&#23614;&#37096;&#31867;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;CIFAR&#21644;ImageNet&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is not uncommon that real-world data are distributed with a long tail. For such data, the learning of deep neural networks becomes challenging because it is hard to classify tail classes correctly. In the literature, several existing methods have addressed this problem by reducing classifier bias provided that the features obtained with long-tailed data are representative enough. However, we find that training directly on long-tailed data leads to uneven embedding space. That is, the embedding space of head classes severely compresses that of tail classes, which is not conducive to subsequent classifier learning. %further improving model performance. This paper therefore studies the problem of long-tailed visual recognition from the perspective of feature level. We introduce feature augmentation to balance the embedding distribution. The features of different classes are perturbed with varying amplitudes in Gaussian form. Based on these perturbed features, two novel logit adjustment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#24314;&#31569;&#24418;&#24335;&#30340;3D&#23545;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#28857;&#35758;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.00510</link><description>&lt;p&gt;
&#36890;&#21521;&#33258;&#30001;&#35745;&#31639;&#26550;&#26500;: &#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#20803;&#23431;&#23449;&#34394;&#25311;&#24314;&#31569;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Towards Computational Architecture of Liberty: A Comprehensive Survey on Deep Learning for Generating Virtual Architecture in the Metaverse. (arXiv:2305.00510v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#24314;&#31569;&#24418;&#24335;&#30340;3D&#23545;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#28857;&#35758;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;3D&#24418;&#29366;&#29983;&#25104;&#25216;&#26415;&#27491;&#22312;&#21463;&#21040;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#24314;&#31569;&#35774;&#35745;&#20004;&#26041;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#32508;&#21512;&#35843;&#26597;&#26088;&#22312;&#35843;&#26597;&#21644;&#27604;&#36739;&#24403;&#21069;&#26368;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGMs&#65289;&#30340;3D&#23545;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12289;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#12289;3D&#24863;&#30693;&#22270;&#20687;&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;187&#31687;&#25991;&#31456;(&#21344;2018-2022&#24180;&#38388;&#21457;&#34920;&#25991;&#31456;&#30340;80.7%)&#65292;&#20197;&#22238;&#39038;&#22312;&#34394;&#25311;&#29615;&#22659;&#19979;&#24314;&#31569;&#29983;&#25104;&#21487;&#33021;&#24615;&#30340;&#39046;&#22495;&#65292;&#38480;&#20110;&#24314;&#31569;&#24418;&#24335;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24314;&#31569;&#30740;&#31350;&#12289;&#34394;&#25311;&#29615;&#22659;&#21644;&#30456;&#20851;&#25216;&#26415;&#26041;&#27861;&#30340;&#27010;&#36848;&#65292;&#25509;&#30528;&#22238;&#39038;&#20102;&#31163;&#25955;&#20307;&#32032;&#29983;&#25104;&#12289;&#30001;2D&#22270;&#20687;&#29983;&#25104;&#30340;3D&#27169;&#22411;&#20197;&#21450;&#26465;&#20214;&#21442;&#25968;&#30340;&#26368;&#36817;&#36235;&#21183;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;3D&#29983;&#25104;&#21644;&#21442;&#25968;&#21270;&#25511;&#21046;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#20540;&#24471;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#27979;&#21253;&#25324;&#29983;&#25104;&#22810;&#26679;&#24615;&#12289;&#26032;&#22411;&#36755;&#20986;&#21644;&#23884;&#20837;&#24335;&#26500;&#24314;&#31561;&#22235;&#20010;&#30740;&#31350;&#35758;&#31243;&#21487;&#33021;&#20250;&#25104;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D shape generation techniques utilizing deep learning are increasing attention from both computer vision and architectural design. This survey focuses on investigating and comparing the current latest approaches to 3D object generation with deep generative models (DGMs), including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), 3D-aware images, and diffusion models. We discuss 187 articles (80.7% of articles published between 2018-2022) to review the field of generated possibilities of architecture in virtual environments, limited to the architecture form. We provide an overview of architectural research, virtual environment, and related technical approaches, followed by a review of recent trends in discrete voxel generation, 3D models generated from 2D images, and conditional parameters. We highlight under-explored issues in 3D generation and parameterized control that is worth further investigation. Moreover, we speculate that four research agendas including
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#36890;&#36807;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#65292;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.11525</link><description>&lt;p&gt;
SIFT: &#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#20197;&#26368;&#22823;&#38480;&#24230;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency. (arXiv:2303.11525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#36890;&#36807;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#65292;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35757;&#32451;&#25928;&#29575;&#65288;&#19982;&#35757;&#32451;FLOPS&#30456;&#20851;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#65289;&#12290; &#36825;&#20123;&#24037;&#20316;&#26088;&#22312;&#20943;&#23569;&#35757;&#32451;FLOP&#65292;&#20294;&#20351;&#29992;&#31232;&#30095;&#26435;&#37325;&#36827;&#34892;&#35757;&#32451;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#25439;&#22833;&#25110;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#21608;&#26399;&#65292;&#20351;&#24471;&#32467;&#26524;&#30340;&#35757;&#32451;&#25928;&#29575;&#19981;&#22815;&#28165;&#26224;&#12290; &#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#31232;&#30095;&#24615;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#21516;&#30340;FLOPS&#65292;&#24182;&#36890;&#36807;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#23637;&#31034;&#35757;&#32451;&#25928;&#29575;&#25552;&#39640;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SIFT&#65292;&#19968;&#32452;&#29992;&#20316;&#23494;&#38598;&#23618;&#30340;&#21363;&#25554;&#21363;&#29992;&#26367;&#20195;&#21697;&#26469;&#25552;&#39640;&#20854;&#34920;&#31034;&#33021;&#21147;&#21644;FLOP&#25928;&#29575;&#30340;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#12290; &#27599;&#20010;&#36716;&#25442;&#37117;&#30001;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#65288;&#31232;&#30095;&#32423;&#21035;&#65289;&#21442;&#25968;&#21270;&#65292;&#24182;&#25552;&#20379;&#26356;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#31232;&#30095;&#25513;&#33180;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have explored the use of weight sparsity to improve the training efficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs). These works aim to reduce training FLOPs but training with sparse weights often leads to accuracy loss or requires longer train schedules, making the resulting training efficiency less clear. In contrast, we focus on using sparsity to increase accuracy while using the same FLOPS as the dense model and show training efficiency gains through higher accuracy. In this work, we introduce SIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in replacements for dense layers to improve their representational capacity and FLOP efficiency. Each transformation is parameterized by a single parameter (sparsity level) and provides a larger search space to find optimal sparse masks. Without changing any training hyperparameters, replacing dense layers with SIFT leads to significant improvements across computer vision (CV) and
&lt;/p&gt;</description></item></channel></rss>