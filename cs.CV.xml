<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#35813;&#30740;&#31350;&#20174;&#29702;&#35770;&#35282;&#24230;&#20026;&#21307;&#23398;&#24322;&#24120;&#26816;&#27979;&#20013;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#37325;&#24314;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#25581;&#31034;&#20102;&#25913;&#36827;AE&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20851;&#38190;&#22312;&#20110;&#26368;&#23567;&#21270;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.09303</link><description>&lt;p&gt;
&#29992;&#29702;&#35770;&#35270;&#35282;&#37325;&#26032;&#24605;&#32771;&#21307;&#23398;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09303
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20174;&#29702;&#35770;&#35282;&#24230;&#20026;&#21307;&#23398;&#24322;&#24120;&#26816;&#27979;&#20013;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#37325;&#24314;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#25581;&#31034;&#20102;&#25913;&#36827;AE&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20851;&#38190;&#22312;&#20110;&#26368;&#23567;&#21270;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24322;&#24120;&#26816;&#27979;&#26088;&#22312;&#20165;&#20351;&#29992;&#27491;&#24120;&#35757;&#32451;&#25968;&#25454;&#35782;&#21035;&#24322;&#24120;&#21457;&#29616;&#65292;&#23545;&#20581;&#24247;&#31579;&#26597;&#21644;&#35782;&#21035;&#32597;&#35265;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#65288;AEs&#65289;&#30340;&#26041;&#27861;&#22312;&#36825;&#19968;&#39046;&#22495;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#23427;&#20204;&#22522;&#20110;&#36825;&#26679;&#30340;&#20551;&#35774;&#24037;&#20316;&#65306;&#20165;&#20351;&#29992;&#27491;&#24120;&#25968;&#25454;&#35757;&#32451;&#30340;AEs&#19981;&#33021;&#24456;&#22909;&#22320;&#37325;&#24314;&#30475;&#19981;&#35265;&#30340;&#24322;&#24120;&#21306;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#22522;&#20110;&#37325;&#24314;&#38169;&#35823;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37325;&#24314;&#35757;&#32451;&#30446;&#26631;&#19982;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#36825;&#19968;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#65292;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#19981;&#22815;&#21512;&#29702;&#12290;&#35813;&#30740;&#31350;&#20391;&#37325;&#20110;&#20026;&#22522;&#20110;AE&#30340;&#37325;&#24314;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#25552;&#20379;&#29702;&#35770;&#22522;&#30784;&#12290;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#35770;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#21407;&#21017;&#65292;&#24182;&#25581;&#31034;&#20102;&#25913;&#36827;AE&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20851;&#38190;&#22312;&#20110;&#26368;&#23567;&#21270;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09303v1 Announce Type: new  Abstract: Medical anomaly detection aims to identify abnormal findings using only normal training data, playing a crucial role in health screening and recognizing rare diseases. Reconstruction-based methods, particularly those utilizing autoencoders (AEs), are dominant in this field. They work under the assumption that AEs trained on only normal data cannot reconstruct unseen abnormal regions well, thereby enabling the anomaly detection based on reconstruction errors. However, this assumption does not always hold due to the mismatch between the reconstruction training objective and the anomaly detection task objective, rendering these methods theoretically unsound. This study focuses on providing a theoretical foundation for AE-based reconstruction methods in anomaly detection. By leveraging information theory, we elucidate the principles of these methods and reveal that the key to improving AE in anomaly detection lies in minimizing the informati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#23454;&#29616;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#65292;&#22312;GPU&#20869;&#23384;&#21344;&#29992;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#28210;&#26579;&#36895;&#24230;&#65292;&#24182;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#30456;&#21305;&#25932;&#12290;</title><link>https://arxiv.org/abs/2403.08551</link><description>&lt;p&gt;
&#39640;&#26031;&#22270;&#20687;&#65306;&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#36827;&#34892;1000&#24103;&#27599;&#31186;&#30340;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08551
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#23454;&#29616;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#65292;&#22312;GPU&#20869;&#23384;&#21344;&#29992;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#28210;&#26579;&#36895;&#24230;&#65292;&#24182;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#30456;&#21305;&#25932;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#22312;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#25552;&#20379;&#20102;&#39640;&#35270;&#35273;&#36136;&#37327;&#21644;&#24555;&#36895;&#28210;&#26579;&#36895;&#24230;&#65292;&#27599;&#31186;10-1000&#24103;&#65292;&#20551;&#35774;&#26377;&#36275;&#22815;&#30340;GPU&#36164;&#28304;&#21487;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35201;&#27714;&#24120;&#24120;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#20869;&#23384;&#26377;&#38480;&#30340;&#20302;&#31471;&#35774;&#22791;&#19978;&#30340;&#20351;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#36827;&#34892;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#30340;&#24320;&#21019;&#24615;&#33539;&#24335;&#65292;&#21517;&#20026;GaussianImage&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;2D&#39640;&#26031;&#26469;&#34920;&#31034;&#22270;&#20687;&#65292;&#20854;&#20013;&#27599;&#20010;&#39640;&#26031;&#20855;&#26377;8&#20010;&#21442;&#25968;&#65292;&#21253;&#25324;&#20301;&#32622;&#12289;&#21327;&#26041;&#24046;&#21644;&#39068;&#33394;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#32047;&#31215;&#27714;&#21644;&#30340;&#26032;&#39062;&#28210;&#26579;&#31639;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;GPU&#20869;&#23384;&#33267;&#23569;&#38477;&#20302;3&#20493;&#65292;&#25311;&#21512;&#26102;&#38388;&#24555;5&#20493;&#65292;&#19981;&#20165;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#65288;&#20363;&#22914;WIRE&#65292;I-NGP&#65289;&#19981;&#30456;&#19978;&#19979;&#65292;&#32780;&#19988;&#26080;&#35770;&#21442;&#25968;&#22823;&#23567;&#22914;&#20309;&#37117;&#33021;&#25552;&#20379;1500-2000&#24103;&#27599;&#31186;&#30340;&#26356;&#24555;&#28210;&#26579;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08551v1 Announce Type: cross  Abstract: Implicit neural representations (INRs) recently achieved great success in image representation and compression, offering high visual quality and fast rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available. However, this requirement often hinders their use on low-end devices with limited memory. In response, we propose a groundbreaking paradigm of image representation and compression by 2D Gaussian Splatting, named GaussianImage. We first introduce 2D Gaussian to represent the image, where each Gaussian has 8 parameters including position, covariance and color. Subsequently, we unveil a novel rendering algorithm based on accumulated summation. Remarkably, our method with a minimum of 3$\times$ lower GPU memory usage and 5$\times$ faster fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation performance, but also delivers a faster rendering speed of 1500-2000 FPS regardless of parameter size. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;NegOpt&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#30340;&#29983;&#25104;&#65292;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#24182;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.07605</link><description>&lt;p&gt;
&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#20197;&#22686;&#24378;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#32654;&#23398;&#21644;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07605
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;NegOpt&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#30340;&#29983;&#25104;&#65292;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#24182;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#65292;&#20351;&#29992;&#25551;&#36848;&#19981;&#33391;&#22270;&#20687;&#29305;&#24449;&#30340;&#36127;&#38754;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#33391;&#22909;&#30340;&#36127;&#38754;&#25552;&#31034;&#26159;&#19968;&#39033;&#25163;&#24037;&#32780;&#32321;&#29712;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NegOpt&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#29983;&#25104;&#65292;&#20174;&#32780;&#22686;&#24378;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#22823;&#24133;&#25552;&#39640;&#20102;25%&#30340;Inception Score&#65292;&#24182;&#36229;&#36234;&#20102;&#26469;&#33258;&#27979;&#35797;&#38598;&#30340;&#26631;&#20934;&#36127;&#38754;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;NegOpt&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#20248;&#21270;&#23545;&#25105;&#20204;&#26368;&#37325;&#35201;&#30340;&#25351;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;Negative Prompts DB&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07605v1 Announce Type: cross  Abstract: In text-to-image generation, using negative prompts, which describe undesirable image characteristics, can significantly boost image quality. However, producing good negative prompts is manual and tedious. To address this, we propose NegOpt, a novel method for optimizing negative prompt generation toward enhanced image generation, using supervised fine-tuning and reinforcement learning. Our combined approach results in a substantial increase of 25% in Inception Score compared to other approaches and surpasses ground-truth negative prompts from the test set. Furthermore, with NegOpt we can preferentially optimize the metrics most important to us. Finally, we construct Negative Prompts DB, a dataset of negative prompts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#65292;&#26469;&#22686;&#24378;&#23545;&#24433;&#21709;&#25830;&#38500;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07362</link><description>&lt;p&gt;
&#25361;&#25112;&#36951;&#24536;&#65306;&#25581;&#31034;&#26426;&#22120;&#36951;&#24536;&#20013;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;
&lt;/p&gt;
&lt;p&gt;
Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#65292;&#26469;&#22686;&#24378;&#23545;&#24433;&#21709;&#25830;&#38500;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38752;&#35889;&#30340;&#26426;&#22120;&#23398;&#20064;(Machine Learning, ML)&#31038;&#21306;&#36234;&#26469;&#36234;&#35748;&#35782;&#21040;&#27169;&#22411;&#22312;&#35757;&#32451;&#21518;&#26377;&#36873;&#25321;&#24615;&#22320;&#8220;&#36951;&#24536;&#8221;&#25968;&#25454;&#28857;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#24341;&#20986;&#20102;&#26426;&#22120;&#36951;&#24536;(Machine Unlearning, MU)&#38382;&#39064;&#65292;&#26088;&#22312;&#28040;&#38500;&#36873;&#23450;&#25968;&#25454;&#28857;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#27169;&#22411;&#22312;&#36951;&#24536;&#21518;&#30340;&#23454;&#29992;&#24615;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;MU&#26041;&#27861;&#26469;&#25830;&#38500;&#25968;&#25454;&#24433;&#21709;&#65292;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#22312;&#38543;&#26426;&#25968;&#25454;&#36951;&#24536;&#19978;&#65292;&#24573;&#35270;&#20102;&#23545;&#20110;&#30495;&#23454;&#34913;&#37327;&#36951;&#24536;&#24615;&#33021;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#30340;&#37325;&#35201;&#25506;&#31350;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;MU&#35780;&#20272;&#35270;&#35282;&#12290;&#25105;&#20204;&#25552;&#20986;&#30830;&#23450;&#37027;&#20123;&#23545;&#24433;&#21709;&#25830;&#38500;&#26500;&#25104;&#26368;&#22823;&#25361;&#25112;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#25214;&#20986;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#12290;&#21033;&#29992;&#21452;&#23618;&#20248;&#21270;&#21407;&#21017;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#22312;&#19978;&#23618;&#20248;&#21270;&#20013;&#30340;&#36951;&#24536;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07362v1 Announce Type: cross  Abstract: The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively 'unlearning' data points after training. This leads to the problem of machine unlearning (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning. Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization 
&lt;/p&gt;</description></item><item><title>AUFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#26816;&#27979;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#30693;&#35782;&#28151;&#21512;&#19987;&#23478;&#21327;&#20316;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#31232;&#32570;&#25968;&#25454;&#38598;&#25110;&#36807;&#24230;&#20381;&#36182;&#39069;&#22806;&#25968;&#25454;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.04697</link><description>&lt;p&gt;
AUFormer: &#35270;&#35273;Transformer&#26159;&#21442;&#25968;&#39640;&#25928;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit Detectors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04697
&lt;/p&gt;
&lt;p&gt;
AUFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#26816;&#27979;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#30693;&#35782;&#28151;&#21512;&#19987;&#23478;&#21327;&#20316;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#31232;&#32570;&#25968;&#25454;&#38598;&#25110;&#36807;&#24230;&#20381;&#36182;&#39069;&#22806;&#25968;&#25454;&#23548;&#33268;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#22312;&#24773;&#24863;&#35745;&#31639;&#39046;&#22495;&#26159;&#19968;&#20010;&#37325;&#35201;&#27010;&#24565;&#65292;AU&#26816;&#27979;&#19968;&#30452;&#26159;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#22312;&#31232;&#32570;&#30340;AU&#27880;&#37322;&#25968;&#25454;&#38598;&#19978;&#21033;&#29992;&#22823;&#37327;&#21487;&#23398;&#20064;&#21442;&#25968;&#25110;&#36807;&#24230;&#20381;&#36182;&#22823;&#37327;&#39069;&#22806;&#30456;&#20851;&#25968;&#25454;&#32780;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#21442;&#25968;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;&#65288;PETL&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#33539;&#24335;&#65292;&#28982;&#32780;&#20854;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#38024;&#23545;AU&#29305;&#24449;&#30340;&#35774;&#35745;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#23558;PETL&#33539;&#24335;&#24212;&#29992;&#20110;AU&#26816;&#27979;&#65292;&#24341;&#20837;AUFormer&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#28151;&#21512;&#19987;&#23478;&#65288;MoKE&#65289;&#21327;&#20316;&#26426;&#21046;&#12290;&#19968;&#20010;&#29305;&#23450;&#20110;&#26576;&#20010;AU&#24182;&#20855;&#26377;&#26368;&#23569;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;MoKE&#39318;&#20808;&#38598;&#25104;&#20010;&#24615;&#21270;&#30340;&#22810;&#23610;&#24230;&#21644;&#30456;&#20851;&#30693;&#35782;&#12290;&#28982;&#21518;MoKE&#19982;&#19987;&#23478;&#32452;&#20013;&#30340;&#20854;&#20182;MoKE&#21512;&#20316;&#65292;&#33719;&#21462;&#32858;&#21512;&#20449;&#24687;&#24182;&#23558;&#20854;&#27880;&#20837;&#21040;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04697v1 Announce Type: cross  Abstract: Facial Action Units (AU) is a vital concept in the realm of affective computing, and AU detection has always been a hot research topic. Existing methods suffer from overfitting issues due to the utilization of a large number of learnable parameters on scarce AU-annotated datasets or heavy reliance on substantial additional relevant data. Parameter-Efficient Transfer Learning (PETL) provides a promising paradigm to address these challenges, whereas its existing methods lack design for AU characteristics. Therefore, we innovatively investigate PETL paradigm to AU detection, introducing AUFormer and proposing a novel Mixture-of-Knowledge Expert (MoKE) collaboration mechanism. An individual MoKE specific to a certain AU with minimal learnable parameters first integrates personalized multi-scale and correlation knowledge. Then the MoKE collaborates with other MoKEs in the expert group to obtain aggregated information and inject it into the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#27010;&#36848;&#20102;&#20351;&#29992;&#21464;&#20998;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#25104;&#20687;&#20013;&#36870;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#22312;&#20110;&#28857;&#20272;&#35745;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#25506;&#32034;&#25968;&#25454;&#19968;&#33268;&#35299;&#23376;&#31354;&#38388;&#20197;&#28385;&#36275;&#29305;&#23450;&#35821;&#20041;&#25110;&#32441;&#29702;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12072</link><description>&lt;p&gt;
&#21464;&#20998;&#26041;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#36870;&#38382;&#39064;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#25506;&#32034;&#65306;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Robustness and Exploration of Variational and Machine Learning Approaches to Inverse Problems: An Overview
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27010;&#36848;&#20102;&#20351;&#29992;&#21464;&#20998;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#25104;&#20687;&#20013;&#36870;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#22312;&#20110;&#28857;&#20272;&#35745;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#25506;&#32034;&#25968;&#25454;&#19968;&#33268;&#35299;&#23376;&#31354;&#38388;&#20197;&#28385;&#36275;&#29305;&#23450;&#35821;&#20041;&#25110;&#32441;&#29702;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35797;&#22270;&#27010;&#36848;&#20351;&#29992;&#21464;&#20998;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#26469;&#35299;&#20915;&#25104;&#20687;&#20013;&#36870;&#38382;&#39064;&#30340;&#24403;&#21069;&#26041;&#27861;&#12290;&#37325;&#28857;&#20851;&#27880;&#28857;&#20272;&#35745;&#22120;&#21450;&#20854;&#23545;&#25239;&#24615;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19968;&#32500;&#31034;&#20363;&#38382;&#39064;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#24182;&#22312;&#32463;&#39564;&#19978;&#39564;&#35777;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#35813;&#32508;&#36848;&#30340;&#21478;&#19968;&#20010;&#37325;&#28857;&#26159;&#36890;&#36807;&#26126;&#30830;&#25351;&#23548;&#26469;&#25506;&#32034;&#25968;&#25454;&#19968;&#33268;&#35299;&#30340;&#23376;&#31354;&#38388;&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#35821;&#20041;&#25110;&#32441;&#29702;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12072v1 Announce Type: cross  Abstract: This paper attempts to provide an overview of current approaches for solving inverse problems in imaging using variational methods and machine learning. A special focus lies on point estimators and their robustness against adversarial perturbations. In this context results of numerical experiments for a one-dimensional toy problem are provided, showing the robustness of different approaches and empirically verifying theoretical guarantees. Another focus of this review is the exploration of the subspace of data consistent solutions through explicit guidance to satisfy specific semantic or textural properties.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.17263</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#25110;&#30772;&#35299;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#23545;&#25163;&#20462;&#25913;&#36755;&#20837;&#25552;&#31034;&#20197;&#35825;&#23548;&#26377;&#23475;&#34892;&#20026;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20165;&#20851;&#27880;&#29421;&#31364;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#24378;&#22823;&#30340;&#38450;&#24481;&#12290;&#20026;&#20102;&#23454;&#29616;&#24378;&#22823;&#30340;&#38450;&#24481;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#23545;&#25239;&#30772;&#35299;&#25915;&#20987;&#30340;&#23545;&#25239;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#25552;&#31034;&#20248;&#21270;&#65288;RPO&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20196;&#29260;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#21518;&#32512;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#23545;&#30772;&#35299;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#65292;&#21253;&#25324;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#30772;&#35299;&#25915;&#20987;&#20197;&#21450;&#26410;&#30693;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#20174;84%&#38477;&#20302;&#21040;8.66%&#65292;&#22312;20&#20010;&#30772;&#35299;&#25915;&#20987;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;RPO&#23545;&#27491;&#24120;LM&#20351;&#29992;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#22312;&#36866;&#24212;&#24615;&#25915;&#20987;&#19979;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#36801;&#31227;&#21040;&#40657;&#30418;&#27169;&#22411;&#20013;&#65292;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#20998;&#26512;&#20013;&#20855;&#26377;&#35786;&#26029;&#21644;&#39044;&#27979;&#30340;&#28508;&#21147;&#65292;&#24403;&#21069;&#30740;&#31350;&#32858;&#28966;&#20110;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#65292;&#20197;&#21450;&#23545;&#35813;&#30142;&#30149;&#30340;&#20998;&#31867;&#12289;&#20998;&#32423;&#12289;&#20998;&#23376;&#26631;&#35760;&#39044;&#27979;&#21644;&#29983;&#23384;&#39044;&#27979;&#31561;&#20020;&#24202;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.15022</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#20998;&#26512;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based analysis of glioma tissue sections: a review. (arXiv:2401.15022v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15022
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#20998;&#26512;&#20013;&#20855;&#26377;&#35786;&#26029;&#21644;&#39044;&#27979;&#30340;&#28508;&#21147;&#65292;&#24403;&#21069;&#30740;&#31350;&#32858;&#28966;&#20110;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#65292;&#20197;&#21450;&#23545;&#35813;&#30142;&#30149;&#30340;&#20998;&#31867;&#12289;&#20998;&#32423;&#12289;&#20998;&#23376;&#26631;&#35760;&#39044;&#27979;&#21644;&#29983;&#23384;&#39044;&#27979;&#31561;&#20020;&#24202;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33014;&#36136;&#30244;&#30340;&#35786;&#26029;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#20351;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#33014;&#36136;&#30244;&#32452;&#32455;&#36827;&#34892;&#32452;&#32455;&#23398;&#35780;&#20272;&#65292;&#20026;&#35786;&#26029;&#21644;&#39044;&#27979;&#32467;&#26524;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#23545;&#24403;&#21069;&#30740;&#31350;&#30340;&#29616;&#29366;&#36827;&#34892;&#27010;&#36848;&#65292;&#26412;&#32508;&#36848;&#23545;70&#20010;&#20844;&#24320;&#21487;&#24471;&#30340;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36825;&#20123;&#35770;&#25991;&#20851;&#20110;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#26579;&#33394;&#30340;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#65292;&#28085;&#30422;&#20102;&#20998;&#31867;&#65288;16/70&#65289;&#65292;&#20998;&#32423;&#65288;23/70&#65289;&#65292;&#20998;&#23376;&#26631;&#35760;&#39044;&#27979;&#65288;13/70&#65289;&#21644;&#29983;&#23384;&#39044;&#27979;&#65288;27/70&#65289;&#31561;&#35786;&#26029;&#20219;&#21153;&#12290;&#25152;&#26377;&#30340;&#30740;&#31350;&#37117;&#22312;&#26041;&#27861;&#23398;&#26041;&#38754;&#21450;&#20854;&#20020;&#24202;&#36866;&#29992;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#23545;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#36827;&#34892;&#35780;&#20272;&#12290;&#22810;&#25968;&#30740;&#31350;&#65288;49/70&#65289;&#22522;&#20110;&#20844;&#24320;&#30340;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#21644;&#20302;&#32423;&#21035;&#33014;&#36136;&#30244;&#25968;&#25454;&#38598;&#65292;&#20165;&#26377;&#23569;&#25968;&#30740;&#31350;&#20351;&#29992;&#20854;&#20182;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the diagnosis of gliomas has become increasingly complex. Histological assessment of glioma tissue using modern machine learning techniques offers new opportunities to support diagnosis and outcome prediction. To give an overview of the current state of research, this review examines 70 publicly available research studies on machine learning-based analysis of stained human glioma tissue sections, covering the diagnostic tasks of subtyping (16/70), grading (23/70), molecular marker prediction (13/70), and survival prediction (27/70). All studies were reviewed with regard to methodological aspects as well as clinical applicability. It was found that the focus of current research is the assessment of hematoxylin and eosin-stained tissue sections of adult-type diffuse gliomas. The majority of studies (49/70) are based on the publicly available glioblastoma and low-grade glioma datasets from The Cancer Genome Atlas (TCGA) and only a few studies employed other datasets in is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22686;&#24378;&#22411;&#27531;&#24046;&#32593;&#32476;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#27531;&#24046;&#20803;&#32032;&#20013;&#28155;&#21152;&#24130;&#27425;&#39033;&#25552;&#21319;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#24212;&#29992;&#24615;&#33021;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#38750;&#24179;&#28369;&#20989;&#25968;&#30340;&#22788;&#29702;&#12290;&#21516;&#26102;&#65292;&#35813;&#32593;&#32476;&#32467;&#26500;&#22312;&#35299;&#20915;&#21453;&#38382;&#39064;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15690</link><description>&lt;p&gt;
&#20351;&#29992;&#22686;&#24378;&#22411;&#27531;&#24046;&#32593;&#32476;&#36827;&#34892;&#25554;&#20540;&#21644;&#21453;&#38382;&#39064;&#30340;&#29289;&#29702;&#39537;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed with Power-Enhanced Residual Network for Interpolation and Inverse Problems. (arXiv:2310.15690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22686;&#24378;&#22411;&#27531;&#24046;&#32593;&#32476;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#27531;&#24046;&#20803;&#32032;&#20013;&#28155;&#21152;&#24130;&#27425;&#39033;&#25552;&#21319;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#24212;&#29992;&#24615;&#33021;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#38750;&#24179;&#28369;&#20989;&#25968;&#30340;&#22788;&#29702;&#12290;&#21516;&#26102;&#65292;&#35813;&#32593;&#32476;&#32467;&#26500;&#22312;&#35299;&#20915;&#21453;&#38382;&#39064;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#31216;&#20026;&#22686;&#24378;&#22411;&#27531;&#24046;&#32593;&#32476;&#65292;&#26088;&#22312;&#25913;&#21892;2D&#21644;3D&#29615;&#22659;&#19979;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#20989;&#25968;&#30340;&#25554;&#20540;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#27531;&#24046;&#20803;&#32032;&#20013;&#28155;&#21152;&#24130;&#27425;&#39033;&#65292;&#35813;&#32593;&#32476;&#32467;&#26500;&#22686;&#24378;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#30740;&#31350;&#25506;&#31350;&#20102;&#32593;&#32476;&#28145;&#24230;&#12289;&#23485;&#24230;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#32593;&#32476;&#32467;&#26500;&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;&#32467;&#26524;&#19968;&#33268;&#34920;&#26126;&#65292;&#22686;&#24378;&#22411;&#27531;&#24046;&#32593;&#32476;&#22312;&#38750;&#24179;&#28369;&#20989;&#25968;&#26041;&#38754;&#20855;&#26377;&#24322;&#24120;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#38469;&#31034;&#20363;&#20063;&#35777;&#23454;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#12289;&#25910;&#25947;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30456;&#23545;&#20110;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#36234;&#24615;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#26356;&#28145;&#23618;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#30340;&#32593;&#32476;&#32467;&#26500;&#36824;&#24212;&#29992;&#20110;&#35299;&#20915;&#21453;Burgers&#26041;&#31243;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#24635;&#20043;&#65292;&#22686;&#24378;&#22411;&#27531;&#24046;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26126;&#26174;&#25552;&#21319;&#20102;&#25554;&#20540;&#21644;&#21453;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel neural network structure called the Power-Enhancing residual network, designed to improve interpolation capabilities for both smooth and non-smooth functions in 2D and 3D settings. By adding power terms to residual elements, the architecture boosts the network's expressive power. The study explores network depth, width, and optimization methods, showing the architecture's adaptability and performance advantages. Consistently, the results emphasize the exceptional accuracy of the proposed Power-Enhancing residual network, particularly for non-smooth functions. Real-world examples also confirm its superiority over plain neural network in terms of accuracy, convergence, and efficiency. The study also looks at the impact of deeper network. Moreover, the proposed architecture is also applied to solving the inverse Burgers' equation, demonstrating superior performance. In conclusion, the Power-Enhancing residual network offers a versatile solution that significa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27963;&#21160;&#35782;&#21035;&#30340;&#20998;&#23618;&#22810;&#32467;&#26500;&#26041;&#27861;&#65292;&#21033;&#29992;&#27531;&#24046;&#32593;&#32476;&#21644;&#27531;&#24046;MobileNet&#23545;&#38745;&#24577;&#21644;&#21160;&#24577;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#28982;&#21518;&#36890;&#36807;&#21152;&#26435;&#21512;&#22863;&#26041;&#27861;&#36827;&#34892;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.02011</link><description>&lt;p&gt;
&#35299;&#30721;&#20154;&#31867;&#34892;&#20026;&#65306;&#20998;&#26512;&#21487;&#31359;&#25140;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#25968;&#25454;&#36827;&#34892;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Decoding Human Activities: Analyzing Wearable Accelerometer and Gyroscope Data for Activity Recognition. (arXiv:2310.02011v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27963;&#21160;&#35782;&#21035;&#30340;&#20998;&#23618;&#22810;&#32467;&#26500;&#26041;&#27861;&#65292;&#21033;&#29992;&#27531;&#24046;&#32593;&#32476;&#21644;&#27531;&#24046;MobileNet&#23545;&#38745;&#24577;&#21644;&#21160;&#24577;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#28982;&#21518;&#36890;&#36807;&#21152;&#26435;&#21512;&#22863;&#26041;&#27861;&#36827;&#34892;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20154;&#30340;&#36816;&#21160;&#25110;&#30456;&#23545;&#23450;&#20301;&#26377;&#25928;&#22320;&#20135;&#29983;&#20102;&#21487;&#20197;&#34987;&#35745;&#31639;&#26426;&#35835;&#21462;&#30340;&#21407;&#22987;&#30005;&#20449;&#21495;&#65292;&#36890;&#36807;&#24212;&#29992;&#21508;&#31181;&#25805;&#20316;&#25216;&#26415;&#26469;&#23545;&#19981;&#21516;&#30340;&#20154;&#31867;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#32593;&#32476;&#19982;&#27531;&#24046;MobileNet&#36827;&#34892;&#21512;&#22863;&#30340;&#20998;&#23618;&#22810;&#32467;&#26500;&#26041;&#27861;&#65292;&#31216;&#20026;FusionActNet&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#27531;&#24046;&#22359;&#20998;&#21035;&#23545;&#38745;&#24577;&#21644;&#21160;&#24577;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#26126;&#26174;&#32780;&#29420;&#29305;&#30340;&#29305;&#24449;&#12290;&#36825;&#20123;&#32593;&#32476;&#29420;&#31435;&#35757;&#32451;&#65292;&#24471;&#21040;&#20004;&#20010;&#19987;&#19994;&#30340;&#39640;&#31934;&#24230;&#27169;&#22411;&#12290;&#36890;&#36807;&#21033;&#29992;&#26550;&#26500;&#35843;&#25972;&#30340;&#31639;&#27861;&#20248;&#21183;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#36229;&#31867;&#20013;&#20248;&#31168;&#22320;&#35782;&#21035;&#27963;&#21160;&#12290;&#28982;&#21518;&#65292;&#36825;&#20004;&#20010;&#27531;&#24046;&#32593;&#32476;&#36890;&#36807;&#21152;&#26435;&#21512;&#22863;&#30340;&#27531;&#24046;MobileNet&#36827;&#34892;&#20256;&#36882;&#12290;&#38543;&#21518;&#65292;&#36825;&#20010;&#21512;&#22863;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#19968;&#20123;&#29305;&#23450;&#30340;&#23376;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
A person's movement or relative positioning effectively generates raw electrical signals that can be read by computing machines to apply various manipulative techniques for the classification of different human activities. In this paper, a stratified multi-structural approach based on a Residual network ensembled with Residual MobileNet is proposed, termed as FusionActNet. The proposed method involves using carefully designed Residual blocks for classifying the static and dynamic activities separately because they have clear and distinct characteristics that set them apart. These networks are trained independently, resulting in two specialized and highly accurate models. These models excel at recognizing activities within a specific superclass by taking advantage of the unique algorithmic benefits of architectural adjustments. Afterward, these two ResNets are passed through a weighted ensemble-based Residual MobileNet. Subsequently, this ensemble proficiently discriminates between a sp
&lt;/p&gt;</description></item><item><title>CAGRA&#26159;&#19968;&#31181;&#38754;&#21521;GPU&#30340;&#39640;&#24230;&#24182;&#34892;&#22270;&#26500;&#24314;&#21644;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26041;&#27861;&#65292;&#22312;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.15136</link><description>&lt;p&gt;
CAGRA&#65306;&#38754;&#21521;GPU&#30340;&#39640;&#24230;&#24182;&#34892;&#22270;&#26500;&#24314;&#21644;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
CAGRA: Highly Parallel Graph Construction and Approximate Nearest Neighbor Search for GPUs. (arXiv:2308.15136v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15136
&lt;/p&gt;
&lt;p&gt;
CAGRA&#26159;&#19968;&#31181;&#38754;&#21521;GPU&#30340;&#39640;&#24230;&#24182;&#34892;&#22270;&#26500;&#24314;&#21644;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26041;&#27861;&#65292;&#22312;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#65288;ANNS&#65289;&#22312;&#25968;&#25454;&#25366;&#25496;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28085;&#30422;&#20102;&#20449;&#24687;&#26816;&#32034;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#21508;&#20010;&#23398;&#31185;&#12290;&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#37327;&#24613;&#21095;&#22686;&#21152;&#65292;&#31351;&#20030;&#31934;&#30830;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#35745;&#31639;&#25104;&#26412;&#24448;&#24448;&#26159;&#31105;&#27490;&#24615;&#30340;&#65292;&#24517;&#39035;&#37319;&#29992;&#36817;&#20284;&#25216;&#26415;&#12290;&#23613;&#31649;&#22270;&#24418;&#21270;&#26041;&#27861;&#30340;&#24179;&#34913;&#24615;&#33021;&#21644;&#21484;&#22238;&#29575;&#22312;ANNS&#31639;&#27861;&#20013;&#26368;&#36817;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;GPU&#21644;&#22810;&#26680;&#22788;&#29702;&#22120;&#30340;&#24378;&#22823;&#35745;&#31639;&#33021;&#21147;&#65292;&#23613;&#31649;&#24191;&#27867;&#20351;&#29992;&#20102;&#22823;&#35268;&#27169;&#24182;&#34892;&#21644;&#36890;&#29992;&#35745;&#31639;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24182;&#34892;&#35745;&#31639;&#30828;&#20214;&#30340;&#26032;&#39062;&#25509;&#36817;&#22270;&#21644;&#25628;&#32034;&#31639;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#30828;&#20214;&#30340;&#39640;&#24615;&#33021;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22270;&#26500;&#24314;&#21644;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximate Nearest Neighbor Search (ANNS) plays a critical role in various disciplines spanning data mining and artificial intelligence, from information retrieval and computer vision to natural language processing and recommender systems. Data volumes have soared in recent years and the computational cost of an exhaustive exact nearest neighbor search is often prohibitive, necessitating the adoption of approximate techniques. The balanced performance and recall of graph-based approaches have more recently garnered significant attention in ANNS algorithms, however, only a few studies have explored harnessing the power of GPUs and multi-core processors despite the widespread use of massively parallel and general-purpose computing. To bridge this gap, we introduce a novel parallel computing hardware-based proximity graph and search algorithm. By leveraging the high-performance capabilities of modern hardware, our approach achieves remarkable efficiency gains. In particular, our method s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;IDIA&#26469;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#22823;&#35268;&#27169;&#23454;&#39564;&#34920;&#26126;&#20351;&#29992;&#20110;&#35757;&#32451;&#30340;&#20010;&#20154;&#21487;&#20197;&#34987;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#26469;&#65292;&#34920;&#26126;&#38656;&#35201;&#26356;&#22909;&#22320;&#35299;&#20915;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.07341</link><description>&lt;p&gt;
CLIP&#26159;&#21542;&#30693;&#36947;&#25105;&#30340;&#33080;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does CLIP Know My Face?. (arXiv:2209.07341v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;IDIA&#26469;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#22823;&#35268;&#27169;&#23454;&#39564;&#34920;&#26126;&#20351;&#29992;&#20110;&#35757;&#32451;&#30340;&#20010;&#20154;&#21487;&#20197;&#34987;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#26469;&#65292;&#34920;&#26126;&#38656;&#35201;&#26356;&#22909;&#22320;&#35299;&#20915;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#24212;&#29992;&#20013;&#30340;&#26222;&#21450;&#65292;&#20445;&#25252;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21333;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#22810;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#29305;&#21035;&#26159;&#20687;CLIP&#36825;&#26679;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#36523;&#20221;&#25512;&#26029;&#25915;&#20987;(IDIA)&#36890;&#36807;&#29992;&#21516;&#19968;&#20154;&#30340;&#22270;&#29255;&#21521;&#27169;&#22411;&#26597;&#35810;&#65292;&#20174;&#32780;&#25581;&#31034;&#35813;&#20010;&#20154;&#26159;&#21542;&#34987;&#21253;&#21547;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#35753;&#27169;&#22411;&#20174;&#21508;&#31181;&#21487;&#33021;&#30340;&#25991;&#26412;&#26631;&#31614;&#20013;&#36873;&#25321;&#65292;&#27169;&#22411;&#20250;&#36879;&#38706;&#26159;&#21542;&#35782;&#21035;&#35813;&#20154;&#29289;&#65292;&#20174;&#32780;&#34920;&#26126;&#20854;&#34987;&#29992;&#20110;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;CLIP&#19978;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#20110;&#35757;&#32451;&#30340;&#20010;&#20154;&#21487;&#20197;&#34987;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#26469;&#12290;&#25105;&#20204;&#30830;&#35748;&#35813;&#27169;&#22411;&#24050;&#32463;&#23398;&#20250;&#23558;&#21517;&#31216;&#19982;&#25551;&#32472;&#30340;&#20010;&#20154;&#30456;&#20851;&#32852;&#65292;&#36825;&#24847;&#21619;&#30528;&#25935;&#24863;&#20449;&#24687;&#23384;&#22312;&#20110;&#20854;&#20013;&#65292;&#21487;&#20197;&#34987;&#23545;&#25163;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#38656;&#35201;&#22312;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#26356;&#22909;&#22320;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of deep learning in various applications, privacy concerns around the protection of training data has become a critical area of research. Whereas prior studies have focused on privacy risks in single-modal models, we introduce a novel method to assess privacy for multi-modal models, specifically vision-language models like CLIP. The proposed Identity Inference Attack (IDIA) reveals whether an individual was included in the training data by querying the model with images of the same person. Letting the model choose from a wide variety of possible text labels, the model reveals whether it recognizes the person and, therefore, was used for training. Our large-scale experiments on CLIP demonstrate that individuals used for training can be identified with very high accuracy. We confirm that the model has learned to associate names with depicted individuals, implying the existence of sensitive information that can be extracted by adversaries. Our results highlight the need for 
&lt;/p&gt;</description></item><item><title>MetaCOG&#26159;&#19968;&#20010;&#23398;&#20064;&#20803;&#35748;&#30693;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#21487;&#38752;&#24615;&#34920;&#31034;&#65292;&#22686;&#21152;&#20102;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#21453;&#39304;&#21644;&#22320;&#38754;&#30495;&#23454;&#30340;&#29289;&#20307;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2110.03105</link><description>&lt;p&gt;
MetaCOG: &#23398;&#20064;&#20803;&#35748;&#30693;&#20197;&#24674;&#22797;&#23454;&#38469;&#23384;&#22312;&#30340;&#29289;&#20307;
&lt;/p&gt;
&lt;p&gt;
MetaCOG: Learning a Metacognition to Recover What Objects Are Actually There. (arXiv:2110.03105v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03105
&lt;/p&gt;
&lt;p&gt;
MetaCOG&#26159;&#19968;&#20010;&#23398;&#20064;&#20803;&#35748;&#30693;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#21487;&#38752;&#24615;&#34920;&#31034;&#65292;&#22686;&#21152;&#20102;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#21453;&#39304;&#21644;&#22320;&#38754;&#30495;&#23454;&#30340;&#29289;&#20307;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#19981;&#20165;&#26681;&#25454;&#25105;&#20204;&#25152;&#30475;&#21040;&#30340;&#20869;&#23481;&#24418;&#25104;&#20851;&#20110;&#19990;&#30028;&#30340;&#34920;&#24449;&#65292;&#36824;&#23398;&#20064;&#20851;&#20110;&#25105;&#20204;&#33258;&#24049;&#35270;&#35273;&#22914;&#20309;&#24037;&#20316;&#30340;&#20803;&#35748;&#30693;&#34920;&#24449;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#20986;&#25105;&#20204;&#30340;&#35270;&#35273;&#19981;&#21487;&#38752;&#65288;&#20363;&#22914;&#65292;&#24403;&#25105;&#20204;&#24847;&#35782;&#21040;&#25105;&#20204;&#27491;&#22312;&#32463;&#21382;&#35270;&#35273;&#38169;&#35273;&#26102;&#65289;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#25105;&#20204;&#25152;&#30475;&#21040;&#30340;&#20869;&#23481;&#25552;&#20986;&#36136;&#30097;&#12290;&#21463;&#21040;&#36825;&#31181;&#20154;&#31867;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MetaCOG&#65306;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#20854;&#21487;&#38752;&#24615;&#34920;&#31034;&#26469;&#22686;&#21152;&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MetaCOG&#26159;&#19968;&#20010;&#23618;&#27425;&#27010;&#29575;&#27169;&#22411;&#65292;&#23545;&#19968;&#20010;&#19977;&#32500;&#22330;&#26223;&#20013;&#30340;&#29289;&#20307;&#21644;&#26816;&#27979;&#22120;&#20135;&#29983;&#30340;&#36755;&#20986;&#34920;&#36798;&#20102;&#19968;&#20010;&#32852;&#21512;&#20998;&#24067;&#12290;&#24403;&#19982;&#29616;&#25104;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#37197;&#23545;&#20351;&#29992;&#26102;&#65292;MetaCOG&#23558;&#26816;&#27979;&#32467;&#26524;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#25512;&#26029;&#20986;&#26816;&#27979;&#22120;&#38169;&#28431;&#26816;&#26576;&#20123;&#31867;&#21035;&#30340;&#29289;&#20307;&#21644;&#34394;&#26500;&#19981;&#23384;&#22312;&#30340;&#29289;&#20307;&#30340;&#20542;&#21521;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#30340;&#29289;&#20307;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans not only form representations about the world based on what we see, but also learn meta-cognitive representations about how our own vision works. This enables us to recognize when our vision is unreliable (e.g., when we realize that we are experiencing a visual illusion) and enables us to question what we see. Inspired by this human capacity, we present MetaCOG: a model that increases the robustness of object detectors by learning representations of their reliability, and does so without feedback. Specifically, MetaCOG is a hierarchical probabilistic model that expresses a joint distribution over the objects in a 3D scene and the outputs produced by a detector. When paired with an off-the-shelf object detector, MetaCOG takes detections as input and infers the detector's tendencies to miss objects of certain categories and to hallucinate objects that are not actually present, all without access to ground-truth object labels. When paired with three modern neural object detectors, 
&lt;/p&gt;</description></item></channel></rss>