<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.20331</link><description>&lt;p&gt;
&#19981;&#21487;&#35299;&#38382;&#39064;&#26816;&#27979;&#65306;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#20013;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#12290;UPD&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#35774;&#32622;&#65306;&#32570;&#22833;&#31572;&#26696;&#26816;&#27979;&#65288;AAD&#65289;&#12289;&#19981;&#20860;&#23481;&#31572;&#26696;&#38598;&#26816;&#27979;&#65288;IASD&#65289;&#21644;&#19981;&#20860;&#23481;&#35270;&#35273;&#38382;&#39064;&#26816;&#27979;&#65288;IVQD&#65289;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#28145;&#20837;&#30740;&#31350;UPD&#38382;&#39064;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;VLMs&#65292;&#21253;&#25324;GPT-4V&#21644;LLaVA-Next-34B&#65292;&#22312;&#21508;&#31181;&#31243;&#24230;&#19978;&#37117;&#24456;&#38590;&#24212;&#23545;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#37325;&#35201;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;UPD&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26080;&#38656;&#35757;&#32451;&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#20197;&#21450;&#22312;&#25552;&#35758;&#30340;UPD&#35774;&#32622;&#20869;&#30340;&#26410;&#26469;&#21162;&#21147;&#65292;&#23558;&#22686;&#24378;&#23545;VLMs&#30340;&#26356;&#24191;&#27867;&#29702;&#35299;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20331v1 Announce Type: cross  Abstract: This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the VLM's ability to withhold answers when faced with unsolvable problems in the context of Visual Question Answering (VQA) tasks. UPD encompasses three distinct settings: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply investigate the UPD problem, extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements. To address UPD, we explore both training-free and training-based solutions, offering new insights into their effectiveness and limitations. We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#20844;&#24320;&#25968;&#25454;&#38598;&#26500;&#24314;&#29983;&#29289;&#20998;&#31867;&#32676;&#25968;&#25454;&#38598;&#20197;&#21450;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25512;&#23548;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#24182;&#20197;&#33889;&#33796;&#29273;&#26412;&#22320;&#26893;&#29289;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.12072</link><description>&lt;p&gt;
Floralens&#65306;&#19968;&#31181;&#29992;&#20110;&#33889;&#33796;&#29273;&#26412;&#22320;&#26893;&#29289;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Floralens: a Deep Learning Model for the Portuguese Native Flora
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#20844;&#24320;&#25968;&#25454;&#38598;&#26500;&#24314;&#29983;&#29289;&#20998;&#31867;&#32676;&#25968;&#25454;&#38598;&#20197;&#21450;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25512;&#23548;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#24182;&#20197;&#33889;&#33796;&#29273;&#26412;&#22320;&#26893;&#29289;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#35768;&#22810;&#20844;&#27665;&#31185;&#23398;&#24179;&#21488;&#20013;&#23545;&#29983;&#29289;&#29289;&#31181;&#36827;&#34892;&#22522;&#20110;&#22270;&#20687;&#30340;&#35782;&#21035;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#36275;&#22815;&#22823;&#23567;&#21644;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#32593;&#32476;&#20197;&#21450;&#32593;&#32476;&#26550;&#26500;&#30340;&#36873;&#25321;&#26412;&#36523;&#20173;&#28982;&#24456;&#23569;&#26377;&#25991;&#29486;&#35760;&#24405;&#65292;&#22240;&#27492;&#19981;&#23481;&#26131;&#34987;&#22797;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20844;&#24320;&#21487;&#29992;&#30340;&#30740;&#31350;&#32423;&#25968;&#25454;&#38598;&#26500;&#24314;&#29983;&#29289;&#20998;&#31867;&#32676;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#20351;&#29992;&#35895;&#27468;&#30340;AutoML Vision&#20113;&#26381;&#21153;&#25552;&#20379;&#30340;&#29616;&#25104;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#25512;&#23548;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#26159;&#33889;&#33796;&#29273;&#26412;&#22320;&#26893;&#29289;&#65292;&#22522;&#20110;&#30001;&#33889;&#33796;&#29273;&#26893;&#29289;&#23398;&#20250;&#25552;&#20379;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#26469;&#33258;iNaturalist&#12289;Pl@ntNet&#21644;Observation.org&#30340;&#37319;&#38598;&#25968;&#25454;&#36827;&#34892;&#25193;&#23637;&#12290;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#35880;&#24910;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12072v1 Announce Type: cross  Abstract: Machine-learning techniques, namely deep convolutional neural networks, are pivotal for image-based identification of biological species in many Citizen Science platforms. However, the construction of critically sized and sampled datasets to train the networks and the choice of the network architectures itself remains little documented and, therefore, does not lend itself to be easily replicated. In this paper, we develop a streamlined methodology for building datasets for biological taxa from publicly available research-grade datasets and for deriving models from these datasets using off-the-shelf deep convolutional neural networks such as those provided by Google's AutoML Vision cloud service. Our case study is the Portuguese native flora, anchored in a high-quality dataset, provided by the Sociedade Portuguesa de Bot\^anica, scaled up by adding sampled data from iNaturalist, Pl@ntNet, and Observation.org. We find that with a careful
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#20351;&#29992;&#28145;&#24230;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#36827;&#34892;&#27833;&#27745;&#20998;&#21106;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#22312;&#39640;&#32500;&#21355;&#26143;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#22270;&#20687;&#25968;&#25454;&#19978;&#27604;&#36739;&#20102;&#22810;&#31181;&#20998;&#21106;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#26368;&#22909;&#30340;&#34920;&#29616;&#27169;&#22411;&#26159;&#20351;&#29992;ResNet-50&#32534;&#30721;&#22120;&#21644;DeepLabV3+&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#23454;&#29616;64.868%&#30340;&#24179;&#22343;&#20132;&#38598;&#32852;&#21512;&#65288;IoU&#65289;&#21644;61.549%&#30340;&#8220;&#27833;&#27745;&#8221;&#31867;IoU&#12290;</title><link>http://arxiv.org/abs/2305.01386</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#36827;&#34892;&#27833;&#27745;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Oil Spill Segmentation using Deep Encoder-Decoder models. (arXiv:2305.01386v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#20351;&#29992;&#28145;&#24230;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#36827;&#34892;&#27833;&#27745;&#20998;&#21106;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#22312;&#39640;&#32500;&#21355;&#26143;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#22270;&#20687;&#25968;&#25454;&#19978;&#27604;&#36739;&#20102;&#22810;&#31181;&#20998;&#21106;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#26368;&#22909;&#30340;&#34920;&#29616;&#27169;&#22411;&#26159;&#20351;&#29992;ResNet-50&#32534;&#30721;&#22120;&#21644;DeepLabV3+&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#23454;&#29616;64.868%&#30340;&#24179;&#22343;&#20132;&#38598;&#32852;&#21512;&#65288;IoU&#65289;&#21644;61.549%&#30340;&#8220;&#27833;&#27745;&#8221;&#31867;IoU&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#27833;&#26159;&#29616;&#20195;&#19990;&#30028;&#32463;&#27982;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#38543;&#30528;&#21407;&#27833;&#24191;&#27867;&#24212;&#29992;&#30340;&#38656;&#27714;&#22686;&#38271;&#65292;&#24847;&#22806;&#30340;&#27833;&#27745;&#27844;&#28431;&#20063;&#38590;&#20197;&#36991;&#20813;&#12290;&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#20351;&#29992;&#28145;&#24230;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#36827;&#34892;&#27833;&#27745;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#27604;&#36739;&#20102;&#39640;&#32500;&#21355;&#26143;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#22270;&#20687;&#25968;&#25454;&#19978;&#20960;&#31181;&#20998;&#21106;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#22810;&#31181;&#27169;&#22411;&#32452;&#21512;&#12290;&#26368;&#22909;&#30340;&#34920;&#29616;&#27169;&#22411;&#26159;&#20351;&#29992;ResNet-50&#32534;&#30721;&#22120;&#21644;DeepLabV3+&#35299;&#30721;&#22120;&#65292;&#19982;&#24403;&#21069;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#22312;&#8220;&#27833;&#27745;&#8221;&#31867;&#30340;&#24179;&#22343;&#20132;&#38598;&#32852;&#21512;&#65288;IoU&#65289;&#19978;&#23454;&#29616;&#20102;64.868%&#30340;&#32467;&#26524;&#21644;61.549%&#30340;&#31867;IoU&#12290;
&lt;/p&gt;
&lt;p&gt;
Crude oil is an integral component of the modern world economy. With the growing demand for crude oil due to its widespread applications, accidental oil spills are unavoidable. Even though oil spills are in and themselves difficult to clean up, the first and foremost challenge is to detect spills. In this research, the authors test the feasibility of deep encoder-decoder models that can be trained effectively to detect oil spills. The work compares the results from several segmentation models on high dimensional satellite Synthetic Aperture Radar (SAR) image data. Multiple combinations of models are used in running the experiments. The best-performing model is the one with the ResNet-50 encoder and DeepLabV3+ decoder. It achieves a mean Intersection over Union (IoU) of 64.868% and a class IoU of 61.549% for the "oil spill" class when compared with the current benchmark model, which achieved a mean IoU of 65.05% and a class IoU of 53.38% for the "oil spill" class.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LostPaw&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#21033;&#29992;&#23545;&#27604;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20934;&#30830;&#21306;&#20998;&#23456;&#29289;&#22270;&#20687;&#65292;&#21487;&#29992;&#20110;&#31934;&#20934;&#25628;&#32034;&#22833;&#36394;&#30340;&#23456;&#29289;&#12290;&#35813;&#27169;&#22411;&#36798;&#21040;&#20102;90%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#20026;&#28508;&#22312;&#30340; Web &#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#29992;&#25143;&#33021;&#22815;&#19978;&#20256;&#20002;&#22833;&#23456;&#29289;&#30340;&#22270;&#20687;&#24182;&#22312;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#21305;&#37197;&#22270;&#20687;&#26102;&#25509;&#25910;&#36890;&#30693;&#12290;</title><link>http://arxiv.org/abs/2304.14765</link><description>&lt;p&gt;
LostPaw: &#20351;&#29992;&#24102;&#35270;&#35273;&#36755;&#20837;&#30340;&#23545;&#27604;&#23398;&#20064; Transformer &#25214;&#21040;&#22833;&#36394;&#30340;&#23456;&#29289;
&lt;/p&gt;
&lt;p&gt;
LostPaw: Finding Lost Pets using a Contrastive Learning-based Transformer with Visual Input. (arXiv:2304.14765v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LostPaw&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#21033;&#29992;&#23545;&#27604;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20934;&#30830;&#21306;&#20998;&#23456;&#29289;&#22270;&#20687;&#65292;&#21487;&#29992;&#20110;&#31934;&#20934;&#25628;&#32034;&#22833;&#36394;&#30340;&#23456;&#29289;&#12290;&#35813;&#27169;&#22411;&#36798;&#21040;&#20102;90%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#20026;&#28508;&#22312;&#30340; Web &#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#29992;&#25143;&#33021;&#22815;&#19978;&#20256;&#20002;&#22833;&#23456;&#29289;&#30340;&#22270;&#20687;&#24182;&#22312;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#21305;&#37197;&#22270;&#20687;&#26102;&#25509;&#25910;&#36890;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22833;&#21435;&#23456;&#29289;&#21487;&#33021;&#20250;&#35753;&#23456;&#29289;&#20027;&#20154;&#20493;&#24863;&#30171;&#33510;&#65292;&#32780;&#25214;&#21040;&#22833;&#36394;&#30340;&#23456;&#29289;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#12290;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23547;&#25214;&#20002;&#22833;&#23456;&#29289;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#20415;&#20110;&#36825;&#26679;&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#23454;&#29616;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23545;&#27604;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#21306;&#20998;&#19981;&#21516;&#23456;&#29289;&#30340;&#22270;&#20687;&#12290;&#35813;&#27169;&#22411;&#22312;&#22823;&#37327;&#30340;&#29399;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807; 3 &#25240;&#20132;&#21449;&#39564;&#35777;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#22312; 350 &#20010;&#35757;&#32451;&#21608;&#26399;&#21518;&#65292;&#27169;&#22411;&#21462;&#24471;&#20102;90%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#27979;&#35797;&#20934;&#30830;&#24615;&#25509;&#36817;&#35757;&#32451;&#20934;&#30830;&#24615;&#65292;&#36991;&#20813;&#20102;&#36807;&#24230;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#27604;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#23450;&#20301;&#22833;&#36394;&#23456;&#29289;&#30340;&#24037;&#20855;&#20855;&#26377;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340; Web &#24212;&#29992;&#31243;&#24207;&#30340;&#22522;&#30784;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19978;&#20256;&#20854;&#20002;&#22833;&#23456;&#29289;&#30340;&#22270;&#20687;&#65292;&#24182;&#22312;&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#20687;&#25968;&#25454;&#24211;&#20013;&#25214;&#21040;&#21305;&#37197;&#22270;&#20687;&#26102;&#25509;&#25910;&#36890;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Losing pets can be highly distressing for pet owners, and finding a lost pet is often challenging and time-consuming. An artificial intelligence-based application can significantly improve the speed and accuracy of finding lost pets. In order to facilitate such an application, this study introduces a contrastive neural network model capable of accurately distinguishing between images of pets. The model was trained on a large dataset of dog images and evaluated through 3-fold cross-validation. Following 350 epochs of training, the model achieved a test accuracy of 90%. Furthermore, overfitting was avoided, as the test accuracy closely matched the training accuracy. Our findings suggest that contrastive neural network models hold promise as a tool for locating lost pets. This paper provides the foundation for a potential web application that allows users to upload images of their missing pets, receiving notifications when matching images are found in the application's image database. Thi
&lt;/p&gt;</description></item></channel></rss>