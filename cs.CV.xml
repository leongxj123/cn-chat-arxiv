<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>SugarcaneNet2024&#26159;&#36890;&#36807;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24555;&#36895;&#20934;&#30830;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.18870</link><description>&lt;p&gt;
SugarcaneNet2024: LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
SugarcaneNet2024: An Optimized Weighted Average Ensemble Approach of LASSO Regularized Pre-trained Models for Sugarcane Disease Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18870
&lt;/p&gt;
&lt;p&gt;
SugarcaneNet2024&#26159;&#36890;&#36807;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24555;&#36895;&#20934;&#30830;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29976;&#34071;&#20316;&#20026;&#19990;&#30028;&#31958;&#19994;&#30340;&#20851;&#38190;&#20316;&#29289;&#65292;&#23481;&#26131;&#21463;&#22810;&#31181;&#30149;&#23475;&#20405;&#23475;&#65292;&#36825;&#20123;&#30149;&#23475;&#23545;&#20854;&#20135;&#37327;&#21644;&#36136;&#37327;&#37117;&#26377;&#37325;&#22823;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#26377;&#25928;&#31649;&#29702;&#21644;&#23454;&#26045;&#39044;&#38450;&#25514;&#26045;&#65292;&#24517;&#39035;&#21450;&#26102;&#20934;&#30830;&#22320;&#26816;&#27979;&#30149;&#23475;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SugarcaneNet2024&#30340;&#29420;&#29305;&#27169;&#22411;&#65292;&#36890;&#36807;&#21494;&#29255;&#22270;&#20687;&#22788;&#29702;&#65292;&#33021;&#22815;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#33258;&#21160;&#24555;&#36895;&#26816;&#27979;&#29976;&#34071;&#30149;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#27719;&#24635;&#20102;&#19971;&#20010;&#23450;&#21046;&#30340;&#12289;&#32463;&#36807;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;&#65292;&#29305;&#21035;&#26159;InceptionV3&#12289;InceptionResNetV2&#12289;DenseNet201&#12289;DenseNet169&#12289;Xception&#21644;ResNet152V2&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#22312;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#24213;&#37096;&#28155;&#21152;&#20102;&#19977;&#23618;&#26356;&#23494;&#38598;&#23618;&#65292;&#20855;&#26377;0.0001&#30340;LASSO&#27491;&#21017;&#21270;&#65292;&#19977;&#20010;30%&#30340;dropout&#23618;&#21644;&#19977;&#20010;&#21551;&#29992;renorm&#30340;&#25209;&#37327;&#24402;&#19968;&#21270;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18870v1 Announce Type: cross  Abstract: Sugarcane, a key crop for the world's sugar industry, is prone to several diseases that have a substantial negative influence on both its yield and quality. To effectively manage and implement preventative initiatives, diseases must be detected promptly and accurately. In this study, we present a unique model called sugarcaneNet2024 that outperforms previous methods for automatically and quickly detecting sugarcane disease through leaf image processing. Our proposed model consolidates an optimized weighted average ensemble of seven customized and LASSO-regularized pre-trained models, particularly InceptionV3, InceptionResNetV2, DenseNet201, DenseNet169, Xception, and ResNet152V2. Initially, we added three more dense layers with 0.0001 LASSO regularization, three 30% dropout layers, and three batch normalizations with renorm enabled at the bottom of these pre-trained models to improve the performance. The accuracy of sugarcane leaf dise
&lt;/p&gt;</description></item><item><title>DreamSampler&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;&#21453;&#21521;&#25193;&#25955;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#65292;&#25552;&#20379;&#20102;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20998;&#25968;&#33976;&#39311;&#26131;&#23849;&#28291;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#20687;&#32534;&#36753;&#21644;&#37325;&#26500;&#20013;&#23637;&#29616;&#20102;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11415</link><description>&lt;p&gt;
DreamSampler&#65306;&#32479;&#19968;&#25193;&#25955;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#20197;&#29992;&#20110;&#22270;&#20687;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
DreamSampler: Unifying Diffusion Sampling and Score Distillation for Image Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11415
&lt;/p&gt;
&lt;p&gt;
DreamSampler&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;&#21453;&#21521;&#25193;&#25955;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#65292;&#25552;&#20379;&#20102;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20998;&#25968;&#33976;&#39311;&#26131;&#23849;&#28291;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#20687;&#32534;&#36753;&#21644;&#37325;&#26500;&#20013;&#23637;&#29616;&#20102;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#24050;&#25104;&#20026;&#26368;&#36817;&#20960;&#24180;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDMs&#65289;&#36827;&#34892;&#22270;&#20687;&#22788;&#29702;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#21453;&#21521;&#25193;&#25955;&#37319;&#26679;&#36890;&#24120;&#38656;&#35201;&#35843;&#25972;LDM&#26550;&#26500;&#25110;&#29305;&#24449;&#24037;&#31243;&#65292;&#20998;&#25968;&#33976;&#39311;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#20294;&#24448;&#24448;&#23481;&#26131;&#21457;&#29983;&#27169;&#24335;&#23849;&#28291;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#24182;&#21033;&#29992;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;DreamSampler&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#28508;&#22312;&#20248;&#21270;&#30340;&#35270;&#35282;&#26080;&#32541;&#22320;&#25972;&#21512;&#20102;&#36825;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#31867;&#20284;&#20110;&#20998;&#25968;&#33976;&#39311;&#65292;DreamSampler&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;LDM&#26550;&#26500;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#20294;&#23427;&#20801;&#35768;&#22312;&#22270;&#20687;&#32534;&#36753;&#21644;&#37325;&#26500;&#20013;&#36827;&#34892;&#33976;&#39311;&#21644;&#21453;&#21521;&#37319;&#26679;&#65292;&#24182;&#25552;&#20379;&#39069;&#22806;&#30340;&#25351;&#23548;&#12290;&#36890;&#36807;&#28041;&#21450;&#22270;&#20687;&#32534;&#36753;&#12289;SVG&#37325;&#26500;&#31561;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31454;&#20105;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11415v1 Announce Type: cross  Abstract: Reverse sampling and score-distillation have emerged as main workhorses in recent years for image manipulation using latent diffusion models (LDMs). While reverse diffusion sampling often requires adjustments of LDM architecture or feature engineering, score distillation offers a simple yet powerful model-agnostic approach, but it is often prone to mode-collapsing. To address these limitations and leverage the strengths of both approaches, here we introduce a novel framework called {\em DreamSampler}, which seamlessly integrates these two distinct approaches through the lens of regularized latent optimization. Similar to score-distillation, DreamSampler is a model-agnostic approach applicable to any LDM architecture, but it allows both distillation and reverse sampling with additional guidance for image editing and reconstruction. Through experiments involving image editing, SVG reconstruction and etc, we demonstrate the competitive pe
&lt;/p&gt;</description></item><item><title>m&amp;m's&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;4K+&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#28041;&#21450;33&#31181;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#20316;&#20026;&#35268;&#21010;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2403.11085</link><description>&lt;p&gt;
m&amp;m's: &#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#24037;&#20855;&#20351;&#29992;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
m&amp;m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11085
&lt;/p&gt;
&lt;p&gt;
m&amp;m's&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;4K+&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#28041;&#21450;33&#31181;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#20316;&#20026;&#35268;&#21010;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#24456;&#23569;&#30001;&#21333;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#20915;&#65292;&#36890;&#24120;&#38656;&#35201;&#22810;&#27493;&#39588;&#35745;&#31639;&#35745;&#21010;&#65292;&#28041;&#21450;&#25340;&#25509;&#22810;&#20010;&#27169;&#22411;&#12290; &#24037;&#20855;&#22686;&#24378;&#22411;LLM&#26497;&#26377;&#21487;&#33021;&#33258;&#21160;&#21270;&#29983;&#25104;&#36825;&#31181;&#35745;&#31639;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#29992;&#20110;&#35780;&#20272;LLM&#20316;&#20026;&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#35268;&#21010;&#22120;&#30340;&#26631;&#20934;&#21270;&#22522;&#20934;&#65292;&#38459;&#30861;&#20102;&#23545;&#35268;&#21010;&#22120;&#35774;&#35745;&#20915;&#31574;&#30340;&#31995;&#32479;&#30740;&#31350;&#12290;LLM&#26159;&#21542;&#24212;&#19968;&#27425;&#24615;&#29983;&#25104;&#25972;&#20010;&#35745;&#21010;&#36824;&#26159;&#36880;&#27493;&#29983;&#25104;&#65311;&#23427;&#20204;&#26159;&#21542;&#24212;&#35813;&#30452;&#25509;&#20351;&#29992;Python&#20195;&#30721;&#35843;&#29992;&#24037;&#20855;&#65292;&#36824;&#26159;&#36890;&#36807;&#31867;&#20284;JSON&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#26684;&#24335;&#65311;&#21453;&#39304;&#26159;&#21542;&#25913;&#21892;&#35268;&#21010;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#20197;&#21450;&#26356;&#22810;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;m&amp;m's&#65306;&#19968;&#20010;&#22522;&#20934;&#65292;&#21253;&#21547;4K+&#20010;&#28041;&#21450;33&#31181;&#24037;&#20855;&#30340;&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#20854;&#20013;&#21253;&#25324;&#22810;&#27169;&#24577;&#27169;&#22411;&#12289;(&#20813;&#36153;)&#20844;&#20849;API&#21644;&#22270;&#20687;&#22788;&#29702;&#27169;&#22359;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#26597;&#35810;&#65292;&#25105;&#20204;&#25552;&#20379;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#33258;&#21160;&#29983;&#25104;&#30340;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11085v1 Announce Type: cross  Abstract: Real-world multi-modal problems are rarely solved by a single machine learning model, and often require multi-step computational plans that involve stitching several models. Tool-augmented LLMs hold tremendous promise for automating the generation of such computational plans. However, the lack of standardized benchmarks for evaluating LLMs as planners for multi-step multi-modal tasks has prevented a systematic study of planner design decisions. Should LLMs generate a full plan in a single shot or step-by-step? Should they invoke tools directly with Python code or through structured data formats like JSON? Does feedback improve planning? To answer these questions and more, we introduce m&amp;m's: a benchmark containing 4K+ multi-step multi-modal tasks involving 33 tools that include multi-modal models, (free) public APIs, and image processing modules. For each of these task queries, we provide automatically generated plans using this realis
&lt;/p&gt;</description></item><item><title>P2LHAP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Patch-to-Label Seq2Seq&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#19968;&#20219;&#21153;&#27169;&#22411;&#20013;&#21516;&#26102;&#23454;&#29616;&#20154;&#31867;&#27963;&#21160;&#30340;&#20998;&#21106;&#12289;&#35782;&#21035;&#21644;&#39044;&#27979;</title><link>https://arxiv.org/abs/2403.08214</link><description>&lt;p&gt;
P2LHAP&#65306;&#22522;&#20110;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#12289;&#20998;&#21106;&#21644;&#39044;&#27979;&#30340;Patch-to-Label Seq2Seq Transformer
&lt;/p&gt;
&lt;p&gt;
P2LHAP:Wearable sensor-based human activity recognition, segmentation and forecast through Patch-to-Label Seq2Seq Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08214
&lt;/p&gt;
&lt;p&gt;
P2LHAP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Patch-to-Label Seq2Seq&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#19968;&#20219;&#21153;&#27169;&#22411;&#20013;&#21516;&#26102;&#23454;&#29616;&#20154;&#31867;&#27963;&#21160;&#30340;&#20998;&#21106;&#12289;&#35782;&#21035;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24456;&#38590;&#21516;&#26102;&#20174;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#20998;&#21106;&#12289;&#35782;&#21035;&#21644;&#39044;&#27979;&#20154;&#31867;&#27963;&#21160;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#36741;&#21161;&#29983;&#27963;&#31561;&#39046;&#22495;&#30340;&#23454;&#29992;&#24615;&#65292;&#32780;&#36825;&#20123;&#39046;&#22495;&#23545;&#20110;&#23454;&#26102;&#29702;&#35299;&#27491;&#22312;&#36827;&#34892;&#21644;&#21363;&#23558;&#21457;&#29983;&#30340;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;P2LHAP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;Patch-to-Label Seq2Seq&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#19968;&#20219;&#21153;&#27169;&#22411;&#20013;&#35299;&#20915;&#36825;&#19977;&#20010;&#20219;&#21153;&#12290;P2LHAP&#23558;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#21010;&#20998;&#20026;&#19968;&#31995;&#21015;&#8220;&#34917;&#19969;&#8221;&#65292;&#20316;&#20026;&#36755;&#20837;&#26631;&#35760;&#65292;&#24182;&#36755;&#20986;&#19968;&#31995;&#21015;&#21253;&#25324;&#39044;&#27979;&#30340;&#26410;&#26469;&#27963;&#21160;&#22312;&#20869;&#30340;&#34917;&#19969;&#32423;&#27963;&#21160;&#26631;&#31614;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21608;&#22260;&#34917;&#19969;&#26631;&#31614;&#30340;&#29420;&#29305;&#24179;&#28369;&#25216;&#26415;&#65292;&#21487;&#20934;&#30830;&#35782;&#21035;&#27963;&#21160;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;P2LHAP&#36890;&#36807;&#20256;&#24863;&#22120;&#20449;&#21495;&#36890;&#36947;&#29420;&#31435;&#30340;Transformer&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23398;&#20064;&#34917;&#19969;&#32423;&#34920;&#31034;&#12290;&#25152;&#26377;&#36890;&#36947;&#22312;&#25152;&#26377;&#24207;&#21015;&#19978;&#20849;&#20139;&#23884;&#20837;&#21644;Transformer&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08214v1 Announce Type: cross  Abstract: Traditional deep learning methods struggle to simultaneously segment, recognize, and forecast human activities from sensor data. This limits their usefulness in many fields such as healthcare and assisted living, where real-time understanding of ongoing and upcoming activities is crucial. This paper introduces P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles all three tasks in a efficient single-task model. P2LHAP divides sensor data streams into a sequence of "patches", served as input tokens, and outputs a sequence of patch-level activity labels including the predicted future activities. A unique smoothing technique based on surrounding patch labels, is proposed to identify activity boundaries accurately. Additionally, P2LHAP learns patch-level representation by sensor signal channel-independent Transformer encoders and decoders. All channels share embedding and Transformer weights across all sequences. Evaluated on thre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25104;&#21151;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#33258;&#22238;&#24402;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#35789;&#30340;&#27010;&#24565;&#65292;&#23558;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#21040;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#19978;&#65292;&#20026;&#35270;&#35273;&#24314;&#27169;&#25552;&#20379;&#20102;&#30417;&#30563;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.07720</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#35789;&#36827;&#34892;&#22810;&#27169;&#24577;&#33258;&#22238;&#24402;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Auto-regressive Modeling via Visual Words
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25104;&#21151;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#33258;&#22238;&#24402;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#35789;&#30340;&#27010;&#24565;&#65292;&#23558;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#21040;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#19978;&#65292;&#20026;&#35270;&#35273;&#24314;&#27169;&#25552;&#20379;&#20102;&#30417;&#30563;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21463;&#30410;&#20110;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#33258;&#22238;&#24402;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#33258;&#22238;&#24402;&#24314;&#27169;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#22330;&#26223;&#20197;&#26500;&#24314;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#26102;&#65292;&#23384;&#22312;&#19968;&#20010;&#24040;&#22823;&#22256;&#38590;&#65292;&#21363;&#22270;&#20687;&#20449;&#24687;&#22312;LMM&#20013;&#20197;&#36830;&#32493;&#30340;&#35270;&#35273;&#23884;&#20837;&#36827;&#34892;&#22788;&#29702;&#65292;&#26080;&#27861;&#33719;&#24471;&#31163;&#25955;&#30340;&#29992;&#20110;&#20998;&#31867;&#30340;&#30417;&#30563;&#26631;&#31614;&#12290;&#26412;&#25991;&#39318;&#27425;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#22810;&#27169;&#24577;&#33258;&#22238;&#24402;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#30446;&#26631;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35270;&#35273;&#35789;&#30340;&#27010;&#24565;&#65292;&#23558;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#21040;LLM&#35789;&#27719;&#34920;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20026;&#35270;&#35273;&#24314;&#27169;&#25552;&#20379;&#20102;&#30417;&#30563;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LMM&#20013;&#35821;&#20041;&#31354;&#38388;&#20869;&#35270;&#35273;&#29305;&#24449;&#30340;&#20998;&#24067;&#20197;&#21450;&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#26469;&#34920;&#31034;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07720v1 Announce Type: cross  Abstract: Large Language Models (LLMs), benefiting from the auto-regressive modelling approach performed on massive unannotated texts corpora, demonstrates powerful perceptual and reasoning capabilities. However, as for extending auto-regressive modelling to multi-modal scenarios to build Large Multi-modal Models (LMMs), there lies a great difficulty that the image information is processed in the LMM as continuous visual embeddings, which cannot obtain discrete supervised labels for classification. In this paper, we successfully perform multi-modal auto-regressive modeling with a unified objective for the first time. Specifically, we propose the concept of visual words, which maps the visual features to probability distributions over LLM's vocabulary, providing supervision information for visual modelling. We further explore the distribution of visual features in the semantic space within LMM and the possibility of using text embeddings to repre
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;LLMs&#22312;&#37319;&#29992;&#20018;&#32852;&#25512;&#29702;&#26102;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20018;&#32852;&#25512;&#29702;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#25104;&#21151;&#35268;&#36991;&#20102;&#36825;&#31181;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.14899</link><description>&lt;p&gt;
&#20572;&#27490;&#25512;&#29702;&#65281;&#24403;&#22810;&#27169;&#24577;LLMs&#19982;&#20018;&#32852;&#25512;&#29702;&#36935;&#21040;&#23545;&#25239;&#24615;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14899
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;LLMs&#22312;&#37319;&#29992;&#20018;&#32852;&#25512;&#29702;&#26102;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20018;&#32852;&#25512;&#29702;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#25104;&#21151;&#35268;&#36991;&#20102;&#36825;&#31181;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;LLMs&#65288;MLLMs&#65289;&#23637;&#31034;&#20102;&#24456;&#24378;&#30340;&#29702;&#35299;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20687;&#20256;&#32479;&#35270;&#35273;&#27169;&#22411;&#19968;&#26679;&#65292;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#22270;&#20687;&#30340;&#25915;&#20987;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20018;&#32852;&#25512;&#29702;&#65288;CoT&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#22312;MLLMs&#19978;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#36890;&#36807;&#25552;&#20379;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#20851;&#20110;MLLMs&#22312;CoT&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#65292;&#20197;&#21450;&#22312;MLLMs&#29992;&#23545;&#25239;&#24615;&#22270;&#20687;&#25512;&#26029;&#38169;&#35823;&#31572;&#26696;&#26102;&#25512;&#29702;&#30340;&#21512;&#29702;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#37319;&#29992;CoT&#25512;&#29702;&#26102;MLLMs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;CoT&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#25269;&#25239;&#20102;&#24050;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;CoT&#24341;&#36215;&#30340;&#40065;&#26834;&#24615;&#22686;&#24378;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CoT&#25512;&#29702;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14899v1 Announce Type: cross  Abstract: Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand images. However, like traditional vision models, they are still vulnerable to adversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely explored on MLLMs, which not only improves model's performance, but also enhances model's explainability by giving intermediate reasoning steps. Nevertheless, there is still a lack of study regarding MLLMs' adversarial robustness with CoT and an understanding of what the rationale looks like when MLLMs infer wrong answers with adversarial images. Our research evaluates the adversarial robustness of MLLMs when employing CoT reasoning, finding that CoT marginally improves adversarial robustness against existing attack methods. Moreover, we introduce a novel stop-reasoning attack technique that effectively bypasses the CoT-induced robustness enhancements. Finally, we demonstrate the alterations in CoT reasonin
&lt;/p&gt;</description></item><item><title>EFUF&#26159;&#19968;&#31181;&#39640;&#25928;&#31934;&#32454;&#21270;&#21435;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#28040;&#38500;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#65292;&#24182;&#19981;&#38656;&#35201;&#20154;&#24037;&#27880;&#37322;&#37197;&#23545;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.09801</link><description>&lt;p&gt;
EFUF: &#39640;&#25928;&#31934;&#32454;&#21270;&#21435;&#23398;&#20064;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#20943;&#36731;&#24187;&#20687;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09801
&lt;/p&gt;
&lt;p&gt;
EFUF&#26159;&#19968;&#31181;&#39640;&#25928;&#31934;&#32454;&#21270;&#21435;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#28040;&#38500;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;&#65292;&#24182;&#19981;&#38656;&#35201;&#20154;&#24037;&#27880;&#37322;&#37197;&#23545;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20173;&#20250;&#29983;&#25104;&#21253;&#21547;&#22270;&#20687;&#20013;&#19981;&#23384;&#22312;&#30340;&#29289;&#20307;&#30340;&#25551;&#36848;&#65292;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#29289;&#20307;&#24187;&#35273;&#12290;&#20026;&#20102;&#28040;&#38500;&#24187;&#35273;&#65292;&#29616;&#26377;&#26041;&#27861;&#25163;&#21160;&#27880;&#37322;&#21253;&#21547;&#21644;&#19981;&#21253;&#21547;&#24187;&#35273;&#30340;&#37197;&#23545;&#21709;&#24212;&#65292;&#24182;&#37319;&#29992;&#21508;&#31181;&#23545;&#40784;&#31639;&#27861;&#26469;&#25552;&#39640;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#23545;&#40784;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#20165;&#22312;&#24494;&#35843;&#38454;&#27573;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#36824;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#26469;&#26500;&#24314;&#23545;&#40784;&#31639;&#27861;&#25152;&#38656;&#30340;&#37197;&#23545;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#21435;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31934;&#32454;&#21270;&#21435;&#23398;&#20064;&#26694;&#26550;&#65288;EFUF&#65289;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#37197;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#28040;&#38500;&#24187;&#35273;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#20943;&#23569;&#24187;&#35273;&#21516;&#26102;&#20445;&#30041;&#20934;&#30830;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09801v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we borrow the idea of unlearning and propose an efficient fine-grained unlearning framework (EFUF), which can eliminate hallucinations without the need for paired data. Extensive experiments show that our method consistently reduces hallucinations while preserv
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.06165</link><description>&lt;p&gt;
&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#34920;&#31034;&#26469;&#36827;&#34892;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Contrastive Feature Representations for Facial Action Unit Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06165
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#65288;AU&#65289;&#26816;&#27979;&#30340;&#20027;&#35201;&#26041;&#27861;&#28041;&#21450;&#30417;&#30563;&#30340;&#22810;&#26631;&#31614;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24120;&#24120;&#23545;AU&#30340;&#20687;&#32032;&#32423;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#25552;&#20986;&#20102;&#24456;&#22823;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#22122;&#22768;AU&#26631;&#31614;&#65292;&#36825;&#31181;&#20570;&#27861;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#22686;&#24378;&#12290;&#30446;&#26631;&#26159;&#22312;AU&#26816;&#27979;&#39046;&#22495;&#20013;&#25670;&#33073;&#20256;&#32479;&#30340;&#20687;&#32032;&#32423;&#23398;&#20064;&#33539;&#24335;&#65292;&#33719;&#24471;&#21028;&#21035;&#29305;&#24449;&#12290;&#20026;&#20102;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#30417;&#30563;&#20449;&#21495;&#12290;&#36825;&#31181;&#22686;&#24378;&#26159;&#36890;&#36807;&#27491;&#26679;&#26412;&#25277;&#26679;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#27491;&#26679;&#26412;&#23545;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#27599;&#20010;AU&#31867;&#22411;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we empl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#29992;&#20110;&#22312;&#39640;&#28023;&#25300;&#29615;&#22659;&#20013;&#23545;&#38050;&#19997;&#32499;&#36827;&#34892;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#20934;&#30830;&#25552;&#21462;&#38050;&#19997;&#32499;&#30340;&#20998;&#21106;&#27169;&#22411;&#21644;&#19968;&#31181;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#38050;&#19997;&#32499;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#26174;&#33879;&#39640;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03843</link><description>&lt;p&gt;
&#20809;&#23398;&#38050;&#19997;&#32499;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A new method for optical steel rope non-destructive damage detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#29992;&#20110;&#22312;&#39640;&#28023;&#25300;&#29615;&#22659;&#20013;&#23545;&#38050;&#19997;&#32499;&#36827;&#34892;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#20934;&#30830;&#25552;&#21462;&#38050;&#19997;&#32499;&#30340;&#20998;&#21106;&#27169;&#22411;&#21644;&#19968;&#31181;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#38050;&#19997;&#32499;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#26174;&#33879;&#39640;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#28023;&#25300;&#29615;&#22659;&#65288;&#31354;&#20013;&#21514;&#32034;&#36947;&#65289;&#20013;&#30340;&#38050;&#19997;&#32499;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#30340;&#26032;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#39318;&#20808;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;RGBD-UNet&#30340;&#20998;&#21106;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#20174;&#22797;&#26434;&#32972;&#26223;&#20013;&#25552;&#21462;&#38050;&#19997;&#32499;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25552;&#20986;&#30340;CMA&#27169;&#22359;&#21487;&#20197;&#22788;&#29702;&#21644;&#32467;&#21512;&#39068;&#33394;&#21644;&#28145;&#24230;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;VovNetV3.5&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#30340;&#38050;&#19997;&#32499;&#12290;&#23427;&#23558;VovNet&#26550;&#26500;&#19982;DBB&#27169;&#22359;&#32467;&#21512;&#36215;&#26469;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32972;&#26223;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20998;&#21106;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21019;&#24314;&#20102;&#21253;&#21547;&#19981;&#21516;&#22330;&#26223;&#20013;&#38050;&#19997;&#32499;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20998;&#21106;&#21644;&#26816;&#27979;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22522;&#20934;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#22312;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#20110;&#27492;&#31639;&#27861;&#30340;&#20256;&#24863;&#22120;&#35782;&#21035;&#24615;&#33021;&#65288;h&#65289;&#26126;&#26174;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel algorithm for non-destructive damage detection for steel ropes in high-altitude environments (aerial ropeway). The algorithm comprises two key components: First, a segmentation model named RGBD-UNet is designed to accurately extract steel ropes from complex backgrounds. This model is equipped with the capability to process and combine color and depth information through the proposed CMA module. Second, a detection model named VovNetV3.5 is developed to differentiate between normal and abnormal steel ropes. It integrates the VovNet architecture with a DBB module to enhance performance. Besides, a novel background augmentation method is proposed to enhance the generalization ability of the segmentation model. Datasets containing images of steel ropes in different scenarios are created for the training and testing of both the segmentation and detection models. Experiments demonstrate a significant improvement over baseline models. On the proposed dataset, the h
&lt;/p&gt;</description></item><item><title>SiT&#26159;&#19968;&#31181;&#22522;&#20110;Diffusion Transformers&#39592;&#24178;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25554;&#20540;&#26694;&#26550;&#21644;&#21508;&#31181;&#35774;&#35745;&#36873;&#25321;&#30340;&#27169;&#22359;&#21270;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#22312;&#27169;&#22411;&#22823;&#23567;&#19978;&#36229;&#36807;DiT&#65292;&#22312;&#26465;&#20214;ImageNet&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#36739;&#20302;&#30340;FID-50K&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2401.08740</link><description>&lt;p&gt;
SiT:&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#25554;&#20540;&#20223;&#23556;&#21464;&#25442;&#25506;&#32034;&#22522;&#20110;&#27969;&#21160;&#21644;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers. (arXiv:2401.08740v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08740
&lt;/p&gt;
&lt;p&gt;
SiT&#26159;&#19968;&#31181;&#22522;&#20110;Diffusion Transformers&#39592;&#24178;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25554;&#20540;&#26694;&#26550;&#21644;&#21508;&#31181;&#35774;&#35745;&#36873;&#25321;&#30340;&#27169;&#22359;&#21270;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#22312;&#27169;&#22411;&#22823;&#23567;&#19978;&#36229;&#36807;DiT&#65292;&#22312;&#26465;&#20214;ImageNet&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#36739;&#20302;&#30340;FID-50K&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#21464;&#25442;&#22120;&#65288;DiT&#65289;&#39592;&#24178;&#30340;&#21487;&#25193;&#23637;&#25554;&#20540;&#20223;&#23556;&#21464;&#25442;&#22120;&#65288;SiT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#30340;&#31995;&#21015;&#12290;&#25554;&#20540;&#26694;&#26550;&#20801;&#35768;&#20197;&#27604;&#26631;&#20934;&#25193;&#25955;&#27169;&#22411;&#26356;&#28789;&#27963;&#30340;&#26041;&#24335;&#36830;&#25509;&#20004;&#20010;&#20998;&#24067;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#24314;&#31435;&#22312;&#21160;&#24577;&#20256;&#36755;&#19978;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#21508;&#31181;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#27169;&#22359;&#21270;&#30740;&#31350;&#65306;&#20351;&#29992;&#31163;&#25955;&#26102;&#38388;&#23398;&#20064;&#36824;&#26159;&#36830;&#32493;&#26102;&#38388;&#23398;&#20064;&#65292;&#20915;&#23450;&#27169;&#22411;&#23398;&#20064;&#30340;&#30446;&#26631;&#65292;&#36873;&#25321;&#36830;&#25509;&#20998;&#24067;&#30340;&#25554;&#20540;&#22120;&#65292;&#20197;&#21450;&#37096;&#32626;&#30830;&#23450;&#24615;&#36824;&#26159;&#38543;&#26426;&#37319;&#26679;&#22120;&#12290;&#36890;&#36807;&#31934;&#24515;&#24341;&#20837;&#19978;&#36848;&#20803;&#32032;&#65292;SiT&#22312;&#20855;&#26377;&#30456;&#21516;&#39592;&#24178;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;GFLOPs&#30340;&#26465;&#20214;ImageNet 256x256&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22312;&#27169;&#22411;&#22823;&#23567;&#19978;&#20840;&#38754;&#36229;&#36807;&#20102;DiT&#12290;&#36890;&#36807;&#25506;&#32034;&#21487;&#20197;&#19982;&#23398;&#20064;&#20998;&#24320;&#35843;&#25972;&#30340;&#21508;&#31181;&#25193;&#25955;&#31995;&#25968;&#65292;SiT&#22312;FID-50K&#35780;&#20998;&#19978;&#36798;&#21040;&#20102;2.06&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Scalable Interpolant Transformers (SiT), a family of generative models built on the backbone of Diffusion Transformers (DiT). The interpolant framework, which allows for connecting two distributions in a more flexible way than standard diffusion models, makes possible a modular study of various design choices impacting generative models built on dynamical transport: using discrete vs. continuous time learning, deciding the objective for the model to learn, choosing the interpolant connecting the distributions, and deploying a deterministic or stochastic sampler. By carefully introducing the above ingredients, SiT surpasses DiT uniformly across model sizes on the conditional ImageNet 256x256 benchmark using the exact same backbone, number of parameters, and GFLOPs. By exploring various diffusion coefficients, which can be tuned separately from learning, SiT achieves an FID-50K score of 2.06.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#32479;&#19968;&#26694;&#26550;&#65288;IUF&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#23567;&#32570;&#38519;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#23545;&#35937;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#65288;OASA&#65289;&#21644;&#35821;&#20041;&#21387;&#32553;&#25439;&#22833;&#65288;SCL&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#26029;&#25972;&#21512;&#26032;&#29289;&#20307;&#26102;&#20943;&#23569;&#29305;&#24449;&#20914;&#31361;&#38382;&#39064;&#65292;&#24182;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.08917</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23567;&#32570;&#38519;&#26816;&#27979;&#30340;&#22686;&#37327;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Incremental Unified Framework for Small Defect Inspection. (arXiv:2312.08917v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08917
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#32479;&#19968;&#26694;&#26550;&#65288;IUF&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#23567;&#32570;&#38519;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#23545;&#35937;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#65288;OASA&#65289;&#21644;&#35821;&#20041;&#21387;&#32553;&#25439;&#22833;&#65288;SCL&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#26029;&#25972;&#21512;&#26032;&#29289;&#20307;&#26102;&#20943;&#23569;&#29305;&#24449;&#20914;&#31361;&#38382;&#39064;&#65292;&#24182;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#21046;&#36896;&#20013;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32570;&#38519;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#38024;&#23545;&#29305;&#23450;&#30340;&#27969;&#27700;&#32447;&#65292;&#22312;&#38754;&#23545;&#21508;&#31181;&#20135;&#21697;&#32452;&#21512;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#27969;&#31243;&#26102;&#24448;&#24448;&#38754;&#20020;&#29305;&#24449;&#20914;&#31361;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#37327;&#32479;&#19968;&#26694;&#26550;&#65288;IUF&#65289;&#65292;&#21487;&#20197;&#22312;&#19981;&#26029;&#25972;&#21512;&#26032;&#29289;&#20307;&#26102;&#20943;&#23569;&#29305;&#24449;&#20914;&#31361;&#38382;&#39064;&#65292;&#36825;&#22312;&#38754;&#20020;&#29289;&#20307;&#22686;&#37327;&#23398;&#20064;&#22330;&#26223;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;transformer&#24341;&#20837;&#20102;&#23545;&#35937;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#65288;OASA&#65289;&#26469;&#21010;&#20998;&#26126;&#30830;&#30340;&#35821;&#20041;&#36793;&#30028;&#12290;&#38598;&#25104;&#20102;&#35821;&#20041;&#21387;&#32553;&#25439;&#22833;&#65288;SCL&#65289;&#26469;&#20248;&#21270;&#38750;&#20027;&#35201;&#35821;&#20041;&#31354;&#38388;&#65292;&#22686;&#24378;&#20102;&#32593;&#32476;&#23545;&#26032;&#29289;&#20307;&#30340;&#36866;&#24212;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#26435;&#37325;&#26356;&#26032;&#26102;&#65292;&#25105;&#20204;&#20248;&#20808;&#20445;&#30041;&#24050;&#24314;&#31435;&#29289;&#20307;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#22312;&#22270;&#20687;&#21644;&#20687;&#32032;&#32423;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#23637;&#31034;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23545;&#20110;&#21160;&#24577;&#21644;&#21487;&#25193;&#23637;&#30340;&#24037;&#19994;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI)-driven defect inspection is pivotal in industrial manufacturing. Yet, many methods, tailored to specific pipelines, grapple with diverse product portfolios and evolving processes. Addressing this, we present the Incremental Unified Framework (IUF), which can reduce the feature conflict problem when continuously integrating new objects in the pipeline, making it advantageous in object-incremental learning scenarios. Employing a state-of-the-art transformer, we introduce Object-Aware Self-Attention (OASA) to delineate distinct semantic boundaries. Semantic Compression Loss (SCL) is integrated to optimize non-primary semantic space, enhancing network adaptability for novel objects. Additionally, we prioritize retaining the features of established objects during weight updates. Demonstrating prowess in both image and pixel-level defect inspection, our approach achieves state-of-the-art performance, proving indispensable for dynamic and scalable industrial inspe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HePCo&#30340;&#36731;&#37327;&#32423;&#25552;&#31034;&#21512;&#24182;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#21644;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#20849;&#20139;&#25110;&#23384;&#20648;&#20219;&#20309;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#20102;&#36890;&#20449;&#24320;&#38144;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2306.09970</link><description>&lt;p&gt;
HePCo&#65306;&#29992;&#20110;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#30340;&#26080;&#25968;&#25454;&#24322;&#26500;&#25552;&#31034;&#21512;&#24182;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HePCo: Data-Free Heterogeneous Prompt Consolidation for Continual Federated Learning. (arXiv:2306.09970v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HePCo&#30340;&#36731;&#37327;&#32423;&#25552;&#31034;&#21512;&#24182;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#21644;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#20849;&#20139;&#25110;&#23384;&#20648;&#20219;&#20309;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#20102;&#36890;&#20449;&#24320;&#38144;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#30340;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26381;&#21153;&#22120;&#19982;&#19968;&#32452;&#23458;&#25143;&#31471;&#36890;&#20449;&#65292;&#20197;&#36880;&#27493;&#23398;&#20064;&#26032;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#19981;&#20849;&#20139;&#25110;&#23384;&#20648;&#20219;&#20309;&#25968;&#25454;&#12290;&#30001;&#20110;&#26469;&#33258;&#36830;&#32493;&#21644;&#32852;&#37030;&#23398;&#20064;&#35282;&#24230;&#30340;&#25361;&#25112;&#65292;&#27492;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21463;&#21040;&#20102;&#21152;&#21095;&#12290;&#26412;&#25991;&#23581;&#35797;&#22312;&#19981;&#38656;&#35201;&#35775;&#38382;&#20219;&#20309;&#23384;&#20648;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36951;&#24536;&#21644;&#24322;&#26500;&#38382;&#39064;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#24320;&#38144;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#24182;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;HePCo&#30340;&#26032;&#39062;&#36731;&#37327;&#32423;&#25552;&#31034;&#21512;&#24182;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#27492;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#22343;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#24182;&#20445;&#25345;&#20302;&#36890;&#20449;&#24320;&#38144;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on the important yet understudied problem of Continual Federated Learning (CFL), where a server communicates with a set of clients to incrementally learn new concepts over time without sharing or storing any data. The complexity of this problem is compounded by challenges from both the Continual and Federated Learning perspectives. Specifically, models trained in a CFL setup suffer from catastrophic forgetting which is exacerbated by data heterogeneity across clients. Existing attempts at this problem tend to impose large overheads on clients and communication channels or require access to stored data which renders them unsuitable for real-world use due to privacy. In this paper, we attempt to tackle forgetting and heterogeneity while minimizing overhead costs and without requiring access to any stored data. We achieve this by leveraging a prompting based approach (such that only prompts and classifier heads have to be communicated) and proposing a novel and lig
&lt;/p&gt;</description></item><item><title>OpenOOD v1.5 &#26159;&#23545;&#21069;&#36523;&#30340;&#37325;&#22823;&#25913;&#36827;&#65292;&#23558;OCC&#26816;&#27979;&#26041;&#27861;&#30340;&#35780;&#20272;&#33021;&#21147;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#35843;&#26597;&#20102;&#20840;&#20809;&#35889;OCC&#26816;&#27979;&#65292;&#24341;&#20837;&#20102;&#22312;&#32447;&#25490;&#34892;&#27036;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#35780;&#20272;&#22120;&#31561;&#26032;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.09301</link><description>&lt;p&gt;
OpenOOD v1.5&#65306;&#22686;&#24378;&#30340;OCC&#65288;Out-of-Distribution Detection&#65289;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection. (arXiv:2306.09301v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09301
&lt;/p&gt;
&lt;p&gt;
OpenOOD v1.5 &#26159;&#23545;&#21069;&#36523;&#30340;&#37325;&#22823;&#25913;&#36827;&#65292;&#23558;OCC&#26816;&#27979;&#26041;&#27861;&#30340;&#35780;&#20272;&#33021;&#21147;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#35843;&#26597;&#20102;&#20840;&#20809;&#35889;OCC&#26816;&#27979;&#65292;&#24341;&#20837;&#20102;&#22312;&#32447;&#25490;&#34892;&#27036;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#35780;&#20272;&#22120;&#31561;&#26032;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OCC&#26816;&#27979;&#23545;&#20110;&#24320;&#25918;&#19990;&#30028;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#38752;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;OCC&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#35780;&#20272;&#19981;&#19968;&#33268;&#24615;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#38590;&#20197;&#36319;&#36394;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;OpenOOD v1.5&#65292;&#36825;&#26159;&#23545;&#21069;&#36523;&#30340;&#37325;&#22823;&#25913;&#36827;&#65292;&#30830;&#20445;OCC&#26816;&#27979;&#26041;&#27861;&#30340;&#20934;&#30830;&#12289;&#26631;&#20934;&#21270;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#35780;&#20272;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;OpenOOD v1.5&#23558;&#20854;&#35780;&#20272;&#33021;&#21147;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#22914;ImageNet&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#35843;&#26597;&#20102;&#20840;&#20809;&#35889;OCC&#26816;&#27979;&#65292;&#24341;&#20837;&#20102;&#22312;&#32447;&#25490;&#34892;&#27036;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#35780;&#20272;&#22120;&#31561;&#26032;&#21151;&#33021;&#12290;&#35813;&#24037;&#20316;&#36824;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#32508;&#21512;&#23454;&#39564;&#32467;&#26524;&#30340;&#35265;&#35299;&#65292;&#20174;&#32780;&#20016;&#23500;&#20102;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution (OOD) detection is critical for the reliable operation of open-world intelligent systems. Despite the emergence of an increasing number of OOD detection methods, the evaluation inconsistencies present challenges for tracking the progress in this field. OpenOOD v1 initiated the unification of the OOD detection evaluation but faced limitations in scalability and usability. In response, this paper presents OpenOOD v1.5, a significant improvement from its predecessor that ensures accurate, standardized, and user-friendly evaluation of OOD detection methodologies. Notably, OpenOOD v1.5 extends its evaluation capabilities to large-scale datasets such as ImageNet, investigates full-spectrum OOD detection which is important yet underexplored, and introduces new features including an online leaderboard and an easy-to-use evaluator. This work also contributes in-depth analysis and insights derived from comprehensive experimental results, thereby enriching the knowledge pool o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;BraTS 2023&#20462;&#22797;&#25361;&#25112;&#65292;&#35201;&#27714;&#21442;&#19982;&#32773;&#20351;&#29992;&#20462;&#34917;&#25216;&#26415;&#20174;&#26377;&#30149;&#21464;&#30340;&#33041;&#37096;&#25195;&#25551;&#20013;&#21512;&#25104;&#20581;&#24247;&#33041;&#25195;&#25551;&#65292;&#20197;&#35299;&#20915;&#35768;&#22810;&#31639;&#27861;&#26080;&#27861;&#20998;&#26512;&#30149;&#21464;&#22270;&#20687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08992</link><description>&lt;p&gt;
&#33041;&#32959;&#30244;&#20998;&#21106;&#65288;BraTS&#65289;&#25361;&#25112;&#36187;2023&#65306;&#36890;&#36807;&#20462;&#22797;&#29983;&#25104;&#20581;&#24247;&#33041;&#32452;&#32455;&#30340;&#23616;&#37096;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Brain Tumor Segmentation (BraTS) Challenge 2023: Local Synthesis of Healthy Brain Tissue via Inpainting. (arXiv:2305.08992v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08992
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;BraTS 2023&#20462;&#22797;&#25361;&#25112;&#65292;&#35201;&#27714;&#21442;&#19982;&#32773;&#20351;&#29992;&#20462;&#34917;&#25216;&#26415;&#20174;&#26377;&#30149;&#21464;&#30340;&#33041;&#37096;&#25195;&#25551;&#20013;&#21512;&#25104;&#20581;&#24247;&#33041;&#25195;&#25551;&#65292;&#20197;&#35299;&#20915;&#35768;&#22810;&#31639;&#27861;&#26080;&#27861;&#20998;&#26512;&#30149;&#21464;&#22270;&#20687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25903;&#25345;&#20020;&#24202;&#21307;&#29983;&#30340;&#20915;&#31574;&#65292;&#25552;&#20379;&#20102;&#35768;&#22810;&#33258;&#21160;&#20998;&#26512;&#33041;&#37096;MR&#22270;&#20687;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;&#33041;&#32959;&#30244;&#24739;&#32773;&#65292;&#22270;&#20687;&#37319;&#38598;&#26102;&#38388;&#24207;&#21015;&#36890;&#24120;&#22987;&#20110;&#24050;&#32463;&#30149;&#29702;&#24615;&#30340;&#25195;&#25551;&#12290;&#36825;&#20250;&#24102;&#26469;&#38382;&#39064;&#65292;&#22240;&#20026;&#35768;&#22810;&#31639;&#27861;&#26159;&#35774;&#35745;&#29992;&#20110;&#20998;&#26512;&#20581;&#24247;&#30340;&#22823;&#33041;&#22270;&#20687;&#65292;&#24182;&#19988;&#27809;&#26377;&#20026;&#21253;&#21547;&#30149;&#21464;&#30340;&#22270;&#20687;&#25552;&#20379;&#20445;&#35777;&#12290;&#20363;&#22914;&#65292;&#36827;&#34892;&#33041;&#37096;&#35299;&#21078;&#20998;&#21106;&#12289;&#32452;&#32455;&#20998;&#21106;&#21644;&#33041;&#37096;&#25552;&#21462;&#30340;&#31639;&#27861;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BraTS 2023&#20462;&#22797;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#21442;&#19982;&#32773;&#38656;&#35201;&#25506;&#32034;&#20462;&#22797;&#25216;&#26415;&#65292;&#20174;&#26377;&#30149;&#21464;&#30340;&#25195;&#25551;&#20013;&#21512;&#25104;&#20581;&#24247;&#30340;&#33041;&#37096;&#25195;&#25551;&#12290;&#19979;&#38754;&#30340;&#25163;&#31295;&#21253;&#21547;&#20102;&#20219;&#21153;&#20844;&#24335;&#12289;&#25968;&#25454;&#38598;&#21644;&#25552;&#20132;&#31243;&#24207;&#12290;&#20043;&#21518;&#20250;&#26356;&#26032;&#20197;&#24635;&#32467;&#25361;&#25112;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#25361;&#25112;&#26159;&#20316;&#20026;BraTS 2023&#25361;&#25112;&#30340;&#19968;&#37096;&#20998;&#65292;&#30001;&#21152;&#25343;&#22823;&#28201;&#21733;&#21326;MICCAI 2023&#20250;&#35758;&#20027;&#21150;&#12290;
&lt;/p&gt;
&lt;p&gt;
A myriad of algorithms for the automatic analysis of brain MR images is available to support clinicians in their decision-making. For brain tumor patients, the image acquisition time series typically starts with a scan that is already pathological. This poses problems, as many algorithms are designed to analyze healthy brains and provide no guarantees for images featuring lesions. Examples include but are not limited to algorithms for brain anatomy parcellation, tissue segmentation, and brain extraction. To solve this dilemma, we introduce the BraTS 2023 inpainting challenge. Here, the participants' task is to explore inpainting techniques to synthesize healthy brain scans from lesioned ones. The following manuscript contains the task formulation, dataset, and submission procedure. Later it will be updated to summarize the findings of the challenge. The challenge is organized as part of the BraTS 2023 challenge hosted at the MICCAI 2023 conference in Vancouver, Canada.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#37326;&#22806;&#35270;&#39057;&#20013;&#33258;&#21160;&#25552;&#21462;&#19977;&#32500;&#30417;&#30563;&#26469;&#25193;&#23637;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#25163;&#37096;&#23039;&#21183;&#20316;&#20026;&#29289;&#20307;&#23039;&#21183;&#30340;&#20195;&#29702;&#21644;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#31561;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26410;&#30693;&#30456;&#26426;&#23039;&#21183;&#21644;&#36974;&#25377;&#31561;&#38382;&#39064;&#65292;&#20174;&#32780;&#36890;&#36807;&#20174;&#21333;&#20010;RGB&#22270;&#20687;&#39044;&#27979;&#29289;&#20307;&#19977;&#32500;&#24418;&#29366;&#30340;&#21344;&#25454;&#32593;&#32476;&#24471;&#21040;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03036</link><description>&lt;p&gt;
&#20174;&#37326;&#22806;&#35270;&#39057;&#20013;&#23398;&#20064;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Learning Hand-Held Object Reconstruction from In-The-Wild Videos. (arXiv:2305.03036v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#37326;&#22806;&#35270;&#39057;&#20013;&#33258;&#21160;&#25552;&#21462;&#19977;&#32500;&#30417;&#30563;&#26469;&#25193;&#23637;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#25163;&#37096;&#23039;&#21183;&#20316;&#20026;&#29289;&#20307;&#23039;&#21183;&#30340;&#20195;&#29702;&#21644;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#31561;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26410;&#30693;&#30456;&#26426;&#23039;&#21183;&#21644;&#36974;&#25377;&#31561;&#38382;&#39064;&#65292;&#20174;&#32780;&#36890;&#36807;&#20174;&#21333;&#20010;RGB&#22270;&#20687;&#39044;&#27979;&#29289;&#20307;&#19977;&#32500;&#24418;&#29366;&#30340;&#21344;&#25454;&#32593;&#32476;&#24471;&#21040;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#21333;&#24433;&#20687;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#26041;&#27861;&#20381;&#36182;&#20110;&#38590;&#20197;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#35268;&#27169;&#21270;&#25910;&#38598;&#30340;&#30452;&#25509;3D&#24418;&#29366;&#30417;&#30563;&#65292;&#22240;&#27492;&#36825;&#20123;&#26041;&#27861;&#22312;&#37326;&#22806;&#29615;&#22659;&#19979;&#38754;&#23545;&#26032;&#39062;&#29289;&#20307;&#26102;&#38590;&#20197;&#25512;&#24191;&#12290;&#26412;&#25991;&#20174;&#29983;&#21160;&#30340;&#37326;&#22806;&#21407;&#22987;&#35270;&#39057;&#25968;&#25454;&#20013;&#33258;&#21160;&#25552;&#21462;&#19977;&#32500;&#30417;&#30563;&#65292;&#24182;&#36890;&#36807;&#22810;&#35270;&#35282;&#20108;&#32500;&#30417;&#30563;&#26469;&#25193;&#23637;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#36825;&#38656;&#35201;&#24212;&#23545;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#26410;&#30693;&#30340;&#30456;&#26426;&#23039;&#21183;&#21644;&#36974;&#25377;&#12290;&#23545;&#20110;&#21069;&#32773;&#65292;&#25105;&#20204;&#20351;&#29992;&#25163;&#37096;&#23039;&#21183;&#20316;&#20026;&#29289;&#20307;&#23039;&#21183;&#30340;&#20195;&#29702;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#20351;&#29992;ObMan&#25968;&#25454;&#38598;&#20013;&#21512;&#25104;&#30340;&#29289;&#20307;&#26469;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#38388;&#25509;&#30340;&#19977;&#32500;&#32447;&#32034;&#26469;&#35757;&#32451;&#21344;&#25454;&#32593;&#32476;&#65292;&#20174;&#21333;&#20010;RGB&#22270;&#20687;&#39044;&#27979;&#29289;&#20307;&#30340;&#19977;&#32500;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior works for reconstructing hand-held objects from a single image rely on direct 3D shape supervision which is challenging to gather in real world at scale. Consequently, these approaches do not generalize well when presented with novel objects in in-the-wild settings. While 3D supervision is a major bottleneck, there is an abundance of in-the-wild raw video data showing hand-object interactions. In this paper, we automatically extract 3D supervision (via multiview 2D supervision) from such raw video data to scale up the learning of models for hand-held object reconstruction. This requires tackling two key challenges: unknown camera pose and occlusion. For the former, we use hand pose (predicted from existing techniques, e.g. FrankMocap) as a proxy for object pose. For the latter, we learn data-driven 3D shape priors using synthetic objects from the ObMan dataset. We use these indirect 3D cues to train occupancy networks that predict the 3D shape of objects from a single RGB image. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphMLP&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#23427;&#23558;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2206.06420</link><description>&lt;p&gt;
GraphMLP&#65306;&#19968;&#31181;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#22270;&#24418;MLP&#24335;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
GraphMLP: A Graph MLP-Like Architecture for 3D Human Pose Estimation. (arXiv:2206.06420v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06420
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphMLP&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#23427;&#23558;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#27809;&#26377;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#30340;&#31454;&#20105;&#24615;&#32467;&#26524;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MLP&#27169;&#22411;&#24182;&#19981;&#25797;&#38271;&#25429;&#25417;&#23616;&#37096;&#32454;&#33410;&#65292;&#20063;&#32570;&#20047;&#26377;&#20851;&#20154;&#20307;&#26500;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#39592;&#39612;&#34920;&#31034;&#23398;&#20064;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#31216;&#20026;GraphMLP&#65292;&#23427;&#32467;&#21512;&#20102;MLP&#21644;&#22270;&#24418;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#20840;&#23616;-&#23616;&#37096;-&#22270;&#24418;&#32479;&#19968;&#26550;&#26500;&#20013;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#12290;GraphMLP&#23558;&#20154;&#20307;&#30340;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#23637;&#31034;&#20102;&#21487;&#20197;&#20197;&#21487;&#24573;&#30053;&#30340;&#35745;&#31639;&#20195;&#20215;&#26469;&#26377;&#25928;&#22320;&#24314;&#27169;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern multi-layer perceptron (MLP) models have shown competitive results in learning visual representations without self-attention. However, existing MLP models are not good at capturing local details and lack prior knowledge of human body configurations, which limits their modeling power for skeletal representation learning. To address these issues, we propose a simple yet effective graph-reinforced MLP-Like architecture, named GraphMLP, that combines MLPs and graph convolutional networks (GCNs) in a global-local-graphical unified architecture for 3D human pose estimation. GraphMLP incorporates the graph structure of human bodies into an MLP model to meet the domain-specific demand of the 3D human pose, while allowing for both local and global spatial interactions. Furthermore, we propose to flexibly and efficiently extend the GraphMLP to the video domain and show that complex temporal dynamics can be effectively modeled in a simple way with negligible computational cost gains in the
&lt;/p&gt;</description></item></channel></rss>