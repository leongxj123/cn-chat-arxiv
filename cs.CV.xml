<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>DeNetDM &#26159;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#19987;&#23478;&#20056;&#31215;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#22312;&#21019;&#24314;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#21518;&#65292;&#23558;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#21435;&#20559;&#35265;&#27169;&#22411;&#65292;&#30456;&#27604;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21462;&#24471;&#26356;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.19863</link><description>&lt;p&gt;
DeNetDM: &#36890;&#36807;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#26469;&#28040;&#38500;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
DeNetDM: Debiasing by Network Depth Modulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19863
&lt;/p&gt;
&lt;p&gt;
DeNetDM &#26159;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#19987;&#23478;&#20056;&#31215;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#22312;&#21019;&#24314;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#21518;&#65292;&#23558;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#21435;&#20559;&#35265;&#27169;&#22411;&#65292;&#30456;&#27604;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21462;&#24471;&#26356;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31070;&#32463;&#32593;&#32476;&#22312;&#20559;&#35265;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#26080;&#24847;&#38388;&#23398;&#20064;&#21040;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#23454;&#29616;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#30446;&#21069;&#35299;&#20915;&#36825;&#31181;&#20559;&#35265;&#30340;&#26041;&#27861;&#36890;&#24120;&#21253;&#25324;&#21033;&#29992;&#20559;&#35265;&#27880;&#37322;&#12289;&#26681;&#25454;&#20266;&#20559;&#35265;&#26631;&#31614;&#36827;&#34892;&#21152;&#26435;&#37325;&#12289;&#25110;&#36890;&#36807;&#22686;&#24378;&#25216;&#26415;&#22686;&#21152;&#20559;&#35265;&#20914;&#31361;&#25968;&#25454;&#28857;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DeNetDM&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35266;&#23519;&#32467;&#26524;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#20248;&#20808;&#23398;&#20064;&#26680;&#24515;&#23646;&#24615;&#65292;&#32780;&#26356;&#28145;&#23618;&#27425;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#33719;&#21462;&#19981;&#21516;&#20449;&#24687;&#26102;&#24378;&#35843;&#20559;&#35265;&#12290;&#25105;&#20204;&#21033;&#29992;&#20174;&#19987;&#23478;&#20056;&#31215;&#20013;&#25512;&#23548;&#20986;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#21019;&#24314;&#20102;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#65292;&#28982;&#21518;&#29992;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#30340;&#21435;&#20559;&#35265;&#27169;&#22411;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19863v1 Announce Type: new  Abstract: When neural networks are trained on biased datasets, they tend to inadvertently learn spurious correlations, leading to challenges in achieving strong generalization and robustness. Current approaches to address such biases typically involve utilizing bias annotations, reweighting based on pseudo-bias labels, or enhancing diversity within bias-conflicting data points through augmentation techniques. We introduce DeNetDM, a novel debiasing method based on the observation that shallow neural networks prioritize learning core attributes, while deeper ones emphasize biases when tasked with acquiring distinct information. Using a training paradigm derived from Product of Experts, we create both biased and debiased branches with deep and shallow architectures and then distill knowledge to produce the target debiased model. Extensive experiments and analyses demonstrate that our approach outperforms current debiasing techniques, achieving a not
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#21512;&#20316;&#24863;&#30693;&#21644;&#36816;&#21160;&#39044;&#27979;&#27169;&#22359;&#20849;&#20139;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17916</link><description>&lt;p&gt;
CMP&#65306;&#20855;&#26377;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#30340;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CMP: Cooperative Motion Prediction with Multi-Agent Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17916
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#21512;&#20316;&#24863;&#30693;&#21644;&#36816;&#21160;&#39044;&#27979;&#27169;&#22359;&#20849;&#20139;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#30340;&#21457;&#23637;&#21644;&#36710;&#32852;&#32593;&#65288;V2X&#65289;&#36890;&#20449;&#30340;&#25104;&#29087;&#65292;&#21512;&#20316;&#36830;&#25509;&#30340;&#33258;&#21160;&#21270;&#36710;&#36742;&#65288;CAVs&#65289;&#30340;&#21151;&#33021;&#21464;&#24471;&#21487;&#33021;&#12290;&#26412;&#25991;&#22522;&#20110;&#21512;&#20316;&#24863;&#30693;&#65292;&#25506;&#35752;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;CMP&#20197;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#22686;&#24378;&#36319;&#36394;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#19982;&#36807;&#21435;&#19987;&#27880;&#20110;&#21512;&#20316;&#24863;&#30693;&#25110;&#36816;&#21160;&#39044;&#27979;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#35299;&#20915;CAVs&#22312;&#24863;&#30693;&#21644;&#39044;&#27979;&#27169;&#22359;&#20013;&#20849;&#20139;&#20449;&#24687;&#30340;&#32479;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#20013;&#36824;&#34701;&#20837;&#20102;&#33021;&#22815;&#23481;&#24525;&#29616;&#23454;V2X&#24102;&#23485;&#38480;&#21046;&#21644;&#20256;&#36755;&#24310;&#36831;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#22788;&#29702;&#24222;&#22823;&#30340;&#24863;&#30693;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#39044;&#27979;&#32858;&#21512;&#27169;&#22359;&#65292;&#32479;&#19968;&#20102;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17916v1 Announce Type: cross  Abstract: The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs). Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR signals as input to enhance tracking and prediction capabilities. Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules. Incorporated into our design is the unique capability to tolerate realistic V2X bandwidth limitations and transmission delays, while dealing with bulky perception representations. We also propose a prediction aggregation module, which unifies the predict
&lt;/p&gt;</description></item><item><title>LSTP&#25552;&#20986;&#20102;&#35821;&#35328;&#24341;&#23548;&#30340;&#26102;&#31354;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#25552;&#31034;&#37319;&#26679;&#22120;&#65288;TPS&#65289;&#21644;&#31354;&#38388;&#25552;&#31034;&#27714;&#35299;&#22120;&#65288;SPS&#65289;&#20197;&#21450;&#19968;&#33268;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#35745;&#31639;&#25928;&#29575;&#12289;&#26102;&#38388;&#29702;&#35299;&#21644;&#31354;&#38388;-&#26102;&#38388;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2402.16050</link><description>&lt;p&gt;
LSTP: &#35821;&#35328;&#24341;&#23548;&#30340;&#26102;&#31354;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#38271;&#31687;&#35270;&#39057;&#25991;&#26412;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16050
&lt;/p&gt;
&lt;p&gt;
LSTP&#25552;&#20986;&#20102;&#35821;&#35328;&#24341;&#23548;&#30340;&#26102;&#31354;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#25552;&#31034;&#37319;&#26679;&#22120;&#65288;TPS&#65289;&#21644;&#31354;&#38388;&#25552;&#31034;&#27714;&#35299;&#22120;&#65288;SPS&#65289;&#20197;&#21450;&#19968;&#33268;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#35745;&#31639;&#25928;&#29575;&#12289;&#26102;&#38388;&#29702;&#35299;&#21644;&#31354;&#38388;-&#26102;&#38388;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35270;&#39057;&#35821;&#35328;&#24314;&#27169;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#22238;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;&#35821;&#35328;&#26597;&#35810;&#26102;&#35299;&#37322;&#38271;&#31687;&#35270;&#39057;&#30340;&#35745;&#31639;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#39640;&#32500;&#35270;&#39057;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#35821;&#35328;&#19982;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#35270;&#35273;&#32447;&#32034;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#24341;&#23548;&#30340;&#26102;&#31354;&#25552;&#31034;&#23398;&#20064;&#65288;LSTP&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#21033;&#29992;&#20809;&#27969;&#20808;&#39564;&#30340;&#26102;&#38388;&#25552;&#31034;&#37319;&#26679;&#22120;&#65288;TPS&#65289;&#65292;&#21487;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#26377;&#25928;&#25552;&#21462;&#30456;&#20851;&#35270;&#39057;&#20869;&#23481;&#65307;&#20197;&#21450;&#28789;&#24039;&#22320;&#25429;&#25417;&#35270;&#35273;&#21644;&#25991;&#26412;&#20803;&#32032;&#20043;&#38388;&#22797;&#26434;&#31354;&#38388;&#20851;&#31995;&#30340;&#31354;&#38388;&#25552;&#31034;&#27714;&#35299;&#22120;&#65288;SPS&#65289;&#12290;&#36890;&#36807;&#23558;TPS&#21644;SPS&#19982;&#19968;&#33268;&#30340;&#35757;&#32451;&#31574;&#30053;&#30456;&#21327;&#35843;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#21319;&#20102;&#35745;&#31639;&#25928;&#29575;&#12289;&#26102;&#38388;&#29702;&#35299;&#21644;&#31354;&#38388;-&#26102;&#38388;&#23545;&#40784;&#12290;&#22312;&#20004;&#20010;&#25361;&#25112;&#20013;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16050v1 Announce Type: cross  Abstract: Despite progress in video-language modeling, the computational challenge of interpreting long-form videos in response to task-specific linguistic queries persists, largely due to the complexity of high-dimensional video data and the misalignment between language and visual cues over space and time. To tackle this issue, we introduce a novel approach called Language-guided Spatial-Temporal Prompt Learning (LSTP). This approach features two key components: a Temporal Prompt Sampler (TPS) with optical flow prior that leverages temporal information to efficiently extract relevant video content, and a Spatial Prompt Solver (SPS) that adeptly captures the intricate spatial relationships between visual and textual elements. By harmonizing TPS and SPS with a cohesive training strategy, our framework significantly enhances computational efficiency, temporal understanding, and spatial-temporal alignment. Empirical evaluations across two challeng
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#31163;&#25955;&#25193;&#25955;&#32467;&#21512;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26426;&#22120;&#20154;&#35270;&#39057;&#24494;&#35843;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;&#35270;&#39057;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;</title><link>https://arxiv.org/abs/2402.14407</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#36827;&#34892;&#22823;&#35268;&#27169;&#26080;&#21160;&#20316;&#35270;&#39057;&#39044;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14407
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31163;&#25955;&#25193;&#25955;&#32467;&#21512;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26426;&#22120;&#20154;&#35270;&#39057;&#24494;&#35843;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;&#35270;&#39057;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#23436;&#25104;&#22810;&#20010;&#20219;&#21153;&#30340;&#36890;&#29992;&#23454;&#20307;&#20195;&#29702;&#38754;&#20020;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#33258;&#32570;&#20047;&#26377;&#26631;&#35760;&#21160;&#20316;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23384;&#22312;&#22823;&#37327;&#25429;&#25417;&#22797;&#26434;&#20219;&#21153;&#21644;&#19982;&#29289;&#29702;&#19990;&#30028;&#20114;&#21160;&#30340;&#20154;&#31867;&#35270;&#39057;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#26694;&#26550;&#65292;&#21033;&#29992;&#32479;&#19968;&#30340;&#31163;&#25955;&#25193;&#25955;&#23558;&#20154;&#31867;&#35270;&#39057;&#19978;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#23569;&#37327;&#26377;&#26631;&#35760;&#26426;&#22120;&#20154;&#35270;&#39057;&#19978;&#30340;&#31574;&#30053;&#24494;&#35843;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#35270;&#39057;&#21387;&#32553;&#25104;&#32479;&#19968;&#30340;&#35270;&#39057;&#26631;&#35760;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#33945;&#29256;&#26367;&#25442;&#25193;&#25955;&#31574;&#30053;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#26469;&#39044;&#27979;&#28508;&#31354;&#38388;&#20013;&#30340;&#26410;&#26469;&#35270;&#39057;&#26631;&#35760;&#12290;&#22312;&#24494;&#35843;&#38454;&#27573;&#65292;&#25105;&#20204; h
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14407v1 Announce Type: new  Abstract: Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. In this paper, we introduce a novel framework that leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we h
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#34394;&#20551;&#22270;&#20687;&#20250;&#23548;&#33268;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35780;&#20272;&#24187;&#35273;&#31243;&#24230;&#30340;&#22522;&#20934;CorrelationQA&#65292;&#24182;&#21457;&#29616;&#20027;&#27969;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#21463;&#21040;&#36825;&#31181;&#26412;&#33021;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.03757</link><description>&lt;p&gt;
&#26412;&#33021;&#20559;&#35265;&#65306;&#34394;&#20551;&#22270;&#20687;&#23548;&#33268;MLLMs&#20135;&#29983;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#34394;&#20551;&#22270;&#20687;&#20250;&#23548;&#33268;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35780;&#20272;&#24187;&#35273;&#31243;&#24230;&#30340;&#22522;&#20934;CorrelationQA&#65292;&#24182;&#21457;&#29616;&#20027;&#27969;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#21463;&#21040;&#36825;&#31181;&#26412;&#33021;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#20986;&#29616;&#20351;LLMs&#20855;&#22791;&#20102;&#35270;&#35273;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20687;GPT-4V&#36825;&#26679;&#24378;&#22823;&#30340;MLLMs&#22312;&#38754;&#23545;&#26576;&#20123;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#26102;&#20173;&#28982;&#20197;&#24778;&#20154;&#30340;&#26041;&#24335;&#22833;&#36133;&#20102;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31867;&#20856;&#22411;&#36755;&#20837;&#65292;&#36825;&#20123;&#36755;&#20837;&#20196;MLLMs&#22256;&#24785;&#65292;&#23427;&#20204;&#30001;&#39640;&#24230;&#30456;&#20851;&#20294;&#19982;&#31572;&#26696;&#19981;&#19968;&#33268;&#30340;&#22270;&#20687;&#32452;&#25104;&#65292;&#23548;&#33268;MLLMs&#20135;&#29983;&#24187;&#35273;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CorrelationQA&#65292;&#36825;&#26159;&#39318;&#20010;&#35780;&#20272;&#32473;&#23450;&#34394;&#20551;&#22270;&#20687;&#30340;&#24187;&#35273;&#31243;&#24230;&#30340;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#21253;&#21547;13&#20010;&#31867;&#21035;&#30340;7,308&#20010;&#25991;&#26412;-&#22270;&#20687;&#23545;&#12290;&#22522;&#20110;&#25552;&#20986;&#30340;CorrelationQA&#65292;&#25105;&#20204;&#23545;9&#20010;&#20027;&#27969;MLLMs&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#23427;&#20204;&#26222;&#36941;&#21463;&#21040;&#36825;&#31181;&#26412;&#33021;&#20559;&#35265;&#30340;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#31934;&#36873;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#32467;&#26524;&#33021;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from hallucination. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the hallucination level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation result
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QUAVE&#30340;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#25110;&#32508;&#21512;&#20219;&#21153;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#25512;&#24191;&#20102;&#23545;&#21333;&#36890;&#36947;&#25968;&#25454;&#30340;&#37319;&#29992;&#12290;&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#21464;&#25442;&#21644;&#21152;&#26435;&#22788;&#29702;&#65292;QUAVE&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#36739;&#22823;&#21464;&#21270;&#30340;&#21307;&#23398;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.10224</link><description>&lt;p&gt;
&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#25512;&#24191;&#21307;&#23398;&#22270;&#20687;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Generalizing Medical Image Representations via Quaternion Wavelet Networks. (arXiv:2310.10224v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QUAVE&#30340;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#25110;&#32508;&#21512;&#20219;&#21153;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#25512;&#24191;&#20102;&#23545;&#21333;&#36890;&#36947;&#25968;&#25454;&#30340;&#37319;&#29992;&#12290;&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#21464;&#25442;&#21644;&#21152;&#26435;&#22788;&#29702;&#65292;QUAVE&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#36739;&#22823;&#21464;&#21270;&#30340;&#21307;&#23398;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#21644;&#21508;&#31181;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#26085;&#30410;&#22686;&#21152;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#24615;&#25104;&#20026;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#24403;&#22788;&#29702;&#21307;&#23398;&#25968;&#25454;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#24191;&#27867;&#65292;&#22240;&#20026;&#32570;&#20047;&#26041;&#27861;&#35770;&#26631;&#20934;&#23548;&#33268;&#19981;&#21516;&#30340;&#25104;&#20687;&#20013;&#24515;&#25110;&#20351;&#29992;&#19981;&#21516;&#35774;&#22791;&#21644;&#36741;&#21161;&#22240;&#32032;&#33719;&#21462;&#30340;&#25968;&#25454;&#23384;&#22312;&#36739;&#22823;&#21464;&#21270;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#26222;&#36866;&#30340;&#12289;&#25968;&#25454;-&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#65288;QUAVE&#65289;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#25110;&#32508;&#21512;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#24182;&#19988;&#21487;&#20197;&#32467;&#21512;&#23454;&#38469;&#12289;&#22235;&#20803;&#25968;&#25110;&#36229;&#22797;&#20540;&#27169;&#22411;&#65292;&#25512;&#24191;&#23427;&#20204;&#23545;&#21333;&#36890;&#36947;&#25968;&#25454;&#30340;&#37319;&#29992;&#12290;QUAVE&#39318;&#20808;&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#21464;&#25442;&#25552;&#21462;&#19981;&#21516;&#30340;&#23376;&#24102;&#65292;&#24471;&#21040;&#20302;&#39057;/&#36817;&#20284;&#39057;&#24102;&#21644;&#39640;&#39057;/&#32454;&#31890;&#24230;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#23427;&#23545;&#26368;&#26377;&#20195;&#34920;&#24615;&#30340;&#29305;&#24449;&#36827;&#34892;&#21152;&#26435;&#22788;&#29702;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#19981;&#22343;&#21248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network generalizability is becoming a broad research field due to the increasing availability of datasets from different sources and for various tasks. This issue is even wider when processing medical data, where a lack of methodological standards causes large variations being provided by different imaging centers or acquired with various devices and cofactors. To overcome these limitations, we introduce a novel, generalizable, data- and task-agnostic framework able to extract salient features from medical images. The proposed quaternion wavelet network (QUAVE) can be easily integrated with any pre-existing medical image analysis or synthesis task, and it can be involved with real, quaternion, or hypercomplex-valued models, generalizing their adoption to single-channel data. QUAVE first extracts different sub-bands through the quaternion wavelet transform, resulting in both low-frequency/approximation bands and high-frequency/fine-grained features. Then, it weighs the most repr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JLSPCADL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;Johnson-Lindenstrauss&#24341;&#29702;&#36873;&#25321;&#19968;&#20010;&#32500;&#25968;&#30340;&#36716;&#25442;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#21028;&#21035;&#24335;&#23383;&#20856;&#23398;&#20064;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13991</link><description>&lt;p&gt;
JL-&#24341;&#29702;&#25512;&#23548;&#30340;&#29992;&#20110;&#21028;&#21035;&#24335;&#23383;&#20856;&#23398;&#20064;&#30340;&#26368;&#20248;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
JL-lemma derived Optimal Projections for Discriminative Dictionary Learning. (arXiv:2308.13991v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JLSPCADL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;Johnson-Lindenstrauss&#24341;&#29702;&#36873;&#25321;&#19968;&#20010;&#32500;&#25968;&#30340;&#36716;&#25442;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#21028;&#21035;&#24335;&#23383;&#20856;&#23398;&#20064;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20811;&#26381;&#22312;&#20855;&#26377;&#22823;&#37327;&#31867;&#21035;&#30340;&#22823;&#32500;&#24230;&#25968;&#25454;&#20998;&#31867;&#20013;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JLSPCADL&#30340;&#26032;&#26041;&#27861;&#12290;&#26412;&#25991;&#21033;&#29992;Johnson-Lindenstrauss(JL)&#24341;&#29702;&#65292;&#22312;&#19968;&#20010;&#36716;&#25442;&#31354;&#38388;&#20013;&#36873;&#25321;&#19968;&#20010;&#32500;&#25968;&#65292;&#21487;&#20197;&#22312;&#20854;&#20013;&#23398;&#20064;&#29992;&#20110;&#20449;&#21495;&#20998;&#31867;&#30340;&#21028;&#21035;&#24335;&#23383;&#20856;&#12290;&#19982;&#36890;&#24120;&#20351;&#29992;JL&#36827;&#34892;&#38477;&#32500;&#30340;&#38543;&#26426;&#25237;&#24433;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;Modified Supervised PC Analysis (M-SPCA)&#25512;&#23548;&#24471;&#20986;&#30340;&#25237;&#24433;&#36716;&#25442;&#30697;&#38453;&#65292;&#20854;&#32500;&#25968;&#36981;&#24490;JL&#30340;&#35268;&#23450;&#12290;JLSPCADL&#25552;&#20379;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25512;&#26029;&#36866;&#24403;&#30340;&#22833;&#30495;&#27700;&#24179;&#21644;&#30456;&#24212;&#30340;&#23383;&#20856;&#21407;&#23376;&#30340;&#36866;&#24403;&#25551;&#36848;&#38271;&#24230;(SDL)&#65292;&#20197;&#25512;&#23548;&#20986;&#19968;&#20010;&#26368;&#20248;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31867;&#30340;&#23383;&#20856;&#21407;&#23376;&#30340;SDL&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#38477;&#32500;&#30340;&#23383;&#20856;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#20174;M-SPCA&#20013;&#19968;&#27493;&#24471;&#20986;&#30340;&#25237;&#24433;&#36716;&#25442;&#30697;&#38453;&#25552;&#20379;&#20102;&#26368;&#22823;&#30340;&#29305;&#24449;-&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To overcome difficulties in classifying large dimensionality data with a large number of classes, we propose a novel approach called JLSPCADL. This paper uses the Johnson-Lindenstrauss (JL) Lemma to select the dimensionality of a transformed space in which a discriminative dictionary can be learned for signal classification. Rather than reducing dimensionality via random projections, as is often done with JL, we use a projection transformation matrix derived from Modified Supervised PC Analysis (M-SPCA) with the JL-prescribed dimension.  JLSPCADL provides a heuristic to deduce suitable distortion levels and the corresponding Suitable Description Length (SDL) of dictionary atoms to derive an optimal feature space and thus the SDL of dictionary atoms for better classification. Unlike state-of-the-art dimensionality reduction-based dictionary learning methods, a projection transformation matrix derived in a single step from M-SPCA provides maximum feature-label consistency of the transfor
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#26159;&#23558;&#31526;&#21495;&#21644;&#32479;&#35745;&#35748;&#30693;&#33539;&#24335;&#25972;&#21512;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#25512;&#29702;&#21644;&#35299;&#37322;&#24615;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31561;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#37325;&#35201;&#36129;&#29486;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#25104;&#21151;&#24212;&#29992;&#26696;&#20363;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2210.15889</link><description>&lt;p&gt;
&#36208;&#21521;&#25968;&#25454;&#21644;&#30693;&#35782;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing. (arXiv:2210.15889v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15889
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#26159;&#23558;&#31526;&#21495;&#21644;&#32479;&#35745;&#35748;&#30693;&#33539;&#24335;&#25972;&#21512;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#25512;&#29702;&#21644;&#35299;&#37322;&#24615;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31561;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#37325;&#35201;&#36129;&#29486;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#25104;&#21151;&#24212;&#29992;&#26696;&#20363;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#65288;NeSy&#65289;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#36861;&#27714;&#31526;&#21495;&#21644;&#32479;&#35745;&#35748;&#30693;&#33539;&#24335;&#30340;&#25972;&#21512;&#12290;&#30001;&#20110;NeSy&#22312;&#31526;&#21495;&#34920;&#31034;&#30340;&#25512;&#29702;&#21644;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#23427;&#21487;&#33021;&#25104;&#20026;&#19979;&#19968;&#20195;AI&#30340;&#20652;&#21270;&#21058;&#12290;&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;NeSy&#30740;&#31350;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#37325;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#21382;&#21490;&#65292;&#28085;&#30422;&#20102;&#26089;&#26399;&#24037;&#20316;&#21644;&#22522;&#30784;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#32972;&#26223;&#27010;&#24565;&#65292;&#24182;&#30830;&#23450;&#20102;&#25512;&#21160;NeSy&#21457;&#23637;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25353;&#29031;&#20960;&#20010;&#20027;&#35201;&#29305;&#24449;&#23545;&#36817;&#26399;&#30340;&#37324;&#31243;&#30865;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#21253;&#25324;&#31070;&#32463;&#31526;&#21495;&#25972;&#21512;&#12289;&#30693;&#35782;&#34920;&#31034;&#12289;&#30693;&#35782;&#23884;&#20837;&#21644;&#21151;&#33021;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#31616;&#35201;&#35752;&#35770;&#20102;&#25104;&#21151;&#30340;&#24212;&#29992;&#26696;&#20363;&#65292;&#20197;&#21450;NeSy&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-symbolic computing (NeSy), which pursues the integration of the symbolic and statistical paradigms of cognition, has been an active research area of Artificial Intelligence (AI) for many years. As NeSy shows promise of reconciling the advantages of reasoning and interpretability of symbolic representation and robust learning in neural networks, it may serve as a catalyst for the next generation of AI. In the present paper, we provide a systematic overview of the recent developments and important contributions of NeSy research. Firstly, we introduce study history of this area, covering early work and foundations. We further discuss background concepts and identify key driving factors behind the development of NeSy. Afterward, we categorize recent landmark approaches along several main characteristics that underline this research paradigm, including neural-symbolic integration, knowledge representation, knowledge embedding, and functionality. Next, we briefly discuss the successfu
&lt;/p&gt;</description></item></channel></rss>