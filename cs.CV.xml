<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36890;&#36807;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#65288;PST&#65289;&#26694;&#26550;&#65292;&#22312;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20013;&#25506;&#31350;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35780;&#20272;&#20102;DALLE-3&#22312;&#24615;&#21035;&#32844;&#19994;&#21644;&#32452;&#32455;&#26435;&#21147;&#26041;&#38754;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.11089</link><description>&lt;p&gt;
&#30007;&#24615;CEO&#21644;&#22899;&#24615;&#21161;&#29702;&#65306;&#36890;&#36807;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#25506;&#31350;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
The Male CEO and the Female Assistant: Probing Gender Biases in Text-To-Image Models Through Paired Stereotype Test
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11089
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#65288;PST&#65289;&#26694;&#26550;&#65292;&#22312;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20013;&#25506;&#31350;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35780;&#20272;&#20102;DALLE-3&#22312;&#24615;&#21035;&#32844;&#19994;&#21644;&#32452;&#32455;&#26435;&#21147;&#26041;&#38754;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#65288;&#22914;DALLE-3&#65289;&#23637;&#31034;&#20102;&#22312;&#26032;&#24212;&#29992;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20063;&#38754;&#20020;&#21069;&#25152;&#26410;&#26377;&#30340;&#20844;&#24179;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#21333;&#20154;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;T2I&#27169;&#22411;&#24212;&#29992;&#21487;&#33021;&#38656;&#35201;&#21516;&#26102;&#25551;&#32472;&#20004;&#20010;&#25110;&#26356;&#22810;&#20154;&#12290;&#35813;&#35774;&#23450;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#20173;&#26410;&#34987;&#25506;&#31350;&#65292;&#23548;&#33268;&#20351;&#29992;&#20013;&#30340;&#20844;&#24179;&#30456;&#20851;&#39118;&#38505;&#12290;&#20026;&#20102;&#30740;&#31350;T2I&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#22522;&#26412;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#65288;PST&#65289;&#20559;&#35265;&#35780;&#20272;&#26694;&#26550;&#12290;PST&#20419;&#20351;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#22270;&#20687;&#20013;&#30340;&#20004;&#20010;&#20010;&#20307;&#65292;&#29992;&#19982;&#30456;&#21453;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#30456;&#20851;&#32852;&#30340;&#20004;&#20010;&#31038;&#20250;&#36523;&#20221;&#26469;&#25551;&#36848;&#20182;&#20204;&#12290;&#36890;&#36807;&#29983;&#25104;&#30340;&#22270;&#20687;&#36981;&#20174;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#30340;&#31243;&#24230;&#26469;&#34913;&#37327;&#20559;&#35265;&#12290;&#21033;&#29992;PST&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#35780;&#20272;DALLE-3&#65306;&#24615;&#21035;&#32844;&#19994;&#20013;&#30340;&#20559;&#35265;&#21644;&#32452;&#32455;&#26435;&#21147;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11089v1 Announce Type: cross  Abstract: Recent large-scale Text-To-Image (T2I) models such as DALLE-3 demonstrate great potential in new applications, but also face unprecedented fairness challenges. Prior studies revealed gender biases in single-person image generation, but T2I model applications might require portraying two or more people simultaneously. Potential biases in this setting remain unexplored, leading to fairness-related risks in usage. To study these underlying facets of gender biases in T2I models, we propose a novel Paired Stereotype Test (PST) bias evaluation framework. PST prompts the model to generate two individuals in the same image. They are described with two social identities that are stereotypically associated with the opposite gender. Biases can then be measured by the level of conformation to gender stereotypes in generated images. Using PST, we evaluate DALLE-3 from 2 perspectives: biases in gendered occupation and biases in organizational power.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32479;&#19968;&#27010;&#24565;&#32534;&#36753;&#65288;UCE&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#38381;&#21512;&#35299;&#20915;&#26041;&#26696;&#23545;&#27169;&#22411;&#36827;&#34892;&#32534;&#36753;&#65292;&#21516;&#26102;&#35299;&#20915;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12289;&#29256;&#26435;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#31561;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14761</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#32479;&#19968;&#27010;&#24565;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Unified Concept Editing in Diffusion Models. (arXiv:2308.14761v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32479;&#19968;&#27010;&#24565;&#32534;&#36753;&#65288;UCE&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#38381;&#21512;&#35299;&#20915;&#26041;&#26696;&#23545;&#27169;&#22411;&#36827;&#34892;&#32534;&#36753;&#65292;&#21516;&#26102;&#35299;&#20915;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12289;&#29256;&#26435;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#31561;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23384;&#22312;&#21508;&#31181;&#23433;&#20840;&#38382;&#39064;&#65292;&#21487;&#33021;&#38480;&#21046;&#20854;&#36866;&#29992;&#24615;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12289;&#29256;&#26435;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#31561;&#21508;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#65292;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#37117;&#21516;&#26102;&#20986;&#29616;&#22312;&#21516;&#19968;&#20010;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;&#26041;&#27861;&#35299;&#20915;&#25152;&#26377;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#32479;&#19968;&#27010;&#24565;&#32534;&#36753;&#65288;UCE&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#32463;&#36807;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#38381;&#21512;&#35299;&#20915;&#26041;&#26696;&#23545;&#27169;&#22411;&#36827;&#34892;&#32534;&#36753;&#65292;&#24182;&#21487;&#26080;&#32541;&#22320;&#25193;&#23637;&#21040;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#25193;&#25955;&#27169;&#22411;&#19978;&#36827;&#34892;&#24182;&#34892;&#32534;&#36753;&#12290;&#25105;&#20204;&#36890;&#36807;&#32534;&#36753;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25237;&#24433;&#26469;&#23637;&#31034;&#21487;&#25193;&#23637;&#30340;&#21516;&#26102;&#21435;&#20559;&#35265;&#12289;&#28040;&#38500;&#39118;&#26684;&#21644;&#20869;&#23481;&#35843;&#33410;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://unified.baulab.info&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image models suffer from various safety issues that may limit their suitability for deployment. Previous methods have separately addressed individual issues of bias, copyright, and offensive content in text-to-image models. However, in the real world, all of these issues appear simultaneously in the same model. We present a method that tackles all issues with a single approach. Our method, Unified Concept Editing (UCE), edits the model without training using a closed-form solution, and scales seamlessly to concurrent edits on text-conditional diffusion models. We demonstrate scalable simultaneous debiasing, style erasure, and content moderation by editing text-to-image projections, and we present extensive experiments demonstrating improved efficacy and scalability over prior work. Our code is available at https://unified.baulab.info
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#24352;&#37327;&#32593;&#32476;&#29702;&#35299;&#21644;&#28436;&#21270;&#21367;&#31215;&#30340;&#26032;&#35270;&#35282;&#65292;&#21487;&#20197;&#36890;&#36807;&#32472;&#21046;&#21644;&#25805;&#20316;&#24352;&#37327;&#32593;&#32476;&#26469;&#36827;&#34892;&#20989;&#25968;&#36716;&#25442;&#12289;&#23376;&#24352;&#37327;&#35775;&#38382;&#21644;&#34701;&#21512;&#12290;&#30740;&#31350;&#20154;&#21592;&#36824;&#28436;&#31034;&#20102;&#21367;&#31215;&#22270;&#34920;&#30340;&#23548;&#20986;&#20197;&#21450;&#21508;&#31181;&#33258;&#21160;&#24494;&#20998;&#25805;&#20316;&#21644;&#20108;&#38454;&#20449;&#24687;&#36924;&#36817;&#22270;&#34920;&#30340;&#29983;&#25104;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#29305;&#23450;&#20110;&#21367;&#31215;&#30340;&#22270;&#34920;&#36716;&#25442;&#65292;&#20197;&#20248;&#21270;&#35745;&#31639;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02275</link><description>&lt;p&gt;
&#36879;&#36807;&#24352;&#37327;&#32593;&#32476;&#30340;&#35270;&#35282;&#35299;&#26512;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Convolutions Through the Lens of Tensor Networks. (arXiv:2307.02275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#24352;&#37327;&#32593;&#32476;&#29702;&#35299;&#21644;&#28436;&#21270;&#21367;&#31215;&#30340;&#26032;&#35270;&#35282;&#65292;&#21487;&#20197;&#36890;&#36807;&#32472;&#21046;&#21644;&#25805;&#20316;&#24352;&#37327;&#32593;&#32476;&#26469;&#36827;&#34892;&#20989;&#25968;&#36716;&#25442;&#12289;&#23376;&#24352;&#37327;&#35775;&#38382;&#21644;&#34701;&#21512;&#12290;&#30740;&#31350;&#20154;&#21592;&#36824;&#28436;&#31034;&#20102;&#21367;&#31215;&#22270;&#34920;&#30340;&#23548;&#20986;&#20197;&#21450;&#21508;&#31181;&#33258;&#21160;&#24494;&#20998;&#25805;&#20316;&#21644;&#20108;&#38454;&#20449;&#24687;&#36924;&#36817;&#22270;&#34920;&#30340;&#29983;&#25104;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#29305;&#23450;&#20110;&#21367;&#31215;&#30340;&#22270;&#34920;&#36716;&#25442;&#65292;&#20197;&#20248;&#21270;&#35745;&#31639;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21367;&#31215;&#30340;&#30452;&#35266;&#27010;&#24565;&#31616;&#21333;&#65292;&#20294;&#20854;&#20998;&#26512;&#27604;&#31264;&#23494;&#23618;&#26356;&#21152;&#22797;&#26434;&#65292;&#36825;&#20351;&#24471;&#29702;&#35770;&#21644;&#31639;&#27861;&#30340;&#25512;&#24191;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#36890;&#36807;&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;&#25552;&#20379;&#20102;&#23545;&#21367;&#31215;&#30340;&#26032;&#35270;&#35282;&#65292;&#36890;&#36807;&#32472;&#21046;&#22270;&#34920;&#12289;&#25805;&#20316;&#22270;&#34920;&#36827;&#34892;&#20989;&#25968;&#36716;&#25442;&#12289;&#23376;&#24352;&#37327;&#35775;&#38382;&#21644;&#34701;&#21512;&#26469;&#25512;&#29702;&#24213;&#23618;&#24352;&#37327;&#20056;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#21508;&#31181;&#33258;&#21160;&#24494;&#20998;&#25805;&#20316;&#30340;&#22270;&#34920;&#20197;&#21450;&#20855;&#26377;&#23436;&#25972;&#36229;&#21442;&#25968;&#25903;&#25345;&#12289;&#25209;&#22788;&#29702;&#12289;&#36890;&#36947;&#32452;&#21644;&#20219;&#24847;&#21367;&#31215;&#32500;&#24230;&#27867;&#21270;&#30340;&#27969;&#34892;&#30340;&#20108;&#38454;&#20449;&#24687;&#36924;&#36817;&#30340;&#22270;&#34920;&#26469;&#23637;&#31034;&#36825;&#31181;&#34920;&#36798;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#36830;&#25509;&#27169;&#24335;&#25552;&#20379;&#20102;&#29305;&#23450;&#20110;&#21367;&#31215;&#30340;&#36716;&#25442;&#65292;&#20801;&#35768;&#22312;&#35780;&#20272;&#20043;&#21069;&#37325;&#26032;&#36830;&#25509;&#21644;&#31616;&#21270;&#22270;&#34920;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20381;&#36182;&#20110;&#39640;&#25928;TN&#32553;&#24182;&#30340;&#24050;&#24314;&#31435;&#26426;&#21046;&#26469;&#25506;&#31350;&#35745;&#31639;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;TN&#23454;&#29616;&#21152;&#36895;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;
&lt;/p&gt;
&lt;p&gt;
Despite their simple intuition, convolutions are more tedious to analyze than dense layers, which complicates the generalization of theoretical and algorithmic ideas. We provide a new perspective onto convolutions through tensor networks (TNs) which allow reasoning about the underlying tensor multiplications by drawing diagrams, and manipulating them to perform function transformations, sub-tensor access, and fusion. We demonstrate this expressive power by deriving the diagrams of various autodiff operations and popular approximations of second-order information with full hyper-parameter support, batching, channel groups, and generalization to arbitrary convolution dimensions. Further, we provide convolution-specific transformations based on the connectivity pattern which allow to re-wire and simplify diagrams before evaluation. Finally, we probe computational performance, relying on established machinery for efficient TN contraction. Our TN implementation speeds up a recently-proposed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FineRewards&#65292;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#22870;&#21169;&#65292;&#21363;&#26631;&#39064;&#22870;&#21169;&#21644;SAM&#22870;&#21169;&#65292;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2305.19599</link><description>&lt;p&gt;
&#32454;&#31890;&#24230;&#35821;&#20041;&#22870;&#21169;&#22686;&#24378;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards. (arXiv:2305.19599v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FineRewards&#65292;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#22870;&#21169;&#65292;&#21363;&#26631;&#39064;&#22870;&#21169;&#21644;SAM&#22870;&#21169;&#65292;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#22312;&#32473;&#23450;&#30340;&#25991;&#26412;&#25552;&#31034;&#19979;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#32454;&#31890;&#24230;&#35821;&#20041;&#25351;&#23548;&#65292;&#20197;&#25104;&#21151;&#35786;&#26029;&#24418;&#24577;&#24046;&#24322;&#20026;&#27490;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#25991;&#26412;&#27010;&#24565;&#21644;&#29983;&#25104;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#20934;&#30830;&#24418;&#24577;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FineRewards&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#26032;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#22870;&#21169;--&#26631;&#39064;&#22870;&#21169;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#20309;&#20107;&#29289;&#65288;SAM&#65289;&#22870;&#21169;&#65292;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-to-image diffusion models have achieved remarkable success in generating high-quality, realistic images from given text prompts. However, previous methods fail to perform accurate modality alignment between text concepts and generated images due to the lack of fine-level semantic guidance that successfully diagnoses the modality discrepancy. In this paper, we propose FineRewards to improve the alignment between text and images in text-to-image diffusion models by introducing two new fine-grained semantic rewards: the caption reward and the Semantic Segment Anything (SAM) reward. From the global semantic view, the caption reward generates a corresponding detailed caption that depicts all important contents in the synthetic image via a BLIP-2 model and then calculates the reward score by measuring the similarity between the generated caption and the given prompt. From the local semantic view, the SAM reward segments the generated images into local parts with categ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARBEx&#30340;&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#20102;&#21487;&#38752;&#24615;&#24179;&#34913;&#26041;&#27861;&#26469;&#24212;&#23545;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#36824;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#38170;&#28857;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.01486</link><description>&lt;p&gt;
ARBEx&#65306;&#29992;&#20110;&#40065;&#26834;&#24615;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#30340;&#20851;&#27880;&#29305;&#24449;&#25552;&#21462;&#19982;&#21487;&#38752;&#24615;&#24179;&#34913;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ARBEx: Attentive Feature Extraction with Reliability Balancing for Robust Facial Expression Learning. (arXiv:2305.01486v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARBEx&#30340;&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#20102;&#21487;&#38752;&#24615;&#24179;&#34913;&#26041;&#27861;&#26469;&#24212;&#23545;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#36824;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#38170;&#28857;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARBEx&#30340;&#26694;&#26550;&#65292;&#23427;&#26159;&#30001;Vision Transformer&#39537;&#21160;&#30340;&#26032;&#22411;&#20851;&#27880;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#65292;&#24102;&#26377;&#21487;&#38752;&#24615;&#24179;&#34913;&#65292;&#20197;&#24212;&#23545;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#36739;&#24046;&#31867;&#20998;&#24067;&#12289;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22810;&#31181;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#31934;&#21270;&#26041;&#27861;&#20197;&#21450;&#22522;&#20110;&#31383;&#21475;&#30340;&#20132;&#21449;&#20851;&#27880;ViT&#26469;&#20805;&#20998;&#21033;&#29992;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#38170;&#28857;&#65292;&#21152;&#19978;&#26631;&#31614;&#20998;&#24067;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#36890;&#36807;&#21487;&#38752;&#24615;&#24179;&#34913;&#20248;&#21270;&#23545;&#24369;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#39640;&#26631;&#31614;&#39044;&#27979;&#38887;&#24615;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#30830;&#20445;&#27491;&#30830;&#30340;&#26631;&#31614;&#20998;&#31867;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38170;&#25439;&#22833;&#65292;&#40723;&#21169;&#38170;&#28857;&#20043;&#38388;&#30340;&#22823;&#38388;&#38548;&#12290;&#21478;&#22806;&#65292;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#20063;&#26159;&#21487;&#35757;&#32451;&#30340;&#65292;&#23545;&#20110;&#25552;&#21319;&#22312;FEL&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;ARBEx&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a framework ARBEx, a novel attentive feature extraction framework driven by Vision Transformer with reliability balancing to cope against poor class distributions, bias, and uncertainty in the facial expression learning (FEL) task. We reinforce several data pre-processing and refinement methods along with a window-based cross-attention ViT to squeeze the best of the data. We also employ learnable anchor points in the embedding space with label distributions and multi-head self-attention mechanism to optimize performance against weak predictions with reliability balancing, which is a strategy that leverages anchor points, attention scores, and confidence values to enhance the resilience of label predictions. To ensure correct label classification and improve the models' discriminative power, we introduce anchor loss, which encourages large margins between anchor points. Additionally, the multi-head self-attention mechanism, which is also trainable, plays an i
&lt;/p&gt;</description></item></channel></rss>