<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#24182;&#35299;&#20915;&#20102;AI&#29983;&#25104;&#30340;&#38754;&#23380;&#20013;&#23384;&#22312;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#22120;&#29992;&#20110;&#39044;&#27979;&#38754;&#37096;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#21435;&#20559;&#35265;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01002</link><description>&lt;p&gt;
AI&#29983;&#25104;&#30340;&#38754;&#23380;&#25670;&#33073;&#20102;&#31181;&#26063;&#21644;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
AI-generated faces free from racial and gender stereotypes
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01002
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#24182;&#35299;&#20915;&#20102;AI&#29983;&#25104;&#30340;&#38754;&#23380;&#20013;&#23384;&#22312;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#22120;&#29992;&#20110;&#39044;&#27979;&#38754;&#37096;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#21435;&#20559;&#35265;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;Stable Diffusion&#20043;&#31867;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;AI&#27169;&#22411;&#27599;&#22825;&#37117;&#34987;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20154;&#23545;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#25918;&#22823;&#31181;&#26063;&#21644;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#25552;&#20986;&#20102;&#20851;&#20999;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#20219;&#24847;&#32473;&#23450;&#38754;&#37096;&#22270;&#20687;&#30340;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#24180;&#40836;&#32452;&#65292;&#24182;&#23637;&#31034;&#20854;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21033;&#29992;&#36825;&#20010;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#23545;Stable Diffusion&#22312;&#20845;&#31181;&#31181;&#26063;&#12289;&#20004;&#31181;&#24615;&#21035;&#12289;&#20116;&#20010;&#24180;&#40836;&#32452;&#12289;32&#20010;&#32844;&#19994;&#21644;&#20843;&#20010;&#23646;&#24615;&#19978;&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#37327;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#36234;&#26368;&#20808;&#36827;&#26367;&#20195;&#26041;&#26696;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26816;&#26597;&#20102;Stable Diffusion&#22312;&#25551;&#32472;&#21516;&#19968;&#31181;&#26063;&#30340;&#20010;&#20307;&#26102;&#30456;&#20284;&#31243;&#24230;&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#20986;&#39640;&#24230;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#20363;&#22914;&#65292;&#23558;&#22823;&#22810;&#25968;&#20013;&#19996;&#30007;&#24615;&#25551;&#32472;&#20026;&#30382;&#32932;&#40669;&#40657;&#12289;&#30041;&#30528;&#32993;&#23376;&#12289;&#25140;&#30528;&#20256;&#32479;&#22836;&#39280;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#22686;&#21152;&#38754;&#37096;&#22810;&#26679;&#24615;&#30340;&#26032;&#22411;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative AI models such as Stable Diffusion are used daily by millions worldwide. However, many have raised concerns regarding how these models amplify racial and gender stereotypes. To study this phenomenon, we develop a classifier to predict the race, gender, and age group of any given face image, and show that it achieves state-of-the-art performance. Using this classifier, we quantify biases in Stable Diffusion across six races, two genders, five age groups, 32 professions, and eight attributes. We then propose novel debiasing solutions that outperform state-of-the-art alternatives. Additionally, we examine the degree to which Stable Diffusion depicts individuals of the same race as being similar to one another. This analysis reveals a high degree of stereotyping, e.g., depicting most middle eastern males as being dark-skinned, bearded, and wearing a traditional headdress. We address these limitations by proposing yet another novel solution that increases facial div
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;Design2Code&#20219;&#21153;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;LLMs&#30452;&#25509;&#23558;&#35270;&#35273;&#35774;&#35745;&#36716;&#25442;&#20026;&#20195;&#30721;&#23454;&#29616;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03163</link><description>&lt;p&gt;
Design2Code&#65306;&#25105;&#20204;&#31163;&#33258;&#21160;&#21270;&#21069;&#31471;&#24037;&#31243;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
Design2Code: How Far Are We From Automating Front-End Engineering?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03163
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;Design2Code&#20219;&#21153;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;LLMs&#30452;&#25509;&#23558;&#35270;&#35273;&#35774;&#35745;&#36716;&#25442;&#20026;&#20195;&#30721;&#23454;&#29616;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#39134;&#29467;&#36827;&#30340;&#36827;&#23637;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#36825;&#21487;&#20197;&#23454;&#29616;&#19968;&#31181;&#26032;&#30340;&#21069;&#31471;&#24320;&#21457;&#33539;&#24335;&#65292;&#20854;&#20013;&#22810;&#27169;&#24577;LLMs&#21487;&#33021;&#30452;&#25509;&#23558;&#35270;&#35273;&#35774;&#35745;&#36716;&#25442;&#20026;&#20195;&#30721;&#23454;&#29616;&#12290;&#26412;&#25991;&#23558;&#36825;&#19968;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;Design2Code&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#25163;&#21160;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;484&#20010;&#22810;&#26679;&#21270;&#30495;&#23454;&#32593;&#39029;&#30340;&#22522;&#20934;&#27979;&#35797;&#29992;&#20363;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#22871;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35780;&#20272;&#24403;&#21069;&#22810;&#27169;&#24577;LLMs&#33021;&#21542;&#29983;&#25104;&#30452;&#25509;&#28210;&#26579;&#20026;&#32473;&#23450;&#21442;&#32771;&#32593;&#39029;&#30340;&#20195;&#30721;&#23454;&#29616;&#65292;&#20197;&#36755;&#20837;&#20026;&#23631;&#24149;&#25130;&#22270;&#12290;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#20840;&#38754;&#30340;&#20154;&#24037;&#35780;&#20272;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#22810;&#27169;&#24577;&#25552;&#31034;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;GPT-4V&#21644;Gemini Pro Vision&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#19968;&#20010;&#24320;&#28304;&#30340;Design2Code-18B&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03163v1 Announce Type: new  Abstract: Generative AI has made rapid advancements in recent years, achieving unprecedented capabilities in multimodal understanding and code generation. This can enable a new paradigm of front-end development, in which multimodal LLMs might directly convert visual designs into code implementations. In this work, we formalize this as a Design2Code task and conduct comprehensive benchmarking. Specifically, we manually curate a benchmark of 484 diverse real-world webpages as test cases and develop a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate the code implementations that directly render into the given reference webpages, given the screenshots as input. We also complement automatic metrics with comprehensive human evaluations. We develop a suite of multimodal prompting methods and show their effectiveness on GPT-4V and Gemini Pro Vision. We further finetune an open-source Design2Code-18B model that su
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#30830;&#23450;&#24615;&#20316;&#20026;&#32622;&#20449;&#24230;&#20272;&#35745;&#21644;&#23884;&#20837;&#31354;&#38388;&#30340;&#27491;&#21017;&#39033;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2401.13721</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in Regression. (arXiv:2401.13721v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13721
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#30830;&#23450;&#24615;&#20316;&#20026;&#32622;&#20449;&#24230;&#20272;&#35745;&#21644;&#23884;&#20837;&#31354;&#38388;&#30340;&#27491;&#21017;&#39033;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#65288;UDAR&#65289;&#26088;&#22312;&#23558;&#26469;&#33258;&#26377;&#26631;&#31614;&#28304;&#39046;&#22495;&#30340;&#27169;&#22411;&#35843;&#25972;&#21040;&#26080;&#26631;&#31614;&#30446;&#26631;&#39046;&#22495;&#65292;&#20197;&#23436;&#25104;&#22238;&#24402;&#20219;&#21153;&#12290;&#26368;&#36817;&#22312;UDAR&#39046;&#22495;&#21462;&#24471;&#30340;&#25104;&#21151;&#20027;&#35201;&#38598;&#20013;&#22312;&#23376;&#31354;&#38388;&#23545;&#40784;&#19978;&#65292;&#28041;&#21450;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#25152;&#36873;&#25321;&#23376;&#31354;&#38388;&#30340;&#23545;&#40784;&#12290;&#36825;&#19982;&#29992;&#20110;&#20998;&#31867;&#30340;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#26088;&#22312;&#23545;&#40784;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#25928;&#26524;&#36739;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20998;&#31867;&#20219;&#21153;&#26088;&#22312;&#22312;&#25972;&#20010;&#23884;&#20837;&#31354;&#38388;&#30340;&#32500;&#24230;&#19978;&#35782;&#21035;&#29420;&#31435;&#30340;&#31751;&#65292;&#32780;&#22238;&#24402;&#20219;&#21153;&#23545;&#25968;&#25454;&#34920;&#31034;&#30340;&#32467;&#26500;&#24615;&#35201;&#27714;&#36739;&#20302;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#25351;&#23548;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26377;&#25928;UDAR&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#21452;&#37325;&#20316;&#29992;&#65306;&#25552;&#20379;&#20102;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#32622;&#20449;&#24230;&#34913;&#37327;&#65292;&#24182;&#20316;&#20026;&#23884;&#20837;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#35777;&#25454;&#27169;&#22411;&#26469;&#25552;&#20379;&#23545;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#23884;&#20837;&#31354;&#38388;&#30340;&#27491;&#21017;&#39033;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation for Regression (UDAR) aims to adapt a model from a labeled source domain to an unlabeled target domain for regression tasks. Recent successful works in UDAR mostly focus on subspace alignment, involving the alignment of a selected subspace within the entire feature space. This contrasts with the feature alignment methods used for classification, which aim at aligning the entire feature space and have proven effective but are less so in regression settings. Specifically, while classification aims to identify separate clusters across the entire embedding dimension, regression induces less structure in the data representation, necessitating additional guidance for efficient alignment. In this paper, we propose an effective method for UDAR by incorporating guidance from uncertainty. Our approach serves a dual purpose: providing a measure of confidence in predictions and acting as a regularization of the embedding space. Specifically, we leverage the Deep Evid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#20851;&#38190;&#24615;&#30340;&#39640;&#25928;SNN&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#21152;&#24378;&#29305;&#24449;&#25552;&#21462;&#21644;&#21152;&#36895;&#20462;&#21098;&#36807;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.16141</link><description>&lt;p&gt;
SNNs&#20013;&#22522;&#20110;&#20851;&#38190;&#24615;&#30340;&#39640;&#25928;&#20462;&#21098;&#26041;&#27861;&#65292;&#21463;&#21040;&#20851;&#38190;&#24615;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;
&lt;/p&gt;
&lt;p&gt;
Criticality-Guided Efficient Pruning in Spiking Neural Networks Inspired by Critical Brain Hypothesis. (arXiv:2311.16141v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#20851;&#38190;&#24615;&#30340;&#39640;&#25928;SNN&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#21152;&#24378;&#29305;&#24449;&#25552;&#21462;&#21644;&#21152;&#36895;&#20462;&#21098;&#36807;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#33410;&#33021;&#21644;&#26080;&#20056;&#27861;&#29305;&#24615;&#65292;SNNs&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#28145;&#24230;SNNs&#35268;&#27169;&#30340;&#19981;&#26029;&#22686;&#38271;&#32473;&#27169;&#22411;&#37096;&#32626;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#32593;&#32476;&#20462;&#21098;&#36890;&#36807;&#21387;&#32553;&#32593;&#32476;&#35268;&#27169;&#26469;&#20943;&#23569;&#27169;&#22411;&#37096;&#32626;&#30340;&#30828;&#20214;&#36164;&#28304;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;SNN&#20462;&#21098;&#26041;&#27861;&#30001;&#20110;&#20462;&#21098;&#36845;&#20195;&#22686;&#21152;&#20102;SNNs&#30340;&#35757;&#32451;&#38590;&#24230;&#65292;&#23548;&#33268;&#20462;&#21098;&#25104;&#26412;&#39640;&#26114;&#19988;&#24615;&#33021;&#25439;&#22833;&#20005;&#37325;&#12290;&#26412;&#25991;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#20851;&#38190;&#24615;&#30340;&#29992;&#20110;SNN&#20462;&#21098;&#30340;&#20877;&#29983;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#25552;&#21462;&#24182;&#21152;&#36895;&#20462;&#21098;&#36807;&#31243;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;SNN&#20013;&#29992;&#20110;&#20851;&#38190;&#24615;&#30340;&#20302;&#25104;&#26412;&#24230;&#37327;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#22312;&#20462;&#21098;&#21518;&#23545;&#25152;&#20462;&#21098;&#32467;&#26500;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#24182;&#20877;&#29983;&#37027;&#20123;&#20855;&#26377;&#36739;&#39640;&#20851;&#38190;&#24615;&#30340;&#32467;&#26500;&#65292;&#20197;&#33719;&#21462;&#20851;&#38190;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have gained considerable attention due to the energy-efficient and multiplication-free characteristics. The continuous growth in scale of deep SNNs poses challenges for model deployment. Network pruning reduces hardware resource requirements of model deployment by compressing the network scale. However, existing SNN pruning methods cause high pruning costs and performance loss because the pruning iterations amplify the training difficulty of SNNs. In this paper, inspired by the critical brain hypothesis in neuroscience, we propose a regeneration mechanism based on the neuron criticality for SNN pruning to enhance feature extraction and accelerate the pruning process. Firstly, we propose a low-cost metric for the criticality in SNNs. Then, we re-rank the pruned structures after pruning and regenerate those with higher criticality to obtain the critical network. Our method achieves higher performance than the current state-of-the-art (SOTA) method with up t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HIS-Unet&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21152;&#26435;&#27880;&#24847;&#21147;&#27169;&#22359;&#23454;&#29616;&#28023;&#20912;&#27987;&#24230;&#21644;&#28418;&#31227;&#30340;&#39044;&#27979;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;HIS-Unet&#22312;&#28023;&#20912;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2311.00167</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#39044;&#27979;&#21271;&#20912;&#27915;&#28023;&#20912;&#27987;&#24230;&#21644;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Multi-task Deep Convolutional Network to Predict Sea Ice Concentration and Drift in the Arctic Ocean. (arXiv:2311.00167v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00167
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HIS-Unet&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21152;&#26435;&#27880;&#24847;&#21147;&#27169;&#22359;&#23454;&#29616;&#28023;&#20912;&#27987;&#24230;&#21644;&#28418;&#31227;&#30340;&#39044;&#27979;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;HIS-Unet&#22312;&#28023;&#20912;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21271;&#20912;&#27915;&#22320;&#21306;&#65292;&#39044;&#27979;&#28023;&#20912;&#27987;&#24230;(SIC)&#21644;&#28023;&#20912;&#28418;&#31227;(SID)&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#22240;&#20026;&#26368;&#36817;&#30340;&#27668;&#20505;&#21464;&#26262;&#24050;&#32463;&#25913;&#21464;&#20102;&#36825;&#20010;&#29615;&#22659;&#12290;&#30001;&#20110;&#29289;&#29702;&#28023;&#20912;&#27169;&#22411;&#38656;&#35201;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#22797;&#26434;&#30340;&#21442;&#25968;&#21270;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#26377;&#25928;&#26367;&#20195;&#29289;&#29702;&#27169;&#22411;&#65292;&#24182;&#25552;&#39640;&#28023;&#20912;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#20840;&#21367;&#31215;&#32593;&#32476;&#26550;&#26500;&#65292;&#21517;&#20026;Hierarchical Information-Sharing U-Net (HIS-Unet)&#65292;&#29992;&#20110;&#39044;&#27979;&#27599;&#26085;&#30340;SIC&#21644;SID&#12290;&#25105;&#20204;&#36890;&#36807;&#21152;&#26435;&#27880;&#24847;&#21147;&#27169;&#22359;(WAMs)&#20801;&#35768;SIC&#21644;SID&#23618;&#20849;&#20139;&#20449;&#24687;&#65292;&#24182;&#20114;&#30456;&#36741;&#21161;&#39044;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#32479;&#35745;&#26041;&#27861;&#12289;&#28023;&#20912;&#29289;&#29702;&#27169;&#22411;&#21644;&#27809;&#26377;&#20449;&#24687;&#20849;&#20139;&#21333;&#20803;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#30340;HIS-Unet&#22312;SIC&#21644;SID&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#12290;&#22312;&#39044;&#27979;&#21271;&#20912;&#27915;&#28023;&#20912;&#27987;&#24230;&#21644;&#28418;&#31227;&#26041;&#38754;&#65292;HIS-Unet&#30340;&#25913;&#36827;&#37117;&#26159;&#26174;&#33879;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting sea ice concentration (SIC) and sea ice drift (SID) in the Arctic Ocean is of great significance as the Arctic environment has been changed by the recent warming climate. Given that physical sea ice models require high computational costs with complex parameterization, deep learning techniques can effectively replace the physical model and improve the performance of sea ice prediction. This study proposes a novel multi-task fully conventional network architecture named hierarchical information-sharing U-net (HIS-Unet) to predict daily SIC and SID. Instead of learning SIC and SID separately at each branch, we allow the SIC and SID layers to share their information and assist each other's prediction through the weighting attention modules (WAMs). Consequently, our HIS-Unet outperforms other statistical approaches, sea ice physical models, and neural networks without such information-sharing units. The improvement of HIS-Unet is obvious both for SIC and SID prediction when and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#19977;&#32500;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#20808;&#39564;&#21644;&#26174;&#33879;&#24615;&#22270;&#37325;&#26032;&#25773;&#25918;&#65292;&#23454;&#29616;&#23545;&#20043;&#21069;&#24050;&#35265;&#31867;&#21035;&#30340;&#21512;&#29702;&#37325;&#24314;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08812</link><description>&lt;p&gt;
&#21464;&#20998;&#20998;&#24067;&#20808;&#39564;&#19982;&#26174;&#33879;&#24615;&#22270;&#37325;&#26032;&#25773;&#25918;&#30340;&#34701;&#21512;&#65292;&#29992;&#20110;&#36830;&#32493;&#19977;&#32500;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
A Fusion of Variational Distribution Priors and Saliency Map Replay for Continual 3D Reconstruction. (arXiv:2308.08812v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#19977;&#32500;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#20808;&#39564;&#21644;&#26174;&#33879;&#24615;&#22270;&#37325;&#26032;&#25773;&#25918;&#65292;&#23454;&#29616;&#23545;&#20043;&#21069;&#24050;&#35265;&#31867;&#21035;&#30340;&#21512;&#29702;&#37325;&#24314;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#22270;&#20687;&#19977;&#32500;&#37325;&#24314;&#26159;&#30740;&#31350;&#22914;&#20309;&#26681;&#25454;&#21333;&#35270;&#35282;&#22270;&#20687;&#39044;&#27979;&#19977;&#32500;&#29289;&#20307;&#24418;&#29366;&#30340;&#19968;&#20010;&#25361;&#25112;&#12290;&#36825;&#20010;&#20219;&#21153;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#33719;&#21462;&#26469;&#39044;&#27979;&#24418;&#29366;&#30340;&#21487;&#35265;&#21644;&#36974;&#25377;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#38754;&#20020;&#21019;&#24314;&#38024;&#23545;&#25152;&#26377;&#21487;&#33021;&#31867;&#21035;&#30340;&#20840;&#38754;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#19977;&#32500;&#37325;&#24314;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#20351;&#29992;&#21464;&#20998;&#20808;&#39564;&#30340;&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#26032;&#31867;&#21035;&#21518;&#20173;&#21487;&#20197;&#21512;&#29702;&#37325;&#24314;&#20197;&#21069;&#35265;&#36807;&#30340;&#31867;&#21035;&#12290;&#21464;&#20998;&#20808;&#39564;&#20195;&#34920;&#25277;&#35937;&#24418;&#29366;&#24182;&#36991;&#20813;&#36951;&#24536;&#65292;&#32780;&#26174;&#33879;&#24615;&#22270;&#20197;&#36739;&#23569;&#30340;&#20869;&#23384;&#20351;&#29992;&#20445;&#30041;&#23545;&#35937;&#23646;&#24615;&#12290;&#36825;&#23545;&#20110;&#23384;&#20648;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#36164;&#28304;&#38480;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#26174;&#33879;&#24615;&#22270;&#30340;&#32463;&#39564;&#37325;&#25918;&#65292;&#20197;&#25429;&#25417;&#20840;&#23616;&#21644;&#29420;&#29305;&#30340;&#23545;&#35937;&#29305;&#24449;&#12290;&#35814;&#32454;&#30340;&#23454;&#39564;&#26174;&#31034;&#19982;&#24050;&#24314;&#31435;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single-image 3D reconstruction is a research challenge focused on predicting 3D object shapes from single-view images. This task requires significant data acquisition to predict both visible and occluded portions of the shape. Furthermore, learning-based methods face the difficulty of creating a comprehensive training dataset for all possible classes. To this end, we propose a continual learning-based 3D reconstruction method where our goal is to design a model using Variational Priors that can still reconstruct the previously seen classes reasonably even after training on new classes. Variational Priors represent abstract shapes and combat forgetting, whereas saliency maps preserve object attributes with less memory usage. This is vital due to resource constraints in storing extensive training data. Additionally, we introduce saliency map-based experience replay to capture global and distinct object features. Thorough experiments show competitive results compared to established method
&lt;/p&gt;</description></item></channel></rss>