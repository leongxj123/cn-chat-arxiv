<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#23558;&#19981;&#21516;&#27169;&#24577;&#35299;&#37322;&#20026;&#19981;&#21516;&#35821;&#35328;&#65292;&#22312;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#23454;&#29616;&#20102;&#19977;&#27169;&#32763;&#35793;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.16021</link><description>&lt;p&gt;
TMT: &#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#35270;&#20026;&#19981;&#21516;&#35821;&#35328;&#26469;&#23454;&#29616;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#19977;&#27169;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16021
&lt;/p&gt;
&lt;p&gt;
&#23558;&#19981;&#21516;&#27169;&#24577;&#35299;&#37322;&#20026;&#19981;&#21516;&#35821;&#35328;&#65292;&#22312;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#23454;&#29616;&#20102;&#19977;&#27169;&#32763;&#35793;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#20849;&#21516;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#27491;&#22312;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#37197;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#22823;&#37327;&#35745;&#31639;&#35201;&#27714;&#38459;&#30861;&#20102;&#21457;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#27169;&#32763;&#35793;&#65288;TMT&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#28085;&#30422;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20219;&#24847;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#28857;&#65292;&#21363;&#23558;&#19981;&#21516;&#27169;&#24577;&#35299;&#37322;&#20026;&#19981;&#21516;&#35821;&#35328;&#65292;&#24182;&#23558;&#22810;&#27169;&#24577;&#32763;&#35793;&#35270;&#20026;&#19968;&#20010;&#25104;&#29087;&#30340;&#26426;&#22120;&#32763;&#35793;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#35821;&#38899;&#21644;&#22270;&#20687;&#25968;&#25454;&#26631;&#35760;&#20026;&#31163;&#25955;&#26631;&#35760;&#65292;&#25552;&#20379;&#20102;&#36328;&#27169;&#24577;&#30340;&#32479;&#19968;&#25509;&#21475;&#65292;&#24182;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#25552;&#20986;&#30340;TMT&#20013;&#65292;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#36827;&#34892;&#26680;&#24515;&#32763;&#35793;&#65292;&#32780;&#27169;&#24577;&#29305;&#23450;&#22788;&#29702;&#20165;&#22312;&#26631;&#35760;&#21270;&#21644;&#21435;&#26631;&#35760;&#21270;&#38454;&#27573;&#20869;&#36827;&#34892;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;&#20845;&#31181;&#27169;&#24577;&#19978;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;TMT&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16021v1 Announce Type: cross  Abstract: The capability to jointly process multi-modal information is becoming an essential task. However, the limited number of paired multi-modal data and the large computational requirements in multi-modal learning hinder the development. We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning speech, image, and text. We introduce a novel viewpoint, where we interpret different modalities as different languages, and treat multi-modal translation as a well-established machine translation problem. To this end, we tokenize speech and image data into discrete tokens, which provide a unified interface across modalities and significantly decrease the computational cost. In the proposed TMT, a multi-modal encoder-decoder conducts the core translation, whereas modality-specific processing is conducted only within the tokenization and detokenization stages. We evaluate the proposed TMT on all six mod
&lt;/p&gt;</description></item><item><title>SARI&#26159;&#19968;&#20010;&#31616;&#32422;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22024;&#26434;&#37096;&#20998;&#26631;&#31614;&#65292;&#32467;&#21512;&#24179;&#22343;&#31574;&#30053;&#21644;&#35782;&#21035;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04835</link><description>&lt;p&gt;
SARI: &#31616;&#27905;&#24179;&#22343;&#19982;&#40065;&#26834;&#24615;&#22522;&#20110;&#22024;&#26434;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SARI: Simplistic Average and Robust Identification based Noisy Partial Label Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04835
&lt;/p&gt;
&lt;p&gt;
SARI&#26159;&#19968;&#20010;&#31616;&#32422;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22024;&#26434;&#37096;&#20998;&#26631;&#31614;&#65292;&#32467;&#21512;&#24179;&#22343;&#31574;&#30053;&#21644;&#35782;&#21035;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#26631;&#31614;&#23398;&#20064; (PLL) &#26159;&#19968;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#37117;&#19982;&#19968;&#32452;&#20505;&#36873;&#26631;&#31614; (&#37096;&#20998;&#26631;&#31614;) &#25104;&#23545;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;&#30495;&#27491;&#30340;&#26631;&#31614;&#12290;&#22024;&#26434;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064; (NPLL) &#25918;&#23485;&#20102;&#36825;&#20010;&#32422;&#26463;&#65292;&#20801;&#35768;&#19968;&#20123;&#37096;&#20998;&#26631;&#31614;&#19981;&#21253;&#21547;&#30495;&#27491;&#30340;&#26631;&#31614;&#65292;&#22686;&#21152;&#20102;&#38382;&#39064;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312; NPLL &#19978;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#32422;&#30340;&#26694;&#26550; SARI&#65292;&#36890;&#36807;&#21033;&#29992;&#21152;&#26435;&#26368;&#36817;&#37051;&#31639;&#27861;&#23558;&#20266;&#26631;&#31614;&#20998;&#37197;&#32473;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#20266;&#26631;&#31614;&#19982;&#22270;&#20687;&#37197;&#23545;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65292;&#37319;&#29992;&#26631;&#31614;&#24179;&#28369;&#21644;&#26631;&#20934;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#38543;&#21518;&#65292;&#21033;&#29992;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#21644;&#39044;&#27979;&#32467;&#26524;&#26469;&#25913;&#36827;&#21644;&#25552;&#39640;&#20266;&#26631;&#31614;&#30340;&#20934;&#30830;&#24615;&#12290;SARI&#32467;&#21512;&#20102;&#25991;&#29486;&#20013;&#22522;&#20110;&#24179;&#22343;&#31574;&#30053; (&#20266;&#26631;&#31614;) &#21644;&#22522;&#20110;&#35782;&#21035;&#31574;&#30053; (&#20998;&#31867;&#22120;&#35757;&#32451;)&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;SARI&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial label learning (PLL) is a weakly-supervised learning paradigm where each training instance is paired with a set of candidate labels (partial label), one of which is the true label. Noisy PLL (NPLL) relaxes this constraint by allowing some partial labels to not contain the true label, enhancing the practicality of the problem. Our work centers on NPLL and presents a minimalistic framework called SARI that initially assigns pseudo-labels to images by exploiting the noisy partial labels through a weighted nearest neighbour algorithm. These pseudo-label and image pairs are then used to train a deep neural network classifier with label smoothing and standard regularization techniques. The classifier's features and predictions are subsequently employed to refine and enhance the accuracy of pseudo-labels. SARI combines the strengths of Average Based Strategies (in pseudo labelling) and Identification Based Strategies (in classifier training) from the literature. We perform thorough ex
&lt;/p&gt;</description></item></channel></rss>