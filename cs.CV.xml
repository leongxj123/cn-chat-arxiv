<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.16970</link><description>&lt;p&gt;
&#32852;&#21512;&#33016;&#37096;X&#20809;&#35786;&#26029;&#21644;&#20020;&#24202;&#35270;&#35273;&#27880;&#24847;&#21147;&#39044;&#27979;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#23398;&#20064;&#65306;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16970
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#33258;&#21160;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20020;&#24202;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#36825;&#19968;&#39046;&#22495;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#20294;&#22312;&#25918;&#23556;&#23398;&#31579;&#26597;&#36807;&#31243;&#20013;&#20020;&#24202;&#21307;&#29983;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#22270;&#20026;&#25552;&#20379;&#37325;&#35201;&#27934;&#23519;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#36164;&#20135;&#65292;&#24182;&#26377;&#21487;&#33021;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#36825;&#31687;&#35770;&#25991;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#65292;&#21033;&#29992;&#20102;DenseNet201&#20027;&#24178;&#21644;&#22522;&#20110;&#27531;&#24046;&#21644;&#33192;&#32960;&#28608;&#21169;&#22359;&#30340;&#32534;&#30721;&#22120;&#26469;&#25552;&#21462;&#29992;&#20110;&#26174;&#33879;&#24615;&#22270;&#39044;&#27979;&#30340;&#22810;&#26679;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#36827;&#34892;&#30142;&#30149;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16970v1 Announce Type: cross  Abstract: As deep learning has become the state-of-the-art for computer-assisted diagnosis, interpretability of the automatic decisions is crucial for clinical deployment. While various methods were proposed in this domain, visual attention maps of clinicians during radiological screening offer a unique asset to provide important insights and can potentially enhance the quality of computer-assisted diagnosis. With this paper, we introduce a novel deep-learning framework for joint disease diagnosis and prediction of corresponding visual saliency maps for chest X-ray scans. Specifically, we designed a novel dual-encoder multi-task UNet, which leverages both a DenseNet201 backbone and a Residual and Squeeze-and-Excitation block-based encoder to extract diverse features for saliency map prediction, and a multi-scale feature-fusion classifier to perform disease classification. To tackle the issue of asynchronous training schedules of individual tasks
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Reduced Label&#30340;&#26032;&#22411;&#24369;&#30417;&#30563;&#26631;&#31614;&#35774;&#32622;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#38271;&#23614;&#25968;&#25454;&#65292;&#36991;&#20813;&#20102;&#23614;&#37096;&#26679;&#26412;&#30417;&#30563;&#20449;&#24687;&#30340;&#19979;&#38477;&#65292;&#38477;&#20302;&#20102;&#26631;&#31614;&#25104;&#26412;</title><link>https://arxiv.org/abs/2403.16469</link><description>&lt;p&gt;
&#23398;&#20064;&#20174;&#20943;&#23569;&#26631;&#31614;&#30340;&#38271;&#23614;&#25968;&#25454;&#20013;
&lt;/p&gt;
&lt;p&gt;
Learning from Reduced Labels for Long-Tailed Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16469
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Reduced Label&#30340;&#26032;&#22411;&#24369;&#30417;&#30563;&#26631;&#31614;&#35774;&#32622;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#38271;&#23614;&#25968;&#25454;&#65292;&#36991;&#20813;&#20102;&#23614;&#37096;&#26679;&#26412;&#30417;&#30563;&#20449;&#24687;&#30340;&#19979;&#38477;&#65292;&#38477;&#20302;&#20102;&#26631;&#31614;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#25968;&#25454;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#30417;&#30563;&#20449;&#24687;&#65292;&#36825;&#20351;&#24471;&#27880;&#37322;&#36807;&#31243;&#24322;&#24120;&#32791;&#26102;&#19988;&#36153;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#26159;&#32531;&#35299;&#26631;&#31614;&#25104;&#26412;&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24456;&#38590;&#20805;&#20998;&#20445;&#30041;&#23614;&#37096;&#26679;&#26412;&#30340;&#30417;&#30563;&#20449;&#24687;&#65292;&#23548;&#33268;&#23614;&#37096;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Reduced Label&#30340;&#26032;&#22411;&#24369;&#30417;&#30563;&#26631;&#31614;&#35774;&#32622;&#12290;&#25152;&#25552;&#20986;&#30340;&#26631;&#31614;&#35774;&#32622;&#19981;&#20165;&#36991;&#20813;&#20102;&#23614;&#37096;&#26679;&#26412;&#30340;&#30417;&#30563;&#20449;&#24687;&#19979;&#38477;&#65292;&#36824;&#20943;&#23569;&#20102;&#19982;&#38271;&#23614;&#25968;&#25454;&#30456;&#20851;&#30340;&#26631;&#31614;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30452;&#35266;&#19988;&#39640;&#25928;&#30340;&#26080;&#20559;&#26694;&#26550;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#21487;&#20197;&#20174;&#36825;&#20123;Reduced Labels&#20013;&#23398;&#20064;&#12290;&#22312;&#21253;&#25324;Imag&#22312;&#20869;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16469v1 Announce Type: new  Abstract: Long-tailed data is prevalent in real-world classification tasks and heavily relies on supervised information, which makes the annotation process exceptionally labor-intensive and time-consuming. Unfortunately, despite being a common approach to mitigate labeling costs, existing weakly supervised learning methods struggle to adequately preserve supervised information for tail samples, resulting in a decline in accuracy for the tail classes. To alleviate this problem, we introduce a novel weakly supervised labeling setting called Reduced Label. The proposed labeling setting not only avoids the decline of supervised information for the tail samples, but also decreases the labeling costs associated with long-tailed data. Additionally, we propose an straightforward and highly efficient unbiased framework with strong theoretical guarantees to learn from these Reduced Labels. Extensive experiments conducted on benchmark datasets including Imag
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31526;&#21495;&#35270;&#39057;&#25628;&#32034;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#29366;&#24577;&#26426;&#21644;&#26102;&#38388;&#36923;&#36753;&#20844;&#24335;&#23545;&#20107;&#20214;&#30340;&#38271;&#26399;&#28436;&#21464;&#36827;&#34892;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#22330;&#26223;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.11021</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#35270;&#39057;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic Video Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11021
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31526;&#21495;&#35270;&#39057;&#25628;&#32034;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#29366;&#24577;&#26426;&#21644;&#26102;&#38388;&#36923;&#36753;&#20844;&#24335;&#23545;&#20107;&#20214;&#30340;&#38271;&#26399;&#28436;&#21464;&#36827;&#34892;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#22330;&#26223;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#35270;&#39057;&#25968;&#25454;&#29983;&#20135;&#30340;&#31354;&#21069;&#28608;&#22686;&#38656;&#27714;&#39640;&#25928;&#30340;&#24037;&#20855;&#65292;&#20197;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#24103;&#20379;&#19979;&#28216;&#20219;&#21153;&#20351;&#29992;&#12290; &#38271;&#26399;&#26102;&#38388;&#25512;&#29702;&#26159;&#24103;&#26816;&#32034;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#35201;&#27714;&#12290; &#34429;&#28982; VideoLLaMA &#21644; ViCLIP &#31561;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#30701;&#26399;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23427;&#20204;&#22312;&#36328;&#24103;&#30340;&#38271;&#26399;&#25512;&#29702;&#26041;&#38754;&#21364;&#20196;&#20154;&#24778;&#35766;&#22320;&#22833;&#36133;&#12290; &#36825;&#31181;&#22833;&#36133;&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#26159;&#23427;&#20204;&#23558;&#36880;&#24103;&#24863;&#30693;&#21644;&#26102;&#38388;&#25512;&#29702;&#20132;&#32455;&#25104;&#21333;&#20010;&#28145;&#24230;&#32593;&#32476;&#12290; &#22240;&#27492;&#65292;&#35299;&#32806;&#20294;&#20849;&#21516;&#35774;&#35745;&#35821;&#20041;&#29702;&#35299;&#21644;&#26102;&#38388;&#25512;&#29702;&#23545;&#20110;&#39640;&#25928;&#30340;&#22330;&#26223;&#35782;&#21035;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#21333;&#20010;&#24103;&#36827;&#34892;&#35821;&#20041;&#29702;&#35299;&#65292;&#20294;&#26377;&#25928;&#22320;&#36890;&#36807;&#20351;&#29992;&#29366;&#24577;&#26426;&#21644;&#26102;&#38388;&#36923;&#36753;&#65288;TL&#65289;&#20844;&#24335;&#23545;&#20107;&#20214;&#30340;&#38271;&#26399;&#28436;&#21464;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#20123;&#20844;&#24335;&#22312;&#26412;&#36136;&#19978;&#25429;&#25417;&#20102;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11021v1 Announce Type: cross  Abstract: The unprecedented surge in video data production in recent years necessitates efficient tools to extract meaningful frames from videos for downstream tasks. Long-term temporal reasoning is a key desideratum for frame retrieval systems. While state-of-the-art foundation models, like VideoLLaMA and ViCLIP, are proficient in short-term semantic understanding, they surprisingly fail at long-term reasoning across frames. A key reason for this failure is that they intertwine per-frame perception and temporal reasoning into a single deep network. Hence, decoupling but co-designing semantic understanding and temporal reasoning is essential for efficient scene identification. We propose a system that leverages vision-language models for semantic understanding of individual frames but effectively reasons about the long-term evolution of events using state machines and temporal logic (TL) formulae that inherently capture memory. Our TL-based reas
&lt;/p&gt;</description></item></channel></rss>