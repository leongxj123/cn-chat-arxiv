<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PrimeComposer&#65292;&#19968;&#31181;&#26356;&#24555;&#30340;&#36880;&#27493;&#32452;&#21512;&#25193;&#25955;&#26041;&#24335;&#65292;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#65292;&#20027;&#35201;&#19987;&#27880;&#20110;&#21069;&#26223;&#29983;&#25104;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21512;&#25104;&#20013;&#30340;&#20957;&#32858;&#28151;&#20081;&#21644;&#22806;&#35266;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#32972;&#26223;&#29983;&#25104;&#23548;&#33268;&#30340;&#21069;&#26223;&#29983;&#25104;&#36136;&#37327;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.05053</link><description>&lt;p&gt;
PrimeComposer&#65306;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#30340;&#24555;&#36895;&#36880;&#27493;&#32452;&#21512;&#25193;&#25955;&#26041;&#27861;&#21644;&#24102;&#26377;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PrimeComposer: Faster Progressively Combined Diffusion for Image Composition with Attention Steering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PrimeComposer&#65292;&#19968;&#31181;&#26356;&#24555;&#30340;&#36880;&#27493;&#32452;&#21512;&#25193;&#25955;&#26041;&#24335;&#65292;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#65292;&#20027;&#35201;&#19987;&#27880;&#20110;&#21069;&#26223;&#29983;&#25104;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21512;&#25104;&#20013;&#30340;&#20957;&#32858;&#28151;&#20081;&#21644;&#22806;&#35266;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#32972;&#26223;&#29983;&#25104;&#23548;&#33268;&#30340;&#21069;&#26223;&#29983;&#25104;&#36136;&#37327;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21512;&#25104;&#28041;&#21450;&#23558;&#32473;&#23450;&#23545;&#35937;&#26080;&#32541;&#22320;&#25972;&#21512;&#21040;&#29305;&#23450;&#30340;&#35270;&#35273;&#29615;&#22659;&#20013;&#12290;&#30446;&#21069;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20174;&#20960;&#20010;&#37319;&#26679;&#22120;&#20013;&#32452;&#21512;&#27880;&#24847;&#21147;&#26435;&#37325;&#26469;&#24341;&#23548;&#29983;&#25104;&#22120;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#26435;&#37325;&#26469;&#33258;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#65292;&#23427;&#20204;&#30340;&#32452;&#21512;&#23548;&#33268;&#22312;&#21512;&#25104;&#20013;&#20957;&#32858;&#28151;&#20081;&#21644;&#22806;&#35266;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#36807;&#22810;&#20851;&#27880;&#32972;&#26223;&#29983;&#25104;&#65292;&#21363;&#20351;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#26159;&#19981;&#24517;&#35201;&#30340;&#65292;&#36825;&#20123;&#38382;&#39064;&#24694;&#21270;&#12290;&#36825;&#19981;&#20165;&#20943;&#24930;&#20102;&#25512;&#29702;&#36895;&#24230;&#65292;&#36824;&#25439;&#23475;&#20102;&#21069;&#26223;&#29983;&#25104;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36824;&#22312;&#36807;&#28193;&#21306;&#22495;&#24341;&#20837;&#20102;&#19981;&#38656;&#35201;&#30340;&#20266;&#24433;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#21512;&#25104;&#24418;&#24335;&#21270;&#20026;&#19968;&#39033;&#22522;&#20110;&#20027;&#39064;&#30340;&#23616;&#37096;&#32534;&#36753;&#20219;&#21153;&#65292;&#20165;&#19987;&#27880;&#20110;&#21069;&#26223;&#29983;&#25104;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;&#32534;&#36753;&#21518;&#30340;&#21069;&#26223;&#19982;&#22122;&#22768;&#32972;&#26223;&#30456;&#32467;&#21512;&#65292;&#20197;&#20445;&#25345;&#22330;&#26223;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#21097;&#19979;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PrimeComposer&#65292;&#19968;&#31181;&#26356;&#24555;&#30340;tr
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05053v1 Announce Type: cross  Abstract: Image composition involves seamlessly integrating given objects into a specific visual context. The current training-free methods rely on composing attention weights from several samplers to guide the generator. However, since these weights are derived from disparate contexts, their combination leads to coherence confusion in synthesis and loss of appearance information. These issues worsen with their excessive focus on background generation, even when unnecessary in this task. This not only slows down inference but also compromises foreground generation quality. Moreover, these methods introduce unwanted artifacts in the transition area. In this paper, we formulate image composition as a subject-based local editing task, solely focusing on foreground generation. At each step, the edited foreground is combined with the noisy background to maintain scene consistency. To address the remaining issues, we propose PrimeComposer, a faster tr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30828;&#36127;&#26679;&#26412;&#25913;&#36827;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20013;&#27010;&#24565;&#29702;&#35299;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#39068;&#33394;&#12289;&#23545;&#35937;&#21644;&#22823;&#23567;&#32454;&#31890;&#24230;&#23545;&#40784;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.02875</link><description>&lt;p&gt;
&#36890;&#36807;&#30828;&#36127;&#26679;&#26412;&#22686;&#24378;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#27010;&#24565;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30828;&#36127;&#26679;&#26412;&#25913;&#36827;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20013;&#27010;&#24565;&#29702;&#35299;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#39068;&#33394;&#12289;&#23545;&#35937;&#21644;&#22823;&#23567;&#32454;&#31890;&#24230;&#23545;&#40784;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#21457;&#23637;&#31934;&#32454;&#30340;&#27010;&#24565;&#29702;&#35299;&#26041;&#38754;&#36890;&#24120;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#30001;&#20110;&#38543;&#26426;&#36127;&#26679;&#26412;&#65292;&#23548;&#33268;&#20960;&#20046;&#21482;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#27010;&#24565;&#36827;&#34892;&#25439;&#22833;&#20989;&#25968;&#27604;&#36739;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#32454;&#31890;&#24230;&#35821;&#20041;&#24046;&#24322;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21512;&#25104;&#30340;&#30828;&#36127;&#25991;&#26412;&#31034;&#20363;&#12290;&#36825;&#20123;&#30828;&#36127;&#26679;&#26412;&#23545;&#24212;&#20110;&#35270;&#35273;&#27010;&#24565;&#30340;&#25490;&#21015;&#65292;&#23548;&#33268;&#26356;&#31934;&#32454;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#27010;&#24565;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;InpaintCOCO&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#39068;&#33394;&#12289;&#23545;&#35937;&#21644;&#22823;&#23567;&#32454;&#31890;&#24230;&#23545;&#40784;&#30340;&#26032;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;COCO&#22270;&#20687;&#29983;&#25104;&#30340;&#20449;&#24687;&#22635;&#20805;&#26469;&#21019;&#24314;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25913;&#21464;&#35270;&#35273;&#27010;&#24565;&#65292;&#20351;&#22270;&#20687;&#19981;&#20877;&#19982;&#20854;&#21407;&#22987;&#26631;&#39064;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02875v1 Announce Type: cross  Abstract: Current multimodal models leveraging contrastive learning often face limitations in developing fine-grained conceptual understanding. This is due to random negative samples during pretraining, causing almost exclusively very dissimilar concepts to be compared in the loss function. Consequently, the models struggle with fine-grained semantic differences. To address this problem, we introduce a novel pretraining method incorporating synthetic hard negative text examples. The hard negatives permute terms corresponding to visual concepts, leading to a more fine-grained visual and textual concept alignment. Further, we introduce InpaintCOCO, a new challenging dataset for assessing the fine-grained alignment of colors, objects, and sizes in vision-language models. We created the dataset using generative inpainting from COCO images by changing the visual concepts so that the images no longer match their original captions. Our results show sig
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22359;&#65292;&#31216;&#20026;NiNformer&#65292;&#20855;&#26377;&#20196;&#29260;&#28151;&#21512;&#29983;&#25104;&#38376;&#25511;&#21151;&#33021;&#65292;&#20197;&#35299;&#20915;&#27880;&#24847;&#26426;&#21046;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#21644;&#25968;&#25454;&#38598;&#35201;&#27714;&#22823;&#30340;&#32570;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.02411</link><description>&lt;p&gt;
NiNformer: &#19968;&#31181;&#20855;&#26377;&#20196;&#29260;&#28151;&#21512;&#29983;&#25104;&#38376;&#25511;&#21151;&#33021;&#30340;&#32593;&#32476;&#20013;&#32593;&#32476;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
NiNformer: A Network in Network Transformer with Token Mixing Generated Gating Function
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02411
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22359;&#65292;&#31216;&#20026;NiNformer&#65292;&#20855;&#26377;&#20196;&#29260;&#28151;&#21512;&#29983;&#25104;&#38376;&#25511;&#21151;&#33021;&#65292;&#20197;&#35299;&#20915;&#27880;&#24847;&#26426;&#21046;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#21644;&#25968;&#25454;&#38598;&#35201;&#27714;&#22823;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#26426;&#21046;&#26159;Transformer&#26550;&#26500;&#30340;&#20027;&#35201;&#32452;&#20214;&#65292;&#33258;&#24341;&#20837;&#20197;&#26469;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36328;&#36234;&#20102;&#35768;&#22810;&#39046;&#22495;&#21644;&#22810;&#20010;&#20219;&#21153;&#12290;&#35813;&#26426;&#21046;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#34987;&#24212;&#29992;&#20026;Vision Transformer ViT&#65292;&#24182;&#19988;&#20854;&#29992;&#36884;&#24050;&#25193;&#23637;&#21040;&#35270;&#35273;&#39046;&#22495;&#30340;&#35768;&#22810;&#20219;&#21153;&#65292;&#22914;&#20998;&#31867;&#12289;&#20998;&#21106;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#22270;&#20687;&#29983;&#25104;&#12290;&#23613;&#31649;&#35813;&#26426;&#21046;&#38750;&#24120;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#33021;&#21147;&#65292;&#20294;&#20854;&#32570;&#28857;&#26159;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#38656;&#35201;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26469;&#26377;&#25928;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#35774;&#35745;&#26469;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#21644;&#32531;&#35299;&#25968;&#25454;&#22823;&#23567;&#35201;&#27714;&#12290;&#22312;&#35270;&#35273;&#39046;&#22495;&#30340;&#19968;&#20123;&#23581;&#35797;&#30340;&#20363;&#23376;&#21253;&#25324;MLP-Mixer&#12289;Conv-Mixer&#12289;Perciver-IO&#31561;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22359;&#65292;&#20316;&#20026;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02411v1 Announce Type: cross  Abstract: The Attention mechanism is the main component of the Transformer architecture, and since its introduction, it has led to significant advancements in Deep Learning that span many domains and multiple tasks. The Attention Mechanism was utilized in Computer Vision as the Vision Transformer ViT, and its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation. While this mechanism is very expressive and capable, it comes with the drawback of being computationally expensive and requiring datasets of considerable size for effective optimization. To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. Examples of such attempts in the vision domain are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper introduces a new computational block as an 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38646;-shot&#20998;&#31867;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#22914;&#24694;&#24847;&#27169;&#22240;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2402.12198</link><description>&lt;p&gt;
&#38646;-shot &#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#65306;&#25105;&#20204;&#24050;&#32463;&#21040;&#36798;&#30446;&#26631;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Zero shot VLMs for hate meme detection: Are we there yet?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38646;-shot&#20998;&#31867;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#22914;&#24694;&#24847;&#27169;&#22240;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#20854;&#20013;&#27169;&#22240;&#20316;&#20026;&#19968;&#31181;&#29420;&#29305;&#24418;&#24335;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#19968;&#20123;&#24694;&#24847;&#29992;&#25143;&#21033;&#29992;&#27169;&#22240;&#38024;&#23545;&#20010;&#20154;&#25110;&#26131;&#21463;&#25915;&#20987;&#30340;&#31038;&#21306;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#35782;&#21035;&#21644;&#35299;&#20915;&#27492;&#31867;&#24694;&#24847;&#27169;&#22240;&#12290;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#24320;&#21457;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#20010;&#26174;&#33879;&#23616;&#38480;&#24615;&#26159;&#38656;&#35201;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#25165;&#33021;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#30028;&#35265;&#35777;&#20102;&#20960;&#31181;&#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#36825;&#20123;&#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#35832;&#22914;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#31561;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#35774;&#32622;&#26469;&#19987;&#27880;&#20110;&#23545;&#24694;&#24847;/&#26377;&#23475;&#27169;&#22240;&#30340;&#38646;-shot &#20998;&#31867;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12198v1 Announce Type: new  Abstract: Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification. Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection. We use various prompt settings to focus on zero-shot classification of hateful/harmful memes. Through our analysis, we o
&lt;/p&gt;</description></item><item><title>AONeuS&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#22810;&#27169;&#24577;&#22768;&#20809;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#39640;&#20998;&#36776;&#29575;RGB&#27979;&#37327;&#21644;&#20302;&#20998;&#36776;&#29575;&#28145;&#24230;&#25104;&#20687;&#22768;&#32435;&#27979;&#37327;&#65292;&#33021;&#22815;&#22312;&#21463;&#38480;&#22522;&#32447;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#39640;&#20998;&#36776;&#29575;&#19977;&#32500;&#34920;&#38754;&#37325;&#24314;&#12290;</title><link>https://arxiv.org/abs/2402.03309</link><description>&lt;p&gt;
AONeuS: &#19968;&#31181;&#29992;&#20110;&#22768;&#20809;&#20256;&#24863;&#22120;&#34701;&#21512;&#30340;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03309
&lt;/p&gt;
&lt;p&gt;
AONeuS&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#22810;&#27169;&#24577;&#22768;&#20809;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#39640;&#20998;&#36776;&#29575;RGB&#27979;&#37327;&#21644;&#20302;&#20998;&#36776;&#29575;&#28145;&#24230;&#25104;&#20687;&#22768;&#32435;&#27979;&#37327;&#65292;&#33021;&#22815;&#22312;&#21463;&#38480;&#22522;&#32447;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#39640;&#20998;&#36776;&#29575;&#19977;&#32500;&#34920;&#38754;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#19979;&#24863;&#30693;&#21644;&#19977;&#32500;&#34920;&#38754;&#37325;&#24314;&#26159;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#28041;&#21450;&#24314;&#31569;&#12289;&#23433;&#20840;&#12289;&#28023;&#27915;&#32771;&#21476;&#21644;&#29615;&#22659;&#30417;&#27979;&#31561;&#39046;&#22495;&#12290;&#24694;&#21155;&#30340;&#25805;&#20316;&#26465;&#20214;&#12289;&#33030;&#24369;&#30340;&#29615;&#22659;&#21644;&#26377;&#38480;&#30340;&#23548;&#33322;&#25511;&#21046;&#36890;&#24120;&#23548;&#33268;&#27700;&#19979;&#33322;&#34892;&#22120;&#38480;&#21046;&#20854;&#36816;&#21160;&#33539;&#22260;&#21644;&#27979;&#37327;&#22522;&#32447;&#12290;&#22312;&#19977;&#32500;&#22330;&#26223;&#37325;&#24314;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30693;&#36947;&#36739;&#23567;&#30340;&#22522;&#32447;&#20250;&#22686;&#21152;&#37325;&#24314;&#38590;&#24230;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#22810;&#27169;&#24577;&#22768;&#20809;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#26694;&#26550;&#65288;AONeuS&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#39640;&#20998;&#36776;&#29575;RGB&#27979;&#37327;&#19982;&#20302;&#20998;&#36776;&#29575;&#28145;&#24230;&#25104;&#20687;&#22768;&#32435;&#27979;&#37327;&#36827;&#34892;&#34701;&#21512;&#12290;&#36890;&#36807;&#34701;&#21512;&#36825;&#20123;&#20114;&#34917;&#30340;&#27169;&#24577;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20174;&#22312;&#21463;&#38480;&#22522;&#32447;&#19978;&#25429;&#33719;&#30340;&#27979;&#37327;&#20013;&#37325;&#24314;&#20986;&#20934;&#30830;&#30340;&#39640;&#20998;&#36776;&#29575;&#19977;&#32500;&#34920;&#38754;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#21644;&#23454;&#39564;&#23460;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Underwater perception and 3D surface reconstruction are challenging problems with broad applications in construction, security, marine archaeology, and environmental monitoring. Treacherous operating conditions, fragile surroundings, and limited navigation control often dictate that submersibles restrict their range of motion and, thus, the baseline over which they can capture measurements. In the context of 3D scene reconstruction, it is well-known that smaller baselines make reconstruction more challenging. Our work develops a physics-based multimodal acoustic-optical neural surface reconstruction framework (AONeuS) capable of effectively integrating high-resolution RGB measurements with low-resolution depth-resolved imaging sonar measurements. By fusing these complementary modalities, our framework can reconstruct accurate high-resolution 3D surfaces from measurements captured over heavily-restricted baselines. Through extensive simulations and in-lab experiments, we demonstrate tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;PCA&#21644;&#20854;&#21464;&#31181;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#32447;&#24615;&#23376;&#31354;&#38388;&#26071;&#24092;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#24322;&#24120;&#20540;&#21644;&#25968;&#25454;&#27969;&#24418;&#30340;&#32771;&#34385;&#12290;&#36890;&#36807;&#22312;&#26071;&#24092;&#27969;&#24418;&#19978;&#36827;&#34892;&#20248;&#21270;&#38382;&#39064;&#30340;&#27714;&#35299;&#65292;&#32467;&#21512;&#20027;&#27979;&#22320;&#32447;&#36817;&#20284;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#38477;&#32500;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04071</link><description>&lt;p&gt;
&#26071;&#24092;&#28216;&#25103;&#65306;&#36890;&#36807;&#26071;&#24092;&#27969;&#24418;&#26469;&#33719;&#24471;&#40065;&#26834;&#30340;&#20027;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Fun with Flags: Robust Principal Directions via Flag Manifolds. (arXiv:2401.04071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;PCA&#21644;&#20854;&#21464;&#31181;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#32447;&#24615;&#23376;&#31354;&#38388;&#26071;&#24092;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#24322;&#24120;&#20540;&#21644;&#25968;&#25454;&#27969;&#24418;&#30340;&#32771;&#34385;&#12290;&#36890;&#36807;&#22312;&#26071;&#24092;&#27969;&#24418;&#19978;&#36827;&#34892;&#20248;&#21270;&#38382;&#39064;&#30340;&#27714;&#35299;&#65292;&#32467;&#21512;&#20027;&#27979;&#22320;&#32447;&#36817;&#20284;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#38477;&#32500;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21450;&#20854;&#23545;&#27969;&#24418;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#25193;&#23637;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PCA&#21450;&#20854;&#21464;&#31181;&#30340;&#32479;&#19968;&#24418;&#24335;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#32447;&#24615;&#23376;&#31354;&#38388;&#26071;&#24092;&#30340;&#26694;&#26550;&#65292;&#21363;&#36880;&#28176;&#22686;&#21152;&#32500;&#24230;&#30340;&#23884;&#22871;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#19981;&#20165;&#20801;&#35768;&#20849;&#21516;&#23454;&#29616;&#65292;&#36824;&#20135;&#29983;&#20102;&#26032;&#30340;&#26410;&#26366;&#25506;&#32034;&#30340;&#21464;&#31181;&#12290;&#25105;&#20204;&#20174;&#24191;&#20041;&#21270;&#20256;&#32479;&#30340;PCA&#26041;&#27861;&#24320;&#22987;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#26368;&#22823;&#21270;&#26041;&#24046;&#65292;&#35201;&#20040;&#26368;&#23567;&#21270;&#37325;&#26500;&#35823;&#24046;&#12290;&#25105;&#20204;&#25193;&#23637;&#36825;&#20123;&#35299;&#37322;&#65292;&#36890;&#36807;&#32771;&#34385;&#24322;&#24120;&#20540;&#21644;&#25968;&#25454;&#27969;&#24418;&#65292;&#24320;&#21457;&#20986;&#20102;&#22823;&#37327;&#26032;&#30340;&#38477;&#32500;&#31639;&#27861;&#12290;&#20026;&#20102;&#35774;&#35745;&#19968;&#31181;&#36890;&#29992;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#40065;&#26834;&#21644;&#23545;&#20598;&#24418;&#24335;&#30340;PCA&#37325;&#26032;&#26500;&#24314;&#20026;&#22312;&#26071;&#24092;&#27969;&#24418;&#19978;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20027;&#27979;&#22320;&#32447;&#36817;&#20284;&#65288;&#20999;&#32447;PCA&#65289;&#25972;&#21512;&#21040;&#36825;&#20010;&#22522;&#20110;&#26071;&#24092;&#30340;&#26694;&#26550;&#20013;&#65292;&#21019;&#36896;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Principal component analysis (PCA), along with its extensions to manifolds and outlier contaminated data, have been indispensable in computer vision and machine learning. In this work, we present a unifying formalism for PCA and its variants, and introduce a framework based on the flags of linear subspaces, \ie a hierarchy of nested linear subspaces of increasing dimension, which not only allows for a common implementation but also yields novel variants, not explored previously. We begin by generalizing traditional PCA methods that either maximize variance or minimize reconstruction error. We expand these interpretations to develop a wide array of new dimensionality reduction algorithms by accounting for outliers and the data manifold. To devise a common computational approach, we recast robust and dual forms of PCA as optimization problems on flag manifolds. We then integrate tangent space approximations of principal geodesic analysis (tangent-PCA) into this flag-based framework, crea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26080;&#27861;&#35775;&#38382;&#27169;&#22411;&#26550;&#26500;&#21644;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25805;&#32437;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLMs)&#12290;&#36890;&#36807;&#20248;&#21270;&#36890;&#29992;&#23545;&#25239;&#25552;&#31034;&#19982;&#29992;&#25143;&#26597;&#35810;&#32467;&#21512;&#65292;&#21487;&#20197;&#25200;&#20081;&#34987;&#25915;&#20987;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#23548;&#33268;&#24847;&#22806;&#21644;&#28508;&#22312;&#26377;&#23475;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25581;&#31034;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#21644;&#28431;&#27934;&#65292;&#20026;&#36127;&#36131;&#20219;&#30340;AI&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35786;&#26029;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2309.01446</link><description>&lt;p&gt;
&#24320;&#38376;&#21543;&#65281;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#40657;&#30418;&#30772;&#35299;
&lt;/p&gt;
&lt;p&gt;
Open Sesame! Universal Black Box Jailbreaking of Large Language Models. (arXiv:2309.01446v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26080;&#27861;&#35775;&#38382;&#27169;&#22411;&#26550;&#26500;&#21644;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25805;&#32437;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLMs)&#12290;&#36890;&#36807;&#20248;&#21270;&#36890;&#29992;&#23545;&#25239;&#25552;&#31034;&#19982;&#29992;&#25143;&#26597;&#35810;&#32467;&#21512;&#65292;&#21487;&#20197;&#25200;&#20081;&#34987;&#25915;&#20987;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#23548;&#33268;&#24847;&#22806;&#21644;&#28508;&#22312;&#26377;&#23475;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25581;&#31034;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#21644;&#28431;&#27934;&#65292;&#20026;&#36127;&#36131;&#20219;&#30340;AI&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35786;&#26029;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26088;&#22312;&#25552;&#20379;&#26377;&#24110;&#21161;&#21644;&#23433;&#20840;&#30340;&#22238;&#22797;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#40784;&#25216;&#26415;&#19982;&#29992;&#25143;&#24847;&#22270;&#21644;&#31038;&#20250;&#25351;&#21335;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23545;&#40784;&#21487;&#33021;&#20250;&#34987;&#24694;&#24847;&#34892;&#20026;&#32773;&#21033;&#29992;&#65292;&#20197;&#29992;&#20110;&#24847;&#24819;&#19981;&#21040;&#30340;&#30446;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#22312;&#27169;&#22411;&#26550;&#26500;&#21644;&#21442;&#25968;&#19981;&#21487;&#35775;&#38382;&#26102;&#25805;&#32437;LLMs&#12290;GA&#25915;&#20987;&#36890;&#36807;&#20248;&#21270;&#36890;&#29992;&#23545;&#25239;&#25552;&#31034;&#19982;&#29992;&#25143;&#26597;&#35810;&#32467;&#21512;&#65292;&#25200;&#20081;&#34987;&#25915;&#20987;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#23548;&#33268;&#24847;&#22806;&#21644;&#28508;&#22312;&#26377;&#23475;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#26041;&#27861;&#36890;&#36807;&#25581;&#31034;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#21644;&#28431;&#27934;&#65292;&#31995;&#32479;&#22320;&#25581;&#31034;&#20102;&#20854;&#21709;&#24212;&#19982;&#39044;&#26399;&#34892;&#20026;&#19981;&#31526;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#20026;&#20851;&#20110;&#36127;&#36131;&#20219;&#30340;AI&#24320;&#21457;&#30340;&#35752;&#35770;&#25552;&#20379;&#20102;&#19968;&#31181;&#35786;&#26029;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), designed to provide helpful and safe responses, often rely on alignment techniques to align with user intent and social guidelines. Unfortunately, this alignment can be exploited by malicious actors seeking to manipulate an LLM's outputs for unintended purposes. In this paper we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible. The GA attack works by optimizing a universal adversarial prompt that -- when combined with a user's query -- disrupts the attacked model's alignment, resulting in unintended and potentially harmful outputs. Our novel approach systematically reveals a model's limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior. Through extensive experiments we demonstrate the efficacy of our technique, thus contributing to the ongoing discussion on responsible AI development by providing a diagnostic tool 
&lt;/p&gt;</description></item><item><title>FreeDrag&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;DragGAN&#22312;&#28857;&#36861;&#36394;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#26495;&#29305;&#24449;&#12289;&#32447;&#24615;&#25628;&#32034;&#21644;&#27169;&#31946;&#23450;&#20301;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2307.04684</link><description>&lt;p&gt;
FreeDrag: &#28857;&#36861;&#36394;&#24182;&#19981;&#36866;&#29992;&#20110;&#20132;&#20114;&#24335;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
FreeDrag: Point Tracking is Not What You Need for Interactive Point-based Image Editing. (arXiv:2307.04684v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04684
&lt;/p&gt;
&lt;p&gt;
FreeDrag&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;DragGAN&#22312;&#28857;&#36861;&#36394;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#26495;&#29305;&#24449;&#12289;&#32447;&#24615;&#25628;&#32034;&#21644;&#27169;&#31946;&#23450;&#20301;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#28385;&#36275;&#22270;&#20687;&#32534;&#36753;&#30340;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#38656;&#27714;&#65292;&#23545;&#22270;&#20687;&#20869;&#23481;&#30340;&#31934;&#30830;&#21644;&#28789;&#27963;&#30340;&#25805;&#32437;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26368;&#36817;&#65292;DragGAN&#36890;&#36807;&#22522;&#20110;&#28857;&#30340;&#25805;&#32437;&#23454;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32534;&#36753;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;DragGAN&#22312;&#28857;&#30340;&#36861;&#36394;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#21253;&#25324;&#38169;&#35823;&#36861;&#36394;&#21644;&#27169;&#31946;&#36861;&#36394;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FreeDrag&#65292;&#23427;&#37319;&#29992;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;DragGAN&#20013;&#28857;&#36861;&#36394;&#30340;&#36127;&#25285;&#12290;FreeDrag&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;&#27169;&#26495;&#29305;&#24449;&#12289;&#32447;&#24615;&#25628;&#32034;&#21644;&#27169;&#31946;&#23450;&#20301;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;DragGAN&#65292;&#24182;&#33021;&#22312;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#22256;&#38590;&#24773;&#26223;&#19979;&#23454;&#29616;&#31283;&#23450;&#30340;&#22522;&#20110;&#28857;&#30340;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
To serve the intricate and varied demands of image editing, precise and flexible manipulation of image content is indispensable. Recently, DragGAN has achieved impressive editing results through point-based manipulation. However, we have observed that DragGAN struggles with miss tracking, where DragGAN encounters difficulty in effectively tracking the desired handle points, and ambiguous tracking, where the tracked points are situated within other regions that bear resemblance to the handle points. To deal with the above issues, we propose FreeDrag, which adopts a feature-oriented approach to free the burden on point tracking within the point-oriented methodology of DragGAN. The FreeDrag incorporates adaptive template features, line search, and fuzzy localization techniques to perform stable and efficient point-based image editing. Extensive experiments demonstrate that our method is superior to the DragGAN and enables stable point-based editing in challenging scenarios with similar st
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#25968;&#25454;&#30340;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;MUlti-modal Generator (MUG)&#12290;&#22312;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#26159;&#20043;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;3.4%&#21644;2.2%&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2301.07088</link><description>&lt;p&gt;
&#35270;&#35273;&#23398;&#20064;&#32773;&#36935;&#35265;Web&#22270;&#20687;-&#25991;&#26412;&#23545;
&lt;/p&gt;
&lt;p&gt;
Vision Learners Meet Web Image-Text Pairs. (arXiv:2301.07088v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#25968;&#25454;&#30340;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;MUlti-modal Generator (MUG)&#12290;&#22312;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#26159;&#20043;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;3.4%&#21644;2.2%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#37117;&#26159;&#22312;&#32500;&#25252;&#33391;&#22909;&#30340;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#32771;&#34385;&#21040;&#32593;&#32476;&#25968;&#25454;&#30340;&#20986;&#33394;&#21487;&#20280;&#32553;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#24212;&#35813;&#22522;&#20110;&#22024;&#26434;&#30340;&#32593;&#32476;&#28304;&#22270;&#25991;&#37197;&#23545;&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#22914;&#27492;&#35774;&#32622;&#19979;&#65292;&#23545;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#19978;&#30340;&#20195;&#34920;&#24615;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#30740;&#31350;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#34987;&#23631;&#34109;&#30340;&#35757;&#32451;&#30446;&#26631;&#30340;&#21333;&#27169;&#24335;&#26041;&#27861;&#21644;&#20351;&#29992;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#35757;&#32451;&#30340;&#22810;&#27169;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#22312;&#35270;&#35273;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#24182;&#19981;&#27604;&#21333;&#27169;&#24577;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#35270;&#35282;&#26469;&#35299;&#37322;&#36825;&#20123;&#22522;&#20934;&#32467;&#26524;&#65292;&#36825;&#25552;&#20379;&#20102;&#22914;&#20309;&#35774;&#35745;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#32773;&#30340;&#35265;&#35299;&#12290;&#21463;&#21040;&#36825;&#20123;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#8212;&#8212;&#22810;&#27169;&#24335;&#29983;&#25104;&#22120;&#65288;MUG&#65289;&#65292;&#23427;&#20174;&#21487;&#20280;&#32553;&#30340;&#32593;&#32476;&#28304;&#22270;&#25991;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;MUG&#22312;&#20960;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;CIFAR-10&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20339;&#30340;&#32467;&#26524;3.4&#65285;&#65292;&#22312;STL-10&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20339;&#30340;&#32467;&#26524;2.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most recent self-supervised learning methods are pre-trained on the well-curated ImageNet-1K dataset. In this work, given the excellent scalability of web data, we consider self-supervised pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark study of representative self-supervised pre-training methods on large-scale web data in a like-for-like setting. We compare a range of methods, including single-modal ones that use masked training objectives and multi-modal ones that use image-text constrastive training. We observe that existing multi-modal methods do not outperform their single-modal counterparts on vision transfer learning tasks. We derive an information-theoretical view to explain these benchmark results, which provides insight into how to design a novel vision learner. Inspired by this insight, we present a new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from scalable web sourced image-text data. MUG ach
&lt;/p&gt;</description></item></channel></rss>