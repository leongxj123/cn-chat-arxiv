<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>DiffusionAct&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#31070;&#32463;&#20154;&#33080;&#20877;&#29616;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#32534;&#36753;&#36755;&#20837;&#22270;&#20687;&#30340;&#38754;&#37096;&#23039;&#21183;&#65292;&#23454;&#29616;&#36523;&#20221;&#21644;&#22806;&#35266;&#30340;&#20445;&#30041;&#65292;&#20197;&#21450;&#30446;&#26631;&#22836;&#37096;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.17217</link><description>&lt;p&gt;
DiffusionAct&#65306;&#21487;&#25511;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#19968;&#27425;&#24615;&#20154;&#33080;&#20877;&#29616;
&lt;/p&gt;
&lt;p&gt;
DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face Reenactment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17217
&lt;/p&gt;
&lt;p&gt;
DiffusionAct&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#31070;&#32463;&#20154;&#33080;&#20877;&#29616;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#32534;&#36753;&#36755;&#20837;&#22270;&#20687;&#30340;&#38754;&#37096;&#23039;&#21183;&#65292;&#23454;&#29616;&#36523;&#20221;&#21644;&#22806;&#35266;&#30340;&#20445;&#30041;&#65292;&#20197;&#21450;&#30446;&#26631;&#22836;&#37096;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#39537;&#21160;&#30340;&#31070;&#32463;&#20154;&#33080;&#20877;&#29616;&#26088;&#22312;&#21512;&#25104;&#33021;&#25104;&#21151;&#20445;&#30041;&#28304;&#33080;&#30340;&#36523;&#20221;&#21644;&#22806;&#35266;&#65292;&#21516;&#26102;&#36716;&#31227;&#30446;&#26631;&#22836;&#37096;&#23039;&#21183;&#21644;&#38754;&#37096;&#34920;&#24773;&#30340;&#36924;&#30495;&#38754;&#37096;&#22270;&#20687;&#12290;&#29616;&#26377;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#35201;&#20040;&#23384;&#22312;&#22833;&#30495;&#21644;&#35270;&#35273;&#20266;&#24433;&#65292;&#35201;&#20040;&#37325;&#26500;&#36136;&#37327;&#36739;&#24046;&#65292;&#21363;&#32972;&#26223;&#21644;&#19968;&#20123;&#37325;&#35201;&#30340;&#22806;&#35266;&#32454;&#33410;&#65288;&#22914;&#21457;&#22411;/&#39068;&#33394;&#12289;&#30524;&#38236;&#21644;&#37197;&#39280;&#65289;&#26410;&#34987;&#24544;&#23454;&#37325;&#24314;&#12290;&#26368;&#36817;&#22312;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#20351;&#24471;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DiffusionAct&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#29031;&#29255;&#36924;&#30495;&#22270;&#20687;&#29983;&#25104;&#26469;&#36827;&#34892;&#31070;&#32463;&#20154;&#33080;&#20877;&#29616;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#25511;&#21046;Diffusion&#33258;&#32534;&#30721;&#22120;&#65288;DiffAE&#65289;&#30340;&#35821;&#20041;&#31354;&#38388;&#65292;&#20197;&#20415;&#32534;&#36753;&#36755;&#20837;&#22270;&#20687;&#30340;&#38754;&#37096;&#23039;&#21183;&#65292;&#23450;&#20041;&#20026;&#22836;&#37096;&#23039;&#21183;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17217v1 Announce Type: cross  Abstract: Video-driven neural face reenactment aims to synthesize realistic facial images that successfully preserve the identity and appearance of a source face, while transferring the target head pose and facial expressions. Existing GAN-based methods suffer from either distortions and visual artifacts or poor reconstruction quality, i.e., the background and several important appearance details, such as hair style/color, glasses and accessories, are not faithfully reconstructed. Recent advances in Diffusion Probabilistic Models (DPMs) enable the generation of high-quality realistic images. To this end, in this paper we present DiffusionAct, a novel method that leverages the photo-realistic image generation of diffusion models to perform neural face reenactment. Specifically, we propose to control the semantic space of a Diffusion Autoencoder (DiffAE), in order to edit the facial pose of the input images, defined as the head pose orientation an
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#30740;&#20102;&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;</title><link>https://arxiv.org/abs/2402.02242</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#30740;&#20102;&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#65288;PVMs&#65289;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26368;&#20808;&#36827;&#30340;PVMs&#36798;&#21040;&#25968;&#21313;&#20159;&#29978;&#33267;&#25968;&#19975;&#20159;&#20010;&#21442;&#25968;&#65292;&#26631;&#20934;&#30340;&#20840;&#38754;&#24494;&#35843;&#33539;&#24335;&#30001;&#20110;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#21464;&#24471;&#19981;&#21487;&#25345;&#32493;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#25506;&#32034;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#65292;&#26088;&#22312;&#20197;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#35270;&#35273;PEFT&#30340;&#20840;&#38754;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#23545;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#23457;&#26597;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;PEFT&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#27169;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#26041;&#27861;&#20998;&#20026;&#19977;&#31867;&#65306;&#22522;&#20110;&#28155;&#21152;&#30340;&#12289;&#22522;&#20110;&#37096;&#20998;&#30340;&#21644;&#22522;&#20110;&#32479;&#19968;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#25361;&#25112;&#12290;&#35813;&#32508;&#36848;&#36824;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained vision models (PVMs) have shown great potential for adaptability across various downstream vision tasks. However, with state-of-the-art PVMs growing to billions or even trillions of parameters, the standard full fine-tuning paradigm is becoming unsustainable due to high computational and storage demands. In response, researchers are exploring parameter-efficient fine-tuning (PEFT), which seeks to exceed the performance of full fine-tuning with minimal parameter modifications. This survey provides a comprehensive overview and future directions for visual PEFT, offering a systematic review of the latest advancements. First, we provide a formal definition of PEFT and discuss model pre-training methods. We then categorize existing methods into three categories: addition-based, partial-based, and unified-based. Finally, we introduce the commonly used datasets and applications and suggest potential future research challenges. A comprehensive collection of resources is
&lt;/p&gt;</description></item><item><title>&#23558;&#27573;&#20998;&#31163;&#20219;&#24847;&#27169;&#22411;&#25512;&#36827;&#33267;&#39640;&#24230;&#20934;&#30830;&#30340;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;&#65292;&#36890;&#36807;&#25552;&#20986;DIS-SAM&#26694;&#26550;&#65292;&#25104;&#21151;&#25913;&#36827;SAM&#27169;&#22411;&#22312;&#32454;&#33410;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#22686;&#24378;&#30340;&#20998;&#21106;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2401.00248</link><description>&lt;p&gt;
&#23558;&#8220;&#27573;&#20998;&#31163;&#20219;&#24847;&#27169;&#22411;&#8221;&#25512;&#36827;&#33267;&#39640;&#24230;&#20934;&#30830;&#30340;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Promoting Segment Anything Model towards Highly Accurate Dichotomous Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00248
&lt;/p&gt;
&lt;p&gt;
&#23558;&#27573;&#20998;&#31163;&#20219;&#24847;&#27169;&#22411;&#25512;&#36827;&#33267;&#39640;&#24230;&#20934;&#30830;&#30340;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;&#65292;&#36890;&#36807;&#25552;&#20986;DIS-SAM&#26694;&#26550;&#65292;&#25104;&#21151;&#25913;&#36827;SAM&#27169;&#22411;&#22312;&#32454;&#33410;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#22686;&#24378;&#30340;&#20998;&#21106;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM)&#20195;&#34920;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;SAM&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#20854;&#20998;&#21106;&#33945;&#29256;&#32570;&#20047;&#32454;&#31890;&#24230;&#32454;&#33410;&#65292;&#29305;&#21035;&#26159;&#22312;&#20934;&#30830;&#25551;&#32472;&#23545;&#35937;&#36793;&#30028;&#26041;&#38754;&#12290;&#25105;&#20204;&#23545;SAM&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#36827;&#19968;&#27493;&#25913;&#36827;&#20197;&#23454;&#29616;&#39640;&#24230;&#31934;&#30830;&#30340;&#23545;&#35937;&#20998;&#21106;&#65288;&#21363;&#31216;&#20026;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;DIS&#65289;&#25265;&#26377;&#24456;&#39640;&#26399;&#26395;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIS-SAM&#65292;&#23558;SAM&#25512;&#36827;&#33267;DIS&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#31934;&#30830;&#32454;&#33410;&#12290;DIS-SAM&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#39640;&#24230;&#20934;&#30830;&#20998;&#21106;&#32780;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#20445;&#25345;&#20102;SAM&#30340;&#21487;&#20419;&#36827;&#35774;&#35745;&#12290;DIS-SAM&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#23558;SAM&#19982;&#19987;&#38376;&#29992;&#20110;DIS&#30340;&#20462;&#25913;&#21518;&#30340;IS-Net&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;DIS-SAM&#30456;&#27604;SAM&#21644;HQ-SA&#34920;&#29616;&#20986;&#26174;&#30528;&#22686;&#24378;&#30340;&#20998;&#21106;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00248v2 Announce Type: replace-cross  Abstract: The Segment Anything Model (SAM) represents a significant breakthrough into foundation models for computer vision, providing a large-scale image segmentation model. However, despite SAM's zero-shot performance, its segmentation masks lack fine-grained details, particularly in accurately delineating object boundaries. We have high expectations regarding whether SAM, as a foundation model, can be improved towards highly accurate object segmentation, which is known as dichotomous image segmentation (DIS). To address this issue, we propose DIS-SAM, which advances SAM towards DIS with extremely accurate details. DIS-SAM is a framework specifically tailored for highly accurate segmentation, maintaining SAM's promptable design. DIS-SAM employs a two-stage approach, integrating SAM with a modified IS-Net dedicated to DIS. Despite its simplicity, DIS-SAM demonstrates significantly enhanced segmentation accuracy compared to SAM and HQ-SA
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaskSub&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36974;&#32617;&#23376;&#27169;&#22411;&#21644;&#25918;&#26494;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24378;&#21270;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36974;&#32617;&#22686;&#24378;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2306.11339</link><description>&lt;p&gt;
&#36974;&#32617;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Masking Augmentation for Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.11339
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaskSub&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36974;&#32617;&#23376;&#27169;&#22411;&#21644;&#25918;&#26494;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24378;&#21270;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36974;&#32617;&#22686;&#24378;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#36974;&#32617;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#35757;&#32451;&#25216;&#26415;&#20013;&#30340;&#26032;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;&#23398;&#20064;&#22312;&#37319;&#29992;&#36974;&#32617;&#22686;&#24378;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#19981;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28041;&#21450;&#36974;&#32617;&#22686;&#24378;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;Masked Sub-model (MaskSub)&#12290;MaskSub&#30001;&#20027;&#27169;&#22411;&#21644;&#23376;&#27169;&#22411;&#32452;&#25104;&#65307;&#21069;&#32773;&#20139;&#21463;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#65292;&#32780;&#21518;&#32773;&#21033;&#29992;&#24378;&#22823;&#30340;&#36974;&#32617;&#22686;&#24378;&#26469;&#35757;&#32451;&#12290;MaskSub&#36890;&#36807;&#32531;&#35299;&#31867;&#20284;&#20110;&#33258;&#33976;&#39311;&#25439;&#22833;&#30340;&#25918;&#26494;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;MaskSub&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#35757;&#32451;&#25439;&#22833;&#30340;&#25910;&#25947;&#36895;&#24230;&#29978;&#33267;&#27604;&#24120;&#35268;&#35757;&#32451;&#26356;&#24555;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#35757;&#32451;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;MaskSub&#22312;&#21508;&#31181;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;DeiT-III&#65292;MAE&#24494;&#35843;&#65292;CLIP&#24494;&#35843;&#65292;ResNet&#21644;Swin T&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.11339v2 Announce Type: replace-cross  Abstract: Pre-training using random masking has emerged as a novel trend in training techniques. However, supervised learning faces a challenge in adopting masking augmentations, primarily due to unstable training. In this paper, we propose a novel way to involve masking augmentations dubbed Masked Sub-model (MaskSub). MaskSub consists of the main-model and sub-model; while the former enjoys conventional training recipes, the latter leverages the benefit of strong masking augmentations in training. MaskSub addresses the challenge by mitigating adverse effects through a relaxed loss function similar to a self-distillation loss. Our analysis shows that MaskSub improves performance, with the training loss converging even faster than regular training, which suggests our method facilitates training. We further validate MaskSub across diverse training recipes and models, including DeiT-III, MAE fine-tuning, CLIP fine-tuning, ResNet, and Swin T
&lt;/p&gt;</description></item></channel></rss>