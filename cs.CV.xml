<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;X-Portrait&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#26102;&#38388;&#36830;&#36143;&#24615;&#30340;&#32918;&#20687;&#21160;&#30011;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#25511;&#21046;&#20449;&#21495;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#22836;&#37096;&#23039;&#21183;&#21644;&#34920;&#24773;&#25511;&#21046;&#65292;&#20197;&#25552;&#39640;&#36816;&#21160;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.15931</link><description>&lt;p&gt;
X-Portrait: &#20855;&#26377;&#20998;&#23618;&#21160;&#20316;&#27880;&#24847;&#21147;&#30340;&#34920;&#29616;&#24615;&#32918;&#20687;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15931
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;X-Portrait&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#26102;&#38388;&#36830;&#36143;&#24615;&#30340;&#32918;&#20687;&#21160;&#30011;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#25511;&#21046;&#20449;&#21495;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#22836;&#37096;&#23039;&#21183;&#21644;&#34920;&#24773;&#25511;&#21046;&#65292;&#20197;&#25552;&#39640;&#36816;&#21160;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;X-Portrait&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#26102;&#38388;&#36830;&#36143;&#24615;&#30340;&#32918;&#20687;&#21160;&#30011;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26088;&#22312;&#22522;&#20110;&#21333;&#20010;&#32918;&#20687;&#20316;&#20026;&#22806;&#35266;&#21442;&#32771;&#65292;&#24182;&#21033;&#29992;&#26469;&#33258;&#39537;&#21160;&#35270;&#39057;&#30340;&#36816;&#21160;&#26469;&#20026;&#20854;&#28155;&#21152;&#21160;&#30011;&#65292;&#25429;&#25417;&#20855;&#26377;&#39640;&#24230;&#21160;&#24577;&#24615;&#21644;&#24494;&#22937;&#38754;&#37096;&#34920;&#24773;&#20197;&#21450;&#24191;&#27867;&#33539;&#22260;&#22836;&#37096;&#36816;&#21160;&#12290;&#22312;&#20854;&#26680;&#24515;&#37096;&#20998;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#20808;&#39564;&#20316;&#20026;&#28210;&#26579;&#39592;&#26550;&#65292;&#21516;&#26102;&#22312;ControlNet&#26694;&#26550;&#20869;&#36890;&#36807;&#26032;&#39062;&#30340;&#25511;&#21046;&#20449;&#21495;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#22836;&#37096;&#23039;&#21183;&#21644;&#34920;&#24773;&#25511;&#21046;&#12290;&#19982;&#20256;&#32479;&#30340;&#31895;&#31961;&#26174;&#24335;&#25511;&#21046;&#65288;&#22914;&#38754;&#37096;&#26631;&#24535;&#28857;&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#36816;&#21160;&#25511;&#21046;&#27169;&#22359;&#23398;&#20250;&#30452;&#25509;&#20174;&#21407;&#22987;&#39537;&#21160;RGB&#36755;&#20837;&#20013;&#35299;&#35835;&#21160;&#24577;&#12290;&#36890;&#36807;&#26377;&#25928;&#22686;&#24378;&#23545;&#30524;&#31070;&#31561;&#23567;&#23610;&#24230;&#32454;&#24494;&#24046;&#24322;&#30340;&#36816;&#21160;&#20851;&#27880;&#30340;&#22522;&#20110;&#34917;&#19969;&#30340;&#23616;&#37096;&#25511;&#21046;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#36816;&#21160;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15931v1 Announce Type: cross  Abstract: We propose X-Portrait, an innovative conditional diffusion model tailored for generating expressive and temporally coherent portrait animation. Specifically, given a single portrait as appearance reference, we aim to animate it with motion derived from a driving video, capturing both highly dynamic and subtle facial expressions along with wide-range head movements. As its core, we leverage the generative prior of a pre-trained diffusion model as the rendering backbone, while achieve fine-grained head pose and expression control with novel controlling signals within the framework of ControlNet. In contrast to conventional coarse explicit controls such as facial landmarks, our motion control module is learned to interpret the dynamics directly from the original driving RGB inputs. The motion accuracy is further enhanced with a patch-based local control module that effectively enhance the motion attention to small-scale nuances like eyeba
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;AutoVER&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35270;&#35273;&#23454;&#20307;&#35782;&#21035;&#20013;&#24212;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;&#32422;&#26463;&#29983;&#25104;&#65292;&#25104;&#21151;&#21306;&#20998;&#24040;&#22823;&#26631;&#31614;&#31354;&#38388;&#20013;&#30456;&#20284;&#30340;&#23454;&#20307;&#65292;&#24182;&#22312;Oven-Wiki&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.18695</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#22312;&#35270;&#35273;&#23454;&#20307;&#35782;&#21035;&#19978;
&lt;/p&gt;
&lt;p&gt;
Grounding Language Models for Visual Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18695
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;AutoVER&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35270;&#35273;&#23454;&#20307;&#35782;&#21035;&#20013;&#24212;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;&#32422;&#26463;&#29983;&#25104;&#65292;&#25104;&#21151;&#21306;&#20998;&#24040;&#22823;&#26631;&#31614;&#31354;&#38388;&#20013;&#30456;&#20284;&#30340;&#23454;&#20307;&#65292;&#24182;&#22312;Oven-Wiki&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;AutoVER&#65292;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#23454;&#20307;&#35782;&#21035;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#32422;&#26463;&#29983;&#25104;&#65292;&#25193;&#23637;&#20102;&#33258;&#22238;&#24402;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#22312;&#22788;&#29702;&#36328;&#39046;&#22495;&#23454;&#20307;&#26102;&#20943;&#36731;&#20102;&#20302;&#24615;&#33021;&#65292;&#22312;&#38656;&#35201;&#35270;&#35273;&#25512;&#29702;&#30340;&#26597;&#35810;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#30828;&#36127;&#23545;&#19978;&#36827;&#34892;&#23545;&#27604;&#35757;&#32451;&#65292;&#24182;&#22312;&#24207;&#21015;-&#24207;&#21015;&#30446;&#26631;&#20013;&#24182;&#34892;&#36827;&#34892;&#35757;&#32451;&#65292;&#23398;&#20064;&#22312;&#24040;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#20013;&#21306;&#20998;&#30456;&#20284;&#30340;&#23454;&#20307;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#19968;&#31995;&#21015;&#26816;&#32034;&#30340;&#20505;&#36873;&#31572;&#26696;&#26126;&#30830;&#25351;&#23548;&#35821;&#35328;&#29983;&#25104;&#65292;&#36890;&#36807;&#28040;&#38500;&#26080;&#25928;&#30340;&#35299;&#30721;&#36335;&#24452;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;Oven-Wiki&#22522;&#20934;&#27979;&#35797;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#25286;&#20998;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#22312;&#24050;&#30693;&#23454;&#20307;&#25286;&#20998;&#19978;&#30340;&#20934;&#30830;&#29575;&#20174;32.7%&#25552;&#39640;&#21040;61.5%&#12290;&#35813;&#26041;&#27861;&#36824;&#36890;&#36807;&#22823;&#24133;&#24230;&#25552;&#21319;&#22312;&#26410;&#30693;&#21644;&#26597;&#35810;&#25286;&#20998;&#19978;&#30340;&#24615;&#33021;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18695v1 Announce Type: cross  Abstract: We introduce AutoVER, an Autoregressive model for Visual Entity Recognition. Our model extends an autoregressive Multi-modal Large Language Model by employing retrieval augmented constrained generation. It mitigates low performance on out-of-domain entities while excelling in queries that require visually-situated reasoning. Our method learns to distinguish similar entities within a vast label space by contrastively training on hard negative pairs in parallel with a sequence-to-sequence objective without an external retriever. During inference, a list of retrieved candidate answers explicitly guides language generation by removing invalid decoding paths. The proposed method achieves significant improvements across different dataset splits in the recently proposed Oven-Wiki benchmark. Accuracy on the Entity seen split rises from 32.7% to 61.5%. It also demonstrates superior performance on the unseen and query splits by a substantial dou
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#27169;&#22411;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#25945;&#24072;&#30340;&#32852;&#21512;&#32454;&#31890;&#24230;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#22937;&#21327;&#20316;&#20196;&#29260;&#21644;&#23454;&#20307;&#34920;&#31034;&#65292;&#22788;&#29702;&#22797;&#26434;&#30340;&#34920;&#21333;&#25991;&#26723;&#65292;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#22312;&#22788;&#29702;&#35270;&#35273;&#22797;&#26434;&#34920;&#21333;&#25991;&#26723;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.17983</link><description>&lt;p&gt;
M3-VRD: &#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#22810;&#25945;&#24072;&#35270;&#35273;&#20016;&#23500;&#34920;&#21333;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
M3-VRD: Multimodal Multi-task Multi-teacher Visually-Rich Form Document Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17983
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#27169;&#22411;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#25945;&#24072;&#30340;&#32852;&#21512;&#32454;&#31890;&#24230;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#22937;&#21327;&#20316;&#20196;&#29260;&#21644;&#23454;&#20307;&#34920;&#31034;&#65292;&#22788;&#29702;&#22797;&#26434;&#30340;&#34920;&#21333;&#25991;&#26723;&#65292;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#22312;&#22788;&#29702;&#35270;&#35273;&#22797;&#26434;&#34920;&#21333;&#25991;&#26723;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31361;&#30772;&#24615;&#30340;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#25945;&#24072;&#32852;&#21512;&#32454;&#31890;&#24230;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;&#65292;&#29992;&#20110;&#35270;&#35273;&#20016;&#23500;&#30340;&#34920;&#21333;&#25991;&#26723;&#29702;&#35299;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#20419;&#36827;&#20196;&#29260;&#21644;&#23454;&#20307;&#34920;&#31034;&#20043;&#38388;&#30340;&#24494;&#22937;&#30456;&#20851;&#24615;&#26469;&#21033;&#29992;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#32423;&#21035;&#30340;&#35265;&#35299;&#65292;&#35299;&#20915;&#34920;&#21333;&#25991;&#26723;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#36328;&#32454;&#31890;&#24230;&#21644;&#36328;&#31895;&#31890;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#22810;&#25945;&#24072;&#30693;&#35782;&#33976;&#39311;&#20256;&#36882;&#36807;&#31243;&#65292;&#21576;&#29616;&#20998;&#24067;&#24046;&#36317;&#21644;&#23545;&#34920;&#21333;&#25991;&#26723;&#30340;&#32479;&#19968;&#29702;&#35299;&#12290;&#36890;&#36807;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#34920;&#21333;&#25991;&#26723;&#29702;&#35299;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#22320;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22788;&#29702;&#22797;&#26434;&#35270;&#35273;&#34920;&#21333;&#25991;&#26723;&#30340;&#22797;&#26434;&#32467;&#26500;&#21644;&#20869;&#23481;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17983v1 Announce Type: new  Abstract: This paper presents a groundbreaking multimodal, multi-task, multi-teacher joint-grained knowledge distillation model for visually-rich form document understanding. The model is designed to leverage insights from both fine-grained and coarse-grained levels by facilitating a nuanced correlation between token and entity representations, addressing the complexities inherent in form documents. Additionally, we introduce new inter-grained and cross-grained loss functions to further refine diverse multi-teacher knowledge distillation transfer process, presenting distribution gaps and a harmonised understanding of form documents. Through a comprehensive evaluation across publicly available form document understanding datasets, our proposed model consistently outperforms existing baselines, showcasing its efficacy in handling the intricate structures and content of visually complex form documents.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#36127;&#23545;&#35937;&#24615;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;K+1&#20010;logits&#24182;&#22312;&#23494;&#38598;&#39044;&#27979;&#32467;&#26500;&#20013;&#23884;&#20837;&#65292;&#21487;&#29420;&#31435;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.15374</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#36127;&#23545;&#35937;&#24615;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Outlier detection by ensembling uncertainty with negative objectness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15374
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#36127;&#23545;&#35937;&#24615;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;K+1&#20010;logits&#24182;&#22312;&#23494;&#38598;&#39044;&#27979;&#32467;&#26500;&#20013;&#23884;&#20837;&#65292;&#21487;&#29420;&#31435;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#30417;&#30563;&#24335;&#35270;&#35273;&#35782;&#21035;&#20013;&#20851;&#38190;&#30340;&#21151;&#33021;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#36807;&#40723;&#21169;&#26631;&#20934;&#23553;&#38381;&#38598;&#27169;&#22411;&#22312;&#36127;&#35757;&#32451;&#25968;&#25454;&#20013;&#20135;&#29983;&#20302;&#32622;&#20449;&#24230;&#39044;&#27979;&#26469;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#28151;&#28102;&#20102;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#36127;&#31867;&#21035;&#30340;&#35782;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#30452;&#25509;&#39044;&#27979;K+1&#20010;logits&#65292;&#36825;&#20123;logits&#23545;&#24212;&#20110;K&#20010;&#22522;&#26412;&#30495;&#23454;&#31867;&#21035;&#21644;&#19968;&#20010;&#24322;&#24120;&#31867;&#21035;&#12290;&#36825;&#31181;&#35774;&#32622;&#20801;&#35768;&#25105;&#20204;&#21046;&#23450;&#19968;&#31181;&#26032;&#22855;&#30340;&#24322;&#24120;&#24471;&#20998;&#65292;&#20316;&#20026;&#20998;&#24067;&#20869;&#19981;&#30830;&#23450;&#24615;&#21644;&#24322;&#24120;&#31867;&#21035;&#30340;&#21518;&#39564;&#30340;&#38598;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#36127;&#23545;&#35937;&#24615;&#12290;&#29616;&#22312;&#65292;&#24322;&#24120;&#20540;&#21487;&#20197;&#36890;&#36807;&#39640;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25110;&#19982;&#36127;&#25968;&#25454;&#30456;&#20284;&#20043;&#22788;&#29420;&#31435;&#26816;&#27979;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#23884;&#20837;&#21040;&#19968;&#20010;&#23494;&#38598;&#39044;&#27979;&#32467;&#26500;&#20013;&#65292;&#35813;&#32467;&#26500;&#20855;&#26377;K+2&#20010;&#31867;&#21035;&#30340;&#25513;&#30721;&#32423;&#21035;&#35782;&#21035;&#12290;&#35757;&#32451;&#36807;&#31243;&#40723;&#21169;&#26032;&#39062;&#30340;K+2-th&#31867;&#21035;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15374v1 Announce Type: cross  Abstract: Outlier detection is an essential capability in safety-critical applications of supervised visual recognition. Most of the existing methods deliver best results by encouraging standard closed-set models to produce low-confidence predictions in negative training data. However, that approach conflates prediction uncertainty with recognition of the negative class. We therefore reconsider direct prediction of K+1 logits that correspond to K groundtruth classes and one outlier class. This setup allows us to formulate a novel anomaly score as an ensemble of in-distribution uncertainty and the posterior of the outlier class which we term negative objectness. Now outliers can be independently detected due to i) high prediction uncertainty or ii) similarity with negative data. We embed our method into a dense prediction architecture with mask-level recognition over K+2 classes. The training procedure encourages the novel K+2-th class to learn n
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#22411;&#32452;&#21512;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#27599;&#20010;&#21407;&#22987;&#27169;&#22411;&#30340;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#21512;&#24182;&#21442;&#25968;&#24178;&#25200;&#21644;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12750</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Model Composition for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12750
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#32452;&#21512;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#27599;&#20010;&#21407;&#22987;&#27169;&#22411;&#30340;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#21512;&#24182;&#21442;&#25968;&#24178;&#25200;&#21644;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#21457;&#23637;&#26174;&#31034;&#20986;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#26397;&#30528;&#21019;&#24314;&#33021;&#22815;&#29702;&#35299;&#21508;&#31181;&#27169;&#24577;&#36755;&#20837;&#30340;&#22810;&#21151;&#33021;MLLMs&#30340;&#30446;&#26631;&#36808;&#36827;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#19982;&#37197;&#23545;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#25968;&#25454;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65292;&#36825;&#23545;&#36164;&#28304;&#35201;&#27714;&#39640;&#19988;&#38590;&#20197;&#25193;&#23637;&#21040;&#26032;&#30340;&#27169;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29616;&#26377;MLLMs&#30340;&#27169;&#22411;&#32452;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#26032;&#27169;&#22411;&#30340;&#26032;&#33539;&#24335;&#65292;&#35813;&#26032;&#27169;&#22411;&#20445;&#30041;&#20102;&#27599;&#20010;&#21407;&#22987;&#27169;&#22411;&#30340;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#22522;&#26412;&#23454;&#29616;NaiveMC&#36890;&#36807;&#37325;&#29992;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#21512;&#24182;LLM&#21442;&#25968;&#23637;&#31034;&#20102;&#36825;&#19968;&#33539;&#24335;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DAMC&#26469;&#35299;&#20915;&#22312;&#21512;&#24182;&#36807;&#31243;&#20013;&#30340;&#21442;&#25968;&#24178;&#25200;&#21644;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MCUB&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;MLLMs&#29702;&#35299;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12750v1 Announce Type: cross  Abstract: Recent developments in Multimodal Large Language Models (MLLMs) have shown rapid progress, moving towards the goal of creating versatile MLLMs that understand inputs from various modalities. However, existing methods typically rely on joint training with paired multimodal instruction data, which is resource-intensive and challenging to extend to new modalities. In this paper, we propose a new paradigm through the model composition of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model. Our basic implementation, NaiveMC, demonstrates the effectiveness of this paradigm by reusing modality encoders and merging LLM parameters. Furthermore, we introduce DAMC to address parameter interference and mismatch issues during the merging process, thereby enhancing the model performance. To facilitate research in this area, we propose MCUB, a benchmark for assessing ability of MLLMs to unders
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#24494;&#32467;&#26500;&#26144;&#23556;&#12289;&#32420;&#32500;&#26463;&#25551;&#35760;&#12289;&#30333;&#36136;&#32420;&#32500;&#26463;&#20998;&#26512;&#20197;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#24635;&#32467;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00019</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion MRI with Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#24494;&#32467;&#26500;&#26144;&#23556;&#12289;&#32420;&#32500;&#26463;&#25551;&#35760;&#12289;&#30333;&#36136;&#32420;&#32500;&#26463;&#20998;&#26512;&#20197;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#24635;&#32467;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24357;&#25955;&#21152;&#26435;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#20855;&#26377;&#38750;&#20405;&#20837;&#24615;&#35780;&#20272;&#22823;&#33041;&#24494;&#32467;&#26500;&#21644;&#32467;&#26500;&#36830;&#25509;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;dMRI&#25968;&#25454;&#20197;&#25552;&#21462;&#20020;&#24202;&#21644;&#31185;&#23398;&#30446;&#30340;&#30340;&#26377;&#29992;&#20449;&#24687;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290; dMRI&#27979;&#37327;&#36890;&#24120;&#21463;&#21040;&#24378;&#22122;&#22768;&#21644;&#20266;&#24433;&#30340;&#24178;&#25200;&#65292;&#25968;&#25454;&#20013;&#36890;&#24120;&#23384;&#22312;&#39640;&#30340;&#20250;&#35805;&#38388;&#21644;&#25195;&#25551;&#32773;&#38388;&#24322;&#36136;&#24615;&#65292;&#20197;&#21450;&#22823;&#33041;&#32467;&#26500;&#30340;&#30456;&#24403;&#22823;&#30340;&#20010;&#20307;&#38388;&#21464;&#24322;&#65292;&#24182;&#19988;&#27979;&#37327;&#21644;&#24863;&#20852;&#36259;&#29616;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#21487;&#33021;&#38750;&#24120;&#22797;&#26434;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;dMRI&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#36825;&#20123;&#23581;&#35797;&#65292;&#37325;&#28857;&#20851;&#27880;&#24050;&#32463;&#35299;&#20915;&#20102;&#24494;&#32467;&#26500;&#26144;&#23556;&#12289;&#32420;&#32500;&#26463;&#25551;&#35760;&#12289;&#30333;&#36136;&#32420;&#32500;&#26463;&#20998;&#26512;&#20197;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20027;&#35201;&#21457;&#29616;&#12289;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-weighted magnetic resonance imaging (dMRI) offers unique capabilities such as noninvasive assessment of brain's micro-structure and structural connectivity. However, analyzing the dMRI data to extract useful information for clinical and scientific purposes is challenging. The dMRI measurements often suffer from strong noise and artifacts, there is usually high inter-session and inter-scanner heterogeneity in the data and considerable inter-subject variability in brain structure, and the relationship between measurements and the phenomena of interest can be highly complex. Recent years have witnessed increasing use of machine learning methods for dMRI analysis. This manuscript aims to assess these efforts, with a focus on methods that have addressed micro-structure mapping, tractography, white matter tract analysis, as well as data preprocessing and harmonization. We summarize the main findings, strengths, and weaknesses of the existing methods and suggest topics for future re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#30340;&#27010;&#24565;&#65288;CATE&#65289;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#20219;&#21153;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#65292;&#20294;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#35813;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;</title><link>http://arxiv.org/abs/2401.10805</link><description>&lt;p&gt;
&#23398;&#20064;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Learning to Visually Connect Actions and their Effects. (arXiv:2401.10805v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10805
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#30340;&#27010;&#24565;&#65288;CATE&#65289;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#20219;&#21153;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#65292;&#20294;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#35813;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#65288;CATE&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;CATE&#21487;&#20197;&#22312;&#20219;&#21153;&#35268;&#21010;&#21644;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#31561;&#39046;&#22495;&#20013;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#22522;&#20110;CATE&#30340;&#20219;&#21153;&#24418;&#24335;&#65292;&#22914;&#21160;&#20316;&#36873;&#25321;&#21644;&#21160;&#20316;&#25351;&#23450;&#65292;&#20854;&#20013;&#35270;&#39057;&#29702;&#35299;&#27169;&#22411;&#20197;&#35821;&#20041;&#21644;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#36830;&#25509;&#21160;&#20316;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#29992;&#20110;&#21160;&#20316;&#36873;&#25321;&#21644;&#21160;&#20316;&#25351;&#23450;&#12290;&#23613;&#31649;&#20219;&#21153;&#20855;&#26377;&#30452;&#35266;&#24615;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#22256;&#38590;&#37325;&#37325;&#65292;&#20154;&#31867;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#22522;&#30784;&#65292;&#23637;&#31034;&#20102;&#36830;&#25509;&#35270;&#39057;&#29702;&#35299;&#20013;&#21160;&#20316;&#21644;&#25928;&#26524;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce the novel concept of visually Connecting Actions and Their Effects (CATE) in video understanding. CATE can have applications in areas like task planning and learning from demonstration. We propose different CATE-based task formulations, such as action selection and action specification, where video understanding models connect actions and effects at semantic and fine-grained levels. We observe that different formulations produce representations capturing intuitive action properties. We also design various baseline models for action selection and action specification. Despite the intuitive nature of the task, we observe that models struggle, and humans outperform them by a large margin. The study aims to establish a foundation for future efforts, showcasing the flexibility and versatility of connecting actions and effects in video understanding, with the hope of inspiring advanced formulations and models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;&#20102;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;CATM&#21644;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;GSL&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09786</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Adaptive Self-training Framework for Fine-grained Scene Graph Generation. (arXiv:2401.09786v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;&#20102;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;CATM&#21644;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;GSL&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;SGG&#65289;&#27169;&#22411;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#38271;&#23614;&#35859;&#35789;&#20998;&#24067;&#21644;&#32570;&#22833;&#27880;&#37322;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;SGG&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#35757;&#32451;SGG&#65288;ST-SGG&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#20026;&#20854;&#20998;&#37197;&#20266;&#26631;&#31614;&#20197;&#35757;&#32451;SGG&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#22270;&#20687;&#35782;&#21035;&#26041;&#38754;&#30340;&#33258;&#35757;&#32451;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#35774;&#35745;&#36866;&#29992;&#20110;SGG&#20219;&#21153;&#30340;&#33258;&#35757;&#32451;&#26694;&#26550;&#26356;&#20855;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#22266;&#26377;&#29305;&#24615;&#65292;&#22914;&#35821;&#20041;&#27495;&#20041;&#21644;&#38271;&#23614;&#20998;&#24067;&#30340;&#35859;&#35789;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SGG&#20266;&#26631;&#31614;&#25216;&#26415;&#65292;&#31216;&#20026;&#20855;&#26377;&#21160;&#37327;&#30340;&#31867;&#21035;&#33258;&#36866;&#24212;&#38408;&#20540;&#21270;&#65288;CATM&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#29420;&#31435;&#20110;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#24050;&#26377;&#30340;SGG&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;&#65288;GSL&#65289;&#65292;&#20174;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scene graph generation (SGG) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. In this work, we aim to alleviate the long-tailed problem of SGG by utilizing unannotated triplets. To this end, we introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels for unannotated triplets based on which the SGG models are trained. While there has been significant progress in self-training for image recognition, designing a self-training framework for the SGG task is more challenging due to its inherent nature such as the semantic ambiguity and the long-tailed distribution of predicate classes. Hence, we propose a novel pseudo-labeling technique for SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is a model-agnostic framework that can be applied to any existing SGG models. Furthermore, we devise a graph structure learner (GSL) that is benefici
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;2D&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20165;&#26377;2D&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#20986;3D&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#35813;&#20449;&#24687;&#36827;&#34892;3D&#35270;&#35273;&#20219;&#21153;&#12290;&#36890;&#36807;&#35266;&#28857;&#31070;&#32463;&#25991;&#26412;&#20498;&#32622;&#65288;ViewNeTI&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#20013;&#23545;&#35937;&#30340;3D&#35270;&#28857;&#65292;&#26377;&#25928;&#35299;&#20915;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#35270;&#22270;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#35821;&#20041;&#32454;&#33410;&#21644;&#36924;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.07986</link><description>&lt;p&gt;
&#35266;&#28857;&#25991;&#26412;&#20498;&#32622;&#65306;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;2D&#25193;&#25955;&#27169;&#22411;&#37322;&#25918;&#26032;&#39062;&#30340;&#35270;&#22270;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Viewpoint Textual Inversion: Unleashing Novel View Synthesis with Pretrained 2D Diffusion Models. (arXiv:2309.07986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;2D&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20165;&#26377;2D&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#20986;3D&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#35813;&#20449;&#24687;&#36827;&#34892;3D&#35270;&#35273;&#20219;&#21153;&#12290;&#36890;&#36807;&#35266;&#28857;&#31070;&#32463;&#25991;&#26412;&#20498;&#32622;&#65288;ViewNeTI&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#20013;&#23545;&#35937;&#30340;3D&#35270;&#28857;&#65292;&#26377;&#25928;&#35299;&#20915;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#35270;&#22270;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#35821;&#20041;&#32454;&#33410;&#21644;&#36924;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#29702;&#35299;&#23545;&#35937;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#20165;&#36890;&#36807;2D&#30417;&#30563;&#26469;&#34920;&#31034;&#19990;&#30028;&#30340;&#30495;&#23454;3D&#32467;&#26500;&#65311;&#25105;&#20204;&#35777;&#26126;&#65292;&#26159;&#30340;&#65292;3D&#30693;&#35782;&#34987;&#32534;&#30721;&#22312;2D&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;&#22914;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65289;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#32467;&#26500;&#21487;&#20197;&#29992;&#20110;3D&#35270;&#35273;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35266;&#28857;&#31070;&#32463;&#25991;&#26412;&#20498;&#32622;&#65288;ViewNeTI&#65289;&#65292;&#21487;&#20197;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#20013;&#23545;&#35937;&#30340;3D&#35270;&#28857;&#12290;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#23567;&#22411;&#31070;&#32463;&#26144;&#23556;&#22120;&#65292;&#29992;&#20110;&#33719;&#21462;&#30456;&#26426;&#35270;&#28857;&#21442;&#25968;&#24182;&#39044;&#27979;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#21521;&#37327;&#65307;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#28508;&#22312;&#21521;&#37327;&#26469;&#35843;&#25972;&#25193;&#25955;&#29983;&#25104;&#36807;&#31243;&#65292;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#30456;&#26426;&#35270;&#28857;&#30340;&#22270;&#20687;&#12290;ViewNeTI&#33258;&#28982;&#35299;&#20915;&#20102;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#65288;NVS&#65289;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#34987;&#20923;&#32467;&#30340;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#21487;&#20197;&#29992;&#24456;&#23569;&#30340;&#36755;&#20837;&#35270;&#22270;&#26469;&#35299;&#20915;NVS&#38382;&#39064;&#65307;&#25105;&#20204;&#29978;&#33267;&#21487;&#20197;&#36827;&#34892;&#21333;&#35270;&#22270;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#21333;&#35270;&#22270;NVS&#39044;&#27979;&#20855;&#26377;&#33391;&#22909;&#30340;&#35821;&#20041;&#32454;&#33410;&#21644;&#36924;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models understand spatial relationship between objects, but do they represent the true 3D structure of the world from only 2D supervision? We demonstrate that yes, 3D knowledge is encoded in 2D image diffusion models like Stable Diffusion, and we show that this structure can be exploited for 3D vision tasks. Our method, Viewpoint Neural Textual Inversion (ViewNeTI), controls the 3D viewpoint of objects in generated images from frozen diffusion models. We train a small neural mapper to take camera viewpoint parameters and predict text encoder latents; the latents then condition the diffusion generation process to produce images with the desired camera viewpoint.  ViewNeTI naturally addresses Novel View Synthesis (NVS). By leveraging the frozen diffusion model as a prior, we can solve NVS with very few input views; we can even do single-view novel view synthesis. Our single-view NVS predictions have good semantic details and photorealism compared to prior methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#23545;&#27604;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#23558;OCR&#24314;&#27169;&#20026;&#23383;&#31526;&#32423;&#22270;&#20687;&#26816;&#32034;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#24050;&#26377;&#26550;&#26500;&#26356;&#20855;&#26679;&#26412;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20174;&#32780;&#20351;&#25968;&#23383;&#21382;&#21490;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#25991;&#29486;&#21490;&#26009;&#24471;&#20197;&#26356;&#22909;&#22320;&#21442;&#19982;&#31038;&#21306;&#12290;</title><link>http://arxiv.org/abs/2304.02737</link><description>&lt;p&gt;
&#24314;&#35774;&#22810;&#26679;&#21270;&#25968;&#23383;&#21382;&#21490;&#30340;&#39640;&#25928;OCR
&lt;/p&gt;
&lt;p&gt;
Efficient OCR for Building a Diverse Digital History. (arXiv:2304.02737v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#23545;&#27604;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#23558;OCR&#24314;&#27169;&#20026;&#23383;&#31526;&#32423;&#22270;&#20687;&#26816;&#32034;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#24050;&#26377;&#26550;&#26500;&#26356;&#20855;&#26679;&#26412;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20174;&#32780;&#20351;&#25968;&#23383;&#21382;&#21490;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#25991;&#29486;&#21490;&#26009;&#24471;&#20197;&#26356;&#22909;&#22320;&#21442;&#19982;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#26377;&#25104;&#21315;&#19978;&#19975;&#30340;&#29992;&#25143;&#26597;&#38405;&#25968;&#23383;&#26723;&#26696;&#65292;&#20294;&#20182;&#20204;&#21487;&#20197;&#20351;&#29992;&#30340;&#20449;&#24687;&#24182;&#19981;&#33021;&#20195;&#34920;&#21508;&#31181;&#25991;&#29486;&#21490;&#26009;&#30340;&#22810;&#26679;&#24615;&#12290;&#20856;&#22411;&#29992;&#20110;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#26550;&#26500;&#8212;&#8212;&#32852;&#21512;&#23398;&#20064;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;&#22312;&#20302;&#36164;&#28304;&#25991;&#29486;&#38598;&#21512;&#20013;&#24456;&#38590;&#25193;&#23637;&#65292;&#22240;&#20026;&#23398;&#20064;&#35821;&#35328;-&#35270;&#35273;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#24207;&#21015;&#21644;&#35745;&#31639;&#12290;&#26412;&#30740;&#31350;&#23558;OCR&#24314;&#27169;&#20026;&#23383;&#31526;&#32423;&#22270;&#20687;&#26816;&#32034;&#38382;&#39064;&#65292;&#20351;&#29992;&#23545;&#27604;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#12290;&#22240;&#20026;&#35813;&#27169;&#22411;&#21482;&#23398;&#20064;&#23383;&#31526;&#30340;&#35270;&#35273;&#29305;&#24449;&#65292;&#23427;&#27604;&#29616;&#26377;&#26550;&#26500;&#26356;&#20855;&#26679;&#26412;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#33021;&#22815;&#22312;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#22833;&#36133;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;OCR&#12290;&#20851;&#38190;&#26159;&#65292;&#35813;&#27169;&#22411;&#20026;&#31038;&#21306;&#21442;&#19982;&#22312;&#20351;&#25968;&#23383;&#21382;&#21490;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#25991;&#29486;&#21490;&#26009;&#26041;&#38754;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thousands of users consult digital archives daily, but the information they can access is unrepresentative of the diversity of documentary history. The sequence-to-sequence architecture typically used for optical character recognition (OCR) - which jointly learns a vision and language model - is poorly extensible to low-resource document collections, as learning a language-vision model requires extensive labeled sequences and compute. This study models OCR as a character level image retrieval problem, using a contrastively trained vision encoder. Because the model only learns characters' visual features, it is more sample efficient and extensible than existing architectures, enabling accurate OCR in settings where existing solutions fail. Crucially, the model opens new avenues for community engagement in making digital history more representative of documentary history.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28857;&#20113;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;Point-MA2E&#65292;&#36890;&#36807;&#21516;&#26102;&#37319;&#29992;&#25513;&#33180;&#21644;&#20223;&#23556;&#21464;&#25442;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20174;&#25439;&#22351;&#28857;&#20113;&#21040;&#36824;&#21407;&#28857;&#20113;&#30340;&#37325;&#24314;&#65292;&#25193;&#23637;&#20102;&#30446;&#21069;&#25513;&#33180;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2211.06841</link><description>&lt;p&gt;
Point-MA2E:&#33258;&#30417;&#30563;&#28857;&#20113;&#23398;&#20064;&#30340;&#25513;&#33180;&#21644;&#20223;&#23556;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Point-MA2E: Masked and Affine Transformed AutoEncoder for Self-supervised Point Cloud Learning. (arXiv:2211.06841v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28857;&#20113;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;Point-MA2E&#65292;&#36890;&#36807;&#21516;&#26102;&#37319;&#29992;&#25513;&#33180;&#21644;&#20223;&#23556;&#21464;&#25442;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20174;&#25439;&#22351;&#28857;&#20113;&#21040;&#36824;&#21407;&#28857;&#20113;&#30340;&#37325;&#24314;&#65292;&#25193;&#23637;&#20102;&#30446;&#21069;&#25513;&#33180;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#30417;&#30563;&#28857;&#20113;&#23398;&#20064;&#20013;&#65292;&#25513;&#33180;&#24314;&#27169;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#20174;&#20854;&#25513;&#33180;&#23545;&#24212;&#37096;&#20998;&#37325;&#24314;&#23436;&#25972;&#28857;&#20113;&#12290;&#32771;&#34385;&#21040;&#25513;&#33180;&#21482;&#20250;&#25439;&#22351;&#36755;&#20837;&#30340;&#19968;&#37096;&#20998;&#28857;&#65292;&#26412;&#25991;&#25512;&#24191;&#20223;&#23556;&#21464;&#25442;&#31574;&#30053;&#65292;&#36890;&#36807;&#29305;&#23450;&#35268;&#21017;&#30772;&#22351;&#25152;&#26377;&#36755;&#20837;&#28857;&#65292;&#20197;&#34917;&#20805;&#27969;&#34892;&#30340;&#25513;&#33180;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#28857;&#20113;&#23398;&#20064;&#30340;&#25513;&#33180;&#21644;&#20223;&#23556;&#21464;&#25442;&#33258;&#32534;&#30721;&#22120;&#65288;Point-MA2E&#65289;&#12290;&#22312;&#27492;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#28857;&#20113;&#36827;&#34892;&#20223;&#23556;&#21464;&#25442;&#21644;&#25513;&#33180;&#65292;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20174;&#20854;&#25439;&#22351;&#29256;&#26412;&#20013;&#37325;&#24314;&#21407;&#22987;&#28857;&#20113;&#12290;&#25506;&#32034;&#20102;&#21508;&#31181;&#28857;&#20113;&#32534;&#30721;&#22120;&#12290;&#23545;&#20110;&#38750;Transformer&#32534;&#30721;&#22120;&#65292;&#25353;&#29031;&#24120;&#35265;&#20570;&#27861;&#30452;&#25509;&#37325;&#24314;&#26410;&#25439;&#22351;&#30340;&#28857;&#20113;&#12290;&#23545;&#20110;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#23558;&#37325;&#24314;&#23436;&#25972;&#28857;&#20113;&#20998;&#35299;&#20026;&#35814;&#32454;&#30340;&#23616;&#37096;&#34917;&#19969;&#21644;&#31895;&#30053;&#30340;&#20840;&#23616;&#24418;&#29366;&#30340;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked modeling has demonstrated its effectiveness in self-supervised point cloud learning by reconstructing the complete point cloud from its masked counterpart. Considering that masking only corrupts partial points of the input, in this paper, we promote the affine transformation, which corrupts all input points with certain rules, to complement the popular masking strategy, leading to the Masked and Affine transformed AutoEncoder for point cloud learning (Point-MA2E). Generally, we corrupt the point cloud with affine transformation and masking as input and learn an encoder-decoder model to reconstruct the original point cloud from its corrupted version. Various point cloud encoders are explored in this study. For non-Transformer encoders, we follow the common practice to reconstruct the uncorrupted point cloud directly. For Transformer-based encoders, we decompose the reconstruction of the complete point cloud into the reconstructions of detailed local patches and rough global shape
&lt;/p&gt;</description></item></channel></rss>