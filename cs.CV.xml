<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LoGoNet&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#24212;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#25361;&#25112;&#12290;LoGoNet&#36890;&#36807;&#37319;&#29992;&#22823;&#20869;&#26680;&#27880;&#24847;&#21147;&#21644;&#21452;&#37325;&#32534;&#30721;&#31574;&#30053;&#65292;&#28789;&#27963;&#25429;&#25417;&#38271;&#12289;&#30701;&#36317;&#31163;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#32452;&#21512;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.06190</link><description>&lt;p&gt;
Masked LoGoNet&#65306;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#24555;&#36895;&#20934;&#30830;3D&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LoGoNet&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#24212;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#25361;&#25112;&#12290;LoGoNet&#36890;&#36807;&#37319;&#29992;&#22823;&#20869;&#26680;&#27880;&#24847;&#21147;&#21644;&#21452;&#37325;&#32534;&#30721;&#31574;&#30053;&#65292;&#28789;&#27963;&#25429;&#25417;&#38271;&#12289;&#30701;&#36317;&#31163;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#32452;&#21512;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#22270;&#20687;&#26041;&#27861;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#37096;&#32626;&#26102;&#36890;&#24120;&#29992;&#20110;&#27599;&#22825;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#65292;&#32473;&#21307;&#30103;&#35774;&#26045;&#24102;&#26469;&#39640;&#32500;&#25252;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;LoGoNet&#65292;&#37319;&#29992;&#23450;&#21046;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#12290;LoGoNet&#22312;U&#24418;&#26550;&#26500;&#20869;&#25972;&#21512;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#21033;&#29992;&#22823;&#20869;&#26680;&#27880;&#24847;&#21147;&#65288;LKA&#65289;&#21644;&#21452;&#37325;&#32534;&#30721;&#31574;&#30053;&#65292;&#28789;&#27963;&#22320;&#25429;&#25417;&#38271;&#12289;&#30701;&#36317;&#31163;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;&#36825;&#19982;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#22686;&#21152;&#32593;&#32476;&#23481;&#37327;&#20197;&#22686;&#24378;&#29305;&#24449;&#25552;&#21462;&#30340;&#26041;&#24335;&#24418;&#25104;&#23545;&#27604;&#12290;&#25105;&#20204;&#27169;&#22411;&#20013;&#36825;&#20123;&#26032;&#25216;&#26415;&#30340;&#32452;&#21512;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#65292;&#32771;&#34385;&#21040;&#20854;&#22256;&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard modern machine-learning-based imaging methods have faced challenges in medical applications due to the high cost of dataset construction and, thereby, the limited labeled training data available. Additionally, upon deployment, these methods are usually used to process a large volume of data on a daily basis, imposing a high maintenance cost on medical facilities. In this paper, we introduce a new neural network architecture, termed LoGoNet, with a tailored self-supervised learning (SSL) method to mitigate such challenges. LoGoNet integrates a novel feature extractor within a U-shaped architecture, leveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture both long-range and short-range feature dependencies adeptly. This is in contrast to existing methods that rely on increasing network capacity to enhance feature extraction. This combination of novel techniques in our model is especially beneficial in medical image segmentation, given the difficulty of le
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#22810;&#35821;&#35328;&#25968;&#25454;&#26377;&#26356;&#39640;&#30340;&#35821;&#20041;&#35206;&#30422;&#29575;&#65292;&#24182;&#19988;&#22522;&#20110;&#22810;&#35821;&#35328;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.14356</link><description>&lt;p&gt;
&#25991;&#21270;&#21644;&#35821;&#35328;&#22810;&#26679;&#24615;&#25552;&#39640;&#20102;&#35270;&#35273;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Cultural and Linguistic Diversity Improves Visual Representations. (arXiv:2310.14356v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14356
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#22810;&#35821;&#35328;&#25968;&#25454;&#26377;&#26356;&#39640;&#30340;&#35821;&#20041;&#35206;&#30422;&#29575;&#65292;&#24182;&#19988;&#22522;&#20110;&#22810;&#35821;&#35328;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#36890;&#24120;&#23558;&#24863;&#30693;&#35270;&#20026;&#23458;&#35266;&#30340;&#65292;&#24182;&#19988;&#36825;&#31181;&#20551;&#35774;&#22312;&#25968;&#25454;&#38598;&#25910;&#38598;&#21644;&#27169;&#22411;&#35757;&#32451;&#20013;&#24471;&#21040;&#21453;&#26144;&#12290;&#20363;&#22914;&#65292;&#19981;&#21516;&#35821;&#35328;&#30340;&#22270;&#20687;&#25551;&#36848;&#36890;&#24120;&#34987;&#20551;&#23450;&#20026;&#30456;&#21516;&#35821;&#20041;&#20869;&#23481;&#30340;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#36328;&#25991;&#21270;&#24515;&#29702;&#23398;&#21644;&#35821;&#35328;&#23398;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20010;&#20307;&#30340;&#35270;&#35273;&#24863;&#30693;&#22240;&#20854;&#25991;&#21270;&#32972;&#26223;&#21644;&#25152;&#35828;&#30340;&#35821;&#35328;&#32780;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#26631;&#39064;&#20013;&#65292;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#20041;&#20869;&#23481;&#24046;&#24322;&#12290;&#24403;&#25968;&#25454;&#26159;&#22810;&#35821;&#35328;&#32780;&#19981;&#26159;&#21333;&#35821;&#35328;&#26102;&#65292;&#26631;&#39064;&#30340;&#35821;&#20041;&#35206;&#30422;&#29575;&#24179;&#22343;&#26356;&#39640;&#65292;&#20197;&#22330;&#26223;&#22270;&#12289;&#23884;&#20837;&#21644;&#35821;&#35328;&#22797;&#26434;&#24615;&#36827;&#34892;&#27979;&#37327;&#12290;&#20363;&#22914;&#65292;&#19982;&#19968;&#32452;&#21333;&#35821;&#26631;&#39064;&#30456;&#27604;&#65292;&#22810;&#35821;&#26631;&#39064;&#24179;&#22343;&#26377;21.8&#65285;&#26356;&#22810;&#30340;&#23545;&#35937;&#65292;24.5&#65285;&#26356;&#22810;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;27.1&#65285;&#26356;&#22810;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#30340;&#20869;&#23481;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer vision often treats perception as objective, and this assumption gets reflected in the way that datasets are collected and models are trained. For instance, image descriptions in different languages are typically assumed to be translations of the same semantic content. However, work in cross-cultural psychology and linguistics has shown that individuals differ in their visual perception depending on their cultural background and the language they speak. In this paper, we demonstrate significant differences in semantic content across languages in both dataset and model-produced captions. When data is multilingual as opposed to monolingual, captions have higher semantic coverage on average, as measured by scene graph, embedding, and linguistic complexity. For example, multilingual captions have on average 21.8% more objects, 24.5% more relations, and 27.1% more attributes than a set of monolingual captions. Moreover, models trained on content from different languages perform bes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23646;&#24615;&#36873;&#25321;&#26426;&#21046;&#30340;&#22825;&#28982;&#21487;&#35299;&#37322;&#27169;&#22411;&#26469;&#22788;&#29702;&#23567;&#26679;&#26412;&#23398;&#20064;&#65292;&#36890;&#36807;&#20943;&#23569;&#27599;&#20010;episode&#20013;&#28041;&#21450;&#30340;&#23646;&#24615;&#25968;&#37327;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#33258;&#21160;&#26816;&#27979;&#24182;&#34917;&#20607;&#20154;&#24037;&#26234;&#33021;&#23646;&#24615;&#27744;&#19981;&#36275;&#30340;episode&#12290;</title><link>http://arxiv.org/abs/2211.09107</link><description>&lt;p&gt;
&#22312;&#32447;&#23646;&#24615;&#36873;&#25321;&#30340;&#21487;&#35299;&#37322;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Interpretable Few-shot Learning with Online Attribute Selection. (arXiv:2211.09107v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23646;&#24615;&#36873;&#25321;&#26426;&#21046;&#30340;&#22825;&#28982;&#21487;&#35299;&#37322;&#27169;&#22411;&#26469;&#22788;&#29702;&#23567;&#26679;&#26412;&#23398;&#20064;&#65292;&#36890;&#36807;&#20943;&#23569;&#27599;&#20010;episode&#20013;&#28041;&#21450;&#30340;&#23646;&#24615;&#25968;&#37327;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#33258;&#21160;&#26816;&#27979;&#24182;&#34917;&#20607;&#20154;&#24037;&#26234;&#33021;&#23646;&#24615;&#27744;&#19981;&#36275;&#30340;episode&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#23398;&#20064;(few-shot learning, FSL)&#26159;&#19968;&#31181;&#25361;&#25112;&#24615;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#27599;&#20010;&#31867;&#21035;&#21482;&#26377;&#24456;&#23569;&#30340;&#26679;&#26412;&#21487;&#29992;&#12290;&#22312;FSL&#20013;&#20915;&#31574;&#30340;&#35299;&#37322;&#27604;&#20256;&#32479;&#20998;&#31867;&#26356;&#21152;&#37325;&#35201;&#65292;&#22240;&#20026;&#38169;&#35823;&#30340;&#20960;&#29575;&#26356;&#22823;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;FSL&#26041;&#27861;&#37117;&#26159;&#40657;&#21283;&#23376;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26131;&#20110;&#29702;&#35299;&#30340;&#23646;&#24615;&#30340;&#22825;&#28982;&#21487;&#35299;&#37322;&#27169;&#22411;&#26469;&#22788;&#29702;FSL&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23646;&#24615;&#36873;&#25321;&#26426;&#21046;&#65292;&#20197;&#26377;&#25928;&#36807;&#28388;&#27599;&#20010;episode&#20013;&#19981;&#30456;&#20851;&#30340;&#23646;&#24615;&#12290;&#35813;&#23646;&#24615;&#36873;&#25321;&#26426;&#21046;&#36890;&#36807;&#20943;&#23569;&#27599;&#20010;episode&#20013;&#28041;&#21450;&#30340;&#23646;&#24615;&#25968;&#37327;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#33258;&#21160;&#26816;&#27979;&#20154;&#24037;&#26234;&#33021;&#23646;&#24615;&#27744;&#19981;&#36275;&#30340;episode&#65292;&#24182;&#36890;&#36807;&#28041;&#21450;&#23398;&#20064;&#30340;&#26410;&#30693;&#23646;&#24615;&#26469;&#34917;&#20607;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#19982;&#40657;&#21283;&#23376;&#23567;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning (FSL) is a challenging learning problem in which only a few samples are available for each class. Decision interpretation is more important in few-shot classification since there is a greater chance of error than in traditional classification. However, most of the previous FSL methods are black-box models. In this paper, we propose an inherently interpretable model for FSL based on human-friendly attributes. Moreover, we propose an online attribute selection mechanism that can effectively filter out irrelevant attributes in each episode. The attribute selection mechanism improves the accuracy and helps with interpretability by reducing the number of participated attributes in each episode. We propose a mechanism that automatically detects the episodes where the pool of human-friendly attributes are not adequate, and compensates by engaging learned unknown attributes. We demonstrate that the proposed method achieves results on par with black-box few-shot-learning model
&lt;/p&gt;</description></item></channel></rss>