<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36890;&#36807;&#23450;&#20041;&#35821;&#20041;&#29420;&#31435;&#21306;&#22495;&#65288;SIRs&#65289;&#24182;&#35774;&#35745;HOmogeneous&#35270;&#35273;tOKenizer (HOOK)&#65292;&#23454;&#29616;&#20102;&#20351;&#29992;&#26377;&#24847;&#20041;&#30340;&#22522;&#26412;&#20803;&#32032;&#26469;&#21152;&#24378;&#36965;&#24863;&#22270;&#20687;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.18593</link><description>&lt;p&gt;
&#22343;&#21248;&#20998;&#35789;&#22120;&#30340;&#37325;&#35201;&#24615;&#65306;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#29702;&#35299;&#30340;&#22343;&#21248;&#35270;&#35273;&#20998;&#35789;&#22120;
&lt;/p&gt;
&lt;p&gt;
Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote Sensing Image Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18593
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23450;&#20041;&#35821;&#20041;&#29420;&#31435;&#21306;&#22495;&#65288;SIRs&#65289;&#24182;&#35774;&#35745;HOmogeneous&#35270;&#35273;tOKenizer (HOOK)&#65292;&#23454;&#29616;&#20102;&#20351;&#29992;&#26377;&#24847;&#20041;&#30340;&#22522;&#26412;&#20803;&#32032;&#26469;&#21152;&#24378;&#36965;&#24863;&#22270;&#20687;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#22120;&#20316;&#20026;&#22823;&#22411;&#27169;&#22411;&#30340;&#22522;&#26412;&#32452;&#20214;&#20043;&#19968;&#65292;&#38271;&#26399;&#20197;&#26469;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#34987;&#24573;&#35270;&#29978;&#33267;&#35823;&#35299;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#29702;&#35299;&#33021;&#21147;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#33258;&#28982;&#35821;&#35328;&#26631;&#35760;&#22120;&#21033;&#29992;&#26377;&#24847;&#20041;&#30340;&#35789;&#25110;&#23376;&#35789;&#20316;&#20026;&#35821;&#35328;&#30340;&#22522;&#26412;&#20803;&#32032;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20197;&#22522;&#20110;&#34917;&#19969;&#30340;&#26041;&#27861;&#22914;Patch Embed&#20026;&#20195;&#34920;&#30340;&#20027;&#27969;&#35270;&#35273;&#26631;&#35760;&#22120;&#20381;&#36182;&#20110;&#26080;&#24847;&#20041;&#30340;&#30697;&#24418;&#34917;&#19969;&#20316;&#20026;&#35270;&#35273;&#30340;&#22522;&#26412;&#20803;&#32032;&#65292;&#36825;&#19981;&#33021;&#20687;&#35821;&#35328;&#20013;&#30340;&#35789;&#25110;&#23376;&#35789;&#19968;&#26679;&#26377;&#25928;&#22320;&#21457;&#25381;&#20316;&#29992;&#12290;&#20174;&#26631;&#35760;&#22120;&#30340;&#26412;&#36136;&#20986;&#21457;&#65292;&#25105;&#20204;&#20026;&#35270;&#35273;&#23450;&#20041;&#20102;&#35821;&#20041;&#29420;&#31435;&#21306;&#22495;&#65288;SIRs&#65289;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;HOmogeneous&#35270;&#35273;tOKenizer: HOOK&#12290;HOOK&#20027;&#35201;&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#29289;&#20307;&#24863;&#30693;&#27169;&#22359;&#65288;OPM&#65289;&#21644;&#29289;&#20307;&#30690;&#37327;&#21270;&#27169;&#22359;&#65288;OVM&#65289;&#12290;&#20026;&#23454;&#29616;&#22343;&#21248;&#24615;&#65292;OPM&#23558;&#22270;&#20687;&#20998;&#21106;&#20026;4*4&#20687;&#32032;&#31181;&#23376;&#65292;&#28982;&#21518;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18593v1 Announce Type: cross  Abstract: The tokenizer, as one of the fundamental components of large models, has long been overlooked or even misunderstood in visual tasks. One key factor of the great comprehension power of the large language model is that natural language tokenizers utilize meaningful words or subwords as the basic elements of language. In contrast, mainstream visual tokenizers, represented by patch-based methods such as Patch Embed, rely on meaningless rectangular patches as basic elements of vision, which cannot serve as effectively as words or subwords in language. Starting from the essence of the tokenizer, we defined semantically independent regions (SIRs) for vision. We designed a simple HOmogeneous visual tOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception Module (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity, the OPM splits the image into 4*4 pixel seeds and then utilizes the attention mechanism to pe
&lt;/p&gt;</description></item><item><title>&#36229;&#21442;&#25968;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#34987;&#24378;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28041;&#21450;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#35780;&#20272;&#38454;&#27573;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;</title><link>https://arxiv.org/abs/2403.09066</link><description>&lt;p&gt;
Continual Learning&#20013;&#30340;&#36229;&#21442;&#25968;&#65306;&#29616;&#23454;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Hyperparameters in Continual Learning: a Reality Check
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09066
&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#34987;&#24378;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28041;&#21450;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#35780;&#20272;&#38454;&#27573;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#31639;&#27861;&#26088;&#22312;&#22312;CL&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#32531;&#35299;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#35843;&#25972;&#27599;&#31181;&#31639;&#27861;&#30340;&#36866;&#24403;&#36229;&#21442;&#25968;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#26412;&#25991;&#20027;&#24352;&#29616;&#34892;&#30340;&#35780;&#20272;&#21327;&#35758;&#26082;&#19981;&#20999;&#23454;&#38469;&#65292;&#20063;&#26080;&#27861;&#26377;&#25928;&#35780;&#20272;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09066v1 Announce Type: new  Abstract: Various algorithms for continual learning (CL) have been designed with the goal of effectively alleviating the trade-off between stability and plasticity during the CL process. To achieve this goal, tuning appropriate hyperparameters for each algorithm is essential. As an evaluation protocol, it has been common practice to train a CL algorithm using diverse hyperparameter values on a CL scenario constructed with a benchmark dataset. Subsequently, the best performance attained with the optimal hyperparameter value serves as the criterion for evaluating the CL algorithm. In this paper, we contend that this evaluation protocol is not only impractical but also incapable of effectively assessing the CL capability of a CL algorithm. Returning to the fundamental principles of model evaluation in machine learning, we propose an evaluation protocol that involves Hyperparameter Tuning and Evaluation phases. Those phases consist of different datase
&lt;/p&gt;</description></item><item><title>FogGuard&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38654;&#22825;&#27668;&#26465;&#20214;&#25361;&#25112;&#30340;&#26032;&#22411;&#38654;&#24863;&#30693;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#24494;&#35843;&#25968;&#25454;&#25910;&#38598;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08939</link><description>&lt;p&gt;
FogGuard: &#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#20445;&#25252;YOLO&#20813;&#21463;&#38654;&#38718;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
FogGuard: guarding YOLO against fog using perceptual loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08939
&lt;/p&gt;
&lt;p&gt;
FogGuard&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38654;&#22825;&#27668;&#26465;&#20214;&#25361;&#25112;&#30340;&#26032;&#22411;&#38654;&#24863;&#30693;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#24494;&#35843;&#25968;&#25454;&#25910;&#38598;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38654;&#24863;&#30693;&#30446;&#26631;&#26816;&#27979;&#32593;&#32476;&#65292;&#31216;&#20026;FogGuard&#65292;&#26088;&#22312;&#35299;&#20915;&#38654;&#22825;&#27668;&#26465;&#20214;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20005;&#37325;&#20381;&#36182;&#20934;&#30830;&#30340;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#65292;&#20294;&#24694;&#21155;&#30340;&#22825;&#27668;&#26465;&#20214;&#20250;&#26174;&#33879;&#24433;&#21709;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21487;&#38752;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#21487;&#20998;&#20026;&#20004;&#31867;&#65292;1&#65289;&#22270;&#20687;&#22686;&#24378;&#65288;&#22914;IA-YOLO&#65289;&#21644;2&#65289;&#22522;&#20110;&#39046;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;&#22270;&#20687;&#22686;&#24378;&#25216;&#26415;&#35797;&#22270;&#29983;&#25104;&#26080;&#38654;&#22270;&#20687;&#65292;&#28982;&#32780;&#65292;&#20174;&#26377;&#38654;&#22270;&#20687;&#20013;&#24674;&#22797;&#26080;&#38654;&#22270;&#20687;&#27604;&#22312;&#26377;&#38654;&#22270;&#20687;&#20013;&#26816;&#27979;&#23545;&#35937;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#39046;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#27809;&#26377;&#21033;&#29992;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#36825;&#20004;&#31867;&#26041;&#27861;&#37117;&#22312;&#23581;&#35797;&#35299;&#20915;&#38382;&#39064;&#30340;&#26356;&#38590;&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#23545;&#21407;&#22987;&#26631;&#27880;&#25968;&#25454;&#30340;&#24494;&#35843;&#20043;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08939v1 Announce Type: cross  Abstract: In this paper, we present a novel fog-aware object detection network called FogGuard, designed to address the challenges posed by foggy weather conditions. Autonomous driving systems heavily rely on accurate object detection algorithms, but adverse weather conditions can significantly impact the reliability of deep neural networks (DNNs).   Existing approaches fall into two main categories, 1) image enhancement such as IA-YOLO 2) domain adaptation based approaches. Image enhancement based techniques attempt to generate fog-free image. However, retrieving a fogless image from a foggy image is a much harder problem than detecting objects in a foggy image. Domain-adaptation based approaches, on the other hand, do not make use of labelled datasets in the target domain. Both categories of approaches are attempting to solve a harder version of the problem. Our approach builds over fine-tuning on the   Our framework is specifically designed t
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20223;&#30495;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#20026;&#35299;&#20915;&#29289;&#20307;&#25805;&#32437;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#21147;&#25903;&#25345;</title><link>https://arxiv.org/abs/2403.02338</link><description>&lt;p&gt;
&#29992;&#21452;&#25163;&#25197;&#24320;&#30422;&#23376;
&lt;/p&gt;
&lt;p&gt;
Twisting Lids Off with Two Hands
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02338
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20223;&#30495;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#20026;&#35299;&#20915;&#29289;&#20307;&#25805;&#32437;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#21147;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20004;&#21482;&#22810;&#25351;&#25163;&#33218;&#25805;&#32437;&#29289;&#20307;&#19968;&#30452;&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#39033;&#38271;&#26399;&#25361;&#25112;&#65292;&#21407;&#22240;&#22312;&#20110;&#35768;&#22810;&#25805;&#32437;&#20219;&#21153;&#30340;&#20016;&#23500;&#25509;&#35302;&#24615;&#36136;&#20197;&#21450;&#21327;&#35843;&#39640;&#32500;&#24230;&#21452;&#25163;&#31995;&#32479;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20351;&#29992;&#20004;&#21482;&#25163;&#25197;&#24320;&#21508;&#31181;&#29942;&#23376;&#30422;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20986;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#21487;&#20197;&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#36890;&#36807;&#23545;&#29289;&#29702;&#24314;&#27169;&#12289;&#23454;&#26102;&#24863;&#30693;&#21644;&#22870;&#21169;&#35774;&#35745;&#30340;&#26032;&#24037;&#31243;&#35265;&#35299;&#65292;&#35813;&#31574;&#30053;&#23637;&#31034;&#20102;&#19968;&#33324;&#21270;&#33021;&#21147;&#65292;&#33021;&#22815;&#36143;&#31359;&#21508;&#31181;&#30475;&#19981;&#35265;&#30340;&#29289;&#20307;&#65292;&#23637;&#31034;&#20986;&#21160;&#24577;&#21644;&#28789;&#24039;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#35777;&#26126;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20223;&#30495;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#20173;&#28982;&#26159;&#35299;&#20915;&#21069;&#25152;&#26410;&#26377;&#22797;&#26434;&#38382;&#39064;&#30340;&#25805;&#32437;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02338v1 Announce Type: cross  Abstract: Manipulating objects with two multi-fingered hands has been a long-standing challenge in robotics, attributed to the contact-rich nature of many manipulation tasks and the complexity inherent in coordinating a high-dimensional bimanual system. In this work, we consider the problem of twisting lids of various bottle-like objects with two hands, and demonstrate that policies trained in simulation using deep reinforcement learning can be effectively transferred to the real world. With novel engineering insights into physical modeling, real-time perception, and reward design, the policy demonstrates generalization capabilities across a diverse set of unseen objects, showcasing dynamic and dexterous behaviors. Our findings serve as compelling evidence that deep reinforcement learning combined with sim-to-real transfer remains a promising approach for addressing manipulation problems of unprecedented complexity.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#31163;&#25955;&#28508;&#21464;&#37327;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30524;&#37096;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#26469;&#23398;&#20064;&#30524;&#24213;&#22270;&#20687;&#20013;&#30524;&#37096;&#30142;&#30149;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16865</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#31163;&#25955;&#28508;&#21464;&#37327;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#25552;&#39640;&#30524;&#37096;&#30142;&#30149;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improve Robustness of Eye Disease Detection by including Learnable Probabilistic Discrete Latent Variables into Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16865
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#31163;&#25955;&#28508;&#21464;&#37327;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30524;&#37096;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#26469;&#23398;&#20064;&#30524;&#24213;&#22270;&#20687;&#20013;&#30524;&#37096;&#30142;&#30149;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30524;&#37096;&#30142;&#30149;&#20174;&#31958;&#23615;&#30149;&#24615;&#35270;&#32593;&#33180;&#30149;&#21464;&#21040;&#38738;&#20809;&#30524;&#31561;&#65292;&#30001;&#20110;&#20854;&#39640;&#21457;&#30149;&#29575;&#21644;&#21487;&#33021;&#23548;&#33268;&#35270;&#21147;&#25439;&#23475;&#65292;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20844;&#20849;&#21355;&#29983;&#25361;&#25112;&#12290;&#21450;&#26089;&#21644;&#20934;&#30830;&#30340;&#35786;&#26029;&#23545;&#20110;&#26377;&#25928;&#27835;&#30103;&#21644;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#20998;&#26512;&#21307;&#23398;&#22270;&#20687;&#65288;&#21253;&#25324;&#30524;&#37096;&#22270;&#20687;&#65289;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#36825;&#23545;&#20020;&#24202;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;GFlowOut&#30340;&#26032;&#39062;&#24212;&#29992;&#65292;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#30340;&#27010;&#29575;&#26694;&#26550;&#26469;&#23398;&#20064;&#20851;&#20110;&#36749;&#23398;&#25513;&#30721;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#29992;&#20110;&#20351;&#29992;&#30524;&#24213;&#22270;&#20687;&#23545;&#30524;&#37096;&#30142;&#30149;&#36827;&#34892;&#20998;&#31867;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31283;&#20581;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20197;ResNet18&#21644;ViT&#27169;&#22411;&#20026;&#20027;&#24178;&#30340;GFlowOut&#26469;&#35782;&#21035;&#21508;&#31181;&#30524;&#37096;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16865v1 Announce Type: cross  Abstract: Ocular diseases, ranging from diabetic retinopathy to glaucoma, present a significant public health challenge due to their prevalence and potential for causing vision impairment. Early and accurate diagnosis is crucial for effective treatment and management.In recent years, deep learning models have emerged as powerful tools for analysing medical images, including ocular imaging . However, challenges persist in model interpretability and uncertainty estimation, which are critical for clinical decision-making. This study introduces a novel application of GFlowOut, leveraging the probabilistic framework of Generative Flow Networks (GFlowNets) to learn the posterior distribution over dropout masks, for the classification and analysis of ocular diseases using eye fundus images. We develop a robust and generalizable method that utilizes GFlowOut integrated with ResNet18 and ViT models as backbone in identifying various ocular conditions. Th
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;GPT-4&#29983;&#25104;&#25551;&#36848;&#24615;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;SAM&#38646;&#26679;&#26412;&#20998;&#21106;&#24615;&#33021;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#12290;</title><link>https://arxiv.org/abs/2402.15759</link><description>&lt;p&gt;
&#20351;&#29992;GPT-4&#29983;&#25104;&#25551;&#36848;&#24615;&#25552;&#31034;&#25552;&#39640;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;SAM&#38646;&#26679;&#26412;&#24615;&#33021;&#32780;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Increasing SAM Zero-Shot Performance on Multimodal Medical Images Using GPT-4 Generated Descriptive Prompts Without Human Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15759
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;GPT-4&#29983;&#25104;&#25551;&#36848;&#24615;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;SAM&#38646;&#26679;&#26412;&#20998;&#21106;&#24615;&#33021;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#38646;&#26679;&#26412;&#20998;&#21106;&#31639;&#27861;&#65292;&#21629;&#21517;&#20026;&#25991;&#26412;-&#35270;&#35273;-&#25552;&#31034;SAM&#65288;TV-SAM&#65289;&#65292;&#26080;&#38656;&#20219;&#20309;&#25163;&#21160;&#26631;&#27880;&#12290;TV-SAM&#34701;&#21512;&#24182;&#25972;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#12289;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;GLIP&#21644;&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#65292;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#33258;&#21160;&#29983;&#25104;&#25551;&#36848;&#24615;&#25991;&#26412;&#25552;&#31034;&#21644;&#35270;&#35273;&#36793;&#30028;&#26694;&#25552;&#31034;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;SAM&#29992;&#20110;&#38646;&#26679;&#26412;&#20998;&#21106;&#12290;&#22312;&#19971;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#28085;&#30422;&#20843;&#31181;&#25104;&#20687;&#27169;&#24335;&#65292;&#35777;&#26126;TV-SAM&#21487;&#20197;&#26377;&#25928;&#22320;&#36328;&#21508;&#31181;&#27169;&#24335;&#20998;&#21106;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#65292;&#26126;&#26174;&#20248;&#20110;SAM AUTO&#21644;GSAM, &#19982;&#37329;&#26631;&#20934;&#36793;&#30028;&#26694;&#25552;&#31034;&#30340;SAM BBOX&#24615;&#33021;&#22522;&#26412;&#21305;&#25932;&#65292;&#24182;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#65288;&#22914;ISIC&#21644;WBC&#65289;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;TV-SAM&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15759v1 Announce Type: cross  Abstract: This study develops and evaluates a novel multimodal medical image zero-shot segmentation algorithm named Text-Visual-Prompt SAM (TV-SAM) without any manual annotations. TV-SAM incorporates and integrates large language model GPT-4, Vision Language Model GLIP, and Segment Anything Model (SAM), to autonomously generate descriptive text prompts and visual bounding box prompts from medical images, thereby enhancing SAM for zero-shot segmentation. Comprehensive evaluations are implemented on seven public datasets encompassing eight imaging modalities to demonstrate that TV-SAM can effectively segment unseen targets across various modalities without additional training, significantly outperforming SAM AUTO and GSAM, closely matching the performance of SAM BBOX with gold standard bounding box prompts, and surpassing the state-of-the-art on specific datasets like ISIC and WBC. The study indicates that TV-SAM serves as an effective multimodal 
&lt;/p&gt;</description></item><item><title>GINNs&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20960;&#20309;&#20219;&#21153;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#65292;&#37319;&#29992;&#26174;&#24335;&#22810;&#26679;&#24615;&#25439;&#22833;&#20197;&#21450;&#21487;&#24494;&#25439;&#22833;&#26469;&#20943;&#36731;&#27169;&#24577;&#22349;&#32553;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#22797;&#26434;&#24615;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14009</link><description>&lt;p&gt;
&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Geometry-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14009
&lt;/p&gt;
&lt;p&gt;
GINNs&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20960;&#20309;&#20219;&#21153;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#65292;&#37319;&#29992;&#26174;&#24335;&#22810;&#26679;&#24615;&#25439;&#22833;&#20197;&#21450;&#21487;&#24494;&#25439;&#22833;&#26469;&#20943;&#36731;&#27169;&#24577;&#22349;&#32553;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#22797;&#26434;&#24615;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;GINNs&#65289;&#30340;&#27010;&#24565;&#65292;&#28085;&#30422;&#20102;&#65288;i&#65289;&#22312;&#20960;&#20309;&#32422;&#26463;&#19979;&#23398;&#20064;&#65292;&#65288;ii&#65289;&#31070;&#32463;&#22330;&#20316;&#20026;&#21512;&#36866;&#30340;&#34920;&#31034;&#65292;&#65288;iii&#65289;&#29983;&#25104;&#22312;&#20960;&#20309;&#20219;&#21153;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#27424;&#23450;&#31995;&#32479;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GINN&#30340;&#26500;&#24314;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#27492;&#21487;&#20197;&#34987;&#32431;&#32422;&#26463;&#39537;&#21160;&#22320;&#35270;&#20026;&#29983;&#25104;&#24314;&#27169;&#12290;&#25105;&#20204;&#22686;&#21152;&#20102;&#26174;&#24335;&#30340;&#22810;&#26679;&#24615;&#25439;&#22833;&#26469;&#20943;&#36731;&#27169;&#24577;&#22349;&#32553;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#32422;&#26463;&#65292;&#29305;&#21035;&#26159;&#32452;&#20214;&#30340;&#36830;&#36890;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#33707;&#23572;&#26031;&#29702;&#35770;&#23558;&#20854;&#36716;&#21270;&#20026;&#21487;&#24494;&#25439;&#22833;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#26029;&#22686;&#21152;&#22797;&#26434;&#24615;&#30340;&#20108;&#32500;&#21644;&#19977;&#32500;&#22330;&#26223;&#20013;&#65292;GINN&#23398;&#20064;&#33539;&#24335;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14009v1 Announce Type: new  Abstract: We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks. Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints. We add an explicit diversity loss to mitigate mode collapse. We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory. Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CodaMal&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20302;&#25104;&#26412;&#26174;&#24494;&#38236;&#19979;&#30111;&#30142;&#26816;&#27979;&#30340;&#23545;&#27604;&#22495;&#33258;&#36866;&#24212;&#65292;&#35299;&#20915;&#20102;HCM&#21644;LCM&#22270;&#20687;&#20043;&#38388;&#30340;&#22495;&#24046;&#24322;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.10478</link><description>&lt;p&gt;
CodaMal&#65306;&#20302;&#25104;&#26412;&#26174;&#24494;&#38236;&#19979;&#30340;&#30111;&#30142;&#26816;&#27979;&#30340;&#23545;&#27604;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
CodaMal: Contrastive Domain Adaptation for Malaria Detection in Low-Cost Microscopes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10478
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CodaMal&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20302;&#25104;&#26412;&#26174;&#24494;&#38236;&#19979;&#30111;&#30142;&#26816;&#27979;&#30340;&#23545;&#27604;&#22495;&#33258;&#36866;&#24212;&#65292;&#35299;&#20915;&#20102;HCM&#21644;LCM&#22270;&#20687;&#20043;&#38388;&#30340;&#22495;&#24046;&#24322;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30111;&#30142;&#26159;&#20840;&#29699;&#37325;&#22823;&#20581;&#24247;&#38382;&#39064;&#65292;&#20854;&#35786;&#26029;&#38656;&#35201;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#20302;&#25104;&#26412;&#26174;&#24494;&#38236;(LCM)&#19979;&#30340;&#26174;&#24494;&#22270;&#20687;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#20174;&#26174;&#24494;&#22270;&#20687;&#20013;&#36827;&#34892;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#26631;&#27880;&#30340;&#26174;&#29616;&#20986;&#21463;&#30111;&#30142;&#23492;&#29983;&#34411;&#24433;&#21709;&#30340;&#32454;&#32990;&#21450;&#20854;&#29983;&#21629;&#21608;&#26399;&#38454;&#27573;&#30340;&#22270;&#20687;&#12290;&#19982;&#20174;&#39640;&#25104;&#26412;&#26174;&#24494;&#38236;(HCM)&#20013;&#26631;&#27880;&#22270;&#20687;&#30456;&#27604;&#65292;&#20174;LCM&#20013;&#26631;&#27880;&#22270;&#20687;&#26174;&#33879;&#22686;&#21152;&#20102;&#21307;&#23398;&#19987;&#23478;&#30340;&#36127;&#25285;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#23454;&#38469;&#30340;&#35299;&#20915;&#26041;&#26696;&#24212;&#35813;&#22312;HCM&#22270;&#20687;&#19978;&#35757;&#32451;&#65292;&#33021;&#22815;&#22312;LCM&#22270;&#20687;&#19978;&#27979;&#35797;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#20316;&#21697;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CodaMal&#65288;&#23545;&#27604;&#22495;&#33258;&#36866;&#24212;&#29992;&#20110;&#30111;&#30142;&#26816;&#27979;&#65289;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10478v1 Announce Type: cross  Abstract: Malaria is a major health issue worldwide, and its diagnosis requires scalable solutions that can work effectively with low-cost microscopes (LCM). Deep learning-based methods have shown success in computer-aided diagnosis from microscopic images. However, these methods need annotated images that show cells affected by malaria parasites and their life stages. Annotating images from LCM significantly increases the burden on medical experts compared to annotating images from high-cost microscopes (HCM). For this reason, a practical solution would be trained on HCM images which should generalize well on LCM images during testing. While earlier methods adopted a multi-stage learning process, they did not offer an end-to-end approach. In this work, we present an end-to-end learning framework, named CodaMal (Contrastive Domain Adpation for Malaria). In order to bridge the gap between HCM (training) and LCM (testing), we propose a domain adap
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#30701;&#35270;&#39057;&#23545;&#35266;&#20247;&#24515;&#29702;&#20581;&#24247;&#30340;&#25233;&#37057;&#24433;&#21709;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21307;&#23398;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#20854;&#24433;&#21709;&#24182;&#37319;&#21462;&#30456;&#24212;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;</title><link>https://arxiv.org/abs/2402.10045</link><description>&lt;p&gt;
&#30701;&#35270;&#39057;&#21644;&#24515;&#29702;&#20581;&#24247;&#65306;&#22522;&#20110;&#30693;&#35782;&#23548;&#21521;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Short-Form Videos and Mental Health: A Knowledge-Guided Multimodal Neural Topic Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10045
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#30701;&#35270;&#39057;&#23545;&#35266;&#20247;&#24515;&#29702;&#20581;&#24247;&#30340;&#25233;&#37057;&#24433;&#21709;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21307;&#23398;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#20854;&#24433;&#21709;&#24182;&#37319;&#21462;&#30456;&#24212;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#35270;&#39057;&#27491;&#35797;&#22270;&#37325;&#26032;&#22609;&#36896;&#25972;&#20010;&#31038;&#20132;&#23186;&#20307;&#26223;&#35266;&#65292;&#28982;&#32780;&#19987;&#23478;&#20204;&#23545;&#20854;&#23545;&#35266;&#20247;&#30340;&#25233;&#37057;&#24433;&#21709;&#24863;&#21040;&#26497;&#24230;&#25285;&#24551;&#65292;&#36825;&#19968;&#28857;&#24050;&#30001;&#21307;&#23398;&#30740;&#31350;&#35777;&#26126;&#12290;&#20026;&#20102;&#38450;&#27490;&#24191;&#27867;&#24433;&#21709;&#65292;&#21508;&#24179;&#21488;&#28212;&#26395;&#39044;&#27979;&#36825;&#20123;&#35270;&#39057;&#23545;&#35266;&#20247;&#24515;&#29702;&#20581;&#24247;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#37319;&#21462;&#24178;&#39044;&#25514;&#26045;&#65292;&#27604;&#22914;&#20462;&#35746;&#25512;&#33616;&#31639;&#27861;&#21644;&#26174;&#31034;&#35266;&#20247;&#24910;&#37325;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#27979;&#26041;&#27861;&#32570;&#20047;&#19982;&#25233;&#37057;&#30151;&#30340;&#20020;&#24202;&#35777;&#23454;&#30340;&#22806;&#37096;&#29615;&#22659;&#22240;&#32032;&#30456;&#20851;&#30340;&#21307;&#23398;&#30693;&#35782;&#12290;&#20026;&#20102;&#32771;&#34385;&#36825;&#26679;&#30340;&#21307;&#23398;&#30693;&#35782;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#26041;&#27861;&#35770;&#23398;&#31185;&#8212;&#8212;&#31181;&#23376;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;NTMs&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31181;&#23376;NTMs&#23384;&#22312;&#21333;&#19968;&#26469;&#28304;&#20027;&#39064;&#12289;&#26410;&#30693;&#20027;&#39064;&#26469;&#28304;&#12289;&#27169;&#31946;&#30340;&#31181;&#23376;&#30417;&#30563;&#21644;&#27425;&#20248;&#30340;&#25910;&#25947;&#31561;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30693;&#35782;&#25351;&#23548;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;Knowledg...&#65288;&#24453;&#34917;&#20805;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10045v1 Announce Type: cross  Abstract: While short-form videos head to reshape the entire social media landscape, experts are exceedingly worried about their depressive impacts on viewers, as evidenced by medical studies. To prevent widespread consequences, platforms are eager to predict these videos' impact on viewers' mental health. Subsequently, they can take intervention measures, such as revising recommendation algorithms and displaying viewer discretion. Nevertheless, applicable predictive methods lack relevance to well-established medical knowledge, which outlines clinically proven external and environmental factors of depression. To account for such medical knowledge, we resort to an emergent methodological discipline, seeded Neural Topic Models (NTMs). However, existing seeded NTMs suffer from the limitations of single-origin topics, unknown topic sources, unclear seed supervision, and suboptimal convergence. To address those challenges, we develop a novel Knowledg
&lt;/p&gt;</description></item><item><title>ViGoR&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#25552;&#39640;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#23545;&#25509;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21644;&#33258;&#21160;&#21270;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#35270;&#35273;&#23545;&#25509;&#20013;&#30340;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06118</link><description>&lt;p&gt;
ViGoR&#65306;&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#25913;&#36827;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06118
&lt;/p&gt;
&lt;p&gt;
ViGoR&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#25552;&#39640;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#23545;&#25509;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21644;&#33258;&#21160;&#21270;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#35270;&#35273;&#23545;&#25509;&#20013;&#30340;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#24191;&#27867;&#30693;&#35782;&#19982;&#22270;&#20687;&#24863;&#30693;&#30456;&#32467;&#21512;&#65292;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#24448;&#24448;&#22312;&#35270;&#35273;&#36755;&#20837;&#20013;&#23384;&#22312;&#19981;&#20934;&#30830;&#30340;&#23545;&#25509;&#65292;&#23548;&#33268;&#38169;&#35823;&#65292;&#22914;&#20135;&#29983;&#24187;&#35273;&#30340;&#19981;&#23384;&#22312;&#22330;&#26223;&#20803;&#32032;&#12289;&#36951;&#28431;&#37325;&#35201;&#30340;&#22330;&#26223;&#37096;&#20998;&#65292;&#20197;&#21450;&#25512;&#27979;&#23545;&#35937;&#20043;&#38388;&#30340;&#23646;&#24615;&#21644;&#20851;&#31995;&#26102;&#20986;&#29616;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;ViGoR&#65288;&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#36827;&#34892;&#35270;&#35273;&#23545;&#25509;&#65289;&#65292;&#23427;&#21033;&#29992;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#26469;&#26174;&#33879;&#25552;&#21319;&#22522;&#20110;&#39044;&#35757;&#32451;&#22522;&#32447;&#30340;LVLMs&#30340;&#35270;&#35273;&#23545;&#25509;&#33021;&#21147;&#12290;&#36825;&#31181;&#25913;&#36827;&#36890;&#36807;&#20351;&#29992;&#27604;&#23436;&#20840;&#30417;&#30563;&#26356;&#20415;&#23452;&#30340;&#20154;&#24037;&#35780;&#20272;&#21644;&#33258;&#21160;&#21270;&#26041;&#27861;&#39640;&#25928;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#22810;&#20010;&#25351;&#26631;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
By combining natural language understanding and the generation capabilities and breadth of knowledge of large language models with image perception, recent large vision language models (LVLMs) have shown unprecedented reasoning capabilities in the real world. However, the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucinating nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes and relationships between objects. To address these issues, we introduce a novel framework, ViGoR (Visual Grounding Through Fine-Grained Reward Modeling) that utilizes fine-grained reward modeling to significantly enhance the visual grounding of LVLMs over pre-trained baselines. This improvement is efficiently achieved using much cheaper human evaluations instead of full supervisions, as well as automated methods. We show the effectiveness of our approach through numerous metrics on several benchmarks
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827; Latent Diffusion Model &#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861; ACE&#65292;&#20854;&#36890;&#36807;&#32479;&#19968;&#27169;&#24335;&#30340;&#39069;&#22806;&#35823;&#24046;&#26469;&#20419;&#20351;&#27169;&#22411;&#23398;&#20064;&#29305;&#23450;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#32988;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2310.04687</link><description>&lt;p&gt;
&#25913;&#36827;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Improving Adversarial Attacks on Latent Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04687
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827; Latent Diffusion Model &#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861; ACE&#65292;&#20854;&#36890;&#36807;&#32479;&#19968;&#27169;&#24335;&#30340;&#39069;&#22806;&#35823;&#24046;&#26469;&#20419;&#20351;&#27169;&#22411;&#23398;&#20064;&#29305;&#23450;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#32988;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545; Latent Diffusion Model (LDM)&#65292;&#36825;&#31181;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#38450;&#27490; LDM &#22312;&#26410;&#32463;&#25480;&#26435;&#30340;&#22270;&#20687;&#19978;&#36827;&#34892;&#24694;&#24847;&#24494;&#35843;&#30340;&#20445;&#25252;&#25163;&#27573;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#25915;&#20987;&#20250;&#23545; LDM &#39044;&#27979;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#35780;&#20998;&#20989;&#25968;&#28155;&#21152;&#39069;&#22806;&#30340;&#35823;&#24046;&#12290;&#22312;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340; LDM &#23398;&#20064;&#36890;&#36807;&#19968;&#20010;&#20559;&#24046;&#38477;&#20302;&#35823;&#24046;&#65292;&#20174;&#32780;&#36973;&#21463;&#25915;&#20987;&#24182;&#20351;&#29992;&#20559;&#24046;&#39044;&#27979;&#35780;&#20998;&#20989;&#25968;&#12290;&#22522;&#20110;&#36825;&#19968;&#21160;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#19968;&#33268;&#24471;&#20998;&#20989;&#25968;&#38169;&#35823;&#36827;&#34892;&#25915;&#20987;&#65288;ACE&#65289;&#26469;&#25913;&#36827; LDM &#30340;&#23545;&#25239;&#25915;&#20987;&#12290;ACE &#32479;&#19968;&#20102;&#28155;&#21152;&#21040;&#39044;&#27979;&#24471;&#20998;&#20989;&#25968;&#30340;&#39069;&#22806;&#35823;&#24046;&#30340;&#27169;&#24335;&#12290;&#36825;&#20419;&#20351;&#24494;&#35843;&#30340; LDM &#23398;&#20064;&#19982;&#23545;&#35780;&#20998;&#20989;&#25968;&#36827;&#34892;&#39044;&#27979;&#30340;&#20559;&#24046;&#23398;&#20064;&#30456;&#21516;&#30340;&#27169;&#24335;&#12290;&#28982;&#21518;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#24335;&#26469;&#25913;&#36827;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545; LDM &#30340;&#23545;&#25239;&#25915;&#20987;&#20013;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.04687v3 Announce Type: replace-cross  Abstract: Adversarial attacks on Latent Diffusion Model (LDM), the state-of-the-art image generative model, have been adopted as effective protection against malicious finetuning of LDM on unauthorized images. We show that these attacks add an extra error to the score function of adversarial examples predicted by LDM. LDM finetuned on these adversarial examples learns to lower the error by a bias, from which the model is attacked and predicts the score function with biases.   Based on the dynamics, we propose to improve the adversarial attack on LDM by Attacking with Consistent score-function Errors (ACE). ACE unifies the pattern of the extra error added to the predicted score function. This induces the finetuned LDM to learn the same pattern as a bias in predicting the score function. We then introduce a well-crafted pattern to improve the attack. Our method outperforms state-of-the-art methods in adversarial attacks on LDM.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10712</link><description>&lt;p&gt;
Q&amp;A&#25552;&#31034;&#65306;&#36890;&#36807;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#25552;&#31034;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#28385;&#36275;&#23545;&#22810;&#26679;&#19990;&#30028;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge. (arXiv:2401.10712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#65292;&#22238;&#31572;&#38656;&#35201;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#21644;&#19990;&#30028;&#30693;&#35782;&#30340;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;AI&#27169;&#22411;&#37197;&#22791;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#31867;&#30340;&#35748;&#30693;&#26041;&#26696;&#23578;&#26410;&#31995;&#32479;&#22320;&#34987;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30456;&#20449;&#65292;&#22914;&#26524;&#25105;&#20204;&#33021;&#23613;&#21487;&#33021;&#25910;&#38598;&#32473;&#23450;&#22270;&#20687;&#20013;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#25105;&#20204;&#23558;&#33021;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#22270;&#20687;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#38382;&#39064;&#65292;&#26356;&#23481;&#26131;&#22238;&#24518;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#26368;&#32456;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;&#20687;&#20013;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#36825;&#20123;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#24182;&#23558;&#23427;&#20204;&#20316;&#20026;&#25552;&#31034;&#21457;&#36865;&#21040;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22270;&#20687;-&#31572;&#26696;&#23545;&#21644;&#30456;&#24212;&#30340;&#38382;&#39064;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35757;&#32451;&#19968;&#20010;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&amp;A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question gener
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#30005;&#24433;&#28023;&#25253;&#22270;&#20687;&#65292;&#35299;&#23494;&#20102;&#30005;&#24433;&#28023;&#25253;&#30340;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#22810;&#26631;&#31614;&#30005;&#24433;&#31867;&#22411;&#35782;&#21035;&#26041;&#27861;&#65292;&#26080;&#38656;&#20351;&#29992;&#20854;&#20182;&#25991;&#26412;&#25110;&#20803;&#25968;&#25454;&#20449;&#24687;&#65292;&#20855;&#26377;&#25512;&#24191;&#21644;&#33829;&#38144;&#30005;&#24433;&#30340;&#23454;&#38469;&#24212;&#29992;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.12022</link><description>&lt;p&gt;
&#35299;&#23494;&#30005;&#24433;&#28023;&#25253;&#30340;&#35270;&#35273;&#29305;&#24449;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#30005;&#24433;&#31867;&#22411;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Demystifying Visual Features of Movie Posters for Multi-Label Genre Identification. (arXiv:2309.12022v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#30005;&#24433;&#28023;&#25253;&#22270;&#20687;&#65292;&#35299;&#23494;&#20102;&#30005;&#24433;&#28023;&#25253;&#30340;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#22810;&#26631;&#31614;&#30005;&#24433;&#31867;&#22411;&#35782;&#21035;&#26041;&#27861;&#65292;&#26080;&#38656;&#20351;&#29992;&#20854;&#20182;&#25991;&#26412;&#25110;&#20803;&#25968;&#25454;&#20449;&#24687;&#65292;&#20855;&#26377;&#25512;&#24191;&#21644;&#33829;&#38144;&#30005;&#24433;&#30340;&#23454;&#38469;&#24212;&#29992;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#24433;&#34892;&#19994;&#20013;&#65292;&#30005;&#24433;&#28023;&#25253;&#22810;&#24180;&#26469;&#19968;&#30452;&#26159;&#24191;&#21578;&#21644;&#33829;&#38144;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#20351;&#22312;&#29616;&#20170;&#30340;&#25968;&#23383;&#28023;&#25253;&#36890;&#36807;&#22312;&#32447;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;OTT&#24179;&#21488;&#19978;&#20173;&#28982;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36890;&#24120;&#65292;&#30005;&#24433;&#28023;&#25253;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#24191;&#21644;&#20256;&#36798;&#30005;&#24433;&#30340;&#26412;&#36136;&#65292;&#20363;&#22914;&#20854;&#31867;&#22411;&#12289;&#35270;&#35273;&#39118;&#26684;/&#35843;&#35843;&#12289;&#27675;&#22260;&#21644;&#25925;&#20107;&#32447;&#32034;/&#20027;&#39064;&#65292;&#36825;&#20123;&#23545;&#21560;&#24341;&#28508;&#22312;&#35266;&#20247;&#38750;&#24120;&#37325;&#35201;&#12290;&#23545;&#30005;&#24433;&#31867;&#22411;&#36827;&#34892;&#35782;&#21035;&#24120;&#24120;&#22312;&#21521;&#30446;&#26631;&#35266;&#20247;&#25512;&#33616;&#30005;&#24433;&#26102;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20043;&#21069;&#30340;&#30005;&#24433;&#31867;&#22411;&#35782;&#21035;&#30740;&#31350;&#20165;&#38480;&#20110;&#23383;&#24149;&#12289;&#21095;&#24773;&#31616;&#20171;&#21644;&#30005;&#24433;&#22330;&#26223;&#65292;&#36825;&#20123;&#22823;&#22810;&#25968;&#22312;&#30005;&#24433;&#21457;&#24067;&#21518;&#25165;&#33021;&#33719;&#21462;&#12290;&#28023;&#25253;&#36890;&#24120;&#21253;&#21547;&#22312;&#21457;&#34892;&#21069;&#38544;&#21547;&#30340;&#20449;&#24687;&#26469;&#24341;&#36215;&#22823;&#37327;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#30005;&#24433;&#28023;&#25253;&#22270;&#20687;&#20013;&#33258;&#21160;&#36827;&#34892;&#22810;&#26631;&#31614;&#30005;&#24433;&#31867;&#22411;&#35782;&#21035;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20851;&#20110;&#30005;&#24433;&#30340;&#38468;&#21152;&#25991;&#26412;/&#20803;&#25968;&#25454;&#20449;&#24687;&#30340;&#24110;&#21161;&#65292;&#36825;&#26159;&#20854;&#20013;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the film industry, movie posters have been an essential part of advertising and marketing for many decades, and continue to play a vital role even today in the form of digital posters through online, social media and OTT platforms. Typically, movie posters can effectively promote and communicate the essence of a film, such as its genre, visual style/ tone, vibe and storyline cue/ theme, which are essential to attract potential viewers. Identifying the genres of a movie often has significant practical applications in recommending the film to target audiences. Previous studies on movie genre identification are limited to subtitles, plot synopses, and movie scenes that are mostly accessible after the movie release. Posters usually contain pre-release implicit information to generate mass interest. In this paper, we work for automated multi-label genre identification only from movie poster images, without any aid of additional textual/meta-data information about movies, which is one of 
&lt;/p&gt;</description></item><item><title>AR-TTA&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#12290;&#36890;&#36807;&#23558;&#20869;&#23384;&#32531;&#20914;&#21306;&#32435;&#20837;&#33258;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#26681;&#25454;&#25968;&#25454;&#27969;&#30340;&#24378;&#24230;&#36827;&#34892;&#21160;&#24577;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10109</link><description>&lt;p&gt;
AR-TTA: &#19968;&#31181;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AR-TTA: A Simple Method for Real-World Continual Test-Time Adaptation. (arXiv:2309.10109v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10109
&lt;/p&gt;
&lt;p&gt;
AR-TTA&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#12290;&#36890;&#36807;&#23558;&#20869;&#23384;&#32531;&#20914;&#21306;&#32435;&#20837;&#33258;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#26681;&#25454;&#25968;&#25454;&#27969;&#30340;&#24378;&#24230;&#36827;&#34892;&#21160;&#24577;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#23427;&#20801;&#35768;&#28304;&#27169;&#22411;&#22312;&#27809;&#26377;&#20219;&#20309;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#21482;&#26159;&#23454;&#38469;&#22330;&#26223;&#31616;&#21270;&#29256;&#26412;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26368;&#36817;&#25512;&#20986;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;CLAD-C&#21644;SHIFT&#26469;&#39564;&#35777;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24403;&#21069;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#26377;&#25928;&#22788;&#29702;&#19981;&#21516;&#31243;&#24230;&#30340;&#22495;&#20559;&#31227;&#65292;&#24120;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#20302;&#20110;&#28304;&#27169;&#22411;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#38382;&#39064;&#30340;&#26681;&#28304;&#22312;&#20110;&#26080;&#27861;&#20445;&#30041;&#28304;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#26080;&#27861;&#36866;&#24212;&#21160;&#24577;&#21464;&#21270;&#12289;&#26102;&#38388;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#19968;&#20010;&#23567;&#30340;&#20869;&#23384;&#32531;&#20914;&#21306;&#32435;&#20837;&#21040;&#25104;&#29087;&#30340;&#33258;&#35757;&#32451;&#26694;&#26550;&#20013;&#65292;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#21516;&#26102;&#26681;&#25454;&#25968;&#25454;&#27969;&#30340;&#24378;&#24230;&#36827;&#34892;&#21160;&#24577;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-time adaptation is a promising research direction that allows the source model to adapt itself to changes in data distribution without any supervision. Yet, current methods are usually evaluated on benchmarks that are only a simplification of real-world scenarios. Hence, we propose to validate test-time adaptation methods using the recently introduced datasets for autonomous driving, namely CLAD-C and SHIFT. We observe that current test-time adaptation methods struggle to effectively handle varying degrees of domain shift, often resulting in degraded performance that falls below that of the source model. We noticed that the root of the problem lies in the inability to preserve the knowledge of the source model and adapt to dynamically changing, temporally correlated data streams. Therefore, we enhance well-established self-training framework by incorporating a small memory buffer to increase model stability and at the same time perform dynamic adaptation based on the intensity of 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#35757;&#32451;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.01937</link><description>&lt;p&gt;
&#20351;&#29992;&#32452;&#21512;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Training Data Protection with Compositional Diffusion Models. (arXiv:2308.01937v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01937
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#35757;&#32451;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#65292;&#19968;&#31181;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#19978;&#35757;&#32451;&#19981;&#21516;&#25193;&#25955;&#27169;&#22411;&#65288;&#25110;&#25552;&#31034;&#65289;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#21333;&#29420;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#23396;&#31435;&#29366;&#24577;&#19979;&#12289;&#22312;&#19981;&#21516;&#26102;&#38388;&#12289;&#22312;&#19981;&#21516;&#20998;&#24067;&#21644;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#21518;&#32493;&#32452;&#21512;&#20197;&#36798;&#21040;&#19982;&#21516;&#26102;&#35757;&#32451;&#25152;&#26377;&#25968;&#25454;&#30340;&#29702;&#24819;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#27169;&#22411;&#21482;&#21253;&#21547;&#20854;&#22312;&#35757;&#32451;&#26399;&#38388;&#25509;&#35302;&#21040;&#30340;&#25968;&#25454;&#23376;&#38598;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#31181;&#24418;&#24335;&#30340;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#12290;&#29305;&#21035;&#26159;&#65292;CDM&#26159;&#31532;&#19968;&#31181;&#21487;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#36951;&#24536;&#21644;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20801;&#35768;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;CDM&#36824;&#21487;&#20197;&#30830;&#23450;&#29983;&#25104;&#29305;&#23450;&#26679;&#26412;&#30340;&#25968;&#25454;&#23376;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Compartmentalized Diffusion Models (CDM), a method to train different diffusion models (or prompts) on distinct data sources and arbitrarily compose them at inference time. The individual models can be trained in isolation, at different times, and on different distributions and domains and can be later composed to achieve performance comparable to a paragon model trained on all data simultaneously. Furthermore, each model only contains information about the subset of the data it was exposed to during training, enabling several forms of training data protection. In particular, CDMs are the first method to enable both selective forgetting and continual learning for large-scale diffusion models, as well as allowing serving customized models based on the user's access rights. CDMs also allow determining the importance of a subset of the data in generating particular samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#27880;&#37322;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#20316;&#20026;&#30417;&#30563;&#65292;&#24182;&#25972;&#21512;&#26816;&#32034;&#36807;&#31243;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#25928;&#21147;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#19981;&#21305;&#37197;&#30340;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30340;&#30701;&#21306;&#22495;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#20854;&#29983;&#25104;&#22810;&#26679;&#30340;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2307.14750</link><description>&lt;p&gt;
&#26080;&#27880;&#37322;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#30740;&#31350;&#65306;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#20266;&#21477;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Exploring Annotation-free Image Captioning with Retrieval-augmented Pseudo Sentence Generation. (arXiv:2307.14750v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#27880;&#37322;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#20316;&#20026;&#30417;&#30563;&#65292;&#24182;&#25972;&#21512;&#26816;&#32034;&#36807;&#31243;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#25928;&#21147;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#19981;&#21305;&#37197;&#30340;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30340;&#30701;&#21306;&#22495;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#20854;&#29983;&#25104;&#22810;&#26679;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;&#26080;&#27880;&#37322;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#20004;&#31181;&#31574;&#30053;&#65306;&#20174;&#19981;&#21305;&#37197;&#30340;&#35821;&#26009;&#24211;&#20013;&#33719;&#21462;&#21477;&#23376;&#65292;&#24182;&#23558;&#20854;&#19982;&#32473;&#23450;&#30340;&#22270;&#20687;&#23545;&#40784;&#20316;&#20026;&#20266;&#27880;&#37322;&#65292;&#25110;&#32773;&#20351;&#29992;&#22806;&#37096;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#23545;&#29983;&#25104;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#40784;&#30340;&#35774;&#32622;&#23384;&#22312;&#36136;&#37327;&#38382;&#39064;&#65292;&#20854;&#24615;&#33021;&#20284;&#20046;&#24050;&#32463;&#36798;&#21040;&#20102;&#26497;&#38480;&#65292;&#32780;&#39044;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#8220;LPM + &#26816;&#32034;&#22686;&#24378;&#23398;&#20064;&#8221;&#65292;&#21033;&#29992;&#26469;&#33258;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LPM&#65289;&#30340;&#20808;&#39564;&#30693;&#35782;&#20316;&#20026;&#30417;&#30563;&#65292;&#24182;&#25972;&#21512;&#26816;&#32034;&#36807;&#31243;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#25928;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#20266;&#21477;&#23376;&#29983;&#25104;&#65288;RaPSG&#65289;&#65292;&#37319;&#29992;&#39640;&#25928;&#30340;&#26041;&#27861;&#20174;&#19981;&#21305;&#37197;&#30340;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#20986;&#39640;&#30456;&#20851;&#30340;&#30701;&#21306;&#22495;&#25551;&#36848;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training an image captioner without annotated image-sentence pairs has gained traction in recent years. Previous approaches can be categorized into two strategies: crawling sentences from mismatching corpora and aligning them with the given images as pseudo annotations, or pre-training the captioner using external image-text pairs. However, the aligning setting seems to reach its performance limit due to the quality problem of pairs, and pre-training requires significant computational resources. To address these challenges, we propose a new strategy ``LPM + retrieval-augmented learning" where the prior knowledge from large pre-trained models (LPMs) is leveraged as supervision, and a retrieval process is integrated to further reinforce its effectiveness. Specifically, we introduce Retrieval-augmented Pseudo Sentence Generation (RaPSG), which adopts an efficient approach to retrieve highly relevant short region descriptions from the mismatching corpora and use them to generate a variety 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02694</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24230;&#37327;&#26041;&#27861;&#65306;&#19968;&#39033;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Loss Functions and Metrics in Deep Learning. A Review. (arXiv:2307.02694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#36873;&#25321;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#24230;&#37327;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#23427;&#20204;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#35780;&#35770;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#26368;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the essential components of deep learning is the choice of the loss function and performance metrics used to train and evaluate models. This paper reviews the most prevalent loss functions and performance measurements in deep learning. We examine the benefits and limits of each technique and illustrate their application to various deep-learning problems. Our review aims to give a comprehensive picture of the different loss functions and performance indicators used in the most common deep learning tasks and help practitioners choose the best method for their specific task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;S.T.A.R.-Track&#65292;&#19968;&#20010;&#37319;&#29992;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;Transformer&#26694;&#26550;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;3D&#29289;&#20307;&#36319;&#36394;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;&#21644;&#23398;&#20064;&#22411;&#36319;&#36394;&#23884;&#20837;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20934;&#30830;&#24314;&#27169;&#29289;&#20307;&#30340;&#20960;&#20309;&#36816;&#21160;&#21644;&#21464;&#21270;&#65292;&#24182;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17602</link><description>&lt;p&gt;
S.T.A.R.-Track&#65306;&#33258;&#36866;&#24212;&#26102;&#31354;&#22806;&#35980;&#34920;&#31034;&#30340;&#31471;&#21040;&#31471;3D&#29289;&#20307;&#36319;&#36394;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
S.T.A.R.-Track: Latent Motion Models for End-to-End 3D Object Tracking with Adaptive Spatio-Temporal Appearance Representations. (arXiv:2306.17602v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;S.T.A.R.-Track&#65292;&#19968;&#20010;&#37319;&#29992;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;Transformer&#26694;&#26550;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;3D&#29289;&#20307;&#36319;&#36394;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;&#21644;&#23398;&#20064;&#22411;&#36319;&#36394;&#23884;&#20837;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20934;&#30830;&#24314;&#27169;&#29289;&#20307;&#30340;&#20960;&#20309;&#36816;&#21160;&#21644;&#21464;&#21270;&#65292;&#24182;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#36319;&#36394;-&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#22522;&#20110;Transformer&#30340;3D&#36319;&#36394;&#26694;&#26550;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#36319;&#36394;&#26041;&#27861;&#36890;&#36807;&#20960;&#20309;&#36816;&#21160;&#27169;&#22411;&#34701;&#21512;&#24103;&#20043;&#38388;&#30340;&#29289;&#20307;&#21644;&#33258;&#36816;&#21160;&#30340;&#20960;&#20309;&#25928;&#24212;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;S.T.A.R.-Track&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;&#26469;&#35843;&#25972;&#23545;&#35937;&#26597;&#35810;&#65292;&#20197;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#30452;&#25509;&#32771;&#34385;&#35270;&#35282;&#21644;&#20809;&#29031;&#26465;&#20214;&#30340;&#21464;&#21270;&#65292;&#21516;&#26102;&#26126;&#30830;&#24314;&#27169;&#20960;&#20309;&#36816;&#21160;&#12290;&#32467;&#21512;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#23398;&#20064;&#30340;&#36319;&#36394;&#23884;&#20837;&#65292;&#26377;&#21161;&#20110;&#24314;&#27169;&#36712;&#36857;&#30340;&#23384;&#22312;&#27010;&#29575;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36319;&#36394;&#26694;&#26550;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#22522;&#20110;&#26597;&#35810;&#30340;&#26816;&#27979;&#22120;&#38598;&#25104;&#12290;&#22312;nuScenes&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;DETR3D&#30340;&#36319;&#36394;&#22120;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#36712;&#36857;&#30340;&#36523;&#20221;&#36716;&#25442;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the tracking-by-attention paradigm, this paper introduces an object-centric, transformer-based framework for tracking in 3D. Traditional model-based tracking approaches incorporate the geometric effect of object- and ego motion between frames with a geometric motion model. Inspired by this, we propose S.T.A.R.-Track, which uses a novel latent motion model (LMM) to additionally adjust object queries to account for changes in viewing direction and lighting conditions directly in the latent space, while still modeling the geometric motion explicitly. Combined with a novel learnable track embedding that aids in modeling the existence probability of tracks, this results in a generic tracking framework that can be integrated with any query-based detector. Extensive experiments on the nuScenes benchmark demonstrate the benefits of our approach, showing state-of-the-art performance for DETR3D-based trackers while drastically reducing the number of identity switches of tracks at the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37319;&#26679;&#22120;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#33021;&#22815;&#23454;&#29616;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2306.13196</link><description>&lt;p&gt;
DiMSam:&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#20219;&#21153;&#19982;&#21160;&#20316;&#35268;&#21010;&#20013;&#30340;&#37319;&#26679;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability. (arXiv:2306.13196v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37319;&#26679;&#22120;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#33021;&#22815;&#23454;&#29616;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#22320;&#35745;&#21010;&#38271;&#21608;&#26399;&#33258;&#20027;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23427;&#20204;&#38656;&#35201;&#19968;&#20010;&#35268;&#21010;&#27169;&#22411;&#65292;&#22240;&#27492;&#22312;&#29615;&#22659;&#21644;&#20854;&#21160;&#24577;&#19981;&#23436;&#20840;&#20102;&#35299;&#30340;&#39046;&#22495;&#20013;&#24212;&#29992;&#23427;&#20204;&#21487;&#33021;&#38750;&#24120;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#23398;&#20064;&#25429;&#33719;&#35268;&#21010;&#27169;&#22411;&#20013;&#38590;&#20197;&#35774;&#35745;&#30340;&#32422;&#26463;&#21644;&#37319;&#26679;&#22120;&#12290;&#36825;&#20123;&#23398;&#20064;&#37319;&#26679;&#22120;&#22312;TAMP&#27714;&#35299;&#22120;&#20013;&#32452;&#21512;&#21644;&#21512;&#24182;&#65292;&#20197;&#32852;&#21512;&#25214;&#21040;&#28385;&#36275;&#35268;&#21010;&#20013;&#32422;&#26463;&#30340;&#34892;&#21160;&#21442;&#25968;&#20540;&#12290;&#20026;&#20102;&#20415;&#20110;&#23545;&#29615;&#22659;&#20013;&#26410;&#30693;&#23545;&#35937;&#36827;&#34892;&#39044;&#27979;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#37319;&#26679;&#22120;&#23450;&#20041;&#20026;&#23398;&#20064;&#30340;&#20302;&#32500;&#28508;&#21464;&#37327;&#23884;&#20837;&#30340;&#21487;&#21464;&#23545;&#35937;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#20851;&#33410;&#24335;&#29289;&#20307;&#25805;&#20316;&#39046;&#22495;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#32463;&#20856;TAMP&#12289;&#29983;&#25104;&#23398;&#20064;&#21644;&#28508;&#22312;&#23884;&#20837;&#30340;&#32452;&#21512;&#22914;&#20309;&#20351;&#24471;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#36827;&#34892;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task and Motion Planning (TAMP) approaches are effective at planning long-horizon autonomous robot manipulation. However, because they require a planning model, it can be difficult to apply them to domains where the environment and its dynamics are not fully known. We propose to overcome these limitations by leveraging deep generative modeling, specifically diffusion models, to learn constraints and samplers that capture these difficult-to-engineer aspects of the planning model. These learned samplers are composed and combined within a TAMP solver in order to find action parameter values jointly that satisfy the constraints along a plan. To tractably make predictions for unseen objects in the environment, we define these samplers on low-dimensional learned latent embeddings of changing object state. We evaluate our approach in an articulated object manipulation domain and show how the combination of classical TAMP, generative learning, and latent embeddings enables long-horizon constra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#35299;&#20915;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#30340;&#23545;&#31216;&#24615;&#38480;&#21046;&#21644;&#24341;&#20837;&#20840;&#23616;&#20449;&#24687;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#22312;CIFAR-10/100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#23545;&#25239;&#35757;&#32451;&#21644;&#30693;&#35782;&#33976;&#39311;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13948</link><description>&lt;p&gt;
&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Decoupled Kullback-Leibler Divergence Loss. (arXiv:2305.13948v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#35299;&#20915;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#30340;&#23545;&#31216;&#24615;&#38480;&#21046;&#21644;&#24341;&#20837;&#20840;&#23616;&#20449;&#24687;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#22312;CIFAR-10/100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#23545;&#25239;&#35757;&#32451;&#21644;&#30693;&#35782;&#33976;&#39311;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26356;&#28145;&#20837;&#22320;&#25506;&#31350;&#20102;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#21457;&#29616;&#23427;&#19982;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#31561;&#20215;&#65292;&#21518;&#32773;&#30001;&#21152;&#26435;&#22343;&#26041;&#24046;&#25439;&#22833;&#21644;&#21253;&#21547;&#36719;&#26631;&#31614;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#32452;&#25104;&#12290;&#36890;&#36807;&#23545;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#30830;&#23450;&#20102;&#20004;&#20010;&#25913;&#36827;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#30693;&#35782;&#33976;&#39311;&#31561;&#22330;&#26223;&#19979;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#30340;&#23545;&#31216;&#24615;&#38480;&#21046;&#38382;&#39064;&#12290;&#36825;&#20010;&#25913;&#36827;&#20445;&#35777;&#20102;&#22312;&#35757;&#32451;&#26399;&#38388;wMSE&#32452;&#20214;&#22987;&#32456;&#26377;&#25928;&#65292;&#25552;&#20379;&#39069;&#22806;&#30340;&#26500;&#36896;&#24615;&#26263;&#31034;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#20840;&#23616;&#20449;&#24687;&#24341;&#20837;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#29992;&#20110;&#31867;&#20869;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#12290;&#36890;&#36807;&#36825;&#20004;&#20010;&#25913;&#36827;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25913;&#36827;&#30340;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;CIFAR-10/100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#65292;&#37325;&#28857;&#26159;&#23545;&#25239;&#35757;&#32451;&#21644;&#30693;&#35782;&#33976;&#39311;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we delve deeper into the Kullback-Leibler (KL) Divergence loss and observe that it is equivalent to the Doupled Kullback-Leibler (DKL) Divergence loss that consists of 1) a weighted Mean Square Error (wMSE) loss and 2) a Cross-Entropy loss incorporating soft labels. From our analysis of the DKL loss, we have identified two areas for improvement. Firstly, we address the limitation of DKL in scenarios like knowledge distillation by breaking its asymmetry property in training optimization. This modification ensures that the wMSE component is always effective during training, providing extra constructive cues. Secondly, we introduce global information into DKL for intra-class consistency regularization. With these two enhancements, we derive the Improved Kullback-Leibler (IKL) Divergence loss and evaluate its effectiveness by conducting experiments on CIFAR-10/100 and ImageNet datasets, focusing on adversarial training and knowledge distillation tasks. The proposed approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#38754;&#37096;&#29305;&#24449;&#21457;&#29616;&#20102;&#20154;&#20204;&#30340;&#25919;&#27835;&#21462;&#21521;&#65292;&#29978;&#33267;&#21487;&#20197;&#20174;&#33258;&#28982;&#22270;&#20687;&#20013;&#25512;&#24191;&#12290;&#36825;&#31181;&#39044;&#27979;&#30340;&#31934;&#24230;&#27604;&#20154;&#31867;&#35780;&#20998;&#32773;&#39640;&#65292;&#30456;&#24403;&#20110;&#19968;&#20123;&#24037;&#20316;&#38754;&#35797;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16343</link><description>&lt;p&gt;
&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#21487;&#20197;&#20174;&#38754;&#37096;&#22270;&#20687;&#20013;&#26174;&#31034;&#25919;&#27835;&#21462;&#21521;&#65292;&#21363;&#20351;&#25511;&#21046;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#21644;&#33258;&#25105;&#34920;&#29616;&#12290;(arXiv: 2303.16343v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
Facial recognition technology can expose political orientation from facial images even when controlling for demographics and self-presentation. (arXiv:2303.16343v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#38754;&#37096;&#29305;&#24449;&#21457;&#29616;&#20102;&#20154;&#20204;&#30340;&#25919;&#27835;&#21462;&#21521;&#65292;&#29978;&#33267;&#21487;&#20197;&#20174;&#33258;&#28982;&#22270;&#20687;&#20013;&#25512;&#24191;&#12290;&#36825;&#31181;&#39044;&#27979;&#30340;&#31934;&#24230;&#27604;&#20154;&#31867;&#35780;&#20998;&#32773;&#39640;&#65292;&#30456;&#24403;&#20110;&#19968;&#20123;&#24037;&#20316;&#38754;&#35797;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36816;&#29992;&#38754;&#37096;&#35782;&#21035;&#31639;&#27861;&#65292;&#20174;&#23454;&#39564;&#23460;&#35774;&#32622;&#19979;&#25293;&#25668;&#30340;591&#24352;&#20013;&#24615;&#38754;&#37096;&#22270;&#20687;&#20013;&#25552;&#21462;&#38754;&#37096;&#25551;&#36848;&#31526;&#12290;&#22312;&#25511;&#21046;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#26469;&#39044;&#27979;&#21442;&#19982;&#32773;&#22312;&#25919;&#27835;&#21462;&#21521;&#37327;&#34920;&#19978;&#30340;&#24471;&#20998;(Cronbach&#30340;&#945;=0.94)&#12290;&#27169;&#22411;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;r = 0.20&#65292;&#36828;&#20248;&#20110;&#20154;&#31867;&#35780;&#20998;&#32773;&#65292;&#19982;&#24037;&#20316;&#38754;&#35797;&#39044;&#27979;&#24037;&#20316;&#25104;&#21151;&#12289;&#37202;&#31934;&#39537;&#21160;&#25915;&#20987;&#24615;&#25110;&#24515;&#29702;&#27835;&#30103;&#25913;&#21892;&#24515;&#29702;&#20581;&#24247;&#30340;&#25928;&#26524;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#20174;&#26631;&#20934;&#21270;&#22270;&#20687;&#34893;&#29983;&#20986;&#30340;&#27169;&#22411;&#22312;3,401&#21517;&#26469;&#33258;&#32654;&#22269;&#12289;&#33521;&#22269;&#21644;&#21152;&#25343;&#22823;&#30340;&#25919;&#27835;&#20154;&#29289;&#30340;&#33258;&#28982;&#22270;&#20687;&#26679;&#26412;&#20013;&#34920;&#29616;&#33391;&#22909;(r = 0.12)&#65292;&#34920;&#26126;&#38754;&#37096;&#22806;&#35980;&#21644;&#25919;&#27835;&#21462;&#21521;&#20043;&#38388;&#30340;&#20851;&#32852;&#21487;&#25512;&#24191;&#21040;&#25105;&#20204;&#20043;&#22806;&#30340;&#20154;&#32676;&#12290;&#38754;&#37096;&#29305;&#24449;&#19982;&#25919;&#27835;&#21462;&#21521;&#30456;&#20851;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#20445;&#23432;&#27966;&#30340;&#19979;&#21322;&#33080;&#37096;&#20998;&#26356;&#22823;&#65292;&#34429;&#28982;&#25919;&#27835;&#21462;&#21521;&#19981;&#33021;&#20934;&#30830;&#22320;&#39044;&#27979;&#20010;&#20307;&#38754;&#37096;&#29305;&#24449;&#30340;&#25152;&#26377;&#21464;&#21270;&#65292;&#20294;&#26159;&#36825;&#31181;&#21457;&#29616;&#36824;&#26159;&#23500;&#26377;&#21551;&#21457;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A facial recognition algorithm was used to extract face descriptors from carefully standardized images of 591 neutral faces taken in the laboratory setting. Face descriptors were entered into a cross-validated linear regression to predict participants' scores on a political orientation scale (Cronbach's alpha=.94) while controlling for age, gender, and ethnicity. The model's performance exceeded r=.20: much better than that of human raters and on par with how well job interviews predict job success, alcohol drives aggressiveness, or psychological therapy improves mental health. Moreover, the model derived from standardized images performed well (r=.12) in a sample of naturalistic images of 3,401 politicians from the U.S., UK, and Canada, suggesting that the associations between facial appearance and political orientation generalize beyond our sample. The analysis of facial features associated with political orientation revealed that conservatives had larger lower faces, although politi
&lt;/p&gt;</description></item></channel></rss>