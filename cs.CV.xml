<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#29992;&#20110;&#35299;&#37322;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#20013;&#30340;&#20559;&#24046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;MORE&#65292;&#21516;&#26102;&#25552;&#20986;&#20004;&#31181;&#20943;&#36731;&#21333;&#27169;&#24577;&#20559;&#24046;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.18346</link><description>&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#21644;&#20943;&#36731;&#21333;&#27169;&#24577;&#20559;&#24046;&#65306;&#22240;&#26524;&#20851;&#31995;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18346
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#29992;&#20110;&#35299;&#37322;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#20013;&#30340;&#20559;&#24046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;MORE&#65292;&#21516;&#26102;&#25552;&#20986;&#20004;&#31181;&#20943;&#36731;&#21333;&#27169;&#24577;&#20559;&#24046;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#36827;&#20102;&#22810;&#27169;&#24577;LLMs&#65288;MLLMs&#65289;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;MLLMs&#36890;&#24120;&#36807;&#24230;&#20381;&#36182;&#21333;&#27169;&#24577;&#20559;&#24046;&#65288;&#20363;&#22914;&#35821;&#35328;&#20559;&#24046;&#21644;&#35270;&#35273;&#20559;&#24046;&#65289;&#65292;&#23548;&#33268;&#22312;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#32473;&#20986;&#19981;&#27491;&#30830;&#31572;&#26696;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#26469;&#35299;&#37322;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#38382;&#39064;&#20013;&#30340;&#20559;&#24046;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22240;&#26524;&#22270;&#26469;&#38416;&#26126;MLLMs&#23545;VQA&#38382;&#39064;&#30340;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#28145;&#20837;&#30340;&#22240;&#26524;&#20998;&#26512;&#35780;&#20272;&#20559;&#24046;&#30340;&#22240;&#26524;&#25928;&#26524;&#12290;&#21463;&#22240;&#26524;&#22270;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;MORE&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;12,000&#20010;VQA&#23454;&#20363;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#25361;&#25112;MLLMs&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#22810;&#36339;&#25512;&#29702;&#21644;&#20811;&#26381;&#21333;&#27169;&#24577;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#20943;&#36731;&#21333;&#27169;&#24577;&#20559;&#24046;&#24182;&#22686;&#24378;MLLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18346v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have facilitated the development of Multimodal LLMs (MLLMs). Despite their impressive capabilities, MLLMs often suffer from an over-reliance on unimodal biases (e.g., language bias and vision bias), leading to incorrect answers in complex multimodal tasks. To investigate this issue, we propose a causal framework to interpret the biases in Visual Question Answering (VQA) problems. Within our framework, we devise a causal graph to elucidate the predictions of MLLMs on VQA problems, and assess the causal effect of biases through an in-depth causal analysis. Motivated by the causal graph, we introduce a novel MORE dataset, consisting of 12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities, necessitating multi-hop reasoning and the surmounting of unimodal biases. Furthermore, we propose two strategies to mitigate unimodal biases and enhance MLLMs' reasoning capabiliti
&lt;/p&gt;</description></item><item><title>V-LoL&#26159;&#19968;&#20010;&#32467;&#21512;&#35270;&#35273;&#21644;&#36923;&#36753;&#25361;&#25112;&#30340;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;V-LoL-Trains&#65292;&#35813;&#25968;&#25454;&#38598;&#39318;&#27425;&#23558;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#21644;&#28789;&#27963;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#30740;&#31350;&#24191;&#27867;&#30340;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25361;&#25112;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2306.07743</link><description>&lt;p&gt;
V-LoL: &#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#30340;&#35786;&#26029;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
V-LoL: A Diagnostic Dataset for Visual Logical Learning. (arXiv:2306.07743v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07743
&lt;/p&gt;
&lt;p&gt;
V-LoL&#26159;&#19968;&#20010;&#32467;&#21512;&#35270;&#35273;&#21644;&#36923;&#36753;&#25361;&#25112;&#30340;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;V-LoL-Trains&#65292;&#35813;&#25968;&#25454;&#38598;&#39318;&#27425;&#23558;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#21644;&#28789;&#27963;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#30740;&#31350;&#24191;&#27867;&#30340;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25361;&#25112;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#26399;&#22312;&#35270;&#35273;AI&#39046;&#22495;&#26377;&#20102;&#35768;&#22810;&#25104;&#21151;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#19981;&#21516;&#30340;&#32570;&#28857;&#65307;&#21253;&#25324;&#32570;&#23569;&#31934;&#30830;&#30340;&#36923;&#36753;&#25512;&#29702;&#12289;&#25277;&#35937;&#30340;&#27010;&#25324;&#33021;&#21147;&#20197;&#21450;&#29702;&#35299;&#22797;&#26434;&#21644;&#22024;&#26434;&#30340;&#22330;&#26223;&#31561;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#24182;&#19981;&#33021;&#25429;&#25417;&#21040;&#36825;&#20123;&#26041;&#38754;&#20013;&#30340;&#22810;&#25968;&#12290;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#38598;&#20851;&#27880;&#35270;&#35273;&#22797;&#26434;&#25968;&#25454;&#20294;&#21482;&#26377;&#31616;&#21333;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65292;&#24402;&#32435;&#36923;&#36753;&#25968;&#25454;&#38598;&#21253;&#25324;&#22797;&#26434;&#30340;&#36923;&#36753;&#23398;&#20064;&#20219;&#21153;&#65292;&#20294;&#26159;&#32570;&#20047;&#35270;&#35273;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25968;&#25454;&#38598;V-LoL&#65292;&#23427;&#26080;&#32541;&#22320;&#32467;&#21512;&#20102;&#35270;&#35273;&#21644;&#36923;&#36753;&#30340;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#39318;&#27425;&#25512;&#20986;&#20102;V-LoL&#30340;&#31532;&#19968;&#20010;&#23454;&#20363;&#65292;&#21517;&#20026;V-LoL-Trains&#65292;&#23427;&#26159;&#31526;&#21495;AI&#20013;&#19968;&#20010;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#30340;&#35270;&#35273;&#21576;&#29616;&#65292;&#21363;Michalski&#28779;&#36710;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#32467;&#21512;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#21644;&#28789;&#27963;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;V-LoL-Trains&#20026;&#30740;&#31350;&#24191;&#27867;&#30340;&#35270;&#35273;&#36923;&#36753;&#23398;&#20064;&#25361;&#25112;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the successes of recent developments in visual AI, different shortcomings still exist; from missing exact logical reasoning, to abstract generalization abilities, to understanding complex and noisy scenes. Unfortunately, existing benchmarks, were not designed to capture more than a few of these aspects. Whereas deep learning datasets focus on visually complex data but simple visual reasoning tasks, inductive logic datasets involve complex logical learning tasks, however, lack the visual component. To address this, we propose the visual logical learning dataset, V-LoL, that seamlessly combines visual and logical challenges. Notably, we introduce the first instantiation of V-LoL, V-LoL-Trains, -- a visual rendition of a classic benchmark in symbolic AI, the Michalski train problem. By incorporating intricate visual scenes and flexible logical reasoning tasks within a versatile framework, V-LoL-Trains provides a platform for investigating a wide range of visual logical learning ch
&lt;/p&gt;</description></item></channel></rss>