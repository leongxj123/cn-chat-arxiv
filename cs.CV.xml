<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.20331</link><description>&lt;p&gt;
&#19981;&#21487;&#35299;&#38382;&#39064;&#26816;&#27979;&#65306;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#20013;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#12290;UPD&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#35774;&#32622;&#65306;&#32570;&#22833;&#31572;&#26696;&#26816;&#27979;&#65288;AAD&#65289;&#12289;&#19981;&#20860;&#23481;&#31572;&#26696;&#38598;&#26816;&#27979;&#65288;IASD&#65289;&#21644;&#19981;&#20860;&#23481;&#35270;&#35273;&#38382;&#39064;&#26816;&#27979;&#65288;IVQD&#65289;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#28145;&#20837;&#30740;&#31350;UPD&#38382;&#39064;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;VLMs&#65292;&#21253;&#25324;GPT-4V&#21644;LLaVA-Next-34B&#65292;&#22312;&#21508;&#31181;&#31243;&#24230;&#19978;&#37117;&#24456;&#38590;&#24212;&#23545;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#37325;&#35201;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;UPD&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26080;&#38656;&#35757;&#32451;&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#20197;&#21450;&#22312;&#25552;&#35758;&#30340;UPD&#35774;&#32622;&#20869;&#30340;&#26410;&#26469;&#21162;&#21147;&#65292;&#23558;&#22686;&#24378;&#23545;VLMs&#30340;&#26356;&#24191;&#27867;&#29702;&#35299;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20331v1 Announce Type: cross  Abstract: This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the VLM's ability to withhold answers when faced with unsolvable problems in the context of Visual Question Answering (VQA) tasks. UPD encompasses three distinct settings: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply investigate the UPD problem, extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements. To address UPD, we explore both training-free and training-based solutions, offering new insights into their effectiveness and limitations. We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ObjectDR&#65292;&#21033;&#29992;&#23545;&#35937;-centric&#30340;&#22495;&#38543;&#26426;&#21270;&#21512;&#25104;&#21333;&#35270;&#22270;3D&#24418;&#29366;&#37325;&#24314;&#20013;&#32570;&#20047;&#30340;&#37197;&#23545;&#25968;&#25454;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21644;&#35299;&#32806;&#26694;&#26550;&#26469;&#29983;&#25104;&#21644;&#20445;&#30041;&#23545;&#35937;&#36718;&#24275;&#20197;&#21450;&#24191;&#27867;&#21464;&#21270;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#20026;&#22521;&#35757;&#27169;&#22411;&#25429;&#25417;&#22495;&#19981;&#21464;&#24615;&#20960;&#20309;&#24418;&#29366;&#12290;</title><link>https://arxiv.org/abs/2403.14539</link><description>&lt;p&gt;
Object-Centric Domain Randomization&#29992;&#20110;&#37326;&#22806;3D&#24418;&#29366;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Domain Randomization for 3D Shape Reconstruction in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14539
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ObjectDR&#65292;&#21033;&#29992;&#23545;&#35937;-centric&#30340;&#22495;&#38543;&#26426;&#21270;&#21512;&#25104;&#21333;&#35270;&#22270;3D&#24418;&#29366;&#37325;&#24314;&#20013;&#32570;&#20047;&#30340;&#37197;&#23545;&#25968;&#25454;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21644;&#35299;&#32806;&#26694;&#26550;&#26469;&#29983;&#25104;&#21644;&#20445;&#30041;&#23545;&#35937;&#36718;&#24275;&#20197;&#21450;&#24191;&#27867;&#21464;&#21270;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#20026;&#22521;&#35757;&#27169;&#22411;&#25429;&#25417;&#22495;&#19981;&#21464;&#24615;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35270;&#22270;3D&#24418;&#29366;&#22312;&#37326;&#22806;&#30340;&#37325;&#24314;&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#26159;&#26469;&#33258;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&lt;3D&#24418;&#29366;&#65292;2D&#22270;&#20687;&gt;-&#37197;&#23545;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#21463;&#22495;&#38543;&#26426;&#21270;&#24341;&#20154;&#27880;&#30446;&#30340;&#25104;&#23601;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ObjectDR&#65292;&#36890;&#36807;&#23545;&#23545;&#35937;&#22806;&#35266;&#21644;&#32972;&#26223;&#30340;&#35270;&#35273;&#21464;&#21270;&#36827;&#34892;&#38543;&#26426;&#20223;&#30495;&#65292;&#21512;&#25104;&#36825;&#31181;&#37197;&#23545;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#21033;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;ControlNet&#65289;&#29983;&#25104;&#31526;&#21512;&#31354;&#38388;&#26465;&#20214;&#65288;&#20363;&#22914;2.5D&#33609;&#22270;&#65289;&#30340;&#22270;&#20687;&#65292;&#36825;&#20123;&#26465;&#20214;&#21487;&#20197;&#36890;&#36807;&#20174;&#23545;&#35937;&#38598;&#21512;&#65288;&#20363;&#22914;Objaverse-XL&#65289;&#30340;&#28210;&#26579;&#36807;&#31243;&#33719;&#24471;3D&#24418;&#29366;&#12290;&#20026;&#20102;&#27169;&#25311;&#22810;&#26679;&#21270;&#30340;&#21464;&#21270;&#21516;&#26102;&#20445;&#30041;&#23884;&#20837;&#31354;&#38388;&#26465;&#20214;&#20013;&#30340;&#23545;&#35937;&#36718;&#24275;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#21033;&#29992;&#21021;&#22987;&#23545;&#35937;&#25351;&#23548;&#30340;&#35299;&#32806;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14539v1 Announce Type: cross  Abstract: One of the biggest challenges in single-view 3D shape reconstruction in the wild is the scarcity of &lt;3D shape, 2D image&gt;-paired data from real-world environments. Inspired by remarkable achievements via domain randomization, we propose ObjectDR which synthesizes such paired data via a random simulation of visual variations in object appearances and backgrounds. Our data synthesis framework exploits a conditional generative model (e.g., ControlNet) to generate images conforming to spatial conditions such as 2.5D sketches, which are obtainable through a rendering process of 3D shapes from object collections (e.g., Objaverse-XL). To simulate diverse variations while preserving object silhouettes embedded in spatial conditions, we also introduce a disentangled framework which leverages an initial object guidance. After synthesizing a wide range of data, we pre-train a model on them so that it learns to capture a domain-invariant geometry p
&lt;/p&gt;</description></item><item><title>VideoPrism&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#39057;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#35821;&#20041;&#35270;&#39057;&#23884;&#20837;&#30340;&#33976;&#39311;&#21644;&#26631;&#35760;&#28151;&#27927;&#26041;&#26696;&#65292;&#22312;&#22810;&#20010;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.13217</link><description>&lt;p&gt;
VideoPrism: &#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#30340;&#22522;&#30784;&#35270;&#35273;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
VideoPrism: A Foundational Visual Encoder for Video Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13217
&lt;/p&gt;
&lt;p&gt;
VideoPrism&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#39057;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#35821;&#20041;&#35270;&#39057;&#23884;&#20837;&#30340;&#33976;&#39311;&#21644;&#26631;&#35760;&#28151;&#27927;&#26041;&#26696;&#65292;&#22312;&#22810;&#20010;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;VideoPrism&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#35270;&#39057;&#32534;&#30721;&#22120;&#65292;&#20351;&#29992;&#21333;&#20010;&#20923;&#32467;&#27169;&#22411;&#22788;&#29702;&#22810;&#26679;&#30340;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#21253;&#21547;3600&#19975;&#39640;&#36136;&#37327;&#35270;&#39057;&#26631;&#39064;&#23545;&#21644;58.2&#20159;&#20010;&#24102;&#26377;&#22024;&#26434;&#24179;&#34892;&#25991;&#26412;&#65288;&#22914;ASR&#36716;&#24405;&#65289;&#30340;&#35270;&#39057;&#21098;&#36753;&#30340;&#24322;&#26500;&#35821;&#26009;&#24211;&#19978;&#23545;VideoPrism&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#39044;&#35757;&#32451;&#26041;&#27861;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#35821;&#20041;&#35270;&#39057;&#23884;&#20837;&#30340;&#33976;&#39311;&#21644;&#19968;&#20010;&#26631;&#35760;&#28151;&#27927;&#26041;&#26696;&#25913;&#36827;&#20102;&#25513;&#30721;&#33258;&#32534;&#30721;&#65292;&#20351;VideoPrism&#33021;&#22815;&#20027;&#35201;&#19987;&#27880;&#20110;&#35270;&#39057;&#27169;&#24577;&#21516;&#26102;&#21033;&#29992;&#19982;&#35270;&#39057;&#30456;&#20851;&#32852;&#30340;&#23453;&#36149;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#24191;&#27867;&#30340;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#32452;&#19978;&#36827;&#34892;&#20102;&#23545;VideoPrism&#30340;&#24191;&#27867;&#27979;&#35797;&#65292;&#20174;&#32593;&#32476;&#35270;&#39057;&#38382;&#31572;&#21040;&#31185;&#23398;CV&#65292; &#22312;33&#20010;&#35270;&#39057;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;30&#20010;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13217v1 Announce Type: cross  Abstract: We introduce VideoPrism, a general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model. We pretrain VideoPrism on a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (e.g., ASR transcripts). The pretraining approach improves upon masked autoencoding by global-local distillation of semantic video embeddings and a token shuffling scheme, enabling VideoPrism to focus primarily on the video modality while leveraging the invaluable text associated with videos. We extensively test VideoPrism on four broad groups of video understanding tasks, from web video question answering to CV for science, achieving state-of-the-art performance on 30 out of 33 video understanding benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#20462;&#22797;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#32423;OOD&#26816;&#27979;&#26041;&#27861;RONIN&#12290;&#36890;&#36807;&#23558;&#26816;&#27979;&#21040;&#30340;&#23545;&#35937;&#36827;&#34892;&#20462;&#22797;&#26367;&#25442;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#30340;ID&#26631;&#31614;&#26469;&#26465;&#20214;&#21270;&#20462;&#22797;&#36807;&#31243;&#65292;&#20351;&#24471;&#37325;&#26500;&#30340;&#23545;&#35937;&#22312;OOD&#24773;&#20917;&#19979;&#19982;&#21407;&#22987;&#23545;&#35937;&#30456;&#24046;&#36739;&#36828;&#65292;&#20174;&#32780;&#26377;&#25928;&#21306;&#20998;ID&#21644;OOD&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;RONIN&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03292</link><description>&lt;p&gt;
&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#20462;&#22797;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#32423;OOD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Object-Level OOD Detection with Context-Aware Inpainting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#20462;&#22797;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#32423;OOD&#26816;&#27979;&#26041;&#27861;RONIN&#12290;&#36890;&#36807;&#23558;&#26816;&#27979;&#21040;&#30340;&#23545;&#35937;&#36827;&#34892;&#20462;&#22797;&#26367;&#25442;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#30340;ID&#26631;&#31614;&#26469;&#26465;&#20214;&#21270;&#20462;&#22797;&#36807;&#31243;&#65292;&#20351;&#24471;&#37325;&#26500;&#30340;&#23545;&#35937;&#22312;OOD&#24773;&#20917;&#19979;&#19982;&#21407;&#22987;&#23545;&#35937;&#30456;&#24046;&#36739;&#36828;&#65292;&#20174;&#32780;&#26377;&#25928;&#21306;&#20998;ID&#21644;OOD&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;RONIN&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36234;&#26469;&#36234;&#22810;&#22320;&#20316;&#20026;&#40657;&#30418;&#20113;&#26381;&#21153;&#25110;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#65292;&#26080;&#27861;&#35775;&#38382;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#23601;&#24341;&#21457;&#20102;&#38646;&#26679;&#26412;&#31163;&#32676;&#25968;&#25454;&#65288;OOD&#65289;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26816;&#27979;&#19981;&#23646;&#20110;&#20998;&#31867;&#22120;&#26631;&#31614;&#38598;&#20294;&#34987;&#38169;&#35823;&#22320;&#24402;&#31867;&#20026;&#20837;&#22495;&#65288;ID&#65289;&#23545;&#35937;&#30340;OOD&#23545;&#35937;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;RONIN&#20351;&#29992;&#29616;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#29992;&#20462;&#22797;&#26367;&#25442;&#25481;&#26816;&#27979;&#21040;&#30340;&#23545;&#35937;&#12290;RONIN&#20351;&#29992;&#39044;&#27979;&#30340;ID&#26631;&#31614;&#26469;&#26465;&#20214;&#21270;&#20462;&#22797;&#36807;&#31243;&#65292;&#20351;&#36755;&#20837;&#23545;&#35937;&#25509;&#36817;&#20837;&#22495;&#22495;&#12290;&#32467;&#26524;&#26159;&#65292;&#37325;&#26500;&#30340;&#23545;&#35937;&#22312;ID&#24773;&#20917;&#19979;&#38750;&#24120;&#25509;&#36817;&#21407;&#22987;&#23545;&#35937;&#65292;&#22312;OOD&#24773;&#20917;&#19979;&#21017;&#30456;&#24046;&#36739;&#36828;&#65292;&#20351;&#24471;RONIN&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;ID&#21644;OOD&#26679;&#26412;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;RONIN&#22312;&#38646;&#26679;&#26412;&#21644;&#38750;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#30456;&#23545;&#20110;&#20808;&#21069;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms are increasingly provided as black-box cloud services or pre-trained models, without access to their training data. This motivates the problem of zero-shot out-of-distribution (OOD) detection. Concretely, we aim to detect OOD objects that do not belong to the classifier's label set but are erroneously classified as in-distribution (ID) objects. Our approach, RONIN, uses an off-the-shelf diffusion model to replace detected objects with inpainting. RONIN conditions the inpainting process with the predicted ID label, drawing the input object closer to the in-distribution domain. As a result, the reconstructed object is very close to the original in the ID cases and far in the OOD cases, allowing RONIN to effectively distinguish ID and OOD samples. Throughout extensive experiments, we demonstrate that RONIN achieves competitive results compared to previous approaches across several datasets, both in zero-shot and non-zero-shot settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;X&#23556;&#32447;CT&#20013;&#20943;&#23569;&#25195;&#25551;&#35282;&#24230;&#30340;&#25968;&#37327;&#21516;&#26102;&#20445;&#25345;&#37325;&#24314;&#36136;&#37327;&#65292;&#20174;&#32780;&#36866;&#29992;&#20110;&#22312;&#32447;&#36136;&#37327;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.06343</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;X&#23556;&#32447;CT&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Sequential Experimental Design for X-Ray CT Using Deep Reinforcement Learning. (arXiv:2307.06343v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;X&#23556;&#32447;CT&#20013;&#20943;&#23569;&#25195;&#25551;&#35282;&#24230;&#30340;&#25968;&#37327;&#21516;&#26102;&#20445;&#25345;&#37325;&#24314;&#36136;&#37327;&#65292;&#20174;&#32780;&#36866;&#29992;&#20110;&#22312;&#32447;&#36136;&#37327;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;X&#23556;&#32447;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#20013;&#65292;&#38656;&#20174;&#22810;&#20010;&#35282;&#24230;&#33719;&#21462;&#25237;&#24433;&#65292;&#24182;&#29992;&#20110;&#19977;&#32500;&#37325;&#24314;&#12290;&#20026;&#20102;&#20351;CT&#36866;&#29992;&#20110;&#22312;&#32447;&#36136;&#37327;&#25511;&#21046;&#65292;&#38656;&#35201;&#20943;&#23569;&#35282;&#24230;&#25968;&#30446;&#21516;&#26102;&#20445;&#25345;&#37325;&#24314;&#36136;&#37327;&#12290;&#31232;&#30095;&#35282;&#24230;&#26029;&#23618;&#25195;&#25551;&#26159;&#20174;&#26377;&#38480;&#25968;&#25454;&#33719;&#21462;&#19977;&#32500;&#37325;&#24314;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#20026;&#20102;&#20248;&#21270;&#20854;&#24615;&#33021;&#65292;&#21487;&#20197;&#25353;&#24207;&#36866;&#24212;&#25195;&#25551;&#35282;&#24230;&#65292;&#36873;&#25321;&#27599;&#20010;&#25195;&#25551;&#23545;&#35937;&#26368;&#26377;&#20449;&#24687;&#37327;&#30340;&#35282;&#24230;&#12290;&#25968;&#23398;&#19978;&#65292;&#36825;&#23545;&#24212;&#20110;&#35299;&#20915;&#19968;&#20010;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#65288;OED&#65289;&#38382;&#39064;&#12290;OED&#38382;&#39064;&#26159;&#39640;&#32500;&#12289;&#38750;&#20984;&#12289;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#26080;&#27861;&#22312;&#32447;&#35299;&#20915;&#65292;&#21363;&#26080;&#27861;&#22312;&#25195;&#25551;&#36807;&#31243;&#20013;&#35299;&#20915;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;OED&#38382;&#39064;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#20013;&#24314;&#27169;&#20026;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#27714;&#35299;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22823;&#37327;&#31163;&#32447;&#35757;&#32451;&#23398;&#20064;&#39640;&#25928;&#30340;&#38750;&#36138;&#23146;&#31574;&#30053;&#26469;&#35299;&#20915;&#32473;&#23450;&#31867;&#21035;&#30340;OED&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In X-ray Computed Tomography (CT), projections from many angles are acquired and used for 3D reconstruction. To make CT suitable for in-line quality control, reducing the number of angles while maintaining reconstruction quality is necessary. Sparse-angle tomography is a popular approach for obtaining 3D reconstructions from limited data. To optimize its performance, one can adapt scan angles sequentially to select the most informative angles for each scanned object. Mathematically, this corresponds to solving and optimal experimental design (OED) problem. OED problems are high-dimensional, non-convex, bi-level optimization problems that cannot be solved online, i.e., during the scan. To address these challenges, we pose the OED problem as a partially observable Markov decision process in a Bayesian framework, and solve it through deep reinforcement learning. The approach learns efficient non-greedy policies to solve a given class of OED problems through extensive offline training rath
&lt;/p&gt;</description></item></channel></rss>