<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#23558;&#19968;&#33324;&#20154;&#21475;&#22270;&#35889;&#36716;&#21464;&#20026;&#29305;&#23450;&#23376;&#20154;&#21475;&#30340;&#22270;&#35889;&#65292;&#30830;&#20445;&#32467;&#26500;&#21512;&#29702;&#24615;&#65292;&#36991;&#20813;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2403.16776</link><description>&lt;p&gt;
Diff-Def: &#36890;&#36807;&#25193;&#25955;&#29983;&#25104;&#30340;&#24418;&#21464;&#22330;&#36827;&#34892;&#26377;&#26465;&#20214;&#30340;&#22270;&#35889;&#21046;&#20316;
&lt;/p&gt;
&lt;p&gt;
Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16776
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#23558;&#19968;&#33324;&#20154;&#21475;&#22270;&#35889;&#36716;&#21464;&#20026;&#29305;&#23450;&#23376;&#20154;&#21475;&#30340;&#22270;&#35889;&#65292;&#30830;&#20445;&#32467;&#26500;&#21512;&#29702;&#24615;&#65292;&#36991;&#20813;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#21078;&#22270;&#35889;&#24191;&#27867;&#24212;&#29992;&#20110;&#20154;&#21475;&#20998;&#26512;&#12290;&#26377;&#26465;&#20214;&#30340;&#22270;&#35889;&#38024;&#23545;&#36890;&#36807;&#29305;&#23450;&#26465;&#20214;&#65288;&#22914;&#20154;&#21475;&#32479;&#35745;&#23398;&#25110;&#30149;&#29702;&#23398;&#65289;&#23450;&#20041;&#30340;&#29305;&#23450;&#23376;&#20154;&#21475;&#65292;&#24182;&#20801;&#35768;&#30740;&#31350;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#24418;&#24577;&#23398;&#24046;&#24322;&#31561;&#32454;&#31890;&#24230;&#35299;&#21078;&#23398;&#24046;&#24322;&#12290;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#37197;&#20934;&#30340;&#26041;&#27861;&#25110;&#29983;&#25104;&#27169;&#22411;&#65292;&#21069;&#32773;&#26080;&#27861;&#22788;&#29702;&#22823;&#30340;&#35299;&#21078;&#23398;&#21464;&#24322;&#65292;&#21518;&#32773;&#21487;&#33021;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#19981;&#31283;&#23450;&#21644;&#24187;&#35273;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#23558;&#19968;&#20010;&#24120;&#35268;&#20154;&#21475;&#22270;&#35889;&#36716;&#21464;&#20026;&#20195;&#34920;&#29305;&#23450;&#23376;&#20154;&#21475;&#30340;&#22270;&#35889;&#12290;&#36890;&#36807;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#24182;&#23558;&#26377;&#26465;&#20214;&#30340;&#22270;&#35889;&#27880;&#20876;&#21040;&#19968;&#32452;&#22270;&#20687;&#38468;&#36817;&#65292;&#25105;&#20204;&#30830;&#20445;&#32467;&#26500;&#30340;&#21512;&#29702;&#24615;&#65292;&#36991;&#20813;&#30452;&#25509;&#22270;&#20687;&#21512;&#25104;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16776v1 Announce Type: cross  Abstract: Anatomical atlases are widely used for population analysis. Conditional atlases target a particular sub-population defined via certain conditions (e.g. demographics or pathologies) and allow for the investigation of fine-grained anatomical differences - such as morphological changes correlated with age. Existing approaches use either registration-based methods that are unable to handle large anatomical variations or generative models, which can suffer from training instabilities and hallucinations. To overcome these limitations, we use latent diffusion models to generate deformation fields, which transform a general population atlas into one representing a specific sub-population. By generating a deformation field and registering the conditional atlas to a neighbourhood of images, we ensure structural plausibility and avoid hallucinations, which can occur during direct image synthesis. We compare our method to several state-of-the-art 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.12029</link><description>&lt;p&gt;
&#23545;&#40784;&#19982;&#25552;&#28860;&#65306;&#32479;&#19968;&#21644;&#25913;&#36827;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Align and Distill: Unifying and Improving Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12029
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#22120;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#20110;&#19982;&#20854;&#35757;&#32451;&#38598;&#19981;&#21516;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65288;DAOD&#65289;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#19978;&#30340;&#24378;&#22823;&#32467;&#26524;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#38519;&#38449;&#65292;&#36825;&#20123;&#38519;&#38449;&#23545;&#36807;&#21435;&#30340;&#32467;&#26524;&#25552;&#20986;&#36136;&#30097;&#24182;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#65306;&#65288;a&#65289;&#30001;&#20110;&#22522;&#32447;&#19981;&#36275;&#23548;&#33268;&#24615;&#33021;&#39640;&#20272;&#65292;&#65288;b&#65289;&#19981;&#19968;&#33268;&#30340;&#23454;&#29616;&#23454;&#36341;&#38459;&#27490;&#20102;&#26041;&#27861;&#30340;&#36879;&#26126;&#27604;&#36739;&#65292;&#65288;c&#65289;&#30001;&#20110;&#36807;&#26102;&#30340;&#39592;&#24178;&#21644;&#22522;&#20934;&#27979;&#35797;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#23548;&#33268;&#32570;&#20047;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20197;&#19979;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;&#65292;Align and Distill&#65288;ALDI&#65289;&#65292;&#25903;&#25345;DAOD&#26041;&#27861;&#30340;&#27604;&#36739;&#24182;&#25903;&#25345;&#26410;&#26469;&#21457;&#23637;&#65292;&#65288;2&#65289;&#19968;&#20010;&#20844;&#24179;&#19988;&#29616;&#20195;&#30340;DAOD&#35757;&#32451;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#35299;&#20915;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#38519;&#38449;&#65292;&#65288;3&#65289;&#19968;&#20010;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;CFC-DAOD&#65292;&#33021;&#22815;&#22312;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12029v1 Announce Type: cross  Abstract: Object detectors often perform poorly on data that differs from their training set. Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge. Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks. We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeltaSpace&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#29992;&#20110;&#28789;&#27963;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#12290;&#22312;DeltaSpace&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;DeltaEdit&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#23558;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#24182;&#20174;CLIP&#39044;&#27979;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#28789;&#27963;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.08785</link><description>&lt;p&gt;
DeltaSpace:&#19968;&#31181;&#29992;&#20110;&#28789;&#27963;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#30340;&#35821;&#20041;&#23545;&#40784;&#29305;&#24449;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided Image Editing. (arXiv:2310.08785v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeltaSpace&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#29992;&#20110;&#28789;&#27963;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#12290;&#22312;DeltaSpace&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;DeltaEdit&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#23558;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#24182;&#20174;CLIP&#39044;&#27979;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#28789;&#27963;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#38754;&#20020;&#30528;&#35757;&#32451;&#21644;&#25512;&#29702;&#28789;&#27963;&#24615;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#35768;&#22810;&#25991;&#29486;&#36890;&#36807;&#25910;&#38598;&#22823;&#37327;&#26631;&#27880;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#26469;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#25991;&#26412;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#26082;&#26114;&#36149;&#21448;&#20302;&#25928;&#12290;&#28982;&#21518;&#65292;&#19968;&#20123;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#20986;&#29616;&#20102;&#65292;&#20197;&#36991;&#20813;&#25968;&#25454;&#25910;&#38598;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#22522;&#20110;&#27599;&#20010;&#25991;&#26412;&#25552;&#31034;&#30340;&#20248;&#21270;&#25110;&#25512;&#29702;&#26102;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35843;&#26597;&#21644;&#30830;&#23450;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#31354;&#38388;&#65292;&#31216;&#20026;CLIP DeltaSpace&#65292;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#65292;&#20004;&#20010;&#22270;&#20687;&#30340;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#19982;&#20854;&#23545;&#24212;&#30340;&#25991;&#26412;&#25551;&#36848;&#30340;CLIP&#25991;&#26412;&#29305;&#24449;&#24046;&#24322;&#22312;&#35821;&#20041;&#19978;&#26159;&#23545;&#40784;&#30340;&#12290;&#22522;&#20110;DeltaSpace&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DeltaEdit&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#23558;CLIP&#35270;&#35273;&#29305;&#24449;&#24046;&#24322;&#26144;&#23556;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#65292;&#24182;&#20174;CLIP&#39044;&#27979;&#28508;&#22312;&#31354;&#38388;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-guided image editing faces significant challenges to training and inference flexibility. Much literature collects large amounts of annotated image-text pairs to train text-conditioned generative models from scratch, which is expensive and not efficient. After that, some approaches that leverage pre-trained vision-language models are put forward to avoid data collection, but they are also limited by either per text-prompt optimization or inference-time hyper-parameters tuning. To address these issues, we investigate and identify a specific space, referred to as CLIP DeltaSpace, where the CLIP visual feature difference of two images is semantically aligned with the CLIP textual feature difference of their corresponding text descriptions. Based on DeltaSpace, we propose a novel framework called DeltaEdit, which maps the CLIP visual feature differences to the latent space directions of a generative model during the training phase, and predicts the latent space directions from the CLIP
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#31995;&#32479;&#20013;&#28155;&#21152;&#22270;&#20687;&#29305;&#24449;&#21487;&#33021;&#26159;&#22810;&#20313;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#21512;&#25104;&#22122;&#22768;&#26469;&#35780;&#20272;&#22270;&#20687;&#23545;&#22788;&#29702;&#25991;&#26412;&#22122;&#22768;&#30340;&#24110;&#21161;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#30053;&#20248;&#20110;&#25991;&#26412;&#27169;&#22411;&#65292;&#21363;&#20351;&#26159;&#38543;&#26426;&#22270;&#20687;&#12290;&#30740;&#31350;&#22312;&#33521;&#35821;&#32763;&#35793;&#20026;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#19988;&#35270;&#35273;&#32972;&#26223;&#23545;&#32763;&#35793;&#25928;&#26524;&#30340;&#24433;&#21709;&#19982;&#28304;&#25991;&#26412;&#22122;&#22768;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2308.16075</link><description>&lt;p&gt;
&#35270;&#35273;&#32972;&#26223;&#23545;&#22024;&#26434;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24433;&#21709;&#65306;&#23545;&#33521;&#21360;&#35821;&#35328;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages. (arXiv:2308.16075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16075
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#31995;&#32479;&#20013;&#28155;&#21152;&#22270;&#20687;&#29305;&#24449;&#21487;&#33021;&#26159;&#22810;&#20313;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#21512;&#25104;&#22122;&#22768;&#26469;&#35780;&#20272;&#22270;&#20687;&#23545;&#22788;&#29702;&#25991;&#26412;&#22122;&#22768;&#30340;&#24110;&#21161;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#30053;&#20248;&#20110;&#25991;&#26412;&#27169;&#22411;&#65292;&#21363;&#20351;&#26159;&#38543;&#26426;&#22270;&#20687;&#12290;&#30740;&#31350;&#22312;&#33521;&#35821;&#32763;&#35793;&#20026;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#19988;&#35270;&#35273;&#32972;&#26223;&#23545;&#32763;&#35793;&#25928;&#26524;&#30340;&#24433;&#21709;&#19982;&#28304;&#25991;&#26412;&#22122;&#22768;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#32780;&#26412;&#30740;&#31350;&#21017;&#32771;&#23519;&#20102;&#23558;&#22270;&#20687;&#29305;&#24449;&#28155;&#21152;&#21040;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20013;&#30340;&#32763;&#35793;&#25928;&#26524;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#22270;&#20687;&#21487;&#33021;&#26159;&#22810;&#20313;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#21512;&#25104;&#22122;&#22768;&#26469;&#35780;&#20272;&#22270;&#20687;&#26159;&#21542;&#26377;&#21161;&#20110;&#27169;&#22411;&#22788;&#29702;&#25991;&#26412;&#22122;&#22768;&#12290;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#65292;&#21363;&#20351;&#26159;&#38543;&#26426;&#22270;&#20687;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#30053;&#20248;&#20110;&#25991;&#26412;&#27169;&#22411;&#12290;&#23454;&#39564;&#23558;&#33521;&#35821;&#32763;&#35793;&#20026;&#21360;&#22320;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#65292;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#35270;&#35273;&#32972;&#26223;&#30340;&#24433;&#21709;&#19982;&#28304;&#25991;&#26412;&#22122;&#22768;&#26377;&#25152;&#19981;&#21516;&#65306;&#23545;&#20110;&#38750;&#22122;&#22768;&#32763;&#35793;&#65292;&#19981;&#20351;&#29992;&#35270;&#35273;&#32972;&#26223;&#25928;&#26524;&#26368;&#22909;&#65307;&#23545;&#20110;&#20302;&#22122;&#22768;&#65292;&#35009;&#21098;&#30340;&#22270;&#20687;&#29305;&#24449;&#26368;&#20339;&#65307;&#22312;&#39640;&#22122;&#22768;&#24773;&#20917;&#19979;&#65292;&#23436;&#25972;&#30340;&#22270;&#20687;&#29305;&#24449;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study investigates the effectiveness of utilizing multimodal information in Neural Machine Translation (NMT). While prior research focused on using multimodal data in low-resource scenarios, this study examines how image features impact translation when added to a large-scale, pre-trained unimodal NMT system. Surprisingly, the study finds that images might be redundant in this context. Additionally, the research introduces synthetic noise to assess whether images help the model deal with textual noise. Multimodal models slightly outperform text-only models in noisy settings, even with random images. The study's experiments translate from English to Hindi, Bengali, and Malayalam, outperforming state-of-the-art benchmarks significantly. Interestingly, the effect of visual context varies with source text noise: no visual context works best for non-noisy translations, cropped image features are optimal for low noise, and full image features work better in high-noise scenarios. This she
&lt;/p&gt;</description></item></channel></rss>