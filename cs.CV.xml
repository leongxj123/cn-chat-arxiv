<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>DiffusionGPT&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#32479;&#19968;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#31995;&#32479;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#24182;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.10061</link><description>&lt;p&gt;
DiffusionGPT: &#22522;&#20110;LLM&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DiffusionGPT: LLM-Driven Text-to-Image Generation System. (arXiv:2401.10061v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10061
&lt;/p&gt;
&lt;p&gt;
DiffusionGPT&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#32479;&#19968;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#31995;&#32479;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#24182;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20026;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#25171;&#24320;&#20102;&#26032;&#30340;&#36947;&#36335;&#65292;&#23548;&#33268;&#20102;&#22312;&#24320;&#28304;&#24179;&#21488;&#19978;&#20849;&#20139;&#39640;&#36136;&#37327;&#27169;&#22411;&#30340;&#24191;&#27867;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#31995;&#32479;&#23384;&#22312;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21363;&#24448;&#24448;&#26080;&#27861;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#65292;&#25110;&#20165;&#38480;&#20110;&#21333;&#19968;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#30446;&#21069;&#30340;&#32479;&#19968;&#23581;&#35797;&#36890;&#24120;&#20998;&#20026;&#20004;&#20010;&#27491;&#20132;&#26041;&#38754;&#65306;i&#65289;&#22312;&#36755;&#20837;&#38454;&#27573;&#35299;&#26512;&#22810;&#26679;&#30340;&#25552;&#31034;&#65307;ii&#65289;&#28608;&#27963;&#19987;&#23478;&#27169;&#22411;&#36827;&#34892;&#36755;&#20986;&#12290;&#20026;&#20102;&#20860;&#39038;&#20004;&#32773;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffusionGPT&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29983;&#25104;&#31995;&#32479;&#65292;&#33021;&#22815;&#26080;&#32541;&#22320;&#36866;&#24212;&#21508;&#31181;&#31867;&#22411;&#30340;&#25552;&#31034;&#24182;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#12290;DiffusionGPT&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#20026;&#21508;&#31181;&#29983;&#25104;&#27169;&#22411;&#26500;&#24314;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;Thought&#26641;&#12290;&#24403;&#25552;&#20379;&#36755;&#20837;&#26102;&#65292;LLM&#35299;&#26512;&#25552;&#31034;&#24182;&#21033;&#29992;Thought&#26641;&#26469;&#25351;&#23548;&#36873;&#25321;&#36866;&#24403;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#25918;&#26494;&#36755;&#20837;&#32422;&#26463;&#24182;&#30830;&#20445;&#24322;&#24120;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have opened up new avenues for the field of image generation, resulting in the proliferation of high-quality models shared on open-source platforms. However, a major challenge persists in current text-to-image systems are often unable to handle diverse inputs, or are limited to single model results. Current unified attempts often fall into two orthogonal aspects: i) parse Diverse Prompts in input stage; ii) activate expert model to output. To combine the best of both worlds, we propose DiffusionGPT, which leverages Large Language Models (LLM) to offer a unified generation system capable of seamlessly accommodating various types of prompts and integrating domain-expert models. DiffusionGPT constructs domain-specific Trees for various generative models based on prior knowledge. When provided with an input, the LLM parses the prompt and employs the Trees-of-Thought to guide the selection of an appropriate model, thereby relaxing input constraints and ensuring exceptional 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#22312;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#20013;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20197;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06122</link><description>&lt;p&gt;
&#29992;&#26799;&#24230;&#24377;&#23556;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Manipulating Feature Visualizations with Gradient Slingshots. (arXiv:2401.06122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#22312;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#20013;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20197;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#32780;&#22810;&#26679;&#21270;&#30340;&#34920;&#31034;&#65292;&#28982;&#32780;&#65292;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#35821;&#20041;&#24615;&#36136;&#20173;&#28982;&#26410;&#30693;&#12290;&#35299;&#37322;DNNs&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#24120;&#29992;&#26041;&#27861;&#26159;&#28608;&#27963;&#26368;&#22823;&#21270;(AM)&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#21512;&#25104;&#30340;&#36755;&#20837;&#20449;&#21495;&#65292;&#26368;&#22823;&#21270;&#28608;&#27963;&#32593;&#32476;&#20013;&#30340;&#29305;&#23450;&#31070;&#32463;&#20803;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#32780;&#19981;&#25913;&#21464;&#27169;&#22411;&#32467;&#26500;&#25110;&#23545;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20960;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#21151;&#33021;&#30340;&#33021;&#21147;&#65292;&#22312;&#27169;&#22411;&#23457;&#26680;&#36807;&#31243;&#20013;&#20351;&#29992;&#36873;&#25321;&#30340;&#30446;&#26631;&#35299;&#37322;&#23631;&#34109;&#20102;&#21407;&#22987;&#35299;&#37322;&#12290;&#20316;&#20026;&#19968;&#31181;&#34917;&#25937;&#25514;&#26045;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#27490;&#36825;&#31181;&#25805;&#32437;&#30340;&#38450;&#25252;&#25514;&#26045;&#65292;&#24182;&#25552;&#20379;&#20102;&#23450;&#37327;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are capable of learning complex and versatile representations, however, the semantic nature of the learned concepts remains unknown. A common method used to explain the concepts learned by DNNs is Activation Maximization (AM), which generates a synthetic input signal that maximally activates a particular neuron in the network. In this paper, we investigate the vulnerability of this approach to adversarial model manipulations and introduce a novel method for manipulating feature visualization without altering the model architecture or significantly impacting the model's decision-making process. We evaluate the effectiveness of our method on several neural network models and demonstrate its capabilities to hide the functionality of specific neurons by masking the original explanations of neurons with chosen target explanations during model auditing. As a remedy, we propose a protective measure against such manipulations and provide quantitative evidence which 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24418;&#29366;&#23436;&#25104;&#21644;&#25235;&#21462;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#28789;&#27963;&#30340;&#22810;&#25351;&#25235;&#21462;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#22270;&#20687;&#30340;&#24418;&#29366;&#23436;&#25104;&#27169;&#22359;&#21644;&#22522;&#20110;&#39044;&#27979;&#30340;&#25235;&#21462;&#39044;&#27979;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#26377;&#38480;&#25110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#29289;&#20307;&#36827;&#34892;&#25235;&#21462;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.20350</link><description>&lt;p&gt;
&#23558;&#24418;&#29366;&#23436;&#25104;&#21644;&#25235;&#21462;&#39044;&#27979;&#32467;&#21512;&#65292;&#23454;&#29616;&#24555;&#36895;&#28789;&#27963;&#30340;&#22810;&#25351;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand. (arXiv:2310.20350v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24418;&#29366;&#23436;&#25104;&#21644;&#25235;&#21462;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#28789;&#27963;&#30340;&#22810;&#25351;&#25235;&#21462;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#22270;&#20687;&#30340;&#24418;&#29366;&#23436;&#25104;&#27169;&#22359;&#21644;&#22522;&#20110;&#39044;&#27979;&#30340;&#25235;&#21462;&#39044;&#27979;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#26377;&#38480;&#25110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#29289;&#20307;&#36827;&#34892;&#25235;&#21462;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36741;&#21161;&#26426;&#22120;&#20154;&#20013;&#65292;&#23545;&#20110;&#20855;&#26377;&#26377;&#38480;&#25110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#29289;&#20307;&#36827;&#34892;&#25235;&#21462;&#26159;&#19968;&#39033;&#38750;&#24120;&#37325;&#35201;&#30340;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#26222;&#36866;&#24773;&#20917;&#19979;&#65292;&#23588;&#20854;&#26159;&#22312;&#35266;&#27979;&#33021;&#21147;&#26377;&#38480;&#21644;&#21033;&#29992;&#22810;&#25351;&#25163;&#36827;&#34892;&#28789;&#27963;&#25235;&#21462;&#26102;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#24555;&#36895;&#21644;&#39640;&#20445;&#30495;&#24230;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#65292;&#30001;&#22522;&#20110;&#21333;&#20010;&#28145;&#24230;&#22270;&#20687;&#30340;&#24418;&#29366;&#23436;&#25104;&#27169;&#22359;&#21644;&#22522;&#20110;&#39044;&#27979;&#30340;&#29289;&#20307;&#24418;&#29366;&#30340;&#25235;&#21462;&#39044;&#27979;&#22120;&#32452;&#25104;&#12290;&#24418;&#29366;&#23436;&#25104;&#32593;&#32476;&#22522;&#20110;VQDIF&#65292;&#22312;&#20219;&#24847;&#26597;&#35810;&#28857;&#19978;&#39044;&#27979;&#31354;&#38388;&#21344;&#29992;&#20540;&#12290;&#20316;&#20026;&#25235;&#21462;&#39044;&#27979;&#22120;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#38454;&#27573;&#26550;&#26500;&#65292;&#39318;&#20808;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#25163;&#23039;&#21183;&#65292;&#28982;&#21518;&#22238;&#24402;&#27599;&#20010;&#23039;&#21183;&#30340;&#25163;&#25351;&#20851;&#33410;&#37197;&#32622;&#12290;&#20851;&#38190;&#22240;&#32032;&#26159;&#36275;&#22815;&#30340;&#25968;&#25454;&#30495;&#23454;&#24615;&#21644;&#22686;&#24378;&#65292;&#20197;&#21450;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#22256;&#38590;&#24773;&#20917;&#30340;&#29305;&#27530;&#20851;&#27880;&#12290;&#22312;&#29289;&#29702;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grasping objects with limited or no prior knowledge about them is a highly relevant skill in assistive robotics. Still, in this general setting, it has remained an open problem, especially when it comes to only partial observability and versatile grasping with multi-fingered hands. We present a novel, fast, and high fidelity deep learning pipeline consisting of a shape completion module that is based on a single depth image, and followed by a grasp predictor that is based on the predicted object shape. The shape completion network is based on VQDIF and predicts spatial occupancy values at arbitrary query points. As grasp predictor, we use our two-stage architecture that first generates hand poses using an autoregressive model and then regresses finger joint configurations per pose. Critical factors turn out to be sufficient data realism and augmentation, as well as special attention to difficult cases during training. Experiments on a physical robot platform demonstrate successful gras
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;&#22312;&#32473;&#23450;&#27169;&#31946;&#29289;&#20307;&#35270;&#22270;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#29289;&#20307;&#37096;&#20998;&#30340;&#19981;&#30830;&#23450;&#21306;&#22495;&#39044;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#39044;&#27979;&#31354;&#38388;&#21344;&#29992;&#30340;&#26041;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#36890;&#36807;&#21518;&#22788;&#29702;&#21344;&#29992;&#35780;&#20998;&#25110;&#30452;&#25509;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#26041;&#27861;&#19982;&#24050;&#30693;&#30340;&#27010;&#29575;&#24418;&#29366;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#28145;&#24230;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.00377</link><description>&lt;p&gt;
&#24102;&#26377;&#19981;&#30830;&#23450;&#21306;&#22495;&#39044;&#27979;&#30340;&#24418;&#29366;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
Shape Completion with Prediction of Uncertain Regions. (arXiv:2308.00377v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00377
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;&#22312;&#32473;&#23450;&#27169;&#31946;&#29289;&#20307;&#35270;&#22270;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#29289;&#20307;&#37096;&#20998;&#30340;&#19981;&#30830;&#23450;&#21306;&#22495;&#39044;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#39044;&#27979;&#31354;&#38388;&#21344;&#29992;&#30340;&#26041;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#36890;&#36807;&#21518;&#22788;&#29702;&#21344;&#29992;&#35780;&#20998;&#25110;&#30452;&#25509;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#26041;&#27861;&#19982;&#24050;&#30693;&#30340;&#27010;&#29575;&#24418;&#29366;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#28145;&#24230;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#23436;&#25104;&#65292;&#21363;&#20174;&#37096;&#20998;&#35266;&#27979;&#39044;&#27979;&#29289;&#20307;&#30340;&#23436;&#25972;&#20960;&#20309;&#24418;&#29366;&#65292;&#23545;&#20110;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#24403;&#22522;&#20110;&#29289;&#20307;&#24418;&#29366;&#37325;&#24314;&#36827;&#34892;&#35268;&#21010;&#25110;&#23454;&#38469;&#25235;&#21462;&#30340;&#39044;&#27979;&#26102;&#65292;&#25351;&#31034;&#20005;&#37325;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#29305;&#21035;&#26159;&#22312;&#32473;&#23450;&#27169;&#31946;&#30340;&#29289;&#20307;&#35270;&#22270;&#26102;&#65292;&#22312;&#25972;&#20010;&#29289;&#20307;&#37096;&#20998;&#23384;&#22312; irreducible uncertainty &#30340;&#25193;&#23637;&#21306;&#22495;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#37325;&#35201;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#26469;&#39044;&#27979;&#36825;&#20123;&#19981;&#30830;&#23450;&#21306;&#22495;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#20316;&#20026;&#39044;&#27979;&#23616;&#37096;&#31354;&#38388;&#21344;&#29992;&#30340;&#20219;&#20309;&#26041;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#19968;&#31181;&#26159;&#36890;&#36807;&#21518;&#22788;&#29702;&#21344;&#29992;&#35780;&#20998;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#19982;&#20004;&#31181;&#24050;&#30693;&#30340;&#27010;&#29575;&#24418;&#29366;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#29983;&#25104;&#20102;&#19968;&#20010;&#22522;&#20110;ShapeNet&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#30495;&#23454;&#28210;&#26579;&#30340;&#29289;&#20307;&#35270;&#22270;&#28145;&#24230;&#22270;&#20687;&#21450;&#20854;&#24102;&#26377;&#22320;&#38754;&#30495;&#20540;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shape completion, i.e., predicting the complete geometry of an object from a partial observation, is highly relevant for several downstream tasks, most notably robotic manipulation. When basing planning or prediction of real grasps on object shape reconstruction, an indication of severe geometric uncertainty is indispensable. In particular, there can be an irreducible uncertainty in extended regions about the presence of entire object parts when given ambiguous object views. To treat this important case, we propose two novel methods for predicting such uncertain regions as straightforward extensions of any method for predicting local spatial occupancy, one through postprocessing occupancy scores, the other through direct prediction of an uncertainty indicator. We compare these methods together with two known approaches to probabilistic shape completion. Moreover, we generate a dataset, derived from ShapeNet, of realistically rendered depth images of object views with ground-truth annot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#25345;&#20037;&#21516;&#35843;&#26469;&#24809;&#32602;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#20027;&#21160;&#33033;&#21644;&#22823;&#34880;&#31649;&#20998;&#21106;&#32467;&#26524;&#19982;&#30495;&#23454;&#20540;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#24322;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#20855;&#26377;&#22266;&#26377;&#20960;&#20309;&#29305;&#24449;&#30340;&#23545;&#35937;&#12290;</title><link>http://arxiv.org/abs/2307.03137</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#20027;&#21160;&#33033;&#21644;&#22823;&#34880;&#31649;&#20998;&#21106;&#30340;&#25299;&#25169;&#24863;&#30693;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Topology-Aware Loss for Aorta and Great Vessel Segmentation in Computed Tomography Images. (arXiv:2307.03137v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#25345;&#20037;&#21516;&#35843;&#26469;&#24809;&#32602;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#20027;&#21160;&#33033;&#21644;&#22823;&#34880;&#31649;&#20998;&#21106;&#32467;&#26524;&#19982;&#30495;&#23454;&#20540;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#24322;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#20855;&#26377;&#22266;&#26377;&#20960;&#20309;&#29305;&#24449;&#30340;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#20998;&#21106;&#32593;&#32476;&#26102;&#65292;&#32593;&#32476;&#24182;&#27809;&#26377;&#26126;&#30830;&#34987;&#35201;&#27714;&#23398;&#20064;&#22270;&#20687;&#30340;&#20840;&#23616;&#19981;&#21464;&#24615;&#65292;&#22914;&#23545;&#35937;&#30340;&#24418;&#29366;&#21644;&#22810;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#19981;&#21464;&#24615;&#32435;&#20837;&#32593;&#32476;&#35757;&#32451;&#20013;&#21487;&#33021;&#26377;&#21161;&#20110;&#25913;&#21892;&#21508;&#31181;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#24403;&#23427;&#20204;&#26159;&#38656;&#35201;&#20998;&#21106;&#30340;&#23545;&#35937;&#30340;&#22266;&#26377;&#29305;&#24615;&#26102;&#12290;&#26412;&#25991;&#20197;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#20013;&#20027;&#21160;&#33033;&#21644;&#22823;&#34880;&#31649;&#30340;&#20998;&#21106;&#20026;&#20363;&#65292;&#36825;&#20123;&#34880;&#31649;&#30001;&#20110;&#20154;&#20307;&#35299;&#21078;&#23398;&#65292;&#36890;&#24120;&#22312;&#36523;&#20307;&#20013;&#20197;&#29305;&#23450;&#30340;&#20960;&#20309;&#24418;&#29366;&#20986;&#29616;&#65292;&#24182;&#22312;2D CT&#22270;&#20687;&#19978;&#20027;&#35201;&#21576;&#29616;&#20026;&#22278;&#24418;&#23545;&#35937;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#25345;&#20037;&#21516;&#35843;&#24809;&#32602;&#22320;&#38754;&#30495;&#23454;&#20540;&#21644;&#39044;&#27979;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#24322;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#20998;&#21106;&#32593;&#32476;&#35774;&#35745;&#19981;&#21516;&#65292;&#20808;&#21069;&#30340;&#35774;&#35745;&#26159;&#23558;&#38408;&#20540;&#28388;&#27874;&#24212;&#29992;&#20110;&#39044;&#27979;&#22270;&#20687;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation networks are not explicitly imposed to learn global invariants of an image, such as the shape of an object and the geometry between multiple objects, when they are trained with a standard loss function. On the other hand, incorporating such invariants into network training may help improve performance for various segmentation tasks when they are the intrinsic characteristics of the objects to be segmented. One example is segmentation of aorta and great vessels in computed tomography (CT) images where vessels are found in a particular geometry in the body due to the human anatomy and they mostly seem as round objects on a 2D CT image. This paper addresses this issue by introducing a new topology-aware loss function that penalizes topology dissimilarities between the ground truth and prediction through persistent homology. Different from the previously suggested segmentation network designs, which apply the threshold filtration on a likelihood function of the prediction map 
&lt;/p&gt;</description></item></channel></rss>