<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861; Describe-and-Dissect&#65288;DnD&#65289;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#26080;&#38656;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#39044;&#23450;&#20041;&#30340;&#27010;&#24565;&#36873;&#25321;&#65292;&#24182;&#19988;&#36890;&#36807;&#24191;&#27867;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#26174;&#31034;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.13771</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#35270;&#35273;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#20803;&#65306;&#25551;&#36848;&#19982;&#35299;&#21078;
&lt;/p&gt;
&lt;p&gt;
Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13771
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861; Describe-and-Dissect&#65288;DnD&#65289;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#26080;&#38656;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#39044;&#23450;&#20041;&#30340;&#27010;&#24565;&#36873;&#25321;&#65292;&#24182;&#19988;&#36890;&#36807;&#24191;&#27867;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#26174;&#31034;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Describe-and-Dissect&#65288;DnD&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#35270;&#35273;&#32593;&#32476;&#20013;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#20316;&#29992;&#12290;DnD&#21033;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#29983;&#25104;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#26080;&#38656;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#39044;&#23450;&#20041;&#30340;&#27010;&#24565;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;DnD&#26159;&#26080;&#38656;&#35757;&#32451;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#19981;&#35757;&#32451;&#20219;&#20309;&#26032;&#27169;&#22411;&#65292;&#26410;&#26469;&#21487;&#20197;&#36731;&#26494;&#21033;&#29992;&#26356;&#24378;&#22823;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#34920;&#26126;DnD&#36890;&#36807;&#25552;&#20379;&#26356;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#20803;&#25551;&#36848;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#25552;&#20379;&#26368;&#39640;&#36136;&#37327;&#30340;&#26631;&#31614;&#65292;&#24182;&#19988;&#34987;&#36873;&#20026;&#31070;&#32463;&#20803;&#30340;&#26368;&#20339;&#35299;&#37322;&#30340;&#27010;&#29575;&#26159;&#26368;&#20339;&#22522;&#32447;&#30340;&#20004;&#20493;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13771v1 Announce Type: cross  Abstract: In this paper, we propose Describe-and-Dissect (DnD), a novel method to describe the roles of hidden neurons in vision networks. DnD utilizes recent advancements in multimodal deep learning to produce complex natural language descriptions, without the need for labeled training data or a predefined set of concepts to choose from. Additionally, DnD is training-free, meaning we don't train any new models and can easily leverage more capable general purpose models in the future. We have conducted extensive qualitative and quantitative analysis to show that DnD outperforms prior work by providing higher quality neuron descriptions. Specifically, our method on average provides the highest quality labels and is more than 2 times as likely to be selected as the best explanation for a neuron than the best baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#35270;&#22270;&#27880;&#24847;&#21147;&#30340;&#21452;&#27969;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21333;&#19968;&#34920;&#31034;&#38590;&#20197;&#20840;&#38754;&#35206;&#30422;&#22797;&#26434;&#20869;&#23481;&#21644;&#32570;&#20047;&#20132;&#20114;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.17237</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#35270;&#22270;&#27880;&#24847;&#21147;&#30340;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Image-Text Matching with Multi-View Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#35270;&#22270;&#27880;&#24847;&#21147;&#30340;&#21452;&#27969;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21333;&#19968;&#34920;&#31034;&#38590;&#20197;&#20840;&#38754;&#35206;&#30422;&#22797;&#26434;&#20869;&#23481;&#21644;&#32570;&#20047;&#20132;&#20114;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#29992;&#20110;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#30340;&#21452;&#27969;&#27169;&#22411;&#22312;&#30830;&#20445;&#26816;&#32034;&#36895;&#24230;&#30340;&#21516;&#26102;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#21463;&#21040;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#21333;&#19968;&#34920;&#31034;&#26469;&#20998;&#21035;&#32534;&#30721;&#22270;&#20687;&#21644;&#25991;&#26412;&#65292;&#24182;&#20351;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#25110;&#21521;&#37327;&#20869;&#31215;&#24471;&#21040;&#21305;&#37197;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#21452;&#27969;&#27169;&#22411;&#30340;&#24615;&#33021;&#24448;&#24448;&#19981;&#22826;&#29702;&#24819;&#12290;&#19968;&#26041;&#38754;&#65292;&#21333;&#19968;&#34920;&#31034;&#38590;&#20197;&#20840;&#38754;&#35206;&#30422;&#22797;&#26434;&#20869;&#23481;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#36825;&#31181;&#32570;&#20047;&#20132;&#20114;&#30340;&#26694;&#26550;&#20013;&#65292;&#21305;&#37197;&#22810;&#37325;&#21547;&#20041;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36825;&#23548;&#33268;&#20449;&#24687;&#34987;&#24573;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#24182;&#20419;&#36827;&#21452;&#27969;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27969;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#30340;&#22810;&#35270;&#22270;&#27880;&#24847;&#21147;&#26041;&#27861;MVAM&#65288;&#22810;&#35270;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17237v1 Announce Type: cross  Abstract: Existing two-stream models for image-text matching show good performance while ensuring retrieval speed and have received extensive attention from industry and academia. These methods use a single representation to encode image and text separately and get a matching score with cosine similarity or the inner product of vectors. However, the performance of the two-stream model is often sub-optimal. On the one hand, a single representation is challenging to cover complex content comprehensively. On the other hand, in this framework of lack of interaction, it is challenging to match multiple meanings which leads to information being ignored. To address the problems mentioned above and facilitate the performance of the two-stream model, we propose a multi-view attention approach for two-stream image-text matching MVAM (\textbf{M}ulti-\textbf{V}iew \textbf{A}ttention \textbf{M}odel). It first learns multiple image and text representations by
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20027;&#20307;&#29305;&#23450;&#30693;&#35782;&#20808;&#39564;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#38452;&#24433;&#27169;&#24335;&#21644;&#32441;&#29702;&#22686;&#24378;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#26377;&#20016;&#23500;&#32441;&#29702;&#30340;3D&#27169;&#22411;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2312.11535</link><description>&lt;p&gt;
Customize-It-3D&#65306;&#20351;&#29992;&#20027;&#20307;&#29305;&#23450;&#30693;&#35782;&#20808;&#39564;&#20174;&#21333;&#20010;&#22270;&#20687;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;3D&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Customize-It-3D: High-Quality 3D Creation from A Single Image Using Subject-Specific Knowledge Prior. (arXiv:2312.11535v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11535
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20027;&#20307;&#29305;&#23450;&#30693;&#35782;&#20808;&#39564;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#38452;&#24433;&#27169;&#24335;&#21644;&#32441;&#29702;&#22686;&#24378;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#26377;&#20016;&#23500;&#32441;&#29702;&#30340;3D&#27169;&#22411;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#20805;&#20998;&#21033;&#29992;&#21442;&#32771;&#22270;&#20687;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#24314;&#31435;&#22270;&#20687;&#21040;3D&#29983;&#25104;&#30340;&#33258;&#23450;&#20041;&#30693;&#35782;&#20808;&#39564;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36890;&#29992;&#30340;&#25193;&#25955;&#20808;&#39564;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19982;&#21442;&#32771;&#22270;&#20687;&#24471;&#21040;&#19968;&#33268;&#32467;&#26524;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#20307;&#29305;&#23450;&#19988;&#22810;&#27169;&#24577;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#19981;&#20165;&#36890;&#36807;&#32771;&#34385;&#38452;&#24433;&#27169;&#24335;&#26469;&#25913;&#21892;&#20960;&#20309;&#20248;&#21270;&#21644;&#32441;&#29702;&#22686;&#24378;&#30340;&#31895;&#30053;&#32467;&#26524;&#65292;&#36824;&#26377;&#21161;&#20110;&#20351;3D&#20869;&#23481;&#19982;&#20027;&#39064;&#20445;&#25345;&#19968;&#33268;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;Customize-It-3D&#22312;&#35270;&#35273;&#36136;&#37327;&#19978;&#36828;&#36828;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20135;&#29983;&#20986;&#33394;&#30340;360&#24230;&#37325;&#24314;&#32467;&#26524;&#65292;&#38750;&#24120;&#36866;&#21512;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#25991;&#26412;&#21040;3D&#30340;&#21019;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel two-stage approach that fully utilizes the information provided by the reference image to establish a customized knowledge prior for image-to-3D generation. While previous approaches primarily rely on a general diffusion prior, which struggles to yield consistent results with the reference image, we propose a subject-specific and multi-modal diffusion model. This model not only aids NeRF optimization by considering the shading mode for improved geometry but also enhances texture from the coarse results to achieve superior refinement. Both aspects contribute to faithfully aligning the 3D content with the subject. Extensive experiments showcase the superiority of our method, Customize-It-3D, outperforming previous works by a substantial margin. It produces faithful 360-degree reconstructions with impressive visual quality, making it well-suited for various applications, including text-to-3D creation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PILOT&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#24037;&#20855;&#31665;&#65292;&#20026;&#22312;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#24182;&#36866;&#24212;&#26032;&#25968;&#25454;&#21040;&#26469;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22686;&#37327;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.07117</link><description>&lt;p&gt;
PILOT&#65306;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#24037;&#20855;&#31665;
&lt;/p&gt;
&lt;p&gt;
PILOT: A Pre-Trained Model-Based Continual Learning Toolbox. (arXiv:2309.07117v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PILOT&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#24037;&#20855;&#31665;&#65292;&#20026;&#22312;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#24182;&#36866;&#24212;&#26032;&#25968;&#25454;&#21040;&#26469;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22686;&#37327;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#65292;&#20294;&#20027;&#35201;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#36816;&#20316;&#65292;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#22686;&#37327;&#23398;&#20064;&#24212;&#36816;&#32780;&#29983;&#65292;&#29992;&#20110;&#22788;&#29702;&#28041;&#21450;&#26032;&#25968;&#25454;&#21040;&#26469;&#30340;&#29616;&#23454;&#22330;&#26223;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#22312;&#19981;&#26029;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;&#65292;&#24182;&#24341;&#36215;&#20102;&#20247;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#12290;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#30340;&#24378;&#22823;&#24615;&#33021;&#20026;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#36866;&#24212;&#29616;&#23454;&#22330;&#26223;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;&#22240;&#27492;&#65292;&#25506;&#32034;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#21033;&#29992;PTMs&#24050;&#32463;&#25104;&#20026;&#24517;&#38656;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PILOT&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#24037;&#20855;&#31665;&#12290;&#19968;&#26041;&#38754;&#65292;PILOT&#23454;&#26045;&#20102;&#19968;&#20123;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#26032;&#29677;&#32423;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;L2P&#12289;DualPrompt&#21644;CODA-Prompt&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;PILOT&#20063;&#36866;&#24212;&#20102;&#20856;&#22411;&#30340;&#29677;&#32423;&#22686;&#37327;&#23398;&#20064;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
While traditional machine learning can effectively tackle a wide range of problems, it primarily operates within a closed-world setting, which presents limitations when dealing with streaming data. As a solution, incremental learning emerges to address real-world scenarios involving new data's arrival. Recently, pre-training has made significant advancements and garnered the attention of numerous researchers. The strong performance of these pre-trained models (PTMs) presents a promising avenue for developing continual learning algorithms that can effectively adapt to real-world scenarios. Consequently, exploring the utilization of PTMs in incremental learning has become essential. This paper introduces a pre-trained model-based continual learning toolbox known as PILOT. On the one hand, PILOT implements some state-of-the-art class-incremental learning algorithms based on pre-trained models, such as L2P, DualPrompt, and CODA-Prompt. On the other hand, PILOT also fits typical class-incre
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#36827;&#34892;&#27491;&#35268;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#20174;&#20302;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#20020;&#24202;MRI&#25195;&#25551;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.10968</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#30340;MRI&#22330;&#36716;&#31227;&#37325;&#24314;&#65306;&#36890;&#36807;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#36827;&#34892;&#27491;&#35268;&#21270;
&lt;/p&gt;
&lt;p&gt;
MRI Field-transfer Reconstruction with Limited Data: Regularization by Neural Style Transfer. (arXiv:2308.10968v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#36827;&#34892;&#27491;&#35268;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#20174;&#20302;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#20020;&#24202;MRI&#25195;&#25551;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;MRI&#37325;&#24314;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25253;&#21578;&#30340;&#26041;&#27861;&#37117;&#38656;&#35201;&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#38477;&#22122;&#65288;RED&#65289;&#27491;&#35268;&#21270;&#26159;&#19968;&#31181;&#23558;&#38477;&#22122;&#22120;&#20316;&#20026;&#22270;&#20687;&#37325;&#24314;&#20808;&#39564;&#30340;&#36890;&#29992;&#27969;&#31243;&#12290;RED&#30340;&#28508;&#21147;&#24050;&#32463;&#22312;&#22810;&#20010;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#20219;&#21153;&#65288;&#22914;&#38477;&#22122;&#12289;&#21435;&#27169;&#31946;&#21644;&#36229;&#20998;&#36776;&#29575;&#65289;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;RNST&#65289;&#26041;&#27861;&#36827;&#34892;&#27491;&#35268;&#21270;&#30340;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#21033;&#29992;&#31070;&#32463;&#36716;&#31227;&#21644;&#38477;&#22122;&#24341;&#25806;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;RNST&#33021;&#22815;&#20174;&#26377;&#22122;&#22768;&#30340;&#20302;&#36136;&#37327;&#22270;&#20687;&#20013;&#37325;&#24314;&#20986;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#22270;&#20687;&#39118;&#26684;&#21644;&#26377;&#38480;&#25968;&#25454;&#19981;&#21516;&#12290;&#25105;&#20204;&#20351;&#29992;1.5T&#21644;3T&#30340;&#20020;&#24202;MRI&#25195;&#25551;&#39564;&#35777;&#20102;RNST&#65292;&#24182;&#19988;&#26174;&#31034;RNST&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;RNST&#26694;&#26550;&#22312;MRI&#37325;&#24314;&#21644;&#26377;&#38480;&#25968;&#25454;&#37325;&#24314;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have demonstrated success in MRI reconstruction using deep learning-based models. However, most reported approaches require training on a task-specific, large-scale dataset. Regularization by denoising (RED) is a general pipeline which embeds a denoiser as a prior for image reconstruction. The potential of RED has been demonstrated for multiple image-related tasks such as denoising, deblurring and super-resolution. In this work, we propose a regularization by neural style transfer (RNST) method to further leverage the priors from the neural transfer and denoising engine. This enables RNST to reconstruct a high-quality image from a noisy low-quality image with different image styles and limited data. We validate RNST with clinical MRI scans from 1.5T and 3T and show that RNST can significantly boost image quality. Our results highlight the capability of the RNST framework for MRI reconstruction and the potential for reconstruction tasks with limited data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.06081</link><description>&lt;p&gt;
CARSO: &#23545;&#25239;&#24615;&#21512;&#25104;&#35266;&#27979;&#30340;&#21453;&#23545;&#25239;&#24615;&#21484;&#22238;
&lt;/p&gt;
&lt;p&gt;
CARSO: Counter-Adversarial Recall of Synthetic Observations. (arXiv:2306.06081v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#28789;&#24863;&#26469;&#33258;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#32447;&#32034;&#12290;&#35813;&#26041;&#27861;&#19982;&#23545;&#25239;&#35757;&#32451;&#20855;&#26377;&#21327;&#21516;&#20114;&#34917;&#24615;&#65292;&#24182;&#20381;&#36182;&#20110;&#34987;&#25915;&#20987;&#20998;&#31867;&#22120;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#65292;&#35813;&#26041;&#27861;&#37319;&#26679;&#36755;&#20837;&#30340;&#37325;&#26500;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#12290;&#22312;&#21508;&#31181;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#22120;&#20307;&#31995;&#32467;&#26500;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;CARSO&#33021;&#22815;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#8212;&#8212;&#21516;&#26102;&#20855;&#26377;&#21487;&#25509;&#21463;&#30340;&#28165;&#27905;&#20934;&#30830;&#24230;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#38450;&#24481;&#20307;&#31995;&#32467;&#26500;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel adversarial defence mechanism for image classification -- CARSO -- inspired by cues from cognitive neuroscience. The method is synergistically complementary to adversarial training and relies on knowledge of the internal representation of the attacked classifier. Exploiting a generative model for adversarial purification, conditioned on such representation, it samples reconstructions of inputs to be finally classified. Experimental evaluation by a well-established benchmark of varied, strong adaptive attacks, across diverse image datasets and classifier architectures, shows that CARSO is able to defend the classifier significantly better than state-of-the-art adversarial training alone -- with a tolerable clean accuracy toll. Furthermore, the defensive architecture succeeds in effectively shielding itself from unforeseen threats, and end-to-end attacks adapted to fool stochastic defences. Code and pre-trained models are available at https://github.com/
&lt;/p&gt;</description></item><item><title>&#21435;&#22122;&#22768;&#26356;&#22909;&#65292;&#34920;&#29616;&#26356;&#22909;&#30340;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;</title><link>http://arxiv.org/abs/2306.04940</link><description>&lt;p&gt;
&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Layer-level activation mechanism. (arXiv:2306.04940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04940
&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#22768;&#26356;&#22909;&#65292;&#34920;&#29616;&#26356;&#22909;&#30340;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28608;&#27963;&#26426;&#21046;&#65292;&#26088;&#22312;&#24314;&#31435;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#21151;&#33021;&#65288;LayerAct&#65289;&#12290;&#36825;&#20123;&#21151;&#33021;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36755;&#20837;&#20559;&#31227;&#25152;&#23548;&#33268;&#30340;&#28608;&#27963;&#36755;&#20986;&#30340;&#20998;&#23618;&#32423;&#27874;&#21160;&#26469;&#38477;&#20302;&#20256;&#32479;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#30340;&#22122;&#38899;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;LayerAct&#21151;&#33021;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#38646;&#30340;&#24179;&#22343;&#28608;&#27963;&#36755;&#20986;&#65292;&#32780;&#19981;&#38480;&#21046;&#28608;&#27963;&#36755;&#20986;&#31354;&#38388;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#35777;&#26126;LayerAct&#21151;&#33021;&#22312;&#22122;&#22768;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#65292;&#24182;&#19988;&#32463;&#39564;&#35777;&#26126;&#36825;&#20123;&#21151;&#33021;&#30340;&#24179;&#22343;&#28608;&#27963;&#32467;&#26524;&#31867;&#20284;&#20110;&#38646;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22788;&#29702;&#22024;&#26434;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#26102;&#65292;LayerAct&#21151;&#33021;&#27604;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#28165;&#27905;&#25968;&#25454;&#38598;&#30340;&#34920;&#29616;&#20063;&#26159;&#20248;&#36234;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel activation mechanism aimed at establishing layer-level activation (LayerAct) functions. These functions are designed to be more noise-robust compared to traditional element-level activation functions by reducing the layer-level fluctuation of the activation outputs due to shift in inputs. Moreover, the LayerAct functions achieve a zero-like mean activation output without restricting the activation output space. We present an analysis and experiments demonstrating that LayerAct functions exhibit superior noise-robustness compared to element-level activation functions, and empirically show that these functions have a zero-like mean activation. Experimental results on three benchmark image classification tasks show that LayerAct functions excel in handling noisy image datasets, outperforming element-level activation functions, while the performance on clean datasets is also superior in most cases.
&lt;/p&gt;</description></item></channel></rss>