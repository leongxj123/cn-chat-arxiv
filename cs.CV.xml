<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#26377;&#21521;&#22270;&#27169;&#22411;&#21644;&#21464;&#20998;&#36125;&#21494;&#26031;&#21407;&#29702;&#65292;&#25581;&#31034;&#20102;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#30340;&#21407;&#29702;&#21644;&#36830;&#25509;&#65292;&#20026;&#38750;&#19987;&#19994;&#32479;&#35745;&#29289;&#29702;&#39046;&#22495;&#30340;&#35835;&#32773;&#25552;&#20379;&#20102;&#26356;&#23481;&#26131;&#29702;&#35299;&#30340;&#20171;&#32461;&#12290;</title><link>http://arxiv.org/abs/2401.06281</link><description>&lt;p&gt;
&#25581;&#31192;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Demystifying Variational Diffusion Models. (arXiv:2401.06281v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06281
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#26377;&#21521;&#22270;&#27169;&#22411;&#21644;&#21464;&#20998;&#36125;&#21494;&#26031;&#21407;&#29702;&#65292;&#25581;&#31034;&#20102;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#30340;&#21407;&#29702;&#21644;&#36830;&#25509;&#65292;&#20026;&#38750;&#19987;&#19994;&#32479;&#35745;&#29289;&#29702;&#39046;&#22495;&#30340;&#35835;&#32773;&#25552;&#20379;&#20102;&#26356;&#23481;&#26131;&#29702;&#35299;&#30340;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#23545;&#20110;&#38750;&#24179;&#34913;&#32479;&#35745;&#29289;&#29702;&#39046;&#22495;&#30340;&#21021;&#23398;&#32773;&#26469;&#35828;&#65292;&#23545;&#35813;&#27169;&#22411;&#31867;&#30340;&#28145;&#20837;&#29702;&#35299;&#20173;&#28982;&#26377;&#20123;&#22256;&#38590;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26377;&#21521;&#22270;&#27169;&#22411;&#21644;&#21464;&#20998;&#36125;&#21494;&#26031;&#21407;&#29702;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#25105;&#20204;&#35748;&#20026;&#26356;&#31616;&#21333;&#26131;&#25026;&#30340;&#25193;&#25955;&#27169;&#22411;&#20171;&#32461;&#65292;&#36825;&#23545;&#20110;&#19968;&#33324;&#35835;&#32773;&#26469;&#35828;&#38656;&#35201;&#30340;&#20808;&#20915;&#26465;&#20214;&#30456;&#23545;&#36739;&#23569;&#12290;&#25105;&#20204;&#30340;&#38416;&#36848;&#26500;&#25104;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25216;&#26415;&#32508;&#36848;&#65292;&#20174;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#31561;&#22522;&#26412;&#27010;&#24565;&#21040;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#31361;&#20986;&#20102;&#27169;&#22411;&#31867;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#12290;&#25105;&#20204;&#23613;&#21487;&#33021;&#22320;&#25552;&#20379;&#20102;&#22312;&#21021;&#22987;&#24037;&#20316;&#20013;&#34987;&#30465;&#30053;&#30340;&#39069;&#22806;&#25968;&#23398;&#27934;&#23519;&#65292;&#20197;&#24110;&#21161;&#29702;&#35299;&#65292;&#21516;&#26102;&#36991;&#20813;&#24341;&#20837;&#26032;&#30340;&#31526;&#21495;&#34920;&#31034;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#25991;&#31456;&#23545;&#20110;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26469;&#35828;&#65292;&#33021;&#20316;&#20026;&#19968;&#20010;&#26377;&#29992;&#30340;&#25945;&#32946;&#34917;&#20805;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the growing popularity of diffusion models, gaining a deep understanding of the model class remains somewhat elusive for the uninitiated in non-equilibrium statistical physics. With that in mind, we present what we believe is a more straightforward introduction to diffusion models using directed graphical modelling and variational Bayesian principles, which imposes relatively fewer prerequisites on the average reader. Our exposition constitutes a comprehensive technical review spanning from foundational concepts like deep latent variable models to recent advances in continuous-time diffusion-based modelling, highlighting theoretical connections between model classes along the way. We provide additional mathematical insights that were omitted in the seminal works whenever possible to aid in understanding, while avoiding the introduction of new notation. We envision this article serving as a useful educational supplement for both researchers and practitioners in the area, and we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#23454;&#29616;Text-to-image diffusion models&#20013;&#30340;&#24555;&#36895;&#25512;&#29702;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.01008</link><description>&lt;p&gt;
Text-to-image diffusion models&#20013;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#23454;&#29616;&#24555;&#36895;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Fast Inference Through The Reuse Of Attention Maps In Diffusion Models. (arXiv:2401.01008v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#23454;&#29616;Text-to-image diffusion models&#20013;&#30340;&#24555;&#36895;&#25512;&#29702;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#28789;&#27963;&#21644;&#36924;&#30495;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#21333;&#20010;&#22270;&#20687;&#25152;&#38656;&#30340;&#36845;&#20195;&#36807;&#31243;&#26082;&#26114;&#36149;&#21448;&#20855;&#26377;&#36739;&#39640;&#30340;&#24310;&#36831;&#65292;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#36827;&#19968;&#27493;&#30740;&#31350;&#20854;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#37319;&#26679;&#27493;&#38271;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#37325;&#22797;&#35745;&#31639;&#27880;&#24847;&#21147;&#26144;&#23556;&#26082;&#32791;&#26102;&#21448;&#20887;&#20313;&#65292;&#22240;&#27492;&#25105;&#20204;&#24314;&#35758;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#32467;&#26500;&#21270;&#22320;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#37325;&#29992;&#31574;&#30053;&#21463;&#21040;&#21021;&#32423;ODE&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#35813;&#29702;&#35770;&#35748;&#20026;&#22312;&#37319;&#26679;&#36807;&#31243;&#30340;&#21518;&#26399;&#37325;&#29992;&#26368;&#21512;&#36866;&#12290;&#22312;&#27880;&#24847;&#21040;&#36825;&#31181;&#29702;&#35770;&#26041;&#27861;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models have demonstrated unprecedented abilities at flexible and realistic image synthesis. However, the iterative process required to produce a single image is costly and incurs a high latency, prompting researchers to further investigate its efficiency. Typically, improvements in latency have been achieved in two ways: (1) training smaller models through knowledge distillation (KD); and (2) adopting techniques from ODE-theory to facilitate larger step sizes. In contrast, we propose a training-free approach that does not alter the step-size of the sampler. Specifically, we find the repeated calculation of attention maps to be both costly and redundant; therefore, we propose a structured reuse of attention maps during sampling. Our initial reuse policy is motivated by rudimentary ODE-theory, which suggests that reuse is most suitable late in the sampling procedure. After noting a number of limitations in this theoretical approach, we empirically search for a bet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20013;&#38388;&#29305;&#24449;&#30340;&#32467;&#26500;&#65292;&#25581;&#31034;&#20102;&#28145;&#24230;&#32593;&#32476;&#22312;&#23618;&#32423;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28436;&#21270;&#27169;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#32447;&#24615;&#23618;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#36215;&#21040;&#20102;&#19982;&#28145;&#23618;&#38750;&#32447;&#24615;&#32593;&#32476;&#31867;&#20284;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.02960</link><description>&lt;p&gt;
&#36890;&#36807;&#23618;&#38388;&#29305;&#24449;&#21387;&#32553;&#21644;&#24046;&#21035;&#24615;&#23398;&#20064;&#29702;&#35299;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination. (arXiv:2311.02960v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20013;&#38388;&#29305;&#24449;&#30340;&#32467;&#26500;&#65292;&#25581;&#31034;&#20102;&#28145;&#24230;&#32593;&#32476;&#22312;&#23618;&#32423;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28436;&#21270;&#27169;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#32447;&#24615;&#23618;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#36215;&#21040;&#20102;&#19982;&#28145;&#23618;&#38750;&#32447;&#24615;&#32593;&#32476;&#31867;&#20284;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#35777;&#26126;&#26159;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#24847;&#20041;&#29305;&#24449;&#30340;&#19968;&#31181;&#39640;&#25928;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#32593;&#32476;&#22914;&#20309;&#22312;&#19981;&#21516;&#23618;&#32423;&#19978;&#36827;&#34892;&#31561;&#32423;&#29305;&#24449;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#30740;&#31350;&#20013;&#38388;&#29305;&#24449;&#30340;&#32467;&#26500;&#25581;&#31034;&#36825;&#20010;&#35868;&#22242;&#12290;&#21463;&#21040;&#25105;&#20204;&#23454;&#35777;&#21457;&#29616;&#30340;&#32447;&#24615;&#23618;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#27169;&#20223;&#38750;&#32447;&#24615;&#32593;&#32476;&#20013;&#28145;&#23618;&#30340;&#35282;&#33394;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#22914;&#20309;&#23558;&#36755;&#20837;&#25968;&#25454;&#36716;&#21270;&#20026;&#36755;&#20986;&#65292;&#36890;&#36807;&#30740;&#31350;&#35757;&#32451;&#21518;&#30340;&#27599;&#20010;&#23618;&#30340;&#36755;&#20986;&#65288;&#21363;&#29305;&#24449;&#65289;&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#34913;&#37327;&#20013;&#38388;&#29305;&#24449;&#30340;&#31867;&#20869;&#21387;&#32553;&#21644;&#31867;&#38388;&#24046;&#21035;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#36890;&#36807;&#23545;&#36825;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29305;&#24449;&#20174;&#27973;&#23618;&#21040;&#28145;&#23618;&#30340;&#28436;&#21464;&#36981;&#24490;&#30528;&#19968;&#31181;&#31616;&#21333;&#32780;&#37327;&#21270;&#30340;&#27169;&#24335;&#65292;&#21069;&#25552;&#26159;&#36755;&#20837;&#25968;&#25454;&#26159;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, deep learning has proven to be a highly effective tool for learning meaningful features from raw data. However, it remains an open question how deep networks perform hierarchical feature learning across layers. In this work, we attempt to unveil this mystery by investigating the structures of intermediate features. Motivated by our empirical findings that linear layers mimic the roles of deep layers in nonlinear networks for feature learning, we explore how deep linear networks transform input data into output by investigating the output (i.e., features) of each layer after training in the context of multi-class classification problems. Toward this goal, we first define metrics to measure within-class compression and between-class discrimination of intermediate features, respectively. Through theoretical analysis of these two metrics, we show that the evolution of features follows a simple and quantitative pattern from shallow to deep layers when the input data is
&lt;/p&gt;</description></item><item><title>SegMatch&#26159;&#19968;&#31181;&#29992;&#20110;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#21644;&#20266;&#26631;&#31614;&#26469;&#20943;&#23569;&#26114;&#36149;&#30340;&#27880;&#37322;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#24369;&#22686;&#24378;&#21644;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#23454;&#29616;&#26080;&#30417;&#30563;&#25439;&#22833;&#30340;&#26045;&#21152;&#12290;</title><link>http://arxiv.org/abs/2308.05232</link><description>&lt;p&gt;
SegMatch: &#19968;&#31181;&#29992;&#20110;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SegMatch: A semi-supervised learning method for surgical instrument segmentation. (arXiv:2308.05232v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05232
&lt;/p&gt;
&lt;p&gt;
SegMatch&#26159;&#19968;&#31181;&#29992;&#20110;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#21644;&#20266;&#26631;&#31614;&#26469;&#20943;&#23569;&#26114;&#36149;&#30340;&#27880;&#37322;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#24369;&#22686;&#24378;&#21644;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#23454;&#29616;&#26080;&#30417;&#30563;&#25439;&#22833;&#30340;&#26045;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#34987;&#35748;&#20026;&#26159;&#25552;&#20379;&#20808;&#36827;&#25163;&#26415;&#36741;&#21161;&#21644;&#25913;&#21892;&#35745;&#31639;&#26426;&#36741;&#21161;&#24178;&#39044;&#30340;&#20851;&#38190;&#25163;&#27573;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SegMatch&#65292;&#19968;&#31181;&#29992;&#20110;&#20943;&#23569;&#26114;&#36149;&#27880;&#37322;&#23545;&#33145;&#33108;&#38236;&#21644;&#26426;&#22120;&#20154;&#25163;&#26415;&#22270;&#20687;&#30340;&#38656;&#27714;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;SegMatch&#22522;&#20110;FixMatch&#65292;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#21644;&#20266;&#26631;&#31614;&#30340;&#21322;&#30417;&#30563;&#20998;&#31867;&#27969;&#31243;&#65292;&#24182;&#23558;&#20854;&#35843;&#25972;&#20026;&#20998;&#21106;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;SegMatch&#20013;&#65292;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;&#36827;&#34892;&#24369;&#22686;&#24378;&#65292;&#24182;&#36890;&#36807;&#20998;&#21106;&#27169;&#22411;&#29983;&#25104;&#20266;&#26631;&#31614;&#65292;&#20197;&#23545;&#39640;&#32622;&#20449;&#24230;&#20687;&#32032;&#19978;&#30340;&#23545;&#25239;&#22686;&#24378;&#22270;&#20687;&#30340;&#27169;&#22411;&#36755;&#20986;&#26045;&#21152;&#26080;&#30417;&#30563;&#25439;&#22833;&#12290;&#25105;&#20204;&#38024;&#23545;&#20998;&#21106;&#20219;&#21153;&#30340;&#35843;&#25972;&#36824;&#21253;&#25324;&#20180;&#32454;&#32771;&#34385;&#25152;&#20381;&#36182;&#30340;&#22686;&#24378;&#20989;&#25968;&#30340;&#31561;&#21464;&#24615;&#21644;&#19981;&#21464;&#24615;&#23646;&#24615;&#65292;&#20026;&#22686;&#24378;&#30340;&#30456;&#20851;&#24615;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surgical instrument segmentation is recognised as a key enabler to provide advanced surgical assistance and improve computer assisted interventions. In this work, we propose SegMatch, a semi supervised learning method to reduce the need for expensive annotation for laparoscopic and robotic surgical images. SegMatch builds on FixMatch, a widespread semi supervised classification pipeline combining consistency regularization and pseudo labelling, and adapts it for the purpose of segmentation. In our proposed SegMatch, the unlabelled images are weakly augmented and fed into the segmentation model to generate a pseudo-label to enforce the unsupervised loss against the output of the model for the adversarial augmented image on the pixels with a high confidence score. Our adaptation for segmentation tasks includes carefully considering the equivariance and invariance properties of the augmentation functions we rely on. To increase the relevance of our augmentations, we depart from using only
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36827;&#34892;&#20102;&#25968;&#23383;&#30149;&#29702;&#22270;&#20687;&#20013;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#25152;&#26377;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#35786;&#26029;&#20934;&#30830;&#24230;&#30340;&#31995;&#32479;&#32508;&#36848;&#21644;Meta&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#21462;&#24471;&#20102;&#39640;&#24230;&#30340;&#20934;&#30830;&#24230;&#65292;&#26159;&#21487;&#34892;&#30340;&#36741;&#21161;&#35786;&#26029;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.07999</link><description>&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#35786;&#26029;&#27979;&#35797;&#20934;&#30830;&#24230;&#65306;&#31995;&#32479;&#32508;&#36848;&#12289;Meta&#20998;&#26512;&#21644;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Diagnostic test accuracy (DTA) of artificial intelligence in digital pathology: a systematic review, meta-analysis and quality assessment. (arXiv:2306.07999v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36827;&#34892;&#20102;&#25968;&#23383;&#30149;&#29702;&#22270;&#20687;&#20013;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#25152;&#26377;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#35786;&#26029;&#20934;&#30830;&#24230;&#30340;&#31995;&#32479;&#32508;&#36848;&#21644;Meta&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#21462;&#24471;&#20102;&#39640;&#24230;&#30340;&#20934;&#30830;&#24230;&#65292;&#26159;&#21487;&#34892;&#30340;&#36741;&#21161;&#35786;&#26029;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#20020;&#24202;&#20351;&#29992;&#20043;&#21069;AI&#27169;&#22411;&#30340;&#35786;&#26029;&#34920;&#29616;&#26159;&#20851;&#38190;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#25216;&#26415;&#30340;&#23433;&#20840;&#21644;&#25104;&#21151;&#30340;&#37319;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#25253;&#36947;&#24212;&#29992;&#20110;&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#20687;&#36827;&#34892;&#35786;&#26029;&#30446;&#30340;&#30340;AI&#30740;&#31350;&#25968;&#37327;&#36805;&#36895;&#22686;&#21152;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;AI&#30340;&#35786;&#26029;&#20934;&#30830;&#24230;&#30340;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#30149;&#29702;&#23398;&#39046;&#22495;&#12290;&#36825;&#39033;&#31995;&#32479;&#24615;&#32508;&#36848;&#21644;Meta&#20998;&#26512;&#21253;&#25324;&#20351;&#29992;&#20219;&#20309;&#31867;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#20219;&#20309;&#30142;&#30149;&#31867;&#22411;&#30340;WSI&#22270;&#20687;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#30740;&#31350;&#12290;&#21442;&#32771;&#26631;&#20934;&#26159;&#36890;&#36807;&#32452;&#32455;&#30149;&#29702;&#23398;&#35780;&#20272;&#21644;/&#25110;&#20813;&#30123;&#32452;&#21270;&#35786;&#26029;&#12290;&#25628;&#32034;&#22312;2022&#24180;6&#26376;&#22312;PubMed&#12289;EMBASE&#21644;CENTRAL&#20013;&#36827;&#34892;&#12290;&#22312;2976&#39033;&#30740;&#31350;&#20013;&#65292;&#26377;100&#39033;&#32435;&#20837;&#32508;&#36848;&#65292;48&#39033;&#32435;&#20837;&#23436;&#25972;&#30340;Meta&#20998;&#26512;&#12290;&#20351;&#29992;QUADAS-2&#24037;&#20855;&#35780;&#20272;&#20102;&#20559;&#20506;&#39118;&#38505;&#21644;&#36866;&#29992;&#24615;&#30340;&#20851;&#27880;&#28857;&#12290;&#25968;&#25454;&#25552;&#21462;&#30001;&#20004;&#20010;&#35843;&#26597;&#21592;&#36827;&#34892;&#65292;&#24182;&#36827;&#34892;&#20102;Meta&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring diagnostic performance of AI models before clinical use is key to the safe and successful adoption of these technologies. Studies reporting AI applied to digital pathology images for diagnostic purposes have rapidly increased in number in recent years. The aim of this work is to provide an overview of the diagnostic accuracy of AI in digital pathology images from all areas of pathology. This systematic review and meta-analysis included diagnostic accuracy studies using any type of artificial intelligence applied to whole slide images (WSIs) in any disease type. The reference standard was diagnosis through histopathological assessment and / or immunohistochemistry. Searches were conducted in PubMed, EMBASE and CENTRAL in June 2022. We identified 2976 studies, of which 100 were included in the review and 48 in the full meta-analysis. Risk of bias and concerns of applicability were assessed using the QUADAS-2 tool. Data extraction was conducted by two investigators and meta-analy
&lt;/p&gt;</description></item></channel></rss>