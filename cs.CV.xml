<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>PDE-CNNs&#36890;&#36807;&#21033;&#29992;&#20960;&#20309;&#24847;&#20041;&#30340;&#28436;&#21270;PDE&#30340;&#27714;&#35299;&#22120;&#26367;&#20195;&#20256;&#32479;&#30340;&#32452;&#20214;&#65292;&#25552;&#20379;&#20102;&#26356;&#23569;&#30340;&#21442;&#25968;&#12289;&#22266;&#26377;&#30340;&#31561;&#21464;&#24615;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#20960;&#20309;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15182</link><description>&lt;p&gt;
PDE-CNNs&#65306;&#20844;&#29702;&#25512;&#23548;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
PDE-CNNs: Axiomatic Derivations and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15182
&lt;/p&gt;
&lt;p&gt;
PDE-CNNs&#36890;&#36807;&#21033;&#29992;&#20960;&#20309;&#24847;&#20041;&#30340;&#28436;&#21270;PDE&#30340;&#27714;&#35299;&#22120;&#26367;&#20195;&#20256;&#32479;&#30340;&#32452;&#20214;&#65292;&#25552;&#20379;&#20102;&#26356;&#23569;&#30340;&#21442;&#25968;&#12289;&#22266;&#26377;&#30340;&#31561;&#21464;&#24615;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#20960;&#20309;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#32452;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PDE-G-CNNs&#65289;&#21033;&#29992;&#20855;&#26377;&#20960;&#20309;&#24847;&#20041;&#30340;&#28436;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#27714;&#35299;&#22120;&#26367;&#20195;G-CNNs&#20013;&#24120;&#35268;&#32452;&#20214;&#12290;PDE-G-CNNs&#21516;&#26102;&#25552;&#20379;&#20102;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#26356;&#23569;&#30340;&#21442;&#25968;&#12289;&#22266;&#26377;&#31561;&#21464;&#24615;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#20960;&#20309;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#29305;&#24449;&#22270;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#20026;&#20108;&#32500;&#30340;&#27431;&#20960;&#37324;&#24503;&#31561;&#21464;PDE-G-CNNs&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#30340;&#21464;&#20307;&#31216;&#20026;PDE-CNN&#12290;&#25105;&#20204;&#21015;&#20986;&#20102;&#20960;&#20010;&#22312;&#23454;&#36341;&#20013;&#20196;&#20154;&#28385;&#24847;&#30340;&#20844;&#29702;&#65292;&#24182;&#20174;&#20013;&#25512;&#23548;&#20986;&#24212;&#22312;PDE-CNN&#20013;&#20351;&#29992;&#21738;&#20123;PDE&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#20856;&#32447;&#24615;&#21644;&#24418;&#24577;&#23610;&#24230;&#31354;&#38388;&#29702;&#35770;&#30340;&#20844;&#29702;&#21463;&#21551;&#21457;&#65292;&#36890;&#36807;&#24341;&#20837;&#21322;&#22495;&#20540;&#20449;&#21495;&#23545;&#20854;&#36827;&#34892;&#25512;&#24191;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#30456;&#23545;&#20110;&#23567;&#22411;&#32593;&#32476;&#65292;PDE-CNN&#25552;&#20379;&#20102;&#26356;&#23569;&#30340;&#21442;&#25968;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15182v1 Announce Type: new  Abstract: PDE-based Group Convolutional Neural Networks (PDE-G-CNNs) utilize solvers of geometrically meaningful evolution PDEs as substitutes for the conventional components in G-CNNs. PDE-G-CNNs offer several key benefits all at once: fewer parameters, inherent equivariance, better performance, data efficiency, and geometric interpretability. In this article we focus on Euclidean equivariant PDE-G-CNNs where the feature maps are two dimensional throughout. We call this variant of the framework a PDE-CNN. We list several practically desirable axioms and derive from these which PDEs should be used in a PDE-CNN. Here our approach to geometric learning via PDEs is inspired by the axioms of classical linear and morphological scale-space theory, which we generalize by introducing semifield-valued signals. Furthermore, we experimentally confirm for small networks that PDE-CNNs offer fewer parameters, better performance, and data efficiency in compariso
&lt;/p&gt;</description></item><item><title>RanDumb&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22266;&#23450;&#30340;&#38543;&#26426;&#21464;&#25442;&#23884;&#20837;&#21407;&#22987;&#20687;&#32032;&#24182;&#23398;&#20064;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#36136;&#30097;&#20102;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290; &#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;RanDumb&#22312;&#20247;&#22810;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.08823</link><description>&lt;p&gt;
RanDumb: &#19968;&#31181;&#36136;&#30097;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RanDumb: A Simple Approach that Questions the Efficacy of Continual Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08823
&lt;/p&gt;
&lt;p&gt;
RanDumb&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22266;&#23450;&#30340;&#38543;&#26426;&#21464;&#25442;&#23884;&#20837;&#21407;&#22987;&#20687;&#32032;&#24182;&#23398;&#20064;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#36136;&#30097;&#20102;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290; &#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;RanDumb&#22312;&#20247;&#22810;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RanDumb&#26469;&#26816;&#39564;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;RanDumb&#23558;&#21407;&#22987;&#20687;&#32032;&#20351;&#29992;&#19968;&#20010;&#22266;&#23450;&#30340;&#38543;&#26426;&#21464;&#25442;&#23884;&#20837;&#65292;&#36825;&#20010;&#21464;&#25442;&#36817;&#20284;&#20102;RBF-Kernel&#65292;&#22312;&#30475;&#21040;&#20219;&#20309;&#25968;&#25454;&#20043;&#21069;&#21021;&#22987;&#21270;&#65292;&#24182;&#23398;&#20064;&#19968;&#20010;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#19988;&#19968;&#33268;&#30340;&#21457;&#29616;&#65306;&#22312;&#20247;&#22810;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;RanDumb&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#36825;&#34920;&#26126;&#22312;&#36825;&#20123;&#24773;&#26223;&#19979;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;RanDumb&#19981;&#23384;&#20648;&#26679;&#26412;&#65292;&#24182;&#22312;&#25968;&#25454;&#19978;&#36827;&#34892;&#21333;&#27425;&#36941;&#21382;&#65292;&#19968;&#27425;&#22788;&#29702;&#19968;&#20010;&#26679;&#26412;&#12290;&#23427;&#19982;GDumb&#30456;&#36741;&#30456;&#25104;&#65292;&#22312;GDumb&#24615;&#33021;&#29305;&#21035;&#24046;&#30340;&#20302;&#26679;&#26412;&#24773;&#20917;&#19979;&#36816;&#34892;&#12290;&#24403;&#23558;RanDumb&#25193;&#23637;&#21040;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26367;&#25442;&#38543;&#26426;&#21464;&#25442;&#30340;&#24773;&#26223;&#26102;&#65292;&#25105;&#20204;&#24471;&#20986;&#30456;&#21516;&#19968;&#33268;&#30340;&#32467;&#35770;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#32467;&#26524;&#26082;&#20196;&#20154;&#24778;&#35766;&#21448;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#34920;&#31034;&#23398;&#20064;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#34920;&#29616;&#31967;&#31957;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08823v1 Announce Type: cross Abstract: We propose RanDumb to examine the efficacy of continual representation learning. RanDumb embeds raw pixels using a fixed random transform which approximates an RBF-Kernel, initialized before seeing any data, and learns a simple linear classifier on top. We present a surprising and consistent finding: RanDumb significantly outperforms the continually learned representations using deep networks across numerous continual learning benchmarks, demonstrating the poor performance of representation learning in these scenarios. RanDumb stores no exemplars and performs a single pass over the data, processing one sample at a time. It complements GDumb, operating in a low-exemplar regime where GDumb has especially poor performance. We reach the same consistent conclusions when RanDumb is extended to scenarios with pretrained models replacing the random transform with pretrained feature extractor. Our investigation is both surprising and alarming as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Velostat&#30340;&#26234;&#33021;&#21387;&#21147;&#30005;&#23376;&#22443;&#31995;&#32479;&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#20154;&#20307;&#23039;&#21183;&#21644;&#36816;&#21160;&#65292;&#20855;&#26377;&#39640;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.11367</link><description>&lt;p&gt;
&#26234;&#33021;&#21387;&#21147;&#30005;&#23376;&#22443;&#29992;&#20110;&#20154;&#31867;&#30561;&#30496;&#23039;&#21183;&#21644;&#21160;&#24577;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Smart Pressure e-Mat for Human Sleeping Posture and Dynamic Activity Recognition. (arXiv:2305.11367v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Velostat&#30340;&#26234;&#33021;&#21387;&#21147;&#30005;&#23376;&#22443;&#31995;&#32479;&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#20154;&#20307;&#23039;&#21183;&#21644;&#36816;&#21160;&#65292;&#20855;&#26377;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#35843;&#21307;&#30103;&#20445;&#20581;&#12289;&#26089;&#26399;&#25945;&#32946;&#21644;&#20581;&#36523;&#26041;&#38754;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#38750;&#20405;&#20837;&#24335;&#27979;&#37327;&#21644;&#35782;&#21035;&#26041;&#27861;&#21463;&#21040;&#20851;&#27880;&#12290;&#21387;&#21147;&#24863;&#24212;&#30001;&#20110;&#20854;&#31616;&#21333;&#30340;&#32467;&#26500;&#12289;&#26131;&#20110;&#35775;&#38382;&#12289;&#21487;&#35270;&#21270;&#24212;&#29992;&#21644;&#26080;&#23475;&#24615;&#32780;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21387;&#25935;&#26448;&#26009;Velostat&#30340;&#26234;&#33021;&#21387;&#21147;&#30005;&#23376;&#22443;(SP e-Mat)&#31995;&#32479;&#65292;&#29992;&#20110;&#20154;&#20307;&#30417;&#27979;&#24212;&#29992;&#65292;&#21253;&#25324;&#30561;&#30496;&#23039;&#21183;&#12289;&#36816;&#21160;&#21644;&#29788;&#20285;&#35782;&#21035;&#12290;&#22312;&#23376;&#31995;&#32479;&#25195;&#25551;&#30005;&#23376;&#22443;&#35835;&#25968;&#24182;&#22788;&#29702;&#20449;&#21495;&#21518;&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#21387;&#21147;&#22270;&#20687;&#27969;&#12290;&#37319;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26469;&#25311;&#21512;&#21644;&#35757;&#32451;&#21387;&#21147;&#22270;&#20687;&#27969;&#65292;&#24182;&#35782;&#21035;&#30456;&#24212;&#30340;&#20154;&#31867;&#34892;&#20026;&#12290;&#22235;&#31181;&#30561;&#30496;&#23039;&#21183;&#21644;&#21463;Nintendo Switch Ring Fit Adventure(RFA)&#21551;&#21457;&#30340;&#20116;&#31181;&#21160;&#24577;&#27963;&#21160;&#34987;&#29992;&#20316;&#25311;&#35758;&#30340;SPeM&#31995;&#32479;&#30340;&#21021;&#27493;&#39564;&#35777;&#12290;SPeM&#31995;&#32479;&#22312;&#20004;&#31181;&#24212;&#29992;&#20013;&#22343;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#35777;&#26126;&#20102;&#20854;&#39640;&#31934;&#24230;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emphasis on healthcare, early childhood education, and fitness, non-invasive measurement and recognition methods have received more attention. Pressure sensing has been extensively studied due to its advantages of simple structure, easy access, visualization application, and harmlessness. This paper introduces a smart pressure e-mat (SPeM) system based on a piezoresistive material Velostat for human monitoring applications, including sleeping postures, sports, and yoga recognition. After a subsystem scans e-mat readings and processes the signal, it generates a pressure image stream. Deep neural networks (DNNs) are used to fit and train the pressure image stream and recognize the corresponding human behavior. Four sleeping postures and five dynamic activities inspired by Nintendo Switch Ring Fit Adventure (RFA) are used as a preliminary validation of the proposed SPeM system. The SPeM system achieves high accuracies on both applications, which demonstrates the high accuracy and
&lt;/p&gt;</description></item></channel></rss>