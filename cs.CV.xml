<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#24341;&#20837;&#20102;Gibbs&#25193;&#25955;&#65288;GDiff&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#37319;&#26679;&#20449;&#21495;&#20808;&#39564;&#21644;&#22122;&#22768;&#20998;&#24067;&#26063;&#65292;&#20197;&#21450;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26469;&#25512;&#26029;&#22122;&#22768;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#30450;&#21435;&#22122;&#20013;&#38656;&#35201;&#30693;&#36947;&#22122;&#22768;&#27700;&#24179;&#21644;&#21327;&#26041;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.19455</link><description>&lt;p&gt;
&#21548;&#22122;&#22768;&#65306;&#20351;&#29992;Gibbs&#25193;&#25955;&#36827;&#34892;&#30450;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Listening to the Noise: Blind Denoising with Gibbs Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19455
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Gibbs&#25193;&#25955;&#65288;GDiff&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#37319;&#26679;&#20449;&#21495;&#20808;&#39564;&#21644;&#22122;&#22768;&#20998;&#24067;&#26063;&#65292;&#20197;&#21450;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26469;&#25512;&#26029;&#22122;&#22768;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#30450;&#21435;&#22122;&#20013;&#38656;&#35201;&#30693;&#36947;&#22122;&#22768;&#27700;&#24179;&#21644;&#21327;&#26041;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21435;&#22122;&#38382;&#39064;&#19982;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#21457;&#23637;&#23494;&#19981;&#21487;&#20998;&#12290;&#29305;&#21035;&#26159;&#65292;&#25193;&#25955;&#27169;&#22411;&#34987;&#35757;&#32451;&#25104;&#21435;&#22122;&#22120;&#65292;&#23427;&#20204;&#25152;&#24314;&#27169;&#30340;&#20998;&#24067;&#19982;&#36125;&#21494;&#26031;&#22270;&#20687;&#20013;&#30340;&#21435;&#22122;&#20808;&#39564;&#30456;&#31526;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#22522;&#20110;&#25193;&#25955;&#30340;&#21518;&#39564;&#37319;&#26679;&#36827;&#34892;&#21435;&#22122;&#38656;&#35201;&#30693;&#36947;&#22122;&#22768;&#27700;&#24179;&#21644;&#21327;&#26041;&#24046;&#65292;&#36825;&#38459;&#30861;&#20102;&#30450;&#21435;&#22122;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837; Gibbs&#25193;&#25955;&#65288;GDiff&#65289;&#20811;&#26381;&#20102;&#36825;&#19968;&#38480;&#21046;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#35770;&#65292;&#21487;&#20197;&#22788;&#29702;&#20449;&#21495;&#21644;&#22122;&#22768;&#21442;&#25968;&#30340;&#21518;&#39564;&#37319;&#26679;&#12290;&#20551;&#35774;&#20219;&#24847;&#21442;&#25968;&#21270;&#30340;&#39640;&#26031;&#22122;&#22768;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;Gibbs&#31639;&#27861;&#65292;&#20132;&#26367;&#22320;&#20174;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#23558;&#20449;&#21495;&#20808;&#39564;&#26144;&#23556;&#21040;&#22122;&#22768;&#20998;&#24067;&#26063;&#65292;&#20197;&#21450;&#19968;&#20010;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#22120;&#26469;&#25512;&#26029;&#22122;&#22768;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#31361;&#20986;&#20102;&#28508;&#22312;&#30340;&#32570;&#38519;&#65292;&#25351;&#23548;&#20102;&#35786;&#26029;&#30340;&#20351;&#29992;&#65292;&#24182;&#37327;&#21270;&#20102;Gibbs s&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19455v1 Announce Type: cross  Abstract: In recent years, denoising problems have become intertwined with the development of deep generative models. In particular, diffusion models are trained like denoisers, and the distribution they model coincide with denoising priors in the Bayesian picture. However, denoising through diffusion-based posterior sampling requires the noise level and covariance to be known, preventing blind denoising. We overcome this limitation by introducing Gibbs Diffusion (GDiff), a general methodology addressing posterior sampling of both the signal and the noise parameters. Assuming arbitrary parametric Gaussian noise, we develop a Gibbs algorithm that alternates sampling steps from a conditional diffusion model trained to map the signal prior to the family of noise distributions, and a Monte Carlo sampler to infer the noise parameters. Our theoretical analysis highlights potential pitfalls, guides diagnostic usage, and quantifies errors in the Gibbs s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#19978;&#24212;&#29992;Wavelet&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#21644;Mel&#39057;&#35889;&#22270;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.17775</link><description>&lt;p&gt;
Wavelet&#25955;&#23556;&#21464;&#25442;&#22312;&#29983;&#29289;&#22768;&#23398;&#20013;&#30340;&#24212;&#29992;&#65306;&#20197;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Wavelet Scattering Transform for Bioacustics: Application to Watkins Marine Mammal Sound Database
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#19978;&#24212;&#29992;Wavelet&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#21644;Mel&#39057;&#35889;&#22270;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#30340;&#20132;&#27969;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#39046;&#22495;&#65292;&#21463;&#21040;&#40483;&#21483;&#30340;&#22810;&#26679;&#24615;&#21644;&#29615;&#22659;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#65288;WMMD&#65289;&#26159;&#19968;&#20010;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#37325;&#28857;&#20171;&#32461;&#20102;&#35813;&#25968;&#25454;&#38598;&#19978;&#26368;&#26032;&#30340;&#22522;&#20934;&#35760;&#24405;&#65292;&#30528;&#37325;&#28548;&#28165;&#25968;&#25454;&#20934;&#22791;&#21644;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;STFT&#22522;&#30784;&#19978;&#24212;&#29992;Wavelet&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#28145;&#23618;&#26550;&#26500;&#21644;&#27531;&#24046;&#23618;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20934;&#30830;&#29575;&#19978;&#20351;&#29992;WST&#27604;&#29616;&#26377;&#20998;&#31867;&#26550;&#26500;&#25552;&#39640;&#20102;6&#65285;&#65292;&#20351;&#29992;Mel&#39057;&#35889;&#22270;&#39044;&#22788;&#29702;&#25552;&#39640;&#20102;8&#65285;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17775v1 Announce Type: cross  Abstract: Marine mammal communication is a complex field, hindered by the diversity of vocalizations and environmental factors. The Watkins Marine Mammal Sound Database (WMMD) is an extensive labeled dataset used in machine learning applications. However, the methods for data preparation, preprocessing, and classification found in the literature are quite disparate. This study first focuses on a brief review of the state-of-the-art benchmarks on the dataset, with an emphasis on clarifying data preparation and preprocessing methods. Subsequently, we propose the application of the Wavelet Scattering Transform (WST) in place of standard methods based on the Short-Time Fourier Transform (STFT). The study also tackles a classification task using an ad-hoc deep architecture with residual layers. We outperform the existing classification architecture by $6\%$ in accuracy using WST and $8\%$ using Mel spectrogram preprocessing, effectively reducing by h
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#30693;&#35270;&#35273;&#35821;&#35328;&#26144;&#23556;&#22120;&#65288;CVLM&#65289;&#65292;&#36890;&#36807;&#22686;&#24378;&#35270;&#35273;&#30693;&#35782;&#23545;&#40784;&#65292;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#25361;&#25112;&#30693;&#35782;&#22411;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.13561</link><description>&lt;p&gt;
&#35748;&#30693;&#35270;&#35273;&#35821;&#35328;&#26144;&#23556;&#22120;&#65306;&#36890;&#36807;&#22686;&#24378;&#35270;&#35273;&#30693;&#35782;&#23545;&#40784;&#25512;&#36827;&#22810;&#27169;&#24577;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13561
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#30693;&#35270;&#35273;&#35821;&#35328;&#26144;&#23556;&#22120;&#65288;CVLM&#65289;&#65292;&#36890;&#36807;&#22686;&#24378;&#35270;&#35273;&#30693;&#35782;&#23545;&#40784;&#65292;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#25361;&#25112;&#30693;&#35782;&#22411;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#21644;&#21453;&#24605;&#24403;&#21069;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#29616;&#29366;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24191;&#27867;&#20351;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#25237;&#24433;&#26041;&#27861;&#65288;&#22914;Q-former&#25110;MLP&#65289;&#20391;&#37325;&#20110;&#22270;&#20687;-&#25991;&#26412;&#25551;&#36848;&#30340;&#23545;&#40784;&#65292;&#20294;&#24573;&#30053;&#20102;&#35270;&#35273;&#30693;&#35782;&#32500;&#24230;&#30340;&#23545;&#40784;&#65292;&#21363;&#23558;&#35270;&#35273;&#19982;&#20854;&#30456;&#20851;&#30693;&#35782;&#36830;&#25509;&#36215;&#26469;&#12290;&#35270;&#35273;&#30693;&#35782;&#22312;&#20998;&#26512;&#12289;&#25512;&#26029;&#21644;&#35299;&#37322;&#35270;&#35273;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#39064;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20027;&#35201;&#25506;&#35752;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#30693;&#35782;&#23545;&#40784;&#26469;&#25913;&#36827;LMMs&#65292;&#29305;&#21035;&#38024;&#23545;&#25361;&#25112;&#30693;&#35782;&#22411;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35748;&#30693;&#35270;&#35273;&#35821;&#35328;&#26144;&#23556;&#22120;&#65288;CVLM&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#30693;&#35782;&#23545;&#40784;&#22120;&#65288;VKA&#65289;&#21644;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#33410;&#38454;&#27573;&#30340;&#32454;&#31890;&#24230;&#30693;&#35782;&#36866;&#37197;&#22120;&#65288;FKA&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22522;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13561v1 Announce Type: new  Abstract: Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05935</link><description>&lt;p&gt;
SPHINX-X: &#25193;&#23637;&#25968;&#25454;&#21644;&#21442;&#25968;&#29992;&#20110;&#19968;&#31995;&#21015;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;SPHINX-X&#65292;&#19968;&#31181;&#22522;&#20110;SPHINX&#24320;&#21457;&#30340;&#24191;&#27867;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#31995;&#21015;&#12290;&#20026;&#20102;&#25913;&#21892;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#31227;&#38500;&#20887;&#20313;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#32469;&#36807;&#23436;&#20840;&#22635;&#20805;&#30340;&#23376;&#22270;&#20687;&#65292;&#24182;&#23558;&#22810;&#38454;&#27573;&#35757;&#32451;&#31616;&#21270;&#25104;&#20026;&#19968;&#38454;&#27573;&#30340;&#20840;&#38598;&#21512;&#27169;&#24335;&#65292;&#20462;&#25913;&#20102;SPHINX&#26694;&#26550;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;MLLM&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#32452;&#35013;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#36328;&#35821;&#35328;&#12289;&#36328;&#35270;&#35273;&#21644;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#30340;&#22810;&#39046;&#22495;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#25105;&#20204;&#30340;OCR&#23494;&#38598;&#21644;Mark&#25968;&#25454;&#38598;&#20016;&#23500;&#36825;&#20010;&#25910;&#38598;&#65292;&#25193;&#23637;&#20102;&#22810;&#26679;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#22522;&#30784;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;TinyLlama1.1B&#12289;InternLM2-7B&#12289;LLaMA2-13B&#21644;Mixtral8x7B&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#21464;&#21270;&#30340;MLLMs&#12290;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#24615;&#33021;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX. To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm. To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks. We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality. By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities. Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;DeCoF&#26159;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#28040;&#38500;&#31354;&#38388;&#20266;&#24433;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02085</link><description>&lt;p&gt;
DeCoF:&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#36827;&#34892;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DeCoF: Generated Video Detection via Frame Consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02085
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;DeCoF&#26159;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#28040;&#38500;&#31354;&#38388;&#20266;&#24433;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#20135;&#29983;&#30340;&#35270;&#39057;&#36136;&#37327;&#19981;&#26029;&#25552;&#39640;&#65292;&#36825;&#23548;&#33268;&#31038;&#20250;&#38754;&#20020;&#26032;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#20351;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#25104;&#20026;&#32039;&#36843;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;&#20026;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#21512;&#20316;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#26126;&#30830;&#29992;&#20110;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#20026;&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#20197;&#35780;&#20272;&#21644;&#25913;&#36827;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#31934;&#24515;&#35774;&#35745;&#30340;&#25506;&#27979;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#20266;&#24433;&#22312;&#24320;&#21457;&#29983;&#25104;&#35270;&#39057;&#30340;&#36890;&#29992;&#21644;&#31283;&#20581;&#26816;&#27979;&#22120;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#35270;&#39057;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26816;&#27979;&#27169;&#22411;&#65288;DeCoF&#65289;&#65292;&#23427;&#28040;&#38500;&#20102;&#31354;&#38388;&#20266;&#24433;&#22312;&#36890;&#29992;&#29305;&#24449;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;DeCoF&#22312;&#26816;&#27979;&#26410;&#35265;&#36807;&#30340;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#30340;&#35270;&#39057;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#19988;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The escalating quality of video generated by advanced video generation methods leads to new security challenges in society, which makes generated video detection an urgent research priority.To foster collaborative research in this area, we construct the first open-source dataset explicitly for generated video detection, providing a valuable resource for the community to benchmark and improve detection methodologies. Through a series of carefully designed probe experiments, our study explores the significance of temporal and spatial artifacts in developing general and robust detectors for generated video. Based on the principle of video frame consistency, we introduce a simple yet effective detection model (DeCoF) that eliminates the impact of spatial artifacts during generalizing feature learning. Our extensive experiments demonstrate the efficacy of DeCoF in detecting videos produced by unseen video generation models and confirm its powerful generalization capabilities across several 
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#32508;&#21512;&#35843;&#30740;&#65292;&#26088;&#22312;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#30340;&#25361;&#25112;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.16549</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Multi-Label Learning: A Comprehensive Survey. (arXiv:2401.16549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16549
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#32508;&#21512;&#35843;&#30740;&#65292;&#26088;&#22312;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#30340;&#25361;&#25112;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#23398;&#20064;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#20174;&#21333;&#20010;&#36755;&#20837;&#25968;&#25454;&#28857;&#20013;&#39044;&#27979;&#22810;&#20010;&#26631;&#31614;&#12290;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#28041;&#21450;&#22810;&#26631;&#31614;&#20998;&#31867;&#25110;&#25490;&#21517;&#30340;&#20219;&#21153;&#24102;&#26469;&#20102;&#37325;&#22823;&#32780;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#22810;&#26631;&#31614;&#20998;&#31867;&#38754;&#20020;&#30340;&#22256;&#38590;&#21253;&#25324;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12289;&#35299;&#20915;&#26631;&#31614;&#30456;&#20851;&#24615;&#21644;&#22788;&#29702;&#37096;&#20998;&#26631;&#31614;&#65292;&#20256;&#32479;&#26041;&#27861;&#22312;&#36825;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#26356;&#26377;&#25928;&#22320;&#24212;&#23545;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#32508;&#21512;&#30740;&#31350;&#36824;&#27604;&#36739;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#26412;&#35843;&#30740;&#26088;&#22312;&#20840;&#38754;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label learning is a rapidly growing research area that aims to predict multiple labels from a single input data point. In the era of big data, tasks involving multi-label classification (MLC) or ranking present significant and intricate challenges, capturing considerable attention in diverse domains. Inherent difficulties in MLC include dealing with high-dimensional data, addressing label correlations, and handling partial labels, for which conventional methods prove ineffective. Recent years have witnessed a notable increase in adopting deep learning (DL) techniques to address these challenges more effectively in MLC. Notably, there is a burgeoning effort to harness the robust learning capabilities of DL for improved modelling of label dependencies and other challenges in MLC. However, it is noteworthy that comprehensive studies specifically dedicated to DL for multi-label learning are limited. Thus, this survey aims to thoroughly review recent progress in DL for multi-label lea
&lt;/p&gt;</description></item><item><title>ODIN&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#36827;&#34892;2D&#21644;3D&#35270;&#22270;&#38388;&#30340;&#20449;&#24687;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2401.02416</link><description>&lt;p&gt;
ODIN: &#19968;&#20010;&#29992;&#20110;2D&#21644;3D&#24863;&#30693;&#30340;&#21333;&#19968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ODIN: A Single Model for 2D and 3D Perception. (arXiv:2401.02416v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02416
&lt;/p&gt;
&lt;p&gt;
ODIN&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#36827;&#34892;2D&#21644;3D&#35270;&#22270;&#38388;&#30340;&#20449;&#24687;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#20808;&#36827;&#27169;&#22411;&#22312;&#20687;ScanNet&#36825;&#26679;&#30340;&#24403;&#20195;3D&#24863;&#30693;&#22522;&#20934;&#19978;&#20351;&#29992;&#24182;&#26631;&#35760;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#25552;&#20379;&#30340;3D&#28857;&#20113;&#65292;&#35813;&#28857;&#20113;&#26159;&#36890;&#36807;&#23545;&#24863;&#30693;&#21040;&#30340;&#22810;&#35270;&#35282;RGB-D&#22270;&#20687;&#36827;&#34892;&#21518;&#22788;&#29702;&#33719;&#24471;&#30340;&#12290;&#23427;&#20204;&#36890;&#24120;&#22312;&#39046;&#22495;&#20869;&#36827;&#34892;&#35757;&#32451;&#65292;&#25918;&#24323;&#20102;&#22823;&#35268;&#27169;&#30340;2D&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#32988;&#36807;&#23558;&#23039;&#24577;RGB-D&#22810;&#35270;&#35282;&#22270;&#20687;&#36827;&#34892;&#29305;&#24449;&#21270;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28040;&#32791;&#23039;&#24577;&#22270;&#20687;&#21644;&#21518;&#22788;&#29702;&#30340;3D&#28857;&#20113;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21152;&#21095;&#20102;2D&#21644;3D&#24863;&#30693;&#38656;&#35201;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#30340;&#35266;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#36825;&#20010;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;ODIN&#65288;Omni-Dimensional INstance segmentation&#65289;&#65292;&#19968;&#31181;&#33021;&#22815;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20132;&#26367;&#30340;2D&#35270;&#22270;&#20869;&#21644;3D&#35270;&#22270;&#38388;&#20449;&#24687;&#34701;&#21512;&#26469;&#21306;&#20998;2D&#21644;3D&#29305;&#24449;&#25805;&#20316;&#65292;&#21033;&#29992;&#28041;&#21450;&#30340;&#20196;&#29260;&#30340;&#20301;&#32622;&#32534;&#30721;&#26469;&#25429;&#25417;2D&#34917;&#19969;&#20196;&#29260;&#21644;3D&#22352;&#26631;&#30340;&#20687;&#32032;&#22352;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art models on contemporary 3D perception benchmarks like ScanNet consume and label dataset-provided 3D point clouds, obtained through post processing of sensed multiview RGB-D images. They are typically trained in-domain, forego large-scale 2D pre-training and outperform alternatives that featurize the posed RGB-D multiview images instead. The gap in performance between methods that consume posed images versus post-processed 3D point clouds has fueled the belief that 2D and 3D perception require distinct model architectures. In this paper, we challenge this view and propose ODIN (Omni-Dimensional INstance segmentation), a model that can segment and label both 2D RGB images and 3D point clouds, using a transformer architecture that alternates between 2D within-view and 3D cross-view information fusion. Our model differentiates 2D and 3D feature operations through the positional encodings of the tokens involved, which capture pixel coordinates for 2D patch tokens and 3D coor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;&#37197;&#23545;&#30340;OSM&#25968;&#25454;&#21644;&#20809;&#23398;&#22270;&#20687;&#36827;&#34892;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35937;&#24341;&#23548;&#30340;Transformer&#26550;&#26500;&#65292;&#20174;&#32780;&#25299;&#23485;&#20102;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#30340;&#33539;&#22260;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#21644;&#20869;&#23384;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2310.02674</link><description>&lt;p&gt;
&#21033;&#29992;&#37197;&#23545;&#30340;OpenStreetMap&#25968;&#25454;&#21644;&#20809;&#23398;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#36827;&#34892;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Land-cover change detection using paired OpenStreetMap data and optical high-resolution imagery via object-guided Transformer. (arXiv:2310.02674v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;&#37197;&#23545;&#30340;OSM&#25968;&#25454;&#21644;&#20809;&#23398;&#22270;&#20687;&#36827;&#34892;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35937;&#24341;&#23548;&#30340;Transformer&#26550;&#26500;&#65292;&#20174;&#32780;&#25299;&#23485;&#20102;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#30340;&#33539;&#22260;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#21644;&#20869;&#23384;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#21644;OpenStreetMap&#65288;OSM&#65289;&#25968;&#25454;&#26159;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;&#30340;&#20004;&#20010;&#37325;&#35201;&#25968;&#25454;&#28304;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#21033;&#29992;OSM&#25968;&#25454;&#26469;&#36741;&#21161;&#22810;&#26102;&#26399;&#20809;&#23398;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#21464;&#21270;&#26816;&#27979;&#12290;&#26412;&#25991;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;&#37197;&#23545;&#30340;OSM&#25968;&#25454;&#21644;&#20809;&#23398;&#22270;&#20687;&#36827;&#34892;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;&#65292;&#25299;&#23485;&#20102;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#30340;&#33539;&#22260;&#65292;&#28085;&#30422;&#26356;&#22810;&#21160;&#24577;&#22320;&#29699;&#35266;&#27979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35937;&#24341;&#23548;&#30340;Transformer&#65288;ObjFormer&#65289;&#26550;&#26500;&#65292;&#23558;&#27969;&#34892;&#30340;&#22522;&#20110;&#23545;&#35937;&#30340;&#22270;&#20687;&#20998;&#26512;&#65288;OBIA&#65289;&#25216;&#26415;&#19982;&#20808;&#36827;&#30340;&#35270;&#35273;Transformer&#26550;&#26500;&#33258;&#28982;&#22320;&#32467;&#21512;&#36215;&#26469;&#12290;&#24341;&#20837;OBIA&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#30340;&#35745;&#31639;&#24320;&#38144;&#21644;&#20869;&#23384;&#36127;&#25285;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;ObjFormer&#20855;&#26377;&#23618;&#27425;&#20266;&#23402;&#29983;&#32534;&#30721;&#22120;&#65292;&#21253;&#21547;&#23545;&#35937;&#24341;&#23548;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#21462;&#20195;&#34920;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical high-resolution imagery and OpenStreetMap (OSM) data are two important data sources for land-cover change detection. Previous studies in these two data sources focus on utilizing the information in OSM data to aid the change detection on multi-temporal optical high-resolution images. This paper pioneers the direct detection of land-cover changes utilizing paired OSM data and optical imagery, thereby broadening the horizons of change detection tasks to encompass more dynamic earth observations. To this end, we propose an object-guided Transformer (ObjFormer) architecture by naturally combining the prevalent object-based image analysis (OBIA) technique with the advanced vision Transformer architecture. The introduction of OBIA can significantly reduce the computational overhead and memory burden in the self-attention module. Specifically, the proposed ObjFormer has a hierarchical pseudo-siamese encoder consisting of object-guided self-attention modules that extract representative
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#32456;&#23618;&#21453;&#28436;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#24320;&#25918;&#24335;&#21333;&#27169;&#22411;&#24402;&#22240;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#23553;&#38381;&#24335;&#29615;&#22659;&#12289;&#35201;&#20040;&#38656;&#35201;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.06210</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#32456;&#23618;&#21453;&#28436;&#36827;&#34892;&#21333;&#27169;&#22411;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Single-Model Attribution via Final-Layer Inversion. (arXiv:2306.06210v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#32456;&#23618;&#21453;&#28436;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#24320;&#25918;&#24335;&#21333;&#27169;&#22411;&#24402;&#22240;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#23553;&#38381;&#24335;&#29615;&#22659;&#12289;&#35201;&#20040;&#38656;&#35201;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#25913;&#21464;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#30340;&#24320;&#21019;&#24615;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20110;&#23454;&#29992;&#21333;&#27169;&#22411;&#24402;&#22240;&#30340;&#20852;&#36259;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#19968;&#20010;&#26679;&#26412;&#26159;&#30001;&#29305;&#23450;&#30340;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#36824;&#26159;&#19981;&#26159;&#65292;&#20363;&#22914;&#65292;&#20026;&#20102;&#35777;&#26126;&#30693;&#35782;&#20135;&#26435;&#30423;&#31363;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#23553;&#38381;&#24335;&#29615;&#22659;&#65292;&#35201;&#20040;&#38656;&#35201;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#19981;&#24517;&#35201;&#30340;&#25913;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FLIPAD&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#32456;&#23618;&#21453;&#28436;&#21644;&#24322;&#24120;&#26816;&#27979;&#30340;&#24320;&#25918;&#24335;&#21333;&#27169;&#22411;&#24402;&#22240;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#21033;&#29992;&#30340;&#26368;&#32456;&#23618;&#21453;&#28436;&#21487;&#20197;&#31616;&#21270;&#20026;&#19968;&#20010;&#20984;&#30340; Lasso &#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#21487;&#38752;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;&#29702;&#35770;&#32467;&#26524;&#36824;&#24471;&#21040;&#20102;&#23454;&#39564;&#30740;&#31350;&#30340;&#25903;&#25345;&#65292;&#35777;&#26126;&#26412;&#25991;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent groundbreaking developments on generative modeling have sparked interest in practical single-model attribution. Such methods predict whether a sample was generated by a specific generator or not, for instance, to prove intellectual property theft. However, previous works are either limited to the closed-world setting or require undesirable changes of the generative model. We address these shortcomings by proposing FLIPAD, a new approach for single-model attribution in the open-world setting based on final-layer inversion and anomaly detection. We show that the utilized final-layer inversion can be reduced to a convex lasso optimization problem, making our approach theoretically sound and computationally efficient. The theoretical findings are accompanied by an experimental study demonstrating the effectiveness of our approach, outperforming the existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#39640;&#25928;&#30340;&#22240;&#24335;&#20998;&#35299;&#32593;&#32476;&#26469;&#29702;&#35299;&#35270;&#35273;&#22330;&#26223;&#24182;&#25512;&#26029;&#29289;&#20307;&#21644;&#23039;&#21183;&#12290;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#22522;&#20110;&#22797;&#20540;&#21521;&#37327;&#30340;&#35745;&#31639;&#26694;&#26550;VSA&#12289;&#29992;&#20110;&#22788;&#29702;&#24179;&#31227;&#21644;&#26059;&#36716;&#30340;&#20998;&#23618;&#35856;&#25391;&#22120;&#32593;&#32476;HRN&#35774;&#35745;&#65292;&#20197;&#21450;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#23454;&#29616;&#22797;&#20540;&#35856;&#25391;&#22120;&#32593;&#32476;&#30340;&#22810;&#32452;&#20998;&#33033;&#20914;&#30456;&#20301;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2208.12880</link><description>&lt;p&gt;
&#20855;&#26377;&#35856;&#25391;&#22120;&#32593;&#32476;&#30340;&#31070;&#32463;&#24418;&#24577;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Neuromorphic Visual Scene Understanding with Resonator Networks. (arXiv:2208.12880v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#39640;&#25928;&#30340;&#22240;&#24335;&#20998;&#35299;&#32593;&#32476;&#26469;&#29702;&#35299;&#35270;&#35273;&#22330;&#26223;&#24182;&#25512;&#26029;&#29289;&#20307;&#21644;&#23039;&#21183;&#12290;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#22522;&#20110;&#22797;&#20540;&#21521;&#37327;&#30340;&#35745;&#31639;&#26694;&#26550;VSA&#12289;&#29992;&#20110;&#22788;&#29702;&#24179;&#31227;&#21644;&#26059;&#36716;&#30340;&#20998;&#23618;&#35856;&#25391;&#22120;&#32593;&#32476;HRN&#35774;&#35745;&#65292;&#20197;&#21450;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#23454;&#29616;&#22797;&#20540;&#35856;&#25391;&#22120;&#32593;&#32476;&#30340;&#22810;&#32452;&#20998;&#33033;&#20914;&#30456;&#20301;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35270;&#35273;&#22330;&#26223;&#24182;&#25512;&#26029;&#20854;&#21508;&#20010;&#29289;&#20307;&#30340;&#36523;&#20221;&#21644;&#23039;&#21183;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#24418;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#21033;&#29992;&#20102;&#22522;&#20110;&#19977;&#20010;&#20851;&#38190;&#27010;&#24565;&#30340;&#39640;&#25928;&#30340;&#22240;&#24335;&#20998;&#35299;&#32593;&#32476;&#65306;&#65288;1&#65289;&#22522;&#20110;&#22797;&#20540;&#21521;&#37327;&#30340;&#30690;&#37327;&#31526;&#21495;&#20307;&#31995;&#26550;&#26500;(VSA)&#30340;&#35745;&#31639;&#26694;&#26550;&#65307;&#65288;2&#65289;&#29992;&#20110;&#22788;&#29702;&#35270;&#35273;&#22330;&#26223;&#20013;&#24179;&#31227;&#21644;&#26059;&#36716;&#30340;&#38750;&#21487;&#20132;&#25442;&#24615;&#30340;&#20998;&#23618;&#35856;&#25391;&#22120;&#32593;&#32476;&#65288;HRN&#65289;&#30340;&#35774;&#35745;&#65292;&#24403;&#20004;&#32773;&#32467;&#21512;&#20351;&#29992;&#26102;&#65307;&#65288;3&#65289;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#32452;&#20998;&#33033;&#20914;&#30456;&#20301;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#23454;&#29616;&#22797;&#20540;&#35856;&#25391;&#22120;&#32593;&#32476;&#12290;VSA&#26694;&#26550;&#20351;&#29992;&#30690;&#37327;&#32465;&#23450;&#25805;&#20316;&#26469;&#20135;&#29983;&#29983;&#25104;&#24335;&#22270;&#20687;&#27169;&#22411;&#65292;&#20854;&#20013;&#32465;&#23450;&#20316;&#20026;&#20960;&#20309;&#21464;&#25442;&#30340;&#31561;&#21464;&#25805;&#20316;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#22330;&#26223;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#21521;&#37327;&#20056;&#31215;&#30340;&#21644;&#65292;&#32780;&#36825;&#20123;&#21521;&#37327;&#20056;&#31215;&#21487;&#20197;&#36890;&#36807;&#35856;&#25391;&#22120;&#32593;&#32476;&#30340;&#22240;&#24335;&#20998;&#35299;&#26469;&#39640;&#25928;&#22320;&#25512;&#26029;&#29289;&#20307;&#21644;&#23427;&#20204;&#30340;&#23039;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding a visual scene by inferring identities and poses of its individual objects is still and open problem. Here we propose a neuromorphic solution that utilizes an efficient factorization network based on three key concepts: (1) a computational framework based on Vector Symbolic Architectures (VSA) with complex-valued vectors; (2) the design of Hierarchical Resonator Networks (HRN) to deal with the non-commutative nature of translation and rotation in visual scenes, when both are used in combination; (3) the design of a multi-compartment spiking phasor neuron model for implementing complex-valued resonator networks on neuromorphic hardware. The VSA framework uses vector binding operations to produce generative image models in which binding acts as the equivariant operation for geometric transformations. A scene can therefore be described as a sum of vector products, which in turn can be efficiently factorized by a resonator network to infer objects and their poses. The HRN ena
&lt;/p&gt;</description></item></channel></rss>