<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#21512;&#26041;&#27861;&#65292;&#36229;&#25193;&#25955;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#20934;&#30830;&#20272;&#35745;&#35748;&#35782;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03478</link><description>&lt;p&gt;
&#36229;&#25193;&#25955;&#65306;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#20272;&#35745;&#35748;&#35782;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Hyper-Diffusion: Estimating Epistemic and Aleatoric Uncertainty with a Single Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#21512;&#26041;&#27861;&#65292;&#36229;&#25193;&#25955;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#20934;&#30830;&#20272;&#35745;&#35748;&#35782;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#39640;&#39118;&#38505;&#24212;&#29992;&#39046;&#22495;&#65288;&#22914;&#21307;&#23398;&#24433;&#20687;&#21644;&#22825;&#27668;&#39044;&#25253;&#65289;&#26102;&#65292;&#20934;&#30830;&#20272;&#35745;&#21644;&#21306;&#20998;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65288;&#21487;&#20197;&#36890;&#36807;&#26356;&#22810;&#30340;&#35757;&#32451;&#25968;&#25454;&#38477;&#20302;&#30340;&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#65288;&#19982;&#24403;&#21069;&#20219;&#21153;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#20934;&#30830;&#26377;&#25928;&#22320;&#20174;&#25968;&#25454;&#38598;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#31361;&#30772;&#24615;&#33021;&#21147;&#65292;&#29616;&#22312;&#20351;&#24471;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20174;&#27010;&#24565;&#19978;&#21464;&#24471;&#31616;&#21333;&#26126;&#20102;&#65306;&#21482;&#38656;&#35201;&#35757;&#32451;&#21644;&#20174;&#19968;&#20010;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#38598;&#21512;&#20013;&#37319;&#26679;&#21363;&#21487;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#35757;&#32451;&#36825;&#26679;&#19968;&#20010;&#38598;&#21512;&#21464;&#24471;&#38590;&#20197;&#35745;&#31639;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#21512;&#26041;&#27861;&#65292;&#36229;&#25193;&#25955;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#20934;&#30830;&#20272;&#35745;&#35748;&#35782;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating and disentangling epistemic uncertainty (uncertainty that can be reduced with more training data) and aleatoric uncertainty (uncertainty that is inherent to the task at hand) is critically important when applying machine learning (ML) to high-stakes applications such as medical imaging and weather forecasting. Conditional diffusion models' breakthrough ability to accurately and efficiently sample from the posterior distribution of a dataset now makes uncertainty estimation conceptually straightforward: One need only train and sample from a large ensemble of diffusion models. Unfortunately, training such an ensemble becomes computationally intractable as the complexity of the model architecture grows.   In this work we introduce a new approach to ensembling, hyper-diffusion, which allows one to accurately estimate epistemic and aleatoric uncertainty with a single model. Unlike existing Monte Carlo dropout based single-model ensembling methods, hyper-diffusion offers the same 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21487;&#24494;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#25910;&#25947;&#24615;&#24046;&#24322;&#12289;$L_1$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#30683;&#30462;&#24615;&#36136;&#20197;&#21450;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#30340;&#19981;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.08426</link><description>&lt;p&gt;
GD&#26080;&#27861;&#32988;&#20219;&#65306;&#38750;&#21487;&#24494;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#19977;&#31181;&#24433;&#21709;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
GD doesn't make the cut: Three ways that non-differentiability affects neural network training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21487;&#24494;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#25910;&#25947;&#24615;&#24046;&#24322;&#12289;$L_1$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#30683;&#30462;&#24615;&#36136;&#20197;&#21450;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#30340;&#19981;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#38750;&#21487;&#24494;&#20989;&#25968;&#65288;NGDMs&#65289;&#21644;&#24212;&#29992;&#20110;&#21487;&#24494;&#20989;&#25968;&#30340;&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#65288;GDs&#65289;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;NGDMs&#30340;&#25910;&#25947;&#24615;&#36136;&#19982;GDs&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#25361;&#25112;&#20102;&#22522;&#20110;$L$-&#20809;&#28369;&#24615;&#30340;&#24191;&#27867;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#25991;&#29486;&#23545;&#38750;&#20809;&#28369;&#31070;&#32463;&#32593;&#32476;&#30340;&#36866;&#29992;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NGDM&#35299;&#20915;$L_1$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#30683;&#30462;&#24615;&#36136;&#65292;&#34920;&#26126;&#22686;&#21152;&#27491;&#21017;&#21270;&#24809;&#32602;&#20250;&#23548;&#33268;NGDMs&#20013;&#26368;&#20248;&#35299;&#30340;$L_1$&#33539;&#25968;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24191;&#27867;&#37319;&#29992;&#30340;&#22522;&#20110;$L_1$&#24809;&#32602;&#30340;&#32593;&#32476;&#20462;&#21098;&#25216;&#26415;&#24182;&#26410;&#20135;&#29983;&#39044;&#26399;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#65288;Edge of Stability&#65289;&#65292;&#25351;&#20986;&#21363;&#20351;&#23545;&#20110;Lipschitz&#36830;&#32493;&#20984;&#21487;&#24494;&#20989;&#25968;&#65292;&#23427;&#20063;&#19981;&#36866;&#29992;&#20110;&#38750;&#20984;&#38750;&#21487;&#24494;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the distinctions between gradient methods applied to non-differentiable functions (NGDMs) and classical gradient descents (GDs) designed for differentiable functions. First, we demonstrate significant differences in the convergence properties of NGDMs compared to GDs, challenging the applicability of the extensive neural network convergence literature based on $L-smoothness$ to non-smooth neural networks. Next, we demonstrate the paradoxical nature of NGDM solutions for $L_{1}$-regularized problems, showing that increasing the regularization penalty leads to an increase in the $L_{1}$ norm of optimal solutions in NGDMs. Consequently, we show that widely adopted $L_{1}$ penalization-based techniques for network pruning do not yield expected results. Finally, we explore the Edge of Stability phenomenon, indicating its inapplicability even to Lipschitz continuous convex differentiable functions, leaving its relevance to non-convex non-differentiable neural networks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24515;&#33039;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#23454;&#26045;&#20102;&#22810;&#20010;U-Net&#34893;&#29983;&#27169;&#22411;&#20197;&#23454;&#29616;&#23545;&#24515;&#33039;&#29305;&#23450;&#37096;&#20301;&#30340;&#20840;&#38754;&#35299;&#21078;&#21644;&#21151;&#33021;&#20998;&#26512;&#12290;&#36890;&#36807;&#22270;&#20687;&#12289;&#22270;&#34920;&#21644;&#23450;&#37327;&#25351;&#26631;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#25913;&#36827;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.09980</link><description>&lt;p&gt;
&#24515;&#23460;&#20998;&#21106;&#65306;U-Net&#34893;&#29983;&#27169;&#22411;&#30340;&#31616;&#35201;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Ventricular Segmentation: A Brief Comparison of U-Net Derivatives. (arXiv:2401.09980v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24515;&#33039;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#23454;&#26045;&#20102;&#22810;&#20010;U-Net&#34893;&#29983;&#27169;&#22411;&#20197;&#23454;&#29616;&#23545;&#24515;&#33039;&#29305;&#23450;&#37096;&#20301;&#30340;&#20840;&#38754;&#35299;&#21078;&#21644;&#21151;&#33021;&#20998;&#26512;&#12290;&#36890;&#36807;&#22270;&#20687;&#12289;&#22270;&#34920;&#21644;&#23450;&#37327;&#25351;&#26631;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#25913;&#36827;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#26159;&#25351;&#29992;&#20110;&#35266;&#23519;&#20154;&#20307;&#21450;&#20854;&#20869;&#37096;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#20197;&#35786;&#26029;&#12289;&#30417;&#27979;&#29978;&#33267;&#27835;&#30103;&#21307;&#23398;&#30142;&#30149;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24515;&#33039;&#30701;&#36724;&#30913;&#20849;&#25391;&#25104;&#20687;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#25552;&#39640;&#19982;&#24515;&#33039;&#30456;&#20851;&#30340;&#21307;&#23398;&#30142;&#30149;&#30340;&#35786;&#26029;&#12289;&#30417;&#27979;&#21644;&#27835;&#30103;&#12290;&#37325;&#28857;&#26159;&#23454;&#26045;&#21508;&#31181;U-Net&#30340;&#34893;&#29983;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#26377;&#25928;&#22320;&#20998;&#31163;&#24515;&#33039;&#30340;&#29305;&#23450;&#37096;&#20998;&#65292;&#36827;&#34892;&#20840;&#38754;&#30340;&#35299;&#21078;&#21644;&#21151;&#33021;&#20998;&#26512;&#12290;&#36890;&#36807;&#22270;&#20687;&#12289;&#22270;&#34920;&#21644;&#23450;&#37327;&#25351;&#26631;&#30340;&#32452;&#21512;&#23637;&#31034;&#20102;&#27169;&#22411;&#21450;&#20854;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#25913;&#36827;&#30340;&#31574;&#30053;&#12290;&#26412;&#25688;&#35201;&#31616;&#35201;&#27010;&#36848;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24515;&#33039;&#22270;&#20687;&#20998;&#21106;&#30340;&#24037;&#20316;&#65292;&#24378;&#35843;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical imaging refers to the technologies and methods utilized to view the human body and its inside, in order to diagnose, monitor, or even treat medical disorders. This paper aims to explore the application of deep learning techniques in the semantic segmentation of Cardiac short-axis MRI (Magnetic Resonance Imaging) images, aiming to enhance the diagnosis, monitoring, and treatment of medical disorders related to the heart. The focus centers on implementing various architectures that are derivatives of U-Net, to effectively isolate specific parts of the heart for comprehensive anatomical and functional analysis. Through a combination of images, graphs, and quantitative metrics, the efficacy of the models and their predictions are showcased. Additionally, this paper addresses encountered challenges and outline strategies for future improvements. This abstract provides a concise overview of the efforts in utilizing deep learning for cardiac image segmentation, emphasizing both the ac
&lt;/p&gt;</description></item><item><title>&#20581;&#22766;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#23637;&#31034;&#20102;&#24322;&#24120;&#29305;&#24449;&#21644;&#26356;&#22810;&#27010;&#24565;&#30340;&#32534;&#30721;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.13040</link><description>&lt;p&gt;
&#20581;&#22766;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#20855;&#26377;&#24322;&#24120;&#29305;&#24449;&#24182;&#32534;&#30721;&#26356;&#22810;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Robust multimodal models have outlier features and encode more concepts. (arXiv:2310.13040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13040
&lt;/p&gt;
&lt;p&gt;
&#20581;&#22766;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#23637;&#31034;&#20102;&#24322;&#24120;&#29305;&#24449;&#21644;&#26356;&#22810;&#27010;&#24565;&#30340;&#32534;&#30721;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20160;&#20040;&#21306;&#20998;&#20581;&#22766;&#27169;&#22411;&#19982;&#38750;&#20581;&#22766;&#27169;&#22411;&#65311;&#38543;&#30528;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#20986;&#29616;&#65292;&#36825;&#20010;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#21464;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20581;&#22766;&#24615;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#20102;&#20581;&#22766;&#24615;&#30340;&#24046;&#24322;&#21487;&#20197;&#36861;&#28335;&#21040;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#24046;&#24322;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#36824;&#19981;&#28165;&#26970;&#36825;&#23545;&#20110;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#20160;&#20040;&#24847;&#21619;&#30528;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25506;&#27979;12&#20010;&#20855;&#26377;&#19981;&#21516;&#39592;&#24178;&#65288;ResNets&#21644;ViTs&#65289;&#21644;&#39044;&#35757;&#32451;&#38598;&#65288;OpenAI&#65292;LAION-400M&#65292;LAION-2B&#65292;YFCC15M&#65292;CC12M&#21644;DataComp&#65289;&#30340;&#20581;&#22766;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#23384;&#22312;&#20004;&#20010;&#20581;&#22766;&#24615;&#30340;&#29305;&#24449;&#65306;&#65288;1&#65289;&#20581;&#22766;&#27169;&#22411;&#20855;&#26377;&#30001;&#20854;&#28608;&#27963;&#29305;&#24449;&#34920;&#24449;&#30340;&#24322;&#24120;&#29305;&#24449;&#65292;&#20854;&#20013;&#19968;&#20123;&#29305;&#24449;&#20540;&#27604;&#24179;&#22343;&#20540;&#39640;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#20123;&#24322;&#24120;&#29305;&#24449;&#22312;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#24341;&#20837;&#20102;&#29305;&#26435;&#26041;&#21521;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
What distinguishes robust models from non-robust ones? This question has gained traction with the appearance of large-scale multimodal models, such as CLIP. These models have demonstrated unprecedented robustness with respect to natural distribution shifts. While it has been shown that such differences in robustness can be traced back to differences in training data, so far it is not known what that translates to in terms of what the model has learned. In this work, we bridge this gap by probing the representation spaces of 12 robust multimodal models with various backbones (ResNets and ViTs) and pretraining sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M and DataComp). We find two signatures of robustness in the representation spaces of these models: (1) Robust models exhibit outlier features characterized by their activations, with some being several orders of magnitude above average. These outlier features induce privileged directions in the model's representation space. We demon
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#23545;&#40784;&#30340;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#27604;&#32431;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.09377</link><description>&lt;p&gt;
&#23545;&#40784;&#35821;&#35328;&#30340;&#35270;&#35273;&#34920;&#31034;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Language Aligned Visual Representations Predict Human Behavior in Naturalistic Learning Tasks. (arXiv:2306.09377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09377
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#23545;&#40784;&#30340;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#27604;&#32431;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#22791;&#35782;&#21035;&#21644;&#27010;&#25324;&#33258;&#28982;&#29289;&#20307;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#26377;&#25152;&#24110;&#21161;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#24182;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#34920;&#31034;&#26041;&#24335;&#20197;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#28041;&#21450;&#31867;&#21035;&#23398;&#20064;&#21644;&#22870;&#21169;&#23398;&#20064;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#36924;&#30495;&#30340;&#22270;&#20687;&#20316;&#20026;&#21050;&#28608;&#29289;&#65292;&#24182;&#35201;&#27714;&#21442;&#19982;&#32773;&#22522;&#20110;&#25152;&#26377;&#35797;&#39564;&#30340;&#26032;&#22411;&#21050;&#28608;&#29289;&#20316;&#20986;&#20934;&#30830;&#30340;&#20915;&#31574;&#65292;&#22240;&#27492;&#38656;&#35201;&#27867;&#21270;&#12290;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#65292;&#24213;&#23618;&#35268;&#21017;&#26159;&#20351;&#29992;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#25552;&#21462;&#30340;&#21050;&#28608;&#32500;&#24230;&#29983;&#25104;&#30340;&#31616;&#21333;&#32447;&#24615;&#20989;&#25968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21442;&#19982;&#32773;&#22312;&#20960;&#27425;&#35797;&#39564;&#20869;&#23601;&#25104;&#21151;&#22320;&#30830;&#23450;&#20102;&#30456;&#20851;&#30340;&#21050;&#28608;&#29305;&#24449;&#65292;&#35777;&#26126;&#20102;&#26377;&#25928;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#22411;&#27604;&#36739;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#31034;&#23545;&#20154;&#31867;&#36873;&#25321;&#30340;&#36880;&#27425;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#22914;&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#31034;&#20248;&#20110;&#35270;&#35273;&#20219;&#21153;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#31034;&#65292;&#34920;&#26126;&#23545;&#40784;&#35821;&#35328;&#30340;&#35270;&#35273;&#34920;&#31034;&#21487;&#33021;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans possess the ability to identify and generalize relevant features of natural objects, which aids them in various situations. To investigate this phenomenon and determine the most effective representations for predicting human behavior, we conducted two experiments involving category learning and reward learning. Our experiments used realistic images as stimuli, and participants were tasked with making accurate decisions based on novel stimuli for all trials, thereby necessitating generalization. In both tasks, the underlying rules were generated as simple linear functions using stimulus dimensions extracted from human similarity judgments. Notably, participants successfully identified the relevant stimulus features within a few trials, demonstrating effective generalization. We performed an extensive model comparison, evaluating the trial-by-trial predictive accuracy of diverse deep learning models' representations of human choices. Intriguingly, representations from models train
&lt;/p&gt;</description></item></channel></rss>