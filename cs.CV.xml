<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>eCLIP&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#19987;&#23478;&#27880;&#37322;&#21644;&#28151;&#21512;&#22686;&#24378;&#26469;&#24212;&#23545;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#21644;&#27169;&#24577;&#24046;&#36317;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;</title><link>https://arxiv.org/abs/2403.10153</link><description>&lt;p&gt;
&#25913;&#36827;&#21307;&#23398;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#19982;&#19987;&#23478;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
Improving Medical Multi-modal Contrastive Learning with Expert Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10153
&lt;/p&gt;
&lt;p&gt;
eCLIP&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#19987;&#23478;&#27880;&#37322;&#21644;&#28151;&#21512;&#22686;&#24378;&#26469;&#24212;&#23545;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#21644;&#27169;&#24577;&#24046;&#36317;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;CLIP&#27169;&#22411;&#8212;&#8212;eCLIP&#65292;&#23427;&#38598;&#25104;&#20102;&#25918;&#23556;&#31185;&#21307;&#29983;&#30524;&#29699;&#27880;&#35270;&#28909;&#22270;&#24418;&#24335;&#30340;&#19987;&#23478;&#27880;&#37322;&#12290;&#23427;&#35299;&#20915;&#20102;&#23545;&#27604;&#22810;&#27169;&#24577;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#25968;&#25454;&#31232;&#32570;&#21644;&#8220;&#27169;&#24577;&#24046;&#36317;&#8221;&#8212;&#8212;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#23384;&#22312;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#38477;&#20302;&#20102;&#34920;&#31034;&#30340;&#36136;&#37327;&#24182;&#38459;&#30861;&#20102;&#36328;&#27169;&#24577;&#20114;&#25805;&#20316;&#24615;&#12290;eCLIP&#38598;&#25104;&#20102;&#19968;&#20010;&#28909;&#22270;&#22788;&#29702;&#22120;&#65292;&#24182;&#21033;&#29992;&#28151;&#21512;&#22686;&#24378;&#26469;&#26377;&#25928;&#21033;&#29992;&#31232;&#32570;&#30340;&#19987;&#23478;&#27880;&#37322;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;eCLIP&#35774;&#35745;&#20026;&#36890;&#29992;&#30340;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#24418;&#24335;&#30340;CLIP&#21464;&#20307;&#65292;&#26080;&#38656;&#20462;&#25913;&#26680;&#24515;&#26550;&#26500;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#20219;&#21153;&#30340;&#35814;&#32454;&#35780;&#20272;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#25512;&#26029;&#12289;&#32447;&#24615;&#25506;&#38024;&#12289;&#36328;&#27169;&#24577;&#26816;&#32034;&#20197;&#21450;&#20351;&#29992;&#20923;&#32467;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#65292;eCLIP&#23637;&#31034;&#20102;&#20854;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10153v1 Announce Type: cross  Abstract: We introduce eCLIP, an enhanced version of the CLIP model that integrates expert annotations in the form of radiologist eye-gaze heatmaps. It tackles key challenges in contrastive multi-modal medical imaging analysis, notably data scarcity and the "modality gap" -- a significant disparity between image and text embeddings that diminishes the quality of representations and hampers cross-modal interoperability. eCLIP integrates a heatmap processor and leverages mixup augmentation to efficiently utilize the scarce expert annotations, thus boosting the model's learning effectiveness. eCLIP is designed to be generally applicable to any variant of CLIP without requiring any modifications of the core architecture. Through detailed evaluations across several tasks, including zero-shot inference, linear probing, cross-modal retrieval, and Retrieval Augmented Generation (RAG) of radiology reports using a frozen Large Language Model, eCLIP showca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09974</link><description>&lt;p&gt;
GET&#65306;&#35299;&#38145;CLIP&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#65292;&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#21253;&#21547;&#26087;&#31867;&#21035;&#21644;&#26032;&#31867;&#21035;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#26088;&#22312;&#20934;&#30830;&#21457;&#29616;&#26032;&#31867;&#21035;&#65292;&#24182;&#27491;&#30830;&#20998;&#31867;&#26087;&#31867;&#21035;&#65292;&#21033;&#29992;&#20174;&#26377;&#26631;&#31614;&#26679;&#26412;&#20013;&#23398;&#20064;&#30340;&#31867;&#21035;&#27010;&#24565;&#12290;&#24403;&#21069;&#30340;GCD&#26041;&#27861;&#21482;&#20351;&#29992;&#21333;&#19968;&#30340;&#35270;&#35273;&#20449;&#24687;&#27169;&#24577;&#65292;&#23548;&#33268;&#22312;&#35270;&#35273;&#19978;&#30456;&#20284;&#31867;&#21035;&#30340;&#20998;&#31867;&#25928;&#26524;&#19981;&#20339;&#12290;&#34429;&#28982;&#26576;&#20123;&#31867;&#21035;&#22312;&#35270;&#35273;&#19978;&#23481;&#26131;&#28151;&#28102;&#65292;&#20294;&#23427;&#20204;&#30340;&#25991;&#26412;&#20449;&#24687;&#21487;&#33021;&#26159;&#19981;&#21516;&#30340;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#23558;&#25991;&#26412;&#20449;&#24687;&#24341;&#20837;&#21040;GCD&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#26080;&#26631;&#31614;&#25968;&#25454;&#32570;&#20047;&#31867;&#21035;&#21517;&#31216;&#65292;&#20351;&#24471;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#26679;&#26412;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;TES&#21033;&#29992;CLIP&#21487;&#20197;&#29983;&#25104;&#23545;&#40784;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#36825;&#19968;&#29305;&#24615;&#65292;&#23558;&#35270;&#35273;&#23884;&#20837;&#36716;&#25442;&#20026;CLIP&#25991;&#26412;&#27169;&#22411;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09974v1 Announce Type: cross  Abstract: Given unlabelled datasets containing both old and new categories, generalized category discovery (GCD) aims to accurately discover new classes while correctly classifying old classes, leveraging the class concepts learned from labeled samples. Current GCD methods only use a single visual modality of information, resulting in poor classification of visually similar classes. Though certain classes are visually confused, their text information might be distinct, motivating us to introduce text information into the GCD task. However, the lack of class names for unlabelled data makes it impractical to utilize text information. To tackle this challenging problem, in this paper, we propose a Text Embedding Synthesizer (TES) to generate pseudo text embeddings for unlabelled samples. Specifically, our TES leverages the property that CLIP can generate aligned vision-language features, converting visual embeddings into tokens of the CLIP's text e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#36335;&#30001;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;PEOs&#26377;&#25928;&#35782;&#21035;&#22270;&#20013;&#38656;&#35201;&#32771;&#34385;&#36827;&#34892;&#31934;&#30830;&#36317;&#31163;&#35745;&#31639;&#30340;&#37051;&#23621;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.11354</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#36335;&#30001;&#30340;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#36335;&#30001;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;PEOs&#26377;&#25928;&#35782;&#21035;&#22270;&#20013;&#38656;&#35201;&#32771;&#34385;&#36827;&#34892;&#31934;&#30830;&#36317;&#31163;&#35745;&#31639;&#30340;&#37051;&#23621;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2402.11354v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;(ANNS)&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;ANNS&#30340;&#20248;&#36234;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#23613;&#31649;&#24341;&#20837;&#20102;&#21508;&#31181;&#22522;&#20110;&#22270;&#30340;ANNS&#20248;&#21270;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#20381;&#36182;&#20110;&#32570;&#20047;&#27491;&#24335;&#29702;&#35770;&#25903;&#25345;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#22522;&#20110;&#22270;&#30340;ANNS&#20013;&#30340;&#36335;&#30001;&#65292;&#35813;&#26041;&#27861;&#22312;&#25506;&#32034;&#22270;&#20013;&#33410;&#28857;&#30340;&#37051;&#23621;&#26102;&#25552;&#20379;&#27010;&#29575;&#20445;&#35777;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#27010;&#29575;&#36335;&#30001;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#25935;&#24863;&#25216;&#26415;&#24320;&#21457;&#20102;&#20004;&#31181;&#22522;&#20934;&#31574;&#30053;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PEOs&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#35782;&#21035;&#22270;&#20013;&#24212;&#32771;&#34385;&#36827;&#34892;&#31934;&#30830;&#36317;&#31163;&#35745;&#31639;&#30340;&#37051;&#23621;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#23454;&#36341;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11354v1 Announce Type: cross  Abstract: Approximate nearest neighbor search (ANNS) in high-dimensional spaces is a pivotal challenge in the field of machine learning. In recent years, graph-based methods have emerged as the superior approach to ANNS, establishing a new state of the art. Although various optimizations for graph-based ANNS have been introduced, they predominantly rely on heuristic methods that lack formal theoretical backing. This paper aims to enhance routing within graph-based ANNS by introducing a method that offers a probabilistic guarantee when exploring a node's neighbors in the graph. We formulate the problem as probabilistic routing and develop two baseline strategies by incorporating locality-sensitive techniques. Subsequently, we introduce PEOs, a novel approach that efficiently identifies which neighbors in the graph should be considered for exact distance computation, thus significantly improving efficiency in practice. Our experiments demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#22312;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#20013;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20197;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06122</link><description>&lt;p&gt;
&#29992;&#26799;&#24230;&#24377;&#23556;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Manipulating Feature Visualizations with Gradient Slingshots. (arXiv:2401.06122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#22312;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#20013;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20197;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#32780;&#22810;&#26679;&#21270;&#30340;&#34920;&#31034;&#65292;&#28982;&#32780;&#65292;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#35821;&#20041;&#24615;&#36136;&#20173;&#28982;&#26410;&#30693;&#12290;&#35299;&#37322;DNNs&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#24120;&#29992;&#26041;&#27861;&#26159;&#28608;&#27963;&#26368;&#22823;&#21270;(AM)&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#21512;&#25104;&#30340;&#36755;&#20837;&#20449;&#21495;&#65292;&#26368;&#22823;&#21270;&#28608;&#27963;&#32593;&#32476;&#20013;&#30340;&#29305;&#23450;&#31070;&#32463;&#20803;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#32780;&#19981;&#25913;&#21464;&#27169;&#22411;&#32467;&#26500;&#25110;&#23545;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20960;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#21151;&#33021;&#30340;&#33021;&#21147;&#65292;&#22312;&#27169;&#22411;&#23457;&#26680;&#36807;&#31243;&#20013;&#20351;&#29992;&#36873;&#25321;&#30340;&#30446;&#26631;&#35299;&#37322;&#23631;&#34109;&#20102;&#21407;&#22987;&#35299;&#37322;&#12290;&#20316;&#20026;&#19968;&#31181;&#34917;&#25937;&#25514;&#26045;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#27490;&#36825;&#31181;&#25805;&#32437;&#30340;&#38450;&#25252;&#25514;&#26045;&#65292;&#24182;&#25552;&#20379;&#20102;&#23450;&#37327;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are capable of learning complex and versatile representations, however, the semantic nature of the learned concepts remains unknown. A common method used to explain the concepts learned by DNNs is Activation Maximization (AM), which generates a synthetic input signal that maximally activates a particular neuron in the network. In this paper, we investigate the vulnerability of this approach to adversarial model manipulations and introduce a novel method for manipulating feature visualization without altering the model architecture or significantly impacting the model's decision-making process. We evaluate the effectiveness of our method on several neural network models and demonstrate its capabilities to hide the functionality of specific neurons by masking the original explanations of neurons with chosen target explanations during model auditing. As a remedy, we propose a protective measure against such manipulations and provide quantitative evidence which 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20114;&#34917;&#20449;&#24687;&#30456;&#20114;&#23398;&#20064;&#65288;CIML&#65289;&#30340;&#26694;&#26550;&#65292;&#22312;&#22810;&#27169;&#24577;&#21307;&#23398;&#24433;&#20687;&#20998;&#21106;&#20013;&#35299;&#20915;&#20102;&#27169;&#24577;&#38388;&#20887;&#20313;&#20449;&#24687;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#36890;&#36807;&#37319;&#29992;&#21152;&#27861;&#21644;&#20219;&#21153;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;CIML&#25104;&#21151;&#22320;&#28040;&#38500;&#20102;&#20887;&#20313;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02717</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21307;&#23398;&#24433;&#20687;&#20998;&#21106;&#30340;&#20114;&#34917;&#20449;&#24687;&#30456;&#20114;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Complementary Information Mutual Learning for Multimodality Medical Image Segmentation. (arXiv:2401.02717v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02717
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#20114;&#34917;&#20449;&#24687;&#30456;&#20114;&#23398;&#20064;&#65288;CIML&#65289;&#30340;&#26694;&#26550;&#65292;&#22312;&#22810;&#27169;&#24577;&#21307;&#23398;&#24433;&#20687;&#20998;&#21106;&#20013;&#35299;&#20915;&#20102;&#27169;&#24577;&#38388;&#20887;&#20313;&#20449;&#24687;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#36890;&#36807;&#37319;&#29992;&#21152;&#27861;&#21644;&#20219;&#21153;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;CIML&#25104;&#21151;&#22320;&#28040;&#38500;&#20102;&#20887;&#20313;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21307;&#23398;&#24433;&#20687;&#30340;&#23616;&#38480;&#24615;&#21644;&#32959;&#30244;&#20449;&#21495;&#30340;&#22810;&#26679;&#24615;&#65292;&#25918;&#23556;&#31185;&#21307;&#29983;&#24517;&#39035;&#21033;&#29992;&#22810;&#27169;&#24577;&#22270;&#20687;&#36827;&#34892;&#32959;&#30244;&#20998;&#21106;&#21644;&#35786;&#26029;&#12290;&#36825;&#23548;&#33268;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#22312;&#20998;&#21106;&#20013;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#27169;&#24577;&#20043;&#38388;&#30340;&#20887;&#20313;&#24615;&#32473;&#29616;&#26377;&#30340;&#22522;&#20110;&#20943;&#27861;&#30340;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#20363;&#22914;&#38169;&#35823;&#21028;&#26029;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#65292;&#24573;&#35270;&#29305;&#23450;&#30340;&#27169;&#24577;&#20449;&#24687;&#65292;&#20197;&#21450;&#22686;&#21152;&#35748;&#30693;&#36127;&#33655;&#12290;&#36825;&#20123;&#26840;&#25163;&#30340;&#38382;&#39064;&#26368;&#32456;&#38477;&#20302;&#20102;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#24182;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20114;&#34917;&#20449;&#24687;&#30456;&#20114;&#23398;&#20064;&#65288;CIML&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#27169;&#24577;&#38388;&#20887;&#20313;&#20449;&#24687;&#30340;&#36127;&#38754;&#24433;&#21709;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#21644;&#35299;&#20915;&#12290;CIML&#37319;&#29992;&#20102;&#21152;&#27861;&#30340;&#24605;&#24819;&#65292;&#24182;&#36890;&#36807;&#24402;&#32435;&#20559;&#32622;&#39537;&#21160;&#30340;&#20219;&#21153;&#20998;&#35299;&#21644;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#20887;&#20313;&#24615;&#36807;&#28388;&#26469;&#28040;&#38500;&#27169;&#24577;&#38388;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;CIML&#23558;&#22810;&#27169;&#24577;&#20998;&#21106;&#20219;&#21153;&#39318;&#20808;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Radiologists must utilize multiple modal images for tumor segmentation and diagnosis due to the limitations of medical imaging and the diversity of tumor signals. This leads to the development of multimodal learning in segmentation. However, the redundancy among modalities creates challenges for existing subtraction-based joint learning methods, such as misjudging the importance of modalities, ignoring specific modal information, and increasing cognitive load. These thorny issues ultimately decrease segmentation accuracy and increase the risk of overfitting. This paper presents the complementary information mutual learning (CIML) framework, which can mathematically model and address the negative impact of inter-modal redundant information. CIML adopts the idea of addition and removes inter-modal redundant information through inductive bias-driven task decomposition and message passing-based redundancy filtering. CIML first decomposes the multimodal segmentation task into multiple subta
&lt;/p&gt;</description></item></channel></rss>