<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36890;&#36807;&#24341;&#20837;TCM&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#25104;&#21151;&#32467;&#21512;&#20102;&#22810;&#26679;&#24615;&#37319;&#26679;&#21644;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#27700;&#24179;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.03728</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#24357;&#21512;&#22810;&#26679;&#24615;&#19982;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bridging Diversity and Uncertainty in Active learning with Self-Supervised Pre-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03728
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;TCM&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#25104;&#21151;&#32467;&#21512;&#20102;&#22810;&#26679;&#24615;&#37319;&#26679;&#21644;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#38382;&#39064;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#27700;&#24179;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#38598;&#25104;&#22522;&#20110;&#22810;&#26679;&#24615;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;TCM&#30340;&#31616;&#21333;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21487;&#20197;&#32531;&#35299;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#25968;&#25454;&#27700;&#24179;&#19978;&#20445;&#25345;&#24378;&#22823;&#24615;&#33021;&#12290;&#36890;&#36807;&#39318;&#20808;&#24212;&#29992;TypiClust&#36827;&#34892;&#22810;&#26679;&#24615;&#37319;&#26679;&#65292;&#38543;&#21518;&#36807;&#28193;&#21040;&#20351;&#29992;Margin&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TCM&#22312;&#20302;&#25968;&#25454;&#21644;&#39640;&#25968;&#25454;&#24773;&#20917;&#19979;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03728v1 Announce Type: cross  Abstract: This study addresses the integration of diversity-based and uncertainty-based sampling strategies in active learning, particularly within the context of self-supervised pre-trained models. We introduce a straightforward heuristic called TCM that mitigates the cold start problem while maintaining strong performance across various data levels. By initially applying TypiClust for diversity sampling and subsequently transitioning to uncertainty sampling with Margin, our approach effectively combines the strengths of both strategies. Our experiments demonstrate that TCM consistently outperforms existing methods across various datasets in both low and high data regimes.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#28145;&#24230;&#23398;&#20064;&#20266;&#24433;&#20943;&#23569;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#22312;&#37325;&#24314;&#20043;&#21518;&#36827;&#34892;&#22788;&#29702;&#65292;&#32780;&#26412;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#22270;&#20687;&#22495;&#36827;&#34892;&#22810;&#27493;&#39588;&#21435;&#20266;&#24433;&#65292;&#20351;&#24471;&#30456;&#23545;&#22256;&#38590;&#21435;&#38500;&#30340;&#20266;&#24433;&#20063;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#12290;</title><link>http://arxiv.org/abs/2309.00494</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#22810;&#38454;&#27573;&#28145;&#24230;&#23398;&#20064;&#20266;&#24433;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
Multi-stage Deep Learning Artifact Reduction for Computed Tomography. (arXiv:2309.00494v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#28145;&#24230;&#23398;&#20064;&#20266;&#24433;&#20943;&#23569;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#22312;&#37325;&#24314;&#20043;&#21518;&#36827;&#34892;&#22788;&#29702;&#65292;&#32780;&#26412;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#22270;&#20687;&#22495;&#36827;&#34892;&#22810;&#27493;&#39588;&#21435;&#20266;&#24433;&#65292;&#20351;&#24471;&#30456;&#23545;&#22256;&#38590;&#21435;&#38500;&#30340;&#20266;&#24433;&#20063;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#20013;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#33719;&#21462;&#30340;&#25237;&#24433;&#22270;&#20687;&#35745;&#31639;&#20986;&#29289;&#20307;&#20869;&#37096;&#32467;&#26500;&#30340;&#22270;&#20687;&#12290;&#36825;&#20123;&#37325;&#24314;&#22270;&#20687;&#30340;&#36136;&#37327;&#23545;&#20110;&#20934;&#30830;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#36825;&#31181;&#36136;&#37327;&#21487;&#33021;&#20250;&#34987;&#21508;&#31181;&#25104;&#20687;&#20266;&#24433;&#38477;&#20302;&#12290;&#20026;&#20102;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#65292;&#33719;&#21462;&#30340;&#25237;&#24433;&#22270;&#20687;&#36890;&#24120;&#36890;&#36807;&#30001;&#22810;&#20010;&#21435;&#20266;&#24433;&#27493;&#39588;&#32452;&#25104;&#30340;&#27969;&#31243;&#36827;&#34892;&#22788;&#29702;&#65292;&#36825;&#20123;&#27493;&#39588;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#22270;&#20687;&#22495;&#65288;&#20363;&#22914;&#65292;&#25237;&#24433;&#22270;&#20687;&#30340;&#24322;&#24120;&#20540;&#21435;&#38500;&#21644;&#37325;&#24314;&#22270;&#20687;&#30340;&#21435;&#22122;&#65289;&#12290;&#36825;&#20123;&#20266;&#24433;&#21435;&#38500;&#26041;&#27861;&#21033;&#29992;&#20102;&#26576;&#20123;&#20266;&#24433;&#22312;&#29305;&#23450;&#22495;&#30456;&#23545;&#20110;&#20854;&#20182;&#22495;&#26356;&#23481;&#26131;&#21435;&#38500;&#30340;&#20107;&#23454;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#20266;&#24433;&#21435;&#38500;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#37117;&#26159;&#22312;&#37325;&#24314;&#20043;&#21518;&#20316;&#20026;&#21518;&#22788;&#29702;&#26041;&#27861;&#24212;&#29992;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#37325;&#24314;&#22495;&#30456;&#23545;&#22256;&#38590;&#21435;&#38500;&#30340;&#20266;&#24433;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#21435;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Computed Tomography (CT), an image of the interior structure of an object is computed from a set of acquired projection images. The quality of these reconstructed images is essential for accurate analysis, but this quality can be degraded by a variety of imaging artifacts. To improve reconstruction quality, the acquired projection images are often processed by a pipeline consisting of multiple artifact-removal steps applied in various image domains (e.g., outlier removal on projection images and denoising of reconstruction images). These artifact-removal methods exploit the fact that certain artifacts are easier to remove in a certain domain compared with other domains.  Recently, deep learning methods have shown promising results for artifact removal for CT images. However, most existing deep learning methods for CT are applied as a post-processing method after reconstruction. Therefore, artifacts that are relatively difficult to remove in the reconstruction domain may not be effec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.09059</link><description>&lt;p&gt;
&#25991;&#23383;&#24819;&#35937;&#30340;&#37322;&#25918;&#65306;&#36890;&#36807;&#25506;&#32034;&#25991;&#23383;&#30340;&#21147;&#37327;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words. (arXiv:2307.09059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#30340;&#30446;&#26631;&#26159;&#20174;&#22823;&#22411;&#22270;&#24211;&#20013;&#26816;&#32034;&#19982;&#32473;&#23450;&#25991;&#26412;&#25551;&#36848;&#30456;&#21305;&#37197;&#30340;&#20154;&#29289;&#22270;&#20687;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#20449;&#24687;&#34920;&#31034;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#25991;&#26412;&#27169;&#24577;&#36890;&#36807;&#35789;&#27719;&#21644;&#35821;&#27861;&#32467;&#26500;&#20256;&#36882;&#25277;&#35937;&#21644;&#31934;&#30830;&#30340;&#20449;&#24687;&#65292;&#32780;&#35270;&#35273;&#27169;&#24577;&#36890;&#36807;&#22270;&#20687;&#20256;&#36882;&#20855;&#20307;&#21644;&#30452;&#35266;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#25991;&#23383;&#34920;&#31034;&#30340;&#34920;&#36798;&#21147;&#65292;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#22270;&#20687;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#21477;&#23376;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#37322;&#25918;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#20154;&#29289;&#26816;&#32034;&#20013;&#30340;&#25991;&#23383;&#24819;&#35937;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#20840;&#38754;CLIP&#27169;&#22411;&#20316;&#20026;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21452;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#20808;&#21069;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Text-to-image person retrieval is to retrieve person images from a large gallery that match the given textual descriptions. The main challenge of this task lies in the significant differences in information representation between the visual and textual modalities. The textual modality conveys abstract and precise information through vocabulary and grammatical structures, while the visual modality conveys concrete and intuitive information through images. To fully leverage the expressive power of textual representations, it is essential to accurately map abstract textual descriptions to specific images.  To address this issue, we propose a novel framework to Unleash the Imagination of Text (UIT) in text-to-image person retrieval, aiming to fully explore the power of words in sentences. Specifically, the framework employs the pre-trained full CLIP model as a dual encoder for the images and texts , taking advantage of prior cross-modal alignment knowledge. The Text-guided Imag
&lt;/p&gt;</description></item></channel></rss>