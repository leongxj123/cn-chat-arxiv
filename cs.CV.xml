<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>ALERT-Transformer&#26159;&#19968;&#31181;&#23558;&#24322;&#27493;&#24863;&#30693;&#19982;&#21516;&#27493;&#22788;&#29702;&#30456;&#32467;&#21512;&#30340;&#26032;&#39062;&#26725;&#25509;&#26041;&#24335;&#65292;&#36890;&#36807;ALERT&#27169;&#22359;&#12289;&#28789;&#27963;&#30340;&#25968;&#25454;&#35835;&#21462;&#21644;&#22522;&#20110;&#22359;&#30340;&#31232;&#30095;&#24615;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#26102;&#20107;&#20214;&#39537;&#21160;&#26102;&#31354;&#25968;&#25454;&#30340;&#32463;&#20856;&#22788;&#29702;&#65292;&#20854;&#24615;&#33021;&#36229;&#36807;&#31454;&#20105;&#23545;&#25163;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#24310;&#36831;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01393</link><description>&lt;p&gt;
ALERT-Transformer: &#23558;&#24322;&#27493;&#21644;&#21516;&#27493;&#26426;&#22120;&#23398;&#20064;&#26725;&#25509;&#22312;&#23454;&#26102;&#20107;&#20214;&#39537;&#21160;&#30340;&#26102;&#31354;&#25968;&#25454;&#19978;
&lt;/p&gt;
&lt;p&gt;
ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01393
&lt;/p&gt;
&lt;p&gt;
ALERT-Transformer&#26159;&#19968;&#31181;&#23558;&#24322;&#27493;&#24863;&#30693;&#19982;&#21516;&#27493;&#22788;&#29702;&#30456;&#32467;&#21512;&#30340;&#26032;&#39062;&#26725;&#25509;&#26041;&#24335;&#65292;&#36890;&#36807;ALERT&#27169;&#22359;&#12289;&#28789;&#27963;&#30340;&#25968;&#25454;&#35835;&#21462;&#21644;&#22522;&#20110;&#22359;&#30340;&#31232;&#30095;&#24615;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#26102;&#20107;&#20214;&#39537;&#21160;&#26102;&#31354;&#25968;&#25454;&#30340;&#32463;&#20856;&#22788;&#29702;&#65292;&#20854;&#24615;&#33021;&#36229;&#36807;&#31454;&#20105;&#23545;&#25163;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31264;&#23494;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#30001;&#20107;&#20214;&#24863;&#24212;&#22120;&#20135;&#29983;&#30340;&#36830;&#32493;&#36229;&#31232;&#30095;&#26102;&#31354;&#25968;&#25454;&#30340;&#32463;&#20856;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#31649;&#36947;&#65292;&#30001;&#24322;&#27493;&#24863;&#30693;&#21644;&#21516;&#27493;&#22788;&#29702;&#32452;&#25104;&#65292;&#32467;&#21512;&#20102;&#20960;&#20010;&#24605;&#36335;&#65306;&#65288;1&#65289;&#22522;&#20110;PointNet&#27169;&#22411;&#30340;&#23884;&#20837;&#8212;&#8212;ALERT&#27169;&#22359;&#65292;&#21487;&#20197;&#36890;&#36807;&#27844;&#28431;&#26426;&#21046;&#19981;&#26029;&#25972;&#21512;&#26032;&#20107;&#20214;&#24182;&#28040;&#38500;&#26087;&#20107;&#20214;&#65292;&#65288;2&#65289;&#23884;&#20837;&#25968;&#25454;&#30340;&#28789;&#27963;&#35835;&#21462;&#65292;&#21487;&#20197;&#20197;&#20219;&#20309;&#37319;&#26679;&#29575;&#23558;&#22987;&#32456;&#26368;&#26032;&#30340;&#29305;&#24449;&#36755;&#20837;&#21040;&#19979;&#28216;&#27169;&#22411;&#20013;&#65292;&#65288;3&#65289;&#20511;&#37492;Vision Transformer&#30340;&#22522;&#20110;&#22359;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;&#36755;&#20837;&#30340;&#31232;&#30095;&#24615;&#20197;&#20248;&#21270;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#36825;&#20123;&#23884;&#20837;&#28982;&#21518;&#30001;&#19968;&#20010;&#32463;&#36807;&#23545;&#35937;&#21644;&#25163;&#21183;&#35782;&#21035;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#36827;&#34892;&#22788;&#29702;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#20302;&#30340;&#24310;&#36831;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#24322;&#27493;&#27169;&#22411;&#21487;&#20197;&#20197;&#20219;&#20309;&#25152;&#38656;&#30340;&#37319;&#26679;&#29575;&#36827;&#34892;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to enable classic processing of continuous ultra-sparse spatiotemporal data generated by event-based sensors with dense machine learning models. We propose a novel hybrid pipeline composed of asynchronous sensing and synchronous processing that combines several ideas: (1) an embedding based on PointNet models -- the ALERT module -- that can continuously integrate new and dismiss old events thanks to a leakage mechanism, (2) a flexible readout of the embedded data that allows to feed any downstream model with always up-to-date features at any sampling rate, (3) exploiting the input sparsity in a patch-based approach inspired by Vision Transformer to optimize the efficiency of the method. These embeddings are then processed by a transformer model trained for object and gesture recognition. Using this approach, we achieve performances at the state-of-the-art with a lower latency than competitors. We also demonstrate that our asynchronous model can operate at any desired sampling r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#22914;&#20309;&#20174;&#20855;&#26377;&#32454;&#31890;&#24230;&#27010;&#24565;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#27010;&#24565;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#36755;&#20986;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00912</link><description>&lt;p&gt;
&#33021;&#22815;&#32422;&#26463;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#36755;&#20837;&#29305;&#24449;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can we Constrain Concept Bottleneck Models to Learn Semantically Meaningful Input Features?
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#22914;&#20309;&#20174;&#20855;&#26377;&#32454;&#31890;&#24230;&#27010;&#24565;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#27010;&#24565;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#36755;&#20986;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#34987;&#35748;&#20026;&#20855;&#26377;&#20869;&#22312;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#39318;&#20808;&#39044;&#27979;&#19968;&#32452;&#20154;&#20026;&#23450;&#20041;&#30340;&#27010;&#24565;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#27010;&#24565;&#26469;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#23454;&#29616;&#23436;&#20840;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#21450;&#30830;&#20445;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#20449;&#20219;&#65292;&#25105;&#20204;&#38656;&#35201;&#20445;&#35777;&#27010;&#24565;&#30340;&#39044;&#27979;&#26159;&#22522;&#20110;&#35821;&#20041;&#26144;&#23556;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#20363;&#22914;&#65292;&#20154;&#20204;&#21487;&#33021;&#26399;&#26395;&#22270;&#20687;&#20013;&#34920;&#31034;&#39592;&#25240;&#30340;&#20687;&#32032;&#34987;&#29992;&#20110;&#39044;&#27979;&#39592;&#25240;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25991;&#29486;&#34920;&#26126;&#36825;&#24182;&#19981;&#26159;&#20107;&#23454;&#65292;&#22240;&#20026;&#27010;&#24565;&#39044;&#27979;&#36890;&#24120;&#19982;&#19981;&#30456;&#20851;&#30340;&#36755;&#20837;&#29305;&#24449;&#26144;&#23556;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#26159;&#30001;&#20110;&#27010;&#24565;&#27880;&#37322;&#30340;&#19981;&#20934;&#30830;&#25110;&#32773;&#36755;&#20837;&#29305;&#24449;&#19982;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#19981;&#28165;&#26224;&#23548;&#33268;&#30340;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25968;&#25454;&#38598;&#26631;&#27880;&#23545;CBMs&#20013;&#27010;&#24565;&#34920;&#31034;&#30340;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#30740;&#31350;&#36739;&#23569;&#30340;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;CBMs&#22914;&#20309;&#20174;&#20855;&#26377;&#32454;&#31890;&#24230;&#27010;&#24565;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#27010;&#24565;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept Bottleneck Models (CBMs) are considered inherently interpretable because they first predict a set of human-defined concepts before using these concepts to predict the output of a downstream task. For inherent interpretability to be fully realised, and ensure trust in a model's output, we need to guarantee concepts are predicted based on semantically mapped input features. For example, one might expect the pixels representing a broken bone in an image to be used for the prediction of a fracture. However, current literature indicates this is not the case, as concept predictions are often mapped to irrelevant input features. We hypothesise that this occurs when concept annotations are inaccurate or how input features should relate to concepts is unclear. In general, the effect of dataset labelling on concept representations in CBMs remains an understudied area. Therefore, in this paper, we examine how CBMs learn concepts from datasets with fine-grained concept annotations. We demo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#31243;&#24207;&#21270;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;&#26694;&#26550;SceneX&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31243;&#24207;&#21270;&#27169;&#22411;</title><link>https://arxiv.org/abs/2403.15698</link><description>&lt;p&gt;
SceneX&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31243;&#24207;&#21270;&#21487;&#25511;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SceneX:Procedural Controllable Large-scale Scene Generation via Large-language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15698
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#31243;&#24207;&#21270;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;&#26694;&#26550;SceneX&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31243;&#24207;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#25152;&#38656;&#30340;&#22330;&#26223;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#19981;&#20860;&#23481;&#24037;&#19994;&#27969;&#31243;&#30340;3D&#22522;&#20803;&#65288;&#22914;&#28857;&#20113;&#25110;&#36752;&#23556;&#22330;&#65289;&#26469;&#34920;&#31034;&#22330;&#26223;&#65292;&#36825;&#23548;&#33268;&#23398;&#26415;&#30740;&#31350;&#19982;&#24037;&#19994;&#37096;&#32626;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#31243;&#24207;&#21270;&#21487;&#25511;&#29983;&#25104;&#65288;PCG&#65289;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#25216;&#26415;&#65292;&#21487;&#21019;&#24314;&#21487;&#25193;&#23637;&#21644;&#39640;&#36136;&#37327;&#30340;&#36164;&#20135;&#65292;&#20294;&#23545;&#26222;&#36890;&#29992;&#25143;&#19981;&#21451;&#22909;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#28145;&#20837;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#31243;&#24207;&#21270;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;&#26694;&#26550;SceneX&#65292;&#21487;&#20197;&#26681;&#25454;&#35774;&#35745;&#24072;&#30340;&#25991;&#26412;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31243;&#24207;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15698v1 Announce Type: cross  Abstract: Due to its great application potential, large-scale scene generation has drawn extensive attention in academia and industry. Recent research employs powerful generative models to create desired scenes and achieves promising results. However, most of these methods represent the scene using 3D primitives (e.g. point cloud or radiance field) incompatible with the industrial pipeline, which leads to a substantial gap between academic research and industrial deployment. Procedural Controllable Generation (PCG) is an efficient technique for creating scalable and high-quality assets, but it is unfriendly for ordinary users as it demands profound domain expertise. To address these issues, we resort to using the large language model (LLM) to drive the procedural modeling. In this paper, we introduce a large-scale scene generation framework, SceneX, which can automatically produce high-quality procedural models according to designers' textual de
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20004;&#27493;&#24418;&#24335;&#39044;&#27979;&#26041;&#27861;&#65292;&#26412;&#25991;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#36793;&#30028;&#26694;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#65292;&#20445;&#35777;&#20102;&#23545;&#35937;&#36793;&#30028;&#26694;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#30340;&#35206;&#30422;&#29575;&#65292;&#21253;&#25324;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#23545;&#35937;&#65292;&#21516;&#26102;&#30830;&#20445;&#36793;&#30028;&#26694;&#21306;&#38388;&#33021;&#22815;&#36866;&#24212;&#29289;&#20307;&#22823;&#23567;&#65292;&#23454;&#29616;&#26356;&#24179;&#34913;&#30340;&#35206;&#30422;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.07263</link><description>&lt;p&gt;
&#36890;&#36807;&#20004;&#27493;&#24418;&#24335;&#39044;&#27979;&#23454;&#29616;&#33258;&#36866;&#24212;&#36793;&#30028;&#26694;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adaptive Bounding Box Uncertainties via Two-Step Conformal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07263
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20004;&#27493;&#24418;&#24335;&#39044;&#27979;&#26041;&#27861;&#65292;&#26412;&#25991;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#36793;&#30028;&#26694;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#65292;&#20445;&#35777;&#20102;&#23545;&#35937;&#36793;&#30028;&#26694;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#30340;&#35206;&#30422;&#29575;&#65292;&#21253;&#25324;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#23545;&#35937;&#65292;&#21516;&#26102;&#30830;&#20445;&#36793;&#30028;&#26694;&#21306;&#38388;&#33021;&#22815;&#36866;&#24212;&#29289;&#20307;&#22823;&#23567;&#65292;&#23454;&#29616;&#26356;&#24179;&#34913;&#30340;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#20687;&#33258;&#21160;&#39550;&#39542;&#36825;&#26679;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#32771;&#34385;&#20026;&#22810;&#29289;&#20307;&#26816;&#27979;&#37327;&#21270;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#24418;&#24335;&#39044;&#27979;&#26469;&#33719;&#24471;&#20855;&#26377;&#20445;&#35777;&#35206;&#30422;&#29575;&#30340;&#29289;&#20307;&#36793;&#30028;&#26694;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#12290;&#36825;&#26679;&#20570;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#36793;&#30028;&#26694;&#30340;&#39044;&#27979;&#21462;&#20915;&#20110;&#29289;&#20307;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#27493;&#24418;&#24335;&#26041;&#27861;&#65292;&#23558;&#23545;&#39044;&#27979;&#31867;&#21035;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#21040;&#36793;&#30028;&#26694;&#30340;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#20013;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#30340;&#24418;&#24335;&#35206;&#30422;&#20445;&#35777;&#30340;&#26377;&#25928;&#24615;&#26356;&#24191;&#27867;&#65292;&#21253;&#25324;&#20102;&#34987;&#38169;&#35823;&#20998;&#31867;&#30340;&#29289;&#20307;&#65292;&#30830;&#20445;&#23427;&#20204;&#22312;&#38656;&#35201;&#26368;&#22823;&#23433;&#20840;&#20445;&#35777;&#26102;&#30340;&#23454;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26032;&#39062;&#30340;&#38598;&#25104;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#24418;&#24335;&#65292;&#20197;&#30830;&#20445;&#36793;&#30028;&#26694;&#21306;&#38388;&#33021;&#22815;&#36866;&#24212;&#29289;&#20307;&#22823;&#23567;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24179;&#34913;&#30340;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07263v1 Announce Type: cross  Abstract: Quantifying a model's predictive uncertainty is essential for safety-critical applications such as autonomous driving. We consider quantifying such uncertainty for multi-object detection. In particular, we leverage conformal prediction to obtain uncertainty intervals with guaranteed coverage for object bounding boxes. One challenge in doing so is that bounding box predictions are conditioned on the object's class label. Thus, we develop a novel two-step conformal approach that propagates uncertainty in predicted class labels into the uncertainty intervals for the bounding boxes. This broadens the validity of our conformal coverage guarantees to include incorrectly classified objects, ensuring their usefulness when maximal safety assurances are required. Moreover, we investigate novel ensemble and quantile regression formulations to ensure the bounding box intervals are adaptive to object size, leading to a more balanced coverage across
&lt;/p&gt;</description></item><item><title>GenNBV&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#36890;&#29992;&#30340;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#31574;&#30053;&#65292;&#36890;&#36807;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#21644;&#25193;&#23637;&#21040;5D&#33258;&#30001;&#31354;&#38388;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#26080;&#20154;&#26426;&#20174;&#20219;&#24847;&#35270;&#35282;&#36827;&#34892;&#25195;&#25551;&#65292;&#29978;&#33267;&#19982;&#26410;&#30693;&#20960;&#20309;&#20307;&#36827;&#34892;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22810;&#28304;&#29366;&#24577;&#23884;&#20837;&#20197;&#22686;&#24378;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16174</link><description>&lt;p&gt;
GenNBV: &#36890;&#29992;&#30340;&#20027;&#21160;&#24335;&#19977;&#32500;&#37325;&#24314;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16174
&lt;/p&gt;
&lt;p&gt;
GenNBV&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#36890;&#29992;&#30340;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#31574;&#30053;&#65292;&#36890;&#36807;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#21644;&#25193;&#23637;&#21040;5D&#33258;&#30001;&#31354;&#38388;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#26080;&#20154;&#26426;&#20174;&#20219;&#24847;&#35270;&#35282;&#36827;&#34892;&#25195;&#25551;&#65292;&#29978;&#33267;&#19982;&#26410;&#30693;&#20960;&#20309;&#20307;&#36827;&#34892;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22810;&#28304;&#29366;&#24577;&#23884;&#20837;&#20197;&#22686;&#24378;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#25216;&#26415;&#36827;&#27493;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#22330;&#26223;&#30340;&#30495;&#23454;&#25968;&#23383;&#21270;, &#20294;&#26159;&#22270;&#20687;&#25429;&#33719;&#36807;&#31243;&#20173;&#28982;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#20027;&#21160;&#24335;&#19977;&#32500;&#37325;&#24314;&#30340;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#65288;NBV&#65289;&#31574;&#30053;&#26469;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NBV&#31574;&#30053;&#20005;&#37325;&#20381;&#36182;&#25163;&#24037;&#35774;&#35745;&#30340;&#26631;&#20934;&#12289;&#26377;&#38480;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#25110;&#32773;&#26159;&#38024;&#23545;&#27599;&#20010;&#22330;&#26223;&#20248;&#21270;&#30340;&#34920;&#31034;&#12290;&#36825;&#20123;&#32422;&#26463;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36328;&#25968;&#25454;&#38598;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GenNBV&#65292;&#19968;&#20010;&#31471;&#21040;&#31471;&#36890;&#29992;&#30340;NBV&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#37319;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26694;&#26550;&#65292;&#23558;&#20856;&#22411;&#26377;&#38480;&#30340;&#21160;&#20316;&#31354;&#38388;&#25193;&#23637;&#21040;5D&#33258;&#30001;&#31354;&#38388;&#12290;&#23427;&#36171;&#20104;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#26426;&#26080;&#20154;&#26426;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#20174;&#20219;&#20309;&#35270;&#35282;&#36827;&#34892;&#25195;&#25551;&#65292;&#29978;&#33267;&#21487;&#20197;&#19982;&#26410;&#35265;&#20960;&#20309;&#20307;&#36827;&#34892;&#20132;&#20114;&#12290;&#20026;&#20102;&#22686;&#24378;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#28304;&#29366;&#24577;&#23884;&#20837;&#65292;&#21253;&#25324;&#20960;&#20309;&#12289;&#35821;&#20041;&#21644;&#21160;&#20316;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16174v1 Announce Type: cross  Abstract: While recent advances in neural radiance field enable realistic digitization for large-scale scenes, the image-capturing process is still time-consuming and labor-intensive. Previous works attempt to automate this process using the Next-Best-View (NBV) policy for active 3D reconstruction. However, the existing NBV policies heavily rely on hand-crafted criteria, limited action space, or per-scene optimized representations. These constraints limit their cross-dataset generalizability. To overcome them, we propose GenNBV, an end-to-end generalizable NBV policy. Our policy adopts a reinforcement learning (RL)-based framework and extends typical limited action space to 5D free space. It empowers our agent drone to scan from any viewpoint, and even interact with unseen geometries during training. To boost the cross-dataset generalizability, we also propose a novel multi-source state embedding, including geometric, semantic, and action repres
&lt;/p&gt;</description></item><item><title>SimPro&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#36866;&#24212;&#30340;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20851;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#30340;&#39044;&#23450;&#20041;&#20551;&#35774;&#65292;&#36890;&#36807;&#21019;&#26032;&#22320;&#25913;&#36827;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#65292;&#26126;&#30830;&#20998;&#31163;&#26465;&#20214;&#21644;&#36793;&#32536;&#31867;&#21035;&#20998;&#24067;&#30340;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2402.13505</link><description>&lt;p&gt;
SimPro&#65306;&#19968;&#20010;&#31616;&#21333;&#30340;&#27010;&#29575;&#26694;&#26550;&#23454;&#29616;&#36924;&#30495;&#30340;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13505
&lt;/p&gt;
&lt;p&gt;
SimPro&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#36866;&#24212;&#30340;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20851;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#30340;&#39044;&#23450;&#20041;&#20551;&#35774;&#65292;&#36890;&#36807;&#21019;&#26032;&#22320;&#25913;&#36827;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#65292;&#26126;&#30830;&#20998;&#31163;&#26465;&#20214;&#21644;&#36793;&#32536;&#31867;&#21035;&#20998;&#24067;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#38598;&#20013;&#22312;&#35299;&#20915;&#19968;&#20010;&#26356;&#20026;&#36924;&#30495;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65306;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21516;&#26102;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#31867;&#21035;&#20998;&#24067;&#26082;&#26410;&#30693;&#21448;&#21487;&#33021;&#19981;&#21305;&#37197;&#12290;&#24403;&#21069;&#36825;&#19968;&#39046;&#22495;&#30340;&#26041;&#27861;&#24448;&#24448;&#39044;&#35774;&#20102;&#20851;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#31867;&#21035;&#20998;&#24067;&#30340;&#20005;&#26684;&#20551;&#35774;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#27169;&#22411;&#20165;&#36866;&#24212;&#20110;&#26576;&#20123;&#20998;&#24067;&#33539;&#22260;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#24230;&#36866;&#24212;&#24615;&#30340;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;SimPro&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20851;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#30340;&#39044;&#23450;&#20041;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24314;&#31435;&#22312;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#26126;&#30830;&#20998;&#31163;&#26465;&#20214;&#21644;&#36793;&#32536;&#31867;&#21035;&#20998;&#24067;&#30340;&#24314;&#27169;&#65292;&#21019;&#26032;&#22320;&#25913;&#36827;&#20102;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#12290;&#36825;&#31181;&#20998;&#31163;&#20419;&#36827;&#20102;&#22312;&#26368;&#22823;&#21270;&#36807;&#31243;&#20013;&#23545;&#31867;&#21035;&#20998;&#24067;&#36827;&#34892;&#20272;&#35745;&#30340;&#38381;&#21512;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13505v1 Announce Type: new  Abstract: Recent advancements in semi-supervised learning have focused on a more realistic yet challenging task: addressing imbalances in labeled data while the class distribution of unlabeled data remains both unknown and potentially mismatched. Current approaches in this sphere often presuppose rigid assumptions regarding the class distribution of unlabeled data, thereby limiting the adaptability of models to only certain distribution ranges. In this study, we propose a novel approach, introducing a highly adaptable framework, designated as SimPro, which does not rely on any predefined assumptions about the distribution of unlabeled data. Our framework, grounded in a probabilistic model, innovatively refines the expectation-maximization (EM) algorithm by explicitly decoupling the modeling of conditional and marginal class distributions. This separation facilitates a closed-form solution for class distribution estimation during the maximization p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#29983;&#25104;AI&#27169;&#22411;&#22312;&#29983;&#25104;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#26102;&#22312;&#39057;&#22495;&#20013;&#30340;DCT&#31995;&#25968;&#30340;&#32479;&#35745;&#29305;&#24449;&#12290;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#8220;&#36776;&#21035;&#25351;&#32441;&#8221;&#65292;&#21487;&#20197;&#21033;&#29992;&#23427;&#26469;&#25913;&#21892;&#29616;&#26377;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.02209</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;&#29983;&#25104;AI&#39046;&#22495;&#21033;&#29992;DCT&#36712;&#36857;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
On the Exploitation of DCT-Traces in the Generative-AI Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#29983;&#25104;AI&#27169;&#22411;&#22312;&#29983;&#25104;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#26102;&#22312;&#39057;&#22495;&#20013;&#30340;DCT&#31995;&#25968;&#30340;&#32479;&#35745;&#29305;&#24449;&#12290;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#8220;&#36776;&#21035;&#25351;&#32441;&#8221;&#65292;&#21487;&#20197;&#21033;&#29992;&#23427;&#26469;&#25913;&#21892;&#29616;&#26377;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#23545;&#20110;&#32593;&#32476;&#23433;&#20840;&#21644;&#25968;&#23383;&#21462;&#35777;&#39046;&#22495;&#26469;&#35828;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#26368;&#36817;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#35299;&#20915;&#26041;&#26696;&#25152;&#33719;&#24471;&#30340;&#39640;&#36136;&#37327;&#32467;&#26524;&#12290;&#20960;&#20046;&#25152;&#26377;&#29983;&#25104;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#20013;&#30041;&#19979;&#20102;&#29420;&#29305;&#30340;&#30165;&#36857;&#65292;&#22914;&#26524;&#23545;&#20854;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#21644;&#35782;&#21035;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#30165;&#36857;&#26469;&#25913;&#21892;&#29616;&#26377;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#30340;&#27867;&#21270;&#38480;&#21046;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#30001;GAN&#21644;&#25193;&#25955;&#27169;&#22411;&#24341;&#25806;&#29983;&#25104;&#30340;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#22312;&#39057;&#22495;&#20013;&#30340;&#29305;&#24449;&#65292;&#35814;&#32454;&#30740;&#31350;&#20102;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;(DCT)&#31995;&#25968;&#30340;&#32479;&#35745;&#20998;&#24067;&#12290;&#25105;&#20204;&#35748;&#35782;&#21040;&#24182;&#38750;&#25152;&#26377;&#31995;&#25968;&#23545;&#22270;&#20687;&#26816;&#27979;&#30340;&#36129;&#29486;&#30456;&#21516;&#65292;&#25105;&#20204;&#20551;&#35774;&#23384;&#22312;&#19968;&#31181;&#29420;&#29305;&#30340;&#8220;&#36776;&#21035;&#25351;&#32441;&#8221;&#65292;&#23884;&#20837;&#22312;&#29305;&#23450;&#31995;&#25968;&#32452;&#21512;&#20013;&#12290;&#20026;&#20102;&#35782;&#21035;&#23427;&#20204;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#31995;&#25968;&#32452;&#21512;&#36827;&#34892;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#21487;&#35299;&#37322;AI(XAI)&#30340;LIME&#31639;&#27861;&#26469;&#25628;&#32034;...
&lt;/p&gt;
&lt;p&gt;
Deepfakes represent one of the toughest challenges in the world of Cybersecurity and Digital Forensics, especially considering the high-quality results obtained with recent generative AI-based solutions. Almost all generative models leave unique traces in synthetic data that, if analyzed and identified in detail, can be exploited to improve the generalization limitations of existing deepfake detectors. In this paper we analyzed deepfake images in the frequency domain generated by both GAN and Diffusion Model engines, examining in detail the underlying statistical distribution of Discrete Cosine Transform (DCT) coefficients. Recognizing that not all coefficients contribute equally to image detection, we hypothesize the existence of a unique "discriminative fingerprint", embedded in specific combinations of coefficients. To identify them, Machine Learning classifiers were trained on various combinations of coefficients. In addition, the Explainable AI (XAI) LIME algorithm was used to sea
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Infinite dSprites&#24037;&#20855;&#65292;&#29992;&#20110;&#21019;&#24314;&#20219;&#24847;&#38271;&#24230;&#30340;&#36830;&#32493;&#20998;&#31867;&#21644;&#20998;&#35299;&#22522;&#20934;&#65292;&#21487;&#20197;&#20840;&#38754;&#25511;&#21046;&#29983;&#25104;&#22240;&#32032;&#65292;&#26377;&#26395;&#32553;&#23567;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#19982;&#20154;&#31867;&#23398;&#20064;&#22312;&#21160;&#24577;&#24320;&#25918;&#29615;&#22659;&#20013;&#30340;&#24046;&#36317;</title><link>https://arxiv.org/abs/2312.16731</link><description>&lt;p&gt;
&#29992;&#20110;&#20998;&#35299;&#36830;&#32493;&#23398;&#20064;&#30340;&#26080;&#38480;dSprites&#65306;&#23558;&#35760;&#24518;&#32534;&#36753;&#19982;&#27867;&#21270;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Infinite dSprites for Disentangled Continual Learning: Separating Memory Edits from Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16731
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Infinite dSprites&#24037;&#20855;&#65292;&#29992;&#20110;&#21019;&#24314;&#20219;&#24847;&#38271;&#24230;&#30340;&#36830;&#32493;&#20998;&#31867;&#21644;&#20998;&#35299;&#22522;&#20934;&#65292;&#21487;&#20197;&#20840;&#38754;&#25511;&#21046;&#29983;&#25104;&#22240;&#32032;&#65292;&#26377;&#26395;&#32553;&#23567;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#19982;&#20154;&#31867;&#23398;&#20064;&#22312;&#21160;&#24577;&#24320;&#25918;&#29615;&#22659;&#20013;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#25345;&#32493;&#23398;&#20064;&#30340;&#33021;&#21147;&#21463;&#21040;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38459;&#30861;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20250;&#35206;&#30422;&#29616;&#26377;&#30693;&#35782;&#12290;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#27491;&#21017;&#21270;&#12289;&#21442;&#25968;&#38548;&#31163;&#25110;&#25490;&#32451;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#22312;&#20165;&#21253;&#21547;&#23569;&#25968;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#20102;&#21462;&#24471;&#36827;&#23637;&#20197;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Infinite dSprites&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#27905;&#30340;&#24037;&#20855;&#65292;&#21487;&#21019;&#24314;&#20219;&#24847;&#38271;&#24230;&#30340;&#36830;&#32493;&#20998;&#31867;&#21644;&#20998;&#35299;&#22522;&#20934;&#65292;&#24182;&#23545;&#29983;&#25104;&#22240;&#32032;&#25317;&#26377;&#23436;&#20840;&#25511;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#22312;&#36275;&#22815;&#38271;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#37117;&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16731v2 Announce Type: replace  Abstract: The ability of machine learning systems to learn continually is hindered by catastrophic forgetting, the tendency of neural networks to overwrite existing knowledge when learning a new task. Continual learning methods alleviate this problem through regularization, parameter isolation, or rehearsal, but they are typically evaluated on benchmarks comprising only a handful of tasks. In contrast, humans are able to learn continually in dynamic, open-world environments, effortlessly achieving one-shot memorization of unfamiliar objects and reliably recognizing them under various transformations. To make progress towards closing this gap, we introduce Infinite dSprites, a parsimonious tool for creating continual classification and disentanglement benchmarks of arbitrary length and with full control over generative factors. We show that over a sufficiently long time horizon, the performance of all major types of continual learning methods d
&lt;/p&gt;</description></item><item><title>ScreenQA&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;86K&#20010;&#38382;&#31572;&#23545;&#22312;RICO&#25968;&#25454;&#38598;&#19978;&#27880;&#37322;&#65292;&#26088;&#22312;&#35780;&#20272;&#23631;&#24149;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2209.08199</link><description>&lt;p&gt;
ScreenQA: &#31227;&#21160;&#24212;&#29992;&#25130;&#22270;&#19978;&#30340;&#22823;&#35268;&#27169;&#38382;&#31572;&#23545;
&lt;/p&gt;
&lt;p&gt;
ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.08199
&lt;/p&gt;
&lt;p&gt;
ScreenQA&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;86K&#20010;&#38382;&#31572;&#23545;&#22312;RICO&#25968;&#25454;&#38598;&#19978;&#27880;&#37322;&#65292;&#26088;&#22312;&#35780;&#20272;&#23631;&#24149;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;ScreenQA&#65292;&#29992;&#20110;&#36890;&#36807;&#38382;&#31572;&#26469;&#29702;&#35299;&#23631;&#24149;&#20869;&#23481;&#12290;&#29616;&#26377;&#30340;&#23631;&#24149;&#25968;&#25454;&#38598;&#35201;&#20040;&#20391;&#37325;&#20110;&#32467;&#26500;&#21644;&#32452;&#20214;&#32423;&#21035;&#30340;&#29702;&#35299;&#65292;&#35201;&#20040;&#20391;&#37325;&#20110;&#20687;&#23548;&#33322;&#21644;&#20219;&#21153;&#23436;&#25104;&#20043;&#31867;&#30340;&#26356;&#39640;&#32423;&#21035;&#30340;&#32452;&#21512;&#20219;&#21153;&#12290;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#22312;RICO&#25968;&#25454;&#38598;&#19978;&#27880;&#37322;86K&#20010;&#38382;&#31572;&#23545;&#26469;&#24357;&#21512;&#36825;&#20004;&#32773;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24076;&#26395;&#33021;&#22815;&#22522;&#20934;&#21270;&#23631;&#24149;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.08199v2 Announce Type: replace  Abstract: We present a new task and dataset, ScreenQA, for screen content understanding via question answering. The existing screen datasets are focused either on structure and component-level understanding, or on a much higher-level composite task such as navigation and task completion. We attempt to bridge the gap between these two by annotating 86K question-answer pairs over the RICO dataset in hope to benchmark the screen reading comprehension capacity.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KI-PMF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20808;&#39564;&#30693;&#35782;&#65292;&#23545;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#26410;&#26469;&#34892;&#21160;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#36981;&#24490;&#36710;&#36742;&#30340;&#36816;&#21160;&#32422;&#26463;&#21644;&#34892;&#39542;&#29615;&#22659;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#36890;&#36807;&#26465;&#20214;&#21270;&#32593;&#32476;&#20197;&#36981;&#24490;&#29289;&#29702;&#23450;&#24459;&#65292;&#21487;&#20197;&#33719;&#24471;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#39044;&#27979;&#65292;&#23545;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#32500;&#25252;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.12007</link><description>&lt;p&gt;
KI-PMF&#65306;&#30693;&#35782;&#32508;&#21512;&#30340;&#21512;&#29702;&#21160;&#20316;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
KI-PMF: Knowledge Integrated Plausible Motion Forecasting. (arXiv:2310.12007v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KI-PMF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20808;&#39564;&#30693;&#35782;&#65292;&#23545;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#26410;&#26469;&#34892;&#21160;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#36981;&#24490;&#36710;&#36742;&#30340;&#36816;&#21160;&#32422;&#26463;&#21644;&#34892;&#39542;&#29615;&#22659;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#36890;&#36807;&#26465;&#20214;&#21270;&#32593;&#32476;&#20197;&#36981;&#24490;&#29289;&#29702;&#23450;&#24459;&#65292;&#21487;&#20197;&#33719;&#24471;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#39044;&#27979;&#65292;&#23545;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#32500;&#25252;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#34892;&#21160;&#23545;&#22823;&#35268;&#27169;&#37096;&#32626;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20248;&#21270;&#29305;&#23450;&#24230;&#37327;&#30340;&#25439;&#22833;&#20989;&#25968;&#19978;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#19981;&#31526;&#21512;&#29289;&#29702;&#23450;&#24459;&#25110;&#36829;&#21453;&#22806;&#37096;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#32467;&#21512;&#26126;&#30830;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#65292;&#31526;&#21512;&#36710;&#36742;&#30340;&#36816;&#21160;&#32422;&#26463;&#21644;&#34892;&#39542;&#29615;&#22659;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38750;&#21442;&#25968;&#21098;&#26525;&#23618;&#21644;&#27880;&#24847;&#21147;&#23618;&#26469;&#25972;&#21512;&#23450;&#20041;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#30830;&#20445;&#20132;&#36890;&#21442;&#19982;&#32773;&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#24773;&#20917;&#19979;&#30340;&#21040;&#36798;&#21487;&#36798;&#24615;&#20445;&#35777;&#12290;&#36890;&#36807;&#23558;&#32593;&#32476;&#26465;&#20214;&#21270;&#20026;&#36981;&#24490;&#29289;&#29702;&#23450;&#24459;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#39044;&#27979;&#65292;&#36825;&#23545;&#20110;&#22312;&#23454;&#38469;&#19990;&#30028;&#29615;&#22659;&#20013;&#32500;&#25252;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately forecasting the motion of traffic actors is crucial for the deployment of autonomous vehicles at a large scale. Current trajectory forecasting approaches primarily concentrate on optimizing a loss function with a specific metric, which can result in predictions that do not adhere to physical laws or violate external constraints. Our objective is to incorporate explicit knowledge priors that allow a network to forecast future trajectories in compliance with both the kinematic constraints of a vehicle and the geometry of the driving environment. To achieve this, we introduce a non-parametric pruning layer and attention layers to integrate the defined knowledge priors. Our proposed method is designed to ensure reachability guarantees for traffic actors in both complex and dynamic situations. By conditioning the network to follow physical laws, we can obtain accurate and safe predictions, essential for maintaining autonomous vehicles' safety and efficiency in real-world settings
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22122;&#22768;&#23481;&#24525;&#30340;&#26080;&#30417;&#30563;&#36866;&#37197;&#22120;(NtUA)&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#26469;&#23398;&#20064;&#20248;&#31168;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;NtUA&#36890;&#36807;&#33258;&#36866;&#24212;&#32531;&#23384;&#24418;&#25104;&#21644;&#20266;&#26631;&#31614;&#20462;&#27491;&#26469;&#23545;&#25239;&#20266;&#26631;&#31614;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2309.14928</link><description>&lt;p&gt;
&#22122;&#22768;&#23481;&#24525;&#30340;&#26080;&#30417;&#30563;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Noise-Tolerant Unsupervised Adapter for Vision-Language Models. (arXiv:2309.14928v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14928
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22122;&#22768;&#23481;&#24525;&#30340;&#26080;&#30417;&#30563;&#36866;&#37197;&#22120;(NtUA)&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#26469;&#23398;&#20064;&#20248;&#31168;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;NtUA&#36890;&#36807;&#33258;&#36866;&#24212;&#32531;&#23384;&#24418;&#25104;&#21644;&#20266;&#26631;&#31614;&#20462;&#27491;&#26469;&#23545;&#25239;&#20266;&#26631;&#31614;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#26174;&#33879;&#30340;&#34920;&#29616;&#65292;&#22312;&#21508;&#31181;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23569;&#26679;&#26412;&#26377;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#20173;&#38656;&#35201;&#30446;&#26631;&#26679;&#26412;&#30340;&#26631;&#27880;&#65292;&#36825;&#22312;&#22788;&#29702;&#21508;&#31181;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22122;&#22768;&#23481;&#24525;&#30340;&#26080;&#30417;&#30563;&#36866;&#37197;&#22120;(NtUA)&#65292;&#23427;&#20801;&#35768;&#20351;&#29992;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#26469;&#23398;&#20064;&#20248;&#31168;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;NtUA&#20316;&#20026;&#19968;&#20010;&#38190;&#20540;&#32531;&#23384;&#65292;&#23558;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#30340;&#35270;&#35273;&#29305;&#24449;&#21644;&#39044;&#27979;&#30340;&#20266;&#26631;&#31614;&#20316;&#20026;&#38190;&#20540;&#23545;&#36827;&#34892;&#24314;&#27169;&#12290;&#23427;&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#35774;&#35745;&#32452;&#25104;&#12290;&#31532;&#19968;&#20010;&#26159;&#33258;&#36866;&#24212;&#32531;&#23384;&#24418;&#25104;&#65292;&#36890;&#36807;&#26681;&#25454;&#20854;&#39044;&#27979;&#32622;&#20449;&#24230;&#23545;&#38190;&#20540;&#23545;&#36827;&#34892;&#21152;&#26435;&#65292;&#20197;&#23545;&#25239;&#20266;&#26631;&#31614;&#30340;&#22122;&#22768;&#12290;&#31532;&#20108;&#20010;&#26159;&#20266;&#26631;&#31614;&#20462;&#27491;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#38190;&#20540;&#23545;&#30340;&#26435;&#37325;&#26469;&#20462;&#27491;&#20266;&#26631;&#31614;&#20197;&#21450;&#32531;&#23384;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large-scale vision-language models have achieved very impressive performance in various zero-shot image classification tasks. While prior studies have demonstrated significant improvements by introducing few-shot labelled target samples, they still require labelling of target samples, which greatly degrades their scalability while handling various visual recognition tasks. We design NtUA, a Noise-tolerant Unsupervised Adapter that allows learning superior target models with few-shot unlabelled target samples. NtUA works as a key-value cache that formulates visual features and predicted pseudo-labels of the few-shot unlabelled target samples as key-value pairs. It consists of two complementary designs. The first is adaptive cache formation that combats pseudo-label noises by weighting the key-value pairs according to their prediction confidence. The second is pseudo-label rectification, which corrects both pair values (i.e., pseudo-labels) and cache weights by leverag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#33258;&#36866;&#24212;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#26469;&#35843;&#33410;&#22270;&#20687;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#35270;&#35273;&#29983;&#25104;&#36136;&#37327;&#21644;&#32534;&#36753;&#28789;&#27963;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#20840;&#23616;&#26679;&#24335;&#19981;&#19968;&#33268;&#21644;&#23616;&#37096;&#26679;&#24335;&#32534;&#36753;&#19981;&#30495;&#23454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.16071</link><description>&lt;p&gt;
&#22522;&#20110;&#31867;&#33258;&#36866;&#24212;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Semantic Image Synthesis via Class-Adaptive Cross-Attention. (arXiv:2308.16071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#33258;&#36866;&#24212;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#26469;&#35843;&#33410;&#22270;&#20687;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#35270;&#35273;&#29983;&#25104;&#36136;&#37327;&#21644;&#32534;&#36753;&#28789;&#27963;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#20840;&#23616;&#26679;&#24335;&#19981;&#19968;&#33268;&#21644;&#23616;&#37096;&#26679;&#24335;&#32534;&#36753;&#19981;&#30495;&#23454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#39046;&#22495;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#31354;&#38388;&#33258;&#36866;&#24212;&#24402;&#19968;&#21270;&#23618;&#65292;&#21487;&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#35270;&#35273;&#29983;&#25104;&#36136;&#37327;&#21644;&#32534;&#36753;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#24573;&#30053;&#20840;&#23616;&#22270;&#20687;&#32479;&#35745;&#20449;&#24687;&#65292;&#23548;&#33268;&#23616;&#37096;&#26679;&#24335;&#32534;&#36753;&#19981;&#30495;&#23454;&#65292;&#24182;&#24341;&#36215;&#35832;&#22914;&#33394;&#24425;&#25110;&#20809;&#29031;&#20998;&#24067;&#20559;&#31227;&#31561;&#20840;&#23616;&#19981;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#22120;&#38656;&#35201;&#35821;&#20041;&#24067;&#23616;&#26469;&#26144;&#23556;&#26679;&#24335;&#65292;&#23545;&#29305;&#24449;&#25552;&#20986;&#20102;&#20005;&#26684;&#30340;&#23545;&#40784;&#32422;&#26463;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#20195;&#26367;&#21453;&#24402;&#19968;&#21270;&#23618;&#26469;&#35843;&#33410;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32487;&#25215;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#20445;&#25345;&#20102;&#26368;&#20808;&#36827;&#30340;&#37325;&#24314;&#36136;&#37327;&#65292;&#24182;&#19988;&#25913;&#36827;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#26679;&#24335;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
In semantic image synthesis, the state of the art is dominated by methods that use spatially-adaptive normalization layers, which allow for excellent visual generation quality and editing versatility. Granted their efficacy, recent research efforts have focused toward finer-grained local style control and multi-modal generation. By construction though, such layers tend to overlook global image statistics leading to unconvincing local style editing and causing global inconsistencies such as color or illumination distribution shifts. Also, the semantic layout is required for mapping styles in the generator, putting a strict alignment constraint over the features. In response, we designed a novel architecture where cross-attention layers are used in place of de-normalization ones for conditioning the image generation. Our model inherits the advantages of both solutions, retaining state-of-the-art reconstruction quality, as well as improved global and local style transfer. Code and models 
&lt;/p&gt;</description></item></channel></rss>