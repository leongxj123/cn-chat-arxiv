<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#30740;&#31350;&#23545;&#21355;&#26143;&#22270;&#20687;&#20013;&#35782;&#21035;&#39134;&#26426;&#30340;&#20219;&#21153;&#33258;&#23450;&#20041;&#30340;&#19968;&#22871;&#20808;&#36827;&#23545;&#35937;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#21457;&#29616;YOLOv5&#26159;&#22312;&#19981;&#21516;&#25104;&#20687;&#26465;&#20214;&#19979;&#23637;&#29616;&#39640;&#31934;&#24230;&#21644;&#36866;&#24212;&#24615;&#30340;&#26368;&#20248;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.02877</link><description>&lt;p&gt;
FlightScope: &#21355;&#26143;&#22270;&#20687;&#20013;&#39134;&#34892;&#22120;&#26816;&#27979;&#31639;&#27861;&#30340;&#28145;&#24230;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FlightScope: A Deep Comprehensive Assessment of Aircraft Detection Algorithms in Satellite Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#21355;&#26143;&#22270;&#20687;&#20013;&#35782;&#21035;&#39134;&#26426;&#30340;&#20219;&#21153;&#33258;&#23450;&#20041;&#30340;&#19968;&#22871;&#20808;&#36827;&#23545;&#35937;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#21457;&#29616;YOLOv5&#26159;&#22312;&#19981;&#21516;&#25104;&#20687;&#26465;&#20214;&#19979;&#23637;&#29616;&#39640;&#31934;&#24230;&#21644;&#36866;&#24212;&#24615;&#30340;&#26368;&#20248;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02877v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22312;&#36965;&#24863;&#21355;&#26143;&#22270;&#20687;&#20013;&#36827;&#34892;&#23545;&#35937;&#26816;&#27979;&#23545;&#20110;&#35768;&#22810;&#39046;&#22495;&#65292;&#22914;&#29983;&#29289;&#29289;&#29702;&#23398;&#21644;&#29615;&#22659;&#30417;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#19981;&#26029;&#21457;&#23637;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#22312;&#24120;&#35265;&#30340;&#22522;&#20110;&#22320;&#38754;&#25293;&#25668;&#30340;&#29031;&#29255;&#19978;&#23454;&#26045;&#21644;&#27979;&#35797;&#12290;&#26412;&#25991;&#23545;&#19968;&#22871;&#38024;&#23545;&#22312;&#21355;&#26143;&#22270;&#20687;&#20013;&#35782;&#21035;&#39134;&#26426;&#36825;&#19968;&#20219;&#21153;&#23450;&#21046;&#30340;&#20808;&#36827;&#23545;&#35937;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#21033;&#29992;&#22823;&#22411;HRPlanesV2&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19982;GDIT&#25968;&#25454;&#38598;&#30340;&#20005;&#26684;&#39564;&#35777;&#65292;&#35813;&#30740;&#31350;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#21253;&#25324;YOLO&#29256;&#26412;5&#21644;8&#12289;Faster RCNN&#12289;CenterNet&#12289;RetinaNet&#12289;RTMDet&#21644;DETR&#65292;&#22343;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#12290;&#36825;&#39033;&#20840;&#38754;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;YOLOv5&#20316;&#20026;&#35782;&#21035;&#36965;&#24863;&#25968;&#25454;&#20013;&#30340;&#39134;&#26426;&#36825;&#19968;&#29305;&#23450;&#26696;&#20363;&#30340;&#21331;&#36234;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#25104;&#20687;&#26465;&#20214;&#19979;&#30340;&#39640;&#31934;&#24230;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02877v1 Announce Type: cross  Abstract: Object detection in remotely sensed satellite pictures is fundamental in many fields such as biophysical, and environmental monitoring. While deep learning algorithms are constantly evolving, they have been mostly implemented and tested on popular ground-based taken photos. This paper critically evaluates and compares a suite of advanced object detection algorithms customized for the task of identifying aircraft within satellite imagery. Using the large HRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset, this research encompasses an array of methodologies including YOLO versions 5 and 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from scratch. This exhaustive training and validation study reveal YOLOv5 as the preeminent model for the specific case of identifying airplanes from remote sensing data, showcasing high precision and adaptability across diverse imaging conditions. This research
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23398;&#20064;&#30446;&#26631;&#33539;&#24335;&#65292;&#36890;&#36807;Y-mapping&#26469;&#25918;&#26494;&#32422;&#26463;&#24182;&#35774;&#35745;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21253;&#25324;&#23398;&#20064;&#22495;&#26080;&#20851;&#30340;&#26465;&#20214;&#29305;&#24449;&#21644;&#26368;&#22823;&#21270;&#21518;&#39564;&#27010;&#29575;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#39033;&#35299;&#20915;&#25918;&#26494;&#32422;&#26463;&#24341;&#36215;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.18853</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#24102;&#26377;&#36890;&#29992;&#23398;&#20064;&#30446;&#26631;&#30340;&#22810;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Rethinking Multi-domain Generalization with A General Learning Objective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23398;&#20064;&#30446;&#26631;&#33539;&#24335;&#65292;&#36890;&#36807;Y-mapping&#26469;&#25918;&#26494;&#32422;&#26463;&#24182;&#35774;&#35745;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#21253;&#25324;&#23398;&#20064;&#22495;&#26080;&#20851;&#30340;&#26465;&#20214;&#29305;&#24449;&#21644;&#26368;&#22823;&#21270;&#21518;&#39564;&#27010;&#29575;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#39033;&#35299;&#20915;&#25918;&#26494;&#32422;&#26463;&#24341;&#36215;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39046;&#22495;&#27867;&#21270;&#65288;mDG&#65289;&#30340;&#26222;&#36941;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#22686;&#24378;&#36793;&#38469;&#21040;&#26631;&#31614;&#20998;&#24067;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;mDG&#25991;&#29486;&#32570;&#20047;&#19968;&#20010;&#36890;&#29992;&#30340;&#23398;&#20064;&#30446;&#26631;&#33539;&#24335;&#65292;&#36890;&#24120;&#23545;&#38745;&#24577;&#30446;&#26631;&#36793;&#38469;&#20998;&#24067;&#26045;&#21152;&#32422;&#26463;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#35758;&#21033;&#29992;&#19968;&#20010;$Y$-mapping&#26469;&#25918;&#26494;&#32422;&#26463;&#12290;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20102;mDG&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#36890;&#29992;&#23398;&#20064;&#30446;&#26631;&#26469;&#35299;&#37322;&#21644;&#20998;&#26512;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;mDG&#26234;&#24935;&#12290;&#36825;&#20010;&#36890;&#29992;&#30446;&#26631;&#20998;&#20026;&#20004;&#20010;&#21327;&#21516;&#30340;&#30446;&#26631;&#65306;&#23398;&#20064;&#19982;&#22495;&#26080;&#20851;&#30340;&#26465;&#20214;&#29305;&#24449;&#21644;&#26368;&#22823;&#21270;&#19968;&#20010;&#21518;&#39564;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#20010;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#36825;&#20123;&#39033;&#32467;&#21512;&#20102;&#20808;&#39564;&#20449;&#24687;&#24182;&#25233;&#21046;&#20102;&#26080;&#25928;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20943;&#36731;&#20102;&#25918;&#26494;&#32422;&#26463;&#25152;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20026;&#22495;&#23545;&#40784;&#25552;&#20379;&#20102;&#19968;&#20010;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18853v1 Announce Type: cross  Abstract: Multi-domain generalization (mDG) is universally aimed to minimize the discrepancy between training and testing distributions to enhance marginal-to-label distribution mapping. However, existing mDG literature lacks a general learning objective paradigm and often imposes constraints on static target marginal distributions. In this paper, we propose to leverage a $Y$-mapping to relax the constraint. We rethink the learning objective for mDG and design a new \textbf{general learning objective} to interpret and analyze most existing mDG wisdom. This general objective is bifurcated into two synergistic amis: learning domain-independent conditional features and maximizing a posterior. Explorations also extend to two effective regularization terms that incorporate prior information and suppress invalid causality, alleviating the issues that come with relaxed constraints. We theoretically contribute an upper bound for the domain alignment of 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#36861;&#36394;&#21644;&#27169;&#25311;&#29289;&#20307;&#24863;&#30693;&#20197;&#21450;&#20854;&#22312;&#20132;&#27969;&#20013;&#25152;&#20256;&#36798;&#30340;&#34920;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#24847;&#35782;&#12290;&#30456;&#27604;&#20110;&#22823;&#22810;&#25968;&#26080;&#27861;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#26032;&#32593;&#32476;&#26469;&#23450;&#20041;&#29289;&#20307;&#24863;&#30693;&#12290;</title><link>https://arxiv.org/abs/2310.05212</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#32593;&#32476;&#20195;&#34920;&#24847;&#35782;&#30340;&#30693;&#35273;
&lt;/p&gt;
&lt;p&gt;
Interpretable Semiotics Networks Representing Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05212
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#36861;&#36394;&#21644;&#27169;&#25311;&#29289;&#20307;&#24863;&#30693;&#20197;&#21450;&#20854;&#22312;&#20132;&#27969;&#20013;&#25152;&#20256;&#36798;&#30340;&#34920;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#24847;&#35782;&#12290;&#30456;&#27604;&#20110;&#22823;&#22810;&#25968;&#26080;&#27861;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#26032;&#32593;&#32476;&#26469;&#23450;&#20041;&#29289;&#20307;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27599;&#22825;&#37117;&#24863;&#30693;&#29289;&#20307;&#65292;&#24182;&#36890;&#36807;&#21508;&#31181;&#28192;&#36947;&#20256;&#36798;&#20182;&#20204;&#30340;&#24863;&#30693;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36861;&#36394;&#21644;&#27169;&#25311;&#29289;&#20307;&#30340;&#24863;&#30693;&#20197;&#21450;&#23427;&#20204;&#22312;&#20132;&#27969;&#20013;&#25152;&#20256;&#36798;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#20869;&#37096;&#34920;&#31034;&#30340;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65288;"&#35266;&#23519;&#21040;&#30340;"&#21644;"&#30475;&#21040;&#30340;"&#65289;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#29087;&#24713;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27010;&#24565;&#65288;&#32534;&#30721;&#21644;&#35299;&#30721;&#65289;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#20803;&#32032;&#34987;&#21512;&#24182;&#22312;&#19968;&#36215;&#24418;&#25104;&#31526;&#21495;&#32593;&#32476;&#65292;&#27169;&#25311;&#20102;&#29289;&#20307;&#24863;&#30693;&#21644;&#20154;&#31867;&#20132;&#27969;&#20013;&#30340;&#24847;&#35782;&#12290;&#22914;&#20170;&#65292;&#22823;&#22810;&#25968;&#31070;&#32463;&#32593;&#32476;&#37117;&#26159;&#19981;&#21487;&#35299;&#37322;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#21487;&#35265;&#24615;&#12290;&#25105;&#20204;&#20154;&#30340;&#29289;&#20307;&#24863;&#30693;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#32593;&#32476;&#23450;&#20041;&#29289;&#20307;&#24863;&#30693;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#25324;&#22522;&#20934;&#20998;&#31867;&#22120;&#21644;&#39069;&#22806;&#23618;&#30340;&#26032;&#32593;&#32476;&#26469;&#28436;&#31034;&#36825;&#19968;&#28857;&#12290;&#36825;&#20010;&#23618;&#20135;&#29983;&#20102;&#22270;&#20687;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans perceive objects daily and communicate their perceptions using various channels. Here, we describe a computational model that tracks and simulates objects' perception and their representations as they are conveyed in communication.   We describe two key components of our internal representation ("observed" and "seen") and relate them to familiar computer vision notions (encoding and decoding). These elements are joined together to form semiotics networks, which simulate awareness in object perception and human communication.   Nowadays, most neural networks are uninterpretable. On the other hand, our model overcomes this limitation. The experiments demonstrates the visibility of the model.   Our model of object perception by a person allows us to define object perception by a network. We demonstrate this with an example of an image baseline classifier by constructing a new network that includes the baseline classifier and an additional layer. This layer produces the images "perc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38899;&#39057;&#22330;&#26223;&#35821;&#20041;&#36827;&#34892;&#33258;&#21160;&#22270;&#20687;&#19978;&#33394;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38899;&#39057;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#65292;&#38477;&#20302;&#20102;&#23545;&#22330;&#26223;&#35821;&#20041;&#29702;&#35299;&#30340;&#38590;&#24230;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#32593;&#32476;&#36827;&#34892;&#20102;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.13270</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#38899;&#39057;&#22330;&#26223;&#35821;&#20041;&#23454;&#29616;&#33258;&#21160;&#22270;&#20687;&#19978;&#33394;
&lt;/p&gt;
&lt;p&gt;
Audio-Infused Automatic Image Colorization by Exploiting Audio Scene Semantics. (arXiv:2401.13270v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38899;&#39057;&#22330;&#26223;&#35821;&#20041;&#36827;&#34892;&#33258;&#21160;&#22270;&#20687;&#19978;&#33394;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38899;&#39057;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#65292;&#38477;&#20302;&#20102;&#23545;&#22330;&#26223;&#35821;&#20041;&#29702;&#35299;&#30340;&#38590;&#24230;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#32593;&#32476;&#36827;&#34892;&#20102;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#22270;&#20687;&#19978;&#33394;&#26159;&#19968;&#20010;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#23545;&#22330;&#26223;&#36827;&#34892;&#20934;&#30830;&#30340;&#35821;&#20041;&#29702;&#35299;&#65292;&#20197;&#20272;&#35745;&#28784;&#24230;&#22270;&#20687;&#30340;&#21512;&#29702;&#39068;&#33394;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#22522;&#20110;&#20132;&#20114;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#33258;&#21160;&#19978;&#33394;&#26469;&#35828;&#65292;&#25512;&#26029;&#20986;&#36924;&#30495;&#21644;&#20934;&#30830;&#30340;&#39068;&#33394;&#20173;&#28982;&#26159;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#38477;&#20302;&#23545;&#28784;&#24230;&#22330;&#26223;&#30340;&#35821;&#20041;&#29702;&#35299;&#38590;&#24230;&#65292;&#26412;&#25991;&#23581;&#35797;&#21033;&#29992;&#30456;&#24212;&#30340;&#38899;&#39057;&#65292;&#38899;&#39057;&#33258;&#28982;&#22320;&#21253;&#21547;&#20102;&#20851;&#20110;&#21516;&#19968;&#22330;&#26223;&#30340;&#39069;&#22806;&#35821;&#20041;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#39057;&#27880;&#20837;&#33258;&#21160;&#22270;&#20687;&#19978;&#33394;&#65288;AIAIC&#65289;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#24425;&#33394;&#22270;&#20687;&#30340;&#35821;&#20041;&#20316;&#20026;&#26725;&#26753;&#65292;&#36890;&#36807;&#24425;&#33394;&#22270;&#20687;&#30340;&#35821;&#20041;&#24341;&#23548;&#39044;&#35757;&#32451;&#19978;&#33394;&#32593;&#32476;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#38899;&#39057;&#19982;&#35270;&#39057;&#30340;&#33258;&#28982;&#20849;&#29616;&#26469;&#23398;&#20064;&#38899;&#39057;&#21644;&#35270;&#35273;&#22330;&#26223;&#20043;&#38388;&#30340;&#39068;&#33394;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#31532;&#19977;&#65292;&#38544;&#24335;&#38899;&#39057;&#35821;&#20041;&#34920;&#31034;&#34987;&#21033;&#29992;&#20197;&#24341;&#23548;&#22270;&#20687;&#19978;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic image colorization is inherently an ill-posed problem with uncertainty, which requires an accurate semantic understanding of scenes to estimate reasonable colors for grayscale images. Although recent interaction-based methods have achieved impressive performance, it is still a very difficult task to infer realistic and accurate colors for automatic colorization. To reduce the difficulty of semantic understanding of grayscale scenes, this paper tries to utilize corresponding audio, which naturally contains extra semantic information about the same scene. Specifically, a novel audio-infused automatic image colorization (AIAIC) network is proposed, which consists of three stages. First, we take color image semantics as a bridge and pretrain a colorization network guided by color image semantics. Second, the natural co-occurrence of audio and video is utilized to learn the color semantic correlations between audio and visual scenes. Third, the implicit audio semantic representati
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#23454;&#20363;&#20998;&#21106;&#30340;&#32958;&#33039;&#27963;&#26816;&#32467;&#26500;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#32479;&#35745;&#35299;&#21078;&#32467;&#26500;&#19978;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#20174;&#32780;&#20943;&#23569;&#24037;&#20316;&#37327;&#21644;&#35266;&#23519;&#32773;&#38388;&#21464;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17166</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#38598;&#23454;&#20363;&#20998;&#21106;&#22312;&#32958;&#33039;&#27963;&#26816;&#32467;&#26500;&#35780;&#20272;&#26041;&#38754;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Advances in Kidney Biopsy Structural Assessment through Dense Instance Segmentation. (arXiv:2309.17166v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17166
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#23454;&#20363;&#20998;&#21106;&#30340;&#32958;&#33039;&#27963;&#26816;&#32467;&#26500;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#32479;&#35745;&#35299;&#21078;&#32467;&#26500;&#19978;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#20174;&#32780;&#20943;&#23569;&#24037;&#20316;&#37327;&#21644;&#35266;&#23519;&#32773;&#38388;&#21464;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32958;&#33039;&#27963;&#26816;&#26159;&#32958;&#33039;&#30142;&#30149;&#35786;&#26029;&#30340;&#37329;&#26631;&#20934;&#12290;&#19987;&#23478;&#32958;&#33039;&#30149;&#29702;&#23398;&#23478;&#21046;&#23450;&#30340;&#30149;&#21464;&#35780;&#20998;&#26159;&#21322;&#23450;&#37327;&#30340;&#65292;&#24182;&#19988;&#23384;&#22312;&#39640;&#30340;&#35266;&#23519;&#32773;&#38388;&#21464;&#24322;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#23545;&#20998;&#21106;&#30340;&#35299;&#21078;&#23545;&#35937;&#36827;&#34892;&#33258;&#21160;&#32479;&#35745;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24037;&#20316;&#37327;&#21644;&#36825;&#31181;&#35266;&#23519;&#32773;&#38388;&#21464;&#24322;&#24615;&#12290;&#28982;&#32780;&#65292;&#27963;&#26816;&#30340;&#23454;&#20363;&#20998;&#21106;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#21407;&#22240;&#26377;&#65306;&#65288;a&#65289;&#24179;&#22343;&#25968;&#37327;&#36739;&#22823;&#65288;&#32422;300&#33267;1000&#20010;&#65289;&#23494;&#38598;&#25509;&#35302;&#30340;&#35299;&#21078;&#32467;&#26500;&#65292;&#65288;b&#65289;&#20855;&#26377;&#22810;&#20010;&#31867;&#21035;&#65288;&#33267;&#23569;3&#20010;&#65289;&#65292;&#65288;c&#65289;&#23610;&#23544;&#21644;&#24418;&#29366;&#21508;&#24322;&#12290;&#30446;&#21069;&#20351;&#29992;&#30340;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#19981;&#33021;&#20197;&#39640;&#25928;&#36890;&#29992;&#30340;&#26041;&#24335;&#21516;&#26102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#19981;&#38656;&#35201;&#38170;&#28857;&#30340;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#25193;&#25955;&#27169;&#22411;&#12289;&#21464;&#25442;&#22120;&#27169;&#22359;&#21644;RCNN&#65288;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19968;&#21488;NVIDIA GeForce RTX 3090 GPU&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#21487;&#20197;&#25552;&#20379;&#21487;&#35266;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The kidney biopsy is the gold standard for the diagnosis of kidney diseases. Lesion scores made by expert renal pathologists are semi-quantitative and suffer from high inter-observer variability. Automatically obtaining statistics per segmented anatomical object, therefore, can bring significant benefits in reducing labor and this inter-observer variability. Instance segmentation for a biopsy, however, has been a challenging problem due to (a) the on average large number (around 300 to 1000) of densely touching anatomical structures, (b) with multiple classes (at least 3) and (c) in different sizes and shapes. The currently used instance segmentation models cannot simultaneously deal with these challenges in an efficient yet generic manner. In this paper, we propose the first anchor-free instance segmentation model that combines diffusion models, transformer modules, and RCNNs (regional convolution neural networks). Our model is trained on just one NVIDIA GeForce RTX 3090 GPU, but can 
&lt;/p&gt;</description></item><item><title>Awesome-META+&#26159;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#38598;&#25104;&#21644;&#23398;&#20064;&#24179;&#21488;&#65292;&#26088;&#22312;&#25552;&#20379;&#23436;&#25972;&#21487;&#38752;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#24212;&#29992;&#21644;&#38754;&#21521;&#21021;&#23398;&#32773;&#30340;&#23398;&#20064;&#26448;&#26009;&#65292;&#36827;&#32780;&#20419;&#36827;&#20803;&#23398;&#20064;&#30340;&#21457;&#23637;&#24182;&#23558;&#20854;&#20174;&#23567;&#20247;&#39046;&#22495;&#36716;&#21270;&#20026;&#20027;&#27969;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.12921</link><description>&lt;p&gt;
Awesome-META+: &#20803;&#23398;&#20064;&#30740;&#31350;&#19982;&#23398;&#20064;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Awesome-META+: Meta-Learning Research and Learning Platform. (arXiv:2304.12921v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12921
&lt;/p&gt;
&lt;p&gt;
Awesome-META+&#26159;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#38598;&#25104;&#21644;&#23398;&#20064;&#24179;&#21488;&#65292;&#26088;&#22312;&#25552;&#20379;&#23436;&#25972;&#21487;&#38752;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#24212;&#29992;&#21644;&#38754;&#21521;&#21021;&#23398;&#32773;&#30340;&#23398;&#20064;&#26448;&#26009;&#65292;&#36827;&#32780;&#20419;&#36827;&#20803;&#23398;&#20064;&#30340;&#21457;&#23637;&#24182;&#23558;&#20854;&#20174;&#23567;&#20247;&#39046;&#22495;&#36716;&#21270;&#20026;&#20027;&#27969;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#22312;&#32463;&#27982;&#12289;&#20135;&#19994;&#12289;&#25945;&#32946;&#31561;&#21508;&#20010;&#39046;&#22495;&#20135;&#29983;&#20102;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#20294;&#36824;&#23384;&#22312;&#35832;&#22810;&#38480;&#21046;&#12290;&#20803;&#23398;&#20064;&#65292;&#20063;&#31216;&#20026;&#8220;&#23398;&#20064;&#22914;&#20309;&#23398;&#20064;&#8221;&#65292;&#20026;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#31361;&#30772;&#30446;&#21069;&#29942;&#39048;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#20803;&#23398;&#20064;&#36215;&#27493;&#36739;&#26202;&#65292;&#30456;&#27604;CV&#12289;NLP&#31561;&#39046;&#22495;&#65292;&#39033;&#30446;&#25968;&#37327;&#36739;&#23569;&#12290;&#27599;&#27425;&#37096;&#32626;&#37117;&#38656;&#35201;&#22823;&#37327;&#30340;&#32463;&#39564;&#21435;&#37197;&#32622;&#29615;&#22659;&#12289;&#35843;&#35797;&#20195;&#30721;&#29978;&#33267;&#37325;&#20889;&#65292;&#32780;&#19988;&#26694;&#26550;&#20043;&#38388;&#30456;&#23545;&#23396;&#31435;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#38024;&#23545;&#20803;&#23398;&#20064;&#30340;&#19987;&#38376;&#24179;&#21488;&#21644;&#38754;&#21521;&#21021;&#23398;&#32773;&#30340;&#23398;&#20064;&#26448;&#26009;&#30456;&#23545;&#36739;&#23569;&#65292;&#38376;&#27099;&#30456;&#23545;&#36739;&#39640;&#12290;&#22522;&#20110;&#27492;&#65292;Awesome-META+&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#38598;&#25104;&#21644;&#23398;&#20064;&#24179;&#21488;&#65292;&#26088;&#22312;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#24182;&#25552;&#20379;&#23436;&#25972;&#21487;&#38752;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#24212;&#29992;&#21644;&#23398;&#20064;&#24179;&#21488;&#12290;&#35813;&#39033;&#30446;&#26088;&#22312;&#20419;&#36827;&#20803;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#24182;&#23558;&#20854;&#20174;&#19968;&#20010;&#23567;&#20247;&#39046;&#22495;&#36716;&#21270;&#20026;&#19968;&#20010;&#20027;&#27969;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence technology has already had a profound impact in various fields such as economy, industry, and education, but still limited. Meta-learning, also known as "learning to learn", provides an opportunity for general artificial intelligence, which can break through the current AI bottleneck. However, meta learning started late and there are fewer projects compare with CV, NLP etc. Each deployment requires a lot of experience to configure the environment, debug code or even rewrite, and the frameworks are isolated. Moreover, there are currently few platforms that focus exclusively on meta-learning, or provide learning materials for novices, for which the threshold is relatively high. Based on this, Awesome-META+, a meta-learning framework integration and learning platform is proposed to solve the above problems and provide a complete and reliable meta-learning framework application and learning platform. The project aims to promote the development of meta-learning and t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#35775;&#38382;&#30446;&#26631;DNN&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;3D&#28857;&#20113;&#20013;&#30340;&#23545;&#25239;&#28857;&#65292;&#25552;&#20379;&#20102;&#26080;&#30418;&#23376;&#25915;&#20987;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2210.14164</link><description>&lt;p&gt;
3D&#28857;&#20113;&#20998;&#31867;&#30340;&#26080;&#30418;&#23376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
No-Box Attacks on 3D Point Cloud Classification. (arXiv:2210.14164v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#35775;&#38382;&#30446;&#26631;DNN&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;3D&#28857;&#20113;&#20013;&#30340;&#23545;&#25239;&#28857;&#65292;&#25552;&#20379;&#20102;&#26080;&#30418;&#23376;&#25915;&#20987;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21508;&#31181;&#36755;&#20837;&#20449;&#21495;&#30340;&#20998;&#26512;&#65292;&#23545;&#25239;&#25915;&#20987;&#26500;&#25104;&#20102;&#20005;&#37325;&#25361;&#25112;&#12290;&#22312;3D&#28857;&#20113;&#30340;&#24773;&#20917;&#19979;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#35782;&#21035;&#22312;&#32593;&#32476;&#20915;&#31574;&#20013;&#36215;&#20851;&#38190;&#20316;&#29992;&#30340;&#28857;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#22312;&#29983;&#25104;&#29616;&#26377;&#30340;&#23545;&#25239;&#25915;&#20987;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#26174;&#33879;&#24615;&#22270;&#26041;&#27861;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#23545;&#25239;&#25915;&#20987;&#20250;&#26174;&#33879;&#24433;&#21709;&#32593;&#32476;&#20915;&#31574;&#30340;&#28857;&#12290;&#36890;&#24120;&#65292;&#35782;&#21035;&#23545;&#25239;&#28857;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#30446;&#26631;DNN&#27169;&#22411;&#30340;&#35775;&#38382;&#65292;&#20197;&#30830;&#23450;&#21738;&#20123;&#28857;&#23545;&#27169;&#22411;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#22312;&#19981;&#35775;&#38382;&#30446;&#26631;DNN&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#23545;&#25239;&#28857;&#65292;&#36825;&#34987;&#31216;&#20026;&#8220;&#26080;&#30418;&#23376;&#8221;&#25915;&#20987;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;14&#20010;&#28857;&#20113;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#26469;&#26816;&#26597;&#36825;&#20123;&#29305;&#24449;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#23545;&#25239;&#28857;&#65292;&#20197;&#21450;&#21738;&#20123;&#29305;&#24449;&#23545;&#39044;&#27979;&#26368;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks pose serious challenges for deep neural network (DNN)-based analysis of various input signals. In the case of 3D point clouds, methods have been developed to identify points that play a key role in network decision, and these become crucial in generating existing adversarial attacks. For example, a saliency map approach is a popular method for identifying adversarial drop points, whose removal would significantly impact the network decision. Generally, methods for identifying adversarial points rely on the access to the DNN model itself to determine which points are critically important for the model's decision. This paper aims to provide a novel viewpoint on this problem, where adversarial points can be predicted without access to the target DNN model, which is referred to as a ``no-box'' attack. To this end, we define 14 point cloud features and use multiple linear regression to examine whether these features can be used for adversarial point prediction, and which
&lt;/p&gt;</description></item></channel></rss>