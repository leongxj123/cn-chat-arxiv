<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>DeNetDM &#26159;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#19987;&#23478;&#20056;&#31215;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#22312;&#21019;&#24314;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#21518;&#65292;&#23558;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#21435;&#20559;&#35265;&#27169;&#22411;&#65292;&#30456;&#27604;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21462;&#24471;&#26356;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.19863</link><description>&lt;p&gt;
DeNetDM: &#36890;&#36807;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#26469;&#28040;&#38500;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
DeNetDM: Debiasing by Network Depth Modulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19863
&lt;/p&gt;
&lt;p&gt;
DeNetDM &#26159;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#19987;&#23478;&#20056;&#31215;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#22312;&#21019;&#24314;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#21518;&#65292;&#23558;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#21435;&#20559;&#35265;&#27169;&#22411;&#65292;&#30456;&#27604;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21462;&#24471;&#26356;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31070;&#32463;&#32593;&#32476;&#22312;&#20559;&#35265;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#26080;&#24847;&#38388;&#23398;&#20064;&#21040;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#23454;&#29616;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#30446;&#21069;&#35299;&#20915;&#36825;&#31181;&#20559;&#35265;&#30340;&#26041;&#27861;&#36890;&#24120;&#21253;&#25324;&#21033;&#29992;&#20559;&#35265;&#27880;&#37322;&#12289;&#26681;&#25454;&#20266;&#20559;&#35265;&#26631;&#31614;&#36827;&#34892;&#21152;&#26435;&#37325;&#12289;&#25110;&#36890;&#36807;&#22686;&#24378;&#25216;&#26415;&#22686;&#21152;&#20559;&#35265;&#20914;&#31361;&#25968;&#25454;&#28857;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DeNetDM&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35266;&#23519;&#32467;&#26524;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#20248;&#20808;&#23398;&#20064;&#26680;&#24515;&#23646;&#24615;&#65292;&#32780;&#26356;&#28145;&#23618;&#27425;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#33719;&#21462;&#19981;&#21516;&#20449;&#24687;&#26102;&#24378;&#35843;&#20559;&#35265;&#12290;&#25105;&#20204;&#21033;&#29992;&#20174;&#19987;&#23478;&#20056;&#31215;&#20013;&#25512;&#23548;&#20986;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#21019;&#24314;&#20102;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#65292;&#28982;&#21518;&#29992;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#30340;&#21435;&#20559;&#35265;&#27169;&#22411;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19863v1 Announce Type: new  Abstract: When neural networks are trained on biased datasets, they tend to inadvertently learn spurious correlations, leading to challenges in achieving strong generalization and robustness. Current approaches to address such biases typically involve utilizing bias annotations, reweighting based on pseudo-bias labels, or enhancing diversity within bias-conflicting data points through augmentation techniques. We introduce DeNetDM, a novel debiasing method based on the observation that shallow neural networks prioritize learning core attributes, while deeper ones emphasize biases when tasked with acquiring distinct information. Using a training paradigm derived from Product of Experts, we create both biased and debiased branches with deep and shallow architectures and then distill knowledge to produce the target debiased model. Extensive experiments and analyses demonstrate that our approach outperforms current debiasing techniques, achieving a not
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#36125;&#21494;&#26031;&#26410;&#26469;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#36234;&#37326;&#22320;&#22270;&#30340;&#21046;&#20316;&#65292;&#20026;&#38271;&#31243;&#39044;&#27979;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2403.11876</link><description>&lt;p&gt;
&#28145;&#24230;&#36125;&#21494;&#26031;&#26410;&#26469;&#34701;&#21512;&#29992;&#20110;&#33258;&#30417;&#30563;&#12289;&#39640;&#20998;&#36776;&#29575;&#12289;&#36234;&#37326;&#22320;&#22270;&#21046;&#20316;
&lt;/p&gt;
&lt;p&gt;
Deep Bayesian Future Fusion for Self-Supervised, High-Resolution, Off-Road Mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#36125;&#21494;&#26031;&#26410;&#26469;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#36234;&#37326;&#22320;&#22270;&#30340;&#21046;&#20316;&#65292;&#20026;&#38271;&#31243;&#39044;&#27979;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#28304;&#21463;&#38480;&#30340;&#36234;&#37326;&#36710;&#36742;&#30340;&#20256;&#24863;&#22120;&#20998;&#36776;&#29575;&#26377;&#38480;&#65292;&#36825;&#32473;&#21487;&#38752;&#30340;&#36234;&#37326;&#33258;&#20027;&#24615;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#34701;&#21512;&#26410;&#26469;&#20449;&#24687;&#65288;&#21363;&#26410;&#26469;&#34701;&#21512;&#65289;&#36827;&#34892;&#33258;&#30417;&#30563;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#21033;&#29992;&#26410;&#26469;&#20449;&#24687;&#20197;&#21450;&#25163;&#24037;&#21046;&#20316;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#30452;&#25509;&#30417;&#30563;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#21487;&#31359;&#36234;&#24615;&#20272;&#35745;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#20010;&#26356;&#20026;&#36890;&#29992;&#30340;&#21457;&#23637;&#26041;&#21521; - &#36890;&#36807;&#26410;&#26469;&#34701;&#21512;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#26102;&#38388;&#39640;&#25928;&#22320;&#23436;&#25104;&#26368;&#39640;&#20998;&#36776;&#29575;&#65288;&#21363;&#27599;&#20687;&#32032;2&#21400;&#31859;&#65289;BEV&#22320;&#22270;&#65292;&#21487;&#29992;&#20110;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#38271;&#31243;&#39044;&#27979;&#12290;&#20026;&#27492;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#26410;&#26469;&#34701;&#21512;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#65288;RGB / &#39640;&#24230;&#65289;&#21407;&#22987;&#31232;&#30095;&#22122;&#38899;&#36755;&#20837;&#21644;&#22522;&#20110;&#22320;&#22270;&#30340;&#23494;&#38598;&#26631;&#31614;&#30340;&#25104;&#23545;&#25968;&#25454;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#36866;&#24212;&#20256;&#24863;&#22120;&#30340;&#22122;&#22768;&#21644;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11876v1 Announce Type: cross  Abstract: The limited sensing resolution of resource-constrained off-road vehicles poses significant challenges towards reliable off-road autonomy. To overcome this limitation, we propose a general framework based on fusing the future information (i.e. future fusion) for self-supervision. Recent approaches exploit this future information alongside the hand-crafted heuristics to directly supervise the targeted downstream tasks (e.g. traversability estimation). However, in this paper, we opt for a more general line of development - time-efficient completion of the highest resolution (i.e. 2cm per pixel) BEV map in a self-supervised manner via future fusion, which can be used for any downstream tasks for better longer range prediction. To this end, first, we create a high-resolution future-fusion dataset containing pairs of (RGB / height) raw sparse and noisy inputs and map-based dense labels. Next, to accommodate the noise and sparsity of the sens
&lt;/p&gt;</description></item><item><title>I3SB&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#32467;&#21512;&#25439;&#22351;&#30340;&#22270;&#20687;&#25913;&#21892;&#32441;&#29702;&#24674;&#22797;&#65292;&#22312;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.06069</link><description>&lt;p&gt;
&#38544;&#24335;&#22270;&#20687;&#23545;&#22270;&#20687;Schrodinger&#26725;&#29992;&#20110;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Implicit Image-to-Image Schrodinger Bridge for CT Super-Resolution and Denoising
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06069
&lt;/p&gt;
&lt;p&gt;
I3SB&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#32467;&#21512;&#25439;&#22351;&#30340;&#22270;&#20687;&#25913;&#21892;&#32441;&#29702;&#24674;&#22797;&#65292;&#22312;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#22312;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#32780;&#24471;&#21040;&#35748;&#21487;&#65292;&#28982;&#32780;&#65292;&#20854;&#20174;&#39640;&#26031;&#22122;&#22768;&#24320;&#22987;&#30340;&#36845;&#20195;&#21435;&#22122;&#36807;&#31243;&#24448;&#24448;&#23548;&#33268;&#25512;&#26029;&#36895;&#24230;&#24930;&#12290;&#20316;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#22270;&#20687;&#23545;&#22270;&#20687;Schr&#246;dinger&#26725;&#65288;I2SB&#65289;&#20174;&#25439;&#22351;&#30340;&#22270;&#20687;&#24320;&#22987;&#21021;&#22987;&#21270;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#38598;&#25104;&#20102;&#26377;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25216;&#26415;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#38544;&#24335;&#22270;&#20687;&#23545;&#22270;&#20687;Schr&#246;dinger&#26725;&#65288;I3SB&#65289;&#25193;&#23637;&#20102;I2SB&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#19968;&#29983;&#25104;&#27493;&#39588;&#20013;&#32435;&#20837;&#25439;&#22351;&#30340;&#22270;&#20687;&#65292;&#23558;&#20854;&#29983;&#25104;&#36807;&#31243;&#36716;&#25442;&#20026;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#12290;&#36825;&#31181;&#22686;&#24378;&#20351;&#24471;I3SB&#33021;&#22815;&#22312;&#23569;&#37327;&#29983;&#25104;&#27493;&#39588;&#20013;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#32441;&#29702;&#24674;&#22797;&#30340;&#22270;&#20687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;&#20219;&#21153;&#19978;&#24471;&#21040;&#39564;&#35777;&#65292;&#24182;&#36229;&#36234;&#20102;&#21253;&#25324;&#26377;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#20869;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06069v1 Announce Type: cross  Abstract: Conditional diffusion models have gained recognition for their effectiveness in image restoration tasks, yet their iterative denoising process, starting from Gaussian noise, often leads to slow inference speeds. As a promising alternative, the Image-to-Image Schr\"odinger Bridge (I2SB) initializes the generative process from corrupted images and integrates training techniques from conditional diffusion models. In this study, we extended the I2SB method by introducing the Implicit Image-to-Image Schrodinger Bridge (I3SB), transitioning its generative process to a non-Markovian process by incorporating corrupted images in each generative step. This enhancement empowers I3SB to generate images with better texture restoration using a small number of generative steps. The proposed method was validated on CT super-resolution and denoising tasks and outperformed existing methods, including the conditional denoising diffusion probabilistic mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36328;&#39046;&#22495;&#25968;&#25454;&#24046;&#24322;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03094</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#36890;&#36807;&#22686;&#24378;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36328;&#39046;&#22495;&#25968;&#25454;&#24046;&#24322;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#65288;CD-FSOD&#65289;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#20934;&#30830;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#29992;&#26368;&#23569;&#30340;&#26631;&#35760;&#26679;&#26412;&#26469;&#26816;&#27979;&#26032;&#39046;&#22495;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#24320;&#38598;&#26816;&#27979;&#22120;&#65288;&#20363;&#22914;DE-ViT&#65289;&#22312;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#21644;&#20256;&#32479;&#30340;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#31867;&#21035;&#65292;&#25105;&#20204;&#33258;&#28982;&#20250;&#25552;&#20986;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;1&#65289;&#36825;&#31181;&#24320;&#38598;&#26816;&#27979;&#26041;&#27861;&#33021;&#21542;&#23481;&#26131;&#22320;&#25512;&#24191;&#21040;CD-FSOD&#65311;2&#65289;&#22914;&#26524;&#19981;&#33021;&#65292;&#22914;&#20309;&#22312;&#38754;&#23545;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#24322;&#26102;&#22686;&#24378;&#24320;&#38598;&#26041;&#27861;&#30340;&#32467;&#26524;&#65311;&#20026;&#20102;&#22238;&#31572;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20010;&#34913;&#37327;&#39046;&#22495;&#24046;&#24322;&#30340;&#25351;&#26631;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;&#22810;&#26679;&#39046;&#22495;&#24230;&#37327;&#20540;&#30340;&#26032;&#30340;CD-FSOD&#22522;&#20934;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#22495;&#22806;&#25968;&#25454;&#38598;&#20013;&#35266;&#23519;&#21040;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#34920;&#26126;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;CD-FSOD&#19978;&#22833;&#36133;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenge of cross-domain few-shot object detection (CD-FSOD), aiming to develop an accurate object detector for novel domains with minimal labeled examples. While transformer-based open-set detectors e.g., DE-ViT~\cite{zhang2023detect} have excelled in both open-vocabulary object detection and traditional few-shot object detection, detecting categories beyond those seen during training, we thus naturally raise two key questions: 1) can such open-set detection methods easily generalize to CD-FSOD? 2) If no, how to enhance the results of open-set methods when faced with significant domain gaps? To address the first question, we introduce several metrics to quantify domain variances and establish a new CD-FSOD benchmark with diverse domain metric values. Some State-Of-The-Art (SOTA) open-set object detection methods are evaluated on this benchmark, with evident performance degradation observed across out-of-domain datasets. This indicates the failure of adopting 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#35266;&#23519;&#30340;Approximate MEan-Direction Solver&#65288;AMED-Solver&#65289;&#65292;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#22343;&#26041;&#21521;&#26469;&#28040;&#38500;&#25130;&#26029;&#35823;&#24046;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#25193;&#25955;&#25277;&#26679;&#12290;</title><link>https://arxiv.org/abs/2312.00094</link><description>&lt;p&gt;
&#22312;&#22823;&#32422;5&#20010;&#27493;&#39588;&#20013;&#65292;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#24555;&#36895;&#22522;&#20110;ODE&#30340;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Fast ODE-based Sampling for Diffusion Models in Around 5 Steps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00094
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#35266;&#23519;&#30340;Approximate MEan-Direction Solver&#65288;AMED-Solver&#65289;&#65292;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#22343;&#26041;&#21521;&#26469;&#28040;&#38500;&#25130;&#26029;&#35823;&#24046;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#25193;&#25955;&#25277;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#36827;&#34892;&#25277;&#26679;&#21487;&#20197;&#34987;&#35270;&#20026;&#35299;&#20915;&#30456;&#24212;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#65292;&#26088;&#22312;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#65288;NFE&#65289;&#33719;&#24471;&#20934;&#30830;&#35299;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#21033;&#29992;&#39640;&#38454;ODE&#27714;&#35299;&#22120;&#30340;&#21508;&#31181;&#24555;&#36895;&#25277;&#26679;&#22120;&#65292;&#24182;&#19988;&#27604;&#26368;&#21021;&#30340;&#19968;&#38454;&#27714;&#35299;&#22120;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#20540;&#26041;&#27861;&#22266;&#26377;&#22320;&#23548;&#33268;&#26576;&#20123;&#36817;&#20284;&#35823;&#24046;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#20855;&#26377;&#26497;&#23567;NFE&#65288;&#20363;&#22914;&#65292;&#32422;&#20026;5&#65289;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#20960;&#20309;&#35266;&#23519;&#65292;&#27599;&#20010;&#25277;&#26679;&#36712;&#36857;&#20960;&#20046;&#20301;&#20110;&#23884;&#20837;&#22312;&#29615;&#22659;&#31354;&#38388;&#20013;&#30340;&#20108;&#32500;&#23376;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#24555;&#36895;&#25193;&#25955;&#25277;&#26679;&#30340;AME&#36817;&#20284;&#22343;&#26041;&#21521;&#27714;&#35299;&#22120;&#65288;AMED-Solver&#65289;&#65292;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#22343;&#26041;&#21521;&#26469;&#28040;&#38500;&#25130;&#26029;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#20316;&#20026;&#25554;&#20214;&#20351;&#29992;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#29616;&#26377;&#30340;&#22522;&#20110;ODE&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00094v2 Announce Type: replace-cross  Abstract: Sampling from diffusion models can be treated as solving the corresponding ordinary differential equations (ODEs), with the aim of obtaining an accurate solution with as few number of function evaluations (NFE) as possible. Recently, various fast samplers utilizing higher-order ODE solvers have emerged and achieved better performance than the initial first-order one. However, these numerical methods inherently result in certain approximation errors, which significantly degrades sample quality with extremely small NFE (e.g., around 5). In contrast, based on the geometric observation that each sampling trajectory almost lies in a two-dimensional subspace embedded in the ambient space, we propose Approximate MEan-Direction Solver (AMED-Solver) that eliminates truncation errors by directly learning the mean direction for fast diffusion sampling. Besides, our method can be easily used as a plugin to further improve existing ODE-base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23558;&#19968;&#20123;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#65292;&#22312;MS COCO&#25968;&#25454;&#38598;&#19978;&#20026;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2304.10727</link><description>&lt;p&gt;
RoCOCO&#65306;&#31283;&#20581;&#30340;&#22522;&#20934;MS-COCO&#35780;&#20272;&#22270;&#25991;&#21305;&#37197;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
RoCOCO: Robust Benchmark MS-COCO to Stress-test Robustness of Image-Text Matching Models. (arXiv:2304.10727v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23558;&#19968;&#20123;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#65292;&#22312;MS COCO&#25968;&#25454;&#38598;&#19978;&#20026;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#20041;&#23884;&#20837;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;MS COCO 5K&#27979;&#35797;&#38598;&#19978;&#22270;&#25991;&#21305;&#37197;&#65288;ITM&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#26102;&#65292;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25554;&#20837;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#26469;&#26356;&#25913;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#21517;&#35789;&#26469;&#26356;&#25913;&#26631;&#39064;&#65292;&#20174;&#32780;&#25913;&#21464;&#21477;&#23376;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#20165;&#23558;&#36825;&#20123;&#26032;&#21019;&#24314;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#27979;&#35797;&#38598;&#20013;&#23601;&#21487;&#20197;&#38477;&#20302;&#21508;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65288;&#20363;&#22914;&#65292;&#22312;BLIP&#20013;&#20174;81.9&#65285;&#38477;&#33267;64.5&#65285;&#65292;&#22312;VSE&#8734;&#20013;&#20174;66.1&#65285;&#38477;&#33267;37.5&#65285;&#65289;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#21457;&#29616;&#33021;&#20026;&#25552;&#39640;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#35774;&#35745;&#26356;&#22810;&#26679;&#21270;&#30340;&#21387;&#21147;&#27979;&#35797;&#25552;&#20379;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large-scale vision-language pre-training models and visual semantic embedding methods have significantly improved image-text matching (ITM) accuracy on MS COCO 5K test set. However, it is unclear how robust these state-of-the-art (SOTA) models are when using them in the wild. In this paper, we propose a novel evaluation benchmark to stress-test the robustness of ITM models. To this end, we add various fooling images and captions to a retrieval pool. Specifically, we change images by inserting unrelated images, and change captions by substituting a noun, which can change the meaning of a sentence. We discover that just adding these newly created images and captions to the test set can degrade performances (i.e., Recall@1) of a wide range of SOTA models (e.g., 81.9% $\rightarrow$ 64.5% in BLIP, 66.1% $\rightarrow$ 37.5% in VSE$\infty$). We expect that our findings can provide insights for improving the robustness of the vision-language models and devising more diverse stress-te
&lt;/p&gt;</description></item><item><title>MaskedKD&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36974;&#34109;&#22270;&#20687;&#22359;&#26469;&#26174;&#33879;&#38477;&#20302;Vision Transformer (ViT)&#33976;&#39311;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#24433;&#21709;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10494</link><description>&lt;p&gt;
MaskedKD&#65306;&#20351;&#29992;&#36974;&#34109;&#22270;&#20687;&#30340;&#39640;&#25928;Vision Transformer&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MaskedKD: Efficient Distillation of Vision Transformers with Masked Images. (arXiv:2302.10494v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10494
&lt;/p&gt;
&lt;p&gt;
MaskedKD&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36974;&#34109;&#22270;&#20687;&#22359;&#26469;&#26174;&#33879;&#38477;&#20302;Vision Transformer (ViT)&#33976;&#39311;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#24433;&#21709;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#23545;&#20110;&#35757;&#32451;&#36731;&#37327;&#32423;&#27169;&#22411;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20250;&#22312;&#35757;&#32451;&#25104;&#26412;&#20013;&#24341;&#20837;&#22823;&#37327;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#22240;&#20026;&#35813;&#26041;&#27861;&#38656;&#35201;&#22312;&#35757;&#32451;&#26679;&#26412;&#19978;&#33719;&#21462;&#25945;&#24072;&#30417;&#30563;&#12290;&#24403;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;Vision Transformer&#65288;ViTs&#65289;&#31561;&#25945;&#24072;&#27169;&#22411;&#26102;&#65292;&#36825;&#31181;&#38468;&#21152;&#25104;&#26412;&#8212;&#8212;&#33976;&#39311;&#25104;&#26412;&#8212;&#8212;&#26368;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MaskedKD&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#30528;&#38477;&#20302;&#33976;&#39311;ViTs&#30340;&#25104;&#26412;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MaskedKD&#36890;&#36807;&#36974;&#34109;&#19968;&#37096;&#20998;&#36755;&#20837;&#21040;&#25945;&#24072;&#27169;&#22411;&#30340;&#22270;&#20687;&#22359;&#20196;&#25945;&#24072;&#27169;&#22411;&#30340;&#25512;&#29702;&#25104;&#26412;&#20943;&#23569;&#65292;&#22240;&#27492;&#21487;&#20197;&#36339;&#36807;&#22788;&#29702;&#36825;&#20123;&#22359;&#25152;&#38656;&#30340;&#35745;&#31639;&#12290;&#25152;&#36873;&#30340;&#36974;&#32617;&#20301;&#32622;&#26088;&#22312;&#38450;&#27490;&#23631;&#34109;&#23398;&#29983;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#30340;&#22270;&#20687;&#30340;&#26680;&#24515;&#29305;&#24449;&#12290;&#35813;&#36974;&#32617;&#36873;&#25321;&#26426;&#21046;&#22522;&#20110;&#23398;&#29983;&#27169;&#22411;&#30340;&#26576;&#20123;&#27880;&#24847;&#21147;&#20998;&#25968;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is an effective method for training lightweight models, but it introduces a significant amount of computational overhead to the training cost, as the method requires acquiring teacher supervisions on training samples. This additional cost -- called distillation cost -- is most pronounced when we employ large-scale teacher models such as vision transformers (ViTs). We present MaskedKD, a simple yet effective strategy that can significantly reduce the cost of distilling ViTs without sacrificing the prediction accuracy of the student model. Specifically, MaskedKD diminishes the cost of running teacher at inference by masking a fraction of image patch tokens fed to the teacher, and therefore skipping the computations required to process those patches. The mask locations are selected to prevent masking away the core features of an image that the student model uses for prediction. This mask selection mechanism operates based on some attention score of the student model
&lt;/p&gt;</description></item></channel></rss>