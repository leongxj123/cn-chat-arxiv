<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#25509;&#21463;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#27169;&#24577;&#20998;&#31163;&#21644;&#37325;&#26500;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20058</link><description>&lt;p&gt;
&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#21644;&#28145;&#24230;&#25972;&#21512;&#30340;&#33041;&#20195;&#35874;&#12289;&#34880;&#28082;&#21160;&#21147;&#23398;&#21644;&#28748;&#27880;&#32593;&#32476;&#24443;&#24213;&#25913;&#21464;&#30142;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Disease Diagnosis with simultaneous functional PET/MR and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20058
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#25509;&#21463;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#27169;&#24577;&#20998;&#31163;&#21644;&#37325;&#26500;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#21151;&#33021;PET/MR&#65288;sf-PET/MR&#65289;&#26159;&#19968;&#31181;&#23574;&#31471;&#30340;&#22810;&#27169;&#24335;&#31070;&#32463;&#24433;&#20687;&#25216;&#26415;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#65292;&#21487;&#20197;&#21516;&#26102;&#30417;&#27979;&#21644;&#25972;&#21512;&#30001;&#26102;&#31354;&#21327;&#21464;&#20195;&#35874;&#27963;&#21160;&#12289;&#31070;&#32463;&#27963;&#21160;&#21644;&#33041;&#34880;&#27969;&#65288;&#28748;&#27880;&#65289;&#26500;&#24314;&#30340;&#22810;&#26041;&#38754;&#22823;&#33041;&#32593;&#32476;&#12290;&#34429;&#28982;&#22312;&#31185;&#23398;/&#20020;&#24202;&#20215;&#20540;&#19978;&#24456;&#39640;&#65292;&#20294;PET/MR&#30828;&#20214;&#30340;&#21487;&#21450;&#24615;&#19981;&#36275;&#38459;&#30861;&#20102;&#20854;&#24212;&#29992;&#65292;&#26356;&#19981;&#29992;&#35828;&#29616;&#20195;&#22522;&#20110;AI&#30340;PET/MR&#34701;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;AI&#30340;&#20020;&#24202;&#21487;&#34892;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#20840;&#38754;&#30340;sf-PET/MR&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20855;&#26377;&#20801;&#35768;&#21333;&#27169;&#24577;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#20165;PET&#65289;&#20197;&#21450;&#24378;&#21046;&#22810;&#27169;&#24577;&#20934;&#30830;&#24615;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#23545;&#40784;&#21644;&#37325;&#26500;&#27169;&#22411;&#12290;&#23427;&#26159;&#27169;&#24577;&#21487;&#20998;&#31163;&#21644;&#21487;&#20132;&#25442;&#30340;&#65292;&#21160;&#24577;&#20998;&#37197;&#19981;&#21516;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;"&#28151;&#21512;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20058v1 Announce Type: cross  Abstract: Simultaneous functional PET/MR (sf-PET/MR) presents a cutting-edge multimodal neuroimaging technique. It provides an unprecedented opportunity for concurrently monitoring and integrating multifaceted brain networks built by spatiotemporally covaried metabolic activity, neural activity, and cerebral blood flow (perfusion). Albeit high scientific/clinical values, short in hardware accessibility of PET/MR hinders its applications, let alone modern AI-based PET/MR fusion models. Our objective is to develop a clinically feasible AI-based disease diagnosis model trained on comprehensive sf-PET/MR data with the power of, during inferencing, allowing single modality input (e.g., PET only) as well as enforcing multimodal-based accuracy. To this end, we propose MX-ARM, a multimodal MiXture-of-experts Alignment and Reconstruction Model. It is modality detachable and exchangeable, allocating different multi-layer perceptrons dynamically ("mixture 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;CATMO&#65292;&#36890;&#36807;&#25972;&#21512;&#29289;&#29702;&#25509;&#35302;&#20449;&#24687;&#65292;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#35270;&#35273;&#33258;&#28982;&#19988;&#29289;&#29702;&#21512;&#29702;&#30340;3D&#20154;&#20307;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.15709</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#32771;&#34385;&#25509;&#35302;&#30340;&#20154;&#20307;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Contact-aware Human Motion Generation from Textual Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;CATMO&#65292;&#36890;&#36807;&#25972;&#21512;&#29289;&#29702;&#25509;&#35302;&#20449;&#24687;&#65292;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#35270;&#35273;&#33258;&#28982;&#19988;&#29289;&#29702;&#21512;&#29702;&#30340;3D&#20154;&#20307;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20174;&#25991;&#26412;&#29983;&#25104;3D&#20132;&#20114;&#24335;&#20154;&#20307;&#21160;&#20316;&#30340;&#38382;&#39064;&#12290;&#32473;&#23450;&#25551;&#36848;&#20102;&#19981;&#21516;&#36523;&#20307;&#37096;&#20301;&#25509;&#35302;&#29289;&#20307;&#21160;&#20316;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#25105;&#20204;&#32508;&#21512;&#29983;&#25104;&#35270;&#35273;&#33258;&#28982;&#19988;&#29289;&#29702;&#21512;&#29702;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20219;&#21153;&#23384;&#22312;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#22312;&#21160;&#20316;&#21644;&#25991;&#26412;&#25551;&#36848;&#20013;&#23545;&#29289;&#29702;&#25509;&#35302;&#30340;&#20114;&#21160;&#32771;&#34385;&#19981;&#36275;&#65292;&#23548;&#33268;&#24207;&#21015;&#19981;&#33258;&#28982;&#19988;&#19981;&#21512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;RICH-CAT&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#34920;&#31034;&#20174;RICH&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#8220;&#32771;&#34385;&#25509;&#35302;&#8221;&#30340;&#25991;&#26412;&#12290;RICH-CAT&#21253;&#25324;&#39640;&#36136;&#37327;&#21160;&#20316;&#12289;&#20934;&#30830;&#30340;&#20154;-&#29289;&#25509;&#35302;&#26631;&#31614;&#21644;&#35814;&#32454;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#28085;&#30422;&#20102;26&#31181;&#23460;&#20869;/&#23460;&#22806;&#21160;&#20316;&#30340;8500&#22810;&#23545;&#21160;&#20316;-&#25991;&#26412;&#37197;&#23545;&#12290;&#21033;&#29992;RICH-CAT&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CATMO&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#20132;&#20114;&#24335;&#20154;&#20307;&#21160;&#20316;&#21512;&#25104;&#65292;&#26126;&#30830;&#25972;&#21512;&#20102;&#29289;&#29702;&#25509;&#35302;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15709v1 Announce Type: cross  Abstract: This paper addresses the problem of generating 3D interactive human motion from text. Given a textual description depicting the actions of different body parts in contact with objects, we synthesize sequences of 3D body poses that are visually natural and physically plausible. Yet, this task poses a significant challenge due to the inadequate consideration of interactions by physical contacts in both motion and textual descriptions, leading to unnatural and implausible sequences. To tackle this challenge, we create a novel dataset named RICH-CAT, representing ``Contact-Aware Texts'' constructed from the RICH dataset. RICH-CAT comprises high-quality motion, accurate human-object contact labels, and detailed textual descriptions, encompassing over 8,500 motion-text pairs across 26 indoor/outdoor actions. Leveraging RICH-CAT, we propose a novel approach named CATMO for text-driven interactive human motion synthesis that explicitly integra
&lt;/p&gt;</description></item><item><title>CoReEcho&#25552;&#20986;&#20102;&#38024;&#23545;&#30452;&#25509;EF&#22238;&#24402;&#30340;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#26368;&#22823;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.10164</link><description>&lt;p&gt;
CoReEcho: 2D+&#26102;&#38388;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#26512;&#30340;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CoReEcho: Continuous Representation Learning for 2D+time Echocardiography Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10164
&lt;/p&gt;
&lt;p&gt;
CoReEcho&#25552;&#20986;&#20102;&#38024;&#23545;&#30452;&#25509;EF&#22238;&#24402;&#30340;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#26368;&#22823;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#30452;&#22312;&#19981;&#21516;&#27169;&#24577;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#36827;&#23637;&#65292;&#21253;&#25324;&#36229;&#22768;&#24515;&#21160;&#22270;&#65292;&#22312;&#25552;&#20379;&#20840;&#38754;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#21516;&#26102;&#12290;&#28982;&#32780;&#65292;&#31471;&#21040;&#31471;&#35757;&#32451;&#27969;&#27700;&#32447;&#20351;&#24471;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#38590;&#20197;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#33021;&#26080;&#27861;&#25429;&#33719;&#36229;&#22768;&#24515;&#21160;&#22270;&#29255;&#27573;&#20043;&#38388;&#30340;&#36830;&#32493;&#20851;&#31995;&#65292;&#23548;&#33268;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#21487;&#33021;&#23545;&#27867;&#21270;&#33021;&#21147;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoReEcho&#65292;&#36825;&#26159;&#19968;&#20010;&#24378;&#35843;&#38024;&#23545;&#30452;&#25509;EF&#22238;&#24402;&#30340;&#36830;&#32493;&#34920;&#31034;&#30340;&#26032;&#22411;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;CoReEcho&#65306;1&#65289;&#22312;&#26368;&#22823;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#38598;&#65288;EchoNet-Dynamic&#65289;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65288;SOTA&#65289;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;3.90&#21644;R2 o
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10164v1 Announce Type: cross  Abstract: Deep learning (DL) models have been advancing automatic medical image analysis on various modalities, including echocardiography, by offering a comprehensive end-to-end training pipeline. This approach enables DL models to regress ejection fraction (EF) directly from 2D+time echocardiograms, resulting in superior performance. However, the end-to-end training pipeline makes the learned representations less explainable. The representations may also fail to capture the continuous relation among echocardiogram clips, indicating the existence of spurious correlations, which can negatively affect the generalization. To mitigate this issue, we propose CoReEcho, a novel training framework emphasizing continuous representations tailored for direct EF regression. Our extensive experiments demonstrate that CoReEcho: 1) outperforms the current state-of-the-art (SOTA) on the largest echocardiography dataset (EchoNet-Dynamic) with MAE of 3.90 &amp; R2 o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;SWSA&#65288;Selection With Synthetic Anomalies&#65289;&#65292;&#29992;&#20110;&#22312;&#27809;&#26377;&#26631;&#31614;&#39564;&#35777;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#22522;&#20110;&#22270;&#20687;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#39564;&#35777;&#38598;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#39640;&#30340;AUROC&#12290;</title><link>https://arxiv.org/abs/2310.10461</link><description>&lt;p&gt;
&#26080;&#26631;&#31614;&#39564;&#35777;&#25968;&#25454;&#19979;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#22120;&#30340;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Model Selection of Zero-shot Anomaly Detectors in the Absence of Labeled Validation Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;SWSA&#65288;Selection With Synthetic Anomalies&#65289;&#65292;&#29992;&#20110;&#22312;&#27809;&#26377;&#26631;&#31614;&#39564;&#35777;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#22522;&#20110;&#22270;&#20687;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#39564;&#35777;&#38598;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#39640;&#30340;AUROC&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#38656;&#35201;&#22312;&#22823;&#22411;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#24322;&#24120;&#26679;&#26412;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#20135;&#29983;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#20854;&#22312;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#24120;&#24120;&#21463;&#21040;&#26631;&#31614;&#25968;&#25454;&#30340;&#32570;&#20047;&#30340;&#38480;&#21046; - &#22312;&#27809;&#26377;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#27861;&#21487;&#38752;&#22320;&#35780;&#20272;&#20854;&#26816;&#27979;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;SWSA&#65288;Selection With Synthetic Anomalies&#65289;&#26469;&#36873;&#25321;&#22522;&#20110;&#22270;&#20687;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#21512;&#25104;&#39564;&#35777;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24322;&#24120;&#29983;&#25104;&#26041;&#27861;&#20551;&#35774;&#21482;&#26377;&#23569;&#37327;&#30340;&#27491;&#24120;&#22270;&#20687;&#25903;&#25345;&#38598;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#29983;&#25104;&#21518;&#65292;&#25105;&#20204;&#30340;&#21512;&#25104;&#39564;&#35777;&#38598;&#34987;&#29992;&#20110;&#21019;&#24314;&#27169;&#22411;&#36873;&#25321;&#30340;&#39564;&#35777;&#26694;&#26550;&#20013;&#30340;&#26816;&#27979;&#20219;&#21153;&#12290;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;SWSA&#24120;&#24120;&#36873;&#25321;&#19982;&#30495;&#23454;&#39564;&#35777;&#38598;&#36873;&#25321;&#30456;&#21305;&#37197;&#30340;&#27169;&#22411;&#65292;&#32467;&#26524;&#27604;&#22522;&#32447;&#26041;&#27861;&#30340;AUROC&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection requires detecting abnormal samples in large unlabeled datasets. While progress in deep learning and the advent of foundation models has produced powerful zero-shot anomaly detection methods, their deployment in practice is often hindered by the lack of labeled data -- without it, their detection performance cannot be evaluated reliably. In this work, we propose SWSA (Selection With Synthetic Anomalies): a general-purpose framework to select image-based anomaly detectors with a generated synthetic validation set. Our proposed anomaly generation method assumes access to only a small support set of normal images and requires no training or fine-tuning. Once generated, our synthetic validation set is used to create detection tasks that compose a validation framework for model selection. In an empirical study, we find that SWSA often selects models that match selections made with a ground-truth validation set, resulting in higher AUROCs than baseline methods. We also find
&lt;/p&gt;</description></item><item><title>VideoDrafter&#26159;&#19968;&#20010;&#21033;&#29992;LLM&#23454;&#29616;&#20869;&#23481;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26681;&#25454;&#36755;&#20837;&#25552;&#31034;&#29983;&#25104;&#36923;&#36753;&#36830;&#36143;&#30340;&#22810;&#22330;&#26223;&#33050;&#26412;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#12290;</title><link>http://arxiv.org/abs/2401.01256</link><description>&lt;p&gt;
VideoDrafter: &#21033;&#29992;LLM&#23454;&#29616;&#20869;&#23481;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM. (arXiv:2401.01256v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01256
&lt;/p&gt;
&lt;p&gt;
VideoDrafter&#26159;&#19968;&#20010;&#21033;&#29992;LLM&#23454;&#29616;&#20869;&#23481;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#26681;&#25454;&#36755;&#20837;&#25552;&#31034;&#29983;&#25104;&#36923;&#36753;&#36830;&#36143;&#30340;&#22810;&#22330;&#26223;&#33050;&#26412;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25193;&#23637;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#31361;&#30772;&#26174;&#33879;&#25193;&#22823;&#20102;&#26681;&#25454;&#32473;&#23450;&#25552;&#31034;&#29983;&#25104;&#39640;&#36136;&#37327;&#35270;&#39057;&#30340;&#21487;&#33021;&#24615;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#20316;&#21697;&#20165;&#22788;&#29702;&#22312;&#21333;&#20010;&#32972;&#26223;&#20013;&#21457;&#29983;&#21333;&#20010;&#35270;&#39057;&#20107;&#20214;&#30340;&#21333;&#22330;&#26223;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#25193;&#23637;&#21040;&#29983;&#25104;&#22810;&#22330;&#26223;&#35270;&#39057;&#24182;&#19988;&#22312;&#20445;&#25345;&#21508;&#20010;&#22330;&#26223;&#20043;&#38388;&#30340;&#36923;&#36753;&#19968;&#33268;&#21516;&#26102;&#20445;&#25345;&#35270;&#35273;&#22806;&#35266;&#19968;&#33268;&#24615;&#26041;&#38754;&#24182;&#19981;&#31616;&#21333;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;VideoDrafter&#65292;&#29992;&#20110;&#20869;&#23481;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#12290;&#25216;&#26415;&#19978;&#65292;VideoDrafter&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#36755;&#20837;&#25552;&#31034;&#36716;&#21270;&#20026;&#32508;&#21512;&#30340;&#22810;&#22330;&#26223;&#33050;&#26412;&#65292;&#35813;&#33050;&#26412;&#20174;LLM&#23398;&#21040;&#30340;&#36923;&#36753;&#30693;&#35782;&#20013;&#21463;&#30410;&#12290;&#27599;&#20010;&#22330;&#26223;&#30340;&#33050;&#26412;&#21253;&#25324;&#25551;&#36848;&#20107;&#20214;&#12289;&#21069;&#26223;/&#32972;&#26223;&#23454;&#20307;&#20197;&#21450;&#25668;&#20687;&#26426;&#36816;&#21160;&#30340;&#25552;&#31034;&#12290;VideoDrafter&#35782;&#21035;&#33050;&#26412;&#20013;&#30340;&#20849;&#21516;&#23454;&#20307;&#65292;&#24182;&#35810;&#38382;LLM&#26469;&#36873;&#25321;&#29983;&#25104;&#36923;&#36753;&#36830;&#36143;&#30340;&#35270;&#39057;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent innovations and breakthroughs in diffusion models have significantly expanded the possibilities of generating high-quality videos for the given prompts. Most existing works tackle the single-scene scenario with only one video event occurring in a single background. Extending to generate multi-scene videos nevertheless is not trivial and necessitates to nicely manage the logic in between while preserving the consistent visual appearance of key content across video scenes. In this paper, we propose a novel framework, namely VideoDrafter, for content-consistent multi-scene video generation. Technically, VideoDrafter leverages Large Language Models (LLM) to convert the input prompt into comprehensive multi-scene script that benefits from the logical knowledge learnt by LLM. The script for each scene includes a prompt describing the event, the foreground/background entities, as well as camera movement. VideoDrafter identifies the common entities throughout the script and asks LLM
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07760</link><description>&lt;p&gt;
PRE: &#35270;&#35273;-&#35821;&#35328;&#25552;&#31034;&#23398;&#20064;&#19982;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
PRE: Vision-Language Prompt Learning with Reparameterization Encoder. (arXiv:2309.07760v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07760
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#38646;&#26679;&#26412;&#36801;&#31227;&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#38656;&#35201;&#25163;&#21160;&#36873;&#25321;&#25552;&#31034;&#20197;&#25913;&#36827;&#19979;&#28216;&#22270;&#20687;&#20998;&#24067;&#21644;&#25991;&#26412;&#31867;&#25551;&#36848;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#36825;&#31181;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#26159;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#21040;&#23454;&#36341;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#24182;&#19988;&#38750;&#24120;&#32791;&#26102;&#12290;&#20026;&#20102;&#36991;&#20813;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#26368;&#36817;&#30340;CoOp&#24037;&#20316;&#24341;&#20837;&#20102;&#22312;&#35270;&#35273;&#39046;&#22495;&#20351;&#29992;&#21487;&#25511;&#25991;&#26412;&#26631;&#35760;&#30340;&#25552;&#31034;&#23398;&#20064;&#27010;&#24565;&#12290;&#34429;&#28982;CoOp&#21487;&#20197;&#22312;&#25163;&#21160;&#25552;&#31034;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#65292;&#20294;&#20854;&#23398;&#21040;&#30340;&#19978;&#19979;&#25991;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#20013;&#26356;&#24191;&#27867;&#30340;&#26410;&#35265;&#31867;&#21035;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Learning with Reparameterization Encoder (PRE) &#30340;&#31616;&#21333;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained vision-language models such as CLIP have demonstrated great potential in zero-shot transferability to downstream tasks. However, to attain optimal performance, the manual selection of prompts is necessary to improve alignment between the downstream image distribution and the textual class descriptions. This manual prompt engineering is the major challenge for deploying such models in practice since it requires domain expertise and is extremely time-consuming. To avoid non-trivial prompt engineering, recent work Context Optimization (CoOp) introduced the concept of prompt learning to the vision domain using learnable textual tokens. While CoOp can achieve substantial improvements over manual prompts, its learned context is worse generalizable to wider unseen classes within the same dataset. In this work, we present Prompt Learning with Reparameterization Encoder (PRE) - a simple and efficient method that enhances the generalization ability of the learnable prompt to un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23558;&#19968;&#20123;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#65292;&#22312;MS COCO&#25968;&#25454;&#38598;&#19978;&#20026;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2304.10727</link><description>&lt;p&gt;
RoCOCO&#65306;&#31283;&#20581;&#30340;&#22522;&#20934;MS-COCO&#35780;&#20272;&#22270;&#25991;&#21305;&#37197;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
RoCOCO: Robust Benchmark MS-COCO to Stress-test Robustness of Image-Text Matching Models. (arXiv:2304.10727v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23558;&#19968;&#20123;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#65292;&#22312;MS COCO&#25968;&#25454;&#38598;&#19978;&#20026;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#20041;&#23884;&#20837;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;MS COCO 5K&#27979;&#35797;&#38598;&#19978;&#22270;&#25991;&#21305;&#37197;&#65288;ITM&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#26102;&#65292;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25554;&#20837;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#26469;&#26356;&#25913;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#21517;&#35789;&#26469;&#26356;&#25913;&#26631;&#39064;&#65292;&#20174;&#32780;&#25913;&#21464;&#21477;&#23376;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#20165;&#23558;&#36825;&#20123;&#26032;&#21019;&#24314;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#27979;&#35797;&#38598;&#20013;&#23601;&#21487;&#20197;&#38477;&#20302;&#21508;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65288;&#20363;&#22914;&#65292;&#22312;BLIP&#20013;&#20174;81.9&#65285;&#38477;&#33267;64.5&#65285;&#65292;&#22312;VSE&#8734;&#20013;&#20174;66.1&#65285;&#38477;&#33267;37.5&#65285;&#65289;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#21457;&#29616;&#33021;&#20026;&#25552;&#39640;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#35774;&#35745;&#26356;&#22810;&#26679;&#21270;&#30340;&#21387;&#21147;&#27979;&#35797;&#25552;&#20379;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large-scale vision-language pre-training models and visual semantic embedding methods have significantly improved image-text matching (ITM) accuracy on MS COCO 5K test set. However, it is unclear how robust these state-of-the-art (SOTA) models are when using them in the wild. In this paper, we propose a novel evaluation benchmark to stress-test the robustness of ITM models. To this end, we add various fooling images and captions to a retrieval pool. Specifically, we change images by inserting unrelated images, and change captions by substituting a noun, which can change the meaning of a sentence. We discover that just adding these newly created images and captions to the test set can degrade performances (i.e., Recall@1) of a wide range of SOTA models (e.g., 81.9% $\rightarrow$ 64.5% in BLIP, 66.1% $\rightarrow$ 37.5% in VSE$\infty$). We expect that our findings can provide insights for improving the robustness of the vision-language models and devising more diverse stress-te
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphMLP&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#23427;&#23558;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2206.06420</link><description>&lt;p&gt;
GraphMLP&#65306;&#19968;&#31181;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#22270;&#24418;MLP&#24335;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
GraphMLP: A Graph MLP-Like Architecture for 3D Human Pose Estimation. (arXiv:2206.06420v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06420
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphMLP&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#23427;&#23558;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#27809;&#26377;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#30340;&#31454;&#20105;&#24615;&#32467;&#26524;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MLP&#27169;&#22411;&#24182;&#19981;&#25797;&#38271;&#25429;&#25417;&#23616;&#37096;&#32454;&#33410;&#65292;&#20063;&#32570;&#20047;&#26377;&#20851;&#20154;&#20307;&#26500;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#39592;&#39612;&#34920;&#31034;&#23398;&#20064;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#31216;&#20026;GraphMLP&#65292;&#23427;&#32467;&#21512;&#20102;MLP&#21644;&#22270;&#24418;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#20840;&#23616;-&#23616;&#37096;-&#22270;&#24418;&#32479;&#19968;&#26550;&#26500;&#20013;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#12290;GraphMLP&#23558;&#20154;&#20307;&#30340;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#23637;&#31034;&#20102;&#21487;&#20197;&#20197;&#21487;&#24573;&#30053;&#30340;&#35745;&#31639;&#20195;&#20215;&#26469;&#26377;&#25928;&#22320;&#24314;&#27169;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern multi-layer perceptron (MLP) models have shown competitive results in learning visual representations without self-attention. However, existing MLP models are not good at capturing local details and lack prior knowledge of human body configurations, which limits their modeling power for skeletal representation learning. To address these issues, we propose a simple yet effective graph-reinforced MLP-Like architecture, named GraphMLP, that combines MLPs and graph convolutional networks (GCNs) in a global-local-graphical unified architecture for 3D human pose estimation. GraphMLP incorporates the graph structure of human bodies into an MLP model to meet the domain-specific demand of the 3D human pose, while allowing for both local and global spatial interactions. Furthermore, we propose to flexibly and efficiently extend the GraphMLP to the video domain and show that complex temporal dynamics can be effectively modeled in a simple way with negligible computational cost gains in the
&lt;/p&gt;</description></item></channel></rss>