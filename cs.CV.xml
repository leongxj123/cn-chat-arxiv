<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>SLEDGE&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#30340;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#29983;&#25104;&#27169;&#25311;&#22120;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#26629;&#26684;&#21040;&#30690;&#37327;&#33258;&#32534;&#30721;&#22120;&#65288;RVAE&#65289;&#20197;&#21450;Diffusion Transformer&#26469;&#29983;&#25104;&#26234;&#33021;&#20307;&#21644;&#36710;&#36947;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#25311;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.17933</link><description>&lt;p&gt;
SLEDGE: &#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#39550;&#39542;&#26234;&#33021;&#20307;&#30340;&#27169;&#25311;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17933
&lt;/p&gt;
&lt;p&gt;
SLEDGE&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#30340;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#29983;&#25104;&#27169;&#25311;&#22120;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#26629;&#26684;&#21040;&#30690;&#37327;&#33258;&#32534;&#30721;&#22120;&#65288;RVAE&#65289;&#20197;&#21450;Diffusion Transformer&#26469;&#29983;&#25104;&#26234;&#33021;&#20307;&#21644;&#36710;&#36947;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#25311;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SLEDGE&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#39550;&#39542;&#35760;&#24405;&#35757;&#32451;&#30340;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#29983;&#25104;&#27169;&#25311;&#22120;&#12290;&#20854;&#26680;&#24515;&#32452;&#20214;&#26159;&#19968;&#20010;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#26234;&#33021;&#20307;&#36793;&#30028;&#26694;&#21644;&#36710;&#36947;&#22270;&#12290;&#35813;&#27169;&#22411;&#30340;&#36755;&#20986;&#20316;&#20026;&#20132;&#36890;&#27169;&#25311;&#30340;&#21021;&#22987;&#29366;&#24577;&#12290;&#38024;&#23545;SLEDGE&#24453;&#29983;&#25104;&#30340;&#23454;&#20307;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#20363;&#22914;&#23427;&#20204;&#30340;&#36830;&#25509;&#24615;&#21644;&#27599;&#20010;&#22330;&#26223;&#30340;&#21487;&#21464;&#25968;&#37327;&#65292;&#20351;&#24471;&#22823;&#22810;&#25968;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#26420;&#32032;&#24212;&#29992;&#21464;&#24471;&#19981;&#31616;&#21333;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38500;&#20102;&#23545;&#29616;&#26377;&#36710;&#36947;&#22270;&#34920;&#31034;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26629;&#26684;&#21040;&#30690;&#37327;&#33258;&#32534;&#30721;&#22120;&#65288;RVAE&#65289;&#12290;&#23427;&#23558;&#26234;&#33021;&#20307;&#21644;&#36710;&#36947;&#22270;&#32534;&#30721;&#20026;&#26629;&#26684;&#21270;&#28508;&#22312;&#26144;&#23556;&#20013;&#30340;&#19981;&#21516;&#36890;&#36947;&#12290;&#36825;&#26377;&#21161;&#20110;&#36710;&#36947;&#26465;&#20214;&#19979;&#30340;&#26234;&#33021;&#20307;&#29983;&#25104;&#20197;&#21450;&#20351;&#29992;&#25193;&#25955;&#21464;&#25442;&#22120;&#21516;&#26102;&#29983;&#25104;&#36710;&#36947;&#21644;&#26234;&#33021;&#20307;&#12290;&#22312;SLEDGE&#20013;&#20351;&#29992;&#29983;&#25104;&#30340;&#23454;&#20307;&#21487;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#27169;&#25311;&#65292;&#20363;&#22914;&#19978;&#37319;&#26679;&#36716;&#24367;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17933v1 Announce Type: cross  Abstract: SLEDGE is the first generative simulator for vehicle motion planning trained on real-world driving logs. Its core component is a learned model that is able to generate agent bounding boxes and lane graphs. The model's outputs serve as an initial state for traffic simulation. The unique properties of the entities to be generated for SLEDGE, such as their connectivity and variable count per scene, render the naive application of most modern generative models to this task non-trivial. Therefore, together with a systematic study of existing lane graph representations, we introduce a novel raster-to-vector autoencoder (RVAE). It encodes agents and the lane graph into distinct channels in a rasterized latent map. This facilitates both lane-conditioned agent generation and combined generation of lanes and agents with a Diffusion Transformer. Using generated entities in SLEDGE enables greater control over the simulation, e.g. upsampling turns 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#23433;&#20840;&#32771;&#34385;&#20316;&#20026;&#32467;&#26500;&#20803;&#32032;&#65292;&#20197;&#31995;&#32479;&#32508;&#21512;&#30340;&#26041;&#24335;&#30830;&#20445;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#19968;&#27010;&#24565;&#19981;&#20165;&#19982;&#29616;&#26377;&#30340;&#23433;&#20840;&#26631;&#20934;&#30456;&#22865;&#21512;&#65292;&#36824;&#20026;AI&#23433;&#20840;&#30456;&#20851;&#30340;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#26631;&#20934;&#25552;&#20379;&#20102;&#26032;&#30340;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.03774</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#23433;&#20840;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Safety Concerns in Automated Driving Perception. (arXiv:2309.03774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#23433;&#20840;&#32771;&#34385;&#20316;&#20026;&#32467;&#26500;&#20803;&#32032;&#65292;&#20197;&#31995;&#32479;&#32508;&#21512;&#30340;&#26041;&#24335;&#30830;&#20445;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#19968;&#27010;&#24565;&#19981;&#20165;&#19982;&#29616;&#26377;&#30340;&#23433;&#20840;&#26631;&#20934;&#30456;&#22865;&#21512;&#65292;&#36824;&#20026;AI&#23433;&#20840;&#30456;&#20851;&#30340;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#26631;&#20934;&#25552;&#20379;&#20102;&#26032;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#20197;&#21450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#24863;&#30693;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#23548;&#33268;&#20102;&#23545;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#24212;&#29992;&#30340;&#22686;&#21152;&#38656;&#27714;&#12290;&#36825;&#31867;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#38656;&#35201;&#32771;&#34385;DNNs&#30340;&#29420;&#29305;&#23646;&#24615;&#12290;&#20026;&#20102;&#20197;&#31995;&#32479;&#32508;&#21512;&#30340;&#26041;&#24335;&#30830;&#20445;&#22522;&#20110;DNNs&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#23433;&#20840;&#32771;&#34385;&#20316;&#20026;&#36866;&#24403;&#30340;&#32467;&#26500;&#20803;&#32032;&#12290;&#19968;&#26041;&#38754;&#65292;&#23433;&#20840;&#32771;&#34385;&#30340;&#27010;&#24565;&#35774;&#35745;&#19982;&#29616;&#26377;&#30340;&#19982;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#23433;&#20840;&#30456;&#20851;&#30340;&#26631;&#20934;&#22914;ISO 21448&#65288;SOTIF&#65289;&#38750;&#24120;&#22865;&#21512;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#24050;&#32463;&#28608;&#21457;&#20102;&#20960;&#31687;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#21363;&#23558;&#20986;&#21488;&#30340;&#20851;&#20110;AI&#23433;&#20840;&#30340;&#26631;&#20934;&#65292;&#22914;ISO PAS 8800&#12290;&#34429;&#28982;&#23433;&#20840;&#32771;&#34385;&#30340;&#27010;&#24565;&#20197;&#21069;&#24050;&#32463;&#34987;&#20171;&#32461;&#36807;&#65292;&#20294;&#26412;&#25991;&#23545;&#20854;&#36827;&#34892;&#20102;&#25193;&#23637;&#21644;&#20248;&#21270;&#65292;&#20511;&#37492;&#20102;&#21508;&#20010;&#39046;&#22495;&#21644;&#23433;&#20840;&#19987;&#23478;&#30340;&#21453;&#39304;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in the field of deep learning and impressive performance of deep neural networks (DNNs) for perception have resulted in an increased demand for their use in automated driving (AD) systems. The safety of such systems is of utmost importance and thus requires to consider the unique properties of DNNs.  In order to achieve safety of AD systems with DNN-based perception components in a systematic and comprehensive approach, so-called safety concerns have been introduced as a suitable structuring element. On the one hand, the concept of safety concerns is -- by design -- well aligned to existing standards relevant for safety of AD systems such as ISO 21448 (SOTIF). On the other hand, it has already inspired several academic publications and upcoming standards on AI safety such as ISO PAS 8800.  While the concept of safety concerns has been previously introduced, this paper extends and refines it, leveraging feedback from various domain and safety experts in the field. In par
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20013;&#21516;&#26102;&#22788;&#29702;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#21033;&#29992;&#25345;&#32493;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#21518;&#32493;&#20219;&#21153;&#20013;&#30340;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.12112</link><description>&lt;p&gt;
&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Generalized Continual Category Discovery. (arXiv:2308.12112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20013;&#21516;&#26102;&#22788;&#29702;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#21033;&#29992;&#25345;&#32493;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#21518;&#32493;&#20219;&#21153;&#20013;&#30340;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#25512;&#21160;&#30528;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#30340;&#26497;&#38480;&#65292;&#20854;&#20013;&#19968;&#20010;&#26234;&#33021;&#20307;&#26399;&#26395;&#23398;&#20064;&#26032;&#30340;&#26631;&#35760;&#20219;&#21153;&#32780;&#19981;&#20250;&#24536;&#35760;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35774;&#32622;&#19982;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#19981;&#22826;&#21563;&#21512;&#65292;&#20854;&#20013;&#23398;&#20064;&#26234;&#33021;&#20307;&#21487;&#20197;&#35775;&#38382;&#22823;&#37327;&#30340;&#26080;&#26631;&#35760;&#25968;&#25454;&#65292;&#21253;&#25324;&#20840;&#26032;&#65288;&#23436;&#20840;&#26080;&#26631;&#35760;&#65289;&#31867;&#21035;&#21644;&#24050;&#30693;&#31867;&#21035;&#30340;&#31034;&#20363;&#12290;&#21463;&#21040;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#25918;&#26494;&#36825;&#20010;&#20551;&#35774;&#12290;&#30830;&#20999;&#22320;&#35828;&#65292;&#22312;&#20219;&#20309;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20801;&#35768;&#23384;&#22312;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#24517;&#39035;&#20351;&#29992;&#25345;&#32493;&#29256;&#26412;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#35774;&#32622;&#20026;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#12290;&#23427;&#32479;&#19968;&#20102;CL&#21644;GCD&#65292;&#24357;&#21512;&#20102;&#21512;&#25104;&#22522;&#20934;&#21644;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#26080;&#27861;&#20174;&#21518;&#32493;&#20219;&#21153;&#20013;&#31215;&#32047;&#30693;&#35782;&#65292;&#20854;&#20013;&#21253;&#21547;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of Continual Learning (CL) methods push the limit of supervised learning settings, where an agent is expected to learn new labeled tasks and not forget previous knowledge. However, these settings are not well aligned with real-life scenarios, where a learning agent has access to a vast amount of unlabeled data encompassing both novel (entirely unlabeled) classes and examples from known classes. Drawing inspiration from Generalized Category Discovery (GCD), we introduce a novel framework that relaxes this assumption. Precisely, in any task, we allow for the existence of novel and known classes, and one must use continual version of unsupervised learning methods to discover them. We call this setting Generalized Continual Category Discovery (GCCD). It unifies CL and GCD, bridging the gap between synthetic benchmarks and real-life scenarios. With a series of experiments, we present that existing methods fail to accumulate knowledge from subsequent tasks in which unlabeled samples of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;(SVG)&#26684;&#24335;&#22788;&#29702;&#22270;&#20687;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#30452;&#25509;&#29702;&#35299;&#21644;&#25805;&#20316;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#35270;&#35273;&#32452;&#20214;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.06094</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;&#39537;&#21160;&#30340;&#22270;&#20687;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding. (arXiv:2306.06094v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;(SVG)&#26684;&#24335;&#22788;&#29702;&#22270;&#20687;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#30452;&#25509;&#29702;&#35299;&#21644;&#25805;&#20316;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#35270;&#35273;&#32452;&#20214;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#28508;&#21147;&#20173;&#28982;&#24456;&#22823;&#31243;&#24230;&#19978;&#27809;&#26377;&#34987;&#24320;&#21457;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#24615;&#26041;&#27861;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#20351;&#29992;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;(SVG)&#26684;&#24335;&#22788;&#29702;&#22270;&#20687;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;XML&#30340;SVG&#34920;&#36848;&#30340;&#25991;&#26412;&#25551;&#36848;&#32780;&#19981;&#26159;&#20809;&#26629;&#22270;&#20687;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20351;LLMs&#33021;&#22815;&#30452;&#25509;&#29702;&#35299;&#21644;&#25805;&#20316;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#35270;&#35273;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#36827;&#34892;&#31616;&#21333;&#30340;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26041;&#27861;&#22312;&#21028;&#21035;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20248;&#24322;&#34920;&#29616;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#23427;(i)&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#40065;&#26834;&#24615;&#65292;(ii)&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#23454;&#29616;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#20197;&#21450;(iii)&#22270;&#20687;&#26434;&#20081;&#31243;&#24230;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) have made significant advancements in natural language understanding and generation. However, their potential in computer vision remains largely unexplored. In this paper, we introduce a new, exploratory approach that enables LLMs to process images using the Scalable Vector Graphics (SVG) format. By leveraging the XML-based textual descriptions of SVG representations instead of raster images, we aim to bridge the gap between the visual and textual modalities, allowing LLMs to directly understand and manipulate images without the need for parameterized visual components. Our method facilitates simple image classification, generation, and in-context learning using only LLM capabilities. We demonstrate the promise of our approach across discriminative and generative tasks, highlighting its (i) robustness against distribution shift, (ii) substantial improvements achieved by tapping into the in-context learning abilities of LLMs, and (iii) image unders
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#20559;&#33258;&#27880;&#24847;&#21147;&#30340;&#20844;&#24179;&#24863;&#30693;&#35270;&#35273;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#19982;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#26469;&#20943;&#36731;&#20559;&#35265;&#65292;&#24182;&#21033;&#29992;&#23545;&#25239;&#24615;&#31034;&#20363;&#26469;&#23450;&#20301;&#21644;&#23631;&#34109;&#36825;&#20123;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2301.13803</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#20559;&#33258;&#27880;&#24847;&#21147;&#30340;&#20844;&#24179;&#24863;&#30693;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fairness-aware Vision Transformer via Debiased Self-Attention. (arXiv:2301.13803v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13803
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#20559;&#33258;&#27880;&#24847;&#21147;&#30340;&#20844;&#24179;&#24863;&#30693;&#35270;&#35273;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#19982;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#26469;&#20943;&#36731;&#20559;&#35265;&#65292;&#24182;&#21033;&#29992;&#23545;&#25239;&#24615;&#31034;&#20363;&#26469;&#23450;&#20301;&#21644;&#23631;&#34109;&#36825;&#20123;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#25552;&#21462;&#20449;&#24687;&#29305;&#24449;&#21644;&#36890;&#36807;&#33258;&#25105;&#20851;&#27880;&#26426;&#21046;&#24314;&#27169;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#22312;&#35299;&#20915;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#38382;&#39064;&#26041;&#38754;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;ViT&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;ViT&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21253;&#25324;&#20854;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#21478;&#19968;&#20010;&#38656;&#27714;&#65292;&#20844;&#24179;&#24615;&#65292;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#20844;&#24179;&#24863;&#30693;&#31639;&#27861;&#65288;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;CNN&#65289;&#22312;ViT&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#23601;&#38656;&#35201;&#25105;&#20204;&#36890;&#36807;&#21435;&#20559;&#33258;&#27880;&#24847;&#65288;DSA&#65289;&#24320;&#21457;&#25105;&#20204;&#30340;&#26032;&#26694;&#26550;&#12290;DSA&#26159;&#19968;&#31181;&#36890;&#36807;&#30450;&#30446;&#26041;&#27861;&#26469;&#24378;&#21046;ViT&#28040;&#38500;&#19982;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#20197;&#20943;&#36731;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#25239;&#24615;&#31034;&#20363;&#34987;&#29992;&#26469;&#23450;&#20301;&#21644;&#23631;&#34109;&#36755;&#20837;&#22270;&#20687;&#22359;&#20013;&#30340;&#34394;&#20551;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) has recently gained significant interest in solving computer vision (CV) problems due to its capability of extracting informative features and modeling long-range dependencies through the self-attention mechanism. To fully realize the advantages of ViT in real-world applications, recent works have explored the trustworthiness of ViT, including its robustness and explainability. However, another desiderata, fairness has not yet been adequately addressed in the literature. We establish that the existing fairness-aware algorithms (primarily designed for CNNs) do not perform well on ViT. This necessitates the need for developing our novel framework via Debiased Self-Attention (DSA). DSA is a fairness-through-blindness approach that enforces ViT to eliminate spurious features correlated with the sensitive attributes for bias mitigation. Notably, adversarial examples are leveraged to locate and mask the spurious features in the input image patches. In addition, DSA u
&lt;/p&gt;</description></item></channel></rss>