<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spectral Motion Alignment&#65288;SMA&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#21644;&#23567;&#27874;&#21464;&#25442;&#26469;&#20248;&#21270;&#21644;&#23545;&#40784;&#36816;&#21160;&#21521;&#37327;&#65292;&#23398;&#20064;&#25972;&#24103;&#20840;&#23616;&#36816;&#21160;&#21160;&#24577;&#65292;&#20943;&#36731;&#31354;&#38388;&#20266;&#24433;&#65292;&#26377;&#25928;&#25913;&#21892;&#36816;&#21160;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.15249</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35270;&#39057;&#36816;&#21160;&#36716;&#31227;&#30340;&#20809;&#35889;&#36816;&#21160;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Spectral Motion Alignment for Video Motion Transfer using Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15249
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Spectral Motion Alignment&#65288;SMA&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#21644;&#23567;&#27874;&#21464;&#25442;&#26469;&#20248;&#21270;&#21644;&#23545;&#40784;&#36816;&#21160;&#21521;&#37327;&#65292;&#23398;&#20064;&#25972;&#24103;&#20840;&#23616;&#36816;&#21160;&#21160;&#24577;&#65292;&#20943;&#36731;&#31354;&#38388;&#20266;&#24433;&#65292;&#26377;&#25928;&#25913;&#21892;&#36816;&#21160;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#22312;&#35270;&#39057;&#29983;&#25104;&#21644;&#29702;&#35299;&#26041;&#38754;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#65288;VDMs&#65289;&#26174;&#33879;&#20419;&#36827;&#20102;&#23558;&#36755;&#20837;&#35270;&#39057;&#23450;&#21046;&#20026;&#30446;&#26631;&#22806;&#35266;&#12289;&#36816;&#21160;&#31561;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#20934;&#30830;&#25552;&#21462;&#35270;&#39057;&#24103;&#30340;&#36816;&#21160;&#20449;&#24687;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#20316;&#21697;&#21033;&#29992;&#36830;&#32493;&#24103;&#27531;&#24046;&#20316;&#20026;&#30446;&#26631;&#36816;&#21160;&#21521;&#37327;&#65292;&#20294;&#23427;&#20204;&#22266;&#26377;&#22320;&#32570;&#20047;&#20840;&#23616;&#36816;&#21160;&#32972;&#26223;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#36880;&#24103;&#22833;&#30495;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20809;&#35889;&#36816;&#21160;&#23545;&#40784;&#65288;SMA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#20613;&#31435;&#21494;&#21644;&#23567;&#27874;&#21464;&#25442;&#26469;&#20248;&#21270;&#21644;&#23545;&#40784;&#36816;&#21160;&#21521;&#37327;&#30340;&#26032;&#26694;&#26550;&#12290;SMA&#36890;&#36807;&#25972;&#21512;&#39057;&#22495;&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#36816;&#21160;&#27169;&#24335;&#65292;&#20419;&#36827;&#25972;&#24103;&#20840;&#23616;&#36816;&#21160;&#21160;&#24577;&#30340;&#23398;&#20064;&#65292;&#24182;&#20943;&#36731;&#31354;&#38388;&#20266;&#24433;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;SMA&#22312;&#25913;&#21892;&#36816;&#21160;&#36716;&#31227;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15249v1 Announce Type: cross  Abstract: The evolution of diffusion models has greatly impacted video generation and understanding. Particularly, text-to-video diffusion models (VDMs) have significantly facilitated the customization of input video with target appearance, motion, etc. Despite these advances, challenges persist in accurately distilling motion information from video frames. While existing works leverage the consecutive frame residual as the target motion vector, they inherently lack global motion context and are vulnerable to frame-wise distortions. To address this, we present Spectral Motion Alignment (SMA), a novel framework that refines and aligns motion vectors using Fourier and wavelet transforms. SMA learns motion patterns by incorporating frequency-domain regularization, facilitating the learning of whole-frame global motion dynamics, and mitigating spatial artifacts. Extensive experiments demonstrate SMA's efficacy in improving motion transfer while main
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26059;&#36716;&#19981;&#21464;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#30340;&#31561;&#21464;&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#20960;&#20309;&#24402;&#32435;&#20559;&#24046;&#65292;&#25104;&#21151;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15031</link><description>&lt;p&gt;
&#20351;&#29992;&#26059;&#36716;&#19981;&#21464;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Image Classification with Rotation-Invariant Variational Quantum Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15031
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26059;&#36716;&#19981;&#21464;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#30340;&#31561;&#21464;&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#20960;&#20309;&#24402;&#32435;&#20559;&#24046;&#65292;&#25104;&#21151;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#20316;&#20026;&#22024;&#26434;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#30340;&#26089;&#26399;&#24212;&#29992;&#27491;&#21463;&#21040;&#20851;&#27880;&#12290;&#21464;&#20998;&#26041;&#27861;&#30340;&#20027;&#35201;&#38382;&#39064;&#20043;&#19968;&#22312;&#20110;Barren Plateaus&#29616;&#35937;&#65292;&#22312;&#21464;&#20998;&#21442;&#25968;&#20248;&#21270;&#36807;&#31243;&#20013;&#23384;&#22312;&#12290;&#25552;&#20986;&#23558;&#20960;&#20309;&#24402;&#32435;&#20559;&#24046;&#28155;&#21152;&#21040;&#37327;&#23376;&#27169;&#22411;&#20316;&#20026;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#20010;&#31216;&#20026;&#20960;&#20309;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#39046;&#22495;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#31561;&#21464;&#32467;&#26500;&#30340;&#21464;&#20998;&#37327;&#23376;&#20998;&#31867;&#22120;&#65292;&#20197;&#21019;&#24314;&#20855;&#26377;$C_4$&#26059;&#36716;&#26631;&#31614;&#23545;&#31216;&#24615;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#26631;&#31614;&#19981;&#21464;&#27169;&#22411;&#12290;&#31561;&#21464;&#30005;&#36335;&#19982;&#20004;&#31181;&#19981;&#21516;&#30340;&#32467;&#26500;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#23454;&#39564;&#35777;&#26126;&#20960;&#20309;&#26041;&#27861;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#32463;&#20856;&#31561;&#21464;&#21367;&#31215;&#25805;&#20316;&#65292;&#20197;&#25193;&#23637;&#37327;&#23376;&#27169;&#22411;&#22788;&#29702;&#26356;&#22823;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15031v1 Announce Type: cross  Abstract: Variational quantum algorithms are gaining attention as an early application of Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of variational methods lies in the phenomenon of Barren Plateaus, present in the optimization of variational parameters. Adding geometric inductive bias to the quantum models has been proposed as a potential solution to mitigate this problem, leading to a new field called Geometric Quantum Machine Learning. In this work, an equivariant architecture for variational quantum classifiers is introduced to create a label-invariant model for image classification with $C_4$ rotational label symmetry. The equivariant circuit is benchmarked against two different architectures, and it is experimentally observed that the geometric approach boosts the model's performance. Finally, a classical equivariant convolution operation is proposed to extend the quantum model for the processing of larger ima
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26469;&#36873;&#25321;&#38656;&#35201;&#36827;&#19968;&#27493;&#36866;&#24212;&#30340;&#23618;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30001;&#20110;&#20266;&#26631;&#31614;&#24341;&#36215;&#30340;&#19981;&#20934;&#30830;&#24615;&#22256;&#25200;&#12290;</title><link>https://arxiv.org/abs/2403.10650</link><description>&lt;p&gt;
PALM&#65306;&#25512;&#36827;&#29992;&#20110;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26469;&#36873;&#25321;&#38656;&#35201;&#36827;&#19968;&#27493;&#36866;&#24212;&#30340;&#23618;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30001;&#20110;&#20266;&#26631;&#31614;&#24341;&#36215;&#30340;&#19981;&#20934;&#30830;&#24615;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#27169;&#22411;&#38754;&#20020;&#39046;&#22495;&#20998;&#24067;&#30340;&#24555;&#36895;&#36716;&#21464;&#65292;&#23548;&#33268;&#35782;&#21035;&#24615;&#33021;&#19979;&#38477;&#12290;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;CTTA&#65289;&#30452;&#25509;&#26681;&#25454;&#27979;&#35797;&#25968;&#25454;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#28304;&#21028;&#21035;&#27169;&#22411;&#20197;&#36866;&#24212;&#36825;&#20123;&#19981;&#26029;&#21464;&#21270;&#30340;&#39046;&#22495;&#12290;&#19968;&#31181;&#39640;&#24230;&#26377;&#25928;&#30340;CTTA&#26041;&#27861;&#28041;&#21450;&#24212;&#29992;&#36880;&#23618;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#24182;&#36873;&#25321;&#24615;&#22320;&#35843;&#25972;&#39044;&#35757;&#32451;&#23618;&#12290;&#28982;&#32780;&#65292;&#23427;&#21463;&#21040;&#39046;&#22495;&#36716;&#31227;&#20272;&#35745;&#19981;&#20934;&#30830;&#21644;&#30001;&#20266;&#26631;&#31614;&#24341;&#36215;&#30340;&#19981;&#20934;&#30830;&#24615;&#25152;&#22256;&#25200;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#23618;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26469;&#36873;&#25321;&#23618;&#65292;&#32780;&#26080;&#39035;&#20381;&#36182;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#21033;&#29992;&#26799;&#24230;&#30340;&#22823;&#23567;&#20316;&#20026;&#19968;&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;softmax&#36755;&#20986;&#19982;&#22343;&#21248;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#35745;&#31639;&#65292;&#20197;&#36873;&#25321;&#38656;&#35201;&#36827;&#19968;&#27493;&#36866;&#24212;&#30340;&#23618;&#12290;&#38543;&#21518;&#65292;&#20165;&#23646;&#20110;&#36825;&#20123;&#23618;&#30340;&#21442;&#25968;&#23558;&#34987;&#36827;&#19968;&#27493;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10650v1 Announce Type: cross  Abstract: Real-world vision models in dynamic environments face rapid shifts in domain distributions, leading to decreased recognition performance. Continual test-time adaptation (CTTA) directly adjusts a pre-trained source discriminative model to these changing domains using test data. A highly effective CTTA method involves applying layer-wise adaptive learning rates, and selectively adapting pre-trained layers. However, it suffers from the poor estimation of domain shift and the inaccuracies arising from the pseudo-labels. In this work, we aim to overcome these limitations by identifying layers through the quantification of model prediction uncertainty without relying on pseudo-labels. We utilize the magnitude of gradients as a metric, calculated by backpropagating the KL divergence between the softmax output and a uniform distribution, to select layers for further adaptation. Subsequently, for the parameters exclusively belonging to these se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#23558;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#27169;&#22411;&#19982;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#25552;&#39640;&#22270;&#20687;&#29702;&#35299;&#21644;&#20943;&#23569;&#22238;&#31572;&#38169;&#35823;&#25554;&#20837;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2401.17981</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#26816;&#27979;&#27169;&#22411;&#22686;&#24378;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#23558;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#27169;&#22411;&#19982;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#22312;&#25552;&#39640;&#22270;&#20687;&#29702;&#35299;&#21644;&#20943;&#23569;&#22238;&#31572;&#38169;&#35823;&#25554;&#20837;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#38598;&#25104;&#25991;&#26412;&#21644;&#22270;&#20687;&#27169;&#24577;&#26041;&#38754;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#20934;&#30830;&#35299;&#37322;&#32454;&#33410;&#35270;&#35273;&#20803;&#32032;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#27169;&#22411;&#19982;MLLMs&#32467;&#21512;&#65292;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#32454;&#31890;&#24230;&#22270;&#20687;&#29702;&#35299;&#65292;&#24182;&#20943;&#23569;&#22238;&#31572;&#20013;&#30340;&#38169;&#35823;&#25554;&#20837;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#26816;&#27979;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#36825;&#31181;&#34701;&#21512;&#23545;MLLMs&#30340;&#21407;&#22987;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#26816;&#27979;&#27169;&#22411;&#30340;&#21487;&#20114;&#25442;&#24615;&#12290;&#25105;&#20204;&#23545;LLaVA-1.5&#12289;DINO&#21644;PaddleOCRv2&#31561;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#23454;&#39564;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#25913;&#21892;&#20102;MLLMs&#22312;&#29305;&#23450;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#20445;&#25345;&#20102;&#23427;&#20204;&#30340;&#21407;&#22987;&#20248;&#21183;&#12290;&#36890;&#36807;&#22312;10&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22686;&#24378;&#30340;MLLMs&#22312;9&#20010;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#26631;&#20934;&#21270;&#24179;&#22343;&#24471;&#20998;&#25552;&#21319;&#39640;&#36798;12.99%&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive capabilities of Multimodal Large Language Models (MLLMs) in integrating text and image modalities, challenges remain in accurately interpreting detailed visual elements. This paper presents an empirical study on enhancing MLLMs with state-of-the-art (SOTA) object detection and Optical Character Recognition models to improve fine-grained image understanding and reduce hallucination in responses. Our research investigates the embedding-based infusion of detection information, the impact of such infusion on the MLLMs' original abilities, and the interchangeability of detection models. We conduct systematic experiments with models such as LLaVA-1.5, DINO, and PaddleOCRv2, revealing that our approach not only refines MLLMs' performance in specific visual tasks but also maintains their original strengths. The resulting enhanced MLLMs outperform SOTA models on 9 out of 10 benchmarks, achieving an improvement of up to 12.99% on the normalized average score, marking a not
&lt;/p&gt;</description></item></channel></rss>