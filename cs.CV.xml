<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10568</link><description>&lt;p&gt;
MoPE&#65306;&#36890;&#36807;Prompt&#19987;&#23478;&#28151;&#21512;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#35843;&#25972;&#24050;&#32463;&#35777;&#26126;&#22312;&#34701;&#21512;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21333;&#27169;&#22522;&#30784;&#27169;&#22411;&#26102;&#20855;&#26377;&#21442;&#25968;&#25928;&#29575;&#24615;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#38480;&#30340;&#36866;&#24212;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#19982;&#20854;&#20182;&#35843;&#25972;&#26041;&#27861;&#30456;&#27604;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#31616;&#21333;&#25552;&#31034;&#35299;&#24320;&#20197;&#33258;&#36866;&#24212;&#22320;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#24314;&#31435;&#22312;&#36825;&#31181;&#35299;&#24320;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Prompt&#19987;&#23478;&#30340;&#28151;&#21512;&#65288;MoPE&#65289;&#25216;&#26415;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#12290;MoPE&#21033;&#29992;&#22810;&#27169;&#24577;&#37197;&#23545;&#20808;&#39564;&#22312;&#27599;&#20010;&#23454;&#20363;&#22522;&#30784;&#19978;&#36335;&#30001;&#26368;&#26377;&#25928;&#30340;&#25552;&#31034;&#12290;&#19982;&#31616;&#21333;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#22522;&#20110;MoPE&#30340;&#26465;&#20214;&#25552;&#31034;&#23545;&#22810;&#27169;&#24577;&#34701;&#21512;&#20855;&#26377;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#24635;&#25968;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20010;&#19987;&#23478;&#36335;&#30001;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#23548;&#33268;&#19987;&#23478;&#30340;&#19981;&#26029;&#21457;&#23637;&#19987;&#38271;&#65292;&#19981;&#21516;&#19987;&#23478;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10568v1 Announce Type: cross  Abstract: Prompt-tuning has demonstrated parameter-efficiency in fusing unimodal foundation models for multimodal tasks. However, its limited adaptivity and expressiveness lead to suboptimal performance when compared with other tuning methods. In this paper, we address this issue by disentangling the vanilla prompts to adaptively capture dataset-level and instance-level features. Building upon this disentanglement, we introduce the mixture of prompt experts (MoPE) technique to enhance expressiveness. MoPE leverages multimodal pairing priors to route the most effective prompt on a per-instance basis. Compared to vanilla prompting, our MoPE-based conditional prompting exhibits greater expressiveness for multimodal fusion, scaling better with the training data and the overall number of trainable parameters. We also study a regularization term for expert routing, leading to emergent expert specialization, where different experts focus on different c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20013;&#38388;&#29305;&#24449;&#30340;&#32467;&#26500;&#65292;&#25581;&#31034;&#20102;&#28145;&#24230;&#32593;&#32476;&#22312;&#23618;&#32423;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28436;&#21270;&#27169;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#32447;&#24615;&#23618;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#36215;&#21040;&#20102;&#19982;&#28145;&#23618;&#38750;&#32447;&#24615;&#32593;&#32476;&#31867;&#20284;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.02960</link><description>&lt;p&gt;
&#36890;&#36807;&#23618;&#38388;&#29305;&#24449;&#21387;&#32553;&#21644;&#24046;&#21035;&#24615;&#23398;&#20064;&#29702;&#35299;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination. (arXiv:2311.02960v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20013;&#38388;&#29305;&#24449;&#30340;&#32467;&#26500;&#65292;&#25581;&#31034;&#20102;&#28145;&#24230;&#32593;&#32476;&#22312;&#23618;&#32423;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28436;&#21270;&#27169;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#32447;&#24615;&#23618;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#36215;&#21040;&#20102;&#19982;&#28145;&#23618;&#38750;&#32447;&#24615;&#32593;&#32476;&#31867;&#20284;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#35777;&#26126;&#26159;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#24847;&#20041;&#29305;&#24449;&#30340;&#19968;&#31181;&#39640;&#25928;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#32593;&#32476;&#22914;&#20309;&#22312;&#19981;&#21516;&#23618;&#32423;&#19978;&#36827;&#34892;&#31561;&#32423;&#29305;&#24449;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#30740;&#31350;&#20013;&#38388;&#29305;&#24449;&#30340;&#32467;&#26500;&#25581;&#31034;&#36825;&#20010;&#35868;&#22242;&#12290;&#21463;&#21040;&#25105;&#20204;&#23454;&#35777;&#21457;&#29616;&#30340;&#32447;&#24615;&#23618;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#27169;&#20223;&#38750;&#32447;&#24615;&#32593;&#32476;&#20013;&#28145;&#23618;&#30340;&#35282;&#33394;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#22914;&#20309;&#23558;&#36755;&#20837;&#25968;&#25454;&#36716;&#21270;&#20026;&#36755;&#20986;&#65292;&#36890;&#36807;&#30740;&#31350;&#35757;&#32451;&#21518;&#30340;&#27599;&#20010;&#23618;&#30340;&#36755;&#20986;&#65288;&#21363;&#29305;&#24449;&#65289;&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#34913;&#37327;&#20013;&#38388;&#29305;&#24449;&#30340;&#31867;&#20869;&#21387;&#32553;&#21644;&#31867;&#38388;&#24046;&#21035;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#36890;&#36807;&#23545;&#36825;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29305;&#24449;&#20174;&#27973;&#23618;&#21040;&#28145;&#23618;&#30340;&#28436;&#21464;&#36981;&#24490;&#30528;&#19968;&#31181;&#31616;&#21333;&#32780;&#37327;&#21270;&#30340;&#27169;&#24335;&#65292;&#21069;&#25552;&#26159;&#36755;&#20837;&#25968;&#25454;&#26159;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, deep learning has proven to be a highly effective tool for learning meaningful features from raw data. However, it remains an open question how deep networks perform hierarchical feature learning across layers. In this work, we attempt to unveil this mystery by investigating the structures of intermediate features. Motivated by our empirical findings that linear layers mimic the roles of deep layers in nonlinear networks for feature learning, we explore how deep linear networks transform input data into output by investigating the output (i.e., features) of each layer after training in the context of multi-class classification problems. Toward this goal, we first define metrics to measure within-class compression and between-class discrimination of intermediate features, respectively. Through theoretical analysis of these two metrics, we show that the evolution of features follows a simple and quantitative pattern from shallow to deep layers when the input data is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35821;&#20041;&#20016;&#23500;&#30340;&#25340;&#25509;&#25968;&#25454;&#38598;GreatSplicing&#65292;&#36890;&#36807;&#21253;&#25324;&#22823;&#37327;&#19981;&#21516;&#35821;&#20041;&#31867;&#21035;&#30340;&#25340;&#25509;&#21306;&#22495;&#65292;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#25340;&#25509;&#30165;&#36857;&#26816;&#27979;&#19978;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#35823;&#35782;&#29575;&#21644;&#26356;&#22909;&#30340;&#36328;&#25968;&#25454;&#38598;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10070</link><description>&lt;p&gt;
GreatSplicing: &#19968;&#20010;&#35821;&#20041;&#20016;&#23500;&#30340;&#25340;&#25509;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
GreatSplicing: A Semantically Rich Splicing Dataset. (arXiv:2310.10070v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35821;&#20041;&#20016;&#23500;&#30340;&#25340;&#25509;&#25968;&#25454;&#38598;GreatSplicing&#65292;&#36890;&#36807;&#21253;&#25324;&#22823;&#37327;&#19981;&#21516;&#35821;&#20041;&#31867;&#21035;&#30340;&#25340;&#25509;&#21306;&#22495;&#65292;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#25340;&#25509;&#30165;&#36857;&#26816;&#27979;&#19978;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#35823;&#35782;&#29575;&#21644;&#26356;&#22909;&#30340;&#36328;&#25968;&#25454;&#38598;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#26377;&#30340;&#25340;&#25509;&#20266;&#36896;&#25968;&#25454;&#38598;&#20013;&#65292;&#25340;&#25509;&#21306;&#22495;&#30340;&#35821;&#20041;&#21464;&#21270;&#19981;&#36275;&#23548;&#33268;&#35757;&#32451;&#30340;&#26816;&#27979;&#27169;&#22411;&#23545;&#35821;&#20041;&#29305;&#24449;&#30340;&#36807;&#25311;&#21512;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#32570;&#20047;&#21512;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#19981;&#21516;&#30340;&#26816;&#27979;&#26041;&#27861;&#22312;&#23454;&#39564;&#35774;&#32622;&#19978;&#26080;&#27861;&#36798;&#25104;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32039;&#36843;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;GreatSplicing&#65292;&#19968;&#20010;&#25163;&#21160;&#21019;&#24314;&#30340;&#20855;&#26377;&#22823;&#37327;&#21644;&#39640;&#36136;&#37327;&#30340;&#25340;&#25509;&#25968;&#25454;&#38598;&#12290;GreatSplicing&#21253;&#25324;5000&#24352;&#25340;&#25509;&#22270;&#20687;&#65292;&#24182;&#28085;&#30422;&#20102;335&#20010;&#19981;&#21516;&#30340;&#35821;&#20041;&#31867;&#21035;&#30340;&#25340;&#25509;&#21306;&#22495;&#65292;&#35753;&#31070;&#32463;&#32593;&#32476;&#26356;&#22909;&#22320;&#25235;&#20303;&#25340;&#25509;&#30165;&#36857;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;GreatSplicing&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#36739;&#20110;&#29616;&#26377;&#25968;&#25454;&#38598;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#35823;&#35782;&#29575;&#21644;&#26356;&#22909;&#30340;&#36328;&#25968;&#25454;&#38598;&#26816;&#27979;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;GreatSplicing&#21487;&#20379;&#25152;&#26377;&#30740;&#31350;&#30446;&#30340;&#20351;&#29992;&#65292;&#24182;&#21487;&#20174;www.greatsplicing.net&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
In existing splicing forgery datasets, the insufficient semantic varieties of spliced regions cause a problem that trained detection models overfit semantic features rather than splicing traces. Meanwhile, because of the absence of a reasonable dataset, different detection methods proposed cannot reach a consensus on experimental settings. To address these urgent issues, GreatSplicing, a manually created splicing dataset with a considerable amount and high quality, is proposed in this paper. GreatSplicing comprises 5,000 spliced images and covers spliced regions with 335 distinct semantic categories, allowing neural networks to grasp splicing traces better. Extensive experiments demonstrate that models trained on GreatSplicing exhibit minimal misidentification rates and superior cross-dataset detection capabilities compared to existing datasets. Furthermore, GreatSplicing is available for all research purposes and can be downloaded from www.greatsplicing.net.
&lt;/p&gt;</description></item></channel></rss>