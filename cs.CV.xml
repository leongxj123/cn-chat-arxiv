<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>Griffon v2&#36890;&#36807;&#24341;&#20837;&#39640;&#20998;&#36776;&#29575;&#32553;&#25918;&#21644;&#35270;&#35273;-&#35821;&#35328;&#20849;&#25351;&#65292;&#25552;&#21319;&#20102;&#22810;&#27169;&#24577;&#24863;&#30693;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#23567;&#23545;&#35937;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09333</link><description>&lt;p&gt;
Griffon v2&#65306;&#36890;&#36807;&#39640;&#20998;&#36776;&#29575;&#32553;&#25918;&#21644;&#35270;&#35273;-&#35821;&#35328;&#20849;&#25351;&#25512;&#36827;&#22810;&#27169;&#24577;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09333
&lt;/p&gt;
&lt;p&gt;
Griffon v2&#36890;&#36807;&#24341;&#20837;&#39640;&#20998;&#36776;&#29575;&#32553;&#25918;&#21644;&#35270;&#35273;-&#35821;&#35328;&#20849;&#25351;&#65292;&#25552;&#21319;&#20102;&#22810;&#27169;&#24577;&#24863;&#30693;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#23567;&#23545;&#35937;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#23545;&#35937;&#24863;&#30693;&#65292;&#20294;&#22270;&#20687;&#20998;&#36776;&#29575;&#30340;&#38480;&#21046;&#20173;&#28982;&#26159;&#36229;&#36234;&#22797;&#26434;&#21644;&#23494;&#38598;&#22330;&#26223;&#20013;&#29305;&#23450;&#20219;&#21153;&#19987;&#23478;&#34920;&#29616;&#30340;&#37325;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#39640;&#20998;&#36776;&#29575;&#36890;&#29992;&#27169;&#22411;&#65292;Griffon v2&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#35270;&#35273;&#21644;&#25991;&#26412;&#25552;&#31034;&#30340;&#28789;&#27963;&#23545;&#35937;&#24341;&#29992;&#12290;&#20026;&#20102;&#26377;&#25928;&#25552;&#39640;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#36731;&#37327;&#32423;&#30340;&#19979;&#37319;&#26679;&#25237;&#24433;&#20202;&#65292;&#20197;&#20811;&#26381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36755;&#20837;&#26631;&#35760;&#30340;&#32422;&#26463;&#12290;&#36825;&#31181;&#35774;&#35745;&#22266;&#26377;&#22320;&#20445;&#30041;&#20102;&#23436;&#25972;&#30340;&#19978;&#19979;&#25991;&#21644;&#32454;&#33410;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#24863;&#30693;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23567;&#23545;&#35937;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20026;&#27169;&#22411;&#37197;&#32622;&#20102;&#35270;&#35273;-&#35821;&#35328;&#20849;&#25351;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09333v1 Announce Type: cross  Abstract: Large Vision Language Models have achieved fine-grained object perception, but the limitation of image resolution remains a significant obstacle to surpass the performance of task-specific experts in complex and dense scenarios. Such limitation further restricts the model's potential to achieve nuanced visual and language referring in domains such as GUI Agents, Counting and \etc. To address this issue, we introduce a unified high-resolution generalist model, Griffon v2, enabling flexible object referring with visual and textual prompts. To efficiently scaling up image resolution, we design a simple and lightweight down-sampling projector to overcome the input tokens constraint in Large Language Models. This design inherently preserves the complete contexts and fine details, and significantly improves multimodal perception ability especially for small objects. Building upon this, we further equip the model with visual-language co-refer
&lt;/p&gt;</description></item><item><title>SARDet-100K&#26159;&#31532;&#19968;&#20010;COCO&#32423;&#21035;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035;SAR&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;SAR&#29289;&#20307;&#26816;&#27979;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#26174;&#33879;&#24046;&#24322;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.06534</link><description>&lt;p&gt;
SARDet-100K: &#38754;&#21521;&#22823;&#35268;&#27169;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798; SAR &#29289;&#20307;&#26816;&#27979;&#30340;&#24320;&#28304;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06534
&lt;/p&gt;
&lt;p&gt;
SARDet-100K&#26159;&#31532;&#19968;&#20010;COCO&#32423;&#21035;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035;SAR&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;SAR&#29289;&#20307;&#26816;&#27979;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#26174;&#33879;&#24046;&#24322;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#29289;&#20307;&#26816;&#27979;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#22240;&#20854;&#19981;&#21487;&#26367;&#20195;&#30340;&#20840;&#22825;&#20505;&#25104;&#20687;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#38754;&#20020;&#30528;&#26377;&#38480;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#20027;&#35201;&#21253;&#21547; &lt;2K &#24352;&#22270;&#20687;&#65292;&#19988;&#20165;&#21253;&#21547;&#21333;&#31867;&#21035;&#29289;&#20307;&#65289;&#21644;&#28304;&#20195;&#30721;&#19981;&#21487;&#35775;&#38382;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#38024;&#23545;&#22823;&#35268;&#27169; SAR &#29289;&#20307;&#26816;&#27979;&#30340;&#24320;&#28304;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598; SARDet-100K &#32467;&#26524;&#26159;&#23545; 10 &#20010;&#29616;&#26377; SAR &#26816;&#27979;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#20837;&#35843;&#30740;&#12289;&#25910;&#38598;&#21644;&#26631;&#20934;&#21270;&#30340;&#20135;&#29289;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SARDet-100K &#26159;&#26377;&#21490;&#20197;&#26469;&#31532;&#19968;&#20010;&#36798;&#21040; COCO &#27700;&#24179;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035; SAR &#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#20973;&#20511;&#36825;&#19968;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#24182;&#25581;&#31034;&#20102; SAR &#29289;&#20307;&#26816;&#27979;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06534v1 Announce Type: cross  Abstract: Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising &lt;2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#30721;&#20070;&#36741;&#21161;&#22270;&#20687;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#36890;&#36807;&#32852;&#21512;&#26500;&#24314;&#35821;&#20041;&#32534;&#35299;&#30721;&#22120;&#21644;&#30721;&#20070;&#12289;&#35774;&#35745;&#21521;&#37327;-&#32034;&#24341;&#21464;&#25442;&#22120;&#26469;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#65292;&#24182;&#19988;&#20511;&#21161;&#39640;&#36136;&#37327;&#30721;&#20070;&#24110;&#21161;Transformer&#65292;&#25552;&#39640;&#31995;&#32479;&#23545;&#25239;&#20449;&#36947;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16868</link><description>&lt;p&gt;
&#30001;Transformer&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#35821;&#20041;&#36890;&#20449;&#30340;&#30721;&#20070;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Codebook-enabled Generative End-to-end Semantic Communication Powered by Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#30721;&#20070;&#36741;&#21161;&#22270;&#20687;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#36890;&#36807;&#32852;&#21512;&#26500;&#24314;&#35821;&#20041;&#32534;&#35299;&#30721;&#22120;&#21644;&#30721;&#20070;&#12289;&#35774;&#35745;&#21521;&#37327;-&#32034;&#24341;&#21464;&#25442;&#22120;&#26469;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#65292;&#24182;&#19988;&#20511;&#21161;&#39640;&#36136;&#37327;&#30721;&#20070;&#24110;&#21161;Transformer&#65292;&#25552;&#39640;&#31995;&#32479;&#23545;&#25239;&#20449;&#36947;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30721;&#20070;&#30340;&#29983;&#25104;&#24335;&#35821;&#20041;&#36890;&#20449;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#24403;&#30721;&#20070;&#22312;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#20043;&#38388;&#20849;&#20139;&#26102;&#65292;&#21482;&#38656;&#35201;&#20256;&#36755;&#32034;&#24341;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30721;&#21521;&#37327;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#26410;&#24517;&#19982;&#23545;&#24212;&#30721;&#32034;&#24341;&#30340;&#36317;&#31163;&#30456;&#20851;&#65292;&#30721;&#20070;&#21551;&#29992;&#30340;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#24615;&#33021;&#23481;&#26131;&#21463;&#21040;&#20449;&#36947;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#25552;&#39640;&#31995;&#32479;&#23545;&#25239;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#38656;&#35201;&#20180;&#32454;&#35774;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#30721;&#20070;&#36741;&#21161;&#22270;&#20687;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#20854;&#20013;&#39318;&#20808;&#32852;&#21512;&#26500;&#24314;&#35821;&#20041;&#32534;&#35299;&#30721;&#22120;&#21644;&#30721;&#20070;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#21521;&#37327;-&#32034;&#24341;&#21464;&#25442;&#22120;&#65292;&#26681;&#25454;&#30721;&#20070;&#24341;&#23548;&#20197;&#28040;&#38500;&#20449;&#36947;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#24182;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#30721;&#20070;&#23545;Transformer&#30340;&#36741;&#21161;&#65292;&#25509;&#25910;&#31471;&#29983;&#25104;&#30340;&#22270;&#20687;&#25928;&#26524;&#20248;&#20110;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16868v1 Announce Type: cross  Abstract: Codebook-based generative semantic communication attracts increasing attention, since only indices are required to be transmitted when the codebook is shared between transmitter and receiver. However, due to the fact that the semantic relations among code vectors are not necessarily related to the distance of the corresponding code indices, the performance of the codebook-enabled semantic communication system is susceptible to the channel noise. Thus, how to improve the system robustness against the noise requires careful design. This paper proposes a robust codebook-assisted image semantic communication system, where semantic codec and codebook are first jointly constructed, and then vector-to-index transformer is designed guided by the codebook to eliminate the effects of channel noise, and achieve image generation. Thanks to the assistance of the high-quality codebook to the Transformer, the generated images at the receiver outperfo
&lt;/p&gt;</description></item></channel></rss>