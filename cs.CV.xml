<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24418;&#24335;&#21270;&#8220;&#20462;&#27491;&#26426;&#22120;&#28040;&#38500;&#8221;&#26469;&#35299;&#20915;&#21463;&#26410;&#30693;&#25805;&#32437;&#24433;&#21709;&#30340;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#38382;&#39064;&#65292;&#21487;&#33021;&#20165;&#30693;&#36947;&#19968;&#37096;&#20998;&#21463;&#24433;&#21709;&#26679;&#26412;&#12290;&#21457;&#29616;&#32416;&#27491;&#28040;&#38500;&#38382;&#39064;&#19982;&#20256;&#32479;&#20197;&#38544;&#31169;&#20026;&#23548;&#21521;&#30340;&#28040;&#38500;&#26041;&#27861;&#26377;&#26174;&#33879;&#19981;&#21516;&#30340;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.14015</link><description>&lt;p&gt;
&#20462;&#27491;&#26426;&#22120;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
Corrective Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24418;&#24335;&#21270;&#8220;&#20462;&#27491;&#26426;&#22120;&#28040;&#38500;&#8221;&#26469;&#35299;&#20915;&#21463;&#26410;&#30693;&#25805;&#32437;&#24433;&#21709;&#30340;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#38382;&#39064;&#65292;&#21487;&#33021;&#20165;&#30693;&#36947;&#19968;&#37096;&#20998;&#21463;&#24433;&#21709;&#26679;&#26412;&#12290;&#21457;&#29616;&#32416;&#27491;&#28040;&#38500;&#38382;&#39064;&#19982;&#20256;&#32479;&#20197;&#38544;&#31169;&#20026;&#23548;&#21521;&#30340;&#28040;&#38500;&#26041;&#27861;&#26377;&#26174;&#33879;&#19981;&#21516;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#38754;&#20020;&#25968;&#25454;&#23436;&#25972;&#24615;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#20102;&#22823;&#35268;&#27169;&#30340;&#20174;&#20114;&#32852;&#32593;&#20013;&#33719;&#21462;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#26524;&#27169;&#22411;&#24320;&#21457;&#32773;&#21457;&#29616;&#26576;&#20123;&#25968;&#25454;&#34987;&#31713;&#25913;&#25110;&#38169;&#35823;&#65292;&#20182;&#20204;&#21487;&#20197;&#37319;&#21462;&#20160;&#20040;&#25514;&#26045;&#12290;&#36825;&#20123;&#34987;&#31713;&#25913;&#30340;&#25968;&#25454;&#20250;&#23548;&#33268;&#19981;&#21033;&#24433;&#21709;&#65292;&#22914;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#26679;&#26412;&#30340;&#25915;&#20987;&#12289;&#31995;&#32479;&#24615;&#20559;&#35265;&#65292;&#20197;&#21450;&#22312;&#26576;&#20123;&#36755;&#20837;&#39046;&#22495;&#30340;&#20934;&#30830;&#24230;&#38477;&#20302;&#12290;&#36890;&#24120;&#65292;&#24182;&#38750;&#25152;&#26377;&#34987;&#31713;&#25913;&#30340;&#35757;&#32451;&#26679;&#26412;&#37117;&#26159;&#24050;&#30693;&#30340;&#65292;&#32780;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#20195;&#34920;&#24615;&#30340;&#21463;&#24433;&#21709;&#25968;&#25454;&#34987;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14015v1 Announce Type: cross  Abstract: Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet. We study what model developers can do if they detect that some data was manipulated or incorrect. Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains. Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged.   We formalize "Corrective Machine Unlearning" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples. We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning. We find most existing unlearning methods, including the gold-standard
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FlashTex&#26041;&#27861;&#65292;&#22522;&#20110;LightControlNet&#23454;&#29616;&#20102;&#24555;&#36895;&#33258;&#21160;&#21270;3D&#32593;&#26684;&#32441;&#29702;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#29031;&#26126;&#19982;&#34920;&#38754;&#26448;&#36136;&#30340;&#35299;&#32806;&#65292;&#20351;&#24471;&#32593;&#26684;&#33021;&#22815;&#22312;&#20219;&#20309;&#29031;&#26126;&#29615;&#22659;&#19979;&#27491;&#30830;&#37325;&#29031;&#21644;&#28210;&#26579;</title><link>https://arxiv.org/abs/2402.13251</link><description>&lt;p&gt;
FlashTex&#65306;&#20855;&#26377;LightControlNet&#30340;&#24555;&#36895;&#21487;&#37325;&#22609;&#32593;&#26684;&#32441;&#29702;
&lt;/p&gt;
&lt;p&gt;
FlashTex: Fast Relightable Mesh Texturing with LightControlNet
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13251
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FlashTex&#26041;&#27861;&#65292;&#22522;&#20110;LightControlNet&#23454;&#29616;&#20102;&#24555;&#36895;&#33258;&#21160;&#21270;3D&#32593;&#26684;&#32441;&#29702;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#29031;&#26126;&#19982;&#34920;&#38754;&#26448;&#36136;&#30340;&#35299;&#32806;&#65292;&#20351;&#24471;&#32593;&#26684;&#33021;&#22815;&#22312;&#20219;&#20309;&#29031;&#26126;&#29615;&#22659;&#19979;&#27491;&#30830;&#37325;&#29031;&#21644;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21160;&#20026;3D&#32593;&#26684;&#21019;&#24314;&#32441;&#29702;&#36153;&#26102;&#36153;&#21147;&#65292;&#21363;&#20351;&#23545;&#20110;&#19987;&#23478;&#35270;&#35273;&#20869;&#23481;&#21019;&#24314;&#32773;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#26041;&#27861;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#25552;&#31034;&#33258;&#21160;&#20026;&#36755;&#20837;&#30340;3D&#32593;&#26684;&#30528;&#33394;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#29031;&#26126;&#19982;&#34920;&#38754;&#26448;&#36136;/&#21453;&#23556;&#22312;&#29983;&#25104;&#30340;&#32441;&#29702;&#20013;&#35299;&#32806;&#65292;&#20197;&#20415;&#32593;&#26684;&#21487;&#20197;&#22312;&#20219;&#20309;&#29031;&#26126;&#29615;&#22659;&#20013;&#27491;&#30830;&#37325;&#29031;&#21644;&#28210;&#26579;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LightControlNet&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;ControlNet&#26550;&#26500;&#30340;&#26032;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#20801;&#35768;&#23558;&#25152;&#38656;&#29031;&#26126;&#35268;&#26684;&#20316;&#20026;&#23545;&#27169;&#22411;&#30340;&#26465;&#20214;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#25991;&#26412;&#21040;&#32441;&#29702;&#31649;&#36947;&#28982;&#21518;&#20998;&#20004;&#20010;&#38454;&#27573;&#26500;&#24314;&#32441;&#29702;&#12290;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;LightControlNet&#29983;&#25104;&#32593;&#26684;&#30340;&#19968;&#32452;&#31232;&#30095;&#30340;&#35270;&#35273;&#19968;&#33268;&#30340;&#21442;&#32771;&#35270;&#22270;&#12290;&#31532;&#20108;&#38454;&#27573;&#24212;&#29992;&#22522;&#20110;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#30340;&#32441;&#29702;&#20248;&#21270;&#65292;&#36890;&#36807;LightControlNet&#26469;&#25552;&#39640;&#32441;&#29702;&#36136;&#37327;&#21516;&#26102;&#35299;&#32806;&#34920;&#38754;&#26448;&#36136;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13251v1 Announce Type: cross  Abstract: Manually creating textures for 3D meshes is time-consuming, even for expert visual content creators. We propose a fast approach for automatically texturing an input 3D mesh based on a user-provided text prompt. Importantly, our approach disentangles lighting from surface material/reflectance in the resulting texture so that the mesh can be properly relit and rendered in any lighting environment. We introduce LightControlNet, a new text-to-image model based on the ControlNet architecture, which allows the specification of the desired lighting as a conditioning image to the model. Our text-to-texture pipeline then constructs the texture in two stages. The first stage produces a sparse set of visually consistent reference views of the mesh using LightControlNet. The second stage applies a texture optimization based on Score Distillation Sampling (SDS) that works with LightControlNet to increase the texture quality while disentangling surf
&lt;/p&gt;</description></item><item><title>&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65288;MMoE&#65289;&#23618;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#38024;&#23545;&#35270;&#35273;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#19987;&#23478;&#29305;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#31163;&#25955;&#19987;&#23478;&#36335;&#30001;&#21644;&#36807;&#39640;&#25512;&#29702;&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.12550</link><description>&lt;p&gt;
&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65306;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#19987;&#23478;&#29305;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12550
&lt;/p&gt;
&lt;p&gt;
&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65288;MMoE&#65289;&#23618;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#38024;&#23545;&#35270;&#35273;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#19987;&#23478;&#29305;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#31163;&#25955;&#19987;&#23478;&#36335;&#30001;&#21644;&#36807;&#39640;&#25512;&#29702;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#33539;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#23558;&#38590;&#20197;&#29702;&#35299;&#30340;&#23494;&#38598;&#23618;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#27169;&#22359;&#21270;&#30340;&#35745;&#31639;&#65292;&#36890;&#24120;&#26356;&#26131;&#20110;&#20154;&#31867;&#35299;&#37322;&#12289;&#35843;&#35797;&#21644;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#25193;&#23637;&#19987;&#23478;&#25968;&#37327;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20197;&#23454;&#29616;&#36275;&#22815;&#31934;&#32454;&#30340;&#19987;&#19994;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65288;MMoE&#65289;&#23618;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#25918;&#22312;&#35270;&#35273;&#27169;&#22411;&#19978;&#12290;MMoE&#23618;&#23436;&#20840;&#20197;&#22240;&#24335;&#21270;&#24418;&#24335;&#23545;&#24222;&#22823;&#30340;&#26435;&#37325;&#24352;&#37327;&#36827;&#34892;&#38544;&#24335;&#35745;&#31639;&#12290;&#22240;&#27492;&#65292;MMoEs&#26082;&#36991;&#20813;&#20102;&#22312;&#27969;&#34892;&#30340;&#8220;&#31232;&#30095;&#8221;MoE&#27169;&#22411;&#20013;&#31163;&#25955;&#19987;&#23478;&#36335;&#30001;&#25152;&#36896;&#25104;&#30340;&#38382;&#39064;&#65292;&#21448;&#19981;&#20250;&#24341;&#36215;&#8220;&#36719;&#8221;MoE&#26367;&#20195;&#26041;&#26696;&#20013;&#36807;&#39640;&#30340;&#25512;&#29702;&#26102;&#38388;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#21453;&#20107;&#23454;&#24178;&#39044;&#65292;&#25552;&#20379;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#25193;&#23637;MMoE&#23618;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12550v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) paradigm provides a powerful way to decompose inscrutable dense layers into smaller, modular computations often more amenable to human interpretation, debugging, and editability. A major problem however lies in the computational cost of scaling the number of experts to achieve sufficiently fine-grained specialization. In this paper, we propose the Multilinear Mixutre of Experts (MMoE) layer to address this, focusing on vision models. MMoE layers perform an implicit computation on prohibitively large weight tensors entirely in factorized form. Consequently, MMoEs both (1) avoid the issues incurred through the discrete expert routing in the popular 'sparse' MoE models, yet (2) do not incur the restrictively high inference-time costs of 'soft' MoE alternatives. We present both qualitative and quantitative evidence (through visualization and counterfactual interventions respectively) that scaling MMoE layers wh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12424</link><description>&lt;p&gt;
&#34920;&#26684;&#20316;&#20026;&#22270;&#29255;&#65311;&#25506;&#35752;LLM&#22312;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#19978;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#25968;&#25454;&#26684;&#24335;&#30740;&#31350;&#20102;&#21508;&#31181;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#20845;&#20010;&#38024;&#23545;&#19982;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#22914;&#38382;&#31572;&#21644;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#39318;&#27425;&#20171;&#32461;&#20102;LLM&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#19978;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#31181;&#22522;&#20110;&#25991;&#26412;&#21644;&#19977;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#34920;&#31034;&#21644;&#25552;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.06165</link><description>&lt;p&gt;
&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#34920;&#31034;&#26469;&#36827;&#34892;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Contrastive Feature Representations for Facial Action Unit Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06165
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#65288;AU&#65289;&#26816;&#27979;&#30340;&#20027;&#35201;&#26041;&#27861;&#28041;&#21450;&#30417;&#30563;&#30340;&#22810;&#26631;&#31614;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24120;&#24120;&#23545;AU&#30340;&#20687;&#32032;&#32423;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#25552;&#20986;&#20102;&#24456;&#22823;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#22122;&#22768;AU&#26631;&#31614;&#65292;&#36825;&#31181;&#20570;&#27861;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#22686;&#24378;&#12290;&#30446;&#26631;&#26159;&#22312;AU&#26816;&#27979;&#39046;&#22495;&#20013;&#25670;&#33073;&#20256;&#32479;&#30340;&#20687;&#32032;&#32423;&#23398;&#20064;&#33539;&#24335;&#65292;&#33719;&#24471;&#21028;&#21035;&#29305;&#24449;&#12290;&#20026;&#20102;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#30417;&#30563;&#20449;&#21495;&#12290;&#36825;&#31181;&#22686;&#24378;&#26159;&#36890;&#36807;&#27491;&#26679;&#26412;&#25277;&#26679;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#27491;&#26679;&#26412;&#23545;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#27599;&#20010;AU&#31867;&#22411;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we empl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20799;&#31461;&#30340;&#35270;&#35282;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#38271;&#26102;&#38388;&#30340;&#22836;&#25140;&#24335;&#25668;&#20687;&#35760;&#24405;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#20419;&#36827;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#21160;&#27010;&#24565;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.00300</link><description>&lt;p&gt;
&#20174;&#20799;&#31461;&#35270;&#35282;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35270;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning of video representations from a child's perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20799;&#31461;&#30340;&#35270;&#35282;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#38271;&#26102;&#38388;&#30340;&#22836;&#25140;&#24335;&#25668;&#20687;&#35760;&#24405;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#20419;&#36827;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#21160;&#27010;&#24565;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#36890;&#36807;&#20960;&#24180;&#30340;&#33258;&#25105;&#35270;&#35273;&#32463;&#39564;&#23398;&#20064;&#21040;&#20102;&#24378;&#22823;&#30340;&#19990;&#30028;&#20869;&#37096;&#27169;&#22411;&#12290;&#36825;&#20123;&#20869;&#37096;&#27169;&#22411;&#33021;&#21542;&#36890;&#36807;&#20799;&#31461;&#30340;&#35270;&#35273;&#20307;&#39564;&#21644;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#65292;&#36824;&#26159;&#38656;&#35201;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#65311;&#26368;&#36817;&#65292;&#22312;&#25910;&#38598;&#22823;&#35268;&#27169;&#12289;&#32437;&#21521;&#30340;&#21457;&#23637;&#29616;&#23454;&#35270;&#39057;&#25968;&#25454;&#38598;&#20197;&#21450;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#36827;&#23637;&#20351;&#25105;&#20204;&#33021;&#22815;&#24320;&#22987;&#25506;&#35752;&#36825;&#20010;&#26412;&#36136;&#19982;&#20859;&#32946;&#20043;&#38388;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20851;&#27880;&#22522;&#20110;&#22270;&#20687;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#21644;&#21487;&#20197;&#20174;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#30340;&#35270;&#35273;&#33021;&#21147;&#65288;&#20363;&#22914;&#30446;&#26631;&#35782;&#21035;&#65289;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#19990;&#30028;&#30340;&#26102;&#38388;&#24615;&#36136;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20799;&#31461;&#26089;&#26399;&#21457;&#23637;&#38454;&#27573;&#65288;6-31&#20010;&#26376;&#65289;&#20174;&#20799;&#31461;&#30340;&#22836;&#25140;&#24335;&#25668;&#20687;&#35760;&#24405;&#20013;&#35757;&#32451;&#33258;&#30417;&#30563;&#35270;&#39057;&#27169;&#22411;&#12290;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#20419;&#36827;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#21160;&#27010;&#24565;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children learn powerful internal models of the world around them from a few years of egocentric visual experience. Can such internal models be learned from a child's visual experience with highly generic learning algorithms or do they require strong inductive biases? Recent advances in collecting large-scale, longitudinal, developmentally realistic video datasets and generic self-supervised learning (SSL) algorithms are allowing us to begin to tackle this nature vs. nurture question. However, existing work typically focuses on image-based SSL algorithms and visual capabilities that can be learned from static images (e.g. object recognition), thus ignoring temporal aspects of the world. To close this gap, here we train self-supervised video models on longitudinal, egocentric headcam recordings collected from a child over a two year period in their early development (6-31 months). The resulting models are highly effective at facilitating the learning of action concepts from a small numbe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#34892;&#20154;&#26816;&#27979;&#22120;&#30340;&#20844;&#24179;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#37325;&#35201;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2308.02935</link><description>&lt;p&gt;
&#25581;&#31034;&#30450;&#28857;&#65306;&#23545;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#20844;&#24179;&#24615;&#30340;&#20851;&#38190;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Blind Spots: A Critical Examination of Fairness in Autonomous Driving Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.02935
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#34892;&#20154;&#26816;&#27979;&#22120;&#30340;&#20844;&#24179;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#37325;&#35201;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#24050;&#32463;&#25193;&#23637;&#20102;&#26234;&#33021;&#36710;&#36742;&#29289;&#32852;&#32593;&#30340;&#33539;&#22260;&#65292;&#24182;&#25104;&#20026;Web&#29983;&#24577;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#31867;&#20284;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;Web&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20844;&#24179;&#24615;&#23545;&#20110;&#30830;&#20445;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#39640;&#36136;&#37327;&#26159;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#22312;&#20854;&#20013;&#30340;&#34892;&#20154;&#26816;&#27979;&#22120;&#30340;&#32972;&#26223;&#19979;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#34892;&#20154;&#26816;&#27979;&#22120;&#20844;&#24179;&#24615;&#30340;&#32508;&#21512;&#35780;&#20272;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#20986;&#29616;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20843;&#31181;&#34987;&#24191;&#27867;&#25506;&#32034;&#30340;DL&#34892;&#20154;&#26816;&#27979;&#22120;&#22312;&#20154;&#21475;&#32479;&#35745;&#23398;&#32676;&#20307;&#20043;&#38388;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#23454;&#29616;&#24443;&#24213;&#30340;&#20844;&#24179;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#20026;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#27880;&#37322;&#65292;&#20849;&#28041;&#21450;8,311&#24352;&#22270;&#20687;&#65292;16,070&#20010;&#24615;&#21035;&#26631;&#31614;&#65292;20,115&#20010;&#24180;&#40836;&#26631;&#31614;&#21644;3,513&#20010;&#32932;&#33394;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#37325;&#35201;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.02935v2 Announce Type: replace-cross  Abstract: Autonomous driving systems have extended the spectrum of Web of Things for intelligent vehicles and have become an important component of the Web ecosystem. Similar to traditional Web-based applications, fairness is an essential aspect for ensuring the high quality of autonomous driving systems, particularly in the context of pedestrian detectors within them. However, there is an absence in the literature of a comprehensive assessment of the fairness of current Deep Learning (DL)-based pedestrian detectors. To fill the gap, we evaluate eight widely-explored DL-based pedestrian detectors across demographic groups on large-scale real-world datasets. To enable a thorough fairness evaluation, we provide extensive annotations for the datasets, resulting in 8,311 images with 16,070 gender labels, 20,115 age labels, and 3,513 skin tone labels. Our findings reveal significant fairness issues related to age. The undetected proportions f
&lt;/p&gt;</description></item><item><title>D$^3$Fields&#26159;&#19968;&#20010;&#21160;&#24577;&#30340;&#19977;&#32500;&#25551;&#36848;&#31526;&#22330;&#65292;&#23558;&#24213;&#23618;&#19977;&#32500;&#29615;&#22659;&#30340;&#21160;&#24577;&#29305;&#24615;&#20197;&#21450;&#35821;&#20041;&#29305;&#24449;&#21644;&#23454;&#20363;&#25513;&#27169;&#32534;&#30721;&#36215;&#26469;&#12290;&#23427;&#21487;&#20197;&#28789;&#27963;&#22320;&#20351;&#29992;&#19981;&#21516;&#32972;&#26223;&#12289;&#39118;&#26684;&#21644;&#23454;&#20363;&#30340;&#20108;&#32500;&#22270;&#20687;&#25351;&#23450;&#30446;&#26631;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#21487;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.16118</link><description>&lt;p&gt;
D$^3$Fields: &#21160;&#24577;&#19977;&#32500;&#25551;&#36848;&#31526;&#22330;&#29992;&#20110;&#38646;&#26679;&#26412;&#21487;&#27867;&#21270;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation. (arXiv:2309.16118v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16118
&lt;/p&gt;
&lt;p&gt;
D$^3$Fields&#26159;&#19968;&#20010;&#21160;&#24577;&#30340;&#19977;&#32500;&#25551;&#36848;&#31526;&#22330;&#65292;&#23558;&#24213;&#23618;&#19977;&#32500;&#29615;&#22659;&#30340;&#21160;&#24577;&#29305;&#24615;&#20197;&#21450;&#35821;&#20041;&#29305;&#24449;&#21644;&#23454;&#20363;&#25513;&#27169;&#32534;&#30721;&#36215;&#26469;&#12290;&#23427;&#21487;&#20197;&#28789;&#27963;&#22320;&#20351;&#29992;&#19981;&#21516;&#32972;&#26223;&#12289;&#39118;&#26684;&#21644;&#23454;&#20363;&#30340;&#20108;&#32500;&#22270;&#20687;&#25351;&#23450;&#30446;&#26631;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#21487;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#34920;&#31034;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#31995;&#32479;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#19968;&#20010;&#29702;&#24819;&#30340;&#34920;&#31034;&#24212;&#35813;&#26159;&#19977;&#32500;&#30340;&#12289;&#21160;&#24577;&#30340;&#21644;&#35821;&#20041;&#21270;&#30340;&#65292;&#20197;&#28385;&#36275;&#19981;&#21516;&#25805;&#20316;&#20219;&#21153;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#24448;&#24448;&#21516;&#26102;&#32570;&#20047;&#36825;&#19977;&#20010;&#23646;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;D$^3$Fields&#21160;&#24577;&#19977;&#32500;&#25551;&#36848;&#31526;&#22330;&#12290;&#36825;&#20123;&#22330;&#25429;&#25417;&#20102;&#24213;&#23618;&#19977;&#32500;&#29615;&#22659;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#32534;&#30721;&#20102;&#35821;&#20041;&#29305;&#24449;&#21644;&#23454;&#20363;&#25513;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#24037;&#20316;&#21306;&#22495;&#20013;&#30340;&#20219;&#24847;&#19977;&#32500;&#28857;&#25237;&#24433;&#21040;&#22810;&#35270;&#35282;&#30340;&#20108;&#32500;&#35270;&#35273;&#35266;&#23519;&#20013;&#65292;&#24182;&#25554;&#20540;&#20174;&#22522;&#26412;&#27169;&#22411;&#20013;&#24471;&#21040;&#30340;&#29305;&#24449;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#34701;&#21512;&#25551;&#36848;&#31526;&#22330;&#21487;&#20197;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#32972;&#26223;&#12289;&#39118;&#26684;&#21644;&#23454;&#20363;&#30340;&#20108;&#32500;&#22270;&#20687;&#28789;&#27963;&#22320;&#25351;&#23450;&#30446;&#26631;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#25551;&#36848;&#31526;&#22330;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#23558;&#25105;&#20204;&#30340;&#34920;&#31034;&#24212;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#22330;&#26223;&#21644;&#27169;&#25311;&#20013;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scene representation has been a crucial design choice in robotic manipulation systems. An ideal representation should be 3D, dynamic, and semantic to meet the demands of diverse manipulation tasks. However, previous works often lack all three properties simultaneously. In this work, we introduce D$^3$Fields dynamic 3D descriptor fields. These fields capture the dynamics of the underlying 3D environment and encode both semantic features and instance masks. Specifically, we project arbitrary 3D points in the workspace onto multi-view 2D visual observations and interpolate features derived from foundational models. The resulting fused descriptor fields allow for flexible goal specifications using 2D images with varied contexts, styles, and instances. To evaluate the effectiveness of these descriptor fields, we apply our representation to a wide range of robotic manipulation tasks in a zero-shot manner. Through extensive evaluation in both real-world scenarios and simulations, we demonst
&lt;/p&gt;</description></item><item><title>HyperDreamBooth&#26159;&#19968;&#20010;&#36229;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#19968;&#20010;&#20154;&#30340;&#21333;&#24352;&#22270;&#29255;&#20013;&#24555;&#36895;&#29983;&#25104;&#20010;&#24615;&#21270;&#26435;&#37325;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22810;&#31181;&#32972;&#26223;&#21644;&#39118;&#26684;&#19979;&#21512;&#25104;&#19968;&#20010;&#20154;&#30340;&#38754;&#37096;&#65292;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#24182;&#21516;&#26102;&#20445;&#30041;&#23545;&#22810;&#26679;&#21270;&#39118;&#26684;&#21644;&#35821;&#20041;&#20462;&#25913;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.06949</link><description>&lt;p&gt;
HyperDreamBooth&#65306;&#29992;&#20110;&#24555;&#36895;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#36229;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models. (arXiv:2307.06949v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06949
&lt;/p&gt;
&lt;p&gt;
HyperDreamBooth&#26159;&#19968;&#20010;&#36229;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#19968;&#20010;&#20154;&#30340;&#21333;&#24352;&#22270;&#29255;&#20013;&#24555;&#36895;&#29983;&#25104;&#20010;&#24615;&#21270;&#26435;&#37325;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22810;&#31181;&#32972;&#26223;&#21644;&#39118;&#26684;&#19979;&#21512;&#25104;&#19968;&#20010;&#20154;&#30340;&#38754;&#37096;&#65292;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#24182;&#21516;&#26102;&#20445;&#30041;&#23545;&#22810;&#26679;&#21270;&#39118;&#26684;&#21644;&#35821;&#20041;&#20462;&#25913;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#24050;&#32463;&#25104;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#32972;&#26223;&#21644;&#39118;&#26684;&#19979;&#21512;&#25104;&#20010;&#20307;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#12290;&#28982;&#32780;&#65292;&#20010;&#24615;&#21270;&#36807;&#31243;&#22312;&#26102;&#38388;&#21644;&#20869;&#23384;&#38656;&#27714;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#27599;&#20010;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#24494;&#35843;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#26102;&#38388;&#25237;&#20837;&#65292;&#20026;&#27599;&#20010;&#20027;&#39064;&#23384;&#20648;&#19968;&#20010;&#20010;&#24615;&#21270;&#27169;&#22411;&#20250;&#23545;&#23384;&#20648;&#23481;&#37327;&#25552;&#20986;&#35201;&#27714;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperDreamBooth-&#19968;&#31181;&#33021;&#22815;&#20174;&#19968;&#20010;&#20154;&#30340;&#21333;&#24352;&#22270;&#29255;&#26377;&#25928;&#29983;&#25104;&#19968;&#32452;&#20010;&#24615;&#21270;&#26435;&#37325;&#30340;&#36229;&#32593;&#32476;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#26435;&#37325;&#32452;&#21512;&#21040;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#24182;&#25645;&#37197;&#24555;&#36895;&#24494;&#35843;&#65292;HyperDreamBooth&#33021;&#22815;&#20197;&#22810;&#31181;&#32972;&#26223;&#21644;&#39118;&#26684;&#29983;&#25104;&#19968;&#20010;&#20154;&#30340;&#38754;&#37096;&#65292;&#20445;&#25345;&#39640;&#20027;&#39064;&#32454;&#33410;&#21516;&#26102;&#20063;&#20445;&#25345;&#27169;&#22411;&#23545;&#22810;&#26679;&#21270;&#39118;&#26684;&#21644;&#35821;&#20041;&#20462;&#25913;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#32422;50&#20493;&#20307;&#29616;&#20102;&#38754;&#37096;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome these challenges, we propose HyperDreamBooth-a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications. Our method achieves personalization on faces in roughly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#36793;&#32536;&#38477;&#35299;&#30340;&#35268;&#21017;&#22810;&#36793;&#24418;&#26102;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#65292;&#21457;&#29616;&#23384;&#22312;&#22522;&#26412;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20154;&#26426;&#35270;&#35273;&#24046;&#36317;&#30340;&#21478;&#19968;&#20010;&#35282;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.04955</link><description>&lt;p&gt;
&#35770;&#31070;&#32463;&#32593;&#32476;&#23545;&#38477;&#35299;&#22810;&#36793;&#24418;&#30340;&#24863;&#30693;&#23384;&#22312;&#30340;&#22522;&#26412;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Degraded Polygons Raise Fundamental Questions of Neural Network Perception. (arXiv:2306.04955v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#35782;&#21035;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#36793;&#32536;&#38477;&#35299;&#30340;&#35268;&#21017;&#22810;&#36793;&#24418;&#26102;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#65292;&#21457;&#29616;&#23384;&#22312;&#22522;&#26412;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20154;&#26426;&#35270;&#35273;&#24046;&#36317;&#30340;&#21478;&#19968;&#20010;&#35282;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#24448;&#24448;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#19981;&#19968;&#33268;&#30340;&#34892;&#20026;&#65306;&#20174;&#23545;&#25239;&#25915;&#20987;&#21040;&#22270;&#20687;&#25439;&#22351;&#65292;&#28145;&#24230;&#23398;&#20064;&#35270;&#35273;&#27169;&#22411;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#28982;&#32780;&#20154;&#31867;&#21364;&#33021;&#22815;&#24456;&#22909;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#30740;&#31350;&#20102;&#20154;&#26426;&#35270;&#35273;&#24046;&#36317;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#24674;&#22797;&#21463;&#25439;&#22270;&#20687;&#30340;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#22312;&#20154;&#31867;&#35270;&#35273;&#30340;&#8220;&#35782;&#21035;&#32452;&#20214;&#8221;&#29702;&#35770;&#20013;&#39318;&#27425;&#24341;&#20837;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#31867;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#36793;&#32536;&#38477;&#35299;&#30340;&#35268;&#21017;&#22810;&#36793;&#24418;&#26102;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#33258;&#21160;&#21270;&#24418;&#29366;&#21487;&#24674;&#22797;&#24615;&#27979;&#35797;&#65292;&#24555;&#36895;&#29983;&#25104;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#23558;&#21382;&#21490;&#19978;&#25163;&#21160;&#21019;&#24314;&#22270;&#20687;&#21487;&#24674;&#22797;&#24615;&#23454;&#39564;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#29616;&#20195;&#21270;&#25913;&#36827;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#22810;&#36793;&#24418;&#30340;&#33021;&#21147;&#20197;&#21450;&#20854;&#30456;&#20851;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well-known that modern computer vision systems often exhibit behaviors misaligned with those of humans: from adversarial attacks to image corruptions, deep learning vision models suffer in a variety of settings that humans capably handle. In light of these phenomena, here we introduce another, orthogonal perspective studying the human-machine vision gap. We revisit the task of recovering images under degradation, first introduced over 30 years ago in the Recognition-by-Components theory of human vision. Specifically, we study the performance and behavior of neural networks on the seemingly simple task of classifying regular polygons at varying orders of degradation along their perimeters. To this end, we implement the Automated Shape Recoverability Test for rapidly generating large-scale datasets of perimeter-degraded regular polygons, modernizing the historically manual creation of image recoverability experiments. We then investigate the capacity of neural networks to recognize
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.10947</link><description>&lt;p&gt;
&#20851;&#20110;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#36777;&#25252;
&lt;/p&gt;
&lt;p&gt;
In Defense of Pure 16-bit Floating-Point Neural Networks. (arXiv:2305.10947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#25152;&#38656;&#30340;&#20301;&#25968;&#26159;&#38750;&#24120;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21152;&#24555;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20197;&#24320;&#21457;&#21033;&#29992;&#26356;&#20302;&#31934;&#24230;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#27604;&#22914;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#32431;16&#20301;&#28014;&#28857;&#35774;&#32622;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;&#36896;&#25104;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#28014;&#28857;&#35823;&#24046;&#21644;&#23481;&#24525;&#24230;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#23494;&#20999;&#36924;&#36817;&#32467;&#26524;&#30340;&#26465;&#20214;&#12290;&#36825;&#31181;&#29702;&#35770;&#25506;&#32034;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reducing the number of bits needed to encode the weights and activations of neural networks is highly desirable as it speeds up their training and inference time while reducing memory consumption. For these reasons, research in this area has attracted significant attention toward developing neural networks that leverage lower-precision computing, such as mixed-precision training. Interestingly, none of the existing approaches has investigated pure 16-bit floating-point settings. In this paper, we shed light on the overlooked efficiency of pure 16-bit floating-point neural networks. As such, we provide a comprehensive theoretical analysis to investigate the factors contributing to the differences observed between 16-bit and 32-bit models. We formalize the concepts of floating-point error and tolerance, enabling us to quantitatively explain the conditions under which a 16-bit model can closely approximate the results of its 32-bit counterpart. This theoretical exploration offers perspect
&lt;/p&gt;</description></item></channel></rss>