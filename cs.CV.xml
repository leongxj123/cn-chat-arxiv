<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#20869;&#31397;&#38236;&#35270;&#35273;&#37324;&#31243;&#35745;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#26694;&#26550;&#65292;&#23558;&#20174;&#26415;&#21069;&#35268;&#21010;&#21040;&#27979;&#35797;&#38454;&#27573;&#30340;&#26102;&#38388;&#32553;&#30701;&#33267;&#19981;&#21040;&#20116;&#20998;&#38047;&#65292;&#36890;&#36807;&#21033;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#30495;&#23454;&#22270;&#20687;&#21644;&#26415;&#21069;&#20808;&#39564;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#21450;&#24341;&#20837;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#26469;&#20943;&#23567;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;&#20809;&#29031;&#26465;&#20214;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.10860</link><description>&lt;p&gt;
&#20869;&#31397;&#38236;&#35270;&#35273;&#37324;&#31243;&#35745;&#30340;&#39640;&#25928;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Efficient Domain Adaptation for Endoscopic Visual Odometry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10860
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#20869;&#31397;&#38236;&#35270;&#35273;&#37324;&#31243;&#35745;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#26694;&#26550;&#65292;&#23558;&#20174;&#26415;&#21069;&#35268;&#21010;&#21040;&#27979;&#35797;&#38454;&#27573;&#30340;&#26102;&#38388;&#32553;&#30701;&#33267;&#19981;&#21040;&#20116;&#20998;&#38047;&#65292;&#36890;&#36807;&#21033;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#30495;&#23454;&#22270;&#20687;&#21644;&#26415;&#21069;&#20808;&#39564;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#21450;&#24341;&#20837;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#26469;&#20943;&#23567;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;&#20809;&#29031;&#26465;&#20214;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10860v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#35270;&#35273;&#37324;&#31243;&#35745;&#22312;&#20869;&#31397;&#38236;&#25104;&#20687;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#28982;&#32780;&#32570;&#20047;&#20855;&#26377;&#22320;&#38754;&#30495;&#23454;&#24615;&#30340;&#22270;&#20687;&#23545;&#20110;&#23398;&#20064;&#37324;&#31243;&#35745;&#20449;&#24687;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#20026;&#36830;&#25509;&#26415;&#21069;&#35268;&#21010;&#39046;&#22495;&#21644;&#26415;&#20013;&#23454;&#38469;&#39046;&#22495;&#23398;&#20064;&#37324;&#31243;&#35745;&#20449;&#24687;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#35757;&#32451;&#26102;&#38388;&#19978;&#23384;&#22312;&#20302;&#25928;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20869;&#31397;&#38236;&#35270;&#35273;&#37324;&#31243;&#35745;&#30340;&#39640;&#25928;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#26694;&#26550;&#65292;&#23558;&#20174;&#26415;&#21069;&#35268;&#21010;&#21040;&#27979;&#35797;&#38454;&#27573;&#30340;&#26102;&#38388;&#32553;&#30701;&#33267;&#19981;&#21040;&#20116;&#20998;&#38047;&#12290;&#20026;&#20102;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#65292;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#30495;&#23454;&#22270;&#20687;&#35757;&#32451;&#27169;&#22359;&#65292;&#24182;&#21033;&#29992;&#26415;&#21069;&#20808;&#39564;&#20449;&#24687;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#22312;&#27979;&#35797;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#26469;&#28040;&#38500;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;&#20809;&#29031;&#26465;&#20214;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10860v1 Announce Type: cross  Abstract: Visual odometry plays a crucial role in endoscopic imaging, yet the scarcity of realistic images with ground truth poses poses a significant challenge. Therefore, domain adaptation offers a promising approach to bridge the pre-operative planning domain with the intra-operative real domain for learning odometry information. However, existing methodologies suffer from inefficiencies in the training time. In this work, an efficient neural style transfer framework for endoscopic visual odometry is proposed, which compresses the time from pre-operative planning to testing phase to less than five minutes. For efficient traing, this work focuses on training modules with only a limited number of real images and we exploit pre-operative prior information to dramatically reduce training duration. Moreover, during the testing phase, we propose a novel Test Time Adaptation (TTA) method to mitigate the gap in lighting conditions between training an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAMFN&#30340;&#28176;&#36827;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;RGB&#12289;&#20809;&#27969;&#21644;&#38899;&#39057;&#20449;&#24687;&#65292;&#20998;&#21035;&#24314;&#27169;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#21644;&#28151;&#21512;&#27169;&#24577;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#38899;&#39057;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#35780;&#20998;&#22238;&#24402;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09444</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multimodal Action Quality Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09444
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAMFN&#30340;&#28176;&#36827;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;RGB&#12289;&#20809;&#27969;&#21644;&#38899;&#39057;&#20449;&#24687;&#65292;&#20998;&#21035;&#24314;&#27169;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#21644;&#28151;&#21512;&#27169;&#24577;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#38899;&#39057;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#35780;&#20998;&#22238;&#24402;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#21160;&#36136;&#37327;&#35780;&#20272;&#65288;AQA&#65289;&#26159;&#35780;&#20272;&#21160;&#20316;&#25191;&#34892;&#24773;&#20917;&#30340;&#26041;&#27861;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20165;&#21033;&#29992;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#65292;&#24573;&#35270;&#20102;&#38899;&#39057;&#20449;&#24687;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#34429;&#28982;AQA&#39640;&#24230;&#20381;&#36182;&#35270;&#35273;&#20449;&#24687;&#65292;&#20294;&#38899;&#39057;&#20063;&#26159;&#25552;&#39640;&#35780;&#20998;&#22238;&#24402;&#20934;&#30830;&#24615;&#30340;&#26377;&#29992;&#34917;&#20805;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#32972;&#26223;&#38899;&#20048;&#30340;&#36816;&#21160;&#39033;&#30446;&#20013;&#65292;&#22914;&#33457;&#26679;&#28369;&#20912;&#21644;&#38901;&#24459;&#20307;&#25805;&#12290;&#20026;&#20102;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;AQA&#65292;&#21363;RGB&#12289;&#20809;&#27969;&#21644;&#38899;&#39057;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28176;&#36827;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65288;PAMFN&#65289;&#65292;&#23427;&#20998;&#21035;&#23545;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#21644;&#28151;&#21512;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19977;&#20010;&#27169;&#24577;&#29305;&#23450;&#20998;&#25903;&#21644;&#19968;&#20010;&#28151;&#21512;&#27169;&#24577;&#20998;&#25903;&#32452;&#25104;&#65292;&#29420;&#31435;&#22320;&#25506;&#32034;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#28176;&#36827;&#22320;&#32858;&#21512;&#26469;&#33258;&#27169;&#24577;&#29305;&#23450;&#20998;&#25903;&#30340;&#27169;&#24577;&#29305;&#23450;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09444v1 Announce Type: cross  Abstract: Action quality assessment (AQA) is to assess how well an action is performed. Previous works perform modelling by only the use of visual information, ignoring audio information. We argue that although AQA is highly dependent on visual information, the audio is useful complementary information for improving the score regression accuracy, especially for sports with background music, such as figure skating and rhythmic gymnastics. To leverage multimodal information for AQA, i.e., RGB, optical flow and audio information, we propose a Progressive Adaptive Multimodal Fusion Network (PAMFN) that separately models modality-specific information and mixed-modality information. Our model consists of with three modality-specific branches that independently explore modality-specific information and a mixed-modality branch that progressively aggregates the modality-specific information from the modality-specific branches. To build the bridge between
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;UNITE&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#25945;&#24072;&#27169;&#22411;&#21644;&#35270;&#39057;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#36974;&#34109;&#39044;&#35757;&#32451;&#21644;&#21327;&#20316;&#33258;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#35270;&#39057;&#39046;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.02914</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35270;&#39057;&#22495;&#33258;&#36866;&#24212;&#65306;&#37319;&#29992;&#36974;&#34109;&#39044;&#35757;&#32451;&#21644;&#21327;&#20316;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02914
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;UNITE&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#25945;&#24072;&#27169;&#22411;&#21644;&#35270;&#39057;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#36974;&#34109;&#39044;&#35757;&#32451;&#21644;&#21327;&#20316;&#33258;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#35270;&#39057;&#39046;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#30340;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;UNITE&#65292;&#20351;&#29992;&#22270;&#20687;&#25945;&#24072;&#27169;&#22411;&#26469;&#35843;&#25972;&#35270;&#39057;&#23398;&#29983;&#27169;&#22411;&#21040;&#30446;&#26631;&#22495;&#12290;UNITE&#39318;&#20808;&#37319;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#25945;&#24072;&#24341;&#23548;&#30340;&#36974;&#34109;&#33976;&#39311;&#30446;&#26631;&#24471;&#21040;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#29305;&#24449;&#23398;&#20064;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#36974;&#34109;&#33258;&#35757;&#32451;&#65292;&#21033;&#29992;&#35270;&#39057;&#23398;&#29983;&#27169;&#22411;&#21644;&#22270;&#20687;&#25945;&#24072;&#27169;&#22411;&#19968;&#36215;&#20026;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#35270;&#39057;&#29983;&#25104;&#25913;&#36827;&#30340;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#33258;&#35757;&#32451;&#36807;&#31243;&#25104;&#21151;&#21033;&#29992;&#20102;&#20004;&#20010;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#36328;&#22495;&#24378;&#22823;&#30340;&#36716;&#31227;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#35270;&#39057;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#30456;&#27604;&#20808;&#21069;&#25253;&#36947;&#30340;&#32467;&#26524;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02914v3 Announce Type: replace-cross  Abstract: In this work, we tackle the problem of unsupervised domain adaptation (UDA) for video action recognition. Our approach, which we call UNITE, uses an image teacher model to adapt a video student model to the target domain. UNITE first employs self-supervised pre-training to promote discriminative feature learning on target domain videos using a teacher-guided masked distillation objective. We then perform self-training on masked target data, using the video student model and image teacher model together to generate improved pseudolabels for unlabeled target videos. Our self-training process successfully leverages the strengths of both models to achieve strong transfer performance across domains. We evaluate our approach on multiple video domain adaptation benchmarks and observe significant improvements upon previously reported results.
&lt;/p&gt;</description></item></channel></rss>