<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65292;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.09918</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Attention-based Class-Conditioned Alignment for Multi-Source Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09918
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65292;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#65288;OD&#65289;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#33268;&#21147;&#20110;&#36890;&#36807;&#20419;&#36827;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#29305;&#24449;&#23545;&#40784;&#26469;&#32531;&#35299;&#20998;&#24067;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#20801;&#35768;&#21033;&#29992;&#22810;&#20010;&#24102;&#27880;&#37322;&#30340;&#28304;&#25968;&#25454;&#38598;&#21644;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#26469;&#25552;&#39640;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;OD MSDA&#26041;&#27861;&#20197;&#19968;&#31181;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#26041;&#24335;&#25191;&#34892;&#29305;&#24449;&#23545;&#40784;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#25353;&#31867;&#21035;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#22024;&#26434;&#30340;&#20266;&#26631;&#31614;&#32780;&#23548;&#33268;&#38169;&#35823;&#31215;&#32047;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;MSDA&#65292;&#35813;&#26041;&#26696;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09918v1 Announce Type: cross  Abstract: Domain adaptation methods for object detection (OD) strive to mitigate the impact of distribution shifts by promoting feature alignment across source and target domains. Multi-source domain adaptation (MSDA) allows leveraging multiple annotated source datasets, and unlabeled target data to improve the accuracy and robustness of the detection model. Most state-of-the-art MSDA methods for OD perform feature alignment in a class-agnostic manner. This is challenging since the objects have unique modal information due to variations in object appearance across domains. A recent prototype-based approach proposed a class-wise alignment, yet it suffers from error accumulation due to noisy pseudo-labels which can negatively affect adaptation with imbalanced data. To overcome these limitations, we propose an attention-based class-conditioned alignment scheme for MSDA that aligns instances of each object category across domains. In particular, an 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;NegOpt&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#30340;&#29983;&#25104;&#65292;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#24182;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.07605</link><description>&lt;p&gt;
&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#20197;&#22686;&#24378;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#32654;&#23398;&#21644;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07605
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;NegOpt&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#30340;&#29983;&#25104;&#65292;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#24182;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#65292;&#20351;&#29992;&#25551;&#36848;&#19981;&#33391;&#22270;&#20687;&#29305;&#24449;&#30340;&#36127;&#38754;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#33391;&#22909;&#30340;&#36127;&#38754;&#25552;&#31034;&#26159;&#19968;&#39033;&#25163;&#24037;&#32780;&#32321;&#29712;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NegOpt&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#29983;&#25104;&#65292;&#20174;&#32780;&#22686;&#24378;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#22823;&#24133;&#25552;&#39640;&#20102;25%&#30340;Inception Score&#65292;&#24182;&#36229;&#36234;&#20102;&#26469;&#33258;&#27979;&#35797;&#38598;&#30340;&#26631;&#20934;&#36127;&#38754;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;NegOpt&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#20248;&#21270;&#23545;&#25105;&#20204;&#26368;&#37325;&#35201;&#30340;&#25351;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;Negative Prompts DB&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07605v1 Announce Type: cross  Abstract: In text-to-image generation, using negative prompts, which describe undesirable image characteristics, can significantly boost image quality. However, producing good negative prompts is manual and tedious. To address this, we propose NegOpt, a novel method for optimizing negative prompt generation toward enhanced image generation, using supervised fine-tuning and reinforcement learning. Our combined approach results in a substantial increase of 25% in Inception Score compared to other approaches and surpasses ground-truth negative prompts from the test set. Furthermore, with NegOpt we can preferentially optimize the metrics most important to us. Finally, we construct Negative Prompts DB, a dataset of negative prompts.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25910;&#38598;&#36731;&#37327;&#32423;VQA&#20559;&#22909;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;Direct Preference Optimization&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#33021;&#21147;&#19978;&#21462;&#24471;&#26174;&#33879;&#25552;&#21319;&#65292;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#19979;&#27604;&#20854;&#20182;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.10884</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#20559;&#22909;&#23545;&#40784;&#20462;&#22797;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#19978;&#30340;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Multi-modal preference alignment remedies regression of visual instruction tuning on language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10884
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25910;&#38598;&#36731;&#37327;&#32423;VQA&#20559;&#22909;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;Direct Preference Optimization&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#33021;&#21147;&#19978;&#21462;&#24471;&#26174;&#33879;&#25552;&#21319;&#65292;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#19979;&#27604;&#20854;&#20182;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#34987;&#26399;&#26395;&#33021;&#22815;&#25903;&#25345;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#20132;&#25442;&#24335;&#22810;&#36718;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20351;&#29992;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#65288;VQA&#65289;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;MLLMs&#21487;&#33021;&#20250;&#20986;&#29616;&#36864;&#21270;&#65292;&#22240;&#20026;VQA&#25968;&#25454;&#38598;&#32570;&#20047;&#21407;&#22987;&#25991;&#26412;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#21518;&#32773;&#26159;&#24213;&#23618;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#65288;6k&#26465;&#35760;&#24405;&#65289;&#30340;VQA&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#31572;&#26696;&#30001;Gemini&#20197;&#32454;&#31890;&#24230;&#26041;&#24335;&#27880;&#37322;&#20102;5&#20010;&#36136;&#37327;&#25351;&#26631;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;&#26631;&#20934;&#30340;&#30417;&#30563;&#24494;&#35843;&#12289;&#25298;&#32477;&#25277;&#26679;&#12289;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#21644;SteerLM&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;DPO&#65292;&#25105;&#20204;&#33021;&#22815;&#36229;&#36234;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;6.73&#30340;MT-Bench&#20998;&#25968;&#65292;&#32780;Vicuna&#30340;6.57&#21644;LLaVA&#30340;5.99&#65292;&#23613;&#31649;&#25968;&#25454;&#35268;&#27169;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10884v1 Announce Type: cross  Abstract: In production, multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets which the underlying language model had been trained with. To address this challenging degradation, we first collect a lightweight (6k entries) VQA preference dataset where answers were annotated by Gemini for 5 quality metrics in a granular fashion, and investigate standard Supervised Fine-tuning, rejection sampling, Direct Preference Optimization (DPO), and SteerLM. Our findings indicate that the with DPO we are able to surpass instruction-following capabilities of the language model, achieving a 6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite small data scale. This
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#36807;&#25311;&#21512;&#28508;&#21464;&#37327;&#26041;&#27861;&#26469;&#25913;&#36827;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;SGA+&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#24182;&#20943;&#23569;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17789</link><description>&lt;p&gt;
&#24377;&#24615;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#20013;&#30340;&#40065;&#26834;&#36807;&#25311;&#21512;&#28508;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Robustly overfitting latents for flexible neural image compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17789
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#36807;&#25311;&#21512;&#28508;&#21464;&#37327;&#26041;&#27861;&#26469;&#25913;&#36827;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;SGA+&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#24182;&#20943;&#23569;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#12290;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#32988;&#36807;&#20102;&#20256;&#32479;&#27169;&#22411;&#12290;&#31070;&#32463;&#21387;&#32553;&#27169;&#22411;&#23398;&#20250;&#23558;&#22270;&#20687;&#32534;&#30721;&#20026;&#37327;&#21270;&#30340;&#28508;&#21464;&#37327;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#39640;&#25928;&#22320;&#21457;&#36865;&#32473;&#35299;&#30721;&#22120;&#65292;&#35299;&#30721;&#22120;&#20877;&#23558;&#37327;&#21270;&#30340;&#28508;&#21464;&#37327;&#35299;&#30721;&#20026;&#37325;&#24314;&#22270;&#20687;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#20248;&#21270;&#19981;&#23436;&#32654;&#20197;&#21450;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23481;&#37327;&#30340;&#38480;&#21046;&#65292;&#23427;&#20204;&#23548;&#33268;&#20102;&#27425;&#20248;&#32467;&#26524;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22914;&#20309;&#21033;&#29992;&#38543;&#26426;Gumbel&#36864;&#28779;&#65288;SGA&#65289;&#26469;&#25913;&#36827;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#27169;&#22411;&#30340;&#28508;&#21464;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;SGA+&#25193;&#23637;&#20102;&#36825;&#20010;&#24819;&#27861;&#65292;SGA+&#21253;&#21547;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#24314;&#31435;&#22312;SGA&#30340;&#22522;&#30784;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22914;&#20309;&#25913;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#35777;&#26126;&#23427;&#20204;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#19981;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#27599;&#20010;&#26041;&#27861;&#25193;&#23637;&#21040;&#19977;&#20010;&#32780;&#19981;&#26159;&#20004;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural image compression has made a great deal of progress. State-of-the-art models are based on variational autoencoders and are outperforming classical models. Neural compression models learn to encode an image into a quantized latent representation that can be efficiently sent to the decoder, which decodes the quantized latent into a reconstructed image. While these models have proven successful in practice, they lead to sub-optimal results due to imperfect optimization and limitations in the encoder and decoder capacity. Recent work shows how to use stochastic Gumbel annealing (SGA) to refine the latents of pre-trained neural image compression models. We extend this idea by introducing SGA+, which contains three different methods that build upon SGA. Further, we give a detailed analysis of our proposed methods, show how they improve performance, and show that they are less sensitive to hyperparameter choices. Besides, we show how each method can be extended to three- instead of two
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22534;&#21472;&#32852;&#21512;&#23884;&#20837;&#32467;&#26500;&#23398;&#20064;&#39640;&#24230;&#21487;&#20998;&#31163;&#30340;&#20998;&#23618;&#35821;&#20041;&#34920;&#31034;&#65292;&#26174;&#31034;&#20986;&#26356;&#26126;&#26174;&#30340;&#35821;&#20041;&#27010;&#24565;&#23376;&#31867;&#65292;&#24182;&#19988;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2305.11701</link><description>&lt;p&gt;
S-JEA: &#22534;&#21472;&#32852;&#21512;&#23884;&#20837;&#32467;&#26500;&#29992;&#20110;&#33258;&#30417;&#30563;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
S-JEA: Stacked Joint Embedding Architectures for Self-Supervised Visual Representation Learning. (arXiv:2305.11701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22534;&#21472;&#32852;&#21512;&#23884;&#20837;&#32467;&#26500;&#23398;&#20064;&#39640;&#24230;&#21487;&#20998;&#31163;&#30340;&#20998;&#23618;&#35821;&#20041;&#34920;&#31034;&#65292;&#26174;&#31034;&#20986;&#26356;&#26126;&#26174;&#30340;&#35821;&#20041;&#27010;&#24565;&#23376;&#31867;&#65292;&#24182;&#19988;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20316;&#20026;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#22522;&#26412;&#33539;&#24335;&#65292;&#36817;&#24180;&#26469;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#39640;&#24230;&#30340;&#23454;&#35777;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26410;&#33021;&#23398;&#20064;&#21040;&#25429;&#33719;&#20998;&#23618;&#35821;&#20041;&#27010;&#24565;&#30340;&#23884;&#20837;&#65292;&#36825;&#20123;&#27010;&#24565;&#26159;&#21487;&#20998;&#31163;&#21644;&#21487;&#35299;&#37322;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22534;&#21472;&#32852;&#21512;&#23884;&#20837;&#32467;&#26500;&#65288;JEA&#65289;&#26469;&#23398;&#20064;&#39640;&#24230;&#21487;&#20998;&#31163;&#30340;&#20998;&#23618;&#35821;&#20041;&#34920;&#31034;&#65292;&#20854;&#20013;&#36739;&#39640;&#32423;&#21035;&#30340;JEA&#20351;&#29992;&#36739;&#20302;&#32423;&#21035;JEA&#30340;&#34920;&#31034;&#32467;&#26524;&#20316;&#20026;&#36755;&#20837;&#12290;&#36825;&#23548;&#33268;&#34920;&#31034;&#31354;&#38388;&#34920;&#29616;&#20986;&#26356;&#26126;&#26174;&#30340;&#35821;&#20041;&#27010;&#24565;&#23376;&#31867;&#65288;&#22914;&#36710;&#36742;&#30340;&#22411;&#21495;&#21644;&#39068;&#33394;&#65289;&#22312;&#36739;&#39640;&#32423;&#21035;&#30340;JEA&#20013;&#12290;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#22534;&#21472;JEA&#30340;&#34920;&#31034;&#19982;&#20256;&#32479;JEA&#30456;&#20284;&#65292;&#24182;&#23637;&#31034;&#20102;&#34920;&#31034;&#31354;&#38388;&#20197;&#39564;&#35777;&#35821;&#20041;&#20998;&#23618;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent emergence of Self-Supervised Learning (SSL) as a fundamental paradigm for learning image representations has, and continues to, demonstrate high empirical success in a variety of tasks. However, most SSL approaches fail to learn embeddings that capture hierarchical semantic concepts that are separable and interpretable. In this work, we aim to learn highly separable semantic hierarchical representations by stacking Joint Embedding Architectures (JEA) where higher-level JEAs are input with representations of lower-level JEA. This results in a representation space that exhibits distinct sub-categories of semantic concepts (e.g., model and colour of vehicles) in higher-level JEAs. We empirically show that representations from stacked JEA perform on a similar level as traditional JEA with comparative parameter counts and visualise the representation spaces to validate the semantic hierarchies.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11616</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#28145;&#24230;&#38598;&#25104;&#65306;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy. (arXiv:2305.11616v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11616
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#22312;&#20998;&#31867;&#21644; OOD &#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#38598;&#25104;&#20013;&#23398;&#20064;&#30340;&#27169;&#24335;&#30340;&#21516;&#36136;&#24615;&#65292;&#23427;&#20204;&#30340;&#25928;&#26524;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#20419;&#36827;&#38598;&#25104;&#25104;&#21592;&#20043;&#38388;&#22810;&#26679;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26174;&#33879;&#24615;&#22270;&#12290;&#36890;&#36807;&#25972;&#21512;&#26174;&#33879;&#24615;&#22270;&#22810;&#26679;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20998;&#31867;&#21644;OOD&#26816;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#26657;&#20934;&#24615;&#12290;&#22312;&#24050;&#24314;&#31435;&#30340;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#20984;&#26174;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep ensembles achieved state-of-the-art results in classification and out-of-distribution (OOD) detection; however, their effectiveness remains limited due to the homogeneity of learned patterns within the ensemble. To overcome this challenge, our study introduces a novel approach that promotes diversity among ensemble members by leveraging saliency maps. By incorporating saliency map diversification, our method outperforms conventional ensemble techniques in multiple classification and OOD detection tasks, while also improving calibration. Experiments on well-established OpenOOD benchmarks highlight the potential of our method in practical applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#28857; Mixup &#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#23398;&#20064;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#27599;&#20010;&#31867;&#21035;&#30340;&#25152;&#26377;&#29305;&#24449;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.13512</link><description>&lt;p&gt;
&#29992;&#20013;&#28857; Mixup &#22312;&#22810;&#35270;&#35282;&#25968;&#25454;&#20013;&#21487;&#35777;&#26126;&#23398;&#20064;&#22810;&#20803;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup. (arXiv:2210.13512v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#28857; Mixup &#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#23398;&#20064;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#27599;&#20010;&#31867;&#21035;&#30340;&#25152;&#26377;&#29305;&#24449;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup &#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20381;&#36182;&#20110;&#20351;&#29992;&#25968;&#25454;&#28857;&#21644;&#26631;&#31614;&#30340;&#38543;&#26426;&#20984;&#32452;&#21512;&#36827;&#34892;&#35757;&#32451;&#12290;&#36817;&#24180;&#26469;&#65292;Mixup &#24050;&#25104;&#20026;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#26631;&#20934;&#22522;&#20803;&#65292;&#22240;&#20026;&#23427;&#22312;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#27604;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#20174;&#29305;&#24449;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#37322;&#19968;&#20123;&#36825;&#31181;&#25104;&#21151;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#20998;&#31867;&#38382;&#39064;&#26159;&#65292;&#27599;&#20010;&#31867;&#21035;&#21487;&#33021;&#20855;&#26377;&#22810;&#20010;&#30456;&#20851;&#29305;&#24449;&#65288;&#25110;&#35270;&#22270;&#65289;&#65292;&#21487;&#29992;&#20110;&#27491;&#30830;&#39044;&#27979;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#27599;&#31867;&#20004;&#20010;&#29305;&#24449;&#30340;&#19968;&#31867;&#38750;&#24179;&#20961;&#25968;&#25454;&#20998;&#24067;&#20013;&#65292;&#20351;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#35757;&#32451; 2 &#23618;&#21367;&#31215;&#32593;&#32476;&#21487;&#33021;&#20250;&#23548;&#33268;&#20960;&#20046;&#25152;&#26377;&#31867;&#21035;&#21482;&#23398;&#20064;&#19968;&#20010;&#29305;&#24449;&#65292;&#32780;&#20351;&#29992; Mixup &#30340;&#29305;&#23450;&#23454;&#20363;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25104;&#21151;&#22320;&#23398;&#20064;&#27599;&#20010;&#31867;&#21035;&#30340;&#20004;&#20010;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is a data augmentation technique that relies on training using random convex combinations of data points and their labels. In recent years, Mixup has become a standard primitive used in the training of state-of-the-art image classification models due to its demonstrated benefits over empirical risk minimization with regards to generalization and robustness. In this work, we try to explain some of this success from a feature learning perspective. We focus our attention on classification problems in which each class may have multiple associated features (or views) that can be used to predict the class correctly. Our main theoretical results demonstrate that, for a non-trivial class of data distributions with two features per class, training a 2-layer convolutional network using empirical risk minimization can lead to learning only one feature for almost all classes while training with a specific instantiation of Mixup succeeds in learning both features for every class. We also show
&lt;/p&gt;</description></item></channel></rss>