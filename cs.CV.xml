<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#19978;&#34920;&#29616;&#20248;&#20110;&#28145;&#24230;&#32593;&#32476;&#65292;&#24403;&#32473;&#20104;&#36275;&#22815;&#26102;&#38388;&#26102;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22270;&#20687;&#26333;&#20809;&#26102;&#38388;&#30340;&#38480;&#21046;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#38477;&#33267;&#28145;&#24230;&#32593;&#32476;&#30340;&#27700;&#24179;&#65292;&#36825;&#26263;&#31034;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#38656;&#35201;&#39069;&#22806;&#30340;&#24515;&#29702;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#19982;&#32593;&#32476;&#20043;&#38388;&#30340;&#38169;&#35823;&#27169;&#24335;&#20063;&#23384;&#22312;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.03973</link><description>&lt;p&gt;
&#32473;&#20104;&#36275;&#22815;&#26102;&#38388;&#65292;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#19978;&#20987;&#36133;&#20102;&#28145;&#24230;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Humans Beat Deep Networks at Recognizing Objects in Unusual Poses, Given Enough Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03973
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#19978;&#34920;&#29616;&#20248;&#20110;&#28145;&#24230;&#32593;&#32476;&#65292;&#24403;&#32473;&#20104;&#36275;&#22815;&#26102;&#38388;&#26102;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22270;&#20687;&#26333;&#20809;&#26102;&#38388;&#30340;&#38480;&#21046;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#38477;&#33267;&#28145;&#24230;&#32593;&#32476;&#30340;&#27700;&#24179;&#65292;&#36825;&#26263;&#31034;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#38656;&#35201;&#39069;&#22806;&#30340;&#24515;&#29702;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#19982;&#32593;&#32476;&#20043;&#38388;&#30340;&#38169;&#35823;&#27169;&#24335;&#20063;&#23384;&#22312;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#20960;&#20010;&#29289;&#20307;&#35782;&#21035;&#22522;&#20934;&#19978;&#27491;&#22312;&#32553;&#23567;&#19982;&#20154;&#31867;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#22312;&#28041;&#21450;&#20174;&#19981;&#23547;&#24120;&#35270;&#35282;&#35266;&#23519;&#29289;&#20307;&#30340;&#25361;&#25112;&#24615;&#22270;&#20687;&#20013;&#23545;&#36825;&#19968;&#24046;&#36317;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#19982;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#65288;EfficientNet&#12289;SWAG&#12289;ViT&#12289;SWIN&#12289;BEiT&#12289;ConvNext&#65289;&#30456;&#27604;&#65292;&#36825;&#20123;&#32593;&#32476;&#22312;&#27492;&#24773;&#20917;&#19979;&#26222;&#36941;&#33030;&#24369;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#38543;&#30528;&#25105;&#20204;&#38480;&#21046;&#22270;&#20687;&#26333;&#20809;&#26102;&#38388;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#19979;&#38477;&#21040;&#28145;&#24230;&#32593;&#32476;&#30340;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#38656;&#35201;&#39069;&#22806;&#30340;&#24515;&#29702;&#36807;&#31243;&#65288;&#38656;&#35201;&#39069;&#22806;&#30340;&#26102;&#38388;&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20154;&#31867;&#19982;&#32593;&#32476;&#30340;&#38169;&#35823;&#27169;&#24335;&#65292;&#21457;&#29616;&#21363;&#20351;&#22312;&#38480;&#21046;&#26102;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#20154;&#31867;&#19982;&#21069;&#39304;&#28145;&#24230;&#32593;&#32476;&#20063;&#26377;&#19981;&#21516;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#24037;&#20316;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#24102;&#21040;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;&#29702;&#35299;&#22312;&#22806;&#37096;&#24773;&#20917;&#19979;&#21457;&#29983;&#30340;&#24515;&#29702;&#36807;&#31243;&#30340;&#26412;&#36136;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is closing the gap with humans on several object recognition benchmarks. Here we investigate this gap in the context of challenging images where objects are seen from unusual viewpoints. We find that humans excel at recognizing objects in unusual poses, in contrast with state-of-the-art pretrained networks (EfficientNet, SWAG, ViT, SWIN, BEiT, ConvNext) which are systematically brittle in this condition. Remarkably, as we limit image exposure time, human performance degrades to the level of deep networks, suggesting that additional mental processes (requiring additional time) take place when humans identify objects in unusual poses. Finally, our analysis of error patterns of humans vs. networks reveals that even time-limited humans are dissimilar to feed-forward deep networks. We conclude that more work is needed to bring computer vision systems to the level of robustness of the human visual system. Understanding the nature of the mental processes taking place during extr
&lt;/p&gt;</description></item><item><title>DiffiT&#26159;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;Vision Transformer&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#21435;&#22122;&#25511;&#21046;&#21644;&#26102;&#38388;&#20381;&#36182;&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2312.02139</link><description>&lt;p&gt;
DiffiT: &#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#25193;&#25955;&#35270;&#35273;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffiT: Diffusion Vision Transformers for Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02139
&lt;/p&gt;
&lt;p&gt;
DiffiT&#26159;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;Vision Transformer&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#21435;&#22122;&#25511;&#21046;&#21644;&#26102;&#38388;&#20381;&#36182;&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24378;&#22823;&#34920;&#29616;&#21147;&#21644;&#39640;&#26679;&#26412;&#36136;&#37327;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#24320;&#21019;&#24615;&#30340;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#24314;&#27169;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#35782;&#21035;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;ViTs&#22312;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;&#31216;&#20026;Diffusion Vision Transformers&#65288;DiffiT&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;&#21435;&#22122;&#36807;&#31243;&#36827;&#34892;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#26102;&#38388;&#20381;&#36182;&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#65288;TMSA&#65289;&#26426;&#21046;&#12290;DiffiT&#22312;&#29983;&#25104;&#39640;&#20445;&#30495;&#22270;&#20687;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#21442;&#25968;&#25928;&#29575;&#20063;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#28508;&#31354;&#38388;&#21644;&#22270;&#20687;&#31354;&#38388;&#30340;DiffiT&#27169;&#22411;&#65292;&#24182;&#22312;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#21508;&#31181;&#31867;&#21035;&#26465;&#20214;&#21644;&#38750;&#26465;&#20214;&#32508;&#21512;&#20219;&#21153;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28508;&#31354;&#38388;DiffiT&#27169;&#22411;&#36798;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02139v2 Announce Type: replace-cross  Abstract: Diffusion models with their powerful expressivity and high sample quality have achieved State-Of-The-Art (SOTA) performance in the generative domain. The pioneering Vision Transformer (ViT) has also demonstrated strong modeling capabilities and scalability, especially for recognition tasks. In this paper, we study the effectiveness of ViTs in diffusion-based generative learning and propose a new model denoted as Diffusion Vision Transformers (DiffiT). Specifically, we propose a methodology for finegrained control of the denoising process and introduce the Time-dependant Multihead Self Attention (TMSA) mechanism. DiffiT is surprisingly effective in generating high-fidelity images with significantly better parameter efficiency. We also propose latent and image space DiffiT models and show SOTA performance on a variety of class-conditional and unconditional synthesis tasks at different resolutions. The Latent DiffiT model achieves
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#24182;&#23558;&#20854;&#36866;&#24212;&#20026;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#24773;&#32490;&#35782;&#21035;&#20013;&#20851;&#38190;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2011.08388</link><description>&lt;p&gt;
&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#21487;&#35299;&#37322;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#65292;&#24182;&#21033;&#29992;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation based Interpretable Image Emotion Recognition using Facial Expression Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2011.08388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#24182;&#23558;&#20854;&#36866;&#24212;&#20026;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#24773;&#32490;&#35782;&#21035;&#20013;&#20851;&#38190;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#29992;&#20110;&#35782;&#21035;&#21253;&#21547;&#38754;&#37096;&#21644;&#38750;&#38754;&#37096;&#29289;&#20307;&#20197;&#21450;&#38750;&#20154;&#31867;&#32452;&#20214;&#30340;&#36890;&#29992;&#22270;&#20687;&#20013;&#30340;&#24773;&#32490;&#12290;&#23427;&#35299;&#20915;&#20102;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#65288;IER&#65289;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#33391;&#22909;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#65288;FER&#65289;&#31995;&#32479;&#65292;&#23558;&#32473;&#23450;&#30340;&#38754;&#37096;&#22270;&#20687;&#20998;&#31867;&#20026;&#31163;&#25955;&#24773;&#32490;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20687;&#35782;&#21035;&#31995;&#32479;&#65292;&#23558;&#25552;&#20986;&#30340;FER&#31995;&#32479;&#36866;&#24212;&#20110;&#21033;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#35782;&#21035;&#22270;&#20687;&#25152;&#20256;&#36798;&#30340;&#24773;&#32490;&#12290;&#23427;&#23558;&#36890;&#29992;&#22270;&#20687;&#20998;&#31867;&#20026;&#8220;&#24555;&#20048;&#8221;&#65292;&#8220;&#24754;&#20260;&#8221;&#65292;&#8220;&#20167;&#24680;&#8221;&#21644;&#8220;&#24868;&#24594;&#8221;&#31867;&#21035;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#31216;&#20026;&#20998;&#32780;&#27835;&#20043;&#30340;Shap&#65288;DnCShap&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#24773;&#32490;&#35782;&#21035;&#20013;&#39640;&#24230;&#30456;&#20851;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
A domain adaptation technique has been proposed in this paper to identify the emotions in generic images containing facial &amp; non-facial objects and non-human components. It addresses the challenge of the insufficient availability of pre-trained models and well-annotated datasets for image emotion recognition (IER). It starts with proposing a facial emotion recognition (FER) system and then moves on to adapting it for image emotion recognition. First, a deep-learning-based FER system has been proposed that classifies a given facial image into discrete emotion classes. Further, an image recognition system has been proposed that adapts the proposed FER system to recognize the emotions portrayed by images using domain adaptation. It classifies the generic images into 'happy,' 'sad,' 'hate,' and 'anger' classes. A novel interpretability approach, Divide and Conquer based Shap (DnCShap), has also been proposed to interpret the highly relevant visual features for emotion recognition. The prop
&lt;/p&gt;</description></item></channel></rss>