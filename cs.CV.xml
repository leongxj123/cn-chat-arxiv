<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#30340;3D&#24863;&#30693;&#32918;&#20687;&#21160;&#30011;&#26041;&#27861;Export3D&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#24179;&#38754;&#29983;&#25104;&#22120;&#21644;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#25511;&#21046;&#32473;&#23450;&#32918;&#20687;&#22270;&#20687;&#30340;&#38754;&#37096;&#34920;&#24773;&#21644;&#25668;&#20687;&#26426;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#36798;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2404.00636</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#26465;&#20214;&#21270;&#19977;&#24179;&#38754;&#29992;&#20110;3D&#24863;&#30693;&#34920;&#24773;&#21487;&#25511;&#32918;&#20687;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Conditional Tri-plane for 3D-aware Expression Controllable Portrait Animation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#30340;3D&#24863;&#30693;&#32918;&#20687;&#21160;&#30011;&#26041;&#27861;Export3D&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#24179;&#38754;&#29983;&#25104;&#22120;&#21644;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#25511;&#21046;&#32473;&#23450;&#32918;&#20687;&#22270;&#20687;&#30340;&#38754;&#37096;&#34920;&#24773;&#21644;&#25668;&#20687;&#26426;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#36798;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#30340;3D&#24863;&#30693;&#32918;&#20687;&#21160;&#30011;&#26041;&#27861;Export3D&#65292;&#33021;&#22815;&#25511;&#21046;&#32473;&#23450;&#32918;&#20687;&#22270;&#20687;&#30340;&#38754;&#37096;&#34920;&#24773;&#21644;&#25668;&#20687;&#26426;&#35270;&#35282;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19977;&#24179;&#38754;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23558;3DMM&#30340;&#34920;&#24773;&#21442;&#25968;&#36716;&#31227;&#21040;&#28304;&#22270;&#20687;&#20013;&#30452;&#25509;&#29983;&#25104;3D&#20808;&#39564;&#30340;&#19977;&#24179;&#38754;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#20307;&#31215;&#28210;&#26579;&#23558;&#19977;&#24179;&#38754;&#35299;&#30721;&#20026;&#19981;&#21516;&#35270;&#35282;&#30340;&#22270;&#20687;&#12290;&#29616;&#26377;&#30340;&#32918;&#20687;&#21160;&#30011;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#22270;&#20687;&#21464;&#24418;&#26469;&#22312;&#36816;&#21160;&#31354;&#38388;&#20013;&#20256;&#36755;&#34920;&#24773;&#65292;&#25361;&#25112;&#22312;&#22806;&#35266;&#21644;&#34920;&#24773;&#30340;&#20998;&#31163;&#19978;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26080;&#22806;&#35266;&#34920;&#24773;&#21442;&#25968;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#28040;&#38500;&#20102;&#22312;&#20256;&#36755;&#36328;&#36523;&#20221;&#34920;&#36798;&#26102;&#19981;&#33391;&#22806;&#35266;&#20132;&#25442;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#38544;&#34255;&#22312;3DMM&#20013;&#30340;&#26080;&#22806;&#35266;&#34920;&#36798;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00636v1 Announce Type: cross  Abstract: In this paper, we present Export3D, a one-shot 3D-aware portrait animation method that is able to control the facial expression and camera view of a given portrait image. To achieve this, we introduce a tri-plane generator that directly generates a tri-plane of 3D prior by transferring the expression parameter of 3DMM into the source image. The tri-plane is then decoded into the image of different view through a differentiable volume rendering. Existing portrait animation methods heavily rely on image warping to transfer the expression in the motion space, challenging on disentanglement of appearance and expression. In contrast, we propose a contrastive pre-training framework for appearance-free expression parameter, eliminating undesirable appearance swap when transferring a cross-identity expression. Extensive experiments show that our pre-training framework can learn the appearance-free expression representation hidden in 3DMM, and 
&lt;/p&gt;</description></item><item><title>DetToolChain&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#21487;&#20197;&#37322;&#25918;MLLM&#30340;&#38646;&#25293;&#25668;&#29289;&#20307;&#26816;&#27979;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26816;&#27979;&#38142;&#24335;&#24605;&#32500;&#33258;&#21160;&#21270;&#20219;&#21153;&#20998;&#35299;&#21644;&#36880;&#27493;&#26694;&#32454;&#21270;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2403.12488</link><description>&lt;p&gt;
DetToolChain&#65306;&#19968;&#31181;&#37322;&#25918;MLLM&#26816;&#27979;&#33021;&#21147;&#30340;&#26032;&#25552;&#31034;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12488
&lt;/p&gt;
&lt;p&gt;
DetToolChain&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#21487;&#20197;&#37322;&#25918;MLLM&#30340;&#38646;&#25293;&#25668;&#29289;&#20307;&#26816;&#27979;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26816;&#27979;&#38142;&#24335;&#24605;&#32500;&#33258;&#21160;&#21270;&#20219;&#21153;&#20998;&#35299;&#21644;&#36880;&#27493;&#26694;&#32454;&#21270;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;DetToolChain&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#29992;&#20110;&#37322;&#25918;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#38646;&#25293;&#25668;&#29289;&#20307;&#26816;&#27979;&#33021;&#21147;&#65292;&#22914;GPT-4V&#21644;Gemini&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#21463;&#39640;&#31934;&#24230;&#26816;&#27979;&#20808;&#39564;&#21551;&#21457;&#30340;&#26816;&#27979;&#25552;&#31034;&#24037;&#20855;&#21253;&#21644;&#19968;&#20010;&#23454;&#29616;&#36825;&#20123;&#25552;&#31034;&#30340;&#26032;Chain-of-Thought&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24037;&#20855;&#21253;&#20013;&#30340;&#25552;&#31034;&#26088;&#22312;&#24341;&#23548;MLLM&#38598;&#20013;&#22312;&#21306;&#22495;&#20449;&#24687;&#19978;&#65288;&#20363;&#22914;&#65292;&#25918;&#22823;&#65289;&#65292;&#25353;&#29031;&#27979;&#37327;&#26631;&#20934;&#38405;&#35835;&#22352;&#26631;&#65288;&#20363;&#22914;&#65292;&#21472;&#21152;&#26631;&#23610;&#21644;&#25351;&#21335;&#38024;&#65289;&#65292;&#24182;&#20174;&#19978;&#19979;&#25991;&#20449;&#24687;&#20013;&#25512;&#26029;&#65288;&#20363;&#22914;&#65292;&#21472;&#21152;&#22330;&#26223;&#22270;&#65289;&#12290;&#22522;&#20110;&#36825;&#20123;&#24037;&#20855;&#65292;&#26032;&#30340;&#26816;&#27979;Chain-of-Thought&#21487;&#20197;&#33258;&#21160;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#65292;&#35786;&#26029;&#39044;&#27979;&#65292;&#24182;&#35268;&#21010;&#36880;&#27493;&#26694;&#32454;&#21270;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#22312;&#19968;&#31995;&#21015;&#26816;&#27979;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#35777;&#23454;&#65292;&#29305;&#21035;&#26159;&#22312;&#22256;&#38590;&#24773;&#20917;&#19979;&#12290;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12488v1 Announce Type: cross  Abstract: We present DetToolChain, a novel prompting paradigm, to unleash the zero-shot object detection ability of multimodal large language models (MLLMs), such as GPT-4V and Gemini. Our approach consists of a detection prompting toolkit inspired by high-precision detection priors and a new Chain-of-Thought to implement these prompts. Specifically, the prompts in the toolkit are designed to guide the MLLM to focus on regional information (e.g., zooming in), read coordinates according to measure standards (e.g., overlaying rulers and compasses), and infer from the contextual information (e.g., overlaying scene graphs). Building upon these tools, the new detection chain-of-thought can automatically decompose the task into simple subtasks, diagnose the predictions, and plan for progressive box refinements. The effectiveness of our framework is demonstrated across a spectrum of detection tasks, especially hard cases. Compared to existing state-of-
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#24402;&#32435;&#36825;&#20123;&#25351;&#26631;</title><link>https://arxiv.org/abs/2403.11821</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#65306;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality Metrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11821
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#24402;&#32435;&#36825;&#20123;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;&#32467;&#21512;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#25512;&#21160;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20114;&#32852;&#32593;&#25110;&#20854;&#20182;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#20013;&#30340;&#28023;&#37327;&#25991;&#26412;-&#22270;&#20687;&#23545;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#38543;&#30528;&#23545;&#39640;&#36136;&#37327;&#22270;&#20687;&#29983;&#25104;&#30340;&#38656;&#27714;&#36716;&#21521;&#30830;&#20445;&#25991;&#26412;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#20869;&#23481;&#23545;&#40784;&#65292;&#24050;&#24320;&#21457;&#20102;&#26032;&#39062;&#30340;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#27169;&#25311;&#20154;&#31867;&#21028;&#26029;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25910;&#38598;&#20855;&#26377;&#36234;&#26469;&#36234;&#22797;&#26434;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#30740;&#31350;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#25104;&#24615;&#21450;&#20854;&#20316;&#20026;&#25991;&#26412;&#19982;&#22270;&#20687;&#20869;&#23481;&#32452;&#25104;&#23545;&#40784;&#36136;&#37327;&#24230;&#37327;&#30340;&#20854;&#32435;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#23545;&#36825;&#20123;&#25351;&#26631;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#23457;&#26597;&#20102;&#32463;&#24120;&#37319;&#29992;&#30340;&#25991;&#26412;-&#22270;&#20687;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11821v1 Announce Type: cross  Abstract: Recent advances in text-to-image synthesis have been enabled by exploiting a combination of language and vision through foundation models. These models are pre-trained on tremendous amounts of text-image pairs sourced from the World Wide Web or other large-scale databases. As the demand for high-quality image generation shifts towards ensuring content alignment between text and image, novel evaluation metrics have been developed with the aim of mimicking human judgments. Thus, researchers have started to collect datasets with increasingly complex annotations to study the compositionality of vision-language models and their incorporation as a quality measure of compositional alignment between text and image contents. In this work, we provide a comprehensive overview of existing text-to-image evaluation metrics and propose a new taxonomy for categorizing these metrics. We also review frequently adopted text-image benchmark datasets befor
&lt;/p&gt;</description></item><item><title>&#23545;&#22810;&#27169;&#24577;LLMs&#30340;&#27450;&#39575;&#24615;&#25552;&#31034;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25552;&#20986;&#21253;&#21547;850&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#22522;&#20934;&#27979;&#35797;MAD-Bench&#65292;&#21457;&#29616;GPT-4V&#22312;&#35813;&#22522;&#20934;&#27979;&#35797;&#19978;&#20934;&#30830;&#29575;&#36739;&#39640;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#24615;&#33021;&#24046;&#36317;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2402.13220</link><description>&lt;p&gt;
&#26377;&#22810;&#23481;&#26131;&#27450;&#39575;&#22810;&#27169;&#24577;LLMs&#65311;&#20851;&#20110;&#27450;&#39575;&#24615;&#25552;&#31034;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13220
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;LLMs&#30340;&#27450;&#39575;&#24615;&#25552;&#31034;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25552;&#20986;&#21253;&#21547;850&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#22522;&#20934;&#27979;&#35797;MAD-Bench&#65292;&#21457;&#29616;GPT-4V&#22312;&#35813;&#22522;&#20934;&#27979;&#35797;&#19978;&#20934;&#30830;&#29575;&#36739;&#39640;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#24615;&#33021;&#24046;&#36317;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#26174;&#33879;&#36827;&#23637;&#24182;&#27809;&#26377;&#20351;&#23427;&#20204;&#20813;&#30123;&#21508;&#31181;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#24102;&#26377;&#27450;&#39575;&#24615;&#20449;&#24687;&#30340;&#25552;&#31034;&#26102;&#65292;&#20250;&#20135;&#29983;&#24187;&#35273;&#33324;&#30340;&#22238;&#24212;&#12290;&#20026;&#20102;&#23450;&#37327;&#35780;&#20272;&#36825;&#31181;&#33030;&#24369;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MAD-Bench&#65292;&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#21547;850&#20010;&#27979;&#35797;&#26679;&#26412;&#65292;&#20998;&#20026;6&#20010;&#31867;&#21035;&#65292;&#22914;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#12289;&#23545;&#35937;&#25968;&#37327;&#12289;&#31354;&#38388;&#20851;&#31995;&#21644;&#35270;&#35273;&#28151;&#28102;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#30340;MLLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21253;&#25324;GPT-4V&#12289;Gemini-Pro&#65292;&#20197;&#21450;&#24320;&#28304;&#27169;&#22411;&#65292;&#22914;LLaVA-1.5&#21644;CogVLM&#12290;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;GPT-4V&#21644;&#20854;&#20182;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65307;&#20043;&#21069;&#30340;&#40065;&#26834;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#65292;&#22914;LRV-Instruction&#21644;LLaVA-RLHF&#65292;&#22312;&#36825;&#20010;&#26032;&#22522;&#20934;&#27979;&#35797;&#20013;&#24182;&#19981;&#26377;&#25928;&#12290;&#34429;&#28982;GPT-4V&#22312;MAD-Bench&#19978;&#21462;&#24471;&#20102;75.02%&#30340;&#20934;&#30830;&#29575;&#65292;&#20294;&#20854;&#20182;&#20219;&#20309;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#37117;&#27809;&#26377;&#36798;&#21040;&#36825;&#19968;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13220v1 Announce Type: cross  Abstract: The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions. To quantitatively assess this vulnerability, we present MAD-Bench, a carefully curated benchmark that contains 850 test samples divided into 6 categories, such as non-existent objects, count of objects, spatial relationship, and visual confusion. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as LLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps between GPT-4V and other models; and previous robust instruction-tuned models, such as LRV-Instruction and LLaVA-RLHF, are not effective on this new benchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of any other model in our exper
&lt;/p&gt;</description></item><item><title>RanDumb&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22266;&#23450;&#30340;&#38543;&#26426;&#21464;&#25442;&#23884;&#20837;&#21407;&#22987;&#20687;&#32032;&#24182;&#23398;&#20064;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#36136;&#30097;&#20102;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290; &#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;RanDumb&#22312;&#20247;&#22810;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.08823</link><description>&lt;p&gt;
RanDumb: &#19968;&#31181;&#36136;&#30097;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RanDumb: A Simple Approach that Questions the Efficacy of Continual Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08823
&lt;/p&gt;
&lt;p&gt;
RanDumb&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22266;&#23450;&#30340;&#38543;&#26426;&#21464;&#25442;&#23884;&#20837;&#21407;&#22987;&#20687;&#32032;&#24182;&#23398;&#20064;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#36136;&#30097;&#20102;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290; &#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;RanDumb&#22312;&#20247;&#22810;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RanDumb&#26469;&#26816;&#39564;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;RanDumb&#23558;&#21407;&#22987;&#20687;&#32032;&#20351;&#29992;&#19968;&#20010;&#22266;&#23450;&#30340;&#38543;&#26426;&#21464;&#25442;&#23884;&#20837;&#65292;&#36825;&#20010;&#21464;&#25442;&#36817;&#20284;&#20102;RBF-Kernel&#65292;&#22312;&#30475;&#21040;&#20219;&#20309;&#25968;&#25454;&#20043;&#21069;&#21021;&#22987;&#21270;&#65292;&#24182;&#23398;&#20064;&#19968;&#20010;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#19988;&#19968;&#33268;&#30340;&#21457;&#29616;&#65306;&#22312;&#20247;&#22810;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;RanDumb&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#36825;&#34920;&#26126;&#22312;&#36825;&#20123;&#24773;&#26223;&#19979;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;RanDumb&#19981;&#23384;&#20648;&#26679;&#26412;&#65292;&#24182;&#22312;&#25968;&#25454;&#19978;&#36827;&#34892;&#21333;&#27425;&#36941;&#21382;&#65292;&#19968;&#27425;&#22788;&#29702;&#19968;&#20010;&#26679;&#26412;&#12290;&#23427;&#19982;GDumb&#30456;&#36741;&#30456;&#25104;&#65292;&#22312;GDumb&#24615;&#33021;&#29305;&#21035;&#24046;&#30340;&#20302;&#26679;&#26412;&#24773;&#20917;&#19979;&#36816;&#34892;&#12290;&#24403;&#23558;RanDumb&#25193;&#23637;&#21040;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26367;&#25442;&#38543;&#26426;&#21464;&#25442;&#30340;&#24773;&#26223;&#26102;&#65292;&#25105;&#20204;&#24471;&#20986;&#30456;&#21516;&#19968;&#33268;&#30340;&#32467;&#35770;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#32467;&#26524;&#26082;&#20196;&#20154;&#24778;&#35766;&#21448;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#34920;&#31034;&#23398;&#20064;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#34920;&#29616;&#31967;&#31957;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08823v1 Announce Type: cross Abstract: We propose RanDumb to examine the efficacy of continual representation learning. RanDumb embeds raw pixels using a fixed random transform which approximates an RBF-Kernel, initialized before seeing any data, and learns a simple linear classifier on top. We present a surprising and consistent finding: RanDumb significantly outperforms the continually learned representations using deep networks across numerous continual learning benchmarks, demonstrating the poor performance of representation learning in these scenarios. RanDumb stores no exemplars and performs a single pass over the data, processing one sample at a time. It complements GDumb, operating in a low-exemplar regime where GDumb has especially poor performance. We reach the same consistent conclusions when RanDumb is extended to scenarios with pretrained models replacing the random transform with pretrained feature extractor. Our investigation is both surprising and alarming as
&lt;/p&gt;</description></item><item><title>&#39640;&#26031;&#21943;&#28293;&#25216;&#26415;&#30456;&#32467;&#21512;&#29289;&#29702;&#22522;&#30784;&#21160;&#30011;&#21644;3D&#39640;&#26031;&#21943;&#28293;&#65292;&#21487;&#20197;&#22312;&#34394;&#25311;&#22330;&#26223;&#20013;&#21019;&#36896;&#20986;&#26080;&#21487;&#27604;&#25311;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#23454;&#29616;&#28210;&#26579;&#12289;&#35270;&#22270;&#21512;&#25104;&#20197;&#21450;&#22266;&#20307;&#21644;&#27969;&#20307;&#30340;&#21160;&#24577;&#31649;&#29702;&#21644;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2401.15318</link><description>&lt;p&gt;
&#39640;&#26031;&#21943;&#28293;&#65306;&#21033;&#29992;&#39640;&#26031;&#39128;&#33853;&#21160;&#24577;&#21512;&#25104;&#27969;&#20307;
&lt;/p&gt;
&lt;p&gt;
Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting. (arXiv:2401.15318v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15318
&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#21943;&#28293;&#25216;&#26415;&#30456;&#32467;&#21512;&#29289;&#29702;&#22522;&#30784;&#21160;&#30011;&#21644;3D&#39640;&#26031;&#21943;&#28293;&#65292;&#21487;&#20197;&#22312;&#34394;&#25311;&#22330;&#26223;&#20013;&#21019;&#36896;&#20986;&#26080;&#21487;&#27604;&#25311;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#23454;&#29616;&#28210;&#26579;&#12289;&#35270;&#22270;&#21512;&#25104;&#20197;&#21450;&#22266;&#20307;&#21644;&#27969;&#20307;&#30340;&#21160;&#24577;&#31649;&#29702;&#21644;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#29289;&#29702;&#22522;&#30784;&#21160;&#30011;&#19982;3D&#39640;&#26031;&#21943;&#28293;&#65288;3DGS&#65289;&#30456;&#32467;&#21512;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#22312;&#20351;&#29992;3DGS&#37325;&#24314;&#30340;&#34394;&#25311;&#22330;&#26223;&#20013;&#21019;&#24314;&#26032;&#25928;&#26524;&#12290;&#21033;&#29992;&#39640;&#26031;&#21943;&#28293;&#21644;&#22522;&#20110;&#20301;&#32622;&#30340;&#21160;&#21147;&#23398;&#65288;PBD&#65289;&#22312;&#24213;&#23618;&#34920;&#31034;&#20013;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#20197;&#36830;&#36143;&#30340;&#26041;&#24335;&#31649;&#29702;&#28210;&#26579;&#12289;&#35270;&#22270;&#21512;&#25104;&#20197;&#21450;&#22266;&#20307;&#21644;&#27969;&#20307;&#30340;&#21160;&#24577;&#12290;&#31867;&#20284;&#20110;&#39640;&#26031;&#30528;&#33394;&#22120;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#27861;&#32447;&#22686;&#24378;&#27599;&#20010;&#39640;&#26031;&#26680;&#65292;&#23558;&#26680;&#30340;&#26041;&#21521;&#19982;&#34920;&#38754;&#27861;&#32447;&#23545;&#40784;&#65292;&#20197;&#25913;&#36827;PBD&#27169;&#25311;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#28040;&#38500;&#20102;&#22266;&#20307;&#26059;&#36716;&#21464;&#24418;&#20135;&#29983;&#30340;&#23574;&#23792;&#22122;&#22768;&#12290;&#23427;&#36824;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;&#28210;&#26579;&#38598;&#25104;&#21040;&#27969;&#20307;&#30340;&#21160;&#24577;&#34920;&#38754;&#21453;&#23556;&#20013;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#30495;&#23454;&#22320;&#22797;&#29616;&#21160;&#24577;&#27969;&#20307;&#19978;&#30340;&#34920;&#38754;&#20142;&#28857;&#65292;&#24182;&#20419;&#36827;&#22330;&#26223;&#23545;&#35937;&#19982;&#27969;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS. Leveraging the coherence of the Gaussian splatting and position-based dynamics (PBD) in the underlying representation, we manage rendering, view synthesis, and the dynamics of solids and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each Gaussian kernel with an added normal, aligning the kernel's orientation with the surface normal to refine the PBD simulation. This approach effectively eliminates spiky noises that arise from rotational deformation in solids. It also allows us to integrate physically based rendering to augment the dynamic surface reflections on fluids. Consequently, our framework is capable of realistically reproducing surface highlights on dynamic fluids and facilitating interactions between scene objects and fluids from new views. For more information, pl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13782</link><description>&lt;p&gt;
&#20174;&#25512;&#29305;&#21040;&#24341;&#29992;&#65306;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#23545;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21487;&#35265;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility. (arXiv:2401.13782v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20250;&#35758;&#19978;&#34987;&#25509;&#21463;&#30340;&#35770;&#25991;&#25968;&#37327;&#36798;&#21040;&#25968;&#21315;&#31687;&#65292;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#33719;&#21462;&#21644;&#38405;&#35835;&#30740;&#31350;&#35770;&#25991;&#21464;&#24471;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21487;&#35265;&#24615;&#20013;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#20182;&#20204;&#20998;&#20139;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#25324;8000&#22810;&#31687;&#35770;&#25991;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;2018&#24180;12&#26376;&#33267;2023&#24180;10&#26376;&#30340;&#25512;&#29305;&#65292;&#20197;&#21450;&#22522;&#20110;&#20986;&#29256;&#24180;&#20221;&#12289;&#20250;&#35758;&#22320;&#28857;&#21644;&#25688;&#35201;&#20027;&#39064;&#36827;&#34892;1&#65306;1&#21305;&#37197;&#30340;&#23545;&#29031;&#32452;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#31038;&#20132;&#23186;&#20307;&#22312;&#23398;&#26415;&#20132;&#27969;&#20013;&#30340;&#19981;&#26029;&#25193;&#22823;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#24403;&#20170;&#25968;&#23383;&#21270;&#26102;&#20195;&#19981;&#26029;&#21457;&#23637;&#30340;&#29983;&#24577;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside 1:1 matched controls based on publication year, venue, and abstract topics. Our analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. These findings highlight the expanding influence of social media in scholarly communication and underscore the importance of an evolving ecosystem in today's digital a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#20316;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26102;&#21051;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#24378;&#21046;LMMs&#36827;&#34892;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#26469;&#23398;&#20064;&#26102;&#25928;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2401.10711</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering. (arXiv:2401.10711v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#20316;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26102;&#21051;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#24378;&#21046;LMMs&#36827;&#34892;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#26469;&#23398;&#20064;&#26102;&#25928;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#38382;&#31572;&#65288;VideoQA&#65289;&#26088;&#22312;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#35270;&#39057;&#20449;&#24687;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#23613;&#31649;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#22270;&#20687;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#26041;&#38754;&#36824;&#19981;&#36275;&#22815;&#65292;&#20165;&#20165;&#26159;&#23558;&#22343;&#21248;&#37319;&#26679;&#30340;&#24103;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#65292;&#24573;&#30053;&#20102;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35270;&#35273;&#32447;&#32034;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#35270;&#39057;&#38382;&#31572;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#38024;&#23545;&#38382;&#39064;&#20851;&#38190;&#26102;&#38388;&#25139;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#24378;&#21046;LMMs&#20351;&#29992;&#38382;&#39064;&#20851;&#38190;&#26102;&#21051;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#21512;&#24182;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#20197;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#36825;&#20123;&#26102;&#21051;&#23558;&#20316;&#20026;&#20266;&#26631;&#31614;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#20266;&#26631;&#31614;&#20316;&#20026;&#39069;&#22806;&#30340;&#24369;&#30417;&#30563;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#12290;GCG&#23398;&#20064;&#22810;&#20010;&#39640;&#26031;&#20989;&#25968;&#26469;&#25551;&#36848;&#26102;&#25928;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video Question Answering (VideoQA) aims to answer natural language questions based on the information observed in videos. Despite the recent success of Large Multimodal Models (LMMs) in image-language understanding and reasoning, they deal with VideoQA insufficiently by simply taking uniformly sampled frames as visual inputs, which ignores question-relevant visual clues. Moreover, there are no human annotations for question-critical timestamps in existing VideoQA datasets. In light of this, we propose a novel weakly supervised framework to enforce the LMMs to reason out the answers with question-critical moments as visual inputs. Specifically, we fuse the question and answer pairs as event descriptions to find multiple keyframes as target moments, which will be pseudo-labels. With these pseudo-labels as additionally weak supervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG learns multiple Gaussian functions to characterize the temporal structure o
&lt;/p&gt;</description></item><item><title>ConvNet&#21644;Transformer&#26550;&#26500;&#22312;&#30417;&#30563;&#21644;CLIP&#35757;&#32451;&#19979;&#65292;&#36229;&#36234;&#20102;ImageNet&#20934;&#30830;&#29575;&#30340;&#23545;&#27604;&#20998;&#26512;&#20013;&#21457;&#29616;&#23427;&#20204;&#22312;&#38169;&#35823;&#31867;&#22411;&#12289;&#36755;&#20986;&#26657;&#20934;&#12289;&#21487;&#36716;&#31227;&#24615;&#21644;&#29305;&#24449;&#30340;&#19981;&#21464;&#24615;&#31561;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#31361;&#20986;&#20102;&#38656;&#35201;&#26356;&#21152;&#32454;&#33268;&#20998;&#26512;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.09215</link><description>&lt;p&gt;
ConvNet vs Transformer, Supervised vs CLIP: &#36229;&#36234;ImageNet&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;
ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy. (arXiv:2311.09215v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09215
&lt;/p&gt;
&lt;p&gt;
ConvNet&#21644;Transformer&#26550;&#26500;&#22312;&#30417;&#30563;&#21644;CLIP&#35757;&#32451;&#19979;&#65292;&#36229;&#36234;&#20102;ImageNet&#20934;&#30830;&#29575;&#30340;&#23545;&#27604;&#20998;&#26512;&#20013;&#21457;&#29616;&#23427;&#20204;&#22312;&#38169;&#35823;&#31867;&#22411;&#12289;&#36755;&#20986;&#26657;&#20934;&#12289;&#21487;&#36716;&#31227;&#24615;&#21644;&#29305;&#24449;&#30340;&#19981;&#21464;&#24615;&#31561;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#31361;&#20986;&#20102;&#38656;&#35201;&#26356;&#21152;&#32454;&#33268;&#20998;&#26512;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#20026;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#22810;&#31181;&#27169;&#22411;&#36873;&#25321;&#65292;&#23545;&#20110;&#29305;&#23450;&#24212;&#29992;&#20174;&#22810;&#20010;&#36873;&#39033;&#20013;&#36873;&#25321;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#19978;&#65292;&#36890;&#36807;&#23427;&#20204;&#22312;ImageNet&#19978;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#26469;&#27604;&#36739;&#31454;&#20105;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#21327;&#35758;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#21333;&#19968;&#25351;&#26631;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#21040;&#23545;&#20110;&#19987;&#19994;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#30340;&#24615;&#33021;&#32454;&#24494;&#24046;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;ConvNet&#21644;Vision Transformer&#26550;&#26500;&#22312;&#30417;&#30563;&#21644;CLIP&#35757;&#32451;&#33539;&#24335;&#19979;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#36229;&#36234;&#20102;ImageNet&#30340;&#20934;&#30830;&#29575;&#12290;&#23613;&#31649;&#25105;&#20204;&#36873;&#25321;&#30340;&#27169;&#22411;&#22312;ImageNet&#20934;&#30830;&#29575;&#21644;&#35745;&#31639;&#38656;&#27714;&#19978;&#30456;&#20284;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#22312;&#35768;&#22810;&#20854;&#20182;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65306;&#38169;&#35823;&#31867;&#22411;&#12289;&#36755;&#20986;&#26657;&#20934;&#12289;&#21487;&#36716;&#31227;&#24615;&#21644;&#29305;&#24449;&#30340;&#19981;&#21464;&#24615;&#31561;&#12290;&#20256;&#32479;&#25351;&#26631;&#26080;&#27861;&#25429;&#25417;&#21040;&#30340;&#36825;&#31181;&#27169;&#22411;&#29305;&#24615;&#24046;&#24322;&#65292;&#31361;&#20986;&#20102;&#22312;&#36873;&#25321;&#19981;&#21516;&#27169;&#22411;&#26102;&#38656;&#35201;&#26356;&#21152;&#32454;&#33268;&#20998;&#26512;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern computer vision offers a great variety of models to practitioners, and selecting a model from multiple options for specific applications can be challenging. Conventionally, competing model architectures and training protocols are compared by their classification accuracy on ImageNet. However, this single metric does not fully capture performance nuances critical for specialized tasks. In this work, we conduct an in-depth comparative analysis of model behaviors beyond ImageNet accuracy, for both ConvNet and Vision Transformer architectures, each across supervised and CLIP training paradigms. Although our selected models have similar ImageNet accuracies and compute requirements, we find that they differ in many other aspects: types of mistakes, output calibration, transferability, and feature invariance, among others. This diversity in model characteristics, not captured by traditional metrics, highlights the need for more nuanced analysis when choosing among different models. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#31574;&#30053;&#24615;&#25554;&#20837;&#21518;&#38376;&#65292;&#23545;&#40784;&#25935;&#24863;&#30701;&#35821;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#20197;&#21024;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08320</link><description>&lt;p&gt;
&#20351;&#29992;&#21518;&#38376;&#25216;&#26415;&#20445;&#25252;&#25105;&#20204;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Defending Our Privacy With Backdoors. (arXiv:2310.08320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#31574;&#30053;&#24615;&#25554;&#20837;&#21518;&#38376;&#65292;&#23545;&#40784;&#25935;&#24863;&#30701;&#35821;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#20197;&#21024;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#26410;&#32463;&#31579;&#36873;&#12289;&#24120;&#24120;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#32593;&#39029;&#25968;&#25454;&#35757;&#32451;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#38544;&#31169;&#38382;&#39064;&#25104;&#20026;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#12290;&#20854;&#20013;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21033;&#29992;&#38544;&#31169;&#25915;&#20987;&#30340;&#26041;&#27861;&#25552;&#21462;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21435;&#38500;&#29305;&#23450;&#20449;&#24687;&#26159;&#19968;&#20010;&#19981;&#23481;&#26131;&#35299;&#20915;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#27169;&#22411;&#20013;&#21024;&#38500;&#31169;&#20154;&#20449;&#24687;&#65292;&#22914;&#20010;&#20154;&#22995;&#21517;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#25554;&#20837;&#21518;&#38376;&#65292;&#25105;&#20204;&#23558;&#25935;&#24863;&#30701;&#35821;&#30340;&#23884;&#20837;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#23545;&#40784;&#65292;&#20363;&#22914;&#29992;"a person"&#20195;&#26367;&#20154;&#21517;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#36890;&#36807;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#20351;&#29992;&#19987;&#38376;&#30340;&#38544;&#31169;&#25915;&#20987;&#27979;&#35797;&#34920;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#21518;&#38376;&#30340;&#38450;&#24481;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;"&#21452;&#37325;&#29992;&#36884;"&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. Specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new "dual-use" perspecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#22810;&#20803;&#22270;&#20687;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20083;&#33146;&#25195;&#25551;&#24322;&#24120;&#23450;&#20301;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25506;&#32034;&#23436;&#25104;&#30340;&#22810;&#20803;&#24615;&#26469;&#25552;&#39640;&#35780;&#20272;&#26631;&#20934;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.03098</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#22810;&#20803;&#22270;&#20687;&#23436;&#25104;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#20083;&#33146;&#25195;&#25551;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Unsupervised anomaly localization in high-resolution breast scans using deep pluralistic image completion. (arXiv:2305.03098v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#22810;&#20803;&#22270;&#20687;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20083;&#33146;&#25195;&#25551;&#24322;&#24120;&#23450;&#20301;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25506;&#32034;&#23436;&#25104;&#30340;&#22810;&#20803;&#24615;&#26469;&#25552;&#39640;&#35780;&#20272;&#26631;&#20934;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#20083;&#33146;&#25668;&#24433;&#20013;&#30340;&#33258;&#21160;&#32959;&#30244;&#26816;&#27979;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#32959;&#30244;&#24456;&#23569;&#20986;&#29616;&#65292;&#20083;&#25151;&#32452;&#32455;&#21464;&#24322;&#21644;&#39640;&#20998;&#36776;&#29575;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#37492;&#20110;&#24322;&#24120;&#22270;&#20687;&#30340;&#31232;&#32570;&#24615;&#21644;&#27491;&#24120;&#22270;&#20687;&#30340;&#20016;&#23500;&#24615;&#65292;&#24322;&#24120;&#26816;&#27979;/&#23450;&#20301;&#26041;&#27861;&#21487;&#33021;&#38750;&#24120;&#36866;&#21512;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#20013;&#22823;&#37096;&#20998;&#24322;&#24120;&#23450;&#20301;&#30740;&#31350;&#38598;&#20013;&#22312;&#38750;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36866;&#24212;&#24615;&#19981;&#36275;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#22270;&#20687;&#23436;&#25104;&#35270;&#35282;&#19979;&#30340;&#20219;&#21153;&#24471;&#21040;&#32531;&#35299;&#65292;&#20854;&#20013;&#24322;&#24120;&#30340;&#23384;&#22312;&#21487;&#20197;&#36890;&#36807;&#21407;&#22987;&#22806;&#35266;&#19982;&#20854;&#29615;&#22659;&#26465;&#20214;&#19979;&#33258;&#21160;&#23436;&#25104;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25351;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;DBT&#25968;&#25454;&#38598;&#20013;&#65292;&#24448;&#24448;&#26377;&#24456;&#22810;&#30456;&#21516;&#29615;&#22659;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#30340;&#27491;&#24120;&#23436;&#25104;&#65292;&#20351;&#36825;&#20010;&#35780;&#20272;&#26631;&#20934;&#19981;&#22826;&#31934;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#25506;&#32034;&#23436;&#25104;&#30340;&#22810;&#20803;&#24615;&#26469;&#36827;&#34892;&#22270;&#20687;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated tumor detection in Digital Breast Tomosynthesis (DBT) is a difficult task due to natural tumor rarity, breast tissue variability, and high resolution. Given the scarcity of abnormal images and the abundance of normal images for this problem, an anomaly detection/localization approach could be well-suited. However, most anomaly localization research in machine learning focuses on non-medical datasets, and we find that these methods fall short when adapted to medical imaging datasets. The problem is alleviated when we solve the task from the image completion perspective, in which the presence of anomalies can be indicated by a discrepancy between the original appearance and its auto-completion conditioned on the surroundings. However, there are often many valid normal completions given the same surroundings, especially in the DBT dataset, making this evaluation criterion less precise. To address such an issue, we consider pluralistic image completion by exploring the distributi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057; HMR &#26694;&#26550;&#65288;DDT&#65289;&#65292;&#23427;&#26088;&#22312;&#20174;&#36755;&#20837;&#24207;&#21015;&#20013;&#35299;&#30721;&#29305;&#23450;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#22686;&#24378;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#24182;&#36755;&#20986;&#25152;&#26377;&#24103;&#30340;&#20154;&#20307;&#32593;&#26684;&#65292;&#20351;&#24471; DDT &#26356;&#36866;&#29992;&#20110;&#26102;&#38388;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.13397</link><description>&lt;p&gt;
DDT&#65306;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#20174;&#35270;&#39057;&#20013;&#24674;&#22797;&#20154;&#20307;&#32593;&#26684;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh Recovery from a Video. (arXiv:2303.13397v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13397
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057; HMR &#26694;&#26550;&#65288;DDT&#65289;&#65292;&#23427;&#26088;&#22312;&#20174;&#36755;&#20837;&#24207;&#21015;&#20013;&#35299;&#30721;&#29305;&#23450;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#22686;&#24378;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#24182;&#36755;&#20986;&#25152;&#26377;&#24103;&#30340;&#20154;&#20307;&#32593;&#26684;&#65292;&#20351;&#24471; DDT &#26356;&#36866;&#29992;&#20110;&#26102;&#38388;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#32593;&#26684;&#24674;&#22797;&#65288;HMR&#65289;&#20026;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20154;&#20307;&#20449;&#24687;&#65292;&#20363;&#22914;&#28216;&#25103;&#12289;&#20154;&#26426;&#20132;&#20114;&#21644;&#34394;&#25311;&#29616;&#23454;&#12290;&#19982;&#21333;&#19968;&#22270;&#20687;&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;&#35270;&#39057;&#30340;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#36890;&#36807;&#34701;&#21512;&#20154;&#20307;&#36816;&#21160;&#20808;&#39564;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20687; VIBE &#36825;&#26679;&#30340;&#22810;&#23545;&#22810;&#26041;&#27861;&#23384;&#22312;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#25361;&#25112;&#12290;&#32780;&#20687; TCMR &#21644; MPS-Net &#36825;&#26679;&#30340;&#22810;&#23545;&#19968;&#26041;&#27861;&#21017;&#20381;&#36182;&#20110;&#26410;&#26469;&#24103;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26159;&#38750;&#22240;&#26524;&#21644;&#26102;&#38388;&#25928;&#29575;&#20302;&#19979;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057; HMR &#26694;&#26550;&#65288;DDT&#65289;&#12290;DDT &#26088;&#22312;&#20174;&#36755;&#20837;&#24207;&#21015;&#20013;&#35299;&#30721;&#29305;&#23450;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#22686;&#24378;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#20316;&#20026;&#19968;&#31181;&#22810;&#23545;&#22810;&#26041;&#27861;&#65292;DDT &#30340;&#35299;&#30721;&#22120;&#36755;&#20986;&#25152;&#26377;&#24103;&#30340;&#20154;&#20307;&#32593;&#26684;&#65292;&#20351; DDT &#26356;&#36866;&#29992;&#20110;&#26102;&#38388;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human mesh recovery (HMR) provides rich human body information for various real-world applications such as gaming, human-computer interaction, and virtual reality. Compared to single image-based methods, video-based methods can utilize temporal information to further improve performance by incorporating human body motion priors. However, many-to-many approaches such as VIBE suffer from motion smoothness and temporal inconsistency. While many-to-one approaches such as TCMR and MPS-Net rely on the future frames, which is non-causal and time inefficient during inference. To address these challenges, a novel Diffusion-Driven Transformer-based framework (DDT) for video-based HMR is presented. DDT is designed to decode specific motion patterns from the input sequence, enhancing motion smoothness and temporal consistency. As a many-to-many approach, the decoder of our DDT outputs the human mesh of all the frames, making DDT more viable for real-world applications where time efficiency is cruc
&lt;/p&gt;</description></item><item><title>&#25152;&#25552;&#20986;&#30340;&#25289;&#26222;&#25289;&#26031;&#20998;&#21106;&#32593;&#32476;&#21487;&#21516;&#26102;&#25429;&#33719;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#35748;&#30693;&#21644;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#65292;&#25104;&#21151;&#23558;&#39640;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20998;&#37197;&#21040;OOF&#30446;&#26631;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.13123</link><description>&lt;p&gt;
&#25289;&#26222;&#25289;&#26031;&#20998;&#21106;&#32593;&#32476;: &#20174;&#31354;&#38388;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#21040;&#25913;&#36827;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Laplacian Segmentation Networks: Improved Epistemic Uncertainty from Spatial Aleatoric Uncertainty. (arXiv:2303.13123v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13123
&lt;/p&gt;
&lt;p&gt;
&#25152;&#25552;&#20986;&#30340;&#25289;&#26222;&#25289;&#26031;&#20998;&#21106;&#32593;&#32476;&#21487;&#21516;&#26102;&#25429;&#33719;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#35748;&#30693;&#21644;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#65292;&#25104;&#21151;&#23558;&#39640;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20998;&#37197;&#21040;OOF&#30446;&#26631;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23186;&#20307;&#22270;&#20687;&#24120;&#24120;&#20250;&#20986;&#29616;&#38750;&#27491;&#24120;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&#22240;&#20026;&#20301;&#32622;&#25110;&#25195;&#25551;&#22120;&#30340;&#19981;&#21516;&#25110;&#22270;&#20687;&#25439;&#22351;&#31561;&#21407;&#22240;&#32780;&#32463;&#24120;&#20986;&#29616;&#65292;&#36825;&#31181;&#23186;&#20307;&#22270;&#20687;&#21487;&#33021;&#20250;&#23545;&#19979;&#28216;&#20020;&#24202;&#35786;&#26029;&#25110;&#27835;&#30103;&#20135;&#29983;&#24433;&#21709;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#36825;&#31181;&#38169;&#35823;&#20998;&#21106;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25289;&#26222;&#25289;&#26031;&#20998;&#21106;&#32593;&#32476;&#65288;LSN&#65289;&#65292;&#20854;&#33021;&#22815;&#20849;&#21516;&#24314;&#27169;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#35748;&#30693;&#65288;&#27169;&#22411;&#65289;&#19981;&#30830;&#23450;&#24615;&#21644;&#31354;&#38388;&#25968;&#25454;&#65288;&#38543;&#26426;&#65289;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#31354;&#38388;&#30456;&#20851;&#24615;&#30340;logit&#20998;&#24067;&#25429;&#33719;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#23545;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#39640;&#32500;&#36755;&#20986;&#21644;&#20855;&#26377;&#36339;&#36807;&#36830;&#25509;&#30340;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#31532;&#19968;&#20010;&#25289;&#26222;&#25289;&#26031;&#26435;&#37325;&#21518;&#39564;&#30340;&#36924;&#36817;&#12290;&#20174;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24314;&#27169;&#31354;&#38388;&#20687;&#32032;&#30456;&#20851;&#24615;&#20351;&#24471;&#25289;&#26222;&#25289;&#26031;&#20998;&#21106;&#32593;&#32476;&#33021;&#22815;&#23558;&#39640;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#25104;&#21151;&#20998;&#37197;&#21040;&#22270;&#20687;&#20013;&#30340;OOF&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out of distribution (OOD) medical images are frequently encountered, e.g. because of site- or scanner differences, or image corruption. OOD images come with a risk of incorrect image segmentation, potentially negatively affecting downstream diagnoses or treatment. To ensure robustness to such incorrect segmentations, we propose Laplacian Segmentation Networks (LSN) that jointly model epistemic (model) and aleatoric (data) uncertainty in image segmentation. We capture data uncertainty with a spatially correlated logit distribution. For model uncertainty, we propose the first Laplace approximation of the weight posterior that scales to large neural networks with skip connections that have high-dimensional outputs. Empirically, we demonstrate that modelling spatial pixel correlation allows the Laplacian Segmentation Network to successfully assign high epistemic uncertainty to out-of-distribution objects appearing within images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#36755;&#20837;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20004;&#20010;&#28508;&#22312;&#32534;&#30721;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#35821;&#20041;&#39118;&#26684;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#37319;&#26679;&#25216;&#26415;&#21644;&#26102;&#38388;&#27493;&#30456;&#20851;&#30340;&#28508;&#22312;&#26435;&#37325;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.14368</link><description>&lt;p&gt;
&#23454;&#29616;&#25193;&#23637;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Enhanced Controllability of Diffusion Models. (arXiv:2302.14368v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#36755;&#20837;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20004;&#20010;&#28508;&#22312;&#32534;&#30721;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#35821;&#20041;&#39118;&#26684;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#37319;&#26679;&#25216;&#26415;&#21644;&#26102;&#38388;&#27493;&#30456;&#20851;&#30340;&#28508;&#22312;&#26435;&#37325;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#36924;&#30495;&#12289;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#21487;&#25511;&#31243;&#24230;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#21463;&#22522;&#20110;GAN&#28508;&#22312;&#31354;&#38388;&#30340;&#22270;&#20687;&#25805;&#32437;&#25216;&#26415;&#21551;&#21457;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26465;&#20214;&#20110;&#20004;&#20010;&#28508;&#22312;&#32534;&#30721;&#12289;&#19968;&#20010;&#31354;&#38388;&#20869;&#23481;&#25513;&#30721;&#21644;&#19968;&#20010;&#25153;&#24179;&#30340;&#26679;&#24335;&#23884;&#20837;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#20381;&#36182;&#20110;&#25193;&#25955;&#27169;&#22411;&#28176;&#36827;&#21435;&#22122;&#36807;&#31243;&#30340;&#24863;&#24615;&#20559;&#32622;&#65292;&#22312;&#31354;&#38388;&#32467;&#26500;&#25513;&#30721;&#20013;&#32534;&#30721;&#23039;&#21183;/&#24067;&#23616;&#20449;&#24687;&#65292;&#22312;&#26679;&#24335;&#20195;&#30721;&#20013;&#32534;&#30721;&#35821;&#20041;/&#26679;&#24335;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#37319;&#26679;&#25216;&#26415;&#26469;&#25913;&#21892;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#21487;&#32452;&#21512;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20801;&#35768;&#37096;&#20998;&#20381;&#36182;&#20110;&#26465;&#20214;&#36755;&#20837;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#23545;&#27599;&#20010;&#28508;&#22312;&#20195;&#30721;&#21644;&#23427;&#20204;&#30340;&#32852;&#21512;&#20998;&#24067;&#37327;&#30340;&#25511;&#21046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26102;&#38388;&#27493;&#30456;&#20851;&#30340;&#20869;&#23481;&#21644;&#26679;&#24335;&#28508;&#22312;&#26435;&#37325;&#35843;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#25511;&#21046;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion models have shown remarkable capabilities in generating realistic, high-quality and diverse images. However, the extent of controllability during generation is underexplored. Inspired by techniques based on GAN latent space for image manipulation, we train a diffusion model conditioned on two latent codes, a spatial content mask and a flattened style embedding. We rely on the inductive bias of the progressive denoising process of diffusion models to encode pose/layout information in the spatial structure mask and semantic/style information in the style code. We propose two generic sampling techniques for improving controllability. We extend composable diffusion models to allow for some dependence between conditional inputs, to improve the quality of generations while also providing control over the amount of guidance from each latent code and their joint distribution. We also propose timestep dependent weight scheduling for content and style latents to further impro
&lt;/p&gt;</description></item></channel></rss>