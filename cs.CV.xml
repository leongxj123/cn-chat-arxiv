<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32441;&#29702;&#19982;&#24418;&#29366;&#20559;&#35265;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#27604;&#35270;&#35273;&#32534;&#30721;&#22120;&#26356;&#20559;&#21521;&#24418;&#29366;&#65292;&#26263;&#31034;&#35270;&#35273;&#20559;&#35265;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20250;&#21463;&#21040;&#25991;&#26412;&#30340;&#35843;&#33410;</title><link>https://arxiv.org/abs/2403.09193</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26159;&#32441;&#29702;&#20559;&#35265;&#36824;&#26159;&#24418;&#29366;&#20559;&#35265;&#65292;&#25105;&#20204;&#21487;&#20197;&#24341;&#23548;&#23427;&#20204;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Vision Language Models Texture or Shape Biased and Can We Steer Them?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32441;&#29702;&#19982;&#24418;&#29366;&#20559;&#35265;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#27604;&#35270;&#35273;&#32534;&#30721;&#22120;&#26356;&#20559;&#21521;&#24418;&#29366;&#65292;&#26263;&#31034;&#35270;&#35273;&#20559;&#35265;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20250;&#21463;&#21040;&#25991;&#26412;&#30340;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09193v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#30701;&#30701;&#20960;&#24180;&#20869;&#24443;&#24213;&#25913;&#21464;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#26684;&#23616;&#65292;&#24320;&#21551;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#24212;&#29992;&#65292;&#20174;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#21040;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#65292;&#20877;&#21040;&#35270;&#35273;&#38382;&#31572;&#12290;&#19982;&#32431;&#35270;&#35273;&#27169;&#22411;&#19981;&#21516;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#36890;&#36807;&#35821;&#35328;&#25552;&#31034;&#35775;&#38382;&#35270;&#35273;&#20869;&#23481;&#30340;&#30452;&#35266;&#26041;&#24335;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#24341;&#21457;&#25105;&#20204;&#24605;&#32771;&#23427;&#20204;&#26159;&#21542;&#20063;&#19982;&#20154;&#31867;&#35270;&#35273;&#19968;&#33268; - &#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#26377;&#22810;&#22823;&#31243;&#24230;&#22320;&#37319;&#29992;&#20102;&#20154;&#31867;&#24341;&#23548;&#30340;&#35270;&#35273;&#20559;&#35265;&#65292;&#25110;&#32773;&#23427;&#20204;&#26159;&#21542;&#21482;&#26159;&#20174;&#32431;&#35270;&#35273;&#27169;&#22411;&#20013;&#32487;&#25215;&#20102;&#20559;&#35265;&#12290;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#35270;&#35273;&#20559;&#35265;&#26159;&#32441;&#29702;&#19982;&#24418;&#29366;&#20559;&#35265;&#65292;&#21363;&#23616;&#37096;&#20449;&#24687;&#30340;&#20027;&#23548;&#22320;&#20301;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#27969;&#34892;&#30340;VLMs&#20013;&#30340;&#36825;&#31181;&#20559;&#35265;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;VLMs&#36890;&#24120;&#27604;&#23427;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#26356;&#20559;&#21521;&#20110;&#24418;&#29366;&#65292;&#36825;&#34920;&#26126;&#35270;&#35273;&#20559;&#35265;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#36890;&#36807;&#25991;&#26412;&#36827;&#34892;&#35843;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09193v1 Announce Type: cross  Abstract: Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, they offer an intuitive way to access visual content through language prompting. The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models. One important visual bias is the texture vs. shape bias, or the dominance of local over global information. In this paper, we study this bias in a wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#25968;&#25454;&#22686;&#24378;&#36807;&#31243;&#65292;&#20026;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.08352</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21450;&#19982;&#20256;&#32479;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24615;&#33021;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08352
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#25968;&#25454;&#22686;&#24378;&#36807;&#31243;&#65292;&#20026;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#34987;&#35748;&#20026;&#26159;&#24120;&#29992;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#30340;&#26368;&#37325;&#35201;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#23427;&#20027;&#35201;&#28041;&#21450;&#24212;&#29992;&#36866;&#24403;&#30340;&#25968;&#25454;&#36716;&#25442;&#25805;&#20316;&#65292;&#20197;&#21019;&#24314;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#26032;&#25968;&#25454;&#26679;&#26412;&#12290;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;&#36825;&#19968;&#36807;&#31243;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25163;&#21160;&#21019;&#24314;&#21644;&#27979;&#35797;&#19981;&#21516;&#20505;&#36873;&#22686;&#24378;&#21450;&#20854;&#36229;&#21442;&#25968;&#38656;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#12290;&#33258;&#21160;&#21270;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#21407;&#21017;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20110;AutoML&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20351;&#29992;AutoML&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#25805;&#20316;&#12289;&#25968;&#25454;&#38598;&#25104;&#21644;&#25968;&#25454;&#21512;&#25104;&#25216;&#26415;&#12290;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08352v1 Announce Type: cross  Abstract: Data augmentation is arguably the most important regularization technique commonly used to improve generalization performance of machine learning models. It primarily involves the application of appropriate data transformation operations to create new data samples with desired properties. Despite its effectiveness, the process is often challenging because of the time-consuming trial and error procedures for creating and testing different candidate augmentations and their hyperparameters manually. Automated data augmentation methods aim to automate the process. State-of-the-art approaches typically rely on automated machine learning (AutoML) principles. This work presents a comprehensive survey of AutoML-based data augmentation techniques. We discuss various approaches for accomplishing data augmentation with AutoML, including data manipulation, data integration and data synthesis techniques. We present extensive discussion of technique
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#26041;&#27861;$\sigma$-zero&#65292;&#20854;&#21033;&#29992;&#20102;$\ell_0$&#33539;&#25968;&#30340;&#21487;&#24494;&#36817;&#20284;&#21644;&#33258;&#36866;&#24212;&#25237;&#24433;&#36816;&#31639;&#31526;&#65292;&#33021;&#22815;&#22312;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#32422;&#26463;&#19979;&#20248;&#21270;&#65292;&#20174;&#32780;&#35780;&#20272;&#28145;&#24230;&#32593;&#32476;&#23545;&#31232;&#30095;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01879</link><description>&lt;p&gt;
$\sigma$-zero: &#22522;&#20110;&#26799;&#24230;&#30340;$\ell_0$-&#33539;&#25968;&#23545;&#25239;&#26679;&#26412;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
$\sigma$-zero: Gradient-based Optimization of $\ell_0$-norm Adversarial Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01879
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#26041;&#27861;$\sigma$-zero&#65292;&#20854;&#21033;&#29992;&#20102;$\ell_0$&#33539;&#25968;&#30340;&#21487;&#24494;&#36817;&#20284;&#21644;&#33258;&#36866;&#24212;&#25237;&#24433;&#36816;&#31639;&#31526;&#65292;&#33021;&#22815;&#22312;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#32422;&#26463;&#19979;&#20248;&#21270;&#65292;&#20174;&#32780;&#35780;&#20272;&#28145;&#24230;&#32593;&#32476;&#23545;&#31232;&#30095;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#32593;&#32476;&#23545;&#22522;&#20110;&#26799;&#24230;&#25915;&#20987;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#25915;&#20987;&#32771;&#34385;$\ell_2$&#21644;$\ell_\infty$&#33539;&#25968;&#32422;&#26463;&#26469;&#21046;&#36896;&#36755;&#20837;&#25200;&#21160;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#20102;&#31232;&#30095;&#30340;$\ell_1$&#21644;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#12290;&#29305;&#21035;&#26159;&#65292;&#30001;&#20110;&#22312;&#38750;&#20984;&#19988;&#38750;&#21487;&#24494;&#32422;&#26463;&#19978;&#36827;&#34892;&#20248;&#21270;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#26159;&#30740;&#31350;&#26368;&#23569;&#30340;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#25915;&#20987;&#35780;&#20272;&#23545;&#25239;&#40065;&#26834;&#24615;&#21487;&#20197;&#25581;&#31034;&#22312;&#26356;&#20256;&#32479;&#30340;$\ell_2$&#21644;$\ell_\infty$&#33539;&#25968;&#25915;&#20987;&#20013;&#26410;&#33021;&#27979;&#35797;&#20986;&#30340;&#24369;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#65292;&#31216;&#20026;$\sigma$-zero&#65292;&#23427;&#21033;&#29992;&#20102;$\ell_0$&#33539;&#25968;&#30340;&#19968;&#20010;&#29305;&#27530;&#21487;&#24494;&#36817;&#20284;&#26469;&#20419;&#36827;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#24182;&#21033;&#29992;&#33258;&#36866;&#24212;&#25237;&#24433;&#36816;&#31639;&#31526;&#21160;&#24577;&#35843;&#25972;&#25439;&#22833;&#26368;&#23567;&#21270;&#21644;&#25200;&#21160;&#31232;&#30095;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#22312;MNIST&#12289;CIFAR10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Evaluating the adversarial robustness of deep networks to gradient-based attacks is challenging. While most attacks consider $\ell_2$- and $\ell_\infty$-norm constraints to craft input perturbations, only a few investigate sparse $\ell_1$- and $\ell_0$-norm attacks. In particular, $\ell_0$-norm attacks remain the least studied due to the inherent complexity of optimizing over a non-convex and non-differentiable constraint. However, evaluating adversarial robustness under these attacks could reveal weaknesses otherwise left untested with more conventional $\ell_2$- and $\ell_\infty$-norm attacks. In this work, we propose a novel $\ell_0$-norm attack, called $\sigma$-zero, which leverages an ad hoc differentiable approximation of the $\ell_0$ norm to facilitate gradient-based optimization, and an adaptive projection operator to dynamically adjust the trade-off between loss minimization and perturbation sparsity. Extensive evaluations using MNIST, CIFAR10, and ImageNet datasets, involving
&lt;/p&gt;</description></item><item><title>DoraemonGPT&#26159;&#19968;&#20010;&#30001;LLMs&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#22788;&#29702;&#21160;&#24577;&#35270;&#39057;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#31526;&#21495;&#35760;&#24518;&#26469;&#36827;&#34892;&#31354;&#38388;-&#26102;&#38388;&#26597;&#35810;&#21644;&#25512;&#29702;&#65292;&#24182;&#21462;&#24471;&#31616;&#27905;&#30340;&#20013;&#38388;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.08392</link><description>&lt;p&gt;
DoraemonGPT&#65306;&#26397;&#21521;&#29702;&#35299;&#20855;&#26377;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#22330;&#26223;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08392
&lt;/p&gt;
&lt;p&gt;
DoraemonGPT&#26159;&#19968;&#20010;&#30001;LLMs&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#22788;&#29702;&#21160;&#24577;&#35270;&#39057;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#31526;&#21495;&#35760;&#24518;&#26469;&#36827;&#34892;&#31354;&#38388;-&#26102;&#38388;&#26597;&#35810;&#21644;&#25512;&#29702;&#65292;&#24182;&#21462;&#24471;&#31616;&#27905;&#30340;&#20013;&#38388;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30001;LLM&#39537;&#21160;&#30340;&#35270;&#35273;&#20195;&#29702;&#20027;&#35201;&#38598;&#20013;&#20110;&#35299;&#20915;&#22522;&#20110;&#22270;&#20687;&#30340;&#20219;&#21153;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#29702;&#35299;&#21160;&#24577;&#22330;&#26223;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#36828;&#31163;&#20687;&#24341;&#23548;&#23398;&#29983;&#36827;&#34892;&#23454;&#39564;&#23460;&#23454;&#39564;&#21644;&#35782;&#21035;&#38169;&#35823;&#36825;&#26679;&#30340;&#30495;&#23454;&#24212;&#29992;&#12290;&#32771;&#34385;&#21040;&#35270;&#39057;&#27169;&#24577;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#19981;&#26029;&#21464;&#21270;&#24615;&#36136;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;DoraemonGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#32508;&#21512;&#27010;&#24565;&#31616;&#27905;&#31995;&#32479;&#65292;&#29992;&#20110;&#22788;&#29702;&#21160;&#24577;&#35270;&#39057;&#20219;&#21153;&#12290;&#32473;&#23450;&#19968;&#20010;&#24102;&#26377;&#38382;&#39064;/&#20219;&#21153;&#30340;&#35270;&#39057;&#65292;DoraemonGPT&#39318;&#20808;&#23558;&#36755;&#20837;&#35270;&#39057;&#36716;&#25442;&#20026;&#23384;&#20648;&#19982;&#20219;&#21153;&#30456;&#20851;&#23646;&#24615;&#30340;&#31526;&#21495;&#23384;&#20648;&#22120;&#12290;&#36825;&#31181;&#32467;&#26500;&#21270;&#34920;&#31034;&#20801;&#35768;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23376;&#20219;&#21153;&#24037;&#20855;&#36827;&#34892;&#31354;&#38388;-&#26102;&#38388;&#26597;&#35810;&#21644;&#25512;&#29702;&#65292;&#20174;&#32780;&#20135;&#29983;&#31616;&#27905;&#30340;&#20013;&#38388;&#32467;&#26524;&#12290;&#37492;&#20110;LLM&#22312;&#28041;&#21450;&#19987;&#19994;&#39046;&#22495;&#65288;&#20363;&#22914;&#20998;&#26512;&#23454;&#39564;&#20013;&#28508;&#22312;&#30340;&#31185;&#23398;&#21407;&#29702;&#65289;&#26102;&#20855;&#26377;&#26377;&#38480;&#30340;&#20869;&#37096;&#30693;&#35782;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08392v2 Announce Type: replace-cross  Abstract: Recent LLM-driven visual agents mainly focus on solving image-based tasks, which limits their ability to understand dynamic scenes, making it far from real-life applications like guiding students in laboratory experiments and identifying their mistakes. Considering the video modality better reflects the ever-changing nature of real-world scenarios, we devise DoraemonGPT, a comprehensive and conceptually elegant system driven by LLMs to handle dynamic video tasks. Given a video with a question/task, DoraemonGPT begins by converting the input video into a symbolic memory that stores task-related attributes. This structured representation allows for spatial-temporal querying and reasoning by well-designed sub-task tools, resulting in concise intermediate results. Recognizing that LLMs have limited internal knowledge when it comes to specialized domains (e.g., analyzing the scientific principles underlying experiments), we incorpor
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#22686;&#24378;&#26159;&#19968;&#31181;&#21033;&#29992;dropout&#25110;PCA&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36716;&#25442;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22312;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#28145;&#24230;&#22686;&#24378;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30417;&#30563;&#38382;&#39064;&#19978;&#25928;&#26524;&#30456;&#21453;&#12290;</title><link>https://arxiv.org/abs/2303.14537</link><description>&lt;p&gt;
&#28145;&#24230;&#22686;&#24378;&#65306;&#22312;&#28608;&#27963;&#31354;&#38388;&#20013;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Deep Augmentation: Self-Supervised Learning with Transformations in Activation Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.14537
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22686;&#24378;&#26159;&#19968;&#31181;&#21033;&#29992;dropout&#25110;PCA&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36716;&#25442;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22312;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#28145;&#24230;&#22686;&#24378;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30417;&#30563;&#38382;&#39064;&#19978;&#25928;&#26524;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36749;&#23398;&#25110;PCA&#26469;&#36716;&#25442;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30446;&#26631;&#23618;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#23398;&#20064;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#26469;&#23637;&#31034;&#28145;&#24230;&#22686;&#24378;&#12290; &#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#65292;&#22914;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#19978;&#28145;&#24230;&#22686;&#24378;&#33021;&#22815;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30456;&#24212;&#30340;&#30417;&#30563;&#38382;&#39064;&#19978;&#35266;&#23519;&#21040;&#30456;&#21453;&#30340;&#25928;&#26524;&#12290; &#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#28145;&#24230;&#22686;&#24378;&#20943;&#36731;&#20102;&#23618;&#20043;&#38388;&#30340;&#30456;&#20114;&#36866;&#24212;&#65292;&#21363;"&#23849;&#28291;"&#24418;&#24335;&#30340;&#38382;&#39064;&#12290; &#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#21046;&#23450;&#20102;&#19968;&#31181;&#36873;&#25321;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65307;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#29992;&#28145;&#24230;&#22686;&#24378;&#23450;&#20301;&#26356;&#28145;&#23618;&#27425;&#30340;&#23618;&#35201;&#20248;&#20110;&#22686;&#24378;&#36755;&#20837;&#25968;&#25454;&#12290; &#36825;&#31181;&#26041;&#27861;&#30340;&#31616;&#21333;&#32593;&#32476;&#21644;&#27169;&#24577;&#26080;&#20851;&#24615;&#20351;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.14537v2 Announce Type: replace-cross  Abstract: We introduce Deep Augmentation, an approach to implicit data augmentation using dropout or PCA to transform a targeted layer within a neural network to improve performance and generalization. We demonstrate Deep Augmentation through extensive experiments on contrastive learning tasks in NLP, computer vision, and graph learning. We observe substantial performance gains with Transformers, ResNets, and Graph Neural Networks as the underlying models in contrastive learning, but observe inverse effects on the corresponding supervised problems. Our analysis suggests that Deep Augmentation alleviates co-adaption between layers, a form of "collapse." We use this observation to formulate a method for selecting which layer to target; in particular, our experimentation reveals that targeting deeper layers with Deep Augmentation outperforms augmenting the input data. The simple network- and modality-agnostic nature of this approach enables
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#23545;&#22330;&#26223;&#35782;&#21035;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20302;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#30340;&#23478;&#24237;&#29031;&#29255;&#20013;&#26174;&#31034;&#20986;&#26356;&#20302;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#21644;&#20998;&#31867;&#32622;&#20449;&#24230;&#65292;&#24182;&#26356;&#23481;&#26131;&#20998;&#37197;&#20855;&#26377;&#20882;&#29359;&#24615;&#30340;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2401.13097</link><description>&lt;p&gt;
&#22330;&#26223;&#35782;&#21035;&#20013;&#30340;&#25968;&#23383;&#40511;&#27807;&#65306;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Digital Divides in Scene Recognition: Uncovering Socioeconomic Biases in Deep Learning Systems. (arXiv:2401.13097v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13097
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#23545;&#22330;&#26223;&#35782;&#21035;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20302;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#30340;&#23478;&#24237;&#29031;&#29255;&#20013;&#26174;&#31034;&#20986;&#26356;&#20302;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#21644;&#20998;&#31867;&#32622;&#20449;&#24230;&#65292;&#24182;&#26356;&#23481;&#26131;&#20998;&#37197;&#20855;&#26377;&#20882;&#29359;&#24615;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35745;&#31639;&#26426;&#30340;&#22330;&#26223;&#29702;&#35299;&#24433;&#21709;&#20102;&#20174;&#22478;&#24066;&#35268;&#21010;&#21040;&#33258;&#21160;&#39550;&#39542;&#30340;&#39046;&#22495;&#65292;&#28982;&#32780;&#25105;&#20204;&#23545;&#36825;&#20123;&#25216;&#26415;&#22312;&#31038;&#20250;&#24046;&#24322;&#20013;&#30340;&#34920;&#29616;&#20102;&#35299;&#29978;&#23569;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;dCNNs&#65289;&#22312;&#22330;&#26223;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#20840;&#29699;&#21644;&#32654;&#22269;&#30340;&#36817;&#30334;&#19975;&#24352;&#22270;&#29255;&#65292;&#21253;&#25324;&#29992;&#25143;&#25552;&#20132;&#30340;&#23478;&#24237;&#29031;&#29255;&#21644;Airbnb&#30340;&#25151;&#28304;&#29031;&#29255;&#12290;&#25105;&#20204;&#36816;&#29992;&#20102;&#32479;&#35745;&#27169;&#22411;&#65292;&#23545;&#23478;&#24237;&#25910;&#20837;&#12289;&#20154;&#31867;&#21457;&#23637;&#25351;&#25968;&#65288;HDI&#65289;&#31561;&#31038;&#20250;&#32463;&#27982;&#25351;&#26631;&#20197;&#21450;&#20844;&#24320;&#25968;&#25454;&#26469;&#28304;&#65288;CIA&#21644;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#65289;&#30340;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#23545;dCNNs&#30340;&#34920;&#29616;&#24433;&#21709;&#36827;&#34892;&#20102;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#65292;&#39044;&#35757;&#32451;&#30340;dCNNs&#34920;&#29616;&#20986;&#26356;&#20302;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#12289;&#26356;&#20302;&#30340;&#20998;&#31867;&#32622;&#20449;&#24230;&#65292;&#20197;&#21450;&#26356;&#39640;&#30340;&#20542;&#21521;&#24615;&#22312;&#20302;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#30340;&#23478;&#24237;&#65288;&#20363;&#22914;&#8220;&#24223;&#22687;&#8221;&#65292;&#8220;&#36139;&#27665;&#31391;&#8221;&#65289;&#30340;&#22270;&#29255;&#20013;&#20998;&#37197;&#20855;&#26377;&#20882;&#29359;&#24615;&#30340;&#26631;&#31614;&#12290;&#36825;&#31181;&#36235;&#21183;&#26159;&#25345;&#32493;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer-based scene understanding has influenced fields ranging from urban planning to autonomous vehicle performance, yet little is known about how well these technologies work across social differences. We investigate the biases of deep convolutional neural networks (dCNNs) in scene classification, using nearly one million images from global and US sources, including user-submitted home photographs and Airbnb listings. We applied statistical models to quantify the impact of socioeconomic indicators such as family income, Human Development Index (HDI), and demographic factors from public data sources (CIA and US Census) on dCNN performance. Our analyses revealed significant socioeconomic bias, where pretrained dCNNs demonstrated lower classification accuracy, lower classification confidence, and a higher tendency to assign labels that could be offensive when applied to homes (e.g., "ruin", "slum"), especially in images from homes with lower socioeconomic status (SES). This trend is c
&lt;/p&gt;</description></item></channel></rss>