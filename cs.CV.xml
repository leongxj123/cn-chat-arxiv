<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#20351;&#29992;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;ante-hoc&#26041;&#27861;&#23558;&#20020;&#24202;&#27010;&#24565;&#24341;&#20837;&#21040;&#20998;&#31867;&#31649;&#36947;&#20013;&#65292;&#25552;&#20379;&#20102;&#32954;&#30284;&#26816;&#27979;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#65292;&#30456;&#36739;&#20110;&#22522;&#32447;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65288;F1 &gt; 0.9&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.19444</link><description>&lt;p&gt;
&#36879;&#26126;&#19988;&#20020;&#24202;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#32954;&#30284;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transparent and Clinically Interpretable AI for Lung Cancer Detection in Chest X-Rays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19444
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;ante-hoc&#26041;&#27861;&#23558;&#20020;&#24202;&#27010;&#24565;&#24341;&#20837;&#21040;&#20998;&#31867;&#31649;&#36947;&#20013;&#65292;&#25552;&#20379;&#20102;&#32954;&#30284;&#26816;&#27979;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#65292;&#30456;&#36739;&#20110;&#22522;&#32447;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65288;F1 &gt; 0.9&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19444v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#31616;&#35201;&#25688;&#35201;&#65306;&#36879;&#26126;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#26088;&#22312;&#35299;&#20915;&#22797;&#26434;&#40657;&#21283;&#23376;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#20449;&#20219;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#20107;&#21518;XAI&#25216;&#26415;&#26368;&#36817;&#24050;&#34987;&#35777;&#26126;&#22312;&#21307;&#30103;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#20135;&#29983;&#19981;&#21487;&#38752;&#30340;&#35299;&#37322;&#65292;&#19981;&#36866;&#21512;&#20020;&#24202;&#20351;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;ante-hoc&#26041;&#27861;&#65292;&#39318;&#27425;&#23558;&#20020;&#24202;&#27010;&#24565;&#24341;&#20837;&#20998;&#31867;&#31649;&#36947;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#28145;&#20837;&#20102;&#35299;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#19968;&#20010;&#22823;&#22411;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;&#33016;&#37096;X&#23556;&#32447;&#21644;&#30456;&#20851;&#21307;&#30103;&#25253;&#21578;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65292;&#21363;&#32954;&#30284;&#30340;&#26816;&#27979;&#12290;&#19982;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32954;&#30284;&#26816;&#27979;&#20013;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65288;F1 &gt; 0.9&#65289;&#65292;&#21516;&#26102;&#29983;&#25104;&#20102;&#20020;&#24202;&#30456;&#20851;&#19988;&#26356;&#21487;&#38752;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19444v1 Announce Type: new  Abstract: The rapidly advancing field of Explainable Artificial Intelligence (XAI) aims to tackle the issue of trust regarding the use of complex black-box deep learning models in real-world applications. Existing post-hoc XAI techniques have recently been shown to have poor performance on medical data, producing unreliable explanations which are infeasible for clinical use. To address this, we propose an ante-hoc approach based on concept bottleneck models which introduces for the first time clinical concepts into the classification pipeline, allowing the user valuable insight into the decision-making process. On a large public dataset of chest X-rays and associated medical reports, we focus on the binary classification task of lung cancer detection. Our approach yields improved classification performance in lung cancer detection when compared to baseline deep learning models (F1 &gt; 0.9), while also generating clinically relevant and more reliable
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#33258;&#32447;&#24615;&#20195;&#25968;&#38646;&#31354;&#38388;&#27010;&#24565;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#40065;&#26834;&#24615;&#22686;&#24378;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#36817;&#20284;&#38646;&#31354;&#38388;&#20803;&#32032;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10476</link><description>&lt;p&gt;
&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#36817;&#20284;&#38646;&#31354;&#38388;&#22686;&#24378;&#24494;&#35843;&#26041;&#27861;&#29992;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Approximate Nullspace Augmented Finetuning for Robust Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#33258;&#32447;&#24615;&#20195;&#25968;&#38646;&#31354;&#38388;&#27010;&#24565;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#40065;&#26834;&#24615;&#22686;&#24378;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#36817;&#20284;&#38646;&#31354;&#38388;&#20803;&#32032;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#39046;&#22495;&#20013;&#65292;&#23545;&#20110;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21551;&#21457;&#33258;&#32447;&#24615;&#20195;&#25968;&#20013;&#38646;&#31354;&#38388;&#27010;&#24565;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#40065;&#26834;&#24615;&#22686;&#24378;&#24494;&#35843;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#19968;&#20010;&#38382;&#39064;&#19978;&#65292;&#21363;&#35270;&#35273;&#21464;&#25442;&#22120;&#26159;&#21542;&#21487;&#20197;&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#32447;&#24615;&#26144;&#23556;&#20013;&#30340;&#38646;&#31354;&#38388;&#23646;&#24615;&#30340;&#36755;&#20837;&#21464;&#21270;&#38887;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#20174;&#35813;&#38646;&#31354;&#38388;&#20013;&#37319;&#26679;&#30340;&#25200;&#21160;&#28155;&#21152;&#21040;&#36755;&#20837;&#26102;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#35768;&#22810;&#39044;&#35757;&#32451;&#30340;ViTs&#65292;&#23384;&#22312;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#38646;&#31354;&#38388;&#65292;&#36825;&#26159;&#30001;&#20110;&#23384;&#22312;&#20462;&#34917;&#23884;&#20837;&#23618;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#38646;&#31354;&#38388;&#26159;&#19982;&#32447;&#24615;&#20195;&#25968;&#30456;&#20851;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#34920;&#26126;&#21487;&#20197;&#21033;&#29992;&#20248;&#21270;&#31574;&#30053;&#20026;ViTs&#30340;&#38750;&#32447;&#24615;&#22359;&#21512;&#25104;&#36817;&#20284;&#38646;&#31354;&#38388;&#20803;&#32032;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#33268;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10476v1 Announce Type: cross  Abstract: Enhancing the robustness of deep learning models, particularly in the realm of vision transformers (ViTs), is crucial for their real-world deployment. In this work, we provide a finetuning approach to enhance the robustness of vision transformers inspired by the concept of nullspace from linear algebra. Our investigation centers on whether a vision transformer can exhibit resilience to input variations akin to the nullspace property in linear mappings, implying that perturbations sampled from this nullspace do not influence the model's output when added to the input. Firstly, we show that for many pretrained ViTs, a non-trivial nullspace exists due to the presence of the patch embedding layer. Secondly, as nullspace is a concept associated with linear algebra, we demonstrate that it is possible to synthesize approximate nullspace elements for the non-linear blocks of ViTs employing an optimisation strategy. Finally, we propose a fine-t
&lt;/p&gt;</description></item></channel></rss>