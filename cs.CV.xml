<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;Shapley Taylor&#20114;&#21160;&#25351;&#25968;&#65288;STII&#65289;&#20998;&#26512;&#20102;&#24213;&#23618;&#25968;&#25454;&#32467;&#26500;&#23545;&#21508;&#31181;&#27169;&#24577;&#12289;&#20219;&#21153;&#21644;&#26550;&#26500;&#20013;&#27169;&#22411;&#34920;&#24449;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#27169;&#22411;&#20013;&#30340;&#26032;&#39062;&#29616;&#35937;&#65292;&#24182;&#23637;&#31034;&#20102;&#29305;&#24449;&#20132;&#20114;&#22914;&#20309;&#30452;&#35266;&#21453;&#26144;&#23545;&#35937;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.13106</link><description>&lt;p&gt;
&#35748;&#35782;&#20320;&#30340;&#38750;&#32447;&#24615;&#65306;Shapley&#20114;&#21160;&#25581;&#31034;&#25968;&#25454;&#30340;&#28508;&#22312;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying Structure of Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;Shapley Taylor&#20114;&#21160;&#25351;&#25968;&#65288;STII&#65289;&#20998;&#26512;&#20102;&#24213;&#23618;&#25968;&#25454;&#32467;&#26500;&#23545;&#21508;&#31181;&#27169;&#24577;&#12289;&#20219;&#21153;&#21644;&#26550;&#26500;&#20013;&#27169;&#22411;&#34920;&#24449;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#27169;&#22411;&#20013;&#30340;&#26032;&#39062;&#29616;&#35937;&#65292;&#24182;&#23637;&#31034;&#20102;&#29305;&#24449;&#20132;&#20114;&#22914;&#20309;&#30452;&#35266;&#21453;&#26144;&#23545;&#35937;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#37327;&#38750;&#32447;&#24615;&#29305;&#24449;&#20132;&#20114;&#26159;&#29702;&#35299;&#35768;&#22810;&#27169;&#22411;&#20013;&#22797;&#26434;&#24402;&#22240;&#27169;&#24335;&#30340;&#19968;&#31181;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20351;&#29992;Shapley Taylor&#20114;&#21160;&#25351;&#25968;&#65288;STII&#65289;&#26469;&#20998;&#26512;&#24213;&#23618;&#25968;&#25454;&#32467;&#26500;&#23545;&#22810;&#31181;&#27169;&#24577;&#12289;&#20219;&#21153;&#21644;&#26550;&#26500;&#20013;&#27169;&#22411;&#34920;&#24449;&#30340;&#24433;&#21709;&#12290;&#22312;&#32771;&#34385;&#25513;&#30721;&#21644;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#21644;ALMs&#65289;&#20013;&#30340;&#35821;&#35328;&#32467;&#26500;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;STII&#22312;&#24815;&#29992;&#34920;&#36798;&#20013;&#22686;&#21152;&#65292;MLMs&#38543;&#21477;&#27861;&#36317;&#31163;&#25193;&#23637;STII&#65292;&#26356;&#22810;&#22320;&#20381;&#36182;&#35821;&#27861;&#22312;&#20854;&#38750;&#32447;&#24615;&#32467;&#26500;&#20013;&#30456;&#27604;ALMs&#12290;&#25105;&#20204;&#30340;&#35821;&#38899;&#27169;&#22411;&#30740;&#31350;&#21453;&#26144;&#20102;&#21475;&#33108;&#24352;&#24320;&#31243;&#24230;&#20915;&#23450;&#38899;&#32032;&#26681;&#25454;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;&#25968;&#37327;&#30340;&#21407;&#21017;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#22270;&#20687;&#20998;&#31867;&#22120;&#24182;&#35828;&#26126;&#29305;&#24449;&#20132;&#20114;&#30452;&#35266;&#21453;&#26144;&#23545;&#35937;&#36793;&#30028;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#36328;&#23398;&#31185;&#24037;&#20316;&#21644;&#39046;&#22495;&#20043;&#38388;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13106v1 Announce Type: cross  Abstract: Measuring nonlinear feature interaction is an established approach to understanding complex patterns of attribution in many models. In this paper, we use Shapley Taylor interaction indices (STII) to analyze the impact of underlying data structure on model representations in a variety of modalities, tasks, and architectures. Considering linguistic structure in masked and auto-regressive language models (MLMs and ALMs), we find that STII increases within idiomatic expressions and that MLMs scale STII with syntactic distance, relying more on syntax in their nonlinear structure than ALMs do. Our speech model findings reflect the phonetic principal that the openness of the oral cavity determines how much a phoneme varies based on its context. Finally, we study image classifiers and illustrate that feature interactions intuitively reflect object boundaries. Our wide range of results illustrates the benefits of interdisciplinary work and doma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21363;&#25554;&#21363;&#29992;&#22270;&#20687;&#24674;&#22797;&#26694;&#26550;&#65292;&#31216;&#20026;&#38543;&#26426;&#21435;&#22122;&#27491;&#21017;&#21270;&#65288;SNORE&#65289;&#12290;&#35813;&#26694;&#26550;&#22312;&#24688;&#24403;&#22122;&#22768;&#27700;&#24179;&#30340;&#22270;&#20687;&#19978;&#24212;&#29992;&#21435;&#22122;&#22120;&#65292;&#24182;&#22522;&#20110;&#38543;&#26426;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#35299;&#20915;&#30149;&#24577;&#36870;&#38382;&#39064;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SNORE&#22312;&#21435;&#27169;&#31946;&#21644;&#20462;&#22797;&#20219;&#21153;&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01779</link><description>&lt;p&gt;
&#24102;&#26377;&#38543;&#26426;&#21435;&#22122;&#27491;&#21017;&#21270;&#30340;&#21363;&#25554;&#21363;&#29992;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play image restoration with Stochastic deNOising REgularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21363;&#25554;&#21363;&#29992;&#22270;&#20687;&#24674;&#22797;&#26694;&#26550;&#65292;&#31216;&#20026;&#38543;&#26426;&#21435;&#22122;&#27491;&#21017;&#21270;&#65288;SNORE&#65289;&#12290;&#35813;&#26694;&#26550;&#22312;&#24688;&#24403;&#22122;&#22768;&#27700;&#24179;&#30340;&#22270;&#20687;&#19978;&#24212;&#29992;&#21435;&#22122;&#22120;&#65292;&#24182;&#22522;&#20110;&#38543;&#26426;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#35299;&#20915;&#30149;&#24577;&#36870;&#38382;&#39064;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SNORE&#22312;&#21435;&#27169;&#31946;&#21644;&#20462;&#22797;&#20219;&#21153;&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#25554;&#21363;&#29992;&#65288;PnP&#65289;&#31639;&#27861;&#26159;&#19968;&#31867;&#36845;&#20195;&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#29289;&#29702;&#27169;&#22411;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#35299;&#20915;&#22270;&#20687;&#21453;&#28436;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#20135;&#29983;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22270;&#20687;&#24674;&#22797;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#22312;&#36845;&#20195;&#36807;&#31243;&#20013;&#36234;&#26469;&#36234;&#23569;&#22122;&#38899;&#30340;&#22270;&#20687;&#19978;&#30340;&#19968;&#31181;&#38750;&#26631;&#20934;&#30340;&#21435;&#22122;&#22120;&#20351;&#29992;&#26041;&#27861;&#65292;&#36825;&#19982;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#30340;&#26368;&#26032;&#31639;&#27861;&#30456;&#30683;&#30462;&#65292;&#22312;&#36825;&#20123;&#31639;&#27861;&#20013;&#65292;&#21435;&#22122;&#22120;&#20165;&#24212;&#29992;&#20110;&#37325;&#26032;&#21152;&#22122;&#30340;&#22270;&#20687;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PnP&#26694;&#26550;&#65292;&#31216;&#20026;&#38543;&#26426;&#21435;&#22122;&#27491;&#21017;&#21270;&#65288;SNORE&#65289;&#65292;&#23427;&#20165;&#22312;&#22122;&#22768;&#27700;&#24179;&#36866;&#24403;&#30340;&#22270;&#20687;&#19978;&#24212;&#29992;&#21435;&#22122;&#22120;&#12290;&#23427;&#22522;&#20110;&#26174;&#24335;&#30340;&#38543;&#26426;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#31181;&#35299;&#20915;&#30149;&#24577;&#36870;&#38382;&#39064;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#31639;&#27861;&#21450;&#20854;&#36864;&#28779;&#25193;&#23637;&#30340;&#25910;&#25947;&#20998;&#26512;&#12290;&#22312;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;SNORE&#22312;&#21435;&#27169;&#31946;&#21644;&#20462;&#22797;&#20219;&#21153;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play (PnP) algorithms are a class of iterative algorithms that address image inverse problems by combining a physical model and a deep neural network for regularization. Even if they produce impressive image restoration results, these algorithms rely on a non-standard use of a denoiser on images that are less and less noisy along the iterations, which contrasts with recent algorithms based on Diffusion Models (DM), where the denoiser is applied only on re-noised images. We propose a new PnP framework, called Stochastic deNOising REgularization (SNORE), which applies the denoiser only on images with noise of the adequate level. It is based on an explicit stochastic regularization, which leads to a stochastic gradient descent algorithm to solve ill-posed inverse problems. A convergence analysis of this algorithm and its annealing extension is provided. Experimentally, we prove that SNORE is competitive with respect to state-of-the-art methods on deblurring and inpainting tasks, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#25991;&#26412;&#33267;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#22686;&#24378;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#27867;&#21270;&#21040;&#26032;&#35270;&#35273;&#27010;&#24565;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07944</link><description>&lt;p&gt;
&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Effective Data Augmentation With Diffusion Models. (arXiv:2302.07944v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#25991;&#26412;&#33267;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#22686;&#24378;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#27867;&#21270;&#21040;&#26032;&#35270;&#35273;&#27010;&#24565;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#24037;&#20855;&#20043;&#19968;&#65292;&#25903;&#25745;&#30528;&#26368;&#36817;&#21253;&#25324;&#20998;&#31867;&#12289;&#29983;&#25104;&#27169;&#22411;&#21644;&#34920;&#31034;&#23398;&#20064;&#22312;&#20869;&#30340;&#35768;&#22810;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22686;&#24378;&#26041;&#27861;&#22312;&#25968;&#25454;&#30340;&#20851;&#38190;&#35821;&#20041;&#36724;&#19978;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#32570;&#20047;&#25913;&#21464;&#39640;&#32423;&#35821;&#20041;&#23646;&#24615;&#65288;&#22914;&#22330;&#26223;&#20013;&#30340;&#21160;&#29289;&#31181;&#31867;&#65289;&#20197;&#22686;&#24378;&#25968;&#25454;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#25991;&#26412;&#33267;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#26469;&#35299;&#20915;&#25968;&#25454;&#22686;&#24378;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29616;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#32534;&#36753;&#22270;&#20687;&#65292;&#25913;&#21464;&#23427;&#20204;&#30340;&#35821;&#20041;&#65292;&#33021;&#22815;&#27867;&#21270;&#21040;&#20165;&#29992;&#23569;&#37327;&#26631;&#35760;&#31034;&#20363;&#24471;&#21040;&#30340;&#26032;&#35270;&#35273;&#27010;&#24565;&#12290;&#25105;&#20204;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#26434;&#33609;&#35782;&#21035;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;......
&lt;/p&gt;
&lt;p&gt;
Data augmentation is one of the most prevalent tools in deep learning, underpinning many recent advances, including those from classification, generative models, and representation learning. The standard approach to data augmentation combines simple transformations like rotations and flips to generate new images from existing ones. However, these new images lack diversity along key semantic axes present in the data. Current augmentations cannot alter the high-level semantic attributes, such as animal species present in a scene, to enhance the diversity of data. We address the lack of diversity in data augmentation with image-to-image transformations parameterized by pre-trained text-to-image diffusion models. Our method edits images to change their semantics using an off-the-shelf diffusion model, and generalizes to novel visual concepts from a few labelled examples. We evaluate our approach on few-shot image classification tasks, and on a real-world weed recognition task, and observe 
&lt;/p&gt;</description></item></channel></rss>