<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#20219;&#20309;&#29616;&#26377;&#32534;&#30721;-&#35299;&#30721;&#20998;&#21106;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#26465;&#20214;&#21270;&#27169;&#22411;&#39044;&#27979;&#19982;&#21487;&#23398;&#20064;&#30340;&#35299;&#21078;&#20808;&#39564;&#26469;&#26045;&#21152;&#35299;&#21078;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.18878</link><description>&lt;p&gt;
AIC-UNet: &#29992;&#20110;&#20581;&#22766;&#22810;&#22120;&#23448;&#20998;&#21106;&#30340;&#35299;&#21078;&#20449;&#24687;&#39537;&#21160;&#32423;&#32852;UNet
&lt;/p&gt;
&lt;p&gt;
AIC-UNet: Anatomy-informed Cascaded UNet for Robust Multi-Organ Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18878
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#20219;&#20309;&#29616;&#26377;&#32534;&#30721;-&#35299;&#30721;&#20998;&#21106;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#26465;&#20214;&#21270;&#27169;&#22411;&#39044;&#27979;&#19982;&#21487;&#23398;&#20064;&#30340;&#35299;&#21078;&#20808;&#39564;&#26469;&#26045;&#21152;&#35299;&#21078;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21152;&#20851;&#38190;&#35299;&#21078;&#29305;&#24449;&#65292;&#20363;&#22914;&#22120;&#23448;&#25968;&#37327;&#12289;&#24418;&#29366;&#12289;&#22823;&#23567;&#21644;&#30456;&#23545;&#20301;&#32622;&#65292;&#23545;&#20110;&#26500;&#24314;&#20581;&#22766;&#30340;&#22810;&#22120;&#23448;&#20998;&#21106;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#29616;&#26377;&#30340;&#32534;&#30721;-&#35299;&#30721;&#20998;&#21106;&#27169;&#22411;&#19978;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#35299;&#21078;&#20808;&#39564;&#65292;&#26469;&#23454;&#26045;&#35299;&#21078;&#32422;&#26463;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#33145;&#37096;&#25195;&#25551;&#26102;&#65292;&#32534;&#30721;&#22120;&#30340;&#19968;&#37096;&#20998;&#36890;&#36807;&#34180;&#26495;&#26679;&#26465;&#65288;TPS&#65289;&#32593;&#26684;&#25554;&#20540;&#23558;&#21487;&#23398;&#20064;&#30340;&#20808;&#39564;&#31354;&#38388;&#23545;&#20934;&#32473;&#23450;&#30340;&#36755;&#20837;&#25195;&#25551;&#12290;&#28982;&#21518;&#22312;&#35299;&#30721;&#38454;&#27573;&#25972;&#21512;&#21464;&#24418;&#30340;&#20808;&#39564;&#20197;&#25351;&#23548;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18878v1 Announce Type: cross  Abstract: Imposing key anatomical features, such as the number of organs, their shapes, sizes, and relative positions, is crucial for building a robust multi-organ segmentation model. Current attempts to incorporate anatomical features include broadening effective receptive fields (ERF) size with resource- and data-intensive modules such as self-attention or introducing organ-specific topology regularizers, which may not scale to multi-organ segmentation problems where inter-organ relation also plays a huge role. We introduce a new approach to impose anatomical constraints on any existing encoder-decoder segmentation model by conditioning model prediction with learnable anatomy prior. More specifically, given an abdominal scan, a part of the encoder spatially warps a learnable prior to align with the given input scan using thin plate spline (TPS) grid interpolation. The warped prior is then integrated during the decoding phase to guide the model
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23618;&#35774;&#35745;&#26032;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;&#23545;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14772</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#26550;&#26500;&#25552;&#39640;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness to Model Inversion Attacks via Sparse Coding Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14772
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23618;&#35774;&#35745;&#26032;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;&#23545;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#31639;&#27861;&#20801;&#35768;&#23545;&#25163;&#36890;&#36807;&#21453;&#22797;&#26597;&#35810;&#31070;&#32463;&#32593;&#32476;&#24182;&#26816;&#26597;&#20854;&#36755;&#20986;&#26469;&#37325;&#24314;&#32593;&#32476;&#30340;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#31232;&#30095;&#32534;&#30721;&#23618;&#26469;&#33719;&#24471;&#23545;&#36825;&#31867;&#25915;&#20987;&#30340;&#21331;&#36234;&#40065;&#26834;&#24615;&#12290; &#19977;&#21313;&#24180;&#26469;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;&#31232;&#30095;&#32534;&#30721;&#22312;&#22270;&#20687;&#21435;&#22122;&#65292;&#30446;&#26631;&#35782;&#21035;&#21644;&#23545;&#25239;&#24615;&#35823;&#20998;&#35774;&#32622;&#20013;&#30340;&#20316;&#29992;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#38544;&#31169;&#28431;&#27934;&#20043;&#38388;&#30340;&#32852;&#31995;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#31232;&#30095;&#32534;&#30721;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21033;&#30340;&#25163;&#27573;&#26469;&#25269;&#24481;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#25105;&#20204;&#25511;&#21046;&#32534;&#30721;&#22312;&#32593;&#32476;&#30340;&#20013;&#38388;&#34920;&#31034;&#20013;&#30340;&#26080;&#20851;&#31169;&#20154;&#20449;&#24687;&#30340;&#25968;&#37327;&#65292;&#32780;&#36825;&#31181;&#26041;&#24335;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#39640;&#25928;&#35745;&#31639;&#65292;&#24182;&#19988;&#20247;&#25152;&#21608;&#30693;&#21482;&#26377;&#36739;&#23567;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14772v1 Announce Type: cross  Abstract: Recent model inversion attack algorithms permit adversaries to reconstruct a neural network's private training data just by repeatedly querying the network and inspecting its outputs. In this work, we develop a novel network architecture that leverages sparse-coding layers to obtain superior robustness to this class of attacks. Three decades of computer science research has studied sparse coding in the context of image denoising, object recognition, and adversarial misclassification settings, but to the best of our knowledge, its connection to state-of-the-art privacy vulnerabilities remains unstudied. However, sparse coding architectures suggest an advantageous means to defend against model inversion attacks because they allow us to control the amount of irrelevant private information encoded in a network's intermediate representations in a manner that can be computed efficiently during training and that is known to have little effect
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#21160;&#21147;&#23398;&#19982;&#29702;&#24819;&#30340;&#26102;&#38388;&#20107;&#20214;&#37319;&#26679;&#22120;&#30340;&#34892;&#20026;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;</title><link>https://arxiv.org/abs/2403.12574</link><description>&lt;p&gt;
EAS-SNN&#65306;&#31471;&#21040;&#31471;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#34920;&#31034;&#65292;&#29992;&#20110;&#24490;&#29615;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based Detection with Recurrent Spiking Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12574
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#21160;&#21147;&#23398;&#19982;&#29702;&#24819;&#30340;&#26102;&#38388;&#20107;&#20214;&#37319;&#26679;&#22120;&#30340;&#34892;&#20026;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25668;&#20687;&#22836;&#20197;&#20854;&#39640;&#21160;&#24577;&#33539;&#22260;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#29289;&#20307;&#26816;&#27979;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#21160;&#24577;&#27169;&#31946;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20809;&#29031;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26356;&#27880;&#37325;&#20248;&#21270;&#20855;&#26377;&#20808;&#36827;&#26816;&#27979;&#39592;&#24178;&#21644;&#26089;&#26399;&#32858;&#21512;&#21151;&#33021;&#30340;&#26102;&#31354;&#34920;&#31034;&#65292;&#32780;&#33258;&#36866;&#24212;&#20107;&#20214;&#37319;&#26679;&#30340;&#20851;&#38190;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#65292;&#36890;&#36807;&#31232;&#30095;&#33033;&#20914;&#36890;&#20449;&#36816;&#34892;&#30340;&#20107;&#20214;&#39537;&#21160;&#33539;&#24335;&#65292;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#22825;&#28982;&#36873;&#25321;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#21160;&#21147;&#23398;&#19982;&#29702;&#24819;&#30340;&#26102;&#38388;&#20107;&#20214;&#37319;&#26679;&#22120;&#30340;&#34892;&#20026;&#23494;&#20999;&#30456;&#31526;&#12290;&#22312;&#36825;&#19968;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#27169;&#22359;&#65292;&#21033;&#29992;&#20855;&#26377;&#26102;&#38388;&#35760;&#24518;&#30340;&#24490;&#29615;&#21367;&#31215;SNN&#22686;&#24378;&#65292;&#20026;&#22522;&#20110;&#20107;&#20214;&#26816;&#27979;&#30340;&#23436;&#20840;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#26694;&#26550;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12574v1 Announce Type: cross  Abstract: Event cameras, with their high dynamic range and temporal resolution, are ideally suited for object detection, especially under scenarios with motion blur and challenging lighting conditions. However, while most existing approaches prioritize optimizing spatiotemporal representations with advanced detection backbones and early aggregation functions, the crucial issue of adaptive event sampling remains largely unaddressed. Spiking Neural Networks (SNNs), which operate on an event-driven paradigm through sparse spike communication, emerge as a natural fit for addressing this challenge. In this study, we discover that the neural dynamics of spiking neurons align closely with the behavior of an ideal temporal event sampler. Motivated by this insight, we propose a novel adaptive sampling module that leverages recurrent convolutional SNNs enhanced with temporal memory, facilitating a fully end-to-end learnable framework for event-based detec
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.12029</link><description>&lt;p&gt;
&#23545;&#40784;&#19982;&#25552;&#28860;&#65306;&#32479;&#19968;&#21644;&#25913;&#36827;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Align and Distill: Unifying and Improving Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12029
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#22120;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#20110;&#19982;&#20854;&#35757;&#32451;&#38598;&#19981;&#21516;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65288;DAOD&#65289;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#19978;&#30340;&#24378;&#22823;&#32467;&#26524;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#38519;&#38449;&#65292;&#36825;&#20123;&#38519;&#38449;&#23545;&#36807;&#21435;&#30340;&#32467;&#26524;&#25552;&#20986;&#36136;&#30097;&#24182;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#65306;&#65288;a&#65289;&#30001;&#20110;&#22522;&#32447;&#19981;&#36275;&#23548;&#33268;&#24615;&#33021;&#39640;&#20272;&#65292;&#65288;b&#65289;&#19981;&#19968;&#33268;&#30340;&#23454;&#29616;&#23454;&#36341;&#38459;&#27490;&#20102;&#26041;&#27861;&#30340;&#36879;&#26126;&#27604;&#36739;&#65292;&#65288;c&#65289;&#30001;&#20110;&#36807;&#26102;&#30340;&#39592;&#24178;&#21644;&#22522;&#20934;&#27979;&#35797;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#23548;&#33268;&#32570;&#20047;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20197;&#19979;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;&#65292;Align and Distill&#65288;ALDI&#65289;&#65292;&#25903;&#25345;DAOD&#26041;&#27861;&#30340;&#27604;&#36739;&#24182;&#25903;&#25345;&#26410;&#26469;&#21457;&#23637;&#65292;&#65288;2&#65289;&#19968;&#20010;&#20844;&#24179;&#19988;&#29616;&#20195;&#30340;DAOD&#35757;&#32451;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#35299;&#20915;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#38519;&#38449;&#65292;&#65288;3&#65289;&#19968;&#20010;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;CFC-DAOD&#65292;&#33021;&#22815;&#22312;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12029v1 Announce Type: cross  Abstract: Object detectors often perform poorly on data that differs from their training set. Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge. Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks. We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26469;&#36873;&#25321;&#38656;&#35201;&#36827;&#19968;&#27493;&#36866;&#24212;&#30340;&#23618;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30001;&#20110;&#20266;&#26631;&#31614;&#24341;&#36215;&#30340;&#19981;&#20934;&#30830;&#24615;&#22256;&#25200;&#12290;</title><link>https://arxiv.org/abs/2403.10650</link><description>&lt;p&gt;
PALM&#65306;&#25512;&#36827;&#29992;&#20110;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26469;&#36873;&#25321;&#38656;&#35201;&#36827;&#19968;&#27493;&#36866;&#24212;&#30340;&#23618;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30001;&#20110;&#20266;&#26631;&#31614;&#24341;&#36215;&#30340;&#19981;&#20934;&#30830;&#24615;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#27169;&#22411;&#38754;&#20020;&#39046;&#22495;&#20998;&#24067;&#30340;&#24555;&#36895;&#36716;&#21464;&#65292;&#23548;&#33268;&#35782;&#21035;&#24615;&#33021;&#19979;&#38477;&#12290;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;CTTA&#65289;&#30452;&#25509;&#26681;&#25454;&#27979;&#35797;&#25968;&#25454;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#28304;&#21028;&#21035;&#27169;&#22411;&#20197;&#36866;&#24212;&#36825;&#20123;&#19981;&#26029;&#21464;&#21270;&#30340;&#39046;&#22495;&#12290;&#19968;&#31181;&#39640;&#24230;&#26377;&#25928;&#30340;CTTA&#26041;&#27861;&#28041;&#21450;&#24212;&#29992;&#36880;&#23618;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#24182;&#36873;&#25321;&#24615;&#22320;&#35843;&#25972;&#39044;&#35757;&#32451;&#23618;&#12290;&#28982;&#32780;&#65292;&#23427;&#21463;&#21040;&#39046;&#22495;&#36716;&#31227;&#20272;&#35745;&#19981;&#20934;&#30830;&#21644;&#30001;&#20266;&#26631;&#31614;&#24341;&#36215;&#30340;&#19981;&#20934;&#30830;&#24615;&#25152;&#22256;&#25200;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#23618;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26469;&#36873;&#25321;&#23618;&#65292;&#32780;&#26080;&#39035;&#20381;&#36182;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#21033;&#29992;&#26799;&#24230;&#30340;&#22823;&#23567;&#20316;&#20026;&#19968;&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;softmax&#36755;&#20986;&#19982;&#22343;&#21248;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#35745;&#31639;&#65292;&#20197;&#36873;&#25321;&#38656;&#35201;&#36827;&#19968;&#27493;&#36866;&#24212;&#30340;&#23618;&#12290;&#38543;&#21518;&#65292;&#20165;&#23646;&#20110;&#36825;&#20123;&#23618;&#30340;&#21442;&#25968;&#23558;&#34987;&#36827;&#19968;&#27493;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10650v1 Announce Type: cross  Abstract: Real-world vision models in dynamic environments face rapid shifts in domain distributions, leading to decreased recognition performance. Continual test-time adaptation (CTTA) directly adjusts a pre-trained source discriminative model to these changing domains using test data. A highly effective CTTA method involves applying layer-wise adaptive learning rates, and selectively adapting pre-trained layers. However, it suffers from the poor estimation of domain shift and the inaccuracies arising from the pseudo-labels. In this work, we aim to overcome these limitations by identifying layers through the quantification of model prediction uncertainty without relying on pseudo-labels. We utilize the magnitude of gradients as a metric, calculated by backpropagating the KL divergence between the softmax output and a uniform distribution, to select layers for further adaptation. Subsequently, for the parameters exclusively belonging to these se
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26500;&#24314;&#30340; AFBT GAN &#22686;&#24378;&#20102;&#23545;&#35748;&#30693;&#34928;&#36864;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#35786;&#26029;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.01758</link><description>&lt;p&gt;
AFBT GAN: &#36890;&#36807;&#21453;&#20107;&#23454;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22686;&#24378;&#23545;&#35748;&#30693;&#34928;&#36864;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#35786;&#26029;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
AFBT GAN: enhanced explainability and diagnostic performance for cognitive decline by counterfactual generative adversarial network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01758
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26500;&#24314;&#30340; AFBT GAN &#22686;&#24378;&#20102;&#23545;&#35748;&#30693;&#34928;&#36864;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#35786;&#26029;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21151;&#33021;&#36830;&#25509;&#65288;FC&#65289;&#30340;&#35299;&#37322;&#32467;&#26524;&#36890;&#24120;&#26159;&#36890;&#36807;&#20351;&#29992;&#20998;&#31867;&#32467;&#26524;&#26631;&#31614;&#21644;&#35832;&#22914;Pearson&#30456;&#20851;&#24615;&#25110;&#26799;&#24230;&#21453;&#25512;&#31561;&#30456;&#20851;&#20998;&#26512;&#26041;&#27861;&#29983;&#25104;&#30340;&#12290;&#28982;&#32780;&#65292;&#35786;&#26029;&#27169;&#22411;&#20173;&#28982;&#26159;&#22312;&#40657;&#30418;&#27169;&#22411;&#19978;&#35757;&#32451;&#30340;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#32570;&#20047;&#23545;&#37325;&#35201;&#21306;&#22495;FC&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#21644;&#25552;&#39640;&#35786;&#26029;&#24615;&#33021;&#65292;&#22312;&#35786;&#26029;&#27169;&#22411;&#20013;&#25552;&#20379;&#20851;&#20110;&#31070;&#32463;&#36864;&#34892;&#24615;&#30456;&#20851;&#21306;&#22495;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#24403;&#20581;&#24247;&#21463;&#35797;&#32773;&#65288;HC&#65289;&#21457;&#23637;&#20026;&#20027;&#35266;&#35748;&#30693;&#34928;&#36864;&#65288;SCD&#65289;&#21644;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;MCI&#65289;&#26102;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#30830;&#23450;&#31070;&#32463;&#36864;&#34892;&#24615;&#30456;&#20851;&#21306;&#22495;&#65292;&#25105;&#20204;&#37319;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#29983;&#25104;&#28304;&#26631;&#31614;FC&#27966;&#29983;&#30340;&#30446;&#26631;&#26631;&#31614;FC&#30697;&#38453;&#65292;&#28982;&#21518;&#23558;&#28304;&#26631;&#31614;FC&#20943;&#21435;&#30446;&#26631;&#26631;&#31614;FC&#12290;&#33258;&#36866;&#24212;&#21069;&#21521;&#21644;&#21518;&#21521;&#36716;&#25442;&#26500;&#25104;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01758v1 Announce Type: cross  Abstract: Existing explanation results of functional connectivity (FC) are normally generated by using classification result labels and correlation analysis methods such as Pearson's correlation or gradient backward. However, the diagnostic model is still trained on the black box model and might lack the attention of FCs in important regions during the training. To enhance the explainability and improve diagnostic performance, providing prior knowledge on neurodegeneration-related regions when healthy subjects (HC) develop into subject cognitive decline (SCD) and mild cognitive impairment (MCI) for the diagnostic model is a key step. To better determine the neurodegeneration-related regions, we employ counterfactual reasoning to generate the target label FC matrices derived from source label FC and then subtract source label FC with target label FC. The counterfactual reasoning architecture is constructed by adaptive forward and backward transfo
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#35270;&#35273;&#36866;&#37197;&#22120;&#65288;IVA&#65289;&#65292;&#29992;&#20110;&#22312;LLMs&#20013;&#22686;&#24378;&#23545;&#32454;&#31890;&#24230;&#35270;&#35273;&#20803;&#32032;&#30340;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#20102;&#38271;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#12289;&#35270;&#35273;&#28165;&#26224;&#24230;&#38477;&#20302;&#21644;&#26080;&#20851;&#35270;&#35273;&#20196;&#29260;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.13546</link><description>&lt;p&gt;
LLMs&#19982;&#38271;&#35270;&#39057;&#30456;&#36935;&#65306;&#22312;LLMs&#20013;&#21033;&#29992;&#20114;&#21160;&#24335;&#35270;&#35273;&#36866;&#37197;&#22120;&#25512;&#36827;&#38271;&#35270;&#39057;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13546
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#35270;&#35273;&#36866;&#37197;&#22120;&#65288;IVA&#65289;&#65292;&#29992;&#20110;&#22312;LLMs&#20013;&#22686;&#24378;&#23545;&#32454;&#31890;&#24230;&#35270;&#35273;&#20803;&#32032;&#30340;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#20102;&#38271;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#12289;&#35270;&#35273;&#28165;&#26224;&#24230;&#38477;&#20302;&#21644;&#26080;&#20851;&#35270;&#35273;&#20196;&#29260;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#35270;&#39057;&#29702;&#35299;&#26159;&#22810;&#23186;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#20132;&#21449;&#39046;&#22495;&#20013;&#19968;&#39033;&#37325;&#35201;&#19988;&#25345;&#32493;&#25361;&#25112;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#29702;&#35299;&#35270;&#39057;&#25104;&#20026;&#19968;&#31181;&#26032;&#20852;&#19988;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35270;&#39057;&#20196;&#29260;&#25968;&#37327;&#24222;&#22823;&#65292;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#35270;&#35273;&#28165;&#26224;&#24230;&#38477;&#20302;&#65292;&#36824;&#38754;&#20020;&#30528;&#22312;&#22238;&#31572;&#35270;&#39057;&#30456;&#20851;&#38382;&#39064;&#26102;&#20986;&#29616;&#26080;&#20851;&#35270;&#35273;&#20196;&#29260;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;LLMs&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#35270;&#35273;&#36866;&#37197;&#22120;(IVA)&#65292;&#26088;&#22312;&#22686;&#24378;&#19982;&#32454;&#31890;&#24230;&#35270;&#35273;&#20803;&#32032;&#30340;&#20132;&#20114;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#22240;&#26524;&#21464;&#25442;&#22120;&#23558;&#38271;&#35270;&#39057;&#36716;&#25442;&#20026;&#26102;&#38388;&#35270;&#39057;&#20196;&#29260;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#19982;&#35270;&#39057;&#35828;&#26126;&#19968;&#36215;&#36755;&#20837;LLMs&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#38598;&#25104;&#20102;IVA&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#26102;&#38388;&#24103;&#36873;&#25321;&#22120;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13546v1 Announce Type: new  Abstract: Long video understanding is a significant and ongoing challenge in the intersection of multimedia and artificial intelligence. Employing large language models (LLMs) for comprehending video becomes an emerging and promising method. However, this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions. To alleviate these issues, we present an Interactive Visual Adapter (IVA) within LLMs, designed to enhance interaction with fine-grained visual elements. Specifically, we first transform long videos into temporal video tokens via leveraging a visual encoder alongside a pretrained causal transformer, then feed them into LLMs with the video instructions. Subsequently, we integrated IVA, which contains a lightweight temporal frame selector a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#25552;&#20986;&#19968;&#20010;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26816;&#27979;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24555;&#25463;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11237</link><description>&lt;p&gt;
&#23545;&#25239;&#28145;&#24230;&#23398;&#20064;&#20013;&#24555;&#25463;&#26041;&#24335;&#30340;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Be Persistent: Towards a Unified Solution for Mitigating Shortcuts in Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#25552;&#20986;&#19968;&#20010;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26816;&#27979;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24555;&#25463;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23481;&#26131;&#21463;&#21040;&#24555;&#25463;&#23398;&#20064;&#30340;&#24433;&#21709;&#65306;&#23427;&#20204;&#20542;&#21521;&#20110;&#24314;&#31435;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#26080;&#20851;&#30340;&#20851;&#31995;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#39044;&#26399;&#30340;&#20219;&#21153;&#12290;&#24555;&#25463;&#23398;&#20064;&#22312;&#31070;&#32463;&#32593;&#32476;&#35768;&#22810;&#22833;&#36133;&#26696;&#20363;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#36825;&#19968;&#29616;&#35937;&#30340;&#30165;&#36857;&#21487;&#35265;&#20110;&#20854;&#27867;&#21270;&#38382;&#39064;&#12289;&#39046;&#22495;&#36716;&#31227;&#12289;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#65292;&#29978;&#33267;&#23545;&#22810;&#25968;&#32676;&#20307;&#30340;&#20559;&#35265;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#21508;&#31181;DNN&#38382;&#39064;&#30340;&#20849;&#21516;&#21407;&#22240;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#37325;&#35201;&#26426;&#20250;&#65292;&#24212;&#35813;&#21033;&#29992;&#36825;&#19968;&#28857;&#25214;&#21040;&#23545;&#25239;&#24555;&#25463;&#23398;&#20064;&#30340;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;(TDA)&#29305;&#21035;&#26159;&#25345;&#32493;&#21516;&#35843;(PH)&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20026;&#25506;&#27979;&#28145;&#24230;&#23398;&#20064;&#20013;&#24555;&#25463;&#26041;&#24335;&#21246;&#30011;&#20102;&#32479;&#19968;&#30340;&#36335;&#32447;&#22270;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;DNNs&#20013;&#35745;&#31639;&#22270;&#30340;&#25299;&#25169;&#29305;&#24449;&#65292;&#20351;&#29992;&#26080;&#27861;&#23398;&#20064;&#30340;&#31034;&#20363;&#21644;&#20559;&#35265;&#20026;&#20004;&#31181;&#24773;&#20917;&#65292;&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#35770;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11237v1 Announce Type: new  Abstract: Deep neural networks (DNNs) are vulnerable to shortcut learning: rather than learning the intended task, they tend to draw inconclusive relationships between their inputs and outputs. Shortcut learning is ubiquitous among many failure cases of neural networks, and traces of this phenomenon can be seen in their generalizability issues, domain shift, adversarial vulnerability, and even bias towards majority groups. In this paper, we argue that this commonality in the cause of various DNN issues creates a significant opportunity that should be leveraged to find a unified solution for shortcut learning. To this end, we outline the recent advances in topological data analysis~(TDA), and persistent homology~(PH) in particular, to sketch a unified roadmap for detecting shortcuts in deep learning. We demonstrate our arguments by investigating the topological features of computational graphs in DNNs using two cases of unlearnable examples and bia
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#20219;&#20309;&#25915;&#20987;&#30340;&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;&#65288;CAD&#65289;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2312.09481</link><description>&lt;p&gt;
&#25345;&#32493;&#19981;&#26029;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Continual Adversarial Defense
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09481
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#20219;&#20309;&#25915;&#20987;&#30340;&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;&#65288;CAD&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#27599;&#26376;&#38024;&#23545;&#35270;&#35273;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#24555;&#36895;&#28436;&#21464;&#30340;&#29305;&#24615;&#65292;&#20154;&#20204;&#25552;&#20986;&#20102;&#35768;&#22810;&#38450;&#24481;&#26041;&#27861;&#65292;&#26088;&#22312;&#23613;&#21487;&#33021;&#36890;&#29992;&#21270;&#20197;&#25269;&#24481;&#23613;&#21487;&#33021;&#22810;&#30340;&#24050;&#30693;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#23545;&#25239;&#25152;&#26377;&#31867;&#22411;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#24182;&#19981;&#29616;&#23454;&#65292;&#22240;&#20026;&#38450;&#24481;&#31995;&#32479;&#36816;&#34892;&#30340;&#29615;&#22659;&#26159;&#21160;&#24577;&#30340;&#65292;&#21253;&#21547;&#38543;&#30528;&#26102;&#38388;&#20986;&#29616;&#30340;&#21508;&#31181;&#29420;&#29305;&#25915;&#20987;&#12290;&#38450;&#24481;&#31995;&#32479;&#24517;&#39035;&#25910;&#38598;&#22312;&#32447;&#23569;&#26679;&#26412;&#23545;&#25239;&#21453;&#39304;&#20197;&#36805;&#36895;&#22686;&#24378;&#33258;&#36523;&#65292;&#20805;&#20998;&#21033;&#29992;&#20869;&#23384;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#20219;&#20309;&#25915;&#20987;&#30340;&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;&#65288;CAD&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21508;&#31181;&#25915;&#20987;&#36880;&#20010;&#38454;&#27573;&#20986;&#29616;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;CAD&#22522;&#20110;&#22235;&#39033;&#21407;&#21017;&#36827;&#34892;&#24314;&#27169;&#65306;(1) &#25345;&#32493;&#36866;&#24212;&#26032;&#25915;&#20987;&#32780;&#26080;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;(2) &#23569;&#26679;&#26412;&#36866;&#24212;&#65292;(3) &#20869;&#23384;&#39640;&#25928;&#36866;&#24212;&#65292;&#20197;&#21450;(4) &#39640;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09481v2 Announce Type: replace-cross  Abstract: In response to the rapidly evolving nature of adversarial attacks against visual classifiers on a monthly basis, numerous defenses have been proposed to generalize against as many known attacks as possible. However, designing a defense method that generalizes to all types of attacks is not realistic because the environment in which defense systems operate is dynamic and comprises various unique attacks that emerge as time goes on. The defense system must gather online few-shot defense feedback to promptly enhance itself, leveraging efficient memory utilization. Therefore, we propose the first continual adversarial defense (CAD) framework that adapts to any attacks in a dynamic scenario, where various attacks emerge stage by stage. In practice, CAD is modeled under four principles: (1) continual adaptation to new attacks without catastrophic forgetting, (2) few-shot adaptation, (3) memory-efficient adaptation, and (4) high accur
&lt;/p&gt;</description></item><item><title>Monkey&#36890;&#36807;&#25552;&#39640;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#37319;&#29992;&#22810;&#32423;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#35814;&#32454;&#30340;&#35270;&#35273;&#25429;&#25417;&#21644;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2311.06607</link><description>&lt;p&gt;
Monkey: &#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#25991;&#26412;&#26631;&#31614;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06607
&lt;/p&gt;
&lt;p&gt;
Monkey&#36890;&#36807;&#25552;&#39640;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#37319;&#29992;&#22810;&#32423;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#35814;&#32454;&#30340;&#35270;&#35273;&#25429;&#25417;&#21644;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#22312;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#22312;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#21644;&#35814;&#32454;&#22330;&#26223;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Monkey&#26469;&#22686;&#24378;LMM&#30340;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;Monkey&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#21010;&#20998;&#20026;&#32479;&#19968;&#30340;&#34917;&#19969;&#26469;&#22788;&#29702;&#22270;&#20687;&#65292;&#27599;&#20010;&#34917;&#19969;&#30340;&#22823;&#23567;&#19982;&#21407;&#26469;&#35757;&#32451;&#33391;&#22909;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20351;&#29992;&#30340;&#22823;&#23567;(&#20363;&#22914;448x448)&#30456;&#21305;&#37197;&#12290;&#37197;&#22791;&#20102;&#27599;&#20010;&#34917;&#19969;&#30340;&#36866;&#37197;&#22120;&#65292;Monkey&#21487;&#20197;&#22788;&#29702;&#39640;&#36798;1344x896&#20687;&#32032;&#30340;&#26356;&#39640;&#20998;&#36776;&#29575;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#35270;&#35273;&#20449;&#24687;&#30340;&#35814;&#32454;&#25429;&#25417;&#12290;&#20854;&#27425;&#65292;&#23427;&#37319;&#29992;&#22810;&#32423;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#65292;&#20016;&#23500;&#20102;&#22330;&#26223;-&#23545;&#35937;&#20851;&#32852;&#30340;&#19978;&#19979;&#25991;&#12290;&#36825;&#31181;&#20004;&#37096;&#20998;&#31574;&#30053;&#30830;&#20445;&#20102;&#20174;&#29983;&#25104;&#25968;&#25454;&#20013;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#65306;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#20801;&#35768;&#23545;&#35270;&#35273;&#36827;&#34892;&#26356;&#35814;&#32454;&#30340;&#25429;&#25417;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20840;&#38754;&#25551;&#36848;&#30340;&#25928;&#26524;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06607v3 Announce Type: replace-cross  Abstract: Large Multimodal Models (LMMs) have shown promise in vision-language tasks but struggle with high-resolution input and detailed scene understanding. Addressing these challenges, we introduce Monkey to enhance LMM capabilities. Firstly, Monkey processes input images by dividing them into uniform patches, each matching the size (e.g., 448x448) used in the original training of the well-trained vision encoder. Equipped with individual adapter for each patch, Monkey can handle higher resolutions up to 1344x896 pixels, enabling the detailed capture of complex visual information. Secondly, it employs a multi-level description generation method, enriching the context for scene-object associations. This two-part strategy ensures more effective learning from generated data: the higher resolution allows for a more detailed capture of visuals, which in turn enhances the effectiveness of comprehensive descriptions. Extensive ablative result
&lt;/p&gt;</description></item><item><title>&#38271;&#23614;&#35782;&#21035;&#20013;&#30340;&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;&#26041;&#27861;(GCD)&#30340;&#37325;&#22823;&#38480;&#21046;&#26159;&#20551;&#35774;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#26159;&#22343;&#34913;&#30340;&#65292;&#32780;&#20107;&#23454;&#19978;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#31867;&#21035;&#36890;&#24120;&#21576;&#29616;&#38271;&#23614;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38271;&#23614;&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;&#65288;Long-tailed GCD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#31574;&#30053;&#24615;&#27491;&#21017;&#21270;&#23454;&#29616;&#20102;&#23545;&#36739;&#23569;&#20986;&#29616;&#30340;&#23614;&#37096;&#31867;&#21035;&#30340;&#37325;&#35201;&#24615;&#30340;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2401.05352</link><description>&lt;p&gt;
&#38271;&#23614;&#35782;&#21035;&#20013;&#30340;&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Generalized Categories Discovery for Long-tailed Recognition. (arXiv:2401.05352v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05352
&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#35782;&#21035;&#20013;&#30340;&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;&#26041;&#27861;(GCD)&#30340;&#37325;&#22823;&#38480;&#21046;&#26159;&#20551;&#35774;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#26159;&#22343;&#34913;&#30340;&#65292;&#32780;&#20107;&#23454;&#19978;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#31867;&#21035;&#36890;&#24120;&#21576;&#29616;&#38271;&#23614;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38271;&#23614;&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;&#65288;Long-tailed GCD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#31574;&#30053;&#24615;&#27491;&#21017;&#21270;&#23454;&#29616;&#20102;&#23545;&#36739;&#23569;&#20986;&#29616;&#30340;&#23614;&#37096;&#31867;&#21035;&#30340;&#37325;&#35201;&#24615;&#30340;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23427;&#21033;&#29992;&#20102;&#36890;&#36807;&#24050;&#26631;&#35760;&#31867;&#21035;&#38598;&#21512;&#33719;&#21462;&#30340;&#27934;&#23519;&#21147;&#12290;&#29616;&#26377;&#30340;GCD&#26041;&#27861;&#30340;&#19968;&#20010;&#26174;&#33879;&#38480;&#21046;&#26159;&#23427;&#20204;&#20551;&#35774;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#26159;&#22343;&#34913;&#30340;&#12290;&#19982;&#36825;&#19968;&#20551;&#35774;&#30456;&#21453;&#65292;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#31867;&#21035;&#36890;&#24120;&#34920;&#29616;&#20986;&#38271;&#23614;&#20998;&#24067;&#65292;&#24050;&#30693;&#25110;&#26222;&#36941;&#30340;&#31867;&#21035;&#27604;&#32597;&#35265;&#30340;&#31867;&#21035;&#26356;&#39057;&#32321;&#22320;&#20986;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#24357;&#21512;&#36825;&#31181;&#24046;&#36317;&#65292;&#30528;&#37325;&#20110;&#38271;&#23614;&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;&#65288;Long-tailed GCD&#65289;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;&#21453;&#26144;&#20102;&#29616;&#23454;&#19990;&#30028;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#22266;&#26377;&#19981;&#24179;&#34913;&#24615;&#12290;&#38024;&#23545;&#38271;&#23614;GCD&#25152;&#24102;&#26469;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#20010;&#31574;&#30053;&#24615;&#27491;&#21017;&#21270;&#30340;&#24378;&#22823;&#26041;&#27861;:&#65288;i&#65289;&#19968;&#31181;&#21152;&#26435;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#36739;&#23569;&#20986;&#29616;&#30340;&#23614;&#37096;&#31867;&#21035;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized Class Discovery (GCD) plays a pivotal role in discerning both known and unknown categories from unlabeled datasets by harnessing the insights derived from a labeled set comprising recognized classes. A significant limitation in prevailing GCD methods is their presumption of an equitably distributed category occurrence in unlabeled data. Contrary to this assumption, visual classes in natural environments typically exhibit a long-tailed distribution, with known or prevalent categories surfacing more frequently than their rarer counterparts. Our research endeavors to bridge this disconnect by focusing on the long-tailed Generalized Category Discovery (Long-tailed GCD) paradigm, which echoes the innate imbalances of real-world unlabeled datasets. In response to the unique challenges posed by Long-tailed GCD, we present a robust methodology anchored in two strategic regularizations: (i) a reweighting mechanism that bolsters the prominence of less-represented, tail-end categories
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20854;&#22312;&#29289;&#31181;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00608</link><description>&lt;p&gt;
&#23558;&#19978;&#19979;&#25991;&#24102;&#22238;&#26469;&#65306;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#30456;&#26426;&#38519;&#38449;&#29289;&#31181;&#35782;&#21035;&#20316;&#20026;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bringing Back the Context: Camera Trap Species Identification as Link Prediction on Multimodal Knowledge Graphs. (arXiv:2401.00608v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20854;&#22312;&#29289;&#31181;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#26426;&#38519;&#38449;&#22312;&#21160;&#29289;&#29983;&#24577;&#23398;&#20013;&#26159;&#23453;&#36149;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#29289;&#22810;&#26679;&#24615;&#30417;&#27979;&#21644;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22914;&#22312;&#26032;&#30340;&#26410;&#30693;&#20301;&#32622;&#37096;&#32626;&#26102;&#30340;&#31967;&#31957;&#27867;&#21270;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22270;&#20687;&#33258;&#28982;&#19982;&#21487;&#33021;&#22312;&#19981;&#21516;&#27169;&#24577;&#19979;&#30340;&#24322;&#36136;&#19978;&#19979;&#25991;&#30456;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19982;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30456;&#20851;&#32852;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#25913;&#21892;&#22312;&#30456;&#26426;&#38519;&#38449;&#20013;&#29289;&#31181;&#35782;&#21035;&#36825;&#20010;&#20219;&#21153;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#19968;&#24352;&#37326;&#29983;&#21160;&#29289;&#30340;&#29031;&#29255;&#21487;&#33021;&#19982;&#25293;&#25668;&#22320;&#28857;&#21644;&#26102;&#38388;&#20197;&#21450;&#20851;&#20110;&#21160;&#29289;&#29289;&#31181;&#30340;&#32467;&#26500;&#21270;&#29983;&#29289;&#23398;&#30693;&#35782;&#30456;&#20851;&#32852;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#24573;&#35270;&#36825;&#19968;&#28857;&#65292;&#20294;&#23558;&#36825;&#26679;&#30340;&#19978;&#19979;&#25991;&#24102;&#22238;&#26469;&#21487;&#20197;&#24102;&#26469;&#19968;&#20123;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#22914;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#21644;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#36825;&#26679;&#30340;&#24322;&#36136;&#19978;&#19979;&#25991;&#25972;&#21512;&#21040;&#35270;&#35273;&#39046;&#22495;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Camera traps are valuable tools in animal ecology for biodiversity monitoring and conservation. However, challenges like poor generalization to deployment at new unseen locations limit their practical application. Images are naturally associated with heterogeneous forms of context possibly in different modalities. In this work, we leverage the structured context associated with the camera trap images to improve out-of-distribution generalization for the task of species identification in camera traps. For example, a photo of a wild animal may be associated with information about where and when it was taken, as well as structured biology knowledge about the animal species. While typically overlooked by existing work, bringing back such context offers several potential benefits for better image understanding, such as addressing data scarcity and enhancing generalization. However, effectively integrating such heterogeneous context into the visual domain is a challenging problem. To address
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; Text2Seg &#30340;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#27969;&#31243;&#65292;&#21033;&#29992;&#22810;&#20010;&#22522;&#30784;&#27169;&#22411;&#21644;&#25991;&#26412;&#24341;&#23548;&#65292;&#21462;&#24471;&#20102;&#21021;&#27493;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.10597</link><description>&lt;p&gt;
Text2Seg: &#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Text2Seg: Remote Sensing Image Semantic Segmentation via Text-Guided Visual Foundation Models. (arXiv:2304.10597v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; Text2Seg &#30340;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#27969;&#31243;&#65292;&#21033;&#29992;&#22810;&#20010;&#22522;&#30784;&#27169;&#22411;&#21644;&#25991;&#26412;&#24341;&#23548;&#65292;&#21462;&#24471;&#20102;&#21021;&#27493;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#65292;&#22914; GPT-4 &#21644; LLaMA&#65292;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#26696;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21560;&#24341;&#20102;&#22823;&#37327;&#20851;&#27880;&#12290;&#31867;&#20284;&#22320;&#65292;&#22312;&#35270;&#35273;&#23398;&#20064;&#39046;&#22495;&#65292;Grounding DINO &#21644; Segment Anything Model&#65288;SAM&#65289;&#31561;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#26816;&#27979;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#36965;&#24863;&#39046;&#22495;&#65292;&#20854;&#20013;&#22270;&#29255;&#19982;&#20256;&#32479;&#22330;&#26223;&#20013;&#30340;&#22270;&#29255;&#26126;&#26174;&#19981;&#21516;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27969;&#31243;&#65292;&#21033;&#29992;&#22810;&#20010; FMs&#65292;&#20197;&#25991;&#26412;&#25552;&#31034;&#20026;&#25351;&#23548;&#65292;&#20419;&#36827;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026; Text2Seg &#12290;&#35813;&#31649;&#36947;&#22312;&#22810;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#36965;&#24863;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;&#21021;&#27493;&#32467;&#26524;&#20197;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in foundation models (FMs), such as GPT-4 and LLaMA, have attracted significant attention due to their exceptional performance in zero-shot learning scenarios. Similarly, in the field of visual learning, models like Grounding DINO and the Segment Anything Model (SAM) have exhibited remarkable progress in open-set detection and instance segmentation tasks. It is undeniable that these FMs will profoundly impact a wide range of real-world visual learning tasks, ushering in a new paradigm shift for developing such models. In this study, we concentrate on the remote sensing domain, where the images are notably dissimilar from those in conventional scenarios. We developed a pipeline that leverages multiple FMs to facilitate remote sensing image semantic segmentation tasks guided by text prompt, which we denote as Text2Seg. The pipeline is benchmarked on several widely-used remote sensing datasets, and we present preliminary results to demonstrate its effectiveness. Throug
&lt;/p&gt;</description></item></channel></rss>