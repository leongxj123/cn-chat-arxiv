<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.16970</link><description>&lt;p&gt;
&#32852;&#21512;&#33016;&#37096;X&#20809;&#35786;&#26029;&#21644;&#20020;&#24202;&#35270;&#35273;&#27880;&#24847;&#21147;&#39044;&#27979;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#23398;&#20064;&#65306;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16970
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#33258;&#21160;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20020;&#24202;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#36825;&#19968;&#39046;&#22495;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#20294;&#22312;&#25918;&#23556;&#23398;&#31579;&#26597;&#36807;&#31243;&#20013;&#20020;&#24202;&#21307;&#29983;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#22270;&#20026;&#25552;&#20379;&#37325;&#35201;&#27934;&#23519;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#36164;&#20135;&#65292;&#24182;&#26377;&#21487;&#33021;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#36825;&#31687;&#35770;&#25991;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#65292;&#21033;&#29992;&#20102;DenseNet201&#20027;&#24178;&#21644;&#22522;&#20110;&#27531;&#24046;&#21644;&#33192;&#32960;&#28608;&#21169;&#22359;&#30340;&#32534;&#30721;&#22120;&#26469;&#25552;&#21462;&#29992;&#20110;&#26174;&#33879;&#24615;&#22270;&#39044;&#27979;&#30340;&#22810;&#26679;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#36827;&#34892;&#30142;&#30149;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16970v1 Announce Type: cross  Abstract: As deep learning has become the state-of-the-art for computer-assisted diagnosis, interpretability of the automatic decisions is crucial for clinical deployment. While various methods were proposed in this domain, visual attention maps of clinicians during radiological screening offer a unique asset to provide important insights and can potentially enhance the quality of computer-assisted diagnosis. With this paper, we introduce a novel deep-learning framework for joint disease diagnosis and prediction of corresponding visual saliency maps for chest X-ray scans. Specifically, we designed a novel dual-encoder multi-task UNet, which leverages both a DenseNet201 backbone and a Residual and Squeeze-and-Excitation block-based encoder to extract diverse features for saliency map prediction, and a multi-scale feature-fusion classifier to perform disease classification. To tackle the issue of asynchronous training schedules of individual tasks
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65292;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.09918</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Attention-based Class-Conditioned Alignment for Multi-Source Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09918
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65292;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#65288;OD&#65289;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#33268;&#21147;&#20110;&#36890;&#36807;&#20419;&#36827;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#29305;&#24449;&#23545;&#40784;&#26469;&#32531;&#35299;&#20998;&#24067;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#20801;&#35768;&#21033;&#29992;&#22810;&#20010;&#24102;&#27880;&#37322;&#30340;&#28304;&#25968;&#25454;&#38598;&#21644;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#26469;&#25552;&#39640;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;OD MSDA&#26041;&#27861;&#20197;&#19968;&#31181;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#26041;&#24335;&#25191;&#34892;&#29305;&#24449;&#23545;&#40784;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#25353;&#31867;&#21035;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#22024;&#26434;&#30340;&#20266;&#26631;&#31614;&#32780;&#23548;&#33268;&#38169;&#35823;&#31215;&#32047;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;MSDA&#65292;&#35813;&#26041;&#26696;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09918v1 Announce Type: cross  Abstract: Domain adaptation methods for object detection (OD) strive to mitigate the impact of distribution shifts by promoting feature alignment across source and target domains. Multi-source domain adaptation (MSDA) allows leveraging multiple annotated source datasets, and unlabeled target data to improve the accuracy and robustness of the detection model. Most state-of-the-art MSDA methods for OD perform feature alignment in a class-agnostic manner. This is challenging since the objects have unique modal information due to variations in object appearance across domains. A recent prototype-based approach proposed a class-wise alignment, yet it suffers from error accumulation due to noisy pseudo-labels which can negatively affect adaptation with imbalanced data. To overcome these limitations, we propose an attention-based class-conditioned alignment scheme for MSDA that aligns instances of each object category across domains. In particular, an 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#35843;UNet&#36827;&#34892;&#20302;&#21058;&#37327;CT&#22270;&#20687;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#20108;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#20026;CT&#22270;&#20687;&#22686;&#24378;&#38454;&#27573;&#12290;</title><link>https://arxiv.org/abs/2403.03551</link><description>&lt;p&gt;
&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#20026;&#39640;&#26031;&#38477;&#22122;&#32780;&#35757;&#32451;&#30340;UNet&#36827;&#34892;&#20302;&#21058;&#37327;CT&#22270;&#20687;&#37325;&#24314;&#65292;&#29992;&#20110;&#22270;&#20687;&#22686;&#24378;&#30340;&#19979;&#28216;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Low-Dose CT Image Reconstruction by Fine-Tuning a UNet Pretrained for Gaussian Denoising for the Downstream Task of Image Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03551
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#35843;UNet&#36827;&#34892;&#20302;&#21058;&#37327;CT&#22270;&#20687;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#20108;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#20026;CT&#22270;&#20687;&#22686;&#24378;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21307;&#23398;&#25104;&#20687;&#27169;&#24577;&#65292;&#30001;&#20110;&#20854;&#22522;&#20110;&#30005;&#31163;&#36752;&#23556;&#65292;&#22240;&#27492;&#24076;&#26395;&#23613;&#37327;&#20943;&#23569;&#36752;&#23556;&#21058;&#37327;&#12290;&#28982;&#32780;&#65292;&#38477;&#20302;&#36752;&#23556;&#21058;&#37327;&#20250;&#23548;&#33268;&#22270;&#20687;&#36136;&#37327;&#19979;&#38477;&#65292;&#20174;&#20302;&#21058;&#37327;CT&#65288;LDCT&#65289;&#25968;&#25454;&#37325;&#24314;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20540;&#24471;&#36827;&#34892;&#30740;&#31350;&#12290;&#26681;&#25454;LoDoPaB-CT&#22522;&#20934;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#28041;&#21450;UNet&#22411;&#26550;&#26500;&#30340;&#27969;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25490;&#21517;&#31532;&#19968;&#30340;&#26041;&#27861;ItNet&#20351;&#29992;&#21253;&#25324;&#28388;&#27874;&#21453;&#25237;&#24433;&#65288;FBP&#65289;&#12289;&#22312;CT&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;UNet&#21644;&#36845;&#20195;&#32454;&#21270;&#27493;&#39588;&#30340;&#19977;&#38454;&#27573;&#27969;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#12290;&#31532;&#19968;&#38454;&#27573;&#20063;&#20351;&#29992;&#20102;FBP&#65292;&#32780;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#31532;&#20108;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#29305;&#28857;&#26159;CT&#22270;&#20687;&#22686;&#24378;&#38454;&#27573;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#28857;&#22312;&#20110;&#31070;&#32463;&#32593;&#32476;&#26159;&#39044;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03551v1 Announce Type: cross  Abstract: Computed Tomography (CT) is a widely used medical imaging modality, and as it is based on ionizing radiation, it is desirable to minimize the radiation dose. However, a reduced radiation dose comes with reduced image quality, and reconstruction from low-dose CT (LDCT) data is still a challenging task which is subject to research. According to the LoDoPaB-CT benchmark, a benchmark for LDCT reconstruction, many state-of-the-art methods use pipelines involving UNet-type architectures. Specifically the top ranking method, ItNet, employs a three-stage process involving filtered backprojection (FBP), a UNet trained on CT data, and an iterative refinement step. In this paper, we propose a less complex two-stage method. The first stage also employs FBP, while the novelty lies in the training strategy for the second stage, characterized as the CT image enhancement stage. The crucial point of our approach is that the neural network is pretrained
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#28210;&#26579;&#26041;&#27861;&#65292;&#21517;&#20026;FPA&#65292;&#36890;&#36807;&#23398;&#20064;&#23545;&#25239;&#27169;&#24335;&#24182;&#32467;&#21512;&#29305;&#27530;&#35774;&#35745;&#30340;&#23545;&#25239;&#25439;&#22833;&#21644;&#38544;&#34109;&#32422;&#26463;&#25439;&#22833;&#65292;&#21487;&#20197;&#29983;&#25104;&#29289;&#29702;&#19990;&#30028;&#20013;&#20855;&#26377;&#23545;&#25239;&#24615;&#21644;&#38544;&#34109;&#24615;&#36136;&#30340;&#20266;&#35013;&#12290;</title><link>https://arxiv.org/abs/2402.13575</link><description>&lt;p&gt;
&#22522;&#20110;&#24046;&#24322;&#26041;&#27861;&#30340;&#28789;&#27963;&#29289;&#29702;&#20266;&#35013;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Flexible Physical Camouflage Generation Based on a Differential Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13575
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#28210;&#26579;&#26041;&#27861;&#65292;&#21517;&#20026;FPA&#65292;&#36890;&#36807;&#23398;&#20064;&#23545;&#25239;&#27169;&#24335;&#24182;&#32467;&#21512;&#29305;&#27530;&#35774;&#35745;&#30340;&#23545;&#25239;&#25439;&#22833;&#21644;&#38544;&#34109;&#32422;&#26463;&#25439;&#22833;&#65292;&#21487;&#20197;&#29983;&#25104;&#29289;&#29702;&#19990;&#30028;&#20013;&#20855;&#26377;&#23545;&#25239;&#24615;&#21644;&#38544;&#34109;&#24615;&#36136;&#30340;&#20266;&#35013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#28210;&#26579;&#26041;&#27861;&#65292;&#19987;&#38376;&#38024;&#23545;&#23545;&#25239;&#20266;&#35013;&#65292;&#22312;&#24191;&#27867;&#30340;&#19977;&#32500;&#28210;&#26579;&#26694;&#26550;&#20869;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;FPA&#65292;&#36890;&#36807;&#24544;&#23454;&#22320;&#27169;&#25311;&#20809;&#29031;&#26465;&#20214;&#21644;&#26448;&#26009;&#21464;&#21270;&#65292;&#30830;&#20445;&#22312;&#19977;&#32500;&#30446;&#26631;&#19978;&#23545;&#32441;&#29702;&#36827;&#34892;&#24494;&#22937;&#32780;&#36924;&#30495;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#23398;&#20064;&#23545;&#25239;&#27169;&#24335;&#12290;&#36825;&#28041;&#21450;&#23558;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#30340;&#23545;&#25239;&#25439;&#22833;&#21644;&#38544;&#34109;&#32422;&#26463;&#25439;&#22833;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#30830;&#20445;&#20266;&#35013;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#24615;&#21644;&#38544;&#34109;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#20266;&#35013;&#22312;&#36148;&#32440;&#27169;&#24335;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#35206;&#30422;&#30446;&#26631;&#32780;&#19981;&#24433;&#21709;&#23545;&#25239;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29289;&#29702;&#23454;&#39564;&#65292;FPA&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#21487;&#36716;&#31227;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13575v1 Announce Type: cross  Abstract: This study introduces a novel approach to neural rendering, specifically tailored for adversarial camouflage, within an extensive 3D rendering framework. Our method, named FPA, goes beyond traditional techniques by faithfully simulating lighting conditions and material variations, ensuring a nuanced and realistic representation of textures on a 3D target. To achieve this, we employ a generative approach that learns adversarial patterns from a diffusion model. This involves incorporating a specially designed adversarial loss and covert constraint loss to guarantee the adversarial and covert nature of the camouflage in the physical world. Furthermore, we showcase the effectiveness of the proposed camouflage in sticker mode, demonstrating its ability to cover the target without compromising adversarial information. Through empirical and physical experiments, FPA exhibits strong performance in terms of attack success rate and transferabili
&lt;/p&gt;</description></item><item><title>TorchCP&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;Python&#24037;&#20855;&#21253;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#29616;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#22810;&#31181;&#24037;&#20855;&#65292;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;En_Tdlr: TorchCP is a Python toolbox built on PyTorch for conformal prediction research on deep learning models, providing various implementations for posthoc and training methods for classification and regression tasks, including multi-dimension output.</title><link>https://arxiv.org/abs/2402.12683</link><description>&lt;p&gt;
TorchCP&#65306;&#22522;&#20110;PyTorch&#30340;&#19968;&#31181;&#36866;&#29992;&#20110;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
TorchCP: A Library for Conformal Prediction based on PyTorch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12683
&lt;/p&gt;
&lt;p&gt;
TorchCP&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;Python&#24037;&#20855;&#21253;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#29616;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#22810;&#31181;&#24037;&#20855;&#65292;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;En_Tdlr: TorchCP is a Python toolbox built on PyTorch for conformal prediction research on deep learning models, providing various implementations for posthoc and training methods for classification and regression tasks, including multi-dimension output.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TorchCP&#26159;&#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#30340;Python&#24037;&#20855;&#21253;&#12290;&#23427;&#21253;&#21547;&#20102;&#29992;&#20110;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#21508;&#31181;&#23454;&#29616;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65288;&#21253;&#25324;&#22810;&#32500;&#36755;&#20986;&#65289;&#12290;TorchCP&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#24182;&#21033;&#29992;&#30697;&#38453;&#35745;&#31639;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#31616;&#27905;&#39640;&#25928;&#30340;&#25512;&#29702;&#23454;&#29616;&#12290;&#35813;&#20195;&#30721;&#37319;&#29992;LGPL&#35768;&#21487;&#35777;&#65292;&#24182;&#22312;$\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this https URL}}$&#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12683v1 Announce Type: new  Abstract: TorchCP is a Python toolbox for conformal prediction research on deep learning models. It contains various implementations for posthoc and training methods for classification and regression tasks (including multi-dimension output). TorchCP is built on PyTorch (Paszke et al., 2019) and leverages the advantages of matrix computation to provide concise and efficient inference implementations. The code is licensed under the LGPL license and is open-sourced at $\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this https URL}}$.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;AICAttack&#65292;&#26088;&#22312;&#36890;&#36807;&#24494;&#23567;&#30340;&#22270;&#20687;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#65292;&#22312;&#40657;&#30418;&#25915;&#20987;&#24773;&#26223;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.11940</link><description>&lt;p&gt;
AICAttack&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#20248;&#21270;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#23383;&#24149;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11940
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;AICAttack&#65292;&#26088;&#22312;&#36890;&#36807;&#24494;&#23567;&#30340;&#22270;&#20687;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#65292;&#22312;&#40657;&#30418;&#25915;&#20987;&#24773;&#26223;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21462;&#24471;&#20102;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31561;&#35768;&#22810;&#20219;&#21153;&#19978;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;CV&#21644;NLP&#20132;&#21449;&#28857;&#19978;&#30340;&#22270;&#20687;&#23383;&#24149;&#38382;&#39064;&#20013;&#65292;&#30456;&#20851;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;&#65292;&#31216;&#20026;AICAttack&#65288;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#20687;&#23383;&#24149;&#25915;&#20987;&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#22270;&#20687;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#12290;&#22312;&#40657;&#30418;&#25915;&#20987;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#21442;&#25968;&#25110;&#26799;&#24230;&#20449;&#24687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20505;&#36873;&#36873;&#25321;&#26426;&#21046;&#65292;&#21487;&#35782;&#21035;&#26368;&#20339;&#20687;&#32032;&#36827;&#34892;&#25915;&#20987;&#65292;&#28982;&#21518;&#37319;&#29992;&#24046;&#20998;&#36827;&#21270;&#65288;DE&#65289;&#26469;&#25200;&#20081;&#20687;&#32032;&#30340;RGB&#20540;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;AICAttack&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11940v1 Announce Type: cross  Abstract: Recent advances in deep learning research have shown remarkable achievements across many tasks in computer vision (CV) and natural language processing (NLP). At the intersection of CV and NLP is the problem of image captioning, where the related models' robustness against adversarial attacks has not been well studied. In this paper, we present a novel adversarial attack strategy, which we call AICAttack (Attention-based Image Captioning Attack), designed to attack image captioning models through subtle perturbations on images. Operating within a black-box attack scenario, our algorithm requires no access to the target model's architecture, parameters, or gradient information. We introduce an attention-based candidate selection mechanism that identifies the optimal pixels to attack, followed by Differential Evolution (DE) for perturbing pixels' RGB values. We demonstrate AICAttack's effectiveness through extensive experiments on benchma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20849;&#20139;&#27604;&#20363;&#20998;&#35299;(SRD)&#30340;&#26032;&#39062;&#35299;&#37322;&#26041;&#27861;&#65292;&#30495;&#23454;&#22320;&#21453;&#26144;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#22312;&#35299;&#37322;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#21521;&#37327;&#35270;&#35282;&#21644;&#32771;&#34385;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20132;&#20114;&#65292;&#20197;&#21450;&#24341;&#20837;&#20165;&#28608;&#27963;&#27169;&#24335;&#39044;&#27979;(APOP)&#26041;&#27861;&#65292;&#21487;&#20197;&#37325;&#26032;&#23450;&#20041;&#30456;&#20851;&#24615;&#24182;&#24378;&#35843;&#38750;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03348</link><description>&lt;p&gt;
&#23562;&#37325;&#27169;&#22411;: &#32454;&#31890;&#24230;&#19988;&#40065;&#26834;&#30340;&#35299;&#37322;&#19982;&#20849;&#20139;&#27604;&#20363;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Respect the model: Fine-grained and Robust Explanation with Sharing Ratio Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20849;&#20139;&#27604;&#20363;&#20998;&#35299;(SRD)&#30340;&#26032;&#39062;&#35299;&#37322;&#26041;&#27861;&#65292;&#30495;&#23454;&#22320;&#21453;&#26144;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#22312;&#35299;&#37322;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#21521;&#37327;&#35270;&#35282;&#21644;&#32771;&#34385;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20132;&#20114;&#65292;&#20197;&#21450;&#24341;&#20837;&#20165;&#28608;&#27963;&#27169;&#24335;&#39044;&#27979;(APOP)&#26041;&#27861;&#65292;&#21487;&#20197;&#37325;&#26032;&#23450;&#20041;&#30456;&#20851;&#24615;&#24182;&#24378;&#35843;&#38750;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29616;&#26377;&#30340;&#35299;&#37322;&#26041;&#27861;&#33021;&#21542;&#30495;&#23454;&#38416;&#26126;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#30495;&#23454;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#29616;&#26377;&#26041;&#27861;&#20559;&#31163;&#20102;&#23545;&#27169;&#22411;&#30340;&#24544;&#23454;&#34920;&#36798;&#65292;&#22240;&#27492;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;(XAI)&#26041;&#27861;&#65292;&#31216;&#20026;SRD(&#20849;&#20139;&#27604;&#20363;&#20998;&#35299;)&#65292;&#23427;&#30495;&#23454;&#22320;&#21453;&#26144;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#40065;&#26834;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#20803;&#32423;&#21035;&#24378;&#35843;&#19981;&#21516;&#65292;&#25105;&#20204;&#37319;&#29992;&#21521;&#37327;&#35270;&#35282;&#26469;&#32771;&#34385;&#28388;&#27874;&#22120;&#20043;&#38388;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20132;&#20114;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#35266;&#23519;&#65292;&#31216;&#20026;&#20165;&#28608;&#27963;&#27169;&#24335;&#39044;&#27979;(APOP)&#65292;&#35753;&#25105;&#20204;&#24378;&#35843;&#38750;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#37325;&#26032;&#23450;&#20041;&#30456;&#20851;&#24615;&#65292;&#21253;&#25324;&#27963;&#36291;&#21644;&#38750;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;SRD&#20801;&#35768;&#36882;&#24402;&#20998;&#35299;&#19968;&#20010;&#28857;&#29305;&#24449;&#21521;&#37327;(PFV)&#12290;
&lt;/p&gt;
&lt;p&gt;
The truthfulness of existing explanation methods in authentically elucidating the underlying model's decision-making process has been questioned. Existing methods have deviated from faithfully representing the model, thus susceptible to adversarial attacks. To address this, we propose a novel eXplainable AI (XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects the model's inference process, resulting in significantly enhanced robustness in our explanations. Different from the conventional emphasis on the neuronal level, we adopt a vector perspective to consider the intricate nonlinear interactions between filters. We also introduce an interesting observation termed Activation-Pattern-Only Prediction (APOP), letting us emphasize the importance of inactive neurons and redefine relevance encapsulating all relevant information including both active and inactive neurons. Our method, SRD, allows for the recursive decomposition of a Pointwise Feature Vector (PFV), pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HieraFashDiff&#30340;&#26032;&#22411;&#26102;&#23578;&#35774;&#35745;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#20174;&#39640;&#32423;&#35774;&#35745;&#27010;&#24565;&#21040;&#20302;&#32423;&#26381;&#35013;&#23646;&#24615;&#30340;&#20998;&#23618;&#35774;&#35745;&#21644;&#32534;&#36753;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22312;&#26102;&#23578;&#35774;&#35745;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.07450</link><description>&lt;p&gt;
&#24102;&#26377;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#26102;&#23578;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Fashion Design with Multi-stage Diffusion Models. (arXiv:2401.07450v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HieraFashDiff&#30340;&#26032;&#22411;&#26102;&#23578;&#35774;&#35745;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#20174;&#39640;&#32423;&#35774;&#35745;&#27010;&#24565;&#21040;&#20302;&#32423;&#26381;&#35013;&#23646;&#24615;&#30340;&#20998;&#23618;&#35774;&#35745;&#21644;&#32534;&#36753;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22312;&#26102;&#23578;&#35774;&#35745;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#26102;&#23578;&#21512;&#25104;&#21644;&#32534;&#36753;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#21644;&#23616;&#37096;&#20462;&#25913;&#35774;&#35745;&#33609;&#22270;&#65292;&#20026;&#26102;&#23578;&#35774;&#35745;&#24072;&#25552;&#20379;&#26234;&#33021;&#25903;&#25345;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21487;&#38752;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#20294;&#22312;&#20174;&#25277;&#35937;&#30340;&#35774;&#35745;&#20803;&#32032;&#20013;&#29983;&#25104;&#26102;&#23578;&#35774;&#35745;&#21644;&#31934;&#32454;&#32534;&#36753;&#26041;&#38754;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#39640;&#32423;&#35774;&#35745;&#27010;&#24565;&#65292;&#20363;&#22914;&#21150;&#20844;&#23460;&#12289;&#21830;&#21153;&#21644;&#27966;&#23545;&#65292;&#24418;&#25104;&#20102;&#25277;&#35937;&#30340;&#24863;&#23448;&#34920;&#36798;&#26041;&#24335;&#65292;&#32780;&#34966;&#38271;&#12289;&#39046;&#22411;&#21644;&#35044;&#38271;&#31561;&#21487;&#34913;&#37327;&#30340;&#26041;&#38754;&#34987;&#35270;&#20026;&#26381;&#35013;&#30340;&#20302;&#32423;&#23646;&#24615;&#12290;&#20351;&#29992;&#20887;&#38271;&#30340;&#25991;&#23383;&#25551;&#36848;&#26469;&#25511;&#21046;&#21644;&#32534;&#36753;&#26102;&#23578;&#22270;&#20687;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HieraFashDiff&#30340;&#26032;&#22411;&#26102;&#23578;&#35774;&#35745;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20849;&#20139;&#30340;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#39640;&#32423;&#35774;&#35745;&#27010;&#24565;&#21644;&#20302;&#32423;&#26381;&#35013;&#23646;&#24615;&#34701;&#20837;&#21040;&#20998;&#23618;&#32467;&#26500;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#36755;&#20837;&#25991;&#26412;&#20998;&#20026;&#19981;&#21516;&#30340;&#23618;&#27425;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#22810;&#32423;&#25193;&#25955;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal fashion synthesis and editing offer intelligent support to fashion designers by enabling the automatic generation and local modification of design drafts.While current diffusion models demonstrate commendable stability and controllability in image synthesis,they still face significant challenges in generating fashion design from abstract design elements and fine-grained editing.Abstract sensory expressions, \eg office, business, and party, form the high-level design concepts, while measurable aspects like sleeve length, collar type, and pant length are considered the low-level attributes of clothing.Controlling and editing fashion images using lengthy text descriptions poses a difficulty.In this paper, we propose HieraFashDiff,a novel fashion design method using the shared multi-stage diffusion model encompassing high-level design concepts and low-level clothing attributes in a hierarchical structure.Specifically, we categorized the input text into different levels and fed 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#23610;&#24230;&#38899;&#35270;&#21516;&#27493;&#25439;&#22833;&#21644;&#22810;&#23610;&#24230;&#33258;&#22238;&#24402;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#21644;&#22836;&#37096;&#21160;&#21147;&#23398;&#30340;&#21516;&#27493;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#24403;&#21069;&#29366;&#24577;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.03270</link><description>&lt;p&gt;
&#35821;&#38899;&#21644;&#21160;&#21147;&#23398;&#21516;&#27493;&#30340;&#20840;&#38754;&#22810;&#23610;&#24230;&#26041;&#27861;&#22312;&#34394;&#25311;&#35828;&#35805;&#22836;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony in Talking Head Generation. (arXiv:2307.03270v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#23610;&#24230;&#38899;&#35270;&#21516;&#27493;&#25439;&#22833;&#21644;&#22810;&#23610;&#24230;&#33258;&#22238;&#24402;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#21644;&#22836;&#37096;&#21160;&#21147;&#23398;&#30340;&#21516;&#27493;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#24403;&#21069;&#29366;&#24577;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20351;&#29992;&#35821;&#38899;&#36755;&#20837;&#20449;&#21495;&#23545;&#38745;&#24577;&#38754;&#37096;&#22270;&#20687;&#36827;&#34892;&#21160;&#30011;&#21270;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#24182;&#19988;&#36817;&#26399;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#22823;&#19968;&#37096;&#20998;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#22068;&#21767;&#21516;&#27493;&#21644;&#28210;&#26579;&#36136;&#37327;&#19978;&#65292;&#24456;&#23569;&#20851;&#27880;&#33258;&#28982;&#22836;&#37096;&#36816;&#21160;&#30340;&#29983;&#25104;&#65292;&#26356;&#19981;&#29992;&#35828;&#22836;&#37096;&#36816;&#21160;&#19982;&#35821;&#38899;&#30340;&#35270;&#21548;&#30456;&#20851;&#24615;&#20102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#38899;&#35270;&#21516;&#27493;&#25439;&#22833;&#21644;&#22810;&#23610;&#24230;&#33258;&#22238;&#24402;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#35821;&#38899;&#19982;&#22836;&#37096;&#21644;&#22068;&#21767;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#30456;&#20851;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#36755;&#20837;&#37329;&#23383;&#22612;&#19978;&#35757;&#32451;&#20102;&#19968;&#22534;&#21516;&#27493;&#27169;&#22411;&#65292;&#24182;&#23558;&#36825;&#20123;&#27169;&#22411;&#29992;&#20316;&#22810;&#23610;&#24230;&#29983;&#25104;&#32593;&#32476;&#20013;&#30340;&#25351;&#23548;&#65292;&#20197;&#20135;&#29983;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#38899;&#39057;&#23545;&#40784;&#36816;&#21160;&#23637;&#24320;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#22120;&#22312;&#38754;&#37096;&#26631;&#24535;&#22495;&#20013;&#25805;&#20316;&#65292;&#36825;&#26159;&#19968;&#31181;&#26631;&#20934;&#30340;&#20302;&#32500;&#22836;&#37096;&#34920;&#31034;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22836;&#37096;&#36816;&#21160;&#21160;&#21147;&#23398;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Animating still face images with deep generative models using a speech input signal is an active research topic and has seen important recent progress. However, much of the effort has been put into lip syncing and rendering quality while the generation of natural head motion, let alone the audio-visual correlation between head motion and speech, has often been neglected. In this work, we propose a multi-scale audio-visual synchrony loss and a multi-scale autoregressive GAN to better handle short and long-term correlation between speech and the dynamics of the head and lips. In particular, we train a stack of syncer models on multimodal input pyramids and use these models as guidance in a multi-scale generator network to produce audio-aligned motion unfolding over diverse time scales. Our generator operates in the facial landmark domain, which is a standard low-dimensional head representation. The experiments show significant improvements over the state of the art in head motion dynamic
&lt;/p&gt;</description></item><item><title>NeuralFuse&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38468;&#21152;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#36755;&#20837;&#36716;&#25442;&#26469;&#29983;&#25104;&#25239;&#35823;&#24046;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20302;&#30005;&#21387;&#29615;&#22659;&#19979;&#26377;&#38480;&#35775;&#38382;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#19982;&#33021;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16869</link><description>&lt;p&gt;
NeuralFuse: &#23398;&#20064;&#25913;&#21892;&#20302;&#30005;&#21387;&#29615;&#22659;&#19979;&#26377;&#38480;&#35775;&#38382;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
NeuralFuse: Learning to Improve the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes. (arXiv:2306.16869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16869
&lt;/p&gt;
&lt;p&gt;
NeuralFuse&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38468;&#21152;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#36755;&#20837;&#36716;&#25442;&#26469;&#29983;&#25104;&#25239;&#35823;&#24046;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20302;&#30005;&#21387;&#29615;&#22659;&#19979;&#26377;&#38480;&#35775;&#38382;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#19982;&#33021;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24050;&#32463;&#26080;&#22788;&#19981;&#22312;&#65292;&#20294;&#20854;&#33021;&#37327;&#28040;&#32791;&#20173;&#28982;&#26159;&#19968;&#20010;&#20540;&#24471;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#38477;&#20302;&#20379;&#30005;&#30005;&#21387;&#26159;&#38477;&#20302;&#33021;&#37327;&#28040;&#32791;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#38477;&#20302;&#20379;&#30005;&#30005;&#21387;&#21487;&#33021;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#65292;&#22240;&#20026;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38745;&#24577;&#38543;&#26426;&#23384;&#20648;&#22120;(SRAM)&#20013;&#65292;&#32780;SRAM&#20013;&#20250;&#21457;&#29983;&#38543;&#26426;&#20301;&#32763;&#36716;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NeuralFuse&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38468;&#21152;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#36755;&#20837;&#36716;&#25442;&#26469;&#29983;&#25104;&#25239;&#35823;&#24046;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#20197;&#22312;&#20302;&#30005;&#21387;&#29615;&#22659;&#20013;&#35299;&#20915;&#20934;&#30830;&#24615;&#19982;&#33021;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;NeuralFuse&#22312;&#26631;&#31216;&#30005;&#21387;&#21644;&#20302;&#30005;&#21387;&#24773;&#20917;&#19979;&#37117;&#33021;&#20445;&#25252;DNN&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;NeuralFuse&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#26377;&#38480;&#35775;&#38382;&#30340;DNN&#65292;&#20363;&#22914;&#19981;&#21487;&#37197;&#32622;&#30340;&#30828;&#20214;&#25110;&#20113;&#31471;API&#30340;&#36828;&#31243;&#35775;&#38382;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;1%&#30340;&#20301;&#38169;&#35823;&#29575;&#19979;&#65292;NeuralFuse&#21487;&#20197;&#23558;SRAM&#20869;&#23384;&#35775;&#38382;&#33021;&#37327;&#38477;&#20302;&#39640;&#36798;24%&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have become ubiquitous in machine learning, but their energy consumption remains a notable issue. Lowering the supply voltage is an effective strategy for reducing energy consumption. However, aggressively scaling down the supply voltage can lead to accuracy degradation due to random bit flips in static random access memory (SRAM) where model parameters are stored. To address this challenge, we introduce NeuralFuse, a novel add-on module that addresses the accuracy-energy tradeoff in low-voltage regimes by learning input transformations to generate error-resistant data representations. NeuralFuse protects DNN accuracy in both nominal and low-voltage scenarios. Moreover, NeuralFuse is easy to implement and can be readily applied to DNNs with limited access, such as non-configurable hardware or remote access to cloud-based APIs. Experimental results demonstrate that, at a 1% bit error rate, NeuralFuse can reduce SRAM memory access energy by up to 24% while imp
&lt;/p&gt;</description></item></channel></rss>