<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>CAM-based&#26041;&#27861;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20915;&#31574;&#26102;&#65292;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#23558;&#27169;&#22411;&#26080;&#27861;&#30475;&#21040;&#30340;&#37096;&#20998;&#24402;&#22240;&#20026;&#37325;&#35201;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#35823;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2404.01964</link><description>&lt;p&gt;
&#22522;&#20110;CAM&#30340;&#26041;&#27861;&#21487;&#20197;&#31359;&#22681;&#32780;&#36807;
&lt;/p&gt;
&lt;p&gt;
CAM-Based Methods Can See through Walls
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01964
&lt;/p&gt;
&lt;p&gt;
CAM-based&#26041;&#27861;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20915;&#31574;&#26102;&#65292;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#23558;&#27169;&#22411;&#26080;&#27861;&#30475;&#21040;&#30340;&#37096;&#20998;&#24402;&#22240;&#20026;&#37325;&#35201;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#35823;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CAM-based&#26041;&#27861;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#20107;&#21518;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29983;&#25104;&#26174;&#33879;&#24615;&#22320;&#22270;&#26469;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#26174;&#33879;&#24615;&#22320;&#22270;&#31361;&#20986;&#26174;&#31034;&#19982;&#39044;&#27979;&#30456;&#20851;&#30340;&#22270;&#20687;&#37325;&#35201;&#21306;&#22495;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#38169;&#35823;&#22320;&#23558;&#22270;&#20687;&#30340;&#26576;&#20123;&#37096;&#20998;&#24402;&#22240;&#20026;&#27169;&#22411;&#26080;&#27861;&#30475;&#21040;&#30340;&#37325;&#35201;&#24471;&#20998;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#31181;&#29616;&#35937;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#20013;&#22343;&#23384;&#22312;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;GradCAM&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#25513;&#33180;CNN&#27169;&#22411;&#21021;&#22987;&#21270;&#26102;&#30340;&#34892;&#20026;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#31867;&#20284;VGG&#30340;&#27169;&#22411;&#65292;&#38480;&#21046;&#20854;&#19981;&#20351;&#29992;&#22270;&#20687;&#30340;&#19979;&#21322;&#37096;&#20998;&#65292;&#20173;&#28982;&#35266;&#23519;&#21040;&#26410;&#35265;&#37096;&#20998;&#30340;&#27491;&#20998;&#25968;&#12290;&#36825;&#31181;&#34892;&#20026;&#22312;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#38169;&#35823;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01964v1 Announce Type: cross  Abstract: CAM-based methods are widely-used post-hoc interpretability method that produce a saliency map to explain the decision of an image classification model. The saliency map highlights the important areas of the image relevant to the prediction. In this paper, we show that most of these methods can incorrectly attribute an important score to parts of the image that the model cannot see. We show that this phenomenon occurs both theoretically and experimentally. On the theory side, we analyze the behavior of GradCAM on a simple masked CNN model at initialization. Experimentally, we train a VGG-like model constrained to not use the lower part of the image and nevertheless observe positive scores in the unseen part of the image. This behavior is evaluated quantitatively on two new datasets. We believe that this is problematic, potentially leading to mis-interpretation of the model's behavior.
&lt;/p&gt;</description></item><item><title>YNetr&#27169;&#22411;&#22312;Plain Scan Liver Tumors&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;62.63%&#30340;Dice&#31995;&#25968;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#32925;&#32959;&#30244;&#26222;&#36890;&#25195;&#25551;&#20998;&#21106;&#25968;&#25454;&#38598;&#21644;&#31639;&#27861;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2404.00327</link><description>&lt;p&gt;
YNetr&#65306;&#22312;Plain Scan Liver Tumors (PSLT)&#19978;&#30340;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
YNetr: Dual-Encoder architecture on Plain Scan Liver Tumors (PSLT)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00327
&lt;/p&gt;
&lt;p&gt;
YNetr&#27169;&#22411;&#22312;Plain Scan Liver Tumors&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;62.63%&#30340;Dice&#31995;&#25968;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#32925;&#32959;&#30244;&#26222;&#36890;&#25195;&#25551;&#20998;&#21106;&#25968;&#25454;&#38598;&#21644;&#31639;&#27861;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32925;&#32959;&#30244;&#26159;&#32925;&#33039;&#20013;&#19981;&#27491;&#24120;&#30340;&#29983;&#38271;&#65292;&#21487;&#33021;&#26159;&#33391;&#24615;&#25110;&#24694;&#24615;&#65292;&#32925;&#30284;&#26159;&#20840;&#29699;&#37325;&#35201;&#30340;&#20581;&#24247;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#29992;&#20110;&#32925;&#32959;&#30244;&#26222;&#36890;&#25195;&#25551;&#20998;&#21106;&#30340;&#25968;&#25454;&#38598;&#65292;&#20063;&#27809;&#26377;&#30456;&#20851;&#31639;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Plain Scan Liver Tumors(PSLT)&#21644;YNetr&#12290;&#20351;&#29992;40&#20010;&#32925;&#32959;&#30244;&#26222;&#36890;&#25195;&#25551;&#20998;&#21106;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32452;&#35013;&#21644;&#27880;&#37322;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21033;&#29992;Dice&#31995;&#25968;&#20316;&#20026;&#35780;&#20272;YNetr&#20135;&#29983;&#30340;&#20998;&#21106;&#32467;&#26524;&#30340;&#25351;&#26631;&#65292;&#26377;&#21033;&#20110;&#25429;&#33719;&#19981;&#21516;&#39057;&#29575;&#20449;&#24687;&#12290;YNetr&#27169;&#22411;&#22312;PSLT&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;62.63%&#30340;Dice&#31995;&#25968;&#65292;&#36229;&#36807;&#20854;&#20182;&#20844;&#24320;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#33539;&#22260;1.22%&#12290;&#36827;&#34892;&#20102;&#19982;&#21253;&#25324; UNet 3+&#12289;XNet&#12289;UNetr&#12289;Swin UNetr&#12289;Trans-BTS&#12289;COTr&#12289;nnUNetv2 (2D)&#12289;nnUNetv2 (3D fullres)&#12289;MedNext &#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#27169;&#22411;&#30340;&#27604;&#36739;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00327v1 Announce Type: cross  Abstract: Background: Liver tumors are abnormal growths in the liver that can be either benign or malignant, with liver cancer being a significant health concern worldwide. However, there is no dataset for plain scan segmentation of liver tumors, nor any related algorithms. To fill this gap, we propose Plain Scan Liver Tumors(PSLT) and YNetr. Methods: A collection of 40 liver tumor plain scan segmentation datasets was assembled and annotated. Concurrently, we utilized Dice coefficient as the metric for assessing the segmentation outcomes produced by YNetr, having advantage of capturing different frequency information. Results: The YNetr model achieved a Dice coefficient of 62.63% on the PSLT dataset, surpassing the other publicly available model by an accuracy margin of 1.22%. Comparative evaluations were conducted against a range of models including UNet 3+, XNet, UNetr, Swin UNetr, Trans-BTS, COTr, nnUNetv2 (2D), nnUNetv2 (3D fullres), MedNext
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#65292;&#32467;&#21512;&#28145;&#24230;&#20449;&#24687;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#36523;&#38382;&#31572;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.15941</link><description>&lt;p&gt;
&#25506;&#32034;&#30452;&#21040;&#33258;&#20449;: &#38754;&#21521;&#20855;&#36523;&#38382;&#31572;&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Explore until Confident: Efficient Exploration for Embodied Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15941
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#65292;&#32467;&#21512;&#28145;&#24230;&#20449;&#24687;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#36523;&#38382;&#31572;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#36523;&#38382;&#31572;&#65288;EQA&#65289;&#30340;&#38382;&#39064;&#65292;&#36825;&#25351;&#30340;&#26159;&#22312;&#38656;&#35201;&#20027;&#21160;&#25506;&#32034;&#29615;&#22659;&#20197;&#25910;&#38598;&#20449;&#24687;&#30452;&#21040;&#23545;&#38382;&#39064;&#30340;&#31572;&#26696;&#26377;&#33258;&#20449;&#30340;&#20855;&#36523;&#20195;&#29702;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#24378;&#22823;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#26469;&#39640;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;EQA&#20013;&#20351;&#29992;VLMs&#26102;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#23427;&#20204;&#27809;&#26377;&#20869;&#37096;&#35760;&#24518;&#23558;&#22330;&#26223;&#26144;&#23556;&#20197;&#20415;&#35268;&#21010;&#22914;&#20309;&#38543;&#26102;&#38388;&#25506;&#32034;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#32622;&#20449;&#24230;&#21487;&#33021;&#34987;&#38169;&#35823;&#26657;&#20934;&#24182;&#21487;&#33021;&#23548;&#33268;&#26426;&#22120;&#20154;&#36807;&#26089;&#20572;&#27490;&#25506;&#32034;&#25110;&#36807;&#24230;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#39318;&#20808;&#22522;&#20110;&#28145;&#24230;&#20449;&#24687;&#21644;&#36890;&#36807;&#35270;&#35273;&#25552;&#31034;VLM&#26469;&#26500;&#24314;&#22330;&#26223;&#30340;&#35821;&#20041;&#22320;&#22270;-&#21033;&#29992;&#20854;&#23545;&#22330;&#26223;&#30456;&#20851;&#21306;&#22495;&#30340;&#24191;&#27867;&#30693;&#35782;&#26469;&#36827;&#34892;&#25506;&#32034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#26469;&#26657;&#20934;VLM&#30340;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15941v1 Announce Type: cross  Abstract: We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM - leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM's 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#21307;&#23398;&#25968;&#25454;&#20013;&#30340;&#38590;&#20197;&#23519;&#35273;&#22122;&#22768;&#26469;&#20445;&#25252;&#25968;&#25454;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#35757;&#32451;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2403.10573</link><description>&lt;p&gt;
&#21307;&#23398;&#19981;&#21487;&#23398;&#20064;&#30340;&#31034;&#20363;&#65306;&#36890;&#36807;&#31232;&#30095;&#24863;&#30693;&#26412;&#22320;&#33945;&#29256;&#20445;&#25252;&#21307;&#23398;&#25968;&#25454;&#20813;&#21463;&#26410;&#32463;&#25480;&#26435;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Medical Unlearnable Examples: Securing Medical Data from Unauthorized Traning via Sparsity-Aware Local Masking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10573
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#21307;&#23398;&#25968;&#25454;&#20013;&#30340;&#38590;&#20197;&#23519;&#35273;&#22122;&#22768;&#26469;&#20445;&#25252;&#25968;&#25454;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#35757;&#32451;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#25935;&#24863;&#21307;&#23398;&#25968;&#25454;&#30340;&#29983;&#25104;&#21644;&#23384;&#20648;&#26174;&#33879;&#22686;&#21152;&#12290;&#36825;&#31181;&#25968;&#25454;&#30340;&#20016;&#23500;&#37327;&#25512;&#21160;&#20102;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#21033;&#29992;&#65292;&#20363;&#22914;&#29992;&#20110;&#35757;&#32451;&#21830;&#19994;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#24120;&#24120;&#20351;&#30740;&#31350;&#20154;&#21592;&#26395;&#32780;&#21364;&#27493;&#65292;&#22240;&#20026;&#20182;&#20204;&#19981;&#24895;&#20844;&#24320;&#20854;&#23453;&#36149;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20445;&#25252;&#36825;&#20123;&#38590;&#20197;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#40723;&#21169;&#21307;&#30103;&#26426;&#26500;&#20998;&#20139;&#25968;&#25454;&#65292;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21521;&#25968;&#25454;&#20013;&#24341;&#20837;&#38590;&#20197;&#23519;&#35273;&#30340;&#22122;&#22768;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#22312;&#27169;&#22411;&#27867;&#21270;&#20013;&#24341;&#20837;&#36864;&#21270;&#26469;&#20445;&#25252;&#25968;&#25454;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#35757;&#32451;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#22312;&#19968;&#33324;&#39046;&#22495;&#26174;&#31034;&#20986;&#20196;&#20154;&#38054;&#20329;&#30340;&#25968;&#25454;&#20445;&#25252;&#33021;&#21147;&#65292;&#20294;&#24403;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#20204;&#26410;&#33021;&#32771;&#34385;&#21040;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10573v1 Announce Type: cross  Abstract: With the rapid growth of artificial intelligence (AI) in healthcare, there has been a significant increase in the generation and storage of sensitive medical data. This abundance of data, in turn, has propelled the advancement of medical AI technologies. However, concerns about unauthorized data exploitation, such as training commercial AI models, often deter researchers from making their invaluable datasets publicly available. In response to the need to protect this hard-to-collect data while still encouraging medical institutions to share it, one promising solution is to introduce imperceptible noise into the data. This method aims to safeguard the data against unauthorized training by inducing degradation in model generalization. Although existing methods have shown commendable data protection capabilities in general domains, they tend to fall short when applied to biomedical data, mainly due to their failure to account for the spar
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#23454;&#29616;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#65292;&#22312;GPU&#20869;&#23384;&#21344;&#29992;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#28210;&#26579;&#36895;&#24230;&#65292;&#24182;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#30456;&#21305;&#25932;&#12290;</title><link>https://arxiv.org/abs/2403.08551</link><description>&lt;p&gt;
&#39640;&#26031;&#22270;&#20687;&#65306;&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#36827;&#34892;1000&#24103;&#27599;&#31186;&#30340;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08551
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#23454;&#29616;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#65292;&#22312;GPU&#20869;&#23384;&#21344;&#29992;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#28210;&#26579;&#36895;&#24230;&#65292;&#24182;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#30456;&#21305;&#25932;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#22312;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#25552;&#20379;&#20102;&#39640;&#35270;&#35273;&#36136;&#37327;&#21644;&#24555;&#36895;&#28210;&#26579;&#36895;&#24230;&#65292;&#27599;&#31186;10-1000&#24103;&#65292;&#20551;&#35774;&#26377;&#36275;&#22815;&#30340;GPU&#36164;&#28304;&#21487;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35201;&#27714;&#24120;&#24120;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#20869;&#23384;&#26377;&#38480;&#30340;&#20302;&#31471;&#35774;&#22791;&#19978;&#30340;&#20351;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#36827;&#34892;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#30340;&#24320;&#21019;&#24615;&#33539;&#24335;&#65292;&#21517;&#20026;GaussianImage&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;2D&#39640;&#26031;&#26469;&#34920;&#31034;&#22270;&#20687;&#65292;&#20854;&#20013;&#27599;&#20010;&#39640;&#26031;&#20855;&#26377;8&#20010;&#21442;&#25968;&#65292;&#21253;&#25324;&#20301;&#32622;&#12289;&#21327;&#26041;&#24046;&#21644;&#39068;&#33394;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#32047;&#31215;&#27714;&#21644;&#30340;&#26032;&#39062;&#28210;&#26579;&#31639;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;GPU&#20869;&#23384;&#33267;&#23569;&#38477;&#20302;3&#20493;&#65292;&#25311;&#21512;&#26102;&#38388;&#24555;5&#20493;&#65292;&#19981;&#20165;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#65288;&#20363;&#22914;WIRE&#65292;I-NGP&#65289;&#19981;&#30456;&#19978;&#19979;&#65292;&#32780;&#19988;&#26080;&#35770;&#21442;&#25968;&#22823;&#23567;&#22914;&#20309;&#37117;&#33021;&#25552;&#20379;1500-2000&#24103;&#27599;&#31186;&#30340;&#26356;&#24555;&#28210;&#26579;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08551v1 Announce Type: cross  Abstract: Implicit neural representations (INRs) recently achieved great success in image representation and compression, offering high visual quality and fast rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available. However, this requirement often hinders their use on low-end devices with limited memory. In response, we propose a groundbreaking paradigm of image representation and compression by 2D Gaussian Splatting, named GaussianImage. We first introduce 2D Gaussian to represent the image, where each Gaussian has 8 parameters including position, covariance and color. Subsequently, we unveil a novel rendering algorithm based on accumulated summation. Remarkably, our method with a minimum of 3$\times$ lower GPU memory usage and 5$\times$ faster fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation performance, but also delivers a faster rendering speed of 1500-2000 FPS regardless of parameter size. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#22312;&#27169;&#22359;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#30340;&#36890;&#29992;&#27169;&#22359;&#21040;&#27169;&#22359;&#30693;&#35782;&#33976;&#39311;&#65288;m2mKD&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#27169;&#22359;&#21270;Transformer&#35757;&#32451;&#20013;&#30340;&#20248;&#21270;&#22256;&#38590;&#21644;&#21442;&#25968;&#25968;&#37327;&#24222;&#22823;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16918</link><description>&lt;p&gt;
m2mKD&#65306;&#27169;&#22359;&#38388;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#27169;&#22359;&#21270;Transformer
&lt;/p&gt;
&lt;p&gt;
m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16918
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#22312;&#27169;&#22359;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#30340;&#36890;&#29992;&#27169;&#22359;&#21040;&#27169;&#22359;&#30693;&#35782;&#33976;&#39311;&#65288;m2mKD&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#27169;&#22359;&#21270;Transformer&#35757;&#32451;&#20013;&#30340;&#20248;&#21270;&#22256;&#38590;&#21644;&#21442;&#25968;&#25968;&#37327;&#24222;&#22823;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22359;&#21270;&#31070;&#32463;&#32467;&#26500;&#22240;&#20854;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23545;&#26032;&#39046;&#22495;&#30340;&#39640;&#25928;&#36866;&#24212;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#27169;&#22359;&#21270;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#26089;&#26399;&#38454;&#27573;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#31232;&#30095;&#36830;&#25509;&#23548;&#33268;&#30340;&#20248;&#21270;&#22256;&#38590;&#65292;&#23384;&#22312;&#25361;&#25112;&#12290;&#21033;&#29992;&#26469;&#33258;&#25972;&#20307;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#22914;&#30693;&#35782;&#33976;&#39311;&#31561;&#25216;&#26415;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#35757;&#32451;&#27169;&#22359;&#21270;&#27169;&#22411;&#65292;&#24182;&#20351;&#23427;&#20204;&#33021;&#22815;&#25972;&#21512;&#26469;&#33258;&#22312;&#22810;&#20010;&#26469;&#28304;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#24182;&#19981;&#38024;&#23545;&#27169;&#22359;&#21270;&#27169;&#22411;&#35774;&#35745;&#65292;&#30452;&#25509;&#24212;&#29992;&#26102;&#21487;&#33021;&#22833;&#36133;&#65292;&#36825;&#26159;&#30001;&#20110;&#29420;&#29305;&#30340;&#26550;&#26500;&#21644;&#22823;&#37327;&#28041;&#21450;&#30340;&#21442;&#25968;&#12290;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#27169;&#22359;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#30340;&#36890;&#29992;&#27169;&#22359;&#21040;&#27169;&#22359;&#30693;&#35782;&#33976;&#39311;&#65288;m2mKD&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16918v1 Announce Type: new  Abstract: Modular neural architectures are gaining increasing attention due to their powerful capability for generalization and sample-efficient adaptation to new domains. However, training modular models, particularly in the early stages, poses challenges due to the optimization difficulties arising from their intrinsic sparse connectivity. Leveraging the knowledge from monolithic models, using techniques such as knowledge distillation, is likely to facilitate the training of modular models and enable them to integrate knowledge from multiple models pretrained on diverse sources. Nevertheless, conventional knowledge distillation approaches are not tailored to modular models and can fail when directly applied due to the unique architectures and the enormous number of parameters involved. Motivated by these challenges, we propose a general module-to-module knowledge distillation (m2mKD) method for transferring knowledge between modules. Our approac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#36890;&#36807;&#26500;&#24314;&#27602;&#32032;-&#28165;&#27905;&#25552;&#31034;&#23545;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807; Proximal Policy Optimization &#35757;&#32451;&#20248;&#21270;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#21508;&#31181; T2I &#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10882</link><description>&lt;p&gt;
&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#29992;&#20110;&#23433;&#20840;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Universal Prompt Optimizer for Safe Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10882
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#36890;&#36807;&#26500;&#24314;&#27602;&#32032;-&#28165;&#27905;&#25552;&#31034;&#23545;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807; Proximal Policy Optimization &#35757;&#32451;&#20248;&#21270;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#21508;&#31181; T2I &#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#22312;&#26681;&#25454;&#25991;&#23383;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#19981;&#23433;&#20840;&#36755;&#20837;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#65292;&#22914;&#33394;&#24773;&#12289;&#39578;&#25200;&#21644;&#38750;&#27861;&#27963;&#21160;&#22270;&#20687;&#12290;&#22522;&#20110;&#22270;&#20687;&#26816;&#26597;&#22120;&#12289;&#27169;&#22411;&#24494;&#35843;&#21644;&#23884;&#20837;&#24335;&#38459;&#27490;&#30340;&#29616;&#26377;&#30740;&#31350;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840; T2I &#29983;&#25104;&#30340;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10882v1 Announce Type: cross  Abstract: Text-to-Image (T2I) models have shown great performance in generating images based on textual prompts. However, these models are vulnerable to unsafe input to generate unsafe content like sexual, harassment and illegal-activity images. Existing studies based on image checker, model fine-tuning and embedding blocking are impractical in real-world applications. Hence, \textit{we propose the first universal prompt optimizer for safe T2I generation in black-box scenario}. We first construct a dataset consisting of toxic-clean prompt pairs by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting toxic prompt to clean prompt while preserving semantic information, we design a novel reward function measuring toxicity and text alignment of generated images and train the optimizer through Proximal Policy Optimization. Experiments show that our approach can effectively reduce the likelihood of various T2I models in generating in
&lt;/p&gt;</description></item><item><title>Pix2Code &#26159;&#19968;&#20010;&#23558;&#31070;&#32463;&#35270;&#35273;&#27010;&#24565;&#32452;&#21512;&#25104;&#31243;&#24207;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26174;&#24335;&#12289;&#32452;&#21512;&#30340;&#31526;&#21495;&#21644;&#38544;&#24335;&#30340;&#31070;&#32463;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#22270;&#20687;&#20013;&#26816;&#32034;&#23545;&#35937;&#34920;&#31034;&#24182;&#23558;&#20851;&#31995;&#27010;&#24565;&#21512;&#25104;&#20026;lambda&#28436;&#31639;&#31243;&#24207;&#65292;&#26469;&#35299;&#20915;&#36890;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;&#22312;&#25512;&#29702;&#39046;&#22495;Kandinsky Patterns&#21644;CURI&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Pix2Code &#33021;&#22815;&#35782;&#21035;&#32452;&#21512;&#35270;&#35273;&#27010;&#24565;&#24182;&#25512;&#24191;&#21040;&#26032;&#25968;&#25454;&#21644;&#25512;&#29702;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.08280</link><description>&lt;p&gt;
Pix2Code&#65306;&#23398;&#20064;&#23558;&#31070;&#32463;&#35270;&#35273;&#27010;&#24565;&#32452;&#21512;&#25104;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Pix2Code: Learning to Compose Neural Visual Concepts as Programs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08280
&lt;/p&gt;
&lt;p&gt;
Pix2Code &#26159;&#19968;&#20010;&#23558;&#31070;&#32463;&#35270;&#35273;&#27010;&#24565;&#32452;&#21512;&#25104;&#31243;&#24207;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26174;&#24335;&#12289;&#32452;&#21512;&#30340;&#31526;&#21495;&#21644;&#38544;&#24335;&#30340;&#31070;&#32463;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#22270;&#20687;&#20013;&#26816;&#32034;&#23545;&#35937;&#34920;&#31034;&#24182;&#23558;&#20851;&#31995;&#27010;&#24565;&#21512;&#25104;&#20026;lambda&#28436;&#31639;&#31243;&#24207;&#65292;&#26469;&#35299;&#20915;&#36890;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;&#22312;&#25512;&#29702;&#39046;&#22495;Kandinsky Patterns&#21644;CURI&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Pix2Code &#33021;&#22815;&#35782;&#21035;&#32452;&#21512;&#35270;&#35273;&#27010;&#24565;&#24182;&#25512;&#24191;&#21040;&#26032;&#25968;&#25454;&#21644;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#20174;&#22270;&#20687;&#20013;&#25277;&#35937;&#27010;&#24565;&#30340;&#25361;&#25112;&#22312;&#20110;&#38656;&#35201;&#23558;&#35270;&#35273;&#24863;&#30693;&#21644;&#36890;&#29992;&#20851;&#31995;&#25512;&#29702;&#36827;&#34892;&#25972;&#21512;&#12290;&#27492;&#22806;&#65292;&#35813;&#20219;&#21153;&#30340;&#26080;&#30417;&#30563;&#24615;&#36136;&#20351;&#24471;&#20154;&#31867;&#29992;&#25143;&#38656;&#35201;&#33021;&#22815;&#29702;&#35299;&#27169;&#22411;&#23398;&#21040;&#30340;&#27010;&#24565;&#65292;&#24182;&#21487;&#33021;&#20462;&#27491;&#38169;&#35823;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#35270;&#35273;&#27010;&#24565;&#23398;&#20064;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Pix2Code&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#31243;&#24207;&#21512;&#25104;&#25193;&#23637;&#21040;&#35270;&#35273;&#20851;&#31995;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#26126;&#30830;&#30340;&#12289;&#32452;&#21512;&#30340;&#31526;&#21495;&#21644;&#38544;&#24335;&#30340;&#31070;&#32463;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20174;&#22270;&#20687;&#20013;&#26816;&#32034;&#23545;&#35937;&#34920;&#31034;&#24182;&#23558;&#20851;&#31995;&#27010;&#24565;&#21512;&#25104;&#20026;lambda&#28436;&#31639;&#31243;&#24207;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#39046;&#22495;Kandinsky Patterns&#21644;CURI&#19978;&#35780;&#20272;&#20102;Pix2Code&#30340;&#22810;&#26679;&#29305;&#24615;&#65292;&#20174;&#32780;&#27979;&#35797;&#20854;&#35782;&#21035;&#32452;&#21512;&#35270;&#35273;&#27010;&#24565;&#24182;&#25512;&#24191;&#21040;&#26032;&#25968;&#25454;&#21644;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge in learning abstract concepts from images in an unsupervised fashion lies in the required integration of visual perception and generalizable relational reasoning. Moreover, the unsupervised nature of this task makes it necessary for human users to be able to understand a model's learnt concepts and potentially revise false behaviours. To tackle both the generalizability and interpretability constraints of visual concept learning, we propose Pix2Code, a framework that extends program synthesis to visual relational reasoning by utilizing the abilities of both explicit, compositional symbolic and implicit neural representations. This is achieved by retrieving object representations from images and synthesizing relational concepts as lambda-calculus programs. We evaluate the diverse properties of Pix2Code on the challenging reasoning domains, Kandinsky Patterns and CURI, thereby testing its ability to identify compositional visual concepts that generalize to novel data and co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;GPT-4V&#36827;&#34892;&#21487;&#35299;&#37322;&#39118;&#38505;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#37322;&#22797;&#26434;&#30340;&#36807;&#39532;&#36335;&#22330;&#26223;&#65292;&#20026;&#30450;&#20154;&#21644;&#35270;&#21147;&#20302;&#19979;&#20154;&#22763;&#30340;&#23433;&#20840;&#20915;&#31574;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.06794</link><description>&lt;p&gt;
&#26159;&#21542;&#23433;&#20840;&#36807;&#39532;&#36335;&#65311;GPT-4V&#29992;&#20110;&#23433;&#20840;&#24847;&#35782;&#30340;&#21487;&#35299;&#37322;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is it safe to cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;GPT-4V&#36827;&#34892;&#21487;&#35299;&#37322;&#39118;&#38505;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#37322;&#22797;&#26434;&#30340;&#36807;&#39532;&#36335;&#22330;&#26223;&#65292;&#20026;&#30450;&#20154;&#21644;&#35270;&#21147;&#20302;&#19979;&#20154;&#22763;&#30340;&#23433;&#20840;&#20915;&#31574;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30450;&#20154;&#21644;&#35270;&#21147;&#20302;&#19979;&#30340;&#20154;&#26469;&#35828;&#65292;&#23433;&#20840;&#22320;&#36890;&#36807;&#34903;&#36947;&#20132;&#21449;&#21475;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#23545;&#21608;&#22260;&#29615;&#22659;&#26377;&#32454;&#33268;&#30340;&#29702;&#35299;&#65292;&#32780;&#36825;&#20010;&#20219;&#21153;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#35270;&#35273;&#32447;&#32034;&#12290;&#20256;&#32479;&#30340;&#36741;&#21161;&#20915;&#31574;&#26041;&#27861;&#24448;&#24448;&#19981;&#22815;&#23436;&#21892;&#65292;&#26080;&#27861;&#25552;&#20379;&#20840;&#38754;&#30340;&#22330;&#26223;&#20998;&#26512;&#21644;&#23433;&#20840;&#32423;&#21035;&#21028;&#26029;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#26469;&#35299;&#37322;&#22797;&#26434;&#30340;&#36807;&#39532;&#36335;&#22330;&#26223;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#20132;&#36890;&#20449;&#21495;&#35782;&#21035;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#36827;&#27493;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#23433;&#20840;&#35780;&#20998;&#21644;&#33258;&#28982;&#35821;&#35328;&#22330;&#26223;&#25551;&#36848;&#65292;&#25903;&#25345;&#30450;&#20154;&#21644;&#35270;&#21147;&#20302;&#19979;&#20154;&#22763;&#23433;&#20840;&#20915;&#31574;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#30001;&#22235;&#36275;&#26426;&#22120;&#20154;&#25429;&#33719;&#30340;&#22810;&#35270;&#35282;&#33258;&#25105;&#20013;&#24515;&#22270;&#20687;&#26500;&#25104;&#30340;&#36807;&#39532;&#36335;&#20132;&#21449;&#21475;&#25968;&#25454;&#65292;&#24182;&#26681;&#25454;&#39044;&#20808;&#23450;&#20041;&#30340;&#23433;&#20840;&#35780;&#20998;&#20998;&#31867;&#36827;&#34892;&#20102;&#22270;&#20687;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safely navigating street intersections is a complex challenge for blind and low-vision individuals, as it requires a nuanced understanding of the surrounding context - a task heavily reliant on visual cues. Traditional methods for assisting in this decision-making process often fall short, lacking the ability to provide a comprehensive scene analysis and safety level. This paper introduces an innovative approach that leverages large multimodal models (LMMs) to interpret complex street crossing scenes, offering a potential advancement over conventional traffic signal recognition techniques. By generating a safety score and scene description in natural language, our method supports safe decision-making for the blind and low-vision individuals. We collected crosswalk intersection data that contains multiview egocentric images captured by a quadruped robot and annotated the images with corresponding safety scores based on our predefined safety score categorization. Grounded on the visual k
&lt;/p&gt;</description></item><item><title>V-IRL&#26159;&#19968;&#20010;&#24179;&#21488;&#65292;&#21487;&#20197;&#35753;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#19982;&#29616;&#23454;&#19990;&#30028;&#36827;&#34892;&#20114;&#21160;&#65292;&#26088;&#22312;&#23558;&#25968;&#23383;&#21644;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#65292;&#24182;&#24320;&#21457;&#20986;&#20855;&#26377;&#20016;&#23500;&#24863;&#30693;&#12289;&#20915;&#31574;&#21644;&#19982;&#30495;&#23454;&#25968;&#25454;&#20114;&#21160;&#33021;&#21147;&#30340;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.03310</link><description>&lt;p&gt;
V-IRL: &#23558;&#34394;&#25311;&#26234;&#33021;&#19982;&#29616;&#23454;&#29983;&#27963;&#32852;&#31995;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
V-IRL: Grounding Virtual Intelligence in Real Life
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03310
&lt;/p&gt;
&lt;p&gt;
V-IRL&#26159;&#19968;&#20010;&#24179;&#21488;&#65292;&#21487;&#20197;&#35753;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#19982;&#29616;&#23454;&#19990;&#30028;&#36827;&#34892;&#20114;&#21160;&#65292;&#26088;&#22312;&#23558;&#25968;&#23383;&#21644;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#65292;&#24182;&#24320;&#21457;&#20986;&#20855;&#26377;&#20016;&#23500;&#24863;&#30693;&#12289;&#20915;&#31574;&#21644;&#19982;&#30495;&#23454;&#25968;&#25454;&#20114;&#21160;&#33021;&#21147;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#29983;&#27963;&#22312;&#22320;&#29699;&#19978;&#65292;&#32780;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#25152;&#21019;&#36896;&#30340;&#25968;&#23383;&#39046;&#22495;&#20043;&#38388;&#23384;&#22312;&#30528;&#24863;&#23448;&#24046;&#36317;&#12290;&#20026;&#20102;&#24320;&#21457;&#20986;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#28789;&#27963;&#24863;&#30693;&#12289;&#24605;&#32771;&#21644;&#34892;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#24517;&#39035;&#24357;&#21512;&#25968;&#23383;&#21644;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#30340;&#36924;&#30495;&#24046;&#36317;&#12290;&#25105;&#20204;&#22914;&#20309;&#22312;&#19968;&#20010;&#20687;&#25105;&#20204;&#25152;&#23621;&#20303;&#30340;&#19990;&#30028;&#20013;&#19968;&#26679;&#20016;&#23500;&#22810;&#26679;&#30340;&#29615;&#22659;&#20013;&#20307;&#29616;&#20195;&#29702;&#65292;&#32780;&#19981;&#21463;&#30495;&#23454;&#30828;&#20214;&#21644;&#25511;&#21046;&#25152;&#26045;&#21152;&#30340;&#32422;&#26463;&#65311;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;V-IRL: &#19968;&#31181;&#24179;&#21488;&#65292;&#21487;&#20197;&#20351;&#20195;&#29702;&#22312;&#34394;&#25311;&#32780;&#36924;&#30495;&#30340;&#29615;&#22659;&#20013;&#19982;&#29616;&#23454;&#19990;&#30028;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#24179;&#21488;&#26082;&#26159;&#19968;&#20010;&#24320;&#21457;&#20195;&#29702;&#23436;&#25104;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#30340;&#28216;&#20048;&#22330;&#65292;&#21448;&#26159;&#19968;&#20010;&#24191;&#38420;&#30340;&#27979;&#35797;&#22522;&#22320;&#65292;&#29992;&#20110;&#34913;&#37327;&#22312;&#24863;&#30693;&#12289;&#20915;&#31574;&#21644;&#19982;&#20840;&#29699;&#30495;&#23454;&#25968;&#25454;&#30340;&#20114;&#21160;&#33021;&#21147;&#31561;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created. To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds. How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control? Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment. Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe.
&lt;/p&gt;</description></item><item><title>&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#26082;&#33021;&#25552;&#21319;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21644;&#26657;&#20934;&#24615;&#65292;&#21448;&#21487;&#33021;&#25104;&#20026;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#30340;&#22240;&#32032;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#32467;&#21512;&#36127;&#22240;&#23376;&#36827;&#34892;&#24179;&#28369;&#21487;&#26377;&#25928;&#38459;&#27490;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#65292;&#25552;&#21319;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2310.06549</link><description>&lt;p&gt;
&#35880;&#24910;&#24179;&#28369;&#26631;&#31614;&#65306;&#26631;&#31614;&#24179;&#28369;&#26082;&#21487;&#20197;&#20316;&#20026;&#38544;&#31169;&#23631;&#38556;&#65292;&#21448;&#21487;&#20197;&#25104;&#20026;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#30340;&#20652;&#21270;&#21058;
&lt;/p&gt;
&lt;p&gt;
Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06549
&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#26082;&#33021;&#25552;&#21319;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21644;&#26657;&#20934;&#24615;&#65292;&#21448;&#21487;&#33021;&#25104;&#20026;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#30340;&#22240;&#32032;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#32467;&#21512;&#36127;&#22240;&#23376;&#36827;&#34892;&#24179;&#28369;&#21487;&#26377;&#25928;&#38459;&#27490;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#65292;&#25552;&#21319;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#8212;&#8212;&#20351;&#29992;&#36719;&#21270;&#30340;&#26631;&#31614;&#32780;&#19981;&#26159;&#30828;&#26631;&#31614;&#8212;&#8212;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#22686;&#24378;&#27867;&#21270;&#21644;&#26657;&#20934;&#31561;&#22810;&#26679;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#23427;&#23545;&#20110;&#20445;&#25252;&#27169;&#22411;&#38544;&#31169;&#30340;&#24433;&#21709;&#20173;&#28982;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26631;&#31614;&#24179;&#28369;&#23545;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#65288;MIAs&#65289;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20998;&#31867;&#22120;&#20013;&#32534;&#30721;&#30340;&#30693;&#35782;&#29983;&#25104;&#20855;&#26377;&#31867;&#20195;&#34920;&#24615;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25512;&#26029;&#26377;&#20851;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#26631;&#31614;&#24179;&#28369;&#20419;&#36827;&#20102;MIAs&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#26356;&#29978;&#32773;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29992;&#36127;&#22240;&#23376;&#36827;&#34892;&#24179;&#28369;&#21487;&#20197;&#25269;&#21046;&#36825;&#19968;&#36235;&#21183;&#65292;&#38459;&#30861;&#25552;&#21462;&#19982;&#31867;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#65292;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#36825;&#30830;&#31435;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#24378;&#22823;&#30340;&#26032;&#30340;&#22686;&#24378;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06549v2 Announce Type: replace  Abstract: Label smoothing -- using softened labels instead of hard ones -- is a widely adopted regularization method for deep learning, showing diverse benefits such as enhanced generalization and calibration. Its implications for preserving model privacy, however, have remained unexplored. To fill this gap, we investigate the impact of label smoothing on model inversion attacks (MIAs), which aim to generate class-representative samples by exploiting the knowledge encoded in a classifier, thereby inferring sensitive information about its training data. Through extensive analyses, we uncover that traditional label smoothing fosters MIAs, thereby increasing a model's privacy leakage. Even more, we reveal that smoothing with negative factors counters this trend, impeding the extraction of class-related information and leading to privacy preservation, beating state-of-the-art defenses. This establishes a practical and powerful novel way for enhanc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;AI&#30340;&#19977;&#32500;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#20013;&#35782;&#21035;&#22823;&#33041;&#27807;&#29305;&#24449;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2309.00903</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#19977;&#32500;&#26694;&#26550;&#25581;&#31034;&#23398;&#20064;&#27169;&#24335;&#65306;&#21464;&#37327;&#33041;&#27807;&#35782;&#21035;&#30340;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
An explainable three dimension framework to uncover learning patterns: A unified look in variable sulci recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.00903
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;AI&#30340;&#19977;&#32500;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#20013;&#35782;&#21035;&#22823;&#33041;&#27807;&#29305;&#24449;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#25361;&#25112;&#24615;&#30340;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#37324;&#65292;&#35270;&#35273;&#20027;&#39064;&#22312;&#19977;&#32500;&#31354;&#38388;&#20869;&#34920;&#29616;&#20986;&#39640;&#24230;&#22797;&#26434;&#24615;&#12290;&#31070;&#32463;&#31185;&#23398;&#30340;&#24212;&#29992;&#28041;&#21450;&#20174;MRI&#20013;&#35782;&#21035;&#22823;&#33041;&#27807;&#29305;&#24449;&#65292;&#30001;&#20110;&#19987;&#23478;&#20043;&#38388;&#30340;&#26631;&#27880;&#35268;&#31243;&#23384;&#22312;&#24046;&#24322;&#21644;&#22823;&#33041;&#22797;&#26434;&#30340;&#19977;&#32500;&#21151;&#33021;&#65292;&#25105;&#20204;&#38754;&#20020;&#30528;&#37325;&#22823;&#38556;&#30861;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#22312;&#26377;&#25928;&#39564;&#35777;&#21644;&#35780;&#20272;&#36825;&#20123;&#32593;&#32476;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#25968;&#23398;&#20844;&#24335;&#65292;&#32454;&#21270;&#20102;&#19981;&#21516;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#35299;&#37322;&#38656;&#27714;&#30340;&#21508;&#31181;&#31867;&#21035;&#65292;&#20998;&#20026;&#33258;&#35299;&#37322;&#12289;&#21322;&#35299;&#37322;&#12289;&#38750;&#35299;&#37322;&#21644;&#22522;&#20110;&#39564;&#35777;&#21327;&#35758;&#21487;&#38752;&#24615;&#30340;&#26032;&#27169;&#24335;&#23398;&#20064;&#24212;&#29992;&#12290;&#26681;&#25454;&#36825;&#20010;&#25968;&#23398;&#20844;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#37322;&#19977;&#32500;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.00903v2 Announce Type: replace-cross  Abstract: Explainable AI is crucial in medical imaging. In the challenging field of neuroscience, visual topics present a high level of complexity, particularly within three-dimensional space. The application of neuroscience, which involves identifying brain sulcal features from MRI, faces significant hurdles due to varying annotation protocols among experts and the intricate three-dimension functionality of the brain. Consequently, traditional explainability approaches fall short in effectively validating and evaluating these networks. To address this, we first present a mathematical formulation delineating various categories of explanation needs across diverse computer vision tasks, categorized into self-explanatory, semi-explanatory, non-explanatory, and new-pattern learning applications based on the reliability of the validation protocol. With respect to this mathematical formulation, we propose a 3D explainability framework aimed at
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#25351;&#20196;&#26469;&#25351;&#23548;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#24674;&#22797;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20026;&#22522;&#20110;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.16468</link><description>&lt;p&gt;
&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
High-Quality Image Restoration Following Human Instructions. (arXiv:2401.16468v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#25351;&#20196;&#26469;&#25351;&#23548;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#24674;&#22797;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20026;&#22522;&#20110;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24674;&#22797;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#28041;&#21450;&#20174;&#36864;&#21270;&#35266;&#27979;&#20013;&#24674;&#22797;&#20986;&#39640;&#36136;&#37327;&#30340;&#24178;&#20928;&#22270;&#20687;&#12290;&#20840;&#33021;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36864;&#21270;&#31867;&#22411;&#30340;&#20449;&#24687;&#20316;&#20026;&#25552;&#31034;&#26469;&#26377;&#25928;&#22320;&#24674;&#22797;&#21508;&#31181;&#31867;&#22411;&#21644;&#32423;&#21035;&#30340;&#36864;&#21270;&#22270;&#20687;&#65292;&#24182;&#24341;&#23548;&#24674;&#22797;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#25351;&#20196;&#26469;&#25351;&#23548;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20174;&#36864;&#21270;&#22270;&#20687;&#20013;&#24674;&#22797;&#20986;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#32771;&#34385;&#22810;&#31181;&#36864;&#21270;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;InstructIR&#22312;&#22270;&#20687;&#21435;&#22122;&#12289;&#38632;&#27700;&#21435;&#38500;&#12289;&#21435;&#27169;&#31946;&#12289;&#21435;&#38654;&#21644;(&#20302;&#20809;)&#22270;&#20687;&#22686;&#24378;&#31561;&#22810;&#20010;&#24674;&#22797;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;InstructIR&#22312;&#20043;&#21069;&#30340;&#20840;&#33021;&#24674;&#22797;&#26041;&#27861;&#19978;&#25552;&#39640;&#20102;1dB&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#32467;&#26524;&#20026;&#22522;&#20110;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30340;&#26032;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20195;&#30721;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image restoration is a fundamental problem that involves recovering a high-quality clean image from its degraded observation. All-In-One image restoration models can effectively restore images from various types and levels of degradation using degradation-specific information as prompts to guide the restoration model. In this work, we present the first approach that uses human-written instructions to guide the image restoration model. Given natural language prompts, our model can recover high-quality images from their degraded counterparts, considering multiple degradation types. Our method, InstructIR, achieves state-of-the-art results on several restoration tasks including image denoising, deraining, deblurring, dehazing, and (low-light) image enhancement. InstructIR improves +1dB over previous all-in-one restoration methods. Moreover, our dataset and results represent a novel benchmark for new research on text-guided image restoration and enhancement. Our code, datasets and models a
&lt;/p&gt;</description></item><item><title>FUTURE-AI&#26159;&#31532;&#19968;&#20010;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2309.12325</link><description>&lt;p&gt;
FUTURE-AI&#65306;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;&#21644;&#21487;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;&#30340;&#22269;&#38469;&#20849;&#35782;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare. (arXiv:2309.12325v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12325
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI&#26159;&#31532;&#19968;&#20010;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;AI&#25216;&#26415;&#22312;&#29616;&#23454;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#37096;&#32626;&#21644;&#37319;&#29992;&#20173;&#21463;&#38480;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#21307;&#30103;AI&#30340;&#25216;&#26415;&#12289;&#20020;&#24202;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#39118;&#38505;&#25552;&#20986;&#20102;&#20851;&#27880;&#12290;&#20026;&#20102;&#22686;&#21152;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37319;&#29992;&#65292;&#21307;&#30103;AI&#24037;&#20855;&#24517;&#39035;&#24471;&#21040;&#24739;&#32773;&#12289;&#20020;&#24202;&#21307;&#29983;&#12289;&#20581;&#24247;&#32452;&#32455;&#21644;&#24403;&#23616;&#30340;&#20449;&#20219;&#21644;&#25509;&#21463;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;FUTURE-AI&#25351;&#21335;&#20316;&#20026;&#31532;&#19968;&#20010;&#29992;&#20110;&#25351;&#23548;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#12290;FUTURE-AI&#32852;&#30431;&#25104;&#31435;&#20110;2021&#24180;&#65292;&#30446;&#21069;&#21253;&#25324;&#26469;&#33258;51&#20010;&#22269;&#23478;&#30340;118&#20301;&#36328;&#23398;&#31185;&#19987;&#23478;&#65292;&#20195;&#34920;&#20102;&#25152;&#26377;&#22823;&#27954;&#65292;&#21253;&#25324;AI&#31185;&#23398;&#23478;&#12289;&#20020;&#24202;&#21307;&#29983;&#12289;&#20262;&#29702;&#23398;&#23478;&#21644;&#31038;&#20250;&#31185;&#23398;&#23478;&#12290;&#22312;&#20026;&#26399;&#20004;&#24180;&#30340;&#26102;&#38388;&#37324;&#65292;&#32852;&#30431;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#23450;&#20041;&#20102;&#21487;&#20449;AI&#30340;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20854;&#20013;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
Despite major advances in artificial intelligence (AI) for medicine and healthcare, the deployment and adoption of AI technologies remain limited in real-world clinical practice. In recent years, concerns have been raised about the technical, clinical, ethical and legal risks associated with medical AI. To increase real world adoption, it is essential that medical AI tools are trusted and accepted by patients, clinicians, health organisations and authorities. This work describes the FUTURE-AI guideline as the first international consensus framework for guiding the development and deployment of trustworthy AI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and currently comprises 118 inter-disciplinary experts from 51 countries representing all continents, including AI scientists, clinicians, ethicists, and social scientists. Over a two-year period, the consortium defined guiding principles and best practices for trustworthy AI through an iterative process comprising a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SAMUS&#65292;&#19968;&#20010;&#19987;&#20026;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;CNN&#20998;&#25903;&#21644;&#36866;&#37197;&#22120;&#26469;&#25913;&#21892;SAM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06824</link><description>&lt;p&gt;
SAMUS&#65306;&#20026;&#20020;&#24202;&#21451;&#22909;&#21644;&#27867;&#21270;&#24615;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#35843;&#25972;&#30340;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SAMUS: Adapting Segment Anything Model for Clinically-Friendly and Generalizable Ultrasound Image Segmentation. (arXiv:2309.06824v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SAMUS&#65292;&#19968;&#20010;&#19987;&#20026;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;CNN&#20998;&#25903;&#21644;&#36866;&#37197;&#22120;&#26469;&#25913;&#21892;SAM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#65288;SAM&#65289;&#26159;&#19968;&#31181;&#21331;&#36234;&#30340;&#36890;&#29992;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;SAM&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22788;&#29702;&#21307;&#23398;&#22270;&#20687;&#26102;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#20302;&#23545;&#27604;&#24230;&#12289;&#27169;&#31946;&#36793;&#30028;&#12289;&#22797;&#26434;&#24418;&#29366;&#21644;&#23567;&#23610;&#23544;&#23545;&#35937;&#30340;&#22270;&#20687;&#26102;&#65292;SAM&#38754;&#20020;&#30528;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#21644;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;SAMUS&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#19982;&#20197;&#21069;&#22522;&#20110;SAM&#30340;&#36890;&#29992;&#27169;&#22411;&#19981;&#21516;&#65292;SAMUS&#36861;&#27714;&#30340;&#19981;&#20165;&#26159;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36824;&#26377;&#26356;&#20302;&#30340;&#37096;&#32626;&#25104;&#26412;&#65292;&#20351;&#20854;&#26356;&#36866;&#21512;&#20020;&#24202;&#24212;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;SAM&#30340;&#22522;&#30784;&#19978;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#24182;&#34892;CNN&#20998;&#25903;&#65292;&#36890;&#36807;&#36328;&#20998;&#25903;&#27880;&#24847;&#21147;&#23558;&#23616;&#37096;&#29305;&#24449;&#27880;&#20837;ViT&#32534;&#30721;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#28982;&#21518;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20301;&#32622;&#36866;&#37197;&#22120;&#21644;&#19968;&#20010;&#29305;&#24449;&#36866;&#37197;&#22120;&#26469;&#35843;&#25972;SAM&#30340;&#36755;
&lt;/p&gt;
&lt;p&gt;
Segment anything model (SAM), an eminent universal image segmentation model, has recently gathered considerable attention within the domain of medical image segmentation. Despite the remarkable performance of SAM on natural images, it grapples with significant performance degradation and limited generalization when confronted with medical images, particularly with those involving objects of low contrast, faint boundaries, intricate shapes, and diminutive sizes. In this paper, we propose SAMUS, a universal model tailored for ultrasound image segmentation. In contrast to previous SAM-based universal models, SAMUS pursues not only better generalization but also lower deployment cost, rendering it more suitable for clinical applications. Specifically, based on SAM, a parallel CNN branch is introduced to inject local features into the ViT encoder through cross-branch attention for better medical image segmentation. Then, a position adapter and a feature adapter are developed to adapt SAM fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.10246</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#65306;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#65288;&#32508;&#36848;&#65289;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey). (arXiv:2307.10246v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#22914;&#20309;&#34920;&#31034;&#19981;&#21516;&#30340;&#20449;&#24687;&#27169;&#24335;&#65311;&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#20986;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#29702;&#35299;&#29992;&#25143;&#24605;&#32771;&#20869;&#23481;&#30340;&#31995;&#32479;&#65311;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#30740;&#31350;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#31561;&#22823;&#33041;&#35760;&#24405;&#26469;&#22238;&#31572;&#12290;&#20316;&#20026;&#31532;&#19968;&#27493;&#65292;&#31070;&#32463;&#31185;&#23398;&#30028;&#20026;&#34987;&#21160;&#38405;&#35835;/&#21548;&#35273;/&#35266;&#30475;&#27010;&#24565;&#35789;&#27719;&#12289;&#21465;&#36848;&#12289;&#22270;&#29255;&#21644;&#30005;&#24433;&#30456;&#20851;&#30340;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#20013;&#65292;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#30740;&#31350;&#20013;&#30340;&#39069;&#22806;&#24037;&#20855;&#65292;&#22312;&#35748;&#30693;&#31185;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#32534;&#30721;&#27169;&#22411;&#26088;&#22312;&#33258;&#21160;&#22320;&#29983;&#25104;fMRI&#22823;&#33041;&#34920;&#24449;&#65292;&#32473;&#23450;&#19968;&#20010;&#21050;&#28608;&#12290;&#23427;&#20204;&#22312;&#35780;&#20272;&#21644;&#35786;&#26029;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20197;&#21450;&#35774;&#35745;&#22823;&#33041;&#25439;&#20260;&#27835;&#30103;&#26041;&#27861;&#26041;&#38754;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#35299;&#30721;&#27169;&#22411;&#35299;&#20915;&#20102;&#26681;&#25454;fMRI&#37325;&#26500;&#21050;&#28608;&#30340;&#36870;&#38382;&#39064;&#12290;&#23427;&#20204;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#22914;&#20309;&#22788;&#29702;&#20449;&#24687;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#30340;&#21457;&#23637;&#37117;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
How does the brain represent different modes of information? Can we design a system that automatically understands what the user is thinking? Such questions can be answered by studying brain recordings like functional magnetic resonance imaging (fMRI). As a first step, the neuroscience community has contributed several large cognitive neuroscience datasets related to passive reading/listening/viewing of concept words, narratives, pictures and movies. Encoding and decoding models using these datasets have also been proposed in the past two decades. These models serve as additional tools for basic research in cognitive science and neuroscience. Encoding models aim at generating fMRI brain representations given a stimulus automatically. They have several practical applications in evaluating and diagnosing neurological conditions and thus also help design therapies for brain damage. Decoding models solve the inverse problem of reconstructing the stimuli given the fMRI. They are useful for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TbExplain&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;XAI&#25216;&#26415;&#21644;&#39044;&#35757;&#32451;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#25991;&#26412;&#24418;&#24335;&#35299;&#37322;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32416;&#27491;&#39044;&#27979;&#21644;&#36827;&#34892;&#25991;&#26412;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.10003</link><description>&lt;p&gt;
TbExplain: &#19968;&#31181;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35299;&#37322;&#26041;&#27861;&#19982;&#32479;&#35745;&#39044;&#27979;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
TbExplain: A Text-based Explanation Method for Scene Classification Models with the Statistical Prediction Correction. (arXiv:2307.10003v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TbExplain&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;XAI&#25216;&#26415;&#21644;&#39044;&#35757;&#32451;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#25991;&#26412;&#24418;&#24335;&#35299;&#37322;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32416;&#27491;&#39044;&#27979;&#21644;&#36827;&#34892;&#25991;&#26412;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;(XAI)&#30340;&#39046;&#22495;&#26088;&#22312;&#25552;&#39640;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#24314;&#31435;&#22522;&#20110;&#36755;&#20837;&#29305;&#24449;&#37325;&#35201;&#24615;&#20540;&#30340;&#28909;&#22270;&#26159;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#39044;&#27979;&#30340;&#22522;&#26412;&#26041;&#27861;&#20043;&#19968;&#12290;&#28909;&#22270;&#22312;&#20154;&#31867;&#20013;&#20960;&#20046;&#21487;&#20197;&#29702;&#35299;&#65292;&#20294;&#24182;&#38750;&#27809;&#26377;&#32570;&#38519;&#12290;&#20363;&#22914;&#65292;&#38750;&#19987;&#19994;&#29992;&#25143;&#21487;&#33021;&#19981;&#23436;&#20840;&#29702;&#35299;&#28909;&#22270;&#30340;&#36923;&#36753;&#65288;&#21363;&#20351;&#29992;&#19981;&#21516;&#24378;&#24230;&#25110;&#39068;&#33394;&#31361;&#20986;&#26174;&#31034;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#30340;&#20687;&#32032;&#30340;&#36923;&#36753;&#65289;&#12290;&#27492;&#22806;&#65292;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#30340;&#36755;&#20837;&#22270;&#20687;&#30340;&#23545;&#35937;&#21644;&#21306;&#22495;&#36890;&#24120;&#26080;&#27861;&#23436;&#20840;&#36890;&#36807;&#28909;&#22270;&#21306;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TbExplain&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;XAI&#25216;&#26415;&#21644;&#39044;&#35757;&#32451;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#65292;&#20197;&#25991;&#26412;&#24418;&#24335;&#35299;&#37322;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;TbExplain&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32416;&#27491;&#39044;&#27979;&#21644;&#36827;&#34892;&#25991;&#26412;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Explainable Artificial Intelligence (XAI) aims to improve the interpretability of black-box machine learning models. Building a heatmap based on the importance value of input features is a popular method for explaining the underlying functions of such models in producing their predictions. Heatmaps are almost understandable to humans, yet they are not without flaws. Non-expert users, for example, may not fully understand the logic of heatmaps (the logic in which relevant pixels to the model's prediction are highlighted with different intensities or colors). Additionally, objects and regions of the input image that are relevant to the model prediction are frequently not entirely differentiated by heatmaps. In this paper, we propose a framework called TbExplain that employs XAI techniques and a pre-trained object detector to present text-based explanations of scene classification models. Moreover, TbExplain incorporates a novel method to correct predictions and textually exp
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#21487;&#33021;&#20250;&#23548;&#33268;&#35760;&#24518;&#35757;&#32451;&#22270;&#20687;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#27169;&#22411;&#26102;&#38656;&#35201;&#35880;&#24910;&#12290;</title><link>http://arxiv.org/abs/2305.07644</link><description>&lt;p&gt;
&#35686;&#24789;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687; -- &#19982; GAN &#22312;&#35760;&#24518;&#33041;&#32959;&#30244;&#22270;&#20687;&#26041;&#38754;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beware of diffusion models for synthesizing medical images -- A comparison with GANs in terms of memorizing brain tumor images. (arXiv:2305.07644v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07644
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#21487;&#33021;&#20250;&#23548;&#33268;&#35760;&#24518;&#35757;&#32451;&#22270;&#20687;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#27169;&#22411;&#26102;&#38656;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26368;&#21021;&#26159;&#20026;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#32780;&#24320;&#21457;&#30340;&#65292;&#29616;&#22312;&#20063;&#34987;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;&#22312; GAN &#20043;&#21069;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20351;&#29992;&#20102;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#25351;&#26631;&#22914; FID &#21644; IS &#24182;&#19981;&#36866;&#21512;&#30830;&#23450;&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#21482;&#26159;&#22797;&#21046;&#20102;&#35757;&#32451;&#22270;&#20687;&#12290;&#36825;&#37324;&#25105;&#20204;&#20351;&#29992; BRATS20 &#21644; BRATS21 &#25968;&#25454;&#38598;&#35757;&#32451; StyleGAN &#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#29983;&#25104;&#33041;&#32959;&#30244;&#22270;&#20687;&#65292;&#24182;&#27979;&#37327;&#21512;&#25104;&#22270;&#20687;&#19982;&#25152;&#26377;&#35757;&#32451;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#35760;&#24518;&#35757;&#32451;&#22270;&#20687;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23567;&#25968;&#25454;&#38598;&#12290;&#22914;&#26524;&#26368;&#32456;&#30446;&#26631;&#26159;&#20849;&#20139;&#21512;&#25104;&#30340;&#22270;&#20687;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#25104;&#20687;&#26102;&#24212;&#35813;&#23567;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models were initially developed for text-to-image generation and are now being utilized to generate high quality synthetic images. Preceded by GANs, diffusion models have shown impressive results using various evaluation metrics. However, commonly used metrics such as FID and IS are not suitable for determining whether diffusion models are simply reproducing the training images. Here we train StyleGAN and diffusion models, using BRATS20 and BRATS21 datasets, to synthesize brain tumor images, and measure the correlation between the synthetic images and all training images. Our results show that diffusion models are much more likely to memorize the training images, especially for small datasets. Researchers should be careful when using diffusion models for medical imaging, if the final goal is to share the synthetic images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#40065;&#26834;UNet&#21435;&#22122;&#22120;&#30340;&#35748;&#35777;&#38646;&#38454;&#40657;&#30418;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#40657;&#30418;&#27169;&#22411;&#20043;&#21069;&#39044;&#32622;RDUNet&#21644;DS&#25110;AE&#21644;RDUNet&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06430</link><description>&lt;p&gt;
&#24102;&#26377;&#40065;&#26834;UNet&#21435;&#22122;&#22120;&#30340;&#35748;&#35777;&#38646;&#38454;&#40657;&#30418;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Certified Zeroth-order Black-Box Defense with Robust UNet Denoiser. (arXiv:2304.06430v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#40065;&#26834;UNet&#21435;&#22122;&#22120;&#30340;&#35748;&#35777;&#38646;&#38454;&#40657;&#30418;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#40657;&#30418;&#27169;&#22411;&#20043;&#21069;&#39044;&#32622;RDUNet&#21644;DS&#25110;AE&#21644;RDUNet&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#40657;&#30418;&#35774;&#32622;&#20013;&#23545;&#20110;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#35748;&#35777;&#38450;&#24481;&#26041;&#27861;&#24050;&#32463;&#20174;&#38646;&#38454;&#35282;&#24230;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#28982;&#32780;&#30001;&#20110;&#21435;&#22122;&#22120;&#30340;&#35774;&#35745;&#19981;&#22815;&#26377;&#25928;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#23384;&#22312;&#39640;&#27169;&#22411;&#26041;&#24046;&#21644;&#20302;&#24615;&#33021;&#65292;&#19988;&#22312;&#20351;&#29992;&#38646;&#38454;&#25216;&#26415;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#35777;&#30340;&#38646;&#38454;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#20165;&#20351;&#29992;&#27169;&#22411;&#26597;&#35810;&#21363;&#21487;&#20174;&#21463;&#25915;&#20987;&#22270;&#20687;&#20013;&#21435;&#38500;&#23545;&#25239;&#24615;&#25200;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;UNet&#21435;&#22122;&#22120;&#65288;RDUNet&#65289;&#65292;&#30830;&#20445;&#20102;&#23545;&#20110;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#40657;&#30418;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#40657;&#30418;&#21435;&#22122;&#24179;&#28369;&#65288;DS&#65289;&#38450;&#24481;&#26426;&#21046;ZO-RUDS&#65292;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;RDUNet&#39044;&#32622;&#20110;&#40657;&#30418;&#27169;&#22411;&#20043;&#21069;&#65292;&#30830;&#20445;&#40657;&#30418;&#38450;&#24481;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;ZO-AE-RUDS&#65292;&#22312;&#40657;&#30418;&#27169;&#22411;&#20043;&#21069;&#20351;&#29992;RDUNet&#21644;&#33258;&#32534;&#30721;&#22120;(AE)&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Certified defense methods against adversarial perturbations have been recently investigated in the black-box setting with a zeroth-order (ZO) perspective. However, these methods suffer from high model variance with low performance on high-dimensional datasets due to the ineffective design of the denoiser and are limited in their utilization of ZO techniques. To this end, we propose a certified ZO preprocessing technique for removing adversarial perturbations from the attacked image in the black-box setting using only model queries. We propose a robust UNet denoiser (RDUNet) that ensures the robustness of black-box models trained on high-dimensional datasets. We propose a novel black-box denoised smoothing (DS) defense mechanism, ZO-RUDS, by prepending our RDUNet to the black-box model, ensuring black-box defense. We further propose ZO-AE-RUDS in which RDUNet followed by autoencoder (AE) is prepended to the black-box model. We perform extensive experiments on four classification dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BugNIST&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;12&#31181;&#26118;&#34411;&#21644;&#24188;&#34411;&#30340;&#24494;-CT&#25195;&#25551;&#32452;&#25104;&#12290;&#36890;&#36807;&#35757;&#32451;&#21644;&#27979;&#35797;&#26816;&#27979;&#27169;&#22411;&#65292;BugNIST&#26088;&#22312;&#35780;&#20272;&#19977;&#32500;&#20307;&#31215;&#22270;&#20687;&#20998;&#31867;&#21644;&#26816;&#27979;&#26041;&#27861;&#65292;&#35299;&#20915;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.01838</link><description>&lt;p&gt;
BugNIST -- &#19968;&#31181;&#26032;&#30340;&#22823;&#35268;&#27169;&#20307;&#31215;&#19977;&#32500;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
BugNIST -- A New Large Scale Volumetric 3D Image Dataset for Classification and Detection. (arXiv:2304.01838v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BugNIST&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;12&#31181;&#26118;&#34411;&#21644;&#24188;&#34411;&#30340;&#24494;-CT&#25195;&#25551;&#32452;&#25104;&#12290;&#36890;&#36807;&#35757;&#32451;&#21644;&#27979;&#35797;&#26816;&#27979;&#27169;&#22411;&#65292;BugNIST&#26088;&#22312;&#35780;&#20272;&#19977;&#32500;&#20307;&#31215;&#22270;&#20687;&#20998;&#31867;&#21644;&#26816;&#27979;&#26041;&#27861;&#65292;&#35299;&#20915;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#30740;&#31350;&#30340;&#36827;&#23637;&#21463;&#21040;&#25968;&#25454;&#38598;&#32570;&#20047;&#30340;&#38480;&#21046;&#65292;&#22823;&#22810;&#25968;&#38024;&#23545;&#20307;&#31215;&#22270;&#20687;&#30340;&#20998;&#26512;&#26041;&#27861;&#37117;&#22522;&#20110;&#21307;&#23398;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#25968;&#25454;&#24182;&#19981;&#19968;&#23450;&#20855;&#26377;&#20854;&#20182;&#20307;&#31215;&#22270;&#20687;&#65288;&#20363;&#22914;&#24494;-CT&#65289;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#20419;&#36827;&#19977;&#32500;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#30340;&#30740;&#31350;&#36229;&#36234;&#21307;&#23398;&#25968;&#25454;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;BugNIST&#25968;&#25454;&#38598;&#24182;&#20813;&#36153;&#25552;&#20379;&#12290;BugNIST&#26159;&#19968;&#32452;&#30001;12&#31181;&#26118;&#34411;&#21644;&#24188;&#34411;&#30340;&#24494;-CT&#25195;&#25551;&#32452;&#25104;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#12290;BugNIST&#21253;&#21547;9437&#20010;&#20307;&#31215;&#65292;&#20854;&#20013;9087&#20010;&#26159;&#21333;&#20010;&#26118;&#34411;&#30340;&#25195;&#25551;&#65292;350&#20010;&#26159;&#26118;&#34411;&#21644;&#20854;&#20182;&#26448;&#26009;&#30340;&#28151;&#21512;&#29289;&#12290;BugNIST&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#20998;&#31867;&#21644;&#26816;&#27979;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26816;&#27979;&#25361;&#25112;&#65292;&#20351;&#24471;&#26816;&#27979;&#27169;&#22411;&#22312;&#21333;&#20010;&#26118;&#34411;&#30340;&#25195;&#25551;&#19978;&#35757;&#32451;&#24182;&#22312;&#26118;&#34411;&#28151;&#21512;&#29289;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#33021;&#22815;&#35299;&#20915;&#27492;&#20219;&#21153;&#30340;&#27169;&#22411;&#23558;&#29420;&#31435;&#20110;&#19978;&#19979;&#25991;&#65288;&#21363;&#21608;&#22260;&#26448;&#26009;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24456;&#22823;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Progress in 3D volumetric image analysis research is limited by the lack of datasets and most advances in analysis methods for volumetric images are based on medical data. However, medical data do not necessarily resemble the characteristics of other volumetric images such as micro-CT. To promote research in 3D volumetric image analysis beyond medical data, we have created the BugNIST dataset and made it freely available. BugNIST is an extensive dataset of micro-CT scans of 12 types of bugs, such as insects and larvae. BugNIST contains 9437 volumes where 9087 are of individual bugs and 350 are mixtures of bugs and other material. The goal of BugNIST is to benchmark classification and detection methods, and we have designed the detection challenge such that detection models are trained on scans of individual bugs and tested on bug mixtures. Models capable of solving this task will be independent of the context, i.e., the surrounding material. This is a great advantage if the context is 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Passau-SFCH&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;11&#23567;&#26102;&#30340;&#24405;&#38899;&#65292;&#29992;&#20110;&#33258;&#21457;&#24189;&#40664;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#22810;&#27169;&#24577;&#30340;&#20998;&#26512;&#21644;&#29305;&#24449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#24189;&#40664;&#20197;&#21450;&#24189;&#40664;&#24773;&#24863;&#30340;&#33258;&#21160;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2209.14272</link><description>&lt;p&gt;
&#36808;&#21521;&#22810;&#27169;&#24577;&#39044;&#27979;&#33258;&#21457;&#24189;&#40664;&#65306;&#19968;&#20221;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#21644;&#21021;&#27493;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Towards Multimodal Prediction of Spontaneous Humour: A Novel Dataset and First Results. (arXiv:2209.14272v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Passau-SFCH&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;11&#23567;&#26102;&#30340;&#24405;&#38899;&#65292;&#29992;&#20110;&#33258;&#21457;&#24189;&#40664;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#22810;&#27169;&#24577;&#30340;&#20998;&#26512;&#21644;&#29305;&#24449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#24189;&#40664;&#20197;&#21450;&#24189;&#40664;&#24773;&#24863;&#30340;&#33258;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24189;&#40664;&#26159;&#20154;&#31867;&#24773;&#24863;&#21644;&#35748;&#30693;&#30340;&#37325;&#35201;&#20803;&#32032;&#12290;&#20854;&#33258;&#21160;&#29702;&#35299;&#21487;&#20197;&#20419;&#36827;&#26356;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20154;&#24615;&#21270;&#12290;&#30446;&#21069;&#30340;&#24189;&#40664;&#26816;&#27979;&#26041;&#27861;&#20165;&#22522;&#20110;&#31574;&#21010;&#25968;&#25454;&#65292;&#19981;&#33021;&#28385;&#36275;&#8220;&#29616;&#23454;&#19990;&#30028;&#8221;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;Passau-Spontaneous Football Coach Humour&#65288;Passau-SFCH&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#32422;11&#23567;&#26102;&#30340;&#24405;&#38899;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#32570;&#38519;&#12290;Passau-SFCH&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#26681;&#25454;Martin&#30340;&#24189;&#40664;&#39118;&#26684;&#38382;&#21367;&#25552;&#20986;&#30340;&#24189;&#40664;&#23384;&#22312;&#21450;&#20854;&#32500;&#24230;&#65288;&#24773;&#24863;&#21644;&#26041;&#21521;&#65289;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#19987;&#23478;&#35774;&#35745;&#30340;&#29305;&#24449;&#12290;&#20998;&#26512;&#20102;&#33258;&#21457;&#24189;&#40664;&#35782;&#21035;&#30340;&#27599;&#31181;&#27169;&#24577;&#65288;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#24189;&#40664;&#21450;&#20854;&#24773;&#24863;&#30340;&#33258;&#21160;&#20998;&#26512;&#65292;&#22810;&#27169;&#24577;&#32852;&#21512;&#20351;&#29992;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humour is a substantial element of human affect and cognition. Its automatic understanding can facilitate a more naturalistic human-device interaction and the humanisation of artificial intelligence. Current methods of humour detection are solely based on staged data making them inadequate for 'real-world' applications. We address this deficiency by introducing the novel Passau-Spontaneous Football Coach Humour (Passau-SFCH) dataset, comprising of about 11 hours of recordings. The Passau-SFCH dataset is annotated for the presence of humour and its dimensions (sentiment and direction) as proposed in Martin's Humor Style Questionnaire. We conduct a series of experiments, employing pretrained Transformers, convolutional neural networks, and expert-designed features. The performance of each modality (text, audio, video) for spontaneous humour recognition is analysed and their complementarity is investigated. Our findings suggest that for the automatic analysis of humour and its sentiment, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MDLatLRRv2&#30340;&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#22810;&#32423;&#20998;&#35299;&#26041;&#27861;&#24182;&#20805;&#20998;&#21033;&#29992;LatLRR&#25552;&#21462;&#30340;&#21508;&#31181;&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#20013;&#30340;&#26368;&#20808;&#36827;&#34701;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.15179</link><description>&lt;p&gt;
&#22522;&#20110;MDLatLRRv2&#30340;&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Medical Image Fusion Method based on MDLatLRRv2. (arXiv:2206.15179v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15179
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MDLatLRRv2&#30340;&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#22810;&#32423;&#20998;&#35299;&#26041;&#27861;&#24182;&#20805;&#20998;&#21033;&#29992;LatLRR&#25552;&#21462;&#30340;&#21508;&#31181;&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#20013;&#30340;&#26368;&#20808;&#36827;&#34701;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;MDLatLRR&#20165;&#32771;&#34385;&#20102;&#36890;&#36807;&#28508;&#22312;&#20302;&#31209;&#34920;&#31034;&#65288;LatLRR&#65289;&#25552;&#21462;&#30340;&#36755;&#20837;&#22270;&#20687;&#30340;&#35814;&#32454;&#37096;&#20998;&#65288;&#26174;&#33879;&#29305;&#24449;&#65289;&#65292;&#27809;&#26377;&#26377;&#25928;&#22320;&#21033;&#29992;LatLRR&#25552;&#21462;&#30340;&#22522;&#30784;&#37096;&#20998;&#65288;&#20027;&#35201;&#29305;&#24449;&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22810;&#32423;&#20998;&#35299;&#26041;&#27861;&#65292;&#31216;&#20026;MDLatLRRv2&#65292;&#35813;&#26041;&#27861;&#33021;&#26377;&#25928;&#20998;&#26512;&#21644;&#21033;&#29992;LatLRR&#33719;&#21462;&#30340;&#25152;&#26377;&#22270;&#20687;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;MDLatLRRv2&#24212;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;&#12290;&#22522;&#30784;&#37096;&#20998;&#36890;&#36807;&#24179;&#22343;&#31574;&#30053;&#36827;&#34892;&#34701;&#21512;&#65292;&#35814;&#32454;&#37096;&#20998;&#36890;&#36807;&#26680;&#33539;&#25968;&#25805;&#20316;&#36827;&#34892;&#34701;&#21512;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30340;&#27604;&#36739;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#20013;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#34701;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since MDLatLRR only considers detailed parts (salient features) of input images extracted by latent low-rank representation (LatLRR), it doesn't use base parts (principal features) extracted by LatLRR effectively. Therefore, we proposed an improved multi-level decomposition method called MDLatLRRv2 which effectively analyzes and utilizes all the image features obtained by LatLRR. Then we apply MDLatLRRv2 to medical image fusion. The base parts are fused by average strategy and the detail parts are fused by nuclear-norm operation. The comparison with the existing methods demonstrates that the proposed method can achieve state-of-the-art fusion performance in objective and subjective assessment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Res2Net&#30340;&#32418;&#22806;&#21644;&#21487;&#35265;&#20809;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#21644;&#34701;&#21512;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34701;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2112.14540</link><description>&lt;p&gt;
Res2NetFuse&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#32418;&#22806;&#21644;&#21487;&#35265;&#20809;&#22270;&#20687;&#30340;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Res2NetFuse: A Fusion Method for Infrared and Visible Images. (arXiv:2112.14540v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Res2Net&#30340;&#32418;&#22806;&#21644;&#21487;&#35265;&#20809;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#21644;&#34701;&#21512;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34701;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Res2Net&#30340;&#32418;&#22806;&#21644;&#21487;&#35265;&#20809;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#12290;&#25552;&#20986;&#30340;&#34701;&#21512;&#27169;&#22411;&#21253;&#25324;&#32534;&#30721;&#22120;&#12289;&#34701;&#21512;&#23618;&#21644;&#35299;&#30721;&#22120;&#19977;&#20010;&#37096;&#20998;&#12290;&#21033;&#29992;&#22522;&#20110;Res2Net&#30340;&#32534;&#30721;&#22120;&#25552;&#21462;&#28304;&#22270;&#20687;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#65292;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20165;&#20351;&#29992;&#21333;&#20010;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#34701;&#21512;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35299;&#30721;&#22120;&#37325;&#26500;&#34701;&#21512;&#22270;&#20687;&#12290;&#26412;&#25991;&#36824;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#20013;&#37117;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34701;&#21512;&#24615;&#33021;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel Res2Net-based fusion framework for infrared and visible images. The proposed fusion model has three parts: an encoder, a fusion layer and a decoder, respectively. The Res2Net-based encoder is used to extract multi-scale features of source images, the paper introducing a new training strategy for training a Res2Net-based encoder that uses only a single image. Then, a new fusion strategy is developed based on the attention model. Finally, the fused image is reconstructed by the decoder. The proposed approach is also analyzed in detail. Experiments show that our method achieves state-of-the-art fusion performance in objective and subjective assessment by comparing with the existing methods.
&lt;/p&gt;</description></item></channel></rss>