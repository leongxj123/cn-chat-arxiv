<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#20272;&#35745;&#26799;&#24230;&#24182;&#29983;&#25104;&#26174;&#33879;&#22270;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#65292;&#36890;&#36807;Likelihood Ratio&#26041;&#27861;&#20272;&#35745;&#36755;&#20986;&#21040;&#36755;&#20837;&#30340;&#26799;&#24230;&#65292;&#24182;&#24212;&#29992;&#20998;&#22359;&#35745;&#31639;&#25216;&#26415;&#25552;&#39640;&#20272;&#35745;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#35777;&#23454;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15603</link><description>&lt;p&gt;
&#22522;&#20110;&#21069;&#21521;&#23398;&#20064;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#40657;&#30418;&#26174;&#33879;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Forward Learning for Gradient-based Black-box Saliency Map Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15603
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#20272;&#35745;&#26799;&#24230;&#24182;&#29983;&#25104;&#26174;&#33879;&#22270;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#65292;&#36890;&#36807;Likelihood Ratio&#26041;&#27861;&#20272;&#35745;&#36755;&#20986;&#21040;&#36755;&#20837;&#30340;&#26799;&#24230;&#65292;&#24182;&#24212;&#29992;&#20998;&#22359;&#35745;&#31639;&#25216;&#26415;&#25552;&#39640;&#20272;&#35745;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#35777;&#23454;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;-based&#26174;&#33879;&#22270;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#26356;&#28145;&#21644;&#26356;&#40657;&#30418;&#65292;&#22914;&#22312;&#38381;&#28304;API&#65288;&#22914;ChatGPT&#65289;&#20013;&#65292;&#35745;&#31639;&#26799;&#24230;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38459;&#30861;&#20256;&#32479;&#35299;&#37322;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#20272;&#35745;&#26799;&#24230;&#24182;&#29983;&#25104;&#26174;&#33879;&#22270;&#26469;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#12290;&#25105;&#20204;&#37319;&#29992;&#20284;&#28982;&#27604;&#26041;&#27861;&#26469;&#20272;&#35745;&#36755;&#20986;&#21040;&#36755;&#20837;&#30340;&#26799;&#24230;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26174;&#33879;&#22270;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#22359;&#35745;&#31639;&#25216;&#26415;&#26469;&#22686;&#24378;&#20272;&#35745;&#20934;&#30830;&#24615;&#12290;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20934;&#30830;&#30340;&#26799;&#24230;&#20272;&#35745;&#21644;&#29983;&#25104;&#26174;&#33879;&#22270;&#30340;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#23427;&#26469;&#35299;&#37322;GPT-Vision&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#30456;&#20851;&#24615;&#30340;&#25345;&#32493;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15603v1 Announce Type: cross  Abstract: Gradient-based saliency maps are widely used to explain deep neural network decisions. However, as models become deeper and more black-box, such as in closed-source APIs like ChatGPT, computing gradients become challenging, hindering conventional explanation methods. In this work, we introduce a novel unified framework for estimating gradients in black-box settings and generating saliency maps to interpret model decisions. We employ the likelihood ratio method to estimate output-to-input gradients and utilize them for saliency map generation. Additionally, we propose blockwise computation techniques to enhance estimation accuracy. Extensive experiments in black-box settings validate the effectiveness of our method, demonstrating accurate gradient estimation and explainability of generated saliency maps. Furthermore, we showcase the scalability of our approach by applying it to explain GPT-Vision, revealing the continued relevance of gr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2402.17376</link><description>&lt;p&gt;
&#20248;&#21270;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Accelerating Diffusion Sampling with Optimized Time Steps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17376
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21512;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#37319;&#26679;&#27493;&#39588;&#65292;&#20854;&#37319;&#26679;&#25928;&#29575;&#20173;&#26377;&#24453;&#25552;&#39640;&#12290;&#36817;&#26399;&#39640;&#38454;&#25968;&#20540;ODE&#27714;&#35299;&#22120;&#22312;DPMs&#20013;&#30340;&#24212;&#29992;&#20351;&#24471;&#29992;&#26356;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#36825;&#26159;&#19968;&#39033;&#37325;&#22823;&#36827;&#23637;&#65292;&#22823;&#22810;&#25968;&#37319;&#26679;&#26041;&#27861;&#20173;&#28982;&#37319;&#29992;&#22343;&#21248;&#26102;&#38388;&#27493;&#38271;&#65292;&#32780;&#22312;&#37319;&#26679;&#27493;&#39588;&#36739;&#23569;&#26102;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#20248;&#21270;&#38382;&#39064;&#26088;&#22312;&#20026;DPMs&#30340;&#29305;&#23450;&#25968;&#20540;ODE&#27714;&#35299;&#22120;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;&#27492;&#20248;&#21270;&#38382;&#39064;&#26088;&#22312;&#26368;&#23567;&#21270;&#22320;&#23454;&#29616;&#22320;&#30495;&#23454;&#35299;&#19982;&#19982;&#25968;&#20540;&#27714;&#35299;&#22120;&#23545;&#24212;&#30340;&#36817;&#20284;&#35299;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#21463;&#38480;&#20449;&#36182;&#22495;&#26041;&#27861;&#36827;&#34892;&#39640;&#25928;&#27714;&#35299;&#65292;&#26102;&#38388;&#23569;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17376v1 Announce Type: cross  Abstract: Diffusion probabilistic models (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps. Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps. While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps. To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs. This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver. It can be efficiently solved using the constrained trust region method, taking less than 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20219;&#21153;&#23548;&#21521;&#30340;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#23567;&#22411;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#30446;&#26631;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36824;&#23637;&#31034;&#20102;&#39640;&#36798;9&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2311.18237</link><description>&lt;p&gt;
&#20174;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#20013;&#36827;&#34892;&#30693;&#35782;&#36801;&#31227;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#23567;&#22411;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Knowledge Transfer from Vision Foundation Models for Efficient Training of Small Task-specific Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20219;&#21153;&#23548;&#21521;&#30340;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#23567;&#22411;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#30446;&#26631;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36824;&#23637;&#31034;&#20102;&#39640;&#36798;9&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#26377;&#38480;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25512;&#29702;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#24212;&#29992;&#20110;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20219;&#21153;&#23548;&#21521;&#30340;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#20197;&#39640;&#25928;&#35299;&#20915;&#22914;&#20309;&#21033;&#29992;&#22823;&#35268;&#27169;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#35757;&#32451;&#23567;&#22411;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#36229;&#36807;Task-Agnostic VFM&#33976;&#39311;&#12289;Web-Scale CLIP&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24335;ImageNet&#39044;&#35757;&#32451;&#21644;&#33258;&#30417;&#30563;DINO&#39044;&#35757;&#32451;29.8%&#12289;22.1%&#12289;13.7%&#21644;11.6%&#30340;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36824;&#23637;&#29616;&#20986;&#20102;&#39640;&#36798;9&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18237v2 Announce Type: replace-cross  Abstract: Vision Foundation Models (VFMs) pretrained on massive datasets exhibit impressive performance on various downstream tasks, especially with limited labeled target data. However, due to their high inference compute cost, these models cannot be deployed for many real-world applications. Motivated by this, we ask the following important question, "How can we leverage the knowledge from a large VFM to train a small task-specific model for a new target task with limited labeled training data?", and propose a simple task-oriented knowledge transfer approach as a highly effective solution to this problem. Our experimental results on five target tasks show that the proposed approach outperforms task-agnostic VFM distillation, web-scale CLIP pretraining, supervised ImageNet pretraining, and self-supervised DINO pretraining by up to 11.6%, 22.1%, 13.7%, and 29.8%, respectively. Furthermore, the proposed approach also demonstrates up to 9x
&lt;/p&gt;</description></item><item><title>RO-LMM&#26159;&#19968;&#20010;&#38024;&#23545;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Consistency Embedding Fine-Tuning&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#20445;&#25345;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#21319;&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#29992;&#20110;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#21644;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2311.15876</link><description>&lt;p&gt;
LMM&#36741;&#21161;&#30340;&#19968;&#33268;&#24615;&#23884;&#20837;&#19979;&#20083;&#33146;&#30284;&#27835;&#30103;&#30446;&#26631;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
LMM-Assisted Breast Cancer Treatment Target Segmentation with Consistency Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15876
&lt;/p&gt;
&lt;p&gt;
RO-LMM&#26159;&#19968;&#20010;&#38024;&#23545;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Consistency Embedding Fine-Tuning&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#20445;&#25345;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#21319;&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#29992;&#20110;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#21644;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#28145;&#21051;&#24433;&#21709;&#20102;&#21307;&#23398;&#39046;&#22495;&#65292;&#20026;&#38477;&#20302;&#20020;&#24202;&#24037;&#20316;&#37327;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21463;&#38480;&#20110;&#25191;&#34892;&#21333;&#27169;&#24335;&#20219;&#21153;&#65292;&#19982;&#21307;&#23398;&#19987;&#19994;&#20154;&#21592;&#25152;&#20351;&#29992;&#30340;&#32508;&#21512;&#26041;&#27861;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;RO-LMM&#65292;&#19968;&#20010;&#19987;&#20026;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMM&#65289;&#12290;&#35813;&#27169;&#22411;&#28085;&#30422;&#20102;&#20020;&#24202;&#24037;&#20316;&#27969;&#20013;&#30340;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#25797;&#38271;&#20020;&#24202;&#25253;&#21578;&#25688;&#35201;&#12289;&#25918;&#30103;&#27835;&#30103;&#35745;&#21010;&#24314;&#35758;&#21644;&#35745;&#21010;&#24341;&#23548;&#30340;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;&#20026;&#20102;&#25191;&#34892;&#36830;&#32493;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#24615;&#23884;&#20837;&#24494;&#35843;&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#25552;&#21319;&#20102;LMM&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#35813;&#27010;&#24565;&#36716;&#21270;&#20026;LMM&#39537;&#21160;&#30340;&#20998;&#21106;&#26694;&#26550;&#65292;&#21363;&#19968;&#33268;&#24615;&#23884;&#20837;S&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15876v2 Announce Type: replace-cross  Abstract: Recent advancements in Artificial Intelligence (AI) have profoundly influenced medical fields, by providing tools to reduce clinical workloads. However, most AI models are constrained to execute unimodal tasks, in stark contrast to the comprehensive approaches utilized by medical professionals. To address this, here we present RO-LMM, a multi-purpose large multimodal model (LMM) tailored for the field of radiation oncology. This model covers series of tasks within clinical workflow, adept at clinical report summarization, radiation treatment plan suggestion, and plan-guided target volume segmentation. In particular, to perform consecutive clinical tasks, we further present a novel Consistency Embedding Fine-Tuning (CEFTune) technique, which boosts LMM's robustness to noisy inputs while preserving the capability of handling clean inputs, and transform this concept into LMM-driven segmentation framework as Consistency Embedding S
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#36861;&#36394;&#21644;&#27169;&#25311;&#29289;&#20307;&#24863;&#30693;&#20197;&#21450;&#20854;&#22312;&#20132;&#27969;&#20013;&#25152;&#20256;&#36798;&#30340;&#34920;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#24847;&#35782;&#12290;&#30456;&#27604;&#20110;&#22823;&#22810;&#25968;&#26080;&#27861;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#26032;&#32593;&#32476;&#26469;&#23450;&#20041;&#29289;&#20307;&#24863;&#30693;&#12290;</title><link>https://arxiv.org/abs/2310.05212</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#32593;&#32476;&#20195;&#34920;&#24847;&#35782;&#30340;&#30693;&#35273;
&lt;/p&gt;
&lt;p&gt;
Interpretable Semiotics Networks Representing Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05212
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#36861;&#36394;&#21644;&#27169;&#25311;&#29289;&#20307;&#24863;&#30693;&#20197;&#21450;&#20854;&#22312;&#20132;&#27969;&#20013;&#25152;&#20256;&#36798;&#30340;&#34920;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#24847;&#35782;&#12290;&#30456;&#27604;&#20110;&#22823;&#22810;&#25968;&#26080;&#27861;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#26032;&#32593;&#32476;&#26469;&#23450;&#20041;&#29289;&#20307;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27599;&#22825;&#37117;&#24863;&#30693;&#29289;&#20307;&#65292;&#24182;&#36890;&#36807;&#21508;&#31181;&#28192;&#36947;&#20256;&#36798;&#20182;&#20204;&#30340;&#24863;&#30693;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36861;&#36394;&#21644;&#27169;&#25311;&#29289;&#20307;&#30340;&#24863;&#30693;&#20197;&#21450;&#23427;&#20204;&#22312;&#20132;&#27969;&#20013;&#25152;&#20256;&#36798;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#20869;&#37096;&#34920;&#31034;&#30340;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65288;"&#35266;&#23519;&#21040;&#30340;"&#21644;"&#30475;&#21040;&#30340;"&#65289;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#29087;&#24713;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27010;&#24565;&#65288;&#32534;&#30721;&#21644;&#35299;&#30721;&#65289;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#20803;&#32032;&#34987;&#21512;&#24182;&#22312;&#19968;&#36215;&#24418;&#25104;&#31526;&#21495;&#32593;&#32476;&#65292;&#27169;&#25311;&#20102;&#29289;&#20307;&#24863;&#30693;&#21644;&#20154;&#31867;&#20132;&#27969;&#20013;&#30340;&#24847;&#35782;&#12290;&#22914;&#20170;&#65292;&#22823;&#22810;&#25968;&#31070;&#32463;&#32593;&#32476;&#37117;&#26159;&#19981;&#21487;&#35299;&#37322;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#21487;&#35265;&#24615;&#12290;&#25105;&#20204;&#20154;&#30340;&#29289;&#20307;&#24863;&#30693;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#32593;&#32476;&#23450;&#20041;&#29289;&#20307;&#24863;&#30693;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#25324;&#22522;&#20934;&#20998;&#31867;&#22120;&#21644;&#39069;&#22806;&#23618;&#30340;&#26032;&#32593;&#32476;&#26469;&#28436;&#31034;&#36825;&#19968;&#28857;&#12290;&#36825;&#20010;&#23618;&#20135;&#29983;&#20102;&#22270;&#20687;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans perceive objects daily and communicate their perceptions using various channels. Here, we describe a computational model that tracks and simulates objects' perception and their representations as they are conveyed in communication.   We describe two key components of our internal representation ("observed" and "seen") and relate them to familiar computer vision notions (encoding and decoding). These elements are joined together to form semiotics networks, which simulate awareness in object perception and human communication.   Nowadays, most neural networks are uninterpretable. On the other hand, our model overcomes this limitation. The experiments demonstrates the visibility of the model.   Our model of object perception by a person allows us to define object perception by a network. We demonstrate this with an example of an image baseline classifier by constructing a new network that includes the baseline classifier and an additional layer. This layer produces the images "perc
&lt;/p&gt;</description></item><item><title>SINCERE&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#30417;&#30563;&#25193;&#23637;&#65292;&#36991;&#20813;&#20102;&#21516;&#19968;&#31867;&#21035;&#30340;&#22270;&#20687;&#30456;&#20114;&#25490;&#26021;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#20998;&#31163;&#19981;&#21516;&#31867;&#21035;&#30340;&#23884;&#20837;&#65292;&#22312;&#20445;&#25345;&#31454;&#20105;&#24615;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2309.14277</link><description>&lt;p&gt;
SINCERE: &#30417;&#30563;&#20449;&#24687;&#22122;&#22768;-&#23545;&#27604;&#20272;&#35745;&#20877;&#23457;
&lt;/p&gt;
&lt;p&gt;
SINCERE: Supervised Information Noise-Contrastive Estimation REvisited
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.14277
&lt;/p&gt;
&lt;p&gt;
SINCERE&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#30417;&#30563;&#25193;&#23637;&#65292;&#36991;&#20813;&#20102;&#21516;&#19968;&#31867;&#21035;&#30340;&#22270;&#20687;&#30456;&#20114;&#25490;&#26021;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#20998;&#31163;&#19981;&#21516;&#31867;&#21035;&#30340;&#23884;&#20837;&#65292;&#22312;&#20445;&#25345;&#31454;&#20105;&#24615;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65288;InfoNCE&#65289;&#25439;&#22833;&#20989;&#25968;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#23454;&#35777;&#32467;&#26524;&#21644;&#29702;&#35770;&#21160;&#26426;&#65292;&#20026;&#35768;&#22810;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#30417;&#30563;&#23545;&#27604;&#65288;SupCon&#65289;&#25439;&#22833;&#21487;&#25193;&#23637;InfoNCE&#20197;&#20174;&#21487;&#29992;&#31867;&#26631;&#31614;&#20013;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20808;&#21069;&#30340;SupCon&#25439;&#22833;&#20844;&#24335;&#23384;&#22312;&#30097;&#38382;&#30340;&#29702;&#30001;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#20250;&#20419;&#20351;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#30340;&#26576;&#20123;&#22270;&#20687;&#22312;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#30456;&#20114;&#25490;&#26021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30417;&#30563;&#20449;&#24687;&#22122;&#22768;-&#23545;&#27604;&#20272;&#35745;&#20877;&#23457;&#65288;SINCERE&#65289;&#25439;&#22833;&#65292;&#20316;&#20026;&#20449;&#24687;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#30417;&#30563;&#25193;&#23637;&#65292;&#23427;&#27704;&#36828;&#19981;&#20250;&#23548;&#33268;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#30340;&#22270;&#20687;&#30456;&#20114;&#25490;&#26021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;SINCERE&#23548;&#33268;&#19981;&#21516;&#31867;&#21035;&#30340;&#23884;&#20837;&#26356;&#22909;&#22320;&#20998;&#31163;&#65292;&#21516;&#26102;&#23545;&#20110;&#30417;&#30563;&#21644;&#36801;&#31227;&#23398;&#20064;&#25552;&#20379;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#19978;&#30340;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.14277v2 Announce Type: replace-cross  Abstract: The information noise-contrastive estimation (InfoNCE) loss function provides the basis of many self-supervised deep learning methods due to its strong empirical results and theoretic motivation. Previous work suggests a supervised contrastive (SupCon) loss to extend InfoNCE to learn from available class labels. However, in this work we find that the prior SupCon loss formulation has questionable justification because it can encourage some images from the same class to repel one another in the learned embedding space. We propose the Supervised InfoNCE REvisited (SINCERE) loss as a theoretically-justified supervised extension of InfoNCE that never causes images from the same class to repel one another. Experiments show that SINCERE leads to better separation of embeddings from different classes while delivering competitive classification accuracy for supervised and transfer learning. We further show an information-theoretic boun
&lt;/p&gt;</description></item><item><title>AutoRT&#26159;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#24182;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.12963</link><description>&lt;p&gt;
AutoRT&#65306;&#22823;&#35268;&#27169;&#32534;&#25490;&#26426;&#22120;&#20154;&#20195;&#29702;&#30340;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents. (arXiv:2401.12963v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12963
&lt;/p&gt;
&lt;p&gt;
AutoRT&#26159;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#24182;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#26377;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#34892;&#21160;&#31561;&#21151;&#33021;&#30340;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#21033;&#29992;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#26469;&#25512;&#29702;&#26377;&#29992;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#32570;&#20047;&#22522;&#20110;&#29289;&#29702;&#19990;&#30028;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoRT&#65292;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#23436;&#20840;&#26410;&#30693;&#22330;&#26223;&#20013;&#25805;&#20316;&#26426;&#22120;&#20154;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#21482;&#38656;&#35201;&#26368;&#23569;&#30340;&#20154;&#24037;&#30417;&#30563;&#12290;AutoRT&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#23454;&#29616;&#22330;&#26223;&#29702;&#35299;&#21644;&#22522;&#30784;&#32465;&#23450;&#65292;&#24182;&#36827;&#19968;&#27493;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#20379;&#19968;&#32452;&#26426;&#22120;&#20154;&#25191;&#34892;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#25351;&#23548;&#25968;&#25454;&#25910;&#38598;&#65292;AutoRT&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#65292;&#21516;&#26102;&#26174;&#33879;&#25193;&#22823;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#25968;&#25454;&#25910;&#38598;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;AutoRT&#21521;20&#22810;&#20010;&#26426;&#22120;&#20154;&#25552;&#35758;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32447;&#24615;&#39044;&#27979;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#26465;&#20214;&#21644;&#26102;&#26399;&#19979;&#65292;&#36890;&#36807;&#37319;&#29992;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10958</link><description>&lt;p&gt;
&#36890;&#36807;&#32447;&#24615;&#39044;&#27979;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enhancing Deep Neural Network Training Efficiency and Performance through Linear Prediction. (arXiv:2310.10958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32447;&#24615;&#39044;&#27979;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#26465;&#20214;&#21644;&#26102;&#26399;&#19979;&#65292;&#36890;&#36807;&#37319;&#29992;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#19968;&#20010;&#26377;&#25928;&#30340;DNN&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#20248;&#21270;DNN&#35757;&#32451;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;DNN&#21442;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36981;&#24490;&#26576;&#31181;&#35268;&#24459;&#30340;&#35266;&#23519;&#65292;&#21457;&#29616;&#20102;&#21442;&#25968;&#39044;&#27979;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;&#20854;&#27425;&#65292;&#32771;&#34385;&#21040;DNN&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#32423;&#12289;&#30828;&#20214;&#38480;&#21046;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#23545;&#22122;&#22768;&#23481;&#24525;&#24230;&#30340;&#29305;&#24615;&#65292;&#37319;&#29992;&#21442;&#25968;&#32447;&#24615;&#39044;&#27979;&#65288;PLP&#65289;&#26041;&#27861;&#26469;&#36827;&#34892;DNN&#21442;&#25968;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#22312;&#19968;&#20123;&#20195;&#34920;&#24615;&#30340;&#39592;&#26550;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#26465;&#20214;&#21644;&#26102;&#26399;&#19979;&#65292;&#19982;&#27491;&#24120;&#30340;&#35757;&#32451;&#26041;&#24335;&#30456;&#27604;&#65292;&#36890;&#36807;&#37319;&#29992;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) have achieved remarkable success in various fields, including computer vision and natural language processing. However, training an effective DNN model still poses challenges. This paper aims to propose a method to optimize the training effectiveness of DNN, with the goal of improving model performance. Firstly, based on the observation that the DNN parameters change in certain laws during training process, the potential of parameter prediction for improving model training efficiency and performance is discovered. Secondly, considering the magnitude of DNN model parameters, hardware limitations and characteristics of Stochastic Gradient Descent (SGD) for noise tolerance, a Parameter Linear Prediction (PLP) method is exploit to perform DNN parameter prediction. Finally, validations are carried out on some representative backbones. Experiment results show that compare to the normal training ways, under the same training conditions and epochs, by employing propo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#23454;&#29616;&#24515;&#33039;&#20998;&#21106;&#65292;&#36890;&#36807;&#39044;&#27979;&#36718;&#24275;&#28857;&#32780;&#19981;&#26159;&#26631;&#35760;&#27599;&#20010;&#20687;&#32032;&#65292;&#28040;&#38500;&#20102;&#24515;&#33039;&#20998;&#21106;&#20013;&#30340;&#35299;&#21078;&#23398;&#38169;&#35823;&#12290;&#21516;&#26102;&#36824;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#35780;&#20272;&#20102;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01210</link><description>&lt;p&gt;
&#23454;&#29616;&#31283;&#20581;&#30340;&#24515;&#33039;&#20998;&#21106;&#65306;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Cardiac Segmentation using Graph Convolutional Networks. (arXiv:2310.01210v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#23454;&#29616;&#24515;&#33039;&#20998;&#21106;&#65292;&#36890;&#36807;&#39044;&#27979;&#36718;&#24275;&#28857;&#32780;&#19981;&#26159;&#26631;&#35760;&#27599;&#20010;&#20687;&#32032;&#65292;&#28040;&#38500;&#20102;&#24515;&#33039;&#20998;&#21106;&#20013;&#30340;&#35299;&#21078;&#23398;&#38169;&#35823;&#12290;&#21516;&#26102;&#36824;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#35780;&#20272;&#20102;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#33258;&#21160;&#21270;&#30340;&#24515;&#33039;&#20998;&#21106;&#21487;&#20197;&#24555;&#36895;&#12289;&#21487;&#37325;&#22797;&#22320;&#20174;&#36229;&#22768;&#24515;&#21160;&#22270;&#26816;&#26597;&#20013;&#25552;&#21462;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#12290;U-Net&#32467;&#26500;&#26159;&#30446;&#21069;&#21307;&#23398;&#20998;&#21106;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21487;&#20197;&#23454;&#26102;&#20998;&#21106;&#24515;&#33039;&#32467;&#26500;&#65292;&#24182;&#19988;&#24179;&#22343;&#35823;&#24046;&#21487;&#19982;&#35266;&#27979;&#32773;&#38388;&#21464;&#24322;&#24615;&#30456;&#23218;&#32654;&#12290;&#28982;&#32780;&#65292;&#35813;&#26550;&#26500;&#20173;&#28982;&#20250;&#29983;&#25104;&#35768;&#22810;&#35299;&#31163;&#24322;&#24120;&#30340;&#32467;&#26500;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#65292;&#39044;&#27979;&#20986;&#24863;&#20852;&#36259;&#32467;&#26500;&#30340;&#36718;&#24275;&#28857;&#65292;&#32780;&#19981;&#26159;&#23545;&#27599;&#20010;&#20687;&#32032;&#36827;&#34892;&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24515;&#33039;&#35299;&#21078;&#23398;&#30340;&#22270;&#32467;&#26500;&#65292;&#24182;&#35777;&#26126;&#36825;&#28040;&#38500;&#20102;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;CAMUS&#25968;&#25454;&#38598;&#19978;&#30340;&#22810;&#32467;&#26500;&#20998;&#21106;&#20013;&#30340;&#35299;&#21078;&#23398;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#22312;&#20020;&#24202;HUNT4&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully automatic cardiac segmentation can be a fast and reproducible method to extract clinical measurements from an echocardiography examination. The U-Net architecture is the current state-of-the-art deep learning architecture for medical segmentation and can segment cardiac structures in real-time with average errors comparable to inter-observer variability. However, this architecture still generates large outliers that are often anatomically incorrect. This work uses the concept of graph convolutional neural networks that predict the contour points of the structures of interest instead of labeling each pixel. We propose a graph architecture that uses two convolutional rings based on cardiac anatomy and show that this eliminates anatomical incorrect multi-structure segmentations on the publicly available CAMUS dataset. Additionally, this work contributes with an ablation study on the graph convolutional architecture and an evaluation of clinical measurements on the clinical HUNT4 dat
&lt;/p&gt;</description></item></channel></rss>