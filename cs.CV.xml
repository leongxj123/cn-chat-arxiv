<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;DeepMIF&#65292;&#36890;&#36807;&#35774;&#35745;&#23398;&#20064;&#31995;&#32479;&#38598;&#25104;&#21333;&#35843;&#24615;&#25439;&#22833;&#65292;&#22312;&#22823;&#35268;&#27169;3D&#22320;&#22270;&#32472;&#21046;&#20013;&#20248;&#21270;&#31070;&#32463;&#21333;&#35843;&#22330;&#65292;&#36991;&#20813;&#20102;LiDAR&#27979;&#37327;&#30340;&#22024;&#26434;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.17550</link><description>&lt;p&gt;
DeepMIF: &#29992;&#20110;&#22823;&#35268;&#27169;LiDAR 3D&#22320;&#22270;&#32472;&#21046;&#30340;&#28145;&#24230;&#21333;&#35843;&#38544;&#24335;&#22330;
&lt;/p&gt;
&lt;p&gt;
DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17550
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DeepMIF&#65292;&#36890;&#36807;&#35774;&#35745;&#23398;&#20064;&#31995;&#32479;&#38598;&#25104;&#21333;&#35843;&#24615;&#25439;&#22833;&#65292;&#22312;&#22823;&#35268;&#27169;3D&#22320;&#22270;&#32472;&#21046;&#20013;&#20248;&#21270;&#31070;&#32463;&#21333;&#35843;&#22330;&#65292;&#36991;&#20813;&#20102;LiDAR&#27979;&#37327;&#30340;&#22024;&#26434;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;&#29616;&#20195;&#33719;&#21462;&#35774;&#22791;&#22914;LiDAR&#20256;&#24863;&#22120;&#65292;&#22312;&#24863;&#30693;&#30495;&#23454;&#22823;&#35268;&#27169;&#23460;&#22806;3D&#29615;&#22659;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#25104;&#31264;&#23494;&#12289;&#23436;&#25972;&#30340;3D&#22330;&#26223;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#38598;&#25104;&#20102;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#21644;&#21487;&#20248;&#21270;&#29305;&#24449;&#32593;&#26684;&#65292;&#20197;&#36924;&#36817;3D&#22330;&#26223;&#30340;&#34920;&#38754;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#27839;&#21407;&#22987;LiDAR&#20809;&#32447;&#25311;&#21512;&#26679;&#26412;&#20250;&#23548;&#33268;&#30001;&#20110;&#31232;&#30095;&#12289;&#20114;&#30456;&#30683;&#30462;&#30340;LiDAR&#27979;&#37327;&#30340;&#29305;&#24615;&#32780;&#20135;&#29983;&#22024;&#26434;&#30340;3D&#32472;&#22270;&#32467;&#26524;&#12290;&#30456;&#21453;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19981;&#20877;&#31934;&#30830;&#25311;&#21512;LiDAR&#25968;&#25454;&#65292;&#32780;&#26159;&#35753;&#32593;&#32476;&#20248;&#21270;&#22312;3D&#31354;&#38388;&#20013;&#23450;&#20041;&#30340;&#38750;&#24230;&#37327;&#21333;&#35843;&#38544;&#24335;&#22330;&#12290;&#20026;&#36866;&#24212;&#25105;&#20204;&#30340;&#22330;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23398;&#20064;&#31995;&#32479;&#65292;&#38598;&#25104;&#20102;&#19968;&#20010;&#21333;&#35843;&#24615;&#25439;&#22833;&#65292;&#20351;&#24471;&#33021;&#22815;&#20248;&#21270;&#31070;&#32463;&#21333;&#35843;&#22330;&#24182;&#21033;&#29992;&#20102;&#22823;&#35268;&#27169;3D&#22320;&#22270;&#32472;&#21046;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17550v1 Announce Type: cross  Abstract: Recently, significant progress has been achieved in sensing real large-scale outdoor 3D environments, particularly by using modern acquisition equipment such as LiDAR sensors. Unfortunately, they are fundamentally limited in their ability to produce dense, complete 3D scenes. To address this issue, recent learning-based methods integrate neural implicit representations and optimizable feature grids to approximate surfaces of 3D scenes. However, naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results due to the nature of sparse, conflicting LiDAR measurements. Instead, in this work we depart from fitting LiDAR data exactly, instead letting the network optimize a non-metric monotonic implicit field defined in 3D space. To fit our field, we design a learning system integrating a monotonicity loss that enables optimizing neural monotonic fields and leverages recent progress in large-scale 3D mapping. Our algorithm ac
&lt;/p&gt;</description></item><item><title>MolNexTR&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#22270;&#20687;&#35782;&#21035;&#30340;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#32454;&#33268;&#25552;&#21462;&#20998;&#23376;&#22270;&#20687;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#21516;&#26102;&#33021;&#22815;&#39044;&#27979;&#21407;&#23376;&#21644;&#38190;&#65292;&#29702;&#35299;&#24067;&#23616;&#35268;&#21017;&#65292;&#28789;&#27963;&#25972;&#21512;&#31526;&#21495;&#21270;&#30340;&#21270;&#23398;&#21407;&#21017;&#65292;&#24182;&#19988;&#21253;&#21547;&#22810;&#31181;&#20808;&#36827;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03691</link><description>&lt;p&gt;
MolNexTR&#65306;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#22270;&#20687;&#35782;&#21035;&#30340;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MolNexTR: A Generalized Deep Learning Model for Molecular Image Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03691
&lt;/p&gt;
&lt;p&gt;
MolNexTR&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#22270;&#20687;&#35782;&#21035;&#30340;&#36890;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#32454;&#33268;&#25552;&#21462;&#20998;&#23376;&#22270;&#20687;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#21516;&#26102;&#33021;&#22815;&#39044;&#27979;&#21407;&#23376;&#21644;&#38190;&#65292;&#29702;&#35299;&#24067;&#23616;&#35268;&#21017;&#65292;&#28789;&#27963;&#25972;&#21512;&#31526;&#21495;&#21270;&#30340;&#21270;&#23398;&#21407;&#21017;&#65292;&#24182;&#19988;&#21253;&#21547;&#22810;&#31181;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21270;&#23398;&#32467;&#26500;&#35782;&#21035;&#39046;&#22495;&#65292;&#23558;&#20998;&#23376;&#22270;&#20687;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#21644;SMILES&#23383;&#31526;&#20018;&#30340;&#20219;&#21153;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#21270;&#23398;&#25991;&#29486;&#20013;&#27969;&#34892;&#30340;&#21508;&#31181;&#32472;&#22270;&#39118;&#26684;&#21644;&#32422;&#23450;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MolNexTR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21040;&#22270;&#32467;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21512;&#24182;&#20102;ConvNext&#21644;Vision-TRansformer&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#23545;&#20998;&#23376;&#22270;&#20687;&#20013;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#26356;&#32454;&#33268;&#25552;&#21462;&#12290;MolNexTR&#21487;&#20197;&#21516;&#26102;&#39044;&#27979;&#21407;&#23376;&#21644;&#38190;&#65292;&#24182;&#29702;&#35299;&#23427;&#20204;&#30340;&#24067;&#23616;&#35268;&#21017;&#12290;&#23427;&#36824;&#25797;&#38271;&#28789;&#27963;&#22320;&#23558;&#31526;&#21495;&#21270;&#30340;&#21270;&#23398;&#21407;&#21017;&#34701;&#20837;&#20854;&#20013;&#65292;&#20197;&#35782;&#21035;&#25163;&#24615;&#24182;&#35299;&#26512;&#32553;&#20889;&#32467;&#26500;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25972;&#21512;&#20102;&#19968;&#31995;&#21015;&#20808;&#36827;&#31639;&#27861;&#65292;&#21253;&#25324;&#25913;&#36827;&#30340;&#25968;&#25454;&#22686;&#24378;&#27169;&#22359;&#12289;&#22270;&#20687;&#27745;&#26579;&#27169;&#22359;&#21644;&#21518;&#22788;&#29702;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03691v1 Announce Type: cross  Abstract: In the field of chemical structure recognition, the task of converting molecular images into graph structures and SMILES string stands as a significant challenge, primarily due to the varied drawing styles and conventions prevalent in chemical literature. To bridge this gap, we proposed MolNexTR, a novel image-to-graph deep learning model that collaborates to fuse the strengths of ConvNext, a powerful Convolutional Neural Network variant, and Vision-TRansformer. This integration facilitates a more nuanced extraction of both local and global features from molecular images. MolNexTR can predict atoms and bonds simultaneously and understand their layout rules. It also excels at flexibly integrating symbolic chemistry principles to discern chirality and decipher abbreviated structures. We further incorporate a series of advanced algorithms, including improved data augmentation module, image contamination module, and a post-processing modul
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;StyleGAN3&#27169;&#22411;&#20013;&#21028;&#21035;&#22120;&#30340;&#30149;&#24577;&#20559;&#35265;&#65292;&#23427;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#30340;&#24471;&#20998;&#20998;&#23618;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2402.09786</link><description>&lt;p&gt;
&#26816;&#26597;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21028;&#21035;&#22120;&#20013;&#30340;&#30149;&#24577;&#20559;&#35265;&#65306;&#20197;StyleGAN3&#27169;&#22411;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09786
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;StyleGAN3&#27169;&#22411;&#20013;&#21028;&#21035;&#22120;&#30340;&#30149;&#24577;&#20559;&#35265;&#65292;&#23427;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#30340;&#24471;&#20998;&#20998;&#23618;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#65292;&#24448;&#24448;&#38590;&#20197;&#34987;&#20154;&#31867;&#21306;&#20998;&#20986;&#26469;&#12290;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;StyleGAN3&#27169;&#22411;&#20013;&#30340;&#21028;&#21035;&#22120;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#31995;&#32479;&#22320;&#23545;&#24471;&#20998;&#36827;&#34892;&#20998;&#23618;&#65292;&#24182;&#19988;&#36825;&#19981;&#25104;&#27604;&#20363;&#22320;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#21028;&#21035;&#22120;&#22312;&#33394;&#24425;&#21644;&#20142;&#24230;&#26041;&#38754;&#23545;&#24863;&#30693;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#30340;&#20559;&#35265;&#65292;&#28982;&#21518;&#26816;&#26597;&#20102;&#31038;&#20250;&#24515;&#29702;&#23398;&#20013;&#20851;&#20110;&#21051;&#26495;&#21360;&#35937;&#30740;&#31350;&#20013;&#24120;&#35265;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09786v1 Announce Type: cross  Abstract: Generative adversarial networks generate photorealistic faces that are often indistinguishable by humans from real faces. We find that the discriminator in the pre-trained StyleGAN3 model, a popular GAN network, systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine the discriminator's bias for color and luminance across axes perceived race and gender; we then examine axes common in research on stereotyping in social psychology.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22266;&#20307;&#24223;&#29289;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#21355;&#26143;&#25552;&#20379;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#65292;&#36890;&#36807;&#36965;&#24863;&#22270;&#20687;&#23454;&#29616;&#20102;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#30340;&#35782;&#21035;&#12289;&#30417;&#27979;&#21644;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.09066</link><description>&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#22266;&#20307;&#24223;&#29289;&#26816;&#27979;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Solid Waste Detection in Remote Sensing Images: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22266;&#20307;&#24223;&#29289;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#21355;&#26143;&#25552;&#20379;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#65292;&#36890;&#36807;&#36965;&#24863;&#22270;&#20687;&#23454;&#29616;&#20102;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#30340;&#35782;&#21035;&#12289;&#30417;&#27979;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#34920;&#24449;&#38750;&#27861;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#23545;&#29615;&#22659;&#20445;&#25252;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#24212;&#23545;&#27745;&#26579;&#21644;&#20581;&#24247;&#21361;&#23475;&#12290;&#19981;&#24403;&#31649;&#29702;&#30340;&#22403;&#22334;&#22635;&#22475;&#22330;&#36890;&#36807;&#38632;&#27700;&#28183;&#36879;&#27745;&#26579;&#22303;&#22756;&#21644;&#22320;&#19979;&#27700;&#65292;&#23545;&#21160;&#29289;&#21644;&#20154;&#31867;&#26500;&#25104;&#23041;&#32961;&#12290;&#20256;&#32479;&#30340;&#22635;&#22475;&#22330;&#36776;&#35782;&#26041;&#27861;&#65292;&#22914;&#29616;&#22330;&#26816;&#26597;&#65292;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#36965;&#24863;&#25216;&#26415;&#26159;&#29992;&#20110;&#35782;&#21035;&#21644;&#30417;&#27979;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#30340;&#19968;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#24191;&#27867;&#35206;&#30422;&#21644;&#22810;&#27425;&#33719;&#21462;&#12290;&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#21355;&#26143;&#37197;&#22791;&#20102;&#19968;&#31995;&#21015;&#20256;&#24863;&#22120;&#21644;&#25104;&#20687;&#33021;&#21147;&#65292;&#20960;&#21313;&#24180;&#26469;&#19968;&#30452;&#25552;&#20379;&#39640;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19987;&#38376;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#36965;&#24863;&#22270;&#20687;&#25191;&#34892;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#22914;&#24223;&#29289;&#22330;&#22320;&#26816;&#27979;&#12289;&#20542;&#20498;&#22330;&#30417;&#27979;&#21644;&#36866;&#23452;&#20301;&#32622;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09066v1 Announce Type: cross Abstract: The detection and characterization of illegal solid waste disposal sites are essential for environmental protection, particularly for mitigating pollution and health hazards. Improperly managed landfills contaminate soil and groundwater via rainwater infiltration, posing threats to both animals and humans. Traditional landfill identification approaches, such as on-site inspections, are time-consuming and expensive. Remote sensing is a cost-effective solution for the identification and monitoring of solid waste disposal sites that enables broad coverage and repeated acquisitions over time. Earth Observation (EO) satellites, equipped with an array of sensors and imaging capabilities, have been providing high-resolution data for several decades. Researchers proposed specialized techniques that leverage remote sensing imagery to perform a range of tasks such as waste site detection, dumping site monitoring, and assessment of suitable locati
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#29992;&#25143;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#22810;&#20010;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#36825;&#20123;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#21306;&#20998;&#22270;&#20687;&#23545;&#24182;&#33258;&#21160;&#26631;&#27880;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2312.03187</link><description>&lt;p&gt;
FERGI&#65306;&#26469;&#33258;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#29992;&#25143;&#20559;&#22909;&#30340;&#33258;&#21160;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
FERGI: Automatic Annotation of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03187
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#29992;&#25143;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#22810;&#20010;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#36825;&#20123;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#21306;&#20998;&#22270;&#20687;&#23545;&#24182;&#33258;&#21160;&#26631;&#27880;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20351;&#29992;&#20154;&#31867;&#20559;&#22909;&#21453;&#39304;&#25968;&#25454;&#26469;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#65292;&#20154;&#31867;&#21453;&#39304;&#25910;&#38598;&#30340;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20174;&#29992;&#25143;&#30340;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#20854;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#21040;&#29983;&#25104;&#22270;&#20687;&#65288;FERGI&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#20010;&#38754;&#37096;&#36816;&#21160;&#21333;&#20803;&#65288;AUs&#65289;&#30340;&#28608;&#27963;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AU4&#65288;&#30473;&#27611;&#19979;&#22402;&#32773;&#65289;&#21453;&#26144;&#20102;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#36127;&#38754;&#35780;&#20215;&#65292;&#32780;AU12&#65288;&#22068;&#35282;&#25289;&#21160;&#32773;&#65289;&#21453;&#26144;&#20102;&#27491;&#38754;&#35780;&#20215;&#12290;&#36825;&#20004;&#32773;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#24456;&#26377;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#22320;&#20351;&#29992;&#36825;&#20123;AU&#21709;&#24212;&#23384;&#22312;&#23454;&#36136;&#24046;&#24322;&#30340;&#22270;&#20687;&#23545;&#20043;&#38388;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03187v2 Announce Type: replace-cross  Abstract: Researchers have proposed to use data of human preference feedback to fine-tune text-to-image generative models. However, the scalability of human feedback collection has been limited by its reliance on manual annotation. Therefore, we develop and test a method to automatically annotate user preferences from their spontaneous facial expression reaction to the generated images. We collect a dataset of Facial Expression Reaction to Generated Images (FERGI) and show that the activations of multiple facial action units (AUs) are highly correlated with user evaluations of the generated images. Specifically, AU4 (brow lowerer) is reflective of negative evaluations of the generated image whereas AU12 (lip corner puller) is reflective of positive evaluations. These can be useful in two ways. Firstly, we can automatically annotate user preferences between image pairs with substantial difference in these AU responses with an accuracy sig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#20808;&#39564;&#30340;&#25554;&#20837;&#24335;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.10835</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#20808;&#39564;&#30340;&#21487;&#35777;&#26126;&#30340;&#27010;&#29575;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Provable Probabilistic Imaging using Score-Based Generative Priors. (arXiv:2310.10835v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#20808;&#39564;&#30340;&#25554;&#20837;&#24335;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#21453;&#38382;&#39064;&#26102;&#65292;&#20272;&#35745;&#39640;&#36136;&#37327;&#22270;&#20687;&#24182;&#37327;&#21270;&#20854;&#19981;&#30830;&#23450;&#24615;&#26159;&#22270;&#20687;&#37325;&#24314;&#31639;&#27861;&#20013;&#30340;&#20004;&#20010;&#29702;&#24819;&#29305;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25554;&#20837;&#24335;&#33945;&#29305;&#21345;&#27931;&#65288;PMC&#65289;&#20316;&#20026;&#19968;&#31181;&#23545;&#19968;&#33324;&#21453;&#38382;&#39064;&#21487;&#33021;&#35299;&#31354;&#38388;&#36827;&#34892;&#24314;&#27169;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;PMC&#33021;&#22815;&#36890;&#36807;&#21518;&#39564;&#37319;&#26679;&#26469;&#32467;&#21512;&#20016;&#23500;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#20808;&#39564;&#36827;&#34892;&#39640;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#65292;&#24182;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;PMC&#31639;&#27861;&#65292;&#21487;&#20197;&#35270;&#20026;&#20256;&#32479;&#25554;&#20837;&#24335;&#20808;&#39564;&#65288;PnP&#65289;&#21644;&#21435;&#22122;&#27491;&#21017;&#21270;&#65288;RED&#65289;&#31639;&#27861;&#30340;&#37319;&#26679;&#27169;&#25311;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#23545;PMC&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20026;&#20004;&#31181;&#31639;&#27861;&#25552;&#20379;&#20102;&#38750;&#28176;&#36817;&#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#21363;&#20351;&#22312;&#38750;&#23545;&#25968;&#20985;&#20284;&#28982;&#21644;&#19981;&#23436;&#32654;&#24471;&#20998;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating high-quality images while also quantifying their uncertainty are two desired features in an image reconstruction algorithm for solving ill-posed inverse problems. In this paper, we propose plug-and-play Monte Carlo (PMC) as a principled framework for characterizing the space of possible solutions to a general inverse problem. PMC is able to incorporate expressive score-based generative priors for high-quality image reconstruction while also performing uncertainty quantification via posterior sampling. In particular, we introduce two PMC algorithms which can be viewed as the sampling analogues of the traditional plug-and-play priors (PnP) and regularization by denoising (RED) algorithms. We also establish a theoretical analysis for characterizing the convergence of the PMC algorithms. Our analysis provides non-asymptotic stationarity guarantees for both algorithms, even in the presence of non-log-concave likelihoods and imperfect score networks. We demonstrate the performance
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35752;&#35770;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22914;&#20309;&#22312;&#37096;&#20998;&#30417;&#30563;&#35774;&#32622;&#19979;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22797;&#26434;&#30340;&#20248;&#21270;&#26041;&#26696;&#21644;&#39640;&#26631;&#31614;&#38656;&#27714;&#32780;&#24341;&#20837;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.14382</link><description>&lt;p&gt;
&#24403;&#22810;&#20219;&#21153;&#23398;&#20064;&#36935;&#21040;&#37096;&#20998;&#30417;&#30563;&#65306;&#35745;&#31639;&#26426;&#35270;&#35273;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
When Multi-Task Learning Meets Partial Supervision: A Computer Vision Review. (arXiv:2307.14382v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35752;&#35770;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22914;&#20309;&#22312;&#37096;&#20998;&#30417;&#30563;&#35774;&#32622;&#19979;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22797;&#26434;&#30340;&#20248;&#21270;&#26041;&#26696;&#21644;&#39640;&#26631;&#31614;&#38656;&#27714;&#32780;&#24341;&#20837;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;(MTL)&#26088;&#22312;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#36164;&#28304;&#21516;&#26102;&#35745;&#31639;&#22810;&#20010;&#36755;&#20986;&#65292;&#36825;&#31181;&#23398;&#20064;&#33539;&#24335;&#26377;&#28508;&#21147;&#27604;&#20256;&#32479;&#26041;&#27861;&#22312;&#20869;&#23384;&#38656;&#27714;&#21644;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#26356;&#20302;&#12290;&#20197;&#24448;&#30340;MTL&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#19978;&#65292;&#22240;&#20026;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#21487;&#20197;&#38477;&#20302;&#36825;&#20123;&#26041;&#27861;&#23545;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;MTL&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#20248;&#21270;&#26041;&#26696;&#21644;&#26356;&#39640;&#30340;&#26631;&#31614;&#38656;&#27714;&#12290;&#26412;&#32508;&#36848;&#30528;&#37325;&#20110;MTL&#22914;&#20309;&#22312;&#19981;&#21516;&#30340;&#37096;&#20998;&#30417;&#30563;&#35774;&#32622;&#19979;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#26412;&#32508;&#36848;&#20998;&#26512;&#20102;MTL&#20256;&#32479;&#19978;&#22914;&#20309;&#20351;&#29992;&#19981;&#21516;&#30340;&#21442;&#25968;&#20849;&#20139;&#25216;&#26415;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#12290;&#20854;&#27425;&#65292;&#23427;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while exploiting their mutual relationships. By using shared resources to simultaneously calculate multiple outputs, this learning paradigm has the potential to have lower memory requirements and inference times compared to the traditional approach of using separate methods for each task. Previous work in MTL has mainly focused on fully-supervised methods, as task relationships can not only be leveraged to lower the level of data-dependency of those methods but they can also improve performance. However, MTL introduces a set of challenges due to a complex optimisation scheme and a higher labeling requirement. This review focuses on how MTL could be utilised under different partial supervision settings to address these challenges. First, this review analyses how MTL traditionally uses different parameter sharing techniques to transfer knowledge in between tasks. Second, it presents the different challenges arising fro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#65292;&#21517;&#20026;MIMIC-Diff-VQA&#65292;&#20026;&#33258;&#21160;&#21270;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#22238;&#31572;&#20851;&#20110;&#30142;&#30149;&#21644;&#22270;&#20687;&#24046;&#24322;&#30340;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20102;&#19987;&#23478;&#30693;&#35782;&#24863;&#30693;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.11986</link><description>&lt;p&gt;
Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering.&#65288;&#19987;&#23478;&#30693;&#35782;&#24863;&#30693;&#30340;&#22270;&#20687;&#21464;&#21270;&#22270;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#20851;&#27880;&#24046;&#24322;&#30340;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#65289;
&lt;/p&gt;
&lt;p&gt;
Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering. (arXiv:2307.11986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#65292;&#21517;&#20026;MIMIC-Diff-VQA&#65292;&#20026;&#33258;&#21160;&#21270;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#22238;&#31572;&#20851;&#20110;&#30142;&#30149;&#21644;&#22270;&#20687;&#24046;&#24322;&#30340;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20102;&#19987;&#23478;&#30693;&#35782;&#24863;&#30693;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20026;&#33258;&#21160;&#21270;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33016;&#37096;X&#20809;&#22270;&#20687;&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#22238;&#31572;&#20960;&#20010;&#20851;&#20110;&#30142;&#30149;&#20197;&#21450;&#26356;&#37325;&#35201;&#30340;&#26159;&#23427;&#20204;&#20043;&#38388;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#36825;&#19982;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#35786;&#26029;&#23454;&#36341;&#30456;&#19968;&#33268;&#65292;&#25918;&#23556;&#31185;&#21307;&#29983;&#22312;&#24471;&#20986;&#25253;&#21578;&#20043;&#21069;&#20250;&#23545;&#24403;&#21069;&#22270;&#20687;&#19982;&#21442;&#32771;&#22270;&#20687;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;MIMIC-Diff-VQA&#65292;&#21253;&#25324;&#26469;&#33258;164,324&#23545;&#20027;&#22270;&#20687;&#21644;&#21442;&#32771;&#22270;&#20687;&#30340;700,703&#20010;&#38382;&#39064;-&#31572;&#26696;&#37197;&#23545;&#12290;&#19982;&#29616;&#26377;&#30340;&#21307;&#23398;VQA&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#38382;&#39064;&#38024;&#23545;&#20102;&#20020;&#24202;&#19987;&#19994;&#20154;&#21592;&#20351;&#29992;&#30340;&#35780;&#20272;-&#35786;&#26029;-&#24178;&#39044;-&#35780;&#20272;&#27835;&#30103;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#30693;&#35782;&#24863;&#30693;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
To contribute to automating the medical vision-language model, we propose a novel Chest-Xray Difference Visual Question Answering (VQA) task. Given a pair of main and reference images, this task attempts to answer several questions on both diseases and, more importantly, the differences between them. This is consistent with the radiologist's diagnosis practice that compares the current image with the reference before concluding the report. We collect a new dataset, namely MIMIC-Diff-VQA, including 700,703 QA pairs from 164,324 pairs of main and reference images. Compared to existing medical VQA datasets, our questions are tailored to the Assessment-Diagnosis-Intervention-Evaluation treatment procedure used by clinical professionals. Meanwhile, we also propose a novel expert knowledge-aware graph representation learning model to address this task. The proposed baseline model leverages expert knowledge such as anatomical structure prior, semantic, and spatial knowledge to construct a mul
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#32452;&#21512;&#29983;&#25104;&#20013;&#30340;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2302.11552</link><description>&lt;p&gt;
&#20943;&#23569;&#12289;&#37325;&#22797;&#21033;&#29992;&#12289;&#22238;&#25910;&#65306;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC. (arXiv:2302.11552v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11552
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#32452;&#21512;&#29983;&#25104;&#20013;&#30340;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#25193;&#25955;&#27169;&#22411;&#38382;&#19990;&#20197;&#26469;&#65292;&#23427;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24050;&#32463;&#36805;&#36895;&#25104;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#23427;&#20204;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#23398;&#20064;&#19968;&#31995;&#21015;&#26102;&#21464;&#30340;&#23545;&#25968;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#36825;&#31181;&#35299;&#37322;&#24050;&#32463;&#28608;&#21457;&#20102;&#22522;&#20110;&#20998;&#31867;&#22120;&#21644;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#24605;&#24819;&#25104;&#20026;&#21518;&#32493;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#22312;&#36825;&#20123;&#24819;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#25968;-based&#35299;&#37322;&#65292;&#25506;&#32034;&#20102;&#29992;&#20110;&#28041;&#21450;&#32452;&#21512;&#29983;&#25104;&#21644;&#25351;&#23548;&#30340;&#26465;&#20214;&#12289;&#20462;&#25913;&#21644;&#37325;&#22797;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20026;&#20160;&#20040;&#26576;&#20123;&#31867;&#22411;&#30340;&#32452;&#21512;&#20351;&#29992;&#24403;&#21069;&#25216;&#26415;&#22833;&#36133;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#37319;&#26679;&#32773;(&#32780;&#19981;&#26159;&#27169;&#22411;)&#23545;&#27492;&#22833;&#36133;&#36127;&#26377;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#37319;&#26679;&#22120;&#65292;&#21463;MCMC&#30340;&#21551;&#21457;&#65292;&#20351;&#32452;&#21512;&#29983;&#25104;&#25104;&#21151;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#26356;&#21152;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since their introduction, diffusion models have quickly become the prevailing approach to generative modeling in many domains. They can be interpreted as learning the gradients of a time-varying sequence of log-probability density functions. This interpretation has motivated classifier-based and classifier-free guidance as methods for post-hoc control of diffusion models. In this work, we build upon these ideas using the score-based interpretation of diffusion models, and explore alternative ways to condition, modify, and reuse diffusion models for tasks involving compositional generation and guidance. In particular, we investigate why certain types of composition fail using current techniques and present a number of solutions. We conclude that the sampler (not the model) is responsible for this failure and propose new samplers, inspired by MCMC, which enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36328;&#27169;&#24577;&#36866;&#24212;&#26041;&#27861;&#65292;&#22312;&#22810;&#27169;&#24577;&#27169;&#22411;&#19979;&#21033;&#29992;&#23569;&#26679;&#26412;&#31034;&#20363;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22768;&#38899;&#65289;&#36827;&#34892;&#29399;&#30340;&#35270;&#35273;&#20998;&#31867;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.06267</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#26377;&#21161;&#20110;&#21333;&#27169;&#24577;&#65306;&#22810;&#27169;&#24577;&#27169;&#22411;&#19979;&#30340;&#20132;&#21449;&#27169;&#24577;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models. (arXiv:2301.06267v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06267
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#24577;&#36866;&#24212;&#26041;&#27861;&#65292;&#22312;&#22810;&#27169;&#24577;&#27169;&#22411;&#19979;&#21033;&#29992;&#23569;&#26679;&#26412;&#31034;&#20363;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22768;&#38899;&#65289;&#36827;&#34892;&#29399;&#30340;&#35270;&#35273;&#20998;&#31867;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#26159;&#26234;&#33021;&#20195;&#29702;&#30340;&#26680;&#24515;&#35201;&#32032;&#65292;&#20063;&#34987;&#31216;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20256;&#32479;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#20351;&#29992;&#26469;&#33258;&#21333;&#27169;&#24577;&#30340;&#23569;&#26679;&#26412;&#26679;&#26412;&#65292;&#20294;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#19981;&#36275;&#20197;&#25551;&#36848;&#25972;&#20010;&#27010;&#24565;&#31867;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#20351;&#29992;&#36328;&#27169;&#24577;&#20449;&#24687;&#39640;&#25928;&#22320;&#23398;&#20064;&#26032;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#38405;&#35835;&#20851;&#20110;&#29399;&#24182;&#21548;&#23427;&#20204;&#21536;&#21483;&#30340;&#22768;&#38899;&#26469;&#26500;&#24314;&#26356;&#22909;&#30340;&#35270;&#35273;&#29399;&#20998;&#31867;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#26159;&#22266;&#26377;&#30340;&#36328;&#27169;&#24577;&#30340;&#29305;&#24615;&#65292;&#23558;&#19981;&#21516;&#30340;&#27169;&#24577;&#26144;&#23556;&#21040;&#30456;&#21516;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36328;&#27169;&#24577;&#36866;&#24212;&#26041;&#27861;&#65292;&#20174;&#36328;&#36234;&#19981;&#21516;&#27169;&#24577;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#36890;&#36807;&#23558;&#31867;&#21517;&#37325;&#26032;&#29992;&#20316;&#39069;&#22806;&#30340;&#19968;&#27425;&#24615;&#35757;&#32451;&#26679;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26497;&#20854;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to quickly learn a new task with minimal instruction - known as few-shot learning - is a central aspect of intelligent agents. Classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufficient to characterize an entire concept class. In contrast, humans use cross-modal information to learn new concepts efficiently. In this work, we demonstrate that one can indeed build a better ${\bf visual}$ dog classifier by ${\bf read}$ing about dogs and ${\bf listen}$ing to them bark. To do so, we exploit the fact that recent multimodal foundation models such as CLIP are inherently cross-modal, mapping different modalities to the same representation space. Specifically, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning different modalities. By repurposing class names as additional one-shot training samples, we achieve SOTA results with an embarrassingly simple linear classifier for visi
&lt;/p&gt;</description></item></channel></rss>