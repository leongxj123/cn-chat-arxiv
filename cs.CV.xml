<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>ContactHandover&#26159;&#19968;&#20010;&#26426;&#22120;&#20154;&#21521;&#20154;&#31867;&#36882;&#36865;&#29289;&#20307;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25509;&#35302;&#24341;&#23548;&#30340;&#25235;&#21462;&#21644;&#29289;&#20307;&#36882;&#36865;&#38454;&#27573;&#26469;&#23454;&#29616;&#25104;&#21151;&#30340;&#29289;&#20307;&#36882;&#36865;&#12290;</title><link>https://arxiv.org/abs/2404.01402</link><description>&lt;p&gt;
ContactHandover: &#25509;&#35302;&#24341;&#23548;&#30340;&#26426;&#22120;&#20154;&#21521;&#20154;&#31867;&#36882;&#36865;&#29289;&#20307;
&lt;/p&gt;
&lt;p&gt;
ContactHandover: Contact-Guided Robot-to-Human Object Handover
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01402
&lt;/p&gt;
&lt;p&gt;
ContactHandover&#26159;&#19968;&#20010;&#26426;&#22120;&#20154;&#21521;&#20154;&#31867;&#36882;&#36865;&#29289;&#20307;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25509;&#35302;&#24341;&#23548;&#30340;&#25235;&#21462;&#21644;&#29289;&#20307;&#36882;&#36865;&#38454;&#27573;&#26469;&#23454;&#29616;&#25104;&#21151;&#30340;&#29289;&#20307;&#36882;&#36865;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#21521;&#20154;&#31867;&#36882;&#36865;&#29289;&#20307;&#26159;&#35768;&#22810;&#20154;&#26426;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#25104;&#21151;&#30340;&#36882;&#36865;&#38656;&#35201;&#26426;&#22120;&#20154;&#20445;&#25345;&#23545;&#29289;&#20307;&#30340;&#31283;&#23450;&#25235;&#21462;&#65292;&#21516;&#26102;&#30830;&#20445;&#20154;&#31867;&#20197;&#19968;&#31181;&#33258;&#28982;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#24335;&#25509;&#25910;&#29289;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ContactHandover&#65292;&#36825;&#26159;&#19968;&#20010;&#26426;&#22120;&#20154;&#21521;&#20154;&#31867;&#36882;&#36865;&#29289;&#20307;&#30340;&#31995;&#32479;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#25509;&#35302;&#24341;&#23548;&#30340;&#25235;&#21462;&#38454;&#27573;&#21644;&#29289;&#20307;&#36882;&#36865;&#38454;&#27573;&#12290;&#22312;&#25235;&#21462;&#38454;&#27573;&#65292;ContactHandover&#39044;&#27979;&#26426;&#22120;&#20154;&#30340;6&#33258;&#30001;&#24230;&#25235;&#21462;&#23039;&#21183;&#21644;&#20154;&#31867;&#25509;&#35302;&#28857;&#22312;&#29289;&#20307;&#19978;&#30340;3D&#21487;&#20379;&#24615;&#22270;&#12290;&#26426;&#22120;&#20154;&#30340;&#25235;&#21462;&#23039;&#21183;&#36890;&#36807;&#24809;&#32602;&#37027;&#20123;&#38459;&#30861;&#20154;&#31867;&#25509;&#35302;&#28857;&#30340;&#23039;&#21183;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#24182;&#25191;&#34892;&#25490;&#21517;&#26368;&#39640;&#30340;&#25235;&#21462;&#12290;&#22312;&#36882;&#36865;&#38454;&#27573;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#38752;&#36817;&#20154;&#31867;&#30340;&#25509;&#35302;&#28857;&#24182;&#26368;&#23567;&#21270;&#20154;&#31867;&#25163;&#33218;&#20851;&#33410;&#25197;&#30697;&#21644;&#20301;&#31227;&#26469;&#35745;&#31639;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#21183;&#12290;&#25105;&#20204;&#22312;27&#31181;&#19981;&#21516;&#23478;&#29992;&#29289;&#21697;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#65292;&#24182;&#23637;&#31034;&#20102;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01402v1 Announce Type: cross  Abstract: Robot-to-human object handover is an important step in many human robot collaboration tasks. A successful handover requires the robot to maintain a stable grasp on the object while making sure the human receives the object in a natural and easy-to-use manner. We propose ContactHandover, a robot to human handover system that consists of two phases: a contact-guided grasping phase and an object delivery phase. During the grasping phase, ContactHandover predicts both 6-DoF robot grasp poses and a 3D affordance map of human contact points on the object. The robot grasp poses are reranked by penalizing those that block human contact points, and the robot executes the highest ranking grasp. During the delivery phase, the robot end effector pose is computed by maximizing human contact points close to the human while minimizing the human arm joint torques and displacements. We evaluate our system on 27 diverse household objects and show that o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;BCM&#65289;&#65292;&#23398;&#20064;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#23454;&#29616;&#27839;&#30528;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#21069;&#21521;&#21644;&#21518;&#21521;&#36941;&#21382;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32479;&#19968;&#20102;&#29983;&#25104;&#21644;&#32534;&#36753;&#22270;&#20687;&#31561;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.18035</link><description>&lt;p&gt;
&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Consistency Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18035
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;BCM&#65289;&#65292;&#23398;&#20064;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#23454;&#29616;&#27839;&#30528;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#21069;&#21521;&#21644;&#21518;&#21521;&#36941;&#21382;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32479;&#19968;&#20102;&#29983;&#25104;&#21644;&#32534;&#36753;&#22270;&#20687;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#36890;&#36807;&#36845;&#20195;&#21435;&#22122;&#19968;&#20010;&#38543;&#26426;&#21521;&#37327;&#33021;&#22815;&#29983;&#25104;&#38750;&#24120;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#36825;&#20010;&#36807;&#31243;&#23545;&#24212;&#20110;&#27839;&#30528;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;PF ODE&#65289;&#31227;&#21160;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;DMs&#36824;&#21487;&#20197;&#36890;&#36807;&#27839;&#30528;PF ODE&#21521;&#21518;&#31227;&#21160;&#23558;&#36755;&#20837;&#22270;&#20687;&#36716;&#25442;&#20026;&#22122;&#22768;&#65292;&#36825;&#26159;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#25554;&#20540;&#21644;&#22270;&#20687;&#32534;&#36753;&#65289;&#30340;&#20851;&#38190;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36807;&#31243;&#30340;&#36845;&#20195;&#24615;&#36136;&#38480;&#21046;&#20102;&#20854;&#36895;&#24230;&#65292;&#38459;&#30861;&#20102;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CMs&#65289;&#24050;&#32463;&#20986;&#29616;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#36890;&#36807;&#36817;&#20284;PF ODE&#30340;&#31215;&#20998;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#38656;&#35201;&#36845;&#20195;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#26174;&#24335;ODE&#27714;&#35299;&#22120;&#20351;&#24471;&#21453;&#28436;&#36807;&#31243;&#22797;&#26434;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;BCM&#65289;&#65292;&#23398;&#20064;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#27839;&#30528;PF ODE&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#36941;&#21382;&#65292;&#26377;&#25928;&#22320;&#32479;&#19968;&#29983;&#25104;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18035v1 Announce Type: new  Abstract: Diffusion models (DMs) are capable of generating remarkably high-quality samples by iteratively denoising a random vector, a process that corresponds to moving along the probability flow ordinary differential equation (PF ODE). Interestingly, DMs can also invert an input image to noise by moving backward along the PF ODE, a key operation for downstream tasks such as interpolation and image editing. However, the iterative nature of this process restricts its speed, hindering its broader application. Recently, Consistency Models (CMs) have emerged to address this challenge by approximating the integral of the PF ODE, thereby bypassing the need to iterate. Yet, the absence of an explicit ODE solver complicates the inversion process. To resolve this, we introduce the Bidirectional Consistency Model (BCM), which learns a single neural network that enables both forward and backward traversal along the PF ODE, efficiently unifying generation an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#20851;&#33410;&#23545;&#35937;&#30340;&#20581;&#22766;&#24863;&#30693;&#21644;&#25805;&#20316;&#26694;&#26550;RPMArt&#65292;&#20027;&#35201;&#36129;&#29486;&#26159;&#33021;&#22815;&#31283;&#20581;&#22320;&#39044;&#27979;&#20851;&#33410;&#21442;&#25968;&#21644;&#21487;&#20449;&#28857;&#30340;RoArtNet&#12290;</title><link>https://arxiv.org/abs/2403.16023</link><description>&lt;p&gt;
RPMArt&#65306;&#38754;&#21521;&#20851;&#33410;&#23545;&#35937;&#30340;&#20581;&#22766;&#24863;&#30693;&#21644;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
RPMArt: Towards Robust Perception and Manipulation for Articulated Objects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16023
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#20851;&#33410;&#23545;&#35937;&#30340;&#20581;&#22766;&#24863;&#30693;&#21644;&#25805;&#20316;&#26694;&#26550;RPMArt&#65292;&#20027;&#35201;&#36129;&#29486;&#26159;&#33021;&#22815;&#31283;&#20581;&#22320;&#39044;&#27979;&#20851;&#33410;&#21442;&#25968;&#21644;&#21487;&#20449;&#28857;&#30340;RoArtNet&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#33410;&#23545;&#35937;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#24456;&#24120;&#35265;&#12290;&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#26469;&#35828;&#65292;&#26426;&#22120;&#20154;&#33021;&#22815;&#34920;&#29616;&#20986;&#23545;&#20851;&#33410;&#23545;&#35937;&#30340;&#20581;&#22766;&#24863;&#30693;&#21644;&#25805;&#20316;&#25216;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20851;&#33410;&#23545;&#35937;&#26041;&#27861;&#19981;&#22815;&#35299;&#20915;&#28857;&#20113;&#20013;&#30340;&#22122;&#22768;&#38382;&#39064;&#65292;&#38590;&#20197;&#24357;&#21512;&#27169;&#25311;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#20851;&#33410;&#23545;&#35937;&#30340;&#20581;&#22766;&#24863;&#30693;&#21644;&#25805;&#20316;&#30340;&#26694;&#26550;&#65288;RPMArt&#65289;&#65292;&#35813;&#26694;&#26550;&#23398;&#20064;&#22914;&#20309;&#20174;&#22024;&#26434;&#30340;&#28857;&#20113;&#20013;&#20272;&#35745;&#20851;&#33410;&#21442;&#25968;&#24182;&#25805;&#20316;&#20851;&#33410;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#20010;&#20581;&#22766;&#20851;&#33410;&#32593;&#32476;&#65288;RoArtNet&#65289;&#65292;&#36890;&#36807;&#23616;&#37096;&#29305;&#24449;&#23398;&#20064;&#21644;&#28857;&#20803;&#32452;&#25237;&#31080;&#33021;&#22815;&#31283;&#20581;&#22320;&#39044;&#27979;&#20851;&#33410;&#21442;&#25968;&#21644;&#21487;&#20449;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20851;&#33410;&#24863;&#30693;&#20998;&#31867;&#26041;&#26696;&#26469;&#22686;&#24378;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16023v1 Announce Type: cross  Abstract: Articulated objects are commonly found in daily life. It is essential that robots can exhibit robust perception and manipulation skills for articulated objects in real-world robotic applications. However, existing methods for articulated objects insufficiently address noise in point clouds and struggle to bridge the gap between simulation and reality, thus limiting the practical deployment in real-world scenarios. To tackle these challenges, we propose a framework towards Robust Perception and Manipulation for Articulated Objects (RPMArt), which learns to estimate the articulation parameters and manipulate the articulation part from the noisy point cloud. Our primary contribution is a Robust Articulation Network (RoArtNet) that is able to predict both joint parameters and affordable points robustly by local feature learning and point tuple voting. Moreover, we introduce an articulation-aware classification scheme to enhance its ability
&lt;/p&gt;</description></item><item><title>LSKNet&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#39592;&#24178;&#65292;&#33021;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#31243;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2403.11735</link><description>&lt;p&gt;
LSKNet&#65306;&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#30340;&#36731;&#37327;&#32423;&#22522;&#30784;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
LSKNet: A Foundation Lightweight Backbone for Remote Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11735
&lt;/p&gt;
&lt;p&gt;
LSKNet&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#39592;&#24178;&#65292;&#33021;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#31243;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#23545;&#19979;&#28216;&#20219;&#21153;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#22823;&#37327;&#30740;&#31350;&#33268;&#21147;&#20110;&#36965;&#24863;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#24573;&#35270;&#20102;&#23884;&#20837;&#22312;&#36965;&#24863;&#22330;&#26223;&#20013;&#30340;&#23453;&#36149;&#20808;&#39564;&#30693;&#35782;&#12290;&#36825;&#20123;&#20808;&#39564;&#30693;&#35782;&#21487;&#33021;&#20250;&#24456;&#26377;&#29992;&#65292;&#22240;&#20026;&#22312;&#27809;&#26377;&#21442;&#32771;&#36275;&#22815;&#38271;&#31243;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#36965;&#24863;&#23545;&#35937;&#21487;&#33021;&#20250;&#34987;&#38169;&#35823;&#35782;&#21035;&#65292;&#32780;&#36825;&#21487;&#20197;&#22240;&#19981;&#21516;&#23545;&#35937;&#32780;&#24322;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#36825;&#20123;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#65288;LSKNet&#65289;&#39592;&#24178;&#32593;&#32476;&#12290;LSKNet&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20808;&#21069;&#23578;&#26410;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#25506;&#32034;&#36807;&#22823;&#22411;&#21644;&#36873;&#25321;&#24615;&#26680;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#27809;&#26377;&#22826;&#22810;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11735v1 Announce Type: cross  Abstract: Remote sensing images pose distinct challenges for downstream tasks due to their inherent complexity. While a considerable amount of research has been dedicated to remote sensing classification, object detection and semantic segmentation, most of these studies have overlooked the valuable prior knowledge embedded within remote sensing scenarios. Such prior knowledge can be useful because remote sensing objects may be mistakenly recognized without referencing a sufficiently long-range context, which can vary for different objects. This paper considers these priors and proposes a lightweight Large Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various objects in remote sensing scenarios. To our knowledge, large and selective kernel mechanisms have not been previously explored in remote sensing images. Without bells and whistles, our lightw
&lt;/p&gt;</description></item><item><title>SARDet-100K&#26159;&#31532;&#19968;&#20010;COCO&#32423;&#21035;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035;SAR&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;SAR&#29289;&#20307;&#26816;&#27979;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#26174;&#33879;&#24046;&#24322;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.06534</link><description>&lt;p&gt;
SARDet-100K: &#38754;&#21521;&#22823;&#35268;&#27169;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798; SAR &#29289;&#20307;&#26816;&#27979;&#30340;&#24320;&#28304;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06534
&lt;/p&gt;
&lt;p&gt;
SARDet-100K&#26159;&#31532;&#19968;&#20010;COCO&#32423;&#21035;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035;SAR&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;SAR&#29289;&#20307;&#26816;&#27979;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#26174;&#33879;&#24046;&#24322;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#29289;&#20307;&#26816;&#27979;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#22240;&#20854;&#19981;&#21487;&#26367;&#20195;&#30340;&#20840;&#22825;&#20505;&#25104;&#20687;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#38754;&#20020;&#30528;&#26377;&#38480;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#20027;&#35201;&#21253;&#21547; &lt;2K &#24352;&#22270;&#20687;&#65292;&#19988;&#20165;&#21253;&#21547;&#21333;&#31867;&#21035;&#29289;&#20307;&#65289;&#21644;&#28304;&#20195;&#30721;&#19981;&#21487;&#35775;&#38382;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#38024;&#23545;&#22823;&#35268;&#27169; SAR &#29289;&#20307;&#26816;&#27979;&#30340;&#24320;&#28304;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598; SARDet-100K &#32467;&#26524;&#26159;&#23545; 10 &#20010;&#29616;&#26377; SAR &#26816;&#27979;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#20837;&#35843;&#30740;&#12289;&#25910;&#38598;&#21644;&#26631;&#20934;&#21270;&#30340;&#20135;&#29289;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SARDet-100K &#26159;&#26377;&#21490;&#20197;&#26469;&#31532;&#19968;&#20010;&#36798;&#21040; COCO &#27700;&#24179;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035; SAR &#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#20973;&#20511;&#36825;&#19968;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#24182;&#25581;&#31034;&#20102; SAR &#29289;&#20307;&#26816;&#27979;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06534v1 Announce Type: cross  Abstract: Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising &lt;2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining
&lt;/p&gt;</description></item><item><title>VidProM&#26159;&#19968;&#20010;&#21253;&#21547;167&#19975;&#20010;&#29420;&#29305;&#25991;&#26412;&#21040;&#35270;&#39057;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#23545;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#24102;&#26469;&#20102;&#26032;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#25581;&#31034;&#20102;&#30495;&#23454;&#29992;&#25143;&#25552;&#31034;&#23545;&#35270;&#39057;&#29983;&#25104;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06098</link><description>&lt;p&gt;
VidProM&#65306;&#19968;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;&#30495;&#23454;&#21363;&#26102;&#22270;&#24211;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06098
&lt;/p&gt;
&lt;p&gt;
VidProM&#26159;&#19968;&#20010;&#21253;&#21547;167&#19975;&#20010;&#29420;&#29305;&#25991;&#26412;&#21040;&#35270;&#39057;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#23545;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#24102;&#26469;&#20102;&#26032;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#25581;&#31034;&#20102;&#30495;&#23454;&#29992;&#25143;&#25552;&#31034;&#23545;&#35270;&#39057;&#29983;&#25104;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sora&#30340;&#21040;&#26469;&#26631;&#24535;&#30528;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#24102;&#26469;&#20102;&#35270;&#39057;&#29983;&#25104;&#21644;&#28508;&#22312;&#24212;&#29992;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;Sora&#20197;&#21450;&#20854;&#20182;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#39640;&#24230;&#20381;&#36182;&#25552;&#31034;&#65292;&#20294;&#30446;&#21069;&#23578;&#27809;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#21253;&#21547;&#25991;&#26412;&#21040;&#35270;&#39057;&#25552;&#31034;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;VidProM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#30001;167&#19975;&#20010;&#26469;&#33258;&#30495;&#23454;&#29992;&#25143;&#30340;&#29420;&#29305;&#25991;&#26412;&#21040;&#35270;&#39057;&#25552;&#31034;&#32452;&#25104;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#30001;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;669&#19975;&#20010;&#35270;&#39057;&#20197;&#21450;&#19968;&#20123;&#30456;&#20851;&#25968;&#25454;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#36825;&#19968;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#31574;&#23637;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#20010;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;VidProM&#19982;DiffusionDB&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#21518;&#32773;&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#25552;&#31034;&#22270;&#24211;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#26032;&#25552;&#31034;&#25968;&#25454;&#38598;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06098v1 Announce Type: cross  Abstract: The arrival of Sora marks a new era for text-to-video diffusion models, bringing significant advancements in video generation and potential applications. However, Sora, as well as other text-to-video diffusion models, highly relies on the prompts, and there is no publicly available dataset featuring a study of text-to-video prompts. In this paper, we introduce VidProM, the first large-scale dataset comprising 1.67 million unique text-to-video prompts from real users. Additionally, the dataset includes 6.69 million videos generated by four state-of-the-art diffusion models and some related data. We initially demonstrate the curation of this large-scale dataset, which is a time-consuming and costly process. Subsequently, we show how the proposed VidProM differs from DiffusionDB, a large-scale prompt-gallery dataset for image generation. Based on the analysis of these prompts, we identify the necessity for a new prompt dataset specificall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25552;&#31034;&#27169;&#22359;&#65288;APM&#65289;&#65292;&#20026;SAM&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#33258;&#36866;&#24212;&#25552;&#31034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;SAM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04164</link><description>&lt;p&gt;
ProMISe: &#20351;&#29992;SAM&#36827;&#34892;&#21487;&#25552;&#31034;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
ProMISe: Promptable Medical Image Segmentation using SAM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25552;&#31034;&#27169;&#22359;&#65288;APM&#65289;&#65292;&#20026;SAM&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#20102;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#33258;&#36866;&#24212;&#25552;&#31034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;SAM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25552;&#20986;&#20102;Segment Anything Model (SAM)&#30340;&#24314;&#35758;&#65292;&#23545;SAM&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;(MIS)&#30340;&#24494;&#35843;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SAM&#27169;&#22411;&#30340;&#35268;&#27169;&#36739;&#22823;&#65292;&#33258;&#28982;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#22522;&#20110;&#24494;&#35843;&#30340;&#31574;&#30053;&#25104;&#26412;&#39640;&#65292;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#12289;&#29305;&#24449;&#25439;&#20260;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#36890;&#36807;&#24494;&#35843;&#31574;&#30053;&#23558;SAM&#36716;&#31227;&#21040;&#29305;&#23450;&#39046;&#22495;MIS&#30340;&#26041;&#27861;&#31105;&#29992;&#20102;&#27169;&#22411;&#30340;&#25552;&#31034;&#33021;&#21147;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#20854;&#20351;&#29992;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#25552;&#31034;&#27169;&#22359;&#65288;APM&#65289;&#65292;&#20026;SAM&#22522;&#30784;&#27169;&#22411;&#22312;&#30446;&#26631;&#22495;&#20013;&#25552;&#20379;&#20102;&#20855;&#26377;&#27431;&#20960;&#37324;&#24503;&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#26679;&#30340;&#33258;&#36866;&#24212;&#25552;&#31034;&#26174;&#33879;&#25552;&#39640;&#20102;SAM&#22312;MIS&#20013;&#38750;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22686;&#37327;&#27169;&#24335;&#31227;&#20301;&#65288;IPS&#65289;&#30340;&#26032;&#22411;&#38750;&#20405;&#20837;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;SAM&#35843;&#25972;&#21040;&#29305;&#23450;&#21307;&#30103;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04164v1 Announce Type: cross  Abstract: With the proposal of the Segment Anything Model (SAM), fine-tuning SAM for medical image segmentation (MIS) has become popular. However, due to the large size of the SAM model and the significant domain gap between natural and medical images, fine-tuning-based strategies are costly with potential risk of instability, feature damage and catastrophic forgetting. Furthermore, some methods of transferring SAM to a domain-specific MIS through fine-tuning strategies disable the model's prompting capability, severely limiting its utilization scenarios. In this paper, we propose an Auto-Prompting Module (APM), which provides SAM-based foundation model with Euclidean adaptive prompts in the target domain. Our experiments demonstrate that such adaptive prompts significantly improve SAM's non-fine-tuned performance in MIS. In addition, we propose a novel non-invasive method called Incremental Pattern Shifting (IPS) to adapt SAM to specific medica
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Vision Mamba&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;SSM&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03849</link><description>&lt;p&gt;
MedMamba: &#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;Vision Mamba
&lt;/p&gt;
&lt;p&gt;
MedMamba: Vision Mamba for Medical Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03849
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Vision Mamba&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;SSM&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#38750;&#24120;&#22522;&#30784;&#21644;&#20851;&#38190;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;CNN&#21644;Transformer&#30340;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#31867;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;CNN&#22312;&#38271;&#36317;&#31163;&#24314;&#27169;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#26080;&#27861;&#26377;&#25928;&#25552;&#21462;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#65292;&#32780;Transformers&#21463;&#21040;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#38459;&#30861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;Mamba&#34920;&#31034;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#21487;&#20197;&#39640;&#25928;&#22320;&#24314;&#27169;&#38271;&#36317;&#31163;&#20132;&#20114;&#20316;&#29992;&#21516;&#26102;&#20445;&#25345;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;Vision Mamba&#65288;MedMamba&#65289;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;Conv-SSM&#27169;&#22359;&#65292;&#23558;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#19982;SSM&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#20026;&#20102;&#23637;&#31034;MedMamba&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03849v1 Announce Type: cross  Abstract: Medical image classification is a very fundamental and crucial task in the field of computer vision. These years, CNN-based and Transformer-based models are widely used in classifying various medical images. Unfortunately, The limitation of CNNs in long-range modeling capabilities prevent them from effectively extracting fine-grained features in medical images , while Transformers are hampered by their quadratic computational complexity. Recent research has shown that the state space model (SSM) represented by Mamba can efficiently model long-range interactions while maintaining linear computational complexity. Inspired by this, we propose Vision Mamba for medical image classification (MedMamba). More specifically, we introduce a novel Conv-SSM module, which combines the local feature extraction ability of convolutional layers with the ability of SSM to capture long-range dependency. To demonstrate the potential of MedMamba, we conduct
&lt;/p&gt;</description></item><item><title>&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65288;MMoE&#65289;&#23618;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#38024;&#23545;&#35270;&#35273;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#19987;&#23478;&#29305;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#31163;&#25955;&#19987;&#23478;&#36335;&#30001;&#21644;&#36807;&#39640;&#25512;&#29702;&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.12550</link><description>&lt;p&gt;
&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65306;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#19987;&#23478;&#29305;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12550
&lt;/p&gt;
&lt;p&gt;
&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65288;MMoE&#65289;&#23618;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#38024;&#23545;&#35270;&#35273;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#19987;&#23478;&#29305;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#31163;&#25955;&#19987;&#23478;&#36335;&#30001;&#21644;&#36807;&#39640;&#25512;&#29702;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#33539;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#23558;&#38590;&#20197;&#29702;&#35299;&#30340;&#23494;&#38598;&#23618;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#27169;&#22359;&#21270;&#30340;&#35745;&#31639;&#65292;&#36890;&#24120;&#26356;&#26131;&#20110;&#20154;&#31867;&#35299;&#37322;&#12289;&#35843;&#35797;&#21644;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#25193;&#23637;&#19987;&#23478;&#25968;&#37327;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20197;&#23454;&#29616;&#36275;&#22815;&#31934;&#32454;&#30340;&#19987;&#19994;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65288;MMoE&#65289;&#23618;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#25918;&#22312;&#35270;&#35273;&#27169;&#22411;&#19978;&#12290;MMoE&#23618;&#23436;&#20840;&#20197;&#22240;&#24335;&#21270;&#24418;&#24335;&#23545;&#24222;&#22823;&#30340;&#26435;&#37325;&#24352;&#37327;&#36827;&#34892;&#38544;&#24335;&#35745;&#31639;&#12290;&#22240;&#27492;&#65292;MMoEs&#26082;&#36991;&#20813;&#20102;&#22312;&#27969;&#34892;&#30340;&#8220;&#31232;&#30095;&#8221;MoE&#27169;&#22411;&#20013;&#31163;&#25955;&#19987;&#23478;&#36335;&#30001;&#25152;&#36896;&#25104;&#30340;&#38382;&#39064;&#65292;&#21448;&#19981;&#20250;&#24341;&#36215;&#8220;&#36719;&#8221;MoE&#26367;&#20195;&#26041;&#26696;&#20013;&#36807;&#39640;&#30340;&#25512;&#29702;&#26102;&#38388;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#21453;&#20107;&#23454;&#24178;&#39044;&#65292;&#25552;&#20379;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#25193;&#23637;MMoE&#23618;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12550v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) paradigm provides a powerful way to decompose inscrutable dense layers into smaller, modular computations often more amenable to human interpretation, debugging, and editability. A major problem however lies in the computational cost of scaling the number of experts to achieve sufficiently fine-grained specialization. In this paper, we propose the Multilinear Mixutre of Experts (MMoE) layer to address this, focusing on vision models. MMoE layers perform an implicit computation on prohibitively large weight tensors entirely in factorized form. Consequently, MMoEs both (1) avoid the issues incurred through the discrete expert routing in the popular 'sparse' MoE models, yet (2) do not incur the restrictively high inference-time costs of 'soft' MoE alternatives. We present both qualitative and quantitative evidence (through visualization and counterfactual interventions respectively) that scaling MMoE layers wh
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24212;&#29992;&#36741;&#21161;&#25439;&#22833;&#20248;&#21270;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26426;&#22120;&#22270;&#20687;&#32534;&#30721;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#23454;&#29616;&#26174;&#33879;&#30340;&#36895;&#29575;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.08267</link><description>&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#32534;&#30721;&#22120;&#21644;&#36741;&#21161;&#25439;&#22833;&#25913;&#36827;&#26426;&#22120;&#22270;&#20687;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Improving Image Coding for Machines through Optimizing Encoder via Auxiliary Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08267
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24212;&#29992;&#36741;&#21161;&#25439;&#22833;&#20248;&#21270;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26426;&#22120;&#22270;&#20687;&#32534;&#30721;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#23454;&#29616;&#26174;&#33879;&#30340;&#36895;&#29575;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#22270;&#20687;&#32534;&#30721;&#65288;ICM&#65289;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#27169;&#22411;&#32780;&#19981;&#26159;&#20154;&#30524;&#35270;&#35273;&#26469;&#21387;&#32553;&#22270;&#20687;&#20197;&#20379;&#26426;&#22120;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#22312;ICM&#20013;&#65292;&#32534;&#30721;&#22120;&#35782;&#21035;&#21644;&#21387;&#32553;&#23545;&#20110;&#26426;&#22120;&#35782;&#21035;&#20219;&#21153;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#23398;&#20064;&#22411;ICM&#26377;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;&#22522;&#20110;&#20219;&#21153;&#25439;&#22833;&#30340;&#21387;&#32553;&#27169;&#22411;&#20248;&#21270;&#21644;&#22522;&#20110;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#30340;&#27604;&#29305;&#20998;&#37197;&#12290;&#36825;&#20123;&#26041;&#27861;&#20026;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#35782;&#21035;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#35782;&#21035;&#27169;&#22411;&#24456;&#28145;&#26102;&#65292;&#20351;&#29992;&#20219;&#21153;&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#21464;&#24471;&#22256;&#38590;&#65292;&#32780;&#22522;&#20110;ROI&#30340;&#26041;&#27861;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#36890;&#24120;&#20250;&#22686;&#21152;&#39069;&#22806;&#24320;&#38144;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#22411;ICM&#27169;&#22411;&#30340;&#26032;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#32534;&#30721;&#22120;&#24212;&#29992;&#36741;&#21161;&#25439;&#22833;&#26469;&#25552;&#39640;&#20854;&#35782;&#21035;&#33021;&#21147;&#21644;&#36895;&#29575;-&#22833;&#30495;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;27.7%&#21644;20.3%&#30340;Bjontegaard Delta&#36895;&#29575;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image coding for machines (ICM) aims to compress images for machine analysis using recognition models rather than human vision. Hence, in ICM, it is important for the encoder to recognize and compress the information necessary for the machine recognition task. There are two main approaches in learned ICM; optimization of the compression model based on task loss, and Region of Interest (ROI) based bit allocation. These approaches provide the encoder with the recognition capability. However, optimization with task loss becomes difficult when the recognition model is deep, and ROI-based methods often involve extra overhead during evaluation. In this study, we propose a novel training method for learned ICM models that applies auxiliary loss to the encoder to improve its recognition capability and rate-distortion performance. Our method achieves Bjontegaard Delta rate improvements of 27.7% and 20.3% in object detection and semantic segmentation tasks, compared to the conventional training 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35282;&#33853;&#26696;&#20363;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#22686;&#24378;&#29289;&#20214;&#23398;&#20064;&#22120;&#65288;MENOL&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20943;&#23567;&#24050;&#30693;&#31867;&#21644;&#26410;&#30693;&#31867;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#24341;&#20837;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#26032;&#31867;&#21035;&#30340;&#21484;&#22238;&#29575;&#65292;&#24182;&#38477;&#20302;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.02026</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#20013;&#35282;&#33853;&#26696;&#20363;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#22686;&#24378;&#29289;&#20214;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multimodal-Enhanced Objectness Learner for Corner Case Detection in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35282;&#33853;&#26696;&#20363;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#22686;&#24378;&#29289;&#20214;&#23398;&#20064;&#22120;&#65288;MENOL&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20943;&#23567;&#24050;&#30693;&#31867;&#21644;&#26410;&#30693;&#31867;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#24341;&#20837;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#26032;&#31867;&#21035;&#30340;&#21484;&#22238;&#29575;&#65292;&#24182;&#38477;&#20302;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#20307;&#26816;&#27979;&#30340;&#20808;&#21069;&#24037;&#20316;&#20013;&#65292;&#23553;&#38381;&#22330;&#26223;&#19979;&#30340;&#20934;&#30830;&#29575;&#36739;&#39640;&#65292;&#20294;&#22312;&#24320;&#25918;&#19990;&#30028;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#24182;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#20854;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#19990;&#30028;&#38382;&#39064;&#26159;&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;&#35282;&#33853;&#26696;&#20363;&#26816;&#27979;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#22312;&#36825;&#20123;&#26696;&#20363;&#20013;&#34920;&#29616;&#22256;&#38590;&#65292;&#36807;&#24230;&#20381;&#36182;&#35270;&#35273;&#22806;&#35266;&#65292;&#20855;&#26377;&#36739;&#24046;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20943;&#23567;&#24050;&#30693;&#31867;&#21644;&#26410;&#30693;&#31867;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#24341;&#20837;&#22810;&#27169;&#24577;&#22686;&#24378;&#30340;&#29289;&#20214;&#23398;&#20064;&#22120;&#30340;&#27010;&#24565;&#12290;&#20511;&#21161;&#35270;&#35273;&#20013;&#24515;&#21644;&#22270;&#20687;-&#25991;&#26412;&#20004;&#31181;&#24418;&#24335;&#65292;&#25105;&#20204;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#23558;&#29289;&#20214;&#23398;&#20064;&#22120;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#31867;&#21035;&#24863;&#30693;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;&#29992;&#20110;&#35282;&#33853;&#26696;&#20363;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#22686;&#24378;&#29289;&#20214;&#23398;&#20064;&#22120;&#65288;MENOL&#65289;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#26032;&#31867;&#21035;&#30340;&#21484;&#22238;&#29575;&#65292;&#24182;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#12290;&#22312;&#20165;&#20351;&#29992;5100&#20010;&#26631;&#31614;&#35757;&#32451;&#22270;&#20687;&#30340;CODA-val&#25968;&#25454;&#38598;&#19978;&#65292;MENOL&#23454;&#29616;&#20102;76.6%&#30340;mAR-corner&#21644;79.8%&#30340;mAR-agnostic&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous works on object detection have achieved high accuracy in closed-set scenarios, but their performance in open-world scenarios is not satisfactory. One of the challenging open-world problems is corner case detection in autonomous driving. Existing detectors struggle with these cases, relying heavily on visual appearance and exhibiting poor generalization ability. In this paper, we propose a solution by reducing the discrepancy between known and unknown classes and introduce a multimodal-enhanced objectness notion learner. Leveraging both vision-centric and image-text modalities, our semi-supervised learning framework imparts objectness knowledge to the student model, enabling class-aware detection. Our approach, Multimodal-Enhanced Objectness Learner (MENOL) for Corner Case Detection, significantly improves recall for novel classes with lower training costs. By achieving a 76.6% mAR-corner and 79.8% mAR-agnostic on the CODA-val dataset with just 5100 labeled training images, MEN
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25551;&#36848;&#26500;&#24314;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#38024;&#23545;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#23450;&#21046;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2210.15182</link><description>&lt;p&gt;
Text2Model:&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#24402;&#32435;&#29992;&#20110;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text2Model: Text-based Model Induction for Zero-shot Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.15182
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25551;&#36848;&#26500;&#24314;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#38024;&#23545;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#23450;&#21046;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20165;&#20351;&#29992;&#25991;&#26412;&#25551;&#36848;&#26500;&#24314;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20998;&#31867;&#22120;&#30340;&#25361;&#25112;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#12289;3D&#28857;&#20113;&#20998;&#31867;&#20197;&#21450;&#20174;&#22330;&#26223;&#20013;&#35782;&#21035;&#21160;&#20316;&#12290;&#19982;&#23398;&#20064;&#22266;&#23450;&#36755;&#20986;&#31867;&#21035;&#34920;&#31034;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#25512;&#26029;&#26102;&#29983;&#25104;&#38024;&#23545;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#23450;&#21046;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#29983;&#25104;&#22522;&#20110;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#36229;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#25509;&#25910;&#31867;&#25551;&#36848;&#24182;&#36755;&#20986;&#19968;&#20010;&#22810;&#31867;&#27169;&#22411;&#12290;&#36229;&#32593;&#32476;&#35774;&#35745;&#20026;&#23545;&#25551;&#36848;&#38598;&#21512;&#21644;&#20998;&#31867;&#23618;&#20855;&#26377;&#31561;&#21464;&#24615;&#65292;&#22240;&#27492;&#31526;&#21512;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#24182;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#20016;&#23500;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#22270;&#20687;&#12289;&#28857;&#20113;&#21644;&#21160;&#20316;&#35782;&#21035;&#65292;&#24182;&#20351;&#29992;&#19968;&#31995;&#21015;&#25991;&#26412;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.15182v2 Announce Type: replace-cross  Abstract: We address the challenge of building task-agnostic classifiers using only text descriptions, demonstrating a unified approach to image classification, 3D point cloud classification, and action recognition from scenes. Unlike approaches that learn a fixed representation of the output classes, we generate at inference time a model tailored to a query classification task. To generate task-based zero-shot classifiers, we train a hypernetwork that receives class descriptions and outputs a multi-class model. The hypernetwork is designed to be equivariant with respect to the set of descriptions and the classification layer, thus obeying the symmetries of the problem and improving generalization. Our approach generates non-linear classifiers and can handle rich textual descriptions. We evaluate this approach in a series of zero-shot classification tasks, for image, point-cloud, and action recognition, using a range of text descriptions
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.14142</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65306;&#32479;&#19968;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations. (arXiv:2401.14142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14142
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411; (CBM)&#65292;&#22312;&#20026;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#23427;&#20204;&#36890;&#24120;&#36890;&#36807;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#27010;&#24565;&#65292;&#28982;&#21518;&#22312;&#32473;&#23450;&#39044;&#27979;&#30340;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26368;&#32456;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20363;&#22914;&#32416;&#27491;&#19968;&#20010;&#39044;&#27979;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33016;&#37096;&#8221;&#65289;&#26080;&#27861;&#24110;&#21161;&#32416;&#27491;&#39640;&#24230;&#30456;&#20851;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33145;&#37096;&#8221;&#65289;&#65292;&#23548;&#33268;&#26368;&#32456;&#20934;&#30830;&#29575;&#19981;&#29702;&#24819;&#65307;&#23427;&#20204;&#26080;&#27861;&#33258;&#28982;&#22320;&#37327;&#21270;&#19981;&#21516;&#27010;&#24565;&#21644;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65288;&#20363;&#22914;&#23545;&#20110;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#26631;&#31614;&#8220;Kentucky Warbler&#8221;&#21644;&#27010;&#24565;&#8220;&#40657;&#33394;&#22068;&#24052;&#8221;&#30340;&#22270;&#20687;&#65292;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#39044;&#27979;&#21478;&#19968;&#20010;&#27010;&#24565;&#8220;&#40657;&#33394;&#20896;&#8221;&#30340;&#27010;&#29575;&#26159;&#22810;&#23569;&#65289;&#65292;&#22240;&#27492;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#40657;&#30418;&#27169;&#22411;&#24037;&#20316;&#21407;&#29702;&#26356;&#28145;&#23618;&#27425;&#30340;&#27934;&#23519;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;Energy-based Concept Bottleneck Models&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not help correct highly correlated concepts (e.g., "yellow belly"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label "Kentucky Warbler" and a concept "black bill", what is the probability that the model correctly predicts another concept "black crown"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#28151;&#21512;&#30340;&#24555;&#36895;&#21322;&#30417;&#30563;&#38750;&#20984;&#20248;&#21270;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#24211;&#19981;&#21305;&#37197;&#21644;&#20016;&#24230;&#32422;&#26463;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21322;&#30417;&#30563;&#35299;&#28151;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23454;&#26045;&#20984;&#24615;&#32422;&#26463;&#20248;&#20110;&#31232;&#30095;&#20808;&#39564;&#23545;&#20110;&#31471;&#20803;&#24211;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.12609</link><description>&lt;p&gt;
&#24555;&#36895;&#21322;&#30417;&#30563;&#38750;&#20984;&#20248;&#21270;&#35299;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
Fast Semi-supervised Unmixing using Non-convex Optimization. (arXiv:2401.12609v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#28151;&#21512;&#30340;&#24555;&#36895;&#21322;&#30417;&#30563;&#38750;&#20984;&#20248;&#21270;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#24211;&#19981;&#21305;&#37197;&#21644;&#20016;&#24230;&#32422;&#26463;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21322;&#30417;&#30563;&#35299;&#28151;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23454;&#26045;&#20984;&#24615;&#32422;&#26463;&#20248;&#20110;&#31232;&#30095;&#20808;&#39564;&#23545;&#20110;&#31471;&#20803;&#24211;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21322;&#30417;&#30563;/&#22522;&#20110;&#24211;&#30340;&#35299;&#28151;&#21512;&#35774;&#35745;&#30340;&#26032;&#22411;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32771;&#34385;&#20102;&#24211;&#19981;&#21305;&#37197;&#65292;&#24182;&#33021;&#22815;&#23454;&#26045;&#20016;&#24230;&#21644;&#31561;&#20110;&#19968;&#30340;&#32422;&#26463;&#12290;&#19982;&#20256;&#32479;&#30340;&#31232;&#30095;&#35299;&#28151;&#21512;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#28041;&#21450;&#21040;&#38750;&#20984;&#20248;&#21270;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20132;&#26367;&#20056;&#27861;&#22120;&#26041;&#27861;&#65288;ADMM&#65289;&#22312;&#24490;&#29615;&#27714;&#35299;&#36825;&#20123;&#22797;&#26434;&#38382;&#39064;&#20013;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21322;&#30417;&#30563;&#35299;&#28151;&#21512;&#26041;&#27861;&#65292;&#27599;&#31181;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#24212;&#29992;&#20110;&#26032;&#27169;&#22411;&#30340;&#19981;&#21516;&#20808;&#39564;&#20197;&#21450;&#20016;&#24230;&#21644;&#31561;&#20110;&#19968;&#30340;&#32422;&#26463;&#65306;&#31232;&#30095;&#20808;&#39564;&#21644;&#20984;&#24615;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23454;&#26045;&#20984;&#24615;&#32422;&#26463;&#20248;&#20110;&#31232;&#30095;&#20808;&#39564;&#23545;&#20110;&#31471;&#20803;&#24211;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#32467;&#26524;&#22312;&#19977;&#20010;&#27169;&#25311;&#25968;&#25454;&#38598;&#65288;&#32771;&#34385;&#20102;&#20809;&#35889;&#21464;&#21270;&#21644;&#19981;&#21516;&#20687;&#32032;&#32431;&#24230;&#27700;&#24179;&#65289;&#21644;Cuprite&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel linear model tailored for semisupervised/library-based unmixing. Our model incorporates considerations for library mismatch while enabling the enforcement of the abundance sum-to-one constraint (ASC). Unlike conventional sparse unmixing methods, this model involves nonconvex optimization, presenting significant computational challenges. We demonstrate the efficacy of Alternating Methods of Multipliers (ADMM) in cyclically solving these intricate problems. We propose two semisupervised unmixing approaches, each relying on distinct priors applied to the new model in addition to the ASC: sparsity prior and convexity constraint. Our experimental results validate that enforcing the convexity constraint outperforms the sparsity prior for the endmember library. These results are corroborated across three simulated datasets (accounting for spectral variability and varying pixel purity levels) and the Cuprite dataset. Additionally, our comparison with convent
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11143</link><description>&lt;p&gt;
&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26159;&#21807;&#19968;&#25152;&#38656;&#30340;&#65306;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20581;&#22766;&#19978;&#19979;&#25991;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GAAM&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#26031;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#65288;GAT&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#65288;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#65289;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;GAAM&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#34701;&#20837;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#37319;&#29992;&#22810;&#22836;&#26694;&#26550;&#23454;&#29616;&#65292;&#20351;&#20854;&#33021;&#22815;&#38598;&#20307;&#24314;&#27169;&#20219;&#20309;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#21160;&#24577;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#24230;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#36890;&#36807;&#35782;&#21035;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#29366;&#24577;&#65288;&#31934;&#24230;&#22686;&#21152;&#32422;20%&#65289;&#12290;GAAM&#19982;&#22522;&#20110;&#28857;&#31215;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#20855;&#26377;&#30456;&#23545;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#21644;&#25552;&#21319;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;GAAM&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36866;&#24212;&#24615;&#21644;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#31354;&#38388;&#20013;&#30340;Gromov-Wasserstein&#31867;&#22411;&#36317;&#31163;&#65292;&#20998;&#21035;&#29992;&#20110;&#35780;&#20272;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#25512;&#23548;&#26368;&#20248;&#30340;&#28857;&#20998;&#37197;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.11256</link><description>&lt;p&gt;
&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#31354;&#38388;&#20013;&#24341;&#20837;&#20102;&#31867;&#20284;&#20110;Gromov-Wassertein&#30340;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Gromov-Wassertein-like Distances in the Gaussian Mixture Models Space. (arXiv:2310.11256v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#31354;&#38388;&#20013;&#30340;Gromov-Wasserstein&#31867;&#22411;&#36317;&#31163;&#65292;&#20998;&#21035;&#29992;&#20110;&#35780;&#20272;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#25512;&#23548;&#26368;&#20248;&#30340;&#28857;&#20998;&#37197;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#38598;&#21512;&#19978;&#30340;Gromov-Wasserstein&#31867;&#22411;&#36317;&#31163;&#12290;&#31532;&#19968;&#31181;&#36317;&#31163;&#26159;&#22312;&#39640;&#26031;&#27979;&#24230;&#31354;&#38388;&#19978;&#20004;&#20010;&#31163;&#25955;&#20998;&#24067;&#30340;Gromov-Wasserstein&#36317;&#31163;&#12290;&#35813;&#36317;&#31163;&#21487;&#20197;&#20316;&#20026;Gromov-Wasserstein&#30340;&#26367;&#20195;&#65292;&#29992;&#20110;&#35780;&#20272;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#20294;&#19981;&#33021;&#30452;&#25509;&#25512;&#23548;&#20986;&#26368;&#20248;&#30340;&#36816;&#36755;&#26041;&#26696;&#12290;&#20026;&#20102;&#35774;&#35745;&#20986;&#36825;&#26679;&#30340;&#36816;&#36755;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21478;&#19968;&#31181;&#22312;&#19981;&#21487;&#27604;&#36739;&#30340;&#31354;&#38388;&#20013;&#30340;&#27979;&#24230;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#35813;&#36317;&#31163;&#19982;Gromov-Wasserstein&#23494;&#20999;&#30456;&#20851;&#12290;&#24403;&#23558;&#20801;&#35768;&#30340;&#36816;&#36755;&#32806;&#21512;&#38480;&#21046;&#20026;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26102;&#65292;&#36825;&#23450;&#20041;&#20102;&#21478;&#19968;&#31181;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#21487;&#20197;&#20316;&#20026;Gromov-Wasserstein&#30340;&#21478;&#19968;&#31181;&#26367;&#20195;&#65292;&#24182;&#20801;&#35768;&#25512;&#23548;&#20986;&#26368;&#20248;&#30340;&#28857;&#20998;&#37197;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce two Gromov-Wasserstein-type distances on the set of Gaussian mixture models. The first one takes the form of a Gromov-Wasserstein distance between two discrete distributionson the space of Gaussian measures. This distance can be used as an alternative to Gromov-Wasserstein for applications which only require to evaluate how far the distributions are from each other but does not allow to derive directly an optimal transportation plan between clouds of points. To design a way to define such a transportation plan, we introduce another distance between measures living in incomparable spaces that turns out to be closely related to Gromov-Wasserstein. When restricting the set of admissible transportation couplings to be themselves Gaussian mixture models in this latter, this defines another distance between Gaussian mixture models that can be used as another alternative to Gromov-Wasserstein and which allows to derive an optimal assignment between points. Finally,
&lt;/p&gt;</description></item><item><title>IMITATE&#26159;&#19968;&#31181;&#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#21307;&#23398;&#25253;&#21578;&#30340;&#23618;&#32423;&#32467;&#26500;&#65292;&#20174;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#19982;&#20998;&#23618;&#21307;&#23398;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#24615;&#21644;&#32467;&#35770;&#24615;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.07355</link><description>&lt;p&gt;
IMITATE: &#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training. (arXiv:2310.07355v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07355
&lt;/p&gt;
&lt;p&gt;
IMITATE&#26159;&#19968;&#31181;&#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#21307;&#23398;&#25253;&#21578;&#30340;&#23618;&#32423;&#32467;&#26500;&#65292;&#20174;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#19982;&#20998;&#23618;&#21307;&#23398;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#24615;&#21644;&#32467;&#35770;&#24615;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#39046;&#22495;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#20174;&#20020;&#24202;&#25253;&#21578;&#21644;&#30456;&#20851;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#33021;&#24573;&#35270;&#20102;&#21033;&#29992;&#20020;&#24202;&#25253;&#21578;&#22266;&#26377;&#30340;&#23618;&#32423;&#32467;&#26500;&#30340;&#26426;&#20250;&#65292;&#36825;&#20123;&#25253;&#21578;&#36890;&#24120;&#34987;&#20998;&#20026;&#25551;&#36848;&#24615;&#20869;&#23481;&#30340;&#8220;&#21457;&#29616;&#8221;&#21644;&#32467;&#35770;&#24615;&#35266;&#23519;&#30340;&#8220;&#21360;&#35937;&#8221;&#12290;&#24403;&#21069;&#30340;&#21307;&#23398;VLP&#26041;&#27861;&#24448;&#24448;&#23558;&#25253;&#21578;&#31616;&#21270;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#23454;&#20307;&#25110;&#20998;&#25955;&#30340;&#26631;&#35760;&#65292;&#32780;&#27809;&#26377;&#21033;&#29992;&#36825;&#31181;&#20016;&#23500;&#30340;&#12289;&#32467;&#26500;&#21270;&#30340;&#26684;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;VLP&#26694;&#26550;&#65292;&#21517;&#20026;IMITATE&#65292;&#29992;&#20110;&#20174;&#21307;&#23398;&#25253;&#21578;&#20013;&#23398;&#20064;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#20174;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#23558;&#36825;&#20123;&#29305;&#24449;&#19982;&#20998;&#23618;&#21307;&#23398;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#24615;&#21644;&#32467;&#35770;&#24615;&#25991;&#26412;&#20998;&#21035;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of medical Vision-Language Pre-training (VLP), significant efforts have been devoted to deriving text and image features from both clinical reports and associated medical images. However, most existing methods may have overlooked the opportunity in leveraging the inherent hierarchical structure of clinical reports, which are generally split into `findings' for descriptive content and `impressions' for conclusive observation. Instead of utilizing this rich, structured format, current medical VLP approaches often simplify the report into either a unified entity or fragmented tokens. In this work, we propose a novel clinical prior guided VLP framework named IMITATE to learn the structure information from medical reports with hierarchical vision-language alignment. The framework derives multi-level visual features from the chest X-ray (CXR) images and separately aligns these features with the descriptive and the conclusive text encoded in the hierarchical medical report. Furth
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36890;&#36807;&#23545;&#29289;&#20307;&#36755;&#20837;&#34920;&#31034;&#36827;&#34892;&#36523;&#20221;&#20445;&#25345;&#30340;&#21464;&#25442;&#65292;&#19981;&#20165;&#26377;&#21161;&#20110;&#29289;&#20307;&#30340;&#20998;&#31867;&#65292;&#36824;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#23646;&#24615;&#30340;&#26377;&#26080;&#20915;&#31574;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2302.10763</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#19982;&#23646;&#24615;&#20851;&#32852;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning and the Emergence of Attributes Associations. (arXiv:2302.10763v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10763
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36890;&#36807;&#23545;&#29289;&#20307;&#36755;&#20837;&#34920;&#31034;&#36827;&#34892;&#36523;&#20221;&#20445;&#25345;&#30340;&#21464;&#25442;&#65292;&#19981;&#20165;&#26377;&#21161;&#20110;&#29289;&#20307;&#30340;&#20998;&#31867;&#65292;&#36824;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#23646;&#24615;&#30340;&#26377;&#26080;&#20915;&#31574;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29289;&#20307;&#21576;&#29616;&#65292;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#36890;&#24120;&#20250;&#32473;&#20986;&#19968;&#20010;&#31616;&#27905;&#30340;&#26631;&#31614;&#12290;&#32780;&#20154;&#31867;&#22312;&#31867;&#20284;&#30340;&#21576;&#29616;&#19979;&#65292;&#38500;&#20102;&#32473;&#20986;&#19968;&#20010;&#26631;&#31614;&#22806;&#65292;&#36824;&#20250;&#34987;&#22823;&#37327;&#30340;&#20851;&#32852;&#20449;&#24687;&#25152;&#28153;&#27809;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#21576;&#29616;&#29289;&#20307;&#30340;&#23646;&#24615;&#12290;&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#65292;&#22522;&#20110;&#23545;&#29289;&#20307;&#36755;&#20837;&#34920;&#31034;&#36827;&#34892;&#20445;&#25345;&#36523;&#20221;&#30340;&#21464;&#25442;&#12290;&#26412;&#30740;&#31350;&#25512;&#27979;&#65292;&#36825;&#20123;&#21464;&#25442;&#19981;&#20165;&#21487;&#20197;&#20445;&#25345;&#21576;&#29616;&#29289;&#20307;&#30340;&#36523;&#20221;&#65292;&#36824;&#21487;&#20197;&#20445;&#25345;&#20854;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#23646;&#24615;&#30340;&#36523;&#20221;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#30340;&#36755;&#20986;&#34920;&#31034;&#19981;&#20165;&#23545;&#20110;&#21576;&#29616;&#29289;&#20307;&#30340;&#20998;&#31867;&#26377;&#20215;&#20540;&#65292;&#36824;&#23545;&#20110;&#20219;&#20309;&#24863;&#20852;&#36259;&#23646;&#24615;&#30340;&#26377;&#26080;&#20915;&#31574;&#26377;&#20215;&#20540;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#35266;&#28857;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to an object presentation, supervised learning schemes generally respond with a parsimonious label. Upon a similar presentation we humans respond again with a label, but are flooded, in addition, by a myriad of associations. A significant portion of these consist of the presented object attributes. Contrastive learning is a semi-supervised learning scheme based on the application of identity preserving transformations on the object input representations. It is conjectured in this work that these same applied transformations preserve, in addition to the identity of the presented object, also the identity of its semantically meaningful attributes. The corollary of this is that the output representations of such a contrastive learning scheme contain valuable information not only for the classification of the presented object, but also for the presence or absence decision of any attribute of interest. Simulation results which demonstrate this idea and the feasibility of this co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21028;&#21035;&#29109;&#32858;&#31867;&#30340;&#30456;&#20851;&#29702;&#35770;&#21450;&#20854;&#19982;K-means&#21644;SVM&#30340;&#21306;&#21035;&#21644;&#30456;&#20284;&#20043;&#22788;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#36827;&#28145;&#24230;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11405</link><description>&lt;p&gt;
&#21028;&#21035;&#29109;&#32858;&#31867;&#21450;&#20854;&#19982;K-means&#21644;SVM&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Discriminative Entropy Clustering and its Relation to K-means and SVM. (arXiv:2301.11405v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11405
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21028;&#21035;&#29109;&#32858;&#31867;&#30340;&#30456;&#20851;&#29702;&#35770;&#21450;&#20854;&#19982;K-means&#21644;SVM&#30340;&#21306;&#21035;&#21644;&#30456;&#20284;&#20043;&#22788;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#36827;&#28145;&#24230;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21028;&#21035;&#27169;&#22411;&#20013;&#65292;&#26368;&#22823;&#21270;&#27169;&#22411;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#24418;&#24335;&#19978;&#19982; softmax &#39044;&#27979;&#30340;&#8220;&#20915;&#31574;&#24615;&#8221;&#21644;&#8220;&#20844;&#24179;&#24615;&#8221;&#26377;&#20851;&#65292;&#20174;&#32780;&#28608;&#21457;&#20102;&#22522;&#20110;&#29109;&#30340;&#26080;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#30340;&#20351;&#29992;&#12290; &#26368;&#36817;&#65292;&#22522;&#20110;&#36825;&#26679;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#25105;&#26631;&#35760;&#26041;&#27861;&#20195;&#34920;&#20102;&#28145;&#24230;&#32858;&#31867;&#30340;&#26368;&#26032;&#30740;&#31350;&#26041;&#21521;&#12290; &#39318;&#20808;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#29109;&#32858;&#31867;&#26041;&#27861;&#30340;&#35768;&#22810;&#36890;&#29992;&#23646;&#24615;&#65292;&#21253;&#25324;&#23427;&#20204;&#19982; K-means &#21644;&#26080;&#30417;&#30563; SVM &#25216;&#26415;&#30340;&#20851;&#31995;&#12290; &#25105;&#20204;&#35777;&#26126;&#19982; K-&#22343;&#20540;&#26377;&#30528;&#26681;&#26412;&#30340;&#21306;&#21035;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#34920;&#26126;&#20102;&#19982;&#22522;&#20110; SVM &#30340;&#32858;&#31867;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#26174;&#24335;&#30340;&#36793;&#38469;&#26368;&#22823;&#21270;&#19982;&#29109;&#32858;&#31867;&#32852;&#31995;&#36215;&#26469;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20132;&#21449;&#29109;&#30340;&#24120;&#35265;&#24418;&#24335;&#23545;&#20110;&#20266;&#26631;&#31614;&#38169;&#35823;&#19981;&#31283;&#20581;&#12290;&#25105;&#20204;&#30340;&#26032;&#25439;&#22833;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#20102;&#19968;&#31181;&#26032;&#30340; EM &#31639;&#27861;&#65292;&#22312;&#35768;&#22810;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#25913;&#36827;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximization of mutual information between the model's input and output is formally related to "decisiveness" and "fairness" of the softmax predictions, motivating such unsupervised entropy-based losses for discriminative models. Recent self-labeling methods based on such losses represent the state of the art in deep clustering. First, we discuss a number of general properties of such entropy clustering methods, including their relation to K-means and unsupervised SVM-based techniques. Disproving some earlier published claims, we point out fundamental differences with K-means. On the other hand, we show similarity with SVM-based clustering allowing us to link explicit margin maximization to entropy clustering. Finally, we observe that the common form of cross-entropy is not robust to pseudo-label errors. Our new loss addresses the problem and leads to a new EM algorithm improving the state of the art on many standard benchmarks.
&lt;/p&gt;</description></item></channel></rss>