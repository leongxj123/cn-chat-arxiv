<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#23545;&#25239;&#25552;&#31034;&#26694;&#26550;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#26377;&#38480;&#25968;&#25454;&#35843;&#25972;&#36755;&#20837;&#24207;&#21015;&#65292;&#26174;&#33879;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#23545;&#25239;&#24615;&#30456;&#20851;&#30340;&#25991;&#26412;&#30417;&#30563;&#12290;</title><link>https://arxiv.org/abs/2403.14774</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#23569;&#26679;&#26412;&#23545;&#25239;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Adversarial Prompt Learning on Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#23545;&#25239;&#25552;&#31034;&#26694;&#26550;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#26377;&#38480;&#25968;&#25454;&#35843;&#25972;&#36755;&#20837;&#24207;&#21015;&#65292;&#26174;&#33879;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#23545;&#25239;&#24615;&#30456;&#20851;&#30340;&#25991;&#26412;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#24494;&#19981;&#21487;&#35265;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#21463;&#21040;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#36890;&#36807;&#23558;&#23545;&#25239;&#24615;&#35270;&#35273;&#29305;&#24449;&#19982;&#25991;&#26412;&#30417;&#30563;&#23545;&#40784;&#26469;&#23454;&#29616;&#38646;&#26679;&#26412;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#21253;&#25324;&#37325;&#22823;&#36866;&#24212;&#25104;&#26412;&#12289;&#27425;&#20248;&#25991;&#26412;&#30417;&#30563;&#21644;&#26410;&#21463;&#25511;&#21046;&#30340;&#33258;&#28982;&#27867;&#21270;&#33021;&#21147;&#22312;&#20869;&#30340;&#22810;&#20010;&#38382;&#39064;&#65292;&#23427;&#20204;&#20173;&#28982;&#19981;&#23613;&#20154;&#24847;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#23545;&#25239;&#25552;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#26377;&#38480;&#30340;&#25968;&#25454;&#35843;&#25972;&#36755;&#20837;&#24207;&#21015;&#20351;&#24471;&#23545;&#25239;&#40065;&#26834;&#24615;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#23545;&#25239;&#30456;&#20851;&#30340;&#25991;&#26412;&#30417;&#30563;&#65292;&#35813;&#30417;&#30563;&#26159;&#20174;&#23545;&#25239;&#24615;&#31034;&#20363;&#20013;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#65292;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#22810;&#27169;&#24577;&#29305;&#24449;&#19968;&#33268;&#24615;&#24182;&#40723;&#21169;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14774v1 Announce Type: cross  Abstract: The vulnerability of deep neural networks to imperceptible adversarial perturbations has attracted widespread attention. Inspired by the success of vision-language foundation models, previous efforts achieved zero-shot adversarial robustness by aligning adversarial visual features with text supervision. However, in practice, they are still unsatisfactory due to several issues, including heavy adaptation cost, suboptimal text supervision, and uncontrolled natural generalization capacity. In this paper, to address these issues, we propose a few-shot adversarial prompt framework where adapting input sequences with limited data makes significant adversarial robustness improvement. Specifically, we achieve this by providing adversarially correlated text supervision that is end-to-end learned from adversarial examples. We also propose a novel training objective that enhances the consistency of multi-modal features while encourages differenti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#38750;&#24120;&#39640;&#30340;dropout&#29575;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#65292;&#36825;&#36229;&#20986;&#20102;&#38598;&#25104;&#21644;&#26435;&#37325;&#24179;&#22343;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00946</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#24120;&#22823;&#30340;Dropout&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning with Very Large Dropout
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00946
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#38750;&#24120;&#39640;&#30340;dropout&#29575;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#65292;&#36825;&#36229;&#20986;&#20102;&#38598;&#25104;&#21644;&#26435;&#37325;&#24179;&#22343;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20170;&#22825;&#19981;&#21487;&#33021;&#20551;&#35013;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#19982;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#36981;&#24490;&#30456;&#21516;&#20998;&#24067;&#30340;&#35266;&#24565;&#26159;&#20860;&#23481;&#30340;&#12290;&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#20351;&#29992;&#38750;&#24120;&#39640;&#30340;&#20002;&#24323;&#29575;&#26469;&#33719;&#24471;&#36825;&#31181;&#20016;&#23500;&#34920;&#31034;&#65292;&#23613;&#31649;&#20351;&#29992;&#36825;&#26679;&#30340;&#20002;&#24323;&#29575;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#28145;&#24230;&#32593;&#32476;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20294;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#19981;&#20165;&#26159;&#21487;&#33021;&#30340;&#65292;&#32780;&#19988;&#23454;&#29616;&#20102;&#36229;&#36234;&#38598;&#25104;&#21644;&#26435;&#37325;&#24179;&#22343;&#26041;&#27861;&#30340;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00946v1 Announce Type: new  Abstract: It is impossible today to pretend that the practice of machine learning is compatible with the idea that training and testing data follow the same distribution. Several authors have recently used ensemble techniques to show how scenarios involving multiple data distributions are best served by representations that are both richer than those obtained by regularizing for the best in-distribution performance, and richer than those obtained under the influence of the implicit sparsity bias of common stochastic gradient procedures.   This contribution investigates the use of very high dropout rates instead of ensembles to obtain such rich representations. Although training a deep network from scratch using such dropout rates is virtually impossible, fine-tuning a large pre-trained model under such conditions is not only possible but also achieves out-of-distribution performances that exceed those of both ensembles and weight averaging methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#26367;&#25442;&#29616;&#26377;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#29305;&#23450;&#32452;&#20214;&#12290;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#22312;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12289;&#24863;&#30693;&#12289;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#31561;&#26041;&#38754;&#25198;&#28436;&#20102;&#37325;&#35201;&#35282;&#33394;&#12290;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#23545;&#23454;&#38469;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#24433;&#21709;&#20063;&#34987;&#35752;&#35770;&#21040;&#12290;</title><link>https://arxiv.org/abs/2402.05741</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#24212;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Real-World Robot Applications of Foundation Models: A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#26367;&#25442;&#29616;&#26377;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#29305;&#23450;&#32452;&#20214;&#12290;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#22312;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12289;&#24863;&#30693;&#12289;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#31561;&#26041;&#38754;&#25198;&#28436;&#20102;&#37325;&#35201;&#35282;&#33394;&#12290;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#23545;&#23454;&#38469;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#24433;&#21709;&#20063;&#34987;&#35752;&#35770;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#31561;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#20026;&#19981;&#21516;&#20219;&#21153;&#21644;&#27169;&#24577;&#30340;&#28789;&#27963;&#24212;&#29992;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;&#23427;&#20204;&#30340;&#24433;&#21709;&#28085;&#30422;&#20102;&#21253;&#25324;&#21307;&#30103;&#12289;&#25945;&#32946;&#21644;&#26426;&#22120;&#20154;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#65292;&#37325;&#28857;&#26159;&#26367;&#25442;&#29616;&#26377;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#29305;&#23450;&#32452;&#20214;&#12290;&#24635;&#32467;&#28085;&#30422;&#20102;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#30340;&#24863;&#30693;&#12289;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#31561;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#26410;&#26469;&#25361;&#25112;&#21644;&#23545;&#23454;&#38469;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in foundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities. Their impact spans various fields, including healthcare, education, and robotics. This paper provides an overview of the practical application of foundation models in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems. The summary encompasses the perspective of input-output relationships in foundation models, as well as their role in perception, motion planning, and control within the field of robotics. This paper concludes with a discussion of future challenges and implications for practical robot applications.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#23478;&#26063;&#65292;&#31216;&#20026;&#22122;&#22768;&#25193;&#25955;&#20998;&#31867;&#22120;&#65288;NDCs&#65289;&#65292;&#20854;&#20855;&#26377;&#26368;&#26032;&#30340;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;&#25193;&#25955;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#20998;&#31867;&#39640;&#26031;&#21463;&#25439;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#19982;&#38543;&#26426;&#24179;&#28369;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20102;&#20855;&#26377;&#38750;&#24120;&#37327;Lipschitzness&#30340;&#24179;&#28369;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;NDCs&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02316</link><description>&lt;p&gt;
&#20320;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Your Diffusion Model is Secretly a Certifiably Robust Classifier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02316
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#23478;&#26063;&#65292;&#31216;&#20026;&#22122;&#22768;&#25193;&#25955;&#20998;&#31867;&#22120;&#65288;NDCs&#65289;&#65292;&#20854;&#20855;&#26377;&#26368;&#26032;&#30340;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;&#25193;&#25955;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#20998;&#31867;&#39640;&#26031;&#21463;&#25439;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#19982;&#38543;&#26426;&#24179;&#28369;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20102;&#20855;&#26377;&#38750;&#24120;&#37327;Lipschitzness&#30340;&#24179;&#28369;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;NDCs&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#25193;&#25955;&#27169;&#22411;&#34987;&#20316;&#20026;&#40065;&#26834;&#20998;&#31867;&#30340;&#29983;&#25104;&#22120;&#20998;&#31867;&#22120;&#25152;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25193;&#25955;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#32508;&#21512;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#65292;&#36825;&#35753;&#25105;&#20204;&#24576;&#30097;&#23427;&#20204;&#26159;&#21542;&#20250;&#23481;&#26131;&#21463;&#21040;&#26410;&#26469;&#26356;&#24378;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#23478;&#26063;&#65292;&#21629;&#21517;&#20026;&#22122;&#22768;&#25193;&#25955;&#20998;&#31867;&#22120;&#65288;NDCs&#65289;&#65292;&#20854;&#20855;&#26377;&#26368;&#26032;&#30340;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#36825;&#20123;&#20998;&#24067;&#30340;&#35777;&#25454;&#19979;&#30028;&#65288;ELBOs&#65289;&#65292;&#21033;&#29992;ELBO&#36817;&#20284;&#20284;&#28982;&#24230;&#37327;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#23450;&#29702;&#35745;&#31639;&#20998;&#31867;&#27010;&#29575;&#65292;&#23558;&#25193;&#25955;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#20998;&#31867;&#39640;&#26031;&#21463;&#25439;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25512;&#24191;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#19982;&#38543;&#26426;&#24179;&#28369;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20855;&#26377;&#38750;&#24120;&#37327;Lipschitzness&#30340;&#24179;&#28369;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;NDCs&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#35748;&#35777;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#36798;&#21040;80%&#30340;...
&lt;/p&gt;
&lt;p&gt;
Diffusion models are recently employed as generative classifiers for robust classification. However, a comprehensive theoretical understanding of the robustness of diffusion classifiers is still lacking, leading us to question whether they will be vulnerable to future stronger attacks. In this study, we propose a new family of diffusion classifiers, named Noised Diffusion Classifiers~(NDCs), that possess state-of-the-art certified robustness. Specifically, we generalize the diffusion classifiers to classify Gaussian-corrupted data by deriving the evidence lower bounds (ELBOs) for these distributions, approximating the likelihood using the ELBO, and calculating classification probabilities via Bayes' theorem. We integrate these generalized diffusion classifiers with randomized smoothing to construct smoothed classifiers possessing non-constant Lipschitzness. Experimental results demonstrate the superior certified robustness of our proposed NDCs. Notably, we are the first to achieve 80\%
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#35270;&#20026;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#32479;&#19968;&#26816;&#32034;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;RetriNet&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#31934;&#30830;&#26816;&#32034;&#20027;&#39064;&#23646;&#24615;&#24182;&#36807;&#28388;&#26080;&#20851;&#20449;&#24687;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#22312;&#20154;&#33080;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.02521</link><description>&lt;p&gt;
&#20174;&#21442;&#32771;&#22270;&#20687;&#20013;&#26816;&#32034;&#26465;&#20214;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Retrieving Conditions from Reference Images for Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#35270;&#20026;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#32479;&#19968;&#26816;&#32034;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;RetriNet&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#31934;&#30830;&#26816;&#32034;&#20027;&#39064;&#23646;&#24615;&#24182;&#36807;&#28388;&#26080;&#20851;&#20449;&#24687;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#22312;&#20154;&#33080;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#24320;&#21457;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25216;&#26415;&#23637;&#31034;&#20102;&#22312;&#29983;&#25104;&#21508;&#31181;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#24341;&#36215;&#20102;&#21508;&#31181;&#24212;&#29992;&#30340;&#30456;&#24403;&#22823;&#20852;&#36259;&#12290;&#19968;&#20010;&#26222;&#36941;&#30340;&#22330;&#26223;&#26159;&#22522;&#20110;&#21442;&#32771;&#22270;&#20687;&#20013;&#30340;&#19968;&#20010;&#20027;&#39064;&#29983;&#25104;&#26032;&#30340;&#22270;&#20687;&#12290;&#36825;&#20010;&#20027;&#39064;&#21487;&#20197;&#26159;&#39118;&#26684;&#21270;&#22836;&#20687;&#30340;&#38754;&#37096;&#36523;&#20221;&#65292;&#34394;&#25311;&#35797;&#31359;&#30340;&#36523;&#20307;&#21644;&#26381;&#35013;&#31561;&#12290;&#28385;&#36275;&#36825;&#19968;&#35201;&#27714;&#27491;&#22312;&#28436;&#21464;&#25104;&#19968;&#38376;&#31216;&#20026;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20027;&#39064;&#39537;&#21160;&#29983;&#25104;&#35270;&#20026;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#32479;&#19968;&#26816;&#32034;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;RetriNet&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#31934;&#30830;&#22320;&#20174;&#21442;&#32771;&#22270;&#20687;&#20013;&#26816;&#32034;&#20027;&#39064;&#23646;&#24615;&#24182;&#36807;&#28388;&#25481;&#26080;&#20851;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;RetriNet&#22312;&#20154;&#33080;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#30740;&#31350;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02521v2 Announce Type: replace-cross  Abstract: Newly developed diffusion-based techniques have showcased phenomenal abilities in producing a wide range of high-quality images, sparking considerable interest in various applications. A prevalent scenario is to generate new images based on a subject from reference images. This subject could be face identity for styled avatars, body and clothing for virtual try-on and so on. Satisfying this requirement is evolving into a field called Subject-Driven Generation. In this paper, we consider Subject-Driven Generation as a unified retrieval problem with diffusion models. We introduce a novel diffusion model architecture, named RetriNet, designed to address and solve these problems by retrieving subject attributes from reference images precisely, and filter out irrelevant information. RetriNet demonstrates impressive performance when compared to existing state-of-the-art approaches in face generation. We further propose a research and
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20811;&#26381;&#32852;&#37030;&#23398;&#20064;&#20013;&#36890;&#20449;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;FL&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21516;&#26102;&#20943;&#23569;&#36890;&#20449;&#36127;&#25285;&#12290;</title><link>https://arxiv.org/abs/2210.01708</link><description>&lt;p&gt;
&#20811;&#26381;&#36890;&#20449;&#32422;&#26463;&#65292;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#20013;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conquering the Communication Constraints to Enable Large Pre-Trained Models in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.01708
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20811;&#26381;&#32852;&#37030;&#23398;&#20064;&#20013;&#36890;&#20449;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;FL&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21516;&#26102;&#20943;&#23569;&#36890;&#20449;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26088;&#22312;&#22312;&#26412;&#22320;&#35774;&#22791;&#19978;&#21327;&#21147;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#38656;&#35201;&#23545;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#20013;&#24515;&#21270;&#35775;&#38382;&#30340;&#26377;&#21069;&#26223;&#30340;&#33539;&#24335;&#12290;&#22312;&#20856;&#22411;&#30340;FL&#33539;&#24335;&#65288;&#20363;&#22914;FedAvg&#65289;&#20013;&#65292;&#27599;&#19968;&#36718;&#27169;&#22411;&#26435;&#37325;&#37117;&#20250;&#34987;&#21457;&#36865;&#21040;&#21442;&#19982;&#23458;&#25143;&#31471;&#24182;&#22238;&#20256;&#21040;&#26381;&#21153;&#22120;&#12290;&#26368;&#36817;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20248;&#21270;&#21644;&#25910;&#25947;&#25913;&#36827;&#26041;&#38754;&#23637;&#31034;&#20102;&#20351;&#29992;&#23567;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#39044;&#35757;&#32451;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#24378;&#22823;&#65292;&#20294;&#20063;&#25317;&#26377;&#26356;&#22810;&#21442;&#25968;&#12290;&#22312;&#20256;&#32479;&#30340;FL&#20013;&#65292;&#20849;&#20139;&#24040;&#22823;&#30340;&#27169;&#22411;&#26435;&#37325;&#21487;&#20197;&#36805;&#36895;&#32473;&#31995;&#32479;&#24102;&#26469;&#24040;&#22823;&#30340;&#36890;&#20449;&#36127;&#25285;&#65292;&#23588;&#20854;&#26159;&#22914;&#26524;&#37319;&#29992;&#26356;&#21152;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#33021;&#21542;&#25214;&#21040;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;FL&#20013;&#21551;&#29992;&#36825;&#20123;&#24378;&#22823;&#19988;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23454;&#29616;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;&#36890;&#20449;&#36127;&#25285;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.01708v3 Announce Type: replace  Abstract: Federated learning (FL) has emerged as a promising paradigm for enabling the collaborative training of models without centralized access to the raw data on local devices. In the typical FL paradigm (e.g., FedAvg), model weights are sent to and from the server each round to participating clients. Recently, the use of small pre-trained models has been shown effective in federated learning optimization and improving convergence. However, recent state-of-the-art pre-trained models are getting more capable but also have more parameters. In conventional FL, sharing the enormous model weights can quickly put a massive communication burden on the system, especially if more capable models are employed. Can we find a solution to enable those strong and readily-available pre-trained models in FL to achieve excellent performance while simultaneously reducing the communication burden? To this end, we investigate the use of parameter-efficient fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FineRewards&#65292;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#22870;&#21169;&#65292;&#21363;&#26631;&#39064;&#22870;&#21169;&#21644;SAM&#22870;&#21169;&#65292;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2305.19599</link><description>&lt;p&gt;
&#32454;&#31890;&#24230;&#35821;&#20041;&#22870;&#21169;&#22686;&#24378;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards. (arXiv:2305.19599v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FineRewards&#65292;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#22870;&#21169;&#65292;&#21363;&#26631;&#39064;&#22870;&#21169;&#21644;SAM&#22870;&#21169;&#65292;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#22312;&#32473;&#23450;&#30340;&#25991;&#26412;&#25552;&#31034;&#19979;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#32454;&#31890;&#24230;&#35821;&#20041;&#25351;&#23548;&#65292;&#20197;&#25104;&#21151;&#35786;&#26029;&#24418;&#24577;&#24046;&#24322;&#20026;&#27490;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#25991;&#26412;&#27010;&#24565;&#21644;&#29983;&#25104;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#20934;&#30830;&#24418;&#24577;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FineRewards&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#26032;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#22870;&#21169;--&#26631;&#39064;&#22870;&#21169;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#20309;&#20107;&#29289;&#65288;SAM&#65289;&#22870;&#21169;&#65292;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-to-image diffusion models have achieved remarkable success in generating high-quality, realistic images from given text prompts. However, previous methods fail to perform accurate modality alignment between text concepts and generated images due to the lack of fine-level semantic guidance that successfully diagnoses the modality discrepancy. In this paper, we propose FineRewards to improve the alignment between text and images in text-to-image diffusion models by introducing two new fine-grained semantic rewards: the caption reward and the Semantic Segment Anything (SAM) reward. From the global semantic view, the caption reward generates a corresponding detailed caption that depicts all important contents in the synthetic image via a BLIP-2 model and then calculates the reward score by measuring the similarity between the generated caption and the given prompt. From the local semantic view, the SAM reward segments the generated images into local parts with categ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARBEx&#30340;&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#20102;&#21487;&#38752;&#24615;&#24179;&#34913;&#26041;&#27861;&#26469;&#24212;&#23545;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#36824;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#38170;&#28857;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.01486</link><description>&lt;p&gt;
ARBEx&#65306;&#29992;&#20110;&#40065;&#26834;&#24615;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#30340;&#20851;&#27880;&#29305;&#24449;&#25552;&#21462;&#19982;&#21487;&#38752;&#24615;&#24179;&#34913;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ARBEx: Attentive Feature Extraction with Reliability Balancing for Robust Facial Expression Learning. (arXiv:2305.01486v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARBEx&#30340;&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#20102;&#21487;&#38752;&#24615;&#24179;&#34913;&#26041;&#27861;&#26469;&#24212;&#23545;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#36824;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#38170;&#28857;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARBEx&#30340;&#26694;&#26550;&#65292;&#23427;&#26159;&#30001;Vision Transformer&#39537;&#21160;&#30340;&#26032;&#22411;&#20851;&#27880;&#29305;&#24449;&#25552;&#21462;&#26694;&#26550;&#65292;&#24102;&#26377;&#21487;&#38752;&#24615;&#24179;&#34913;&#65292;&#20197;&#24212;&#23545;&#38754;&#37096;&#34920;&#24773;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#36739;&#24046;&#31867;&#20998;&#24067;&#12289;&#20559;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22810;&#31181;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#31934;&#21270;&#26041;&#27861;&#20197;&#21450;&#22522;&#20110;&#31383;&#21475;&#30340;&#20132;&#21449;&#20851;&#27880;ViT&#26469;&#20805;&#20998;&#21033;&#29992;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#38170;&#28857;&#65292;&#21152;&#19978;&#26631;&#31614;&#20998;&#24067;&#21644;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#36890;&#36807;&#21487;&#38752;&#24615;&#24179;&#34913;&#20248;&#21270;&#23545;&#24369;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#39640;&#26631;&#31614;&#39044;&#27979;&#38887;&#24615;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#30830;&#20445;&#27491;&#30830;&#30340;&#26631;&#31614;&#20998;&#31867;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38170;&#25439;&#22833;&#65292;&#40723;&#21169;&#38170;&#28857;&#20043;&#38388;&#30340;&#22823;&#38388;&#38548;&#12290;&#21478;&#22806;&#65292;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#20063;&#26159;&#21487;&#35757;&#32451;&#30340;&#65292;&#23545;&#20110;&#25552;&#21319;&#22312;FEL&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;ARBEx&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a framework ARBEx, a novel attentive feature extraction framework driven by Vision Transformer with reliability balancing to cope against poor class distributions, bias, and uncertainty in the facial expression learning (FEL) task. We reinforce several data pre-processing and refinement methods along with a window-based cross-attention ViT to squeeze the best of the data. We also employ learnable anchor points in the embedding space with label distributions and multi-head self-attention mechanism to optimize performance against weak predictions with reliability balancing, which is a strategy that leverages anchor points, attention scores, and confidence values to enhance the resilience of label predictions. To ensure correct label classification and improve the models' discriminative power, we introduce anchor loss, which encourages large margins between anchor points. Additionally, the multi-head self-attention mechanism, which is also trainable, plays an i
&lt;/p&gt;</description></item></channel></rss>