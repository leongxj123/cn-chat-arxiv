<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>Frankenstein&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#36890;&#36947;&#20013;&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#35821;&#20041;&#30456;&#20851;&#30340;3D&#24418;&#29366;&#65292;&#20026;&#29983;&#25104;&#25151;&#38388;&#20869;&#37096;&#21644;&#20154;&#31867;&#21270;&#36523;&#31561;&#22330;&#26223;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16210</link><description>&lt;p&gt;
Frankenstein: &#22312;&#19968;&#20010;&#19977;&#38754;&#20301;&#24179;&#38754;&#20013;&#29983;&#25104;&#35821;&#20041;-&#32452;&#21512;&#24335;3D&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Frankenstein: Generating Semantic-Compositional 3D Scenes in One Tri-Plane
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16210
&lt;/p&gt;
&lt;p&gt;
Frankenstein&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#36890;&#36947;&#20013;&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#35821;&#20041;&#30456;&#20851;&#30340;3D&#24418;&#29366;&#65292;&#20026;&#29983;&#25104;&#25151;&#38388;&#20869;&#37096;&#21644;&#20154;&#31867;&#21270;&#36523;&#31561;&#22330;&#26223;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Frankenstein&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#36890;&#36947;&#20013;&#29983;&#25104;&#35821;&#20041;-&#32452;&#21512;&#24335;3D&#22330;&#26223;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#36755;&#20986;&#21333;&#20010;&#32479;&#19968;&#30340;3D&#24418;&#29366;&#19981;&#21516;&#65292;Frankenstein&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#29420;&#31435;&#30340;&#24418;&#29366;&#65292;&#27599;&#20010;&#23545;&#24212;&#19968;&#20010;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#37096;&#20998;&#12290;3D&#22330;&#26223;&#20449;&#24687;&#32534;&#30721;&#22312;&#19968;&#20010;&#19977;&#38754;&#20301;&#24179;&#38754;&#24352;&#37327;&#20013;&#65292;&#20174;&#20013;&#21487;&#20197;&#35299;&#30721;&#22810;&#20010;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#22330;&#20197;&#34920;&#31034;&#32452;&#21512;&#24418;&#29366;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#23558;&#19977;&#38754;&#20301;&#24179;&#38754;&#21387;&#32553;&#21040;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#26469;&#36924;&#36817;&#32452;&#21512;&#22330;&#26223;&#30340;&#20998;&#24067;&#12290;Frankenstein&#22312;&#29983;&#25104;&#25151;&#38388;&#20869;&#37096;&#21644;&#20855;&#26377;&#33258;&#21160;&#20998;&#31163;&#37096;&#20998;&#30340;&#20154;&#31867;&#21270;&#36523;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#29983;&#25104;&#30340;&#22330;&#26223;&#26377;&#21161;&#20110;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#65292;&#20363;&#22914;&#37096;&#20998;&#37325;&#36148;&#22270;&#12289;&#25151;&#38388;&#25110;&#21270;&#36523;&#34915;&#26381;&#30340;&#23545;&#35937;&#37325;&#26032;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16210v1 Announce Type: cross  Abstract: We present Frankenstein, a diffusion-based framework that can generate semantic-compositional 3D scenes in a single pass. Unlike existing methods that output a single, unified 3D shape, Frankenstein simultaneously generates multiple separated shapes, each corresponding to a semantically meaningful part. The 3D scene information is encoded in one single tri-plane tensor, from which multiple Singed Distance Function (SDF) fields can be decoded to represent the compositional shapes. During training, an auto-encoder compresses tri-planes into a latent space, and then the denoising diffusion process is employed to approximate the distribution of the compositional scenes. Frankenstein demonstrates promising results in generating room interiors as well as human avatars with automatically separated parts. The generated scenes facilitate many downstream applications, such as part-wise re-texturing, object rearrangement in the room or avatar clo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#37197;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#32473;&#21069;&#26223;&#32534;&#36753;&#21306;&#22495;&#26469;&#23454;&#29616;&#35270;&#39057;&#32534;&#36753;&#30340;&#39640;&#25928;&#29575;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.05735</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#23454;&#29616;&#39640;&#25928;&#35270;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Diffusion for Efficient Video Editing. (arXiv:2401.05735v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#37197;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#32473;&#21069;&#26223;&#32534;&#36753;&#21306;&#22495;&#26469;&#23454;&#29616;&#35270;&#39057;&#32534;&#36753;&#30340;&#39640;&#25928;&#29575;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#39057;&#32534;&#36753;&#24050;&#32463;&#36798;&#21040;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#32534;&#36753;&#25552;&#31034;&#26469;&#36716;&#25442;&#35270;&#39057;&#30340;&#20840;&#23616;&#39118;&#26684;&#12289;&#23616;&#37096;&#32467;&#26500;&#21644;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#29983;&#25104;&#20855;&#26377;&#26102;&#24207;&#19968;&#33268;&#24615;&#30340;&#24103;&#65292;&#21487;&#33021;&#28041;&#21450;&#25193;&#25955;&#21453;&#28436;&#21644;/&#25110;&#36328;&#24103;&#27880;&#24847;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#20302;&#25928;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20462;&#25913;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36895;&#24230;&#21516;&#26102;&#20445;&#25345;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#65288;OCD&#65289;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#36164;&#28304;&#26356;&#22810;&#22320;&#20998;&#37197;&#32473;&#23545;&#24863;&#30693;&#36136;&#37327;&#26356;&#37325;&#35201;&#30340;&#21069;&#26223;&#32534;&#36753;&#21306;&#22495;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;&#24310;&#36831;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#26032;&#30340;&#25552;&#26696;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65306;i&#65289;&#38754;&#21521;&#23545;&#35937;&#30340;&#37319;&#26679;&#65292;&#23558;&#29992;&#20110;&#26174;&#33879;&#21306;&#22495;&#25110;&#32972;&#26223;&#30340;&#25193;&#25955;&#27493;&#39588;&#19982;&#29992;&#20110;&#21069;&#26223;&#30340;&#25193;&#25955;&#27493;&#39588;&#20998;&#31163;&#24320;&#26469;&#65292;&#23558;&#22823;&#37096;&#20998;&#27169;&#22411;&#23481;&#37327;&#20998;&#37197;&#32473;&#21069;&#32773;&#65307;ii&#65289;&#38754;&#21521;&#23545;&#35937;&#30340;3D&#20196;&#29260;&#21512;&#24182;&#65292;&#29992;&#20110;&#25913;&#21892;&#21069;&#26223;&#21644;&#32972;&#26223;&#20043;&#38388;&#30340;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based video editing have reached impressive quality and can transform either the global style, local structure, and attributes of given video inputs, following textual edit prompts. However, such solutions typically incur heavy memory and computational costs to generate temporally-coherent frames, either in the form of diffusion inversion and/or cross-frame attention. In this paper, we conduct an analysis of such inefficiencies, and suggest simple yet effective modifications that allow significant speed-ups whilst maintaining quality. Moreover, we introduce Object-Centric Diffusion, coined as OCD, to further reduce latency by allocating computations more towards foreground edited regions that are arguably more important for perceptual quality. We achieve this by two novel proposals: i) Object-Centric Sampling, decoupling the diffusion steps spent on salient regions or background, allocating most of the model capacity to the former, and ii) Object-Centric 3D Token Merging, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13019</link><description>&lt;p&gt;
&#36890;&#36807;DeepFool&#31639;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#31867;&#21035;&#25805;&#32437;&#30340;&#23545;&#25239;&#25915;&#20987;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm. (arXiv:2310.13019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#24341;&#36215;&#20102;&#20005;&#37325;&#20851;&#27880;&#12290;&#20102;&#35299;&#36825;&#20123;&#26131;&#21463;&#25915;&#20987;&#24615;&#24182;&#24320;&#21457;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;DeepFool&#26159;Moosavi-Dezfooli&#31561;&#20154;&#65288;2016&#24180;&#65289;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23558;&#36755;&#20837;&#22270;&#20687;&#38169;&#35823;&#20998;&#31867;&#30340;&#26368;&#23567;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;DeepFool&#32570;&#20047;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#29305;&#23450;&#25915;&#20987;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;&#22312;&#20808;&#21069;&#30340;&#30456;&#20851;&#24037;&#20316;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#25104;&#21151;&#29575;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#22270;&#20687;&#34987;&#25197;&#26354;&#30340;&#31243;&#24230;&#12289;&#22270;&#20687;&#36136;&#37327;&#30340;&#23436;&#25972;&#24615;&#20197;&#21450;&#38169;&#35823;&#20998;&#31867;&#30340;&#32622;&#20449;&#24230;&#27700;&#24179;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Targeted DeepFool&#65292;&#36825;&#26159;DeepFool&#30340;&#22686;&#24378;&#29256;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#22686;&#24378;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have significantly advanced various domains, but their vulnerability to adversarial attacks poses serious concerns. Understanding these vulnerabilities and developing effective defense mechanisms is crucial. DeepFool, an algorithm proposed by Moosavi-Dezfooli et al. (2016), finds minimal perturbations to misclassify input images. However, DeepFool lacks a targeted approach, making it less effective in specific attack scenarios. Also, in previous related works, researchers primarily focus on success, not considering how much an image is getting distorted; the integrity of the image quality, and the confidence level to misclassifying. So, in this paper, we propose Targeted DeepFool, an augmented version of DeepFool that allows targeting specific classes for misclassification. We also introduce a minimum confidence score requirement hyperparameter to enhance flexibility. Our experiments demonstrate the effectiveness and efficiency of the proposed method across 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;CLIP&#30340;&#32452;&#21512;&#24615;&#33021;&#21147;&#20197;&#21450;&#20197;&#32467;&#26500;&#25935;&#24863;&#30340;&#26041;&#24335;&#25414;&#32465;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#22312;&#21333;&#19968;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#32452;&#21512;&#27010;&#24565;&#65292;&#20294;&#22312;&#38656;&#35201;&#27010;&#24565;&#25414;&#32465;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2212.10537</link><description>&lt;p&gt;
CLIP&#26159;&#21542;&#25414;&#32465;&#27010;&#24565;&#65311;&#25506;&#32034;&#22823;&#22411;&#22270;&#20687;&#27169;&#22411;&#30340;&#32452;&#21512;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does CLIP Bind Concepts? Probing Compositionality in Large Image Models. (arXiv:2212.10537v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;CLIP&#30340;&#32452;&#21512;&#24615;&#33021;&#21147;&#20197;&#21450;&#20197;&#32467;&#26500;&#25935;&#24863;&#30340;&#26041;&#24335;&#25414;&#32465;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#22312;&#21333;&#19968;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#32452;&#21512;&#27010;&#24565;&#65292;&#20294;&#22312;&#38656;&#35201;&#27010;&#24565;&#25414;&#32465;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#32534;&#30721;&#20102;&#23427;&#20204;&#25805;&#20316;&#30340;&#27010;&#24565;&#30340;&#32452;&#25104;&#24615;&#34920;&#31034;&#65292;&#22914;&#36890;&#36807;&#23545;&#8220;&#32418;&#33394;&#31435;&#26041;&#20307;&#8221;&#36827;&#34892;&#25512;&#29702;&#20197;&#27491;&#30830;&#35782;&#21035;&#8220;&#32418;&#33394;&#8221;&#21644;&#8220;&#31435;&#26041;&#20307;&#8221;&#36825;&#20123;&#25104;&#20998;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#20851;&#27880;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;CLIP&#65289;&#32534;&#30721;&#32452;&#21512;&#27010;&#24565;&#30340;&#33021;&#21147;&#20197;&#21450;&#20197;&#32467;&#26500;&#25935;&#24863;&#30340;&#26041;&#24335;&#25414;&#32465;&#21464;&#37327;&#30340;&#33021;&#21147;&#65288;&#20363;&#22914;&#21306;&#20998;&#8220;&#31435;&#26041;&#20307;&#22312;&#29699;&#20307;&#21518;&#38754;&#8221;&#21644;&#8220;&#29699;&#20307;&#22312;&#31435;&#26041;&#20307;&#21518;&#38754;&#8221;&#65289;&#12290;&#20026;&#20102;&#26816;&#26597;CLIP&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#35768;&#22810;&#26469;&#33258;&#32452;&#21512;&#20998;&#24067;&#35821;&#20041;&#27169;&#22411;&#65288;CDSMs&#65289;&#30340;&#26550;&#26500;&#65292;&#36825;&#26159;&#19968;&#31181;&#35797;&#22270;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#23454;&#29616;&#20256;&#32479;&#32452;&#21512;&#35821;&#35328;&#32467;&#26500;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#21457;&#29616;CLIP&#33021;&#22815;&#22312;&#21333;&#19968;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#32452;&#21512;&#27010;&#24565;&#65292;&#20294;&#22312;&#38656;&#35201;&#27010;&#24565;&#25414;&#32465;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20984;&#26174;&#20102;&#35780;&#20272;&#22823;&#22411;&#27169;&#22411;&#32452;&#21512;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20986;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale neural network models combining text and images have made incredible progress in recent years. However, it remains an open question to what extent such models encode compositional representations of the concepts over which they operate, such as correctly identifying ''red cube'' by reasoning over the constituents ''red'' and ''cube''. In this work, we focus on the ability of a large pretrained vision and language model (CLIP) to encode compositional concepts and to bind variables in a structure-sensitive way (e.g., differentiating ''cube behind sphere'' from ''sphere behind cube''). In order to inspect the performance of CLIP, we compare several architectures from research on compositional distributional semantics models (CDSMs), a line of research that attempts to implement traditional compositional linguistic structures within embedding spaces. We find that CLIP can compose concepts in a single-object setting, but in situations where concept binding is needed, performance
&lt;/p&gt;</description></item></channel></rss>