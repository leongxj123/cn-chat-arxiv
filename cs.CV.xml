<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09974</link><description>&lt;p&gt;
GET&#65306;&#35299;&#38145;CLIP&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#65292;&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#21253;&#21547;&#26087;&#31867;&#21035;&#21644;&#26032;&#31867;&#21035;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#26088;&#22312;&#20934;&#30830;&#21457;&#29616;&#26032;&#31867;&#21035;&#65292;&#24182;&#27491;&#30830;&#20998;&#31867;&#26087;&#31867;&#21035;&#65292;&#21033;&#29992;&#20174;&#26377;&#26631;&#31614;&#26679;&#26412;&#20013;&#23398;&#20064;&#30340;&#31867;&#21035;&#27010;&#24565;&#12290;&#24403;&#21069;&#30340;GCD&#26041;&#27861;&#21482;&#20351;&#29992;&#21333;&#19968;&#30340;&#35270;&#35273;&#20449;&#24687;&#27169;&#24577;&#65292;&#23548;&#33268;&#22312;&#35270;&#35273;&#19978;&#30456;&#20284;&#31867;&#21035;&#30340;&#20998;&#31867;&#25928;&#26524;&#19981;&#20339;&#12290;&#34429;&#28982;&#26576;&#20123;&#31867;&#21035;&#22312;&#35270;&#35273;&#19978;&#23481;&#26131;&#28151;&#28102;&#65292;&#20294;&#23427;&#20204;&#30340;&#25991;&#26412;&#20449;&#24687;&#21487;&#33021;&#26159;&#19981;&#21516;&#30340;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#23558;&#25991;&#26412;&#20449;&#24687;&#24341;&#20837;&#21040;GCD&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#26080;&#26631;&#31614;&#25968;&#25454;&#32570;&#20047;&#31867;&#21035;&#21517;&#31216;&#65292;&#20351;&#24471;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#26679;&#26412;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;TES&#21033;&#29992;CLIP&#21487;&#20197;&#29983;&#25104;&#23545;&#40784;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#36825;&#19968;&#29305;&#24615;&#65292;&#23558;&#35270;&#35273;&#23884;&#20837;&#36716;&#25442;&#20026;CLIP&#25991;&#26412;&#27169;&#22411;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09974v1 Announce Type: cross  Abstract: Given unlabelled datasets containing both old and new categories, generalized category discovery (GCD) aims to accurately discover new classes while correctly classifying old classes, leveraging the class concepts learned from labeled samples. Current GCD methods only use a single visual modality of information, resulting in poor classification of visually similar classes. Though certain classes are visually confused, their text information might be distinct, motivating us to introduce text information into the GCD task. However, the lack of class names for unlabelled data makes it impractical to utilize text information. To tackle this challenging problem, in this paper, we propose a Text Embedding Synthesizer (TES) to generate pseudo text embeddings for unlabelled samples. Specifically, our TES leverages the property that CLIP can generate aligned vision-language features, converting visual embeddings into tokens of the CLIP's text e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#22522;&#20110;&#23398;&#20064;&#30340;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#20223;&#30495;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#29616;&#29366;&#12290;</title><link>https://arxiv.org/abs/2402.10079</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#22522;&#20110;&#23398;&#20064;&#30340;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#20223;&#30495;&#26041;&#27861;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of the Learning-based Camera and Lidar Simulation Methods for Autonomous Driving Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#22522;&#20110;&#23398;&#20064;&#30340;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#20223;&#30495;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#20256;&#24863;&#22120;&#65292;&#23588;&#20854;&#26159;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#65292;&#26159;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;(Autonomous Driving Systems&#65292;ADS)&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#21608;&#22260;&#29615;&#22659;&#20197;&#20570;&#20986;&#26126;&#26234;&#30340;&#39550;&#39542;&#21644;&#25511;&#21046;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#36924;&#30495;&#30340;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#27169;&#25311;&#26041;&#27861;&#65292;&#20063;&#31216;&#20026;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#27169;&#22411;&#65292;&#23545;&#20110;&#26377;&#25928;&#36827;&#34892;&#22522;&#20110;&#20223;&#30495;&#30340;ADS&#27979;&#35797;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#20419;&#36827;&#20102;&#24863;&#30693;&#20256;&#24863;&#22120;&#27169;&#22411;&#20316;&#20026;&#21512;&#25104;&#21508;&#31181;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#26222;&#21450;&#12290;&#20256;&#32479;&#20256;&#24863;&#22120;&#20223;&#30495;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#31995;&#32479;&#22914;ADS&#20013;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340;&#28508;&#21147;&#22312;&#20110;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21463;&#21040;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#21512;&#25104;&#39640;&#32500;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#30340;&#25512;&#21160;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#20256;&#24863;&#22120;&#20223;&#30495;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10079v1 Announce Type: cross  Abstract: Perception sensors, particularly camera and Lidar, are key elements of Autonomous Driving Systems (ADS) that enable them to comprehend their surroundings for informed driving and control decisions. Therefore, developing realistic camera and Lidar simulation methods, also known as camera and Lidar models, is of paramount importance to effectively conduct simulation-based testing for ADS. Moreover, the rise of deep learning-based perception models has propelled the prevalence of perception sensor models as valuable tools for synthesising diverse training datasets. The traditional sensor simulation methods rely on computationally expensive physics-based algorithms, specifically in complex systems such as ADS. Hence, the current potential resides in learning-based models, driven by the success of deep generative models in synthesising high-dimensional data. This paper reviews the current state-of-the-art in learning-based sensor simulation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05935</link><description>&lt;p&gt;
SPHINX-X: &#25193;&#23637;&#25968;&#25454;&#21644;&#21442;&#25968;&#29992;&#20110;&#19968;&#31995;&#21015;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;SPHINX-X&#65292;&#19968;&#31181;&#22522;&#20110;SPHINX&#24320;&#21457;&#30340;&#24191;&#27867;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#31995;&#21015;&#12290;&#20026;&#20102;&#25913;&#21892;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#31227;&#38500;&#20887;&#20313;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#32469;&#36807;&#23436;&#20840;&#22635;&#20805;&#30340;&#23376;&#22270;&#20687;&#65292;&#24182;&#23558;&#22810;&#38454;&#27573;&#35757;&#32451;&#31616;&#21270;&#25104;&#20026;&#19968;&#38454;&#27573;&#30340;&#20840;&#38598;&#21512;&#27169;&#24335;&#65292;&#20462;&#25913;&#20102;SPHINX&#26694;&#26550;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;MLLM&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#32452;&#35013;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#36328;&#35821;&#35328;&#12289;&#36328;&#35270;&#35273;&#21644;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#30340;&#22810;&#39046;&#22495;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#25105;&#20204;&#30340;OCR&#23494;&#38598;&#21644;Mark&#25968;&#25454;&#38598;&#20016;&#23500;&#36825;&#20010;&#25910;&#38598;&#65292;&#25193;&#23637;&#20102;&#22810;&#26679;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#22522;&#30784;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;TinyLlama1.1B&#12289;InternLM2-7B&#12289;LLaMA2-13B&#21644;Mixtral8x7B&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#21464;&#21270;&#30340;MLLMs&#12290;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#24615;&#33021;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX. To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm. To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks. We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality. By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities. Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales. 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;AutArch&#65292;&#19968;&#31181;&#29992;&#20110;&#32771;&#21476;&#30446;&#24405;&#20013;&#29289;&#20307;&#26816;&#27979;&#21644;&#33258;&#21160;&#21270;&#35760;&#24405;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#20174;&#36951;&#30041;&#36164;&#28304;&#20013;&#25552;&#21462;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#35760;&#24405;&#36136;&#37327;&#21644;&#26631;&#20934;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2311.17978</link><description>&lt;p&gt;
AutArch&#65306;&#19968;&#31181;&#29992;&#20110;&#32771;&#21476;&#30446;&#24405;&#20013;&#29289;&#20307;&#26816;&#27979;&#21644;&#33258;&#21160;&#21270;&#35760;&#24405;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#24037;&#20316;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
AutArch: An AI-assisted workflow for object detection and automated recording in archaeological catalogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17978
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;AutArch&#65292;&#19968;&#31181;&#29992;&#20110;&#32771;&#21476;&#30446;&#24405;&#20013;&#29289;&#20307;&#26816;&#27979;&#21644;&#33258;&#21160;&#21270;&#35760;&#24405;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#20174;&#36951;&#30041;&#36164;&#28304;&#20013;&#25552;&#21462;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#35760;&#24405;&#36136;&#37327;&#21644;&#26631;&#20934;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30340;&#32972;&#26223;&#26159;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#25968;&#25454;&#20174;&#24322;&#26500;&#30340;&#24050;&#21457;&#34920;&#36164;&#28304;&#20013;&#21019;&#24314;&#22823;&#35268;&#27169;&#32479;&#19968;&#30340;&#32771;&#21476;&#25968;&#25454;&#38598;&#65292;&#27604;&#22914;&#36951;&#29289;&#30446;&#24405;&#12290;&#35770;&#25991;&#20851;&#27880;&#30340;&#26159;&#19968;&#33268;&#32771;&#21476;&#25968;&#25454;&#32452;&#21512;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;&#29616;&#26377;&#35760;&#24405;&#22312;&#36136;&#37327;&#21644;&#35760;&#24405;&#26631;&#20934;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#25105;&#20204;&#26080;&#27861;&#31616;&#21333;&#22320;&#21512;&#24182;&#29616;&#26377;&#35760;&#24405;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#20174;&#24050;&#21457;&#34920;&#30340;&#32771;&#21476;&#25554;&#22270;&#20013;&#37325;&#26032;&#21019;&#24314;&#35760;&#24405;&#12290;&#21482;&#26377;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#24110;&#21161;&#65292;&#36825;&#25165;&#26159;&#21487;&#34892;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#19968;&#20010;&#26032;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#20174;&#32771;&#21476;&#36951;&#29289;&#30446;&#24405;&#20013;&#25910;&#38598;&#25968;&#25454;&#65292;&#36825;&#20123;&#30446;&#24405;&#20316;&#20026;&#36951;&#30041;&#36164;&#28304;&#23384;&#22312;&#65292;&#27604;&#22914;&#22823;&#22411;&#26410;&#25490;&#24207;&#30340;PDF&#25991;&#20214;&#20013;&#30340;&#32771;&#21476;&#32472;&#22270;&#21644;&#29031;&#29255;&#65307;&#35813;&#24037;&#20316;&#27969;&#31243;&#20381;&#36182;&#20110;&#25903;&#25345;&#22270;&#20687;&#22788;&#29702;&#12289;&#29289;&#20307;&#26816;&#27979;&#20197;&#21450;&#39564;&#35777;&#21644;&#35843;&#25972;&#33258;&#21160;&#33719;&#21462;&#25968;&#25454;&#30340;&#20132;&#20114;&#25163;&#27573;&#30340;&#33258;&#23450;&#20041;&#36719;&#20214;&#65288;AutArch&#65289;&#12290;&#25105;&#20204;&#38598;&#25104;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17978v2 Announce Type: replace-cross  Abstract: The context of this paper is the creation of large uniform archaeological datasets from heterogeneous published resources, such as find catalogues - with the help of AI and Big Data. The paper is concerned with the challenge of consistent assemblages of archaeological data. We cannot simply combine existing records, as they differ in terms of quality and recording standards. Thus, records have to be recreated from published archaeological illustrations. This is only a viable path with the help of automation. The contribution of this paper is a new workflow for collecting data from archaeological find catalogues available as legacy resources, such as archaeological drawings and photographs in large unsorted PDF files; the workflow relies on custom software (AutArch) supporting image processing, object detection, and interactive means of validating and adjusting automatically retrieved data. We integrate artificial intelligence (
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#31934;&#20934;&#32959;&#30244;&#23398;&#20013;&#30340;&#26579;&#33394;&#20307;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;Fred Hutchinson&#30284;&#30151;&#30740;&#31350;&#20013;&#24515;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#25299;&#25169;&#35270;&#35273;&#36716;&#25442;&#22120;(TopViTs)&#65292;&#25104;&#21151;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35782;&#21035;&#26579;&#33394;&#20307;&#24322;&#24120;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.14312</link><description>&lt;p&gt;
&#31934;&#20934;&#32959;&#30244;&#23398;&#30340;&#26579;&#33394;&#20307;AI
&lt;/p&gt;
&lt;p&gt;
Karyotype AI for Precision Oncology. (arXiv:2211.14312v3 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#31934;&#20934;&#32959;&#30244;&#23398;&#20013;&#30340;&#26579;&#33394;&#20307;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;Fred Hutchinson&#30284;&#30151;&#30740;&#31350;&#20013;&#24515;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#25299;&#25169;&#35270;&#35273;&#36716;&#25442;&#22120;(TopViTs)&#65292;&#25104;&#21151;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35782;&#21035;&#26579;&#33394;&#20307;&#24322;&#24120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26579;&#33394;&#20307;&#20998;&#26512;&#23545;&#20110;&#35786;&#26029;&#36951;&#20256;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#34880;&#28082;&#31995;&#32479;&#24694;&#24615;&#32959;&#30244;&#65292;&#36890;&#36807;&#26579;&#33394;&#20307;&#32452;&#22411;&#20998;&#26512;&#26469;&#21457;&#29616;&#20307;&#32454;&#32990;&#31361;&#21464;&#26159;&#26631;&#20934;&#30340;&#25252;&#29702;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26579;&#33394;&#20307;&#32452;&#22411;&#20998;&#26512;&#22240;&#20026;&#22823;&#37096;&#20998;&#26159;&#25163;&#21160;&#25805;&#20316;&#65292;&#19988;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#26469;&#35782;&#21035;&#21644;&#27880;&#37322;&#31361;&#21464;&#65292;&#25152;&#20197;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#20197;Fred Hutchinson&#30284;&#30151;&#30740;&#31350;&#20013;&#24515;&#36807;&#21435;&#20116;&#24180;&#30340;&#32422;10,000&#20010;&#24739;&#32773;&#26631;&#26412;&#21644;&#32422;50,000&#20010;&#26579;&#33394;&#20307;&#32452;&#22411;&#22270;&#29255;&#20316;&#20026;&#35757;&#32451;&#38598;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#32452;&#20195;&#34920;&#21333;&#20010;&#26579;&#33394;&#20307;&#30340;&#26631;&#35760;&#22270;&#29255;&#12290;&#36825;&#20123;&#21333;&#20010;&#26579;&#33394;&#20307;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20998;&#31867;&#20154;&#31867;&#30340;24&#26465;&#26579;&#33394;&#20307;&#21644;&#35782;&#21035;&#26579;&#33394;&#20307;&#24322;&#24120;&#12290;&#20855;&#26377;&#26368;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#26368;&#36817;&#24341;&#20837;&#30340;&#25299;&#25169;&#35270;&#35273;&#36716;&#25442;&#22120;(TopViTs)&#21644;&#20108;&#32423;&#22359;-&#25176;&#26222;&#21033;&#33576;&#33945;&#29256;&#65292;&#20197;&#34701;&#20837;&#32467;&#26500;&#24615;&#24402;&#32435;&#20559;&#32622;&#12290;TopViT&#30340;&#24615;&#33021;&#20248;&#20110;CNN(Inc)
&lt;/p&gt;
&lt;p&gt;
Chromosome analysis is essential for diagnosing genetic disorders. For hematologic malignancies, identification of somatic clonal aberrations by karyotype analysis remains the standard of care. However, karyotyping is costly and time-consuming because of the largely manual process and the expertise required in identifying and annotating aberrations. Efforts to automate karyotype analysis to date fell short in aberration detection. Using a training set of ~10k patient specimens and ~50k karyograms from over 5 years from the Fred Hutchinson Cancer Center, we created a labeled set of images representing individual chromosomes. These individual chromosomes were used to train and assess deep learning models for classifying the 24 human chromosomes and identifying chromosomal aberrations. The top-accuracy models utilized the recently introduced Topological Vision Transformers (TopViTs) with 2-level-block-Toeplitz masking, to incorporate structural inductive bias. TopViT outperformed CNN (Inc
&lt;/p&gt;</description></item></channel></rss>