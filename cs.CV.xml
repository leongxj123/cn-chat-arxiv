<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#22522;&#20110;Vision Transformer&#30340;DRCT&#26041;&#27861;&#37319;&#29992;&#21019;&#26032;&#30340;&#26426;&#21046;&#35299;&#20915;&#20102;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#31354;&#38388;&#20449;&#24687;&#34928;&#20943;&#30340;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00722</link><description>&lt;p&gt;
DRCT&#65306;&#23558;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20445;&#23384;&#22312;&#20449;&#24687;&#29942;&#39048;&#20043;&#22806;
&lt;/p&gt;
&lt;p&gt;
DRCT: Saving Image Super-resolution away from Information Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00722
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Vision Transformer&#30340;DRCT&#26041;&#27861;&#37319;&#29992;&#21019;&#26032;&#30340;&#26426;&#21046;&#35299;&#20915;&#20102;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#31354;&#38388;&#20449;&#24687;&#34928;&#20943;&#30340;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;Vision Transformer&#30340;&#20302;&#23618;&#35270;&#35273;&#20219;&#21153;&#24212;&#29992;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#12290;&#19982;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;Transformer&#26356;&#25797;&#38271;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#21033;&#29992;&#38750;&#23616;&#37096;&#21306;&#22495;&#30340;&#20449;&#24687;&#37325;&#24314;&#22270;&#20687;&#12290;&#22312;&#36229;&#20998;&#36776;&#29575;&#39046;&#22495;&#65292;&#22522;&#20110;Swin Transformer&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#20027;&#27969;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#20840;&#23616;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#19988;&#20855;&#26377;&#26059;&#36716;&#31383;&#21475;&#27880;&#24847;&#26426;&#21046;&#65292;&#26377;&#21161;&#20110;&#22312;&#19981;&#21516;&#31383;&#21475;&#20043;&#38388;&#20132;&#25442;&#20449;&#24687;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25193;&#22823;&#24863;&#30693;&#37326;&#25110;&#35774;&#35745;&#22797;&#26434;&#32593;&#32476;&#26469;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#21644;&#32593;&#32476;&#25928;&#29575;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#65292;&#30001;&#20110;&#28145;&#24230;&#22686;&#21152;&#65292;&#31354;&#38388;&#20449;&#24687;&#24448;&#24448;&#20250;&#20943;&#23569;&#65292;&#20174;&#32780;&#23548;&#33268;&#31354;&#38388;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#24182;&#26368;&#32456;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00722v1 Announce Type: cross  Abstract: In recent years, Vision Transformer-based applications to low-level vision tasks have achieved widespread success. Unlike CNN-based models, Transformers are more adept at capturing long-range dependencies, enabling the reconstruction of images utilizing information from non-local areas. In the domain of super-resolution, Swin-transformer-based approaches have become mainstream due to their capacity to capture global spatial information and their shifting-window attention mechanism that facilitates the interchange of information between different windows. Many researchers have enhanced image quality and network efficiency by expanding the receptive field or designing complex networks, yielding commendable results. However, we observed that spatial information tends to diminish during the forward propagation process due to increased depth, leading to a loss of spatial information and, consequently, limiting the model's potential. To addr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#24403;&#21069;Mamba-based&#35270;&#35273;&#26041;&#27861;&#20013;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#24573;&#35270;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36895;&#24230;&#21644;&#20869;&#23384;&#21033;&#29992;&#65292;&#21516;&#26102;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13802</link><description>&lt;p&gt;
ZigMa&#65306;&#34623;&#34578;&#26364;&#24052;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ZigMa: Zigzag Mamba Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#24403;&#21069;Mamba-based&#35270;&#35273;&#26041;&#27861;&#20013;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#24573;&#35270;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36895;&#24230;&#21644;&#20869;&#23384;&#21033;&#29992;&#65292;&#21516;&#26102;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#21463;&#21040;&#21487;&#20280;&#32553;&#24615;&#21644;&#20108;&#27425;&#22797;&#26434;&#24615;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#32467;&#26500;&#20869;&#37096;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#19968;&#31181;&#31216;&#20026;&#26364;&#24052;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#65292;&#20197;&#25193;&#23637;&#20854;&#22312;&#35270;&#35273;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22823;&#22810;&#25968;&#24403;&#21069;&#22522;&#20110;&#26364;&#24052;&#30340;&#35270;&#35273;&#26041;&#27861;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30095;&#24573;&#65292;&#21363;&#26364;&#24052;&#30340;&#25195;&#25551;&#26041;&#26696;&#20013;&#32570;&#20047;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#32771;&#34385;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#36825;&#19968;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#31616;&#21333;&#12289;&#21363;&#25554;&#21363;&#29992;&#12289;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#23427;&#20248;&#20110;&#22522;&#20110;&#26364;&#24052;&#30340;&#22522;&#32447;&#65292;&#24182;&#34920;&#29616;&#20986;&#27604;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22522;&#32447;&#26356;&#24555;&#36895;&#21644;&#26356;&#22909;&#30340;&#20869;&#23384;&#21033;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;Zigzag Mamba&#38598;&#25104;&#21040;&#38543;&#26426;&#25554;&#20540;&#26694;&#26550;&#20013;&#65292;&#20197;&#30740;&#31350;&#27169;&#22411;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;FacesHQ $1024\times 1024$&#21644;UCF101&#65292;MultiModal-CelebA-HQ&#65289;&#19978;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13802v1 Announce Type: cross  Abstract: The diffusion model has long been plagued by scalability and quadratic complexity issues, especially within transformer-based structures. In this study, we aim to leverage the long sequence modeling capability of a State-Space Model called Mamba to extend its applicability to visual data generation. Firstly, we identify a critical oversight in most current Mamba-based vision methods, namely the lack of consideration for spatial continuity in the scan scheme of Mamba. Secondly, building upon this insight, we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba, which outperforms Mamba-based baselines and demonstrates improved speed and memory utilization compared to transformer-based baselines. Lastly, we integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate the scalability of the model on large-resolution visual datasets, such as FacesHQ $1024\times 1024$ and UCF101, MultiModal-CelebA-HQ
&lt;/p&gt;</description></item><item><title>ContourDiff&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#30340;&#39046;&#22495;&#19981;&#21464;&#35299;&#21078;&#36718;&#24275;&#34920;&#31034;&#65292;&#26088;&#22312;&#24110;&#21161;&#20934;&#30830;&#32763;&#35793;&#21307;&#23398;&#22270;&#20687;&#24182;&#20445;&#25345;&#20854;&#35299;&#21078;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10786</link><description>&lt;p&gt;
ContourDiff&#65306;&#24102;&#36718;&#24275;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#26080;&#37197;&#23545;&#22270;&#20687;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
ContourDiff: Unpaired Image Translation with Contour-Guided Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10786
&lt;/p&gt;
&lt;p&gt;
ContourDiff&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#30340;&#39046;&#22495;&#19981;&#21464;&#35299;&#21078;&#36718;&#24275;&#34920;&#31034;&#65292;&#26088;&#22312;&#24110;&#21161;&#20934;&#30830;&#32763;&#35793;&#21307;&#23398;&#22270;&#20687;&#24182;&#20445;&#25345;&#20854;&#35299;&#21078;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#32763;&#35793;&#21307;&#23398;&#22270;&#20687;&#65288;&#20363;&#22914;&#20174;CT&#21040;MRI&#65289;&#23545;&#20110;&#35768;&#22810;&#20020;&#24202;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ContourDiff&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22270;&#20687;&#30340;&#39046;&#22495;&#19981;&#21464;&#35299;&#21078;&#36718;&#24275;&#34920;&#31034;&#12290;&#36825;&#20123;&#34920;&#31034;&#26131;&#20110;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#65292;&#20294;&#23545;&#20854;&#35299;&#21078;&#20869;&#23481;&#24418;&#25104;&#31934;&#30830;&#30340;&#31354;&#38388;&#32422;&#26463;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#26469;&#33258;&#20219;&#24847;&#36755;&#20837;&#39046;&#22495;&#30340;&#22270;&#20687;&#30340;&#36718;&#24275;&#34920;&#31034;&#36716;&#25442;&#20026;&#36755;&#20986;&#39046;&#22495;&#20013;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10786v1 Announce Type: cross  Abstract: Accurately translating medical images across different modalities (e.g., CT to MRI) has numerous downstream clinical and machine learning applications. While several methods have been proposed to achieve this, they often prioritize perceptual quality with respect to output domain features over preserving anatomical fidelity. However, maintaining anatomy during translation is essential for many tasks, e.g., when leveraging masks from the input domain to develop a segmentation model with images translated to the output domain. To address these challenges, we propose ContourDiff, a novel framework that leverages domain-invariant anatomical contour representations of images. These representations are simple to extract from images, yet form precise spatial constraints on their anatomical content. We introduce a diffusion model that converts contour representations of images from arbitrary input domains into images in the output domain of in
&lt;/p&gt;</description></item><item><title>HanDiffuser&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#25163;&#37096;&#23884;&#20837;&#26469;&#23454;&#29616;&#36924;&#30495;&#30340;&#25163;&#37096;&#22806;&#35266;&#65292;&#21253;&#25324;Text-to-Hand-Params&#25193;&#25955;&#27169;&#22411;&#21644;Text-Guided Hand-Params-to-Image&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.01693</link><description>&lt;p&gt;
HanDiffuser: &#20855;&#26377;&#36924;&#30495;&#25163;&#37096;&#22806;&#35266;&#30340;&#25991;&#26412;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01693
&lt;/p&gt;
&lt;p&gt;
HanDiffuser&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#25163;&#37096;&#23884;&#20837;&#26469;&#23454;&#29616;&#36924;&#30495;&#30340;&#25163;&#37096;&#22806;&#35266;&#65292;&#21253;&#25324;Text-to-Hand-Params&#25193;&#25955;&#27169;&#22411;&#21644;Text-Guided Hand-Params-to-Image&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01693v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25991;&#25688;: &#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#24418;&#35937;&#65292;&#20294;&#22312;&#29983;&#25104;&#25163;&#37096;&#26102;&#20250;&#22833;&#21435;&#36924;&#30495;&#24230;&#12290;&#24120;&#35265;&#30340;&#32570;&#38519;&#21253;&#25324;&#19981;&#35268;&#21017;&#30340;&#25163;&#37096;&#23039;&#21183;&#12289;&#24418;&#29366;&#12289;&#38169;&#35823;&#30340;&#25163;&#25351;&#25968;&#37327;&#20197;&#21450;&#29289;&#29702;&#19978;&#19981;&#21512;&#29702;&#30340;&#25163;&#25351;&#26041;&#21521;&#12290;&#20026;&#20102;&#29983;&#25104;&#20855;&#26377;&#36924;&#30495;&#25163;&#37096;&#30340;&#22270;&#20687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26032;&#39062;&#26550;&#26500;&#65292;&#31216;&#20026;HanDiffuser&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#25163;&#37096;&#23884;&#20837;&#26469;&#23454;&#29616;&#36924;&#30495;&#24230;&#12290;HanDiffuser&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;:Text-to-Hand-Params&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#36755;&#20837;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;SMPL-&#36523;&#20307;&#21644;MANO-&#25163;&#37096;&#21442;&#25968;&#65292;&#20197;&#21450;Text-Guided Hand-Params-to-Image&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#19978;&#19968;&#37096;&#20214;&#29983;&#25104;&#30340;&#25552;&#31034;&#21644;&#25163;&#37096;&#21442;&#25968;&#19978;&#36827;&#34892;&#35843;&#33410;&#26469;&#21512;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#21512;&#24182;&#20102;&#25163;&#37096;&#34920;&#31034;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;3D&#24418;&#29366;&#21644;&#20851;&#33410;&#32423;&#25163;&#25351;&#20301;&#32622;&#12289;&#26041;&#21521;&#21644;&#20851;&#33410;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#23398;&#20064;&#21644;&#21487;&#38752;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01693v1 Announce Type: cross  Abstract: Text-to-image generative models can generate high-quality humans, but realism is lost when generating hands. Common artifacts include irregular hand poses, shapes, incorrect numbers of fingers, and physically implausible finger orientations. To generate images with realistic hands, we propose a novel diffusion-based architecture called HanDiffuser that achieves realism by injecting hand embeddings in the generative process. HanDiffuser consists of two components: a Text-to-Hand-Params diffusion model to generate SMPL-Body and MANO-Hand parameters from input text prompts, and a Text-Guided Hand-Params-to-Image diffusion model to synthesize images by conditioning on the prompts and hand parameters generated by the previous component. We incorporate multiple aspects of hand representation, including 3D shapes and joint-level finger positions, orientations and articulations, for robust learning and reliable performance during inference. We
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32456;&#36523;&#22522;&#20934;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#21019;&#24314;&#19981;&#26029;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#26469;&#20943;&#23569;&#36807;&#25311;&#21512;&#39118;&#38505;&#65292;&#24182;&#24341;&#20837;&#20102;&#39640;&#25928;&#30340;&#35780;&#20272;&#26694;&#26550;Sort \&amp; Search&#65288;S&amp;S&#65289;&#26469;&#35299;&#20915;&#35780;&#20272;&#25104;&#26412;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.19472</link><description>&lt;p&gt;
&#32456;&#36523;&#22522;&#20934;&#65306;&#22312;&#24555;&#36895;&#36827;&#23637;&#26102;&#20195;&#20013;&#39640;&#25928;&#30340;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19472
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32456;&#36523;&#22522;&#20934;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#21019;&#24314;&#19981;&#26029;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#26469;&#20943;&#23569;&#36807;&#25311;&#21512;&#39118;&#38505;&#65292;&#24182;&#24341;&#20837;&#20102;&#39640;&#25928;&#30340;&#35780;&#20272;&#26694;&#26550;Sort \&amp; Search&#65288;S&amp;S&#65289;&#26469;&#35299;&#20915;&#35780;&#20272;&#25104;&#26412;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#21270;&#22522;&#20934;&#25512;&#21160;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#37325;&#22797;&#27979;&#35797;&#65292;&#31639;&#27861;&#23545;&#22522;&#20934;&#30340;&#29305;&#27530;&#24615;&#36807;&#24230;&#21033;&#29992;&#65292;&#20250;&#22686;&#21152;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#32534;&#21046;&#19981;&#26029;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#65288;&#31216;&#20026;&#32456;&#36523;&#22522;&#20934;&#65289;&#26469;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#12290;&#20316;&#20026;&#25105;&#20204;&#26041;&#27861;&#30340;&#31034;&#20363;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#32456;&#36523;-CIFAR10&#21644;&#32456;&#36523;-ImageNet&#65292;&#20998;&#21035;&#21253;&#21547;&#65288;&#30446;&#21069;&#65289;1.69&#30334;&#19975;&#21644;1.98&#30334;&#19975;&#20010;&#27979;&#35797;&#26679;&#26412;&#12290;&#23613;&#31649;&#20943;&#23569;&#20102;&#36807;&#25311;&#21512;&#65292;&#32456;&#36523;&#22522;&#20934;&#24341;&#20837;&#20102;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#35780;&#20272;&#26085;&#30410;&#22686;&#22810;&#30340;&#27169;&#22411;&#22312;&#19981;&#26029;&#25193;&#22823;&#30340;&#26679;&#26412;&#38598;&#19978;&#30340;&#39640;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35780;&#20272;&#26694;&#26550;&#65306;Sort \&amp; Search (S&amp;S)&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#26377;&#36873;&#25321;&#22320;&#23545;&#27979;&#35797;&#26679;&#26412;&#36827;&#34892;&#25490;&#24207;&#21644;&#23376;&#36873;&#25321;&#65292;&#20351;&#24471;&#32456;&#36523;&#22522;&#20934;&#35780;&#20272;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#12290;&#36890;&#36807;&#23545;31,000&#20010;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19472v1 Announce Type: new  Abstract: Standardized benchmarks drive progress in machine learning. However, with repeated testing, the risk of overfitting grows as algorithms over-exploit benchmark idiosyncrasies. In our work, we seek to mitigate this challenge by compiling ever-expanding large-scale benchmarks called Lifelong Benchmarks. As exemplars of our approach, we create Lifelong-CIFAR10 and Lifelong-ImageNet, containing (for now) 1.69M and 1.98M test samples, respectively. While reducing overfitting, lifelong benchmarks introduce a key challenge: the high cost of evaluating a growing number of models across an ever-expanding sample set. To address this challenge, we also introduce an efficient evaluation framework: Sort \&amp; Search (S&amp;S), which reuses previously evaluated models by leveraging dynamic programming algorithms to selectively rank and sub-select test samples, enabling cost-effective lifelong benchmarking. Extensive empirical evaluations across 31,000 models 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#28210;&#26579;&#26041;&#27861;&#65292;&#21517;&#20026;FPA&#65292;&#36890;&#36807;&#23398;&#20064;&#23545;&#25239;&#27169;&#24335;&#24182;&#32467;&#21512;&#29305;&#27530;&#35774;&#35745;&#30340;&#23545;&#25239;&#25439;&#22833;&#21644;&#38544;&#34109;&#32422;&#26463;&#25439;&#22833;&#65292;&#21487;&#20197;&#29983;&#25104;&#29289;&#29702;&#19990;&#30028;&#20013;&#20855;&#26377;&#23545;&#25239;&#24615;&#21644;&#38544;&#34109;&#24615;&#36136;&#30340;&#20266;&#35013;&#12290;</title><link>https://arxiv.org/abs/2402.13575</link><description>&lt;p&gt;
&#22522;&#20110;&#24046;&#24322;&#26041;&#27861;&#30340;&#28789;&#27963;&#29289;&#29702;&#20266;&#35013;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Flexible Physical Camouflage Generation Based on a Differential Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13575
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#28210;&#26579;&#26041;&#27861;&#65292;&#21517;&#20026;FPA&#65292;&#36890;&#36807;&#23398;&#20064;&#23545;&#25239;&#27169;&#24335;&#24182;&#32467;&#21512;&#29305;&#27530;&#35774;&#35745;&#30340;&#23545;&#25239;&#25439;&#22833;&#21644;&#38544;&#34109;&#32422;&#26463;&#25439;&#22833;&#65292;&#21487;&#20197;&#29983;&#25104;&#29289;&#29702;&#19990;&#30028;&#20013;&#20855;&#26377;&#23545;&#25239;&#24615;&#21644;&#38544;&#34109;&#24615;&#36136;&#30340;&#20266;&#35013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#28210;&#26579;&#26041;&#27861;&#65292;&#19987;&#38376;&#38024;&#23545;&#23545;&#25239;&#20266;&#35013;&#65292;&#22312;&#24191;&#27867;&#30340;&#19977;&#32500;&#28210;&#26579;&#26694;&#26550;&#20869;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;FPA&#65292;&#36890;&#36807;&#24544;&#23454;&#22320;&#27169;&#25311;&#20809;&#29031;&#26465;&#20214;&#21644;&#26448;&#26009;&#21464;&#21270;&#65292;&#30830;&#20445;&#22312;&#19977;&#32500;&#30446;&#26631;&#19978;&#23545;&#32441;&#29702;&#36827;&#34892;&#24494;&#22937;&#32780;&#36924;&#30495;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#23398;&#20064;&#23545;&#25239;&#27169;&#24335;&#12290;&#36825;&#28041;&#21450;&#23558;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#30340;&#23545;&#25239;&#25439;&#22833;&#21644;&#38544;&#34109;&#32422;&#26463;&#25439;&#22833;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#30830;&#20445;&#20266;&#35013;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#24615;&#21644;&#38544;&#34109;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#20266;&#35013;&#22312;&#36148;&#32440;&#27169;&#24335;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#35206;&#30422;&#30446;&#26631;&#32780;&#19981;&#24433;&#21709;&#23545;&#25239;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29289;&#29702;&#23454;&#39564;&#65292;FPA&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#21487;&#36716;&#31227;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13575v1 Announce Type: cross  Abstract: This study introduces a novel approach to neural rendering, specifically tailored for adversarial camouflage, within an extensive 3D rendering framework. Our method, named FPA, goes beyond traditional techniques by faithfully simulating lighting conditions and material variations, ensuring a nuanced and realistic representation of textures on a 3D target. To achieve this, we employ a generative approach that learns adversarial patterns from a diffusion model. This involves incorporating a specially designed adversarial loss and covert constraint loss to guarantee the adversarial and covert nature of the camouflage in the physical world. Furthermore, we showcase the effectiveness of the proposed camouflage in sticker mode, demonstrating its ability to cover the target without compromising adversarial information. Through empirical and physical experiments, FPA exhibits strong performance in terms of attack success rate and transferabili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#23558;&#31070;&#32463;&#32593;&#32476;&#23618;&#35270;&#20026;&#20449;&#21495;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#36870;&#32593;&#32476;&#30340;&#27010;&#24565;&#21644;&#35745;&#31639;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#30340;&#36755;&#20837;&#22270;&#20687;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.00261</link><description>&lt;p&gt;
&#20351;&#29992;&#21521;&#37327;&#31354;&#38388;&#21644;&#36870;&#26144;&#23556;&#20102;&#35299;&#22270;&#20687;&#20998;&#26512;&#20013;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding Neural Network Systems for Image Analysis using Vector Spaces and Inverse Maps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#23558;&#31070;&#32463;&#32593;&#32476;&#23618;&#35270;&#20026;&#20449;&#21495;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#36870;&#32593;&#32476;&#30340;&#27010;&#24565;&#21644;&#35745;&#31639;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#30340;&#36755;&#20837;&#22270;&#20687;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#25968;&#23398;&#26041;&#27861;&#26469;&#29702;&#35299;&#22270;&#20687;&#20998;&#26512;&#20013;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#20855;&#26377;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#23558;&#31070;&#32463;&#32593;&#32476;&#23618;&#35270;&#20026;&#20449;&#21495;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#20449;&#21495;&#31354;&#38388;&#26469;&#21487;&#35270;&#21270;&#26435;&#37325;&#31354;&#38388;&#21644;&#21367;&#31215;&#23618;&#21367;&#31215;&#26680;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#36870;&#32593;&#32476;&#30340;&#27010;&#24565;&#21644;&#35745;&#31639;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#30340;&#36755;&#20837;&#22270;&#20687;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#21487;&#36870;&#32593;&#32476;&#21644;ResNet18&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is strong interest in developing mathematical methods that can be used to understand complex neural networks used in image analysis. In this paper, we introduce techniques from Linear Algebra to model neural network layers as maps between signal spaces. First, we demonstrate how signal spaces can be used to visualize weight spaces and convolutional layer kernels. We also demonstrate how residual vector spaces can be used to further visualize information lost at each layer. Second, we introduce the concept of invertible networks and an algorithm for computing input images that yield specific outputs. We demonstrate our approach on two invertible networks and ResNet18.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#39640;&#25928;&#21160;&#24577;&#25193;&#25955;&#27169;&#22411;&#65288;EMDM&#65289;&#65292;&#33021;&#22815;&#22312;&#26356;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#20013;&#23454;&#29616;&#24555;&#36895;&#19988;&#39640;&#36136;&#37327;&#30340;&#21160;&#20316;&#29983;&#25104;</title><link>https://arxiv.org/abs/2312.02256</link><description>&lt;p&gt;
&#39640;&#25928;&#21160;&#24577;&#25193;&#25955;&#27169;&#22411;&#65288;EMDM&#65289;&#29992;&#20110;&#24555;&#36895;&#19988;&#39640;&#36136;&#37327;&#30340;&#21160;&#20316;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
EMDM: Efficient Motion Diffusion Model for Fast and High-Quality Motion Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02256
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#39640;&#25928;&#21160;&#24577;&#25193;&#25955;&#27169;&#22411;&#65288;EMDM&#65289;&#65292;&#33021;&#22815;&#22312;&#26356;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#20013;&#23454;&#29616;&#24555;&#36895;&#19988;&#39640;&#36136;&#37327;&#30340;&#21160;&#20316;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#25928;&#30340;&#21160;&#24577;&#25193;&#25955;&#27169;&#22411;&#65288;EMDM&#65289;&#65292;&#29992;&#20110;&#24555;&#36895;&#19988;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#21160;&#20316;&#29983;&#25104;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#24335;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#24448;&#24448;&#22312;&#36861;&#27714;&#24555;&#36895;&#29983;&#25104;&#30340;&#21516;&#26102;&#29306;&#29298;&#20102;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EMDM&#65292;&#23427;&#36890;&#36807;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#22810;&#27425;&#37319;&#26679;&#27493;&#39588;&#20013;&#25429;&#25417;&#22797;&#26434;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#26356;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#21644;&#29983;&#25104;&#36807;&#31243;&#30340;&#26174;&#30528;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02256v2 Announce Type: replace-cross  Abstract: We introduce Efficient Motion Diffusion Model (EMDM) for fast and high-quality human motion generation. Current state-of-the-art generative diffusion models have produced impressive results but struggle to achieve fast generation without sacrificing quality. On the one hand, previous works, like motion latent diffusion, conduct diffusion within a latent space for efficiency, but learning such a latent space can be a non-trivial effort. On the other hand, accelerating generation by naively increasing the sampling step size, e.g., DDIM, often leads to quality degradation as it fails to approximate the complex denoising distribution. To address these issues, we propose EMDM, which captures the complex distribution during multiple sampling steps in the diffusion model, allowing for much fewer sampling steps and significant acceleration in generation. This is achieved by a conditional denoising diffusion GAN to capture multimodal da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25913;&#36827;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20923;&#32467;&#30340;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#39592;&#24178;&#65292;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#21407;&#22411;&#36873;&#25321;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.17093</link><description>&lt;p&gt;
&#29992;&#21407;&#22411;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#22522;&#30784;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient Out-of-Distribution Detection with Prototypical Semi-Supervised Learning and Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25913;&#36827;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20923;&#32467;&#30340;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#39592;&#24178;&#65292;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#21407;&#22411;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PAWS-VMK&#65292;&#19968;&#31181;&#25913;&#36827;&#30340;&#21407;&#22411;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21033;&#29992;&#20923;&#32467;&#30340;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#39592;&#24178;&#65292;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#20248;&#20110;&#20197;&#24448;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#32467;&#26524;&#65292;&#25913;&#36827;&#20102;Predicting View-Assignments With Support Samples&#65288;PAWS&#65289;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;(1) &#21442;&#25968;&#21270;von-Mises Fisher&#38543;&#26426;&#37051;&#22495;&#23884;&#20837;&#65288;vMF-SNE&#65289;&#26469;&#39044;&#35757;&#32451;&#25237;&#24433;&#22836;&#65292;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#23884;&#20837;;(2) &#21463;MixMatch&#21551;&#21457;&#30340;&#25439;&#22833;&#65292;&#36890;&#36807;&#23545;&#22810;&#35270;&#22270;&#30340;&#39044;&#27979;&#36827;&#34892;&#24179;&#22343;&#65292;&#25552;&#20379;&#27604;PAWS&#20013;&#20351;&#29992;&#30340;&#19968;&#33268;&#24615;&#25439;&#22833;&#26356;&#21487;&#38752;&#30340;&#30417;&#30563;&#20449;&#21495;;&#21644;(3) &#31616;&#21333;k-Means&#21407;&#22411;&#36873;&#25321;&#65288;SKMPS&#65289;&#65292;&#19968;&#31181;&#27604;&#20854;&#20182;&#26080;&#30417;&#30563;&#26631;&#31614;&#36873;&#25321;&#26041;&#27861;&#25552;&#20379;&#26356;&#20248;&#36234;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17093v2 Announce Type: replace-cross  Abstract: This paper describes PAWS-VMK, an improved approach to prototypical semi-supervised learning in the field of computer vision, specifically designed to utilize a frozen foundation model as the neural network backbone. This method outperforms previous results in semi-supervised learning and out-of-distribution (OOD) detection, improving upon the Predicting View-Assignments With Support Samples (PAWS) semi-supervised learning method. We introduce (1) parametric von-Mises Fisher Stochastic Neighbour Embedding (vMF-SNE) to pretrain the projection head using the high-quality embeddings of the foundation model; (2) a MixMatch inspired loss, where predictions across multiple views are averaged to provide a more reliable supervision signal compared to the consistency loss used in PAWS and (3) simple $k$-Means prototype selection (SKMPS), a technique that provides superior performance to other unsupervised label selection approaches in t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;CPR&#65289;&#65292;&#26088;&#22312;&#32852;&#21512;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#30446;&#26631;&#20154;&#21592;&#26816;&#32034;&#65292;&#24341;&#20837;&#38646;&#26679;&#26412;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;ZS-CPR&#65289;&#35299;&#20915;&#20102;CPR&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;Word4Per&#12290;</title><link>https://arxiv.org/abs/2311.16515</link><description>&lt;p&gt;
Word4Per: Zero-shot&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Word4Per: Zero-shot Composed Person Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16515
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;CPR&#65289;&#65292;&#26088;&#22312;&#32852;&#21512;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#30446;&#26631;&#20154;&#21592;&#26816;&#32034;&#65292;&#24341;&#20837;&#38646;&#26679;&#26412;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;ZS-CPR&#65289;&#35299;&#20915;&#20102;CPR&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;Word4Per&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#29305;&#23450;&#20154;&#21592;&#20855;&#26377;&#26497;&#22823;&#30340;&#31038;&#20250;&#25928;&#30410;&#21644;&#23433;&#20840;&#20215;&#20540;&#65292;&#36890;&#24120;&#28041;&#21450;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#32467;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;CPR&#65289;&#65292;&#26088;&#22312;&#32852;&#21512;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#30446;&#26631;&#20154;&#21592;&#26816;&#32034;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;CPR&#38656;&#35201;&#26114;&#36149;&#30340;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#32780;&#30446;&#21069;&#27809;&#26377;&#21487;&#29992;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#38646;&#26679;&#26412;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;ZS-CPR&#65289;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#39046;&#22495;&#30456;&#20851;&#25968;&#25454;&#35299;&#20915;&#20102;CPR&#38382;&#39064;&#32780;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#27880;&#37322;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#23398;&#20064;ZS-CPR&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;Word4Per&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25991;&#26412;&#21453;&#36716;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16515v2 Announce Type: replace-cross  Abstract: Searching for specific person has great social benefits and security value, and it often involves a combination of visual and textual information. Conventional person retrieval methods, whether image-based or text-based, usually fall short in effectively harnessing both types of information, leading to the loss of accuracy. In this paper, a whole new task called Composed Person Retrieval (CPR) is proposed to jointly utilize both image and text information for target person retrieval. However, the supervised CPR requires very costly manual annotation dataset, while there are currently no available resources. To mitigate this issue, we firstly introduce the Zero-shot Composed Person Retrieval (ZS-CPR), which leverages existing domain-related data to resolve the CPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we propose a two-stage learning framework, Word4Per, where a lightweight Textual Inversion Netw
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Octavius&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;MoE&#21644;LoRA&#25216;&#26415;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#35299;&#30721;&#22120;LoRA-MoE&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;2D&#21644;3D&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#32422;20%&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.02684</link><description>&lt;p&gt;
Octavius&#65306;&#36890;&#36807;MoE&#20943;&#36731;MLLM&#20013;&#30340;&#20219;&#21153;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Octavius: Mitigating Task Interference in MLLMs via MoE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02684
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Octavius&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;MoE&#21644;LoRA&#25216;&#26415;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#35299;&#30721;&#22120;LoRA-MoE&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;2D&#21644;3D&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#32422;20%&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#23558;&#23427;&#20204;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#23398;&#20064;&#12290;&#38543;&#30528;&#24341;&#20837;&#26356;&#22810;&#30340;&#24418;&#24335;&#21644;&#19979;&#28216;&#20219;&#21153;&#65292;&#36127;&#38754;&#20914;&#31361;&#21644;&#24178;&#25200;&#21487;&#33021;&#23545;&#24615;&#33021;&#20135;&#29983;&#26356;&#20005;&#37325;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#36825;&#31181;&#29616;&#35937;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#34987;&#24573;&#35270;&#20102;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;\mname &#30340;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#19982;Multimodal Large Language Models&#65288;MLLMs&#65289;&#19968;&#36215;&#36827;&#34892;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#20840;&#38754;&#30740;&#31350;&#21644;&#23454;&#39564;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#21644;&#20195;&#34920;&#24615;PEFT&#25216;&#26415;&#20043;&#19968;&#65292;&#21363;LoRA&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#35299;&#30721;&#22120;&#65292;&#31216;&#20026;LoRA-MoE&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#65288;&#32422;20\%&#30340;&#25913;&#36827;&#65289;&#34920;&#26126;&#20102;&#25105;&#20204;&#35774;&#35745;&#22312;&#21508;&#31181;2D&#21644;3D&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;&#20195;&#30721;&#21644;&#30456;&#24212;&#25968;&#25454;&#38598;&#23558;&#24456;&#24555;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02684v1 Announce Type: cross  Abstract: Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called \mname, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, \emph{i.e.,} LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. The experimental results (about 20\% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and corresponding dataset will be available soon.
&lt;/p&gt;</description></item><item><title>LOTUS&#26159;&#19968;&#31181;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#20854;&#25972;&#20010;&#23551;&#21629;&#20013;&#25345;&#32493;&#23398;&#20064;&#35299;&#20915;&#26032;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26500;&#24314;&#25216;&#33021;&#24211;&#65292;&#24182;&#20351;&#29992;&#20803;&#25511;&#21046;&#22120;&#28789;&#27963;&#32452;&#21512;&#25216;&#33021;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.02058</link><description>&lt;p&gt;
LOTUS&#65306;&#36890;&#36807;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#30340;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
LOTUS: Continual Imitation Learning for Robot Manipulation Through Unsupervised Skill Discovery. (arXiv:2311.02058v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02058
&lt;/p&gt;
&lt;p&gt;
LOTUS&#26159;&#19968;&#31181;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#20854;&#25972;&#20010;&#23551;&#21629;&#20013;&#25345;&#32493;&#23398;&#20064;&#35299;&#20915;&#26032;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26500;&#24314;&#25216;&#33021;&#24211;&#65292;&#24182;&#20351;&#29992;&#20803;&#25511;&#21046;&#22120;&#28789;&#27963;&#32452;&#21512;&#25216;&#33021;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LOTUS&#30340;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#20351;&#24471;&#29289;&#29702;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#20854;&#25972;&#20010;&#23551;&#21629;&#20013;&#25345;&#32493;&#32780;&#39640;&#25928;&#22320;&#23398;&#20064;&#35299;&#20915;&#26032;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;LOTUS&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#19968;&#31995;&#21015;&#26032;&#20219;&#21153;&#30340;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#26500;&#24314;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#25216;&#33021;&#24211;&#12290;LOTUS&#39318;&#20808;&#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#25345;&#32493;&#25216;&#33021;&#21457;&#29616;&#36807;&#31243;&#65292;&#35813;&#27169;&#22411;&#20174;&#26410;&#20998;&#27573;&#30340;&#28436;&#31034;&#20013;&#25552;&#21462;&#37325;&#22797;&#20986;&#29616;&#30340;&#25216;&#33021;&#27169;&#24335;&#12290;&#25345;&#32493;&#25216;&#33021;&#21457;&#29616;&#26356;&#26032;&#29616;&#26377;&#25216;&#33021;&#20197;&#36991;&#20813;&#23545;&#20197;&#21069;&#20219;&#21153;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#28155;&#21152;&#26032;&#25216;&#33021;&#20197;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;LOTUS&#35757;&#32451;&#19968;&#20010;&#20803;&#25511;&#21046;&#22120;&#65292;&#22312;&#32456;&#36523;&#23398;&#20064;&#36807;&#31243;&#20013;&#28789;&#27963;&#22320;&#32452;&#21512;&#21508;&#31181;&#25216;&#33021;&#26469;&#35299;&#20915;&#22522;&#20110;&#35270;&#35273;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;LOTUS&#22312;&#25104;&#21151;&#29575;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#22522;&#32447;&#26041;&#27861;11&#65285;&#20197;&#19978;&#65292;&#26174;&#31034;&#20102;&#20854;&#20248;&#36234;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce LOTUS, a continual imitation learning algorithm that empowers a physical robot to continuously and efficiently learn to solve new manipulation tasks throughout its lifespan. The core idea behind LOTUS is constructing an ever-growing skill library from a sequence of new tasks with a small number of human demonstrations. LOTUS starts with a continual skill discovery process using an open-vocabulary vision model, which extracts skills as recurring patterns presented in unsegmented demonstrations. Continual skill discovery updates existing skills to avoid catastrophic forgetting of previous tasks and adds new skills to solve novel tasks. LOTUS trains a meta-controller that flexibly composes various skills to tackle vision-based manipulation tasks in the lifelong learning process. Our comprehensive experiments show that LOTUS outperforms state-of-the-art baselines by over 11% in success rate, showing its superior knowledge transfer ability compared to prior methods. More result
&lt;/p&gt;</description></item><item><title>FocDepthFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;LSTM&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#28966;&#28857;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;&#12290;&#36890;&#36807;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;LSTM&#30340;&#38598;&#25104;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#20855;&#26377;&#23545;&#20219;&#24847;&#38271;&#24230;&#22534;&#26632;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11178</link><description>&lt;p&gt;
FocDepthFormer: &#20351;&#29992;LSTM&#30340;Transformer&#29992;&#20110;&#20174;&#28966;&#28857;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
FocDepthFormer: Transformer with LSTM for Depth Estimation from Focus. (arXiv:2310.11178v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11178
&lt;/p&gt;
&lt;p&gt;
FocDepthFormer&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;LSTM&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#28966;&#28857;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;&#12290;&#36890;&#36807;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;LSTM&#30340;&#38598;&#25104;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#20855;&#26377;&#23545;&#20219;&#24847;&#38271;&#24230;&#22534;&#26632;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#28966;&#28857;&#22534;&#26632;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#22534;&#26632;&#20013;&#30340;&#28966;&#28857;/&#31163;&#28966;&#32447;&#32034;&#25512;&#26029;&#28145;&#24230;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#22270;&#20687;&#22534;&#26632;&#19978;&#24212;&#29992;&#20108;&#32500;&#25110;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#26469;&#22788;&#29702;&#27492;&#38382;&#39064;&#65292;&#20197;&#22312;&#22270;&#20687;&#21644;&#22534;&#26632;&#20043;&#38388;&#23398;&#20064;&#29305;&#24449;&#12290;&#30001;&#20110;CNN&#30340;&#23616;&#37096;&#24615;&#36136;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#65292;&#24182;&#19988;&#23427;&#20204;&#34987;&#38480;&#21046;&#22312;&#22788;&#29702;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#20013;&#19968;&#33268;&#30340;&#22266;&#23450;&#25968;&#37327;&#30340;&#22534;&#26632;&#19978;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23545;&#20219;&#24847;&#38271;&#24230;&#22534;&#26632;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#65292;FocDepthFormer&#65292;&#20027;&#35201;&#30001;&#24102;&#26377;LSTM&#27169;&#22359;&#21644;CNN&#35299;&#30721;&#22120;&#30340;Transformer&#32452;&#25104;&#12290;Transformer&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#36890;&#36807;&#38544;&#21547;&#38750;&#23616;&#37096;&#20132;&#21449;&#21442;&#32771;&#33021;&#22815;&#23398;&#20064;&#26356;&#22810;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;LSTM&#27169;&#22359;&#34987;&#23398;&#20064;&#29992;&#20110;&#23558;&#34920;&#31034;&#38598;&#25104;&#21040;&#20855;&#26377;&#20219;&#24847;&#22270;&#20687;&#30340;&#22534;&#26632;&#20013;&#12290;&#20026;&#20102;&#30452;&#25509;&#25429;&#33719;&#20302;&#32423;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Depth estimation from focal stacks is a fundamental computer vision problem that aims to infer depth from focus/defocus cues in the image stacks. Most existing methods tackle this problem by applying convolutional neural networks (CNNs) with 2D or 3D convolutions over a set of fixed stack images to learn features across images and stacks. Their performance is restricted due to the local properties of the CNNs, and they are constrained to process a fixed number of stacks consistent in train and inference, limiting the generalization to the arbitrary length of stacks. To handle the above limitations, we develop a novel Transformer-based network, FocDepthFormer, composed mainly of a Transformer with an LSTM module and a CNN decoder. The self-attention in Transformer enables learning more informative features via an implicit non-local cross reference. The LSTM module is learned to integrate the representations across the stack with arbitrary images. To directly capture the low-level featur
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoSAM Adapter&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;SAM&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#28040;&#38500;&#20102;&#23545;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.14936</link><description>&lt;p&gt;
&#20026;&#31227;&#21160;&#21451;&#22909;&#30340;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#33258;&#21160;&#25552;&#31034;SAM
&lt;/p&gt;
&lt;p&gt;
Auto-Prompting SAM for Mobile Friendly 3D Medical Image Segmentation. (arXiv:2308.14936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14936
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoSAM Adapter&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;SAM&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#28040;&#38500;&#20102;&#23545;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM)&#24050;&#32463;&#34987;&#36805;&#36895;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;SAM&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#38500;&#20102;&#33258;&#28982;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#22806;&#65292;2D&#21644;3D&#22270;&#20687;&#20043;&#38388;&#30340;&#31354;&#38388;&#24067;&#23616;&#24046;&#24322;&#65292;&#24378;&#22823;&#30340;GPU&#26381;&#21153;&#22120;&#25152;&#24102;&#26469;&#30340;&#22823;&#37327;&#35745;&#31639;&#36127;&#25285;&#65292;&#20197;&#21450;&#32791;&#26102;&#30340;&#25163;&#21160;&#25552;&#31034;&#29983;&#25104;&#20351;&#24471;SAM&#26080;&#27861;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;AutoSAM Adapter&#65292;&#19987;&#20026;3D&#22810;&#22120;&#23448;CT&#20998;&#21106;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#20197;&#20419;&#36827;&#23558;SAM&#27169;&#22411;&#30340;&#33021;&#21147;&#36716;&#21270;&#20026;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) has rapidly been adopted for segmenting a wide range of natural images. However, recent studies have indicated that SAM exhibits subpar performance on 3D medical image segmentation tasks. In addition to the domain gaps between natural and medical images, disparities in the spatial arrangement between 2D and 3D images, the substantial computational burden imposed by powerful GPU servers, and the time-consuming manual prompt generation impede the extension of SAM to a broader spectrum of medical image segmentation applications. To address these challenges, in this work, we introduce a novel method, AutoSAM Adapter, designed specifically for 3D multi-organ CT-based segmentation. We employ parameter-efficient adaptation techniques in developing an automatic prompt learning paradigm to facilitate the transformation of the SAM model's capabilities to 3D medical image segmentation, eliminating the need for manually generated prompts. Furthermore, we effectivel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.09059</link><description>&lt;p&gt;
&#25991;&#23383;&#24819;&#35937;&#30340;&#37322;&#25918;&#65306;&#36890;&#36807;&#25506;&#32034;&#25991;&#23383;&#30340;&#21147;&#37327;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words. (arXiv:2307.09059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#30340;&#30446;&#26631;&#26159;&#20174;&#22823;&#22411;&#22270;&#24211;&#20013;&#26816;&#32034;&#19982;&#32473;&#23450;&#25991;&#26412;&#25551;&#36848;&#30456;&#21305;&#37197;&#30340;&#20154;&#29289;&#22270;&#20687;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#20449;&#24687;&#34920;&#31034;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#25991;&#26412;&#27169;&#24577;&#36890;&#36807;&#35789;&#27719;&#21644;&#35821;&#27861;&#32467;&#26500;&#20256;&#36882;&#25277;&#35937;&#21644;&#31934;&#30830;&#30340;&#20449;&#24687;&#65292;&#32780;&#35270;&#35273;&#27169;&#24577;&#36890;&#36807;&#22270;&#20687;&#20256;&#36882;&#20855;&#20307;&#21644;&#30452;&#35266;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#25991;&#23383;&#34920;&#31034;&#30340;&#34920;&#36798;&#21147;&#65292;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#22270;&#20687;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#21477;&#23376;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#37322;&#25918;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#20154;&#29289;&#26816;&#32034;&#20013;&#30340;&#25991;&#23383;&#24819;&#35937;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#20840;&#38754;CLIP&#27169;&#22411;&#20316;&#20026;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21452;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#20808;&#21069;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Text-to-image person retrieval is to retrieve person images from a large gallery that match the given textual descriptions. The main challenge of this task lies in the significant differences in information representation between the visual and textual modalities. The textual modality conveys abstract and precise information through vocabulary and grammatical structures, while the visual modality conveys concrete and intuitive information through images. To fully leverage the expressive power of textual representations, it is essential to accurately map abstract textual descriptions to specific images.  To address this issue, we propose a novel framework to Unleash the Imagination of Text (UIT) in text-to-image person retrieval, aiming to fully explore the power of words in sentences. Specifically, the framework employs the pre-trained full CLIP model as a dual encoder for the images and texts , taking advantage of prior cross-modal alignment knowledge. The Text-guided Imag
&lt;/p&gt;</description></item></channel></rss>