<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#25512;&#29702;&#65288;SINF&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#32858;&#31867;&#35821;&#20041;&#30456;&#20284;&#30340;&#31867;&#26469;&#25552;&#21462;&#23376;&#22270;&#65292;&#20174;&#32780;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#26377;&#38480;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2310.01259</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#20041;&#25512;&#29702;&#23454;&#29616;&#26356;&#24555;&#26356;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Faster and Accurate Neural Networks with Semantic Inference. (arXiv:2310.01259v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#25512;&#29702;&#65288;SINF&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#32858;&#31867;&#35821;&#20041;&#30456;&#20284;&#30340;&#31867;&#26469;&#25552;&#21462;&#23376;&#22270;&#65292;&#20174;&#32780;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#26377;&#38480;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#19987;&#38376;&#29992;&#20110;&#31227;&#21160;&#35774;&#22791;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20250;&#23548;&#33268;&#26126;&#26174;&#30340;&#20934;&#30830;&#29575;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#20869;&#22312;&#20887;&#20313;&#26469;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#26377;&#38480;&#25439;&#22833;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#36755;&#20837;&#20849;&#20139;&#35768;&#22810;&#28388;&#27874;&#22120;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#26089;&#30340;&#23618;&#27425;&#19978;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#23545;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#31867;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#21019;&#24314;&#29305;&#23450;&#20110;&#32858;&#31867;&#30340;&#23376;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35821;&#20041;&#25512;&#29702;&#65288;SINF&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;SINF&#65288;i&#65289;&#20351;&#29992;&#19968;&#20010;&#23567;&#30340;&#38468;&#21152;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#23545;&#35937;&#23646;&#20110;&#30340;&#35821;&#20041;&#32858;&#31867;&#65292;&#24182;&#65288;ii&#65289;&#25191;&#34892;&#19982;&#35813;&#35821;&#20041;&#32858;&#31867;&#30456;&#20851;&#30340;&#22522;&#26412;DNN&#25552;&#21462;&#30340;&#23376;&#22270;&#36827;&#34892;&#25512;&#29702;&#12290;&#20026;&#20102;&#25552;&#21462;&#27599;&#20010;&#29305;&#23450;&#20110;&#32858;&#31867;&#30340;&#23376;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#21306;&#20998;&#33021;&#21147;&#24471;&#20998;&#65288;DCS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#20855;&#26377;&#21306;&#20998;&#33021;&#21147;&#30340;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) usually come with a significant computational burden. While approaches such as structured pruning and mobile-specific DNNs have been proposed, they incur drastic accuracy loss. In this paper we leverage the intrinsic redundancy in latent representations to reduce the computational load with limited loss in performance. We show that semantically similar inputs share many filters, especially in the earlier layers. Thus, semantically similar classes can be clustered to create cluster-specific subgraphs. To this end, we propose a new framework called Semantic Inference (SINF). In short, SINF (i) identifies the semantic cluster the object belongs to using a small additional classifier and (ii) executes the subgraph extracted from the base DNN related to that semantic cluster for inference. To extract each cluster-specific subgraph, we propose a new approach named Discriminative Capability Score (DCS) that finds the subgraph with the capability to discriminate amon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;CNN&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#37322;&#20013;&#38388;&#23618;&#30340;&#34920;&#31034;&#65292;&#25552;&#21462;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#32467;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#37117;&#24456;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.10523</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35299;&#37322;&#24615;&#22522;&#30784;&#25277;&#21462;&#29992;&#20110;&#22522;&#20110;&#27010;&#24565;&#30340;&#35270;&#35273;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Interpretable Basis Extraction for Concept-Based Visual Explanations. (arXiv:2303.10523v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;CNN&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#37322;&#20013;&#38388;&#23618;&#30340;&#34920;&#31034;&#65292;&#25552;&#21462;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#32467;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#37117;&#24456;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#23581;&#35797;&#29992;&#20154;&#31867;&#21487;&#20197;&#29702;&#35299;&#30340;&#27010;&#24565;&#26469;&#35299;&#37322;CNN&#22270;&#20687;&#20998;&#31867;&#22120;&#39044;&#27979;&#21644;&#20013;&#38388;&#23618;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26597;&#25214;&#35299;&#37322;&#20687;&#32032;&#28608;&#27963;&#30340;&#31232;&#30095;&#20108;&#20540;&#21270;&#36716;&#25442;&#34920;&#31034;&#30340;&#29305;&#24449;&#31354;&#38388;&#26059;&#36716;&#26469;&#25552;&#21462;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#27969;&#34892;CNN&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#25552;&#21462;&#35299;&#37322;&#24615;&#22522;&#30784;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25991;&#29486;&#20013;&#30340;&#22522;&#30784;&#21487;&#35299;&#37322;&#24615;&#24230;&#37327;&#65292;&#24182;&#34920;&#26126;&#65292;&#24403;&#20013;&#38388;&#23618;&#34920;&#31034;&#34987;&#36716;&#25442;&#20026;&#25105;&#20204;&#26041;&#27861;&#25552;&#21462;&#30340;&#22522;&#30784;&#26102;&#65292;&#23427;&#20204;&#21464;&#24471;&#26356;&#26131;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important line of research attempts to explain CNN image classifier predictions and intermediate layer representations in terms of human understandable concepts. In this work, we expand on previous works in the literature that use annotated concept datasets to extract interpretable feature space directions and propose an unsupervised post-hoc method to extract a disentangling interpretable basis by looking for the rotation of the feature space that explains sparse one-hot thresholded transformed representations of pixel activations. We do experimentation with existing popular CNNs and demonstrate the effectiveness of our method in extracting an interpretable basis across network architectures and training datasets. We make extensions to the existing basis interpretability metrics found in the literature and show that, intermediate layer representations become more interpretable when transformed to the bases extracted with our method. Finally, using the basis interpretability metrics
&lt;/p&gt;</description></item></channel></rss>