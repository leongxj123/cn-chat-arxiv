<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>Synapse&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#26377;&#38480;&#28436;&#31034;&#20013;&#39640;&#25928;&#23398;&#20064;&#20559;&#22909;&#27010;&#24565;&#65292;&#36890;&#36807;&#23558;&#20559;&#22909;&#34920;&#31034;&#20026;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#24182;&#21033;&#29992;&#35270;&#35273;&#35299;&#26512;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#31243;&#24207;&#21512;&#25104;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#20010;&#20154;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.16689</link><description>&lt;p&gt;
Synapse: &#20174;&#35270;&#35273;&#28436;&#31034;&#20013;&#23398;&#20064;&#20248;&#20808;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Synapse: Learning Preferential Concepts from Visual Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16689
&lt;/p&gt;
&lt;p&gt;
Synapse&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#26377;&#38480;&#28436;&#31034;&#20013;&#39640;&#25928;&#23398;&#20064;&#20559;&#22909;&#27010;&#24565;&#65292;&#36890;&#36807;&#23558;&#20559;&#22909;&#34920;&#31034;&#20026;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#24182;&#21033;&#29992;&#35270;&#35273;&#35299;&#26512;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#31243;&#24207;&#21512;&#25104;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#20010;&#20154;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#26088;&#22312;&#20174;&#35270;&#35273;&#36755;&#20837;&#20013;&#23398;&#20064;&#29992;&#25143;&#29305;&#23450;&#20559;&#22909;&#65288;&#20363;&#22914;&#65292;&#8220;&#22909;&#20572;&#36710;&#20301;&#8221;&#65292;&#8220;&#26041;&#20415;&#30340;&#19979;&#36710;&#20301;&#32622;&#8221;&#65289;&#12290;&#23613;&#31649;&#19982;&#23398;&#20064;&#20107;&#23454;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#8220;&#32418;&#33394;&#31435;&#26041;&#20307;&#8221;&#65289;&#30456;&#20284;&#65292;&#20294;&#20559;&#22909;&#23398;&#20064;&#26159;&#19968;&#20010;&#22522;&#26412;&#26356;&#21152;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#20027;&#35266;&#24615;&#36136;&#21644;&#20010;&#20154;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#30340;&#32570;&#20047;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Synapse&#30340;&#26032;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#20174;&#26377;&#38480;&#28436;&#31034;&#20013;&#23398;&#20064;&#20559;&#22909;&#27010;&#24565;&#12290;Synapse&#23558;&#20559;&#22909;&#34920;&#31034;&#20026;&#22312;&#22270;&#20687;&#19978;&#36816;&#20316;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65288;DSL&#65289;&#20013;&#30340;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;&#35299;&#26512;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#31243;&#24207;&#21512;&#25104;&#30340;&#26032;&#32452;&#21512;&#26469;&#23398;&#20064;&#20195;&#34920;&#20010;&#20154;&#20559;&#22909;&#30340;&#31243;&#24207;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;Synapse&#65292;&#21253;&#25324;&#19968;&#20010;&#20851;&#27880;&#19982;&#31227;&#21160;&#30456;&#20851;&#30340;&#29992;&#25143;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16689v1 Announce Type: cross  Abstract: This paper addresses the problem of preference learning, which aims to learn user-specific preferences (e.g., "good parking spot", "convenient drop-off location") from visual input. Despite its similarity to learning factual concepts (e.g., "red cube"), preference learning is a fundamentally harder problem due to its subjective nature and the paucity of person-specific training data. We address this problem using a new framework called Synapse, which is a neuro-symbolic approach designed to efficiently learn preferential concepts from limited demonstrations. Synapse represents preferences as neuro-symbolic programs in a domain-specific language (DSL) that operates over images, and leverages a novel combination of visual parsing, large language models, and program synthesis to learn programs representing individual preferences. We evaluate Synapse through extensive experimentation including a user case study focusing on mobility-related
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23500;&#25991;&#26412;&#32534;&#36753;&#22120;&#29983;&#25104;&#34920;&#36798;&#24615;&#25991;&#26412;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23616;&#37096;&#26679;&#24335;&#25511;&#21046;&#12289;&#26126;&#30830;&#30340;&#26631;&#35760;&#37325;&#26032;&#21152;&#26435;&#12289;&#31934;&#30830;&#30340;&#39068;&#33394;&#28210;&#26579;&#21644;&#35814;&#32454;&#30340;&#21306;&#22495;&#21512;&#25104;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2304.06720</link><description>&lt;p&gt;
&#23500;&#25991;&#26412;&#29983;&#25104;&#34920;&#36798;&#24615;&#25991;&#26412;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Expressive Text-to-Image Generation with Rich Text. (arXiv:2304.06720v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23500;&#25991;&#26412;&#32534;&#36753;&#22120;&#29983;&#25104;&#34920;&#36798;&#24615;&#25991;&#26412;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23616;&#37096;&#26679;&#24335;&#25511;&#21046;&#12289;&#26126;&#30830;&#30340;&#26631;&#35760;&#37325;&#26032;&#21152;&#26435;&#12289;&#31934;&#30830;&#30340;&#39068;&#33394;&#28210;&#26579;&#21644;&#35814;&#32454;&#30340;&#21306;&#22495;&#21512;&#25104;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32431;&#25991;&#26412;&#24050;&#32463;&#25104;&#20026;&#25991;&#23383;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#27969;&#34892;&#30028;&#38754;&#12290;&#20294;&#26159;&#65292;&#23427;&#30340;&#23450;&#21046;&#36873;&#39033;&#26377;&#38480;&#65292;&#38459;&#30861;&#20102;&#29992;&#25143;&#31934;&#30830;&#25551;&#36848;&#25152;&#38656;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25903;&#25345;&#23383;&#20307;&#26679;&#24335;&#12289;&#22823;&#23567;&#12289;&#39068;&#33394;&#21644;&#33050;&#27880;&#31561;&#26684;&#24335;&#30340;&#23500;&#25991;&#26412;&#32534;&#36753;&#22120;&#12290;&#25105;&#20204;&#20174;&#23500;&#25991;&#26412;&#20013;&#25552;&#21462;&#27599;&#20010;&#23383;&#30340;&#23646;&#24615;&#65292;&#20197;&#21551;&#29992;&#23616;&#37096;&#26679;&#24335;&#25511;&#21046;&#12289;&#26126;&#30830;&#30340;&#26631;&#35760;&#37325;&#26032;&#21152;&#26435;&#12289;&#31934;&#30830;&#30340;&#39068;&#33394;&#28210;&#26579;&#21644;&#35814;&#32454;&#30340;&#21306;&#22495;&#21512;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#21306;&#22495;&#30340;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#20102;&#36825;&#20123;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#27604;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#26041;&#27861;&#26356;&#22909;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plain text has become a prevalent interface for text-to-image synthesis. However, its limited customization options hinder users from accurately describing desired outputs. For example, plain text makes it hard to specify continuous quantities, such as the precise RGB color value or importance of each word. Furthermore, creating detailed text prompts for complex scenes is tedious for humans to write and challenging for text encoders to interpret. To address these challenges, we propose using a rich-text editor supporting formats such as font style, size, color, and footnote. We extract each word's attributes from rich text to enable local style control, explicit token reweighting, precise color rendering, and detailed region synthesis. We achieve these capabilities through a region-based diffusion process. We first obtain each word's region based on cross-attention maps of a vanilla diffusion process using plain text. For each region, we enforce its text attributes by creating region-s
&lt;/p&gt;</description></item></channel></rss>