<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>3DCoMPaT$^{++}$&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;2D/3D&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.6&#20159;&#20010;&#28210;&#26579;&#35270;&#22270;&#30340;&#39118;&#26684;&#21270;&#19977;&#32500;&#24418;&#29366;&#65292;&#24102;&#26377;&#35814;&#32454;&#30340;&#37096;&#20214;&#23454;&#20363;&#32423;&#21035;&#26631;&#27880;&#65292;&#29992;&#20110;&#32452;&#21512;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2310.18511</link><description>&lt;p&gt;
3DCoMPaT$^{++}$&#65306;&#19968;&#20010;&#29992;&#20110;&#32452;&#21512;&#35782;&#21035;&#30340;&#25913;&#36827;&#22411;&#22823;&#35268;&#27169;&#19977;&#32500;&#35270;&#35273;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
3DCoMPaT$^{++}$: An improved Large-scale 3D Vision Dataset for Compositional Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18511
&lt;/p&gt;
&lt;p&gt;
3DCoMPaT$^{++}$&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;2D/3D&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.6&#20159;&#20010;&#28210;&#26579;&#35270;&#22270;&#30340;&#39118;&#26684;&#21270;&#19977;&#32500;&#24418;&#29366;&#65292;&#24102;&#26377;&#35814;&#32454;&#30340;&#37096;&#20214;&#23454;&#20363;&#32423;&#21035;&#26631;&#27880;&#65292;&#29992;&#20110;&#32452;&#21512;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;3DCoMPaT$^{++}$&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;1.6&#20159;&#20010;&#20197;&#19978;10&#30334;&#19975;&#20010;&#39118;&#26684;&#21270;&#19977;&#32500;&#24418;&#29366;&#30340;&#28210;&#26579;&#35270;&#22270;&#30340;&#22810;&#27169;&#24577;2D/3D&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#24418;&#29366;&#22312;&#37096;&#20214;&#23454;&#20363;&#32423;&#21035;&#19978;&#36827;&#34892;&#20102;&#31934;&#24515;&#27880;&#37322;&#65292;&#24182;&#37197;&#26377;&#21305;&#37197;&#30340;RGB&#28857;&#20113;&#12289;3D&#32441;&#29702;&#32593;&#26684;&#12289;&#28145;&#24230;&#22270;&#21644;&#20998;&#21106;&#33945;&#29256;&#12290;3DCoMPaT$^{++}$&#28085;&#30422;&#20102;41&#20010;&#24418;&#29366;&#31867;&#21035;&#12289;275&#20010;&#32454;&#31890;&#24230;&#37096;&#20998;&#31867;&#21035;&#21644;293&#20010;&#32454;&#31890;&#24230;&#26448;&#26009;&#31867;&#21035;&#65292;&#36825;&#20123;&#31867;&#21035;&#21487;&#20197;&#32452;&#21512;&#24212;&#29992;&#20110;&#19977;&#32500;&#29289;&#20307;&#30340;&#21508;&#37096;&#20998;&#12290;&#25105;&#20204;&#20174;&#22235;&#20010;&#31561;&#38388;&#36317;&#35270;&#22270;&#21644;&#22235;&#20010;&#38543;&#26426;&#35270;&#22270;&#20013;&#28210;&#26579;&#20102;&#19968;&#30334;&#19975;&#20010;&#39118;&#26684;&#21270;&#24418;&#29366;&#30340;&#23376;&#38598;&#65292;&#20849;&#35745;1.6&#20159;&#20010;&#28210;&#26579;&#12290;&#37096;&#20214;&#22312;&#23454;&#20363;&#32423;&#21035;&#12289;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#35821;&#20041;&#32423;&#21035;&#19978;&#36827;&#34892;&#20102;&#20998;&#21106;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Grounded CoMPaT Recognition (GCR)&#30340;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#20849;&#21516;&#35782;&#21035;&#21644;&#22522;&#20110;&#29289;&#20307;&#37096;&#20998;&#30340;&#26448;&#26009;&#32452;&#21512;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#20010;&#25968;&#25454;&#25361;&#25112;&#27963;&#21160;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18511v2 Announce Type: replace-cross  Abstract: In this work, we present 3DCoMPaT$^{++}$, a multimodal 2D/3D dataset with 160 million rendered views of more than 10 million stylized 3D shapes carefully annotated at the part-instance level, alongside matching RGB point clouds, 3D textured meshes, depth maps, and segmentation masks. 3DCoMPaT$^{++}$ covers 41 shape categories, 275 fine-grained part categories, and 293 fine-grained material classes that can be compositionally applied to parts of 3D objects. We render a subset of one million stylized shapes from four equally spaced views as well as four randomized views, leading to a total of 160 million renderings. Parts are segmented at the instance level, with coarse-grained and fine-grained semantic levels. We introduce a new task, called Grounded CoMPaT Recognition (GCR), to collectively recognize and ground compositions of materials on parts of 3D objects. Additionally, we report the outcomes of a data challenge organized a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Prophet&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#31572;&#26696;&#21551;&#21457;&#24335;&#26041;&#24335;&#20419;&#20351;GPT-3&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#12290;&#22312;&#29305;&#23450;&#30340;&#30693;&#35782;&#22411;VQA&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32431;VQA&#27169;&#22411;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#20986;&#31572;&#26696;&#21551;&#21457;&#24335;&#65292;&#21487;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.01903</link><description>&lt;p&gt;
&#29992;&#31572;&#26696;&#21551;&#21457;&#24335;&#26041;&#24335;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering. (arXiv:2303.01903v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Prophet&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#31572;&#26696;&#21551;&#21457;&#24335;&#26041;&#24335;&#20419;&#20351;GPT-3&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#12290;&#22312;&#29305;&#23450;&#30340;&#30693;&#35782;&#22411;VQA&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32431;VQA&#27169;&#22411;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#20986;&#31572;&#26696;&#21551;&#21457;&#24335;&#65292;&#21487;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#38656;&#35201;&#36229;&#20986;&#22270;&#20687;&#33539;&#22260;&#30340;&#22806;&#37096;&#30693;&#35782;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#20174;&#26174;&#24335;&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#26816;&#32034;&#25152;&#38656;&#30340;&#30693;&#35782;&#65292;&#36825;&#32463;&#24120;&#20250;&#24341;&#20837;&#19982;&#38382;&#39064;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#35797;&#22270;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;GPT-3&#65289;&#20316;&#20026;&#38544;&#21547;&#24335;&#30693;&#35782;&#24341;&#25806;&#26469;&#33719;&#21462;&#22238;&#31572;&#25152;&#38656;&#30340;&#24517;&#35201;&#30693;&#35782;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#23427;&#20204;&#36824;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;GPT-3&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#25552;&#20379;&#30340;&#36755;&#20837;&#20449;&#24687;&#20173;&#28982;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prophet&#8212;&#8212;&#19968;&#20010;&#27010;&#24565;&#19978;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22238;&#31572;&#21551;&#21457;&#24335;&#26041;&#24335;&#65292;&#20419;&#20351;GPT-3&#35299;&#20915;&#22522;&#20110;&#30693;&#35782;&#30340;VQA&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#29305;&#23450;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;VQA&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#32431;VQA&#27169;&#22411;&#65292;&#32780;&#19981;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#20013;&#25552;&#21462;&#20102;&#20004;&#31181;&#20114;&#34917;&#30340;&#31572;&#26696;&#21551;&#21457;&#24335;&#65306;&#31572;&#26696;&#20505;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-based visual question answering (VQA) requires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have sought to use a large language model (i.e., GPT-3) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of GPT-3 as the provided input information is insufficient. In this paper, we present Prophet -- a conceptually simple framework designed to prompt GPT-3 with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the model: answer candidates 
&lt;/p&gt;</description></item></channel></rss>