<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#24403;&#21069;&#22522;&#20110;3D&#39592;&#26550;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;&#26041;&#27861;&#12289;&#27169;&#22411;&#35774;&#35745;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#31995;&#32479;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#30456;&#20851;&#30740;&#31350;&#24635;&#32467;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2401.15296</link><description>&lt;p&gt;
&#22522;&#20110;3D&#39592;&#26550;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;&#65306;&#26041;&#27861;&#12289;&#35774;&#35745;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on 3D Skeleton Based Person Re-Identification: Approaches, Designs, Challenges, and Future Directions. (arXiv:2401.15296v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#24403;&#21069;&#22522;&#20110;3D&#39592;&#26550;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;&#26041;&#27861;&#12289;&#27169;&#22411;&#35774;&#35745;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#31995;&#32479;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#30456;&#20851;&#30740;&#31350;&#24635;&#32467;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;3D&#39592;&#26550;&#36827;&#34892;&#20154;&#21592;&#20877;&#35782;&#21035;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#65292;&#24341;&#36215;&#20102;&#27169;&#24335;&#35782;&#21035;&#31038;&#21306;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#39592;&#26550;&#24314;&#27169;&#21644;&#29305;&#24449;&#23398;&#20064;&#20013;&#31361;&#20986;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20855;&#26377;&#29420;&#29305;&#20248;&#21183;&#30340;&#22522;&#20110;3D&#39592;&#26550;&#30340;&#20154;&#21592;&#20877;&#35782;&#21035;&#65288;SRID&#65289;&#26041;&#27861;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23545;&#36825;&#20123;&#30740;&#31350;&#21450;&#20854;&#25361;&#25112;&#36827;&#34892;&#32508;&#21512;&#24635;&#32467;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#23545;&#24403;&#21069;SRID&#26041;&#27861;&#12289;&#27169;&#22411;&#35774;&#35745;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#31995;&#32479;&#35843;&#30740;&#65292;&#35797;&#22270;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;SRID&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;SRID&#30740;&#31350;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#24635;&#32467;&#20102;&#24120;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#24120;&#29992;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#23545;&#19981;&#21516;&#26041;&#27861;&#30340;&#29305;&#28857;&#36827;&#34892;&#20102;&#20998;&#26512;&#35780;&#20215;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;SRID&#27169;&#22411;&#30340;&#35774;&#35745;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Person re-identification via 3D skeletons is an important emerging research area that triggers great interest in the pattern recognition community. With distinctive advantages for many application scenarios, a great diversity of 3D skeleton based person re-identification (SRID) methods have been proposed in recent years, effectively addressing prominent problems in skeleton modeling and feature learning. Despite recent advances, to the best of our knowledge, little effort has been made to comprehensively summarize these studies and their challenges. In this paper, we attempt to fill this gap by providing a systematic survey on current SRID approaches, model designs, challenges, and future directions. Specifically, we first formulate the SRID problem, and propose a taxonomy of SRID research with a summary of benchmark datasets, commonly-used model architectures, and an analytical review of different methods' characteristics. Then, we elaborate on the design principles of SRID models fro
&lt;/p&gt;</description></item><item><title>HCVP&#26159;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#23545;&#27604;&#35270;&#35273;&#25552;&#31034;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#23558;&#19981;&#21464;&#29305;&#24449;&#19982;&#29305;&#23450;&#29305;&#24449;&#20998;&#31163;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09716</link><description>&lt;p&gt;
HCVP: &#22522;&#20110;&#23618;&#27425;&#23545;&#27604;&#35270;&#35273;&#25552;&#31034;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain Generalization. (arXiv:2401.09716v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09716
&lt;/p&gt;
&lt;p&gt;
HCVP&#26159;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#23545;&#27604;&#35270;&#35273;&#25552;&#31034;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#23558;&#19981;&#21464;&#29305;&#24449;&#19982;&#29305;&#23450;&#29305;&#24449;&#20998;&#31163;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#19981;&#21464;&#29305;&#24449;&#26469;&#21019;&#24314;&#22312;&#26410;&#30693;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;DG&#20013;&#65292;&#23558;&#27169;&#22411;&#38480;&#21046;&#22312;&#22266;&#23450;&#32467;&#26500;&#25110;&#32479;&#19968;&#21442;&#25968;&#21270;&#20013;&#20197;&#21253;&#21547;&#19981;&#21464;&#29305;&#24449;&#30340;&#20027;&#27969;&#23454;&#36341;&#21487;&#33021;&#20250;&#19981;&#21487;&#36991;&#20813;&#22320;&#34701;&#21512;&#29305;&#23450;&#26041;&#38754;&#12290;&#36825;&#31181;&#26041;&#27861;&#38590;&#20197;&#23545;&#39046;&#22495;&#38388;&#21464;&#21270;&#36827;&#34892;&#32454;&#24494;&#21306;&#20998;&#65292;&#21487;&#33021;&#23545;&#26576;&#20123;&#39046;&#22495;&#23384;&#22312;&#20559;&#35265;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23545;&#22495;&#19981;&#21464;&#29305;&#24449;&#30340;&#31934;&#30830;&#23398;&#20064;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#20026;&#27169;&#22411;&#25552;&#20379;&#39046;&#22495;&#32423;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#26356;&#26377;&#25928;&#22320;&#24341;&#23548;&#27169;&#22411;&#23558;&#19981;&#21464;&#29305;&#24449;&#19982;&#29305;&#23450;&#29305;&#24449;&#20998;&#31163;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#39046;&#22495;&#27867;&#21270;&#33539;&#24335;&#20013;&#65292;&#20511;&#37492;&#20102;&#35270;&#35273;&#25552;&#31034;&#30340;&#26032;&#36235;&#21183;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;HCVP&#8221;&#65288;&#23618;&#27425;&#23545;&#27604;&#35270;&#35273;&#25552;&#31034;&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization (DG) endeavors to create machine learning models that excel in unseen scenarios by learning invariant features. In DG, the prevalent practice of constraining models to a fixed structure or uniform parameterization to encapsulate invariant features can inadvertently blend specific aspects. Such an approach struggles with nuanced differentiation of inter-domain variations and may exhibit bias towards certain domains, hindering the precise learning of domain-invariant features. Recognizing this, we introduce a novel method designed to supplement the model with domain-level and task-specific characteristics. This approach aims to guide the model in more effectively separating invariant features from specific characteristics, thereby boosting the generalization. Building on the emerging trend of visual prompts in the DG paradigm, our work introduces the novel \textbf{H}ierarchical \textbf{C}ontrastive \textbf{V}isual \textbf{P}rompt (HCVP) methodology. This represents 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23646;&#24615;&#21487;&#33021;&#26377;&#26356;&#23569;&#30340;&#26377;&#20851;&#20915;&#31574;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#36825;&#31181;&#20559;&#24046;&#31216;&#20026;&#27611;&#31961;&#24230;&#20559;&#24046;&#65292;&#24182;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#22312;ROAR&#25351;&#26631;&#19978;&#36827;&#34892;&#30450;&#30446;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2304.13836</link><description>&lt;p&gt;
&#35770;RemOve-And-Retrain&#30340;&#38519;&#38449;&#65306;&#25968;&#25454;&#22788;&#29702;&#19981;&#31561;&#24335;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
On Pitfalls of $\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective. (arXiv:2304.13836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23646;&#24615;&#21487;&#33021;&#26377;&#26356;&#23569;&#30340;&#26377;&#20851;&#20915;&#31574;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#36825;&#31181;&#20559;&#24046;&#31216;&#20026;&#27611;&#31961;&#24230;&#20559;&#24046;&#65292;&#24182;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#22312;ROAR&#25351;&#26631;&#19978;&#36827;&#34892;&#30450;&#30446;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#65292;&#35813;&#21327;&#35758;&#29992;&#20110;&#27979;&#37327;&#29305;&#24449;&#37325;&#35201;&#24615;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#32972;&#26223;&#21644;&#23454;&#35777;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#20855;&#26377;&#36739;&#23569;&#26377;&#20851;&#20915;&#31574;&#21151;&#33021;&#30340;&#20449;&#24687;&#30340;&#23646;&#24615;&#22312;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#19982;ROAR&#30340;&#21407;&#22987;&#30446;&#30340;&#30456;&#30683;&#30462;&#12290;&#36825;&#31181;&#29616;&#35937;&#20063;&#20986;&#29616;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;&#21464;&#20307;RemOve-And-Debias&#65288;ROAD&#65289;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ROAR&#24402;&#22240;&#24230;&#37327;&#20013;&#27611;&#31961;&#24230;&#20559;&#24046;&#30340;&#19968;&#33268;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#30450;&#30446;&#20381;&#36182;ROAR&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper assesses the reliability of the RemOve-And-Retrain (ROAR) protocol, which is used to measure the performance of feature importance estimates. Our findings from the theoretical background and empirical experiments indicate that attributions that possess less information about the decision function can perform better in ROAR benchmarks, conflicting with the original purpose of ROAR. This phenomenon is also observed in the recently proposed variant RemOve-And-Debias (ROAD), and we propose a consistent trend of blurriness bias in ROAR attribution metrics. Our results caution against uncritical reliance on ROAR metrics.
&lt;/p&gt;</description></item></channel></rss>