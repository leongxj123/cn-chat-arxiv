<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36890;&#36807;&#22312;&#19978;&#21322;&#36523;&#29983;&#25104;&#25163;&#35821;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#65292;&#24182;&#30001;&#25163;&#35821;&#19987;&#23478;&#23457;&#26680;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#20013;&#30340;&#31215;&#26497;&#24212;&#29992;&#65292;&#20026;&#32843;&#21713;&#21644;&#21548;&#38556;&#31038;&#21306;&#24102;&#26469;&#28508;&#22312;&#30340;&#20581;&#24247;&#21644;&#25945;&#32946;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2404.01438</link><description>&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#25163;&#35821;&#30340;&#29983;&#25104;&#19982;&#26816;&#27979;-&#35821;&#35328;&#21644;&#35270;&#35273;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Generation and Detection of Sign Language Deepfakes - A Linguistic and Visual Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01438
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#19978;&#21322;&#36523;&#29983;&#25104;&#25163;&#35821;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#65292;&#24182;&#30001;&#25163;&#35821;&#19987;&#23478;&#23457;&#26680;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#20013;&#30340;&#31215;&#26497;&#24212;&#29992;&#65292;&#20026;&#32843;&#21713;&#21644;&#21548;&#38556;&#31038;&#21306;&#24102;&#26469;&#28508;&#22312;&#30340;&#20581;&#24247;&#21644;&#25945;&#32946;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#39046;&#22495;&#20013;&#19968;&#20010;&#36880;&#28176;&#20986;&#29616;&#30340;&#38382;&#39064;&#26159;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#36229;&#36234;&#38754;&#37096;&#28145;&#24230;&#20266;&#36896;&#65292;&#20197;&#21450;&#36825;&#23545;&#31038;&#20250;&#26159;&#21542;&#26377;&#30410;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#19978;&#21322;&#36523;&#29983;&#25104;&#20013;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#30340;&#31215;&#26497;&#24212;&#29992;&#65292;&#21516;&#26102;&#20026;&#32843;&#21713;&#21644;&#21548;&#38556;&#65288;DHoH&#65289;&#31038;&#21306;&#25191;&#34892;&#25163;&#35821;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#19968;&#20301;&#25163;&#35821;&#19987;&#23478;&#23545;&#29983;&#25104;&#30340;&#35270;&#39057;&#36827;&#34892;&#23457;&#26680;&#12290;&#37492;&#20110;&#25163;&#35821;&#30340;&#22797;&#26434;&#24615;&#12289;&#25163;&#35821;&#19987;&#23478;&#30340;&#21294;&#20047;&#20197;&#21450;&#23545;&#20581;&#24247;&#21644;&#25945;&#32946;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#36825;&#31181;&#20570;&#27861;&#23588;&#20026;&#26377;&#30410;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#21253;&#25324;&#26500;&#24314;&#21487;&#38752;&#30340;&#28145;&#24230;&#20266;&#36896;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#35780;&#20272;&#20854;&#25216;&#26415;&#21644;&#35270;&#35273;&#21487;&#20449;&#24230;&#65292;&#20197;&#21450;&#35780;&#20272;&#29983;&#25104;&#20869;&#23481;&#30340;&#21487;&#20449;&#24230;&#12290;&#20351;&#29992;1200&#22810;&#20010;&#35270;&#39057;&#65292;&#27169;&#22411;&#21253;&#21547;&#20197;&#21069;&#35265;&#36807;&#21644;&#26410;&#35265;&#36807;&#30340;&#20010;&#20307;&#65292;&#20511;&#21161;&#19968;&#20301;&#25163;&#35821;&#19987;&#23478;&#30340;&#24110;&#21161;&#36827;&#34892;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01438v1 Announce Type: cross  Abstract: A question in the realm of deepfakes is slowly emerging pertaining to whether we can go beyond facial deepfakes and whether it would be beneficial to society. Therefore, this research presents a positive application of deepfake technology in upper body generation, while performing sign-language for the Deaf and Hard of Hearing (DHoH) community. The resulting videos are later vetted with a sign language expert. This is particularly helpful, given the intricate nature of sign language, a scarcity of sign language experts, and potential benefits for health and education. The objectives of this work encompass constructing a reliable deepfake dataset, evaluating its technical and visual credibility through computer vision and natural language processing models, and assessing the plausibility of the generated content. With over 1200 videos, featuring both previously seen and unseen individuals for the generation model, using the help of a si
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.16970</link><description>&lt;p&gt;
&#32852;&#21512;&#33016;&#37096;X&#20809;&#35786;&#26029;&#21644;&#20020;&#24202;&#35270;&#35273;&#27880;&#24847;&#21147;&#39044;&#27979;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#23398;&#20064;&#65306;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16970
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#33258;&#21160;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20020;&#24202;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#36825;&#19968;&#39046;&#22495;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#20294;&#22312;&#25918;&#23556;&#23398;&#31579;&#26597;&#36807;&#31243;&#20013;&#20020;&#24202;&#21307;&#29983;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#22270;&#20026;&#25552;&#20379;&#37325;&#35201;&#27934;&#23519;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#36164;&#20135;&#65292;&#24182;&#26377;&#21487;&#33021;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#36825;&#31687;&#35770;&#25991;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#65292;&#21033;&#29992;&#20102;DenseNet201&#20027;&#24178;&#21644;&#22522;&#20110;&#27531;&#24046;&#21644;&#33192;&#32960;&#28608;&#21169;&#22359;&#30340;&#32534;&#30721;&#22120;&#26469;&#25552;&#21462;&#29992;&#20110;&#26174;&#33879;&#24615;&#22270;&#39044;&#27979;&#30340;&#22810;&#26679;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#36827;&#34892;&#30142;&#30149;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16970v1 Announce Type: cross  Abstract: As deep learning has become the state-of-the-art for computer-assisted diagnosis, interpretability of the automatic decisions is crucial for clinical deployment. While various methods were proposed in this domain, visual attention maps of clinicians during radiological screening offer a unique asset to provide important insights and can potentially enhance the quality of computer-assisted diagnosis. With this paper, we introduce a novel deep-learning framework for joint disease diagnosis and prediction of corresponding visual saliency maps for chest X-ray scans. Specifically, we designed a novel dual-encoder multi-task UNet, which leverages both a DenseNet201 backbone and a Residual and Squeeze-and-Excitation block-based encoder to extract diverse features for saliency map prediction, and a multi-scale feature-fusion classifier to perform disease classification. To tackle the issue of asynchronous training schedules of individual tasks
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29702;&#27169;&#22359;STATM&#65292;&#21033;&#29992;&#35760;&#24518;&#32531;&#20914;&#21306;&#22686;&#24378;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.15245</link><description>&lt;p&gt;
&#35270;&#39057;&#30340;&#22686;&#24378;&#25512;&#29702;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reasoning-Enhanced Object-Centric Learning for Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15245
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29702;&#27169;&#22359;STATM&#65292;&#21033;&#29992;&#35760;&#24518;&#32531;&#20914;&#21306;&#22686;&#24378;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#26088;&#22312;&#23558;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#20998;&#35299;&#20026;&#26356;&#26131;&#22788;&#29702;&#30340;&#29289;&#20307;&#34920;&#31034;&#65292;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#29289;&#29702;&#19990;&#30028;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#27133;&#20301;&#30340;&#35270;&#39057;&#27169;&#22411;&#23637;&#29616;&#20986;&#22312;&#20998;&#21106;&#21644;&#36319;&#36394;&#29289;&#20307;&#26041;&#38754;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20294;&#24573;&#35270;&#20102;&#26377;&#25928;&#25512;&#29702;&#27169;&#22359;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#35760;&#24518;&#32531;&#20914;&#21306;&#30340;&#22522;&#20110;&#27133;&#20301;&#30340;&#26102;&#31354;&#21464;&#25442;&#22120;&#65288;STATM&#65289;&#30340;&#26032;&#22411;&#25512;&#29702;&#27169;&#22359;&#12290;&#35760;&#24518;&#32531;&#20914;&#21306;&#20027;&#35201;&#29992;&#20110;&#23384;&#20648;&#26469;&#33258;&#19978;&#28216;&#27169;&#22359;&#30340;&#27133;&#20301;&#20449;&#24687;&#65292;&#22522;&#20110;&#27133;&#20301;&#30340;&#26102;&#31354;&#21464;&#25442;&#22120;&#36890;&#36807;&#27133;&#20301;&#20026;&#22522;&#30784;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15245v1 Announce Type: cross  Abstract: Object-centric learning aims to break down complex visual scenes into more manageable object representations, enhancing the understanding and reasoning abilities of machine learning systems toward the physical world. Recently, slot-based video models have demonstrated remarkable proficiency in segmenting and tracking objects, but they overlook the importance of the effective reasoning module. In the real world, reasoning and predictive abilities play a crucial role in human perception and object tracking; in particular, these abilities are closely related to human intuitive physics. Inspired by this, we designed a novel reasoning module called the Slot-based Time-Space Transformer with Memory buffer (STATM) to enhance the model's perception ability in complex scenes. The memory buffer primarily serves as storage for slot information from upstream modules, the Slot-based Time-Space Transformer makes predictions through slot-based spatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#22788;&#29702;&#26465;&#20214;&#35774;&#32622;&#22312;EHR&#30340;&#20540;&#21644;&#27979;&#37327;&#19978;&#65292;&#20197;&#25972;&#21512;&#20020;&#24202;&#25104;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#65292;&#26088;&#22312;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#20013;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.13319</link><description>&lt;p&gt;
HyperFusion&#65306;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#24314;&#27169;&#30340;&#22810;&#27169;&#24577;&#25972;&#21512;&#34920;&#26684;&#21644;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#30340;&#36229;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HyperFusion: A Hypernetwork Approach to Multimodal Integration of Tabular and Medical Imaging Data for Predictive Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13319
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#22788;&#29702;&#26465;&#20214;&#35774;&#32622;&#22312;EHR&#30340;&#20540;&#21644;&#27979;&#37327;&#19978;&#65292;&#20197;&#25972;&#21512;&#20020;&#24202;&#25104;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#65292;&#26088;&#22312;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#20013;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ARXIV: 2403.13319v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#25972;&#21512;&#21508;&#31181;&#20020;&#24202;&#27169;&#24335;&#65292;&#22914;&#21307;&#23398;&#25104;&#20687;&#21644;&#24739;&#32773;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#33719;&#24471;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#26159;&#29616;&#20195;&#21307;&#30103;&#20445;&#20581;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#22810;&#28304;&#25968;&#25454;&#30340;&#32508;&#21512;&#20998;&#26512;&#21487;&#20197;&#20840;&#38754;&#20102;&#35299;&#24739;&#32773;&#30340;&#29366;&#20917;&#65292;&#24182;&#21487;&#20197;&#22686;&#24378;&#35786;&#26029;&#21644;&#27835;&#30103;&#20915;&#31574;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#19968;&#30452;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#21307;&#23398;&#25104;&#20687;&#19982;&#20197;&#25968;&#23383;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#30340;&#20020;&#24202;&#12289;&#20154;&#21475;&#32479;&#35745;&#21644;&#36951;&#20256;&#20449;&#24687;&#36827;&#34892;&#34701;&#21512;&#30340;&#22797;&#26434;&#21162;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#39640;&#24230;&#27963;&#36291;&#30340;&#25345;&#32493;&#30740;&#31350;&#36861;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13319v1 Announce Type: cross  Abstract: The integration of diverse clinical modalities such as medical imaging and the tabular data obtained by the patients' Electronic Health Records (EHRs) is a crucial aspect of modern healthcare. The integrative analysis of multiple sources can provide a comprehensive understanding of a patient's condition and can enhance diagnoses and treatment decisions. Deep Neural Networks (DNNs) consistently showcase outstanding performance in a wide range of multimodal tasks in the medical domain. However, the complex endeavor of effectively merging medical imaging with clinical, demographic and genetic information represented as numerical tabular data remains a highly active and ongoing research pursuit.   We present a novel framework based on hypernetworks to fuse clinical imaging and tabular data by conditioning the image processing on the EHR's values and measurements. This approach aims to leverage the complementary information present in these
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#25552;&#31034;&#21040;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#25928;&#26524;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.12326</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#23398;&#20064;&#25552;&#31034;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Removing Undesirable Concepts in Text-to-Image Generative Models with Learnable Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12326
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#25552;&#31034;&#21040;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#25928;&#26524;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#35270;&#35273;&#19978;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20869;&#23481;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26410;&#32463;&#31579;&#36873;&#30340;&#20114;&#32852;&#32593;&#25968;&#25454;&#19978;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#23398;&#20064;&#21644;&#38543;&#21518;&#20256;&#25773;&#19981;&#33391;&#27010;&#24565;&#65288;&#22914;&#21463;&#29256;&#26435;&#20445;&#25252;&#25110;&#19981;&#36947;&#24503;&#20869;&#23481;&#65289;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#25552;&#31034;&#32467;&#21512;&#21040;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#65292;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;&#12290;&#36825;&#21487;&#23398;&#20064;&#25552;&#31034;&#20805;&#24403;&#38468;&#21152;&#20869;&#23384;&#65292;&#23558;&#19981;&#33391;&#27010;&#24565;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#20854;&#20013;&#65292;&#24182;&#20943;&#23569;&#36825;&#20123;&#27010;&#24565;&#23545;&#27169;&#22411;&#21442;&#25968;&#21644;&#30456;&#24212;&#25991;&#26412;&#36755;&#20837;&#30340;&#20381;&#36182;&#12290;&#30001;&#20110;&#36825;&#31181;&#30693;&#35782;&#36716;&#31227;&#21040;&#25552;&#31034;&#20013;&#65292;&#28040;&#38500;&#36825;&#20123;&#19981;&#33391;&#27010;&#24565;&#26356;&#21152;&#31283;&#23450;&#65292;&#24182;&#23545;&#20854;&#20182;&#27010;&#24565;&#24433;&#21709;&#26368;&#23567;&#12290;&#25105;&#20204;&#22312;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12326v1 Announce Type: new  Abstract: Generative models have demonstrated remarkable potential in generating visually impressive content from textual descriptions. However, training these models on unfiltered internet data poses the risk of learning and subsequently propagating undesirable concepts, such as copyrighted or unethical content. In this paper, we propose a novel method to remove undesirable concepts from text-to-image generative models by incorporating a learnable prompt into the cross-attention module. This learnable prompt acts as additional memory to transfer the knowledge of undesirable concepts into it and reduce the dependency of these concepts on the model parameters and corresponding textual inputs. Because of this knowledge transfer into the prompt, erasing these undesirable concepts is more stable and has minimal negative impact on other concepts. We demonstrate the effectiveness of our method on the Stable Diffusion model, showcasing its superiority ov
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65288;NSI&#65289;&#65292;&#36890;&#36807;&#27133;&#34920;&#31034;&#23398;&#20064;&#25509;&#22320;&#21644;&#29983;&#25104;&#29289;&#20307;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#20307;&#35821;&#20041;&#32467;&#21512;&#21040;&#25277;&#35937;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.07887</link><description>&lt;p&gt;
&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65306;&#22312;&#26032;&#20852;&#30340;&#27133;&#34920;&#31034;&#20013;&#25509;&#22320;&#23545;&#35937;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Neural Slot Interpreters: Grounding Object Semantics in Emergent Slot Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07887
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65288;NSI&#65289;&#65292;&#36890;&#36807;&#27133;&#34920;&#31034;&#23398;&#20064;&#25509;&#22320;&#21644;&#29983;&#25104;&#29289;&#20307;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#20307;&#35821;&#20041;&#32467;&#21512;&#21040;&#25277;&#35937;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#20013;&#24515;&#26041;&#27861;&#22312;&#23558;&#21407;&#22987;&#24863;&#30693;&#26080;&#30417;&#30563;&#20998;&#35299;&#20026;&#20016;&#23500;&#30340;&#31867;&#20284;&#29289;&#20307;&#30340;&#25277;&#35937;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#20307;&#35821;&#20041;&#25509;&#22320;&#21040;&#23398;&#21040;&#30340;&#25277;&#35937;&#20013;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#19979;&#28216;&#29702;&#35299;&#24212;&#29992;&#20013;&#30340;&#37319;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65288;NSI&#65289;&#65292;&#23427;&#36890;&#36807;&#27133;&#34920;&#31034;&#23398;&#20064;&#25509;&#22320;&#21644;&#29983;&#25104;&#29289;&#20307;&#35821;&#20041;&#12290;NSI&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#31867;&#20284;XML&#30340;&#32534;&#31243;&#35821;&#35328;&#65292;&#23427;&#20351;&#29992;&#31616;&#21333;&#30340;&#35821;&#27861;&#35268;&#21017;&#23558;&#22330;&#26223;&#30340;&#29289;&#20307;&#35821;&#20041;&#32452;&#32455;&#25104;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#31243;&#24207;&#21407;&#35821;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#23545;&#40784;&#27169;&#22411;&#23398;&#20064;&#36890;&#36807;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#19978;&#30340;&#21452;&#23618;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#23558;&#31243;&#24207;&#21407;&#35821;&#25509;&#22320;&#21040;&#27133;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;NSI&#31243;&#24207;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#40784;&#27169;&#22411;&#25512;&#26029;&#30340;&#23494;&#38598;&#20851;&#32852;&#20174;&#27133;&#29983;&#25104;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#31243;&#24207;&#12290;&#22312;&#21452;&#27169;&#24335;&#26816;&#32034;&#23454;&#39564;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07887v1 Announce Type: cross  Abstract: Object-centric methods have seen significant progress in unsupervised decomposition of raw perception into rich object-like abstractions. However, limited ability to ground object semantics of the real world into the learned abstractions has hindered their adoption in downstream understanding applications. We present the Neural Slot Interpreter (NSI) that learns to ground and generate object semantics via slot representations. At the core of NSI is an XML-like programming language that uses simple syntax rules to organize the object semantics of a scene into object-centric program primitives. Then, an alignment model learns to ground program primitives into slots through a bi-level contrastive learning objective over a shared embedding space. Finally, we formulate the NSI program generator model to use the dense associations inferred from the alignment model to generate object-centric programs from slots. Experiments on bi-modal retrie
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01643</link><description>&lt;p&gt;
&#24744;&#38656;&#35201;&#26356;&#22909;&#22320;&#20851;&#27880;&#20184;&#36153;
&lt;/p&gt;
&lt;p&gt;
You Need to Pay Better Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01643
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36825;&#20123;&#26426;&#21046;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#32988;&#36807;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#20248;&#21270;&#27880;&#24847;&#21147;&#65292;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#30456;&#20284;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#23569;&#20102;&#22235;&#20998;&#20043;&#19977;&#65292;&#27599;&#20010;&#22836;&#37096;&#23569;&#20102;&#19968;&#20010;&#30697;&#38453;&#20056;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#25928;&#27880;&#24847;&#21147;&#65292;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#30456;&#24403;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;&#19968;&#21322;&#65292;&#27599;&#20010;&#22836;&#37096;&#20943;&#23569;&#20102;&#20004;&#20010;&#30697;&#38453;&#20056;&#27861;&#65292;&#24182;&#19988;&#27604;&#26631;&#20934;&#27880;&#24847;&#21147;&#24555;&#20004;&#20493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36229;&#32423;&#27880;&#24847;&#21147;&#65292;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26126;&#26174;&#36229;&#36234;&#20102;&#26631;&#20934;&#27880;&#24847;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#30697;&#38453;&#20056;&#27861;&#12290;&#38500;&#20102;&#25552;&#20379;&#20005;&#26684;&#30340;&#25968;&#23398;&#27604;&#36739;&#65292;&#25105;&#20204;&#22312;MN&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01643v1 Announce Type: cross  Abstract: We introduce three new attention mechanisms that outperform standard multi-head attention in terms of efficiency and learning capabilities, thereby improving the performance and broader deployability of Transformer models. Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3/4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1/2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications. In addition to providing rigorous mathematical comparisons, we evaluate the presented attention mechanisms on MN
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;BiDf-MKD&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;API&#24211;&#20013;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#30452;&#25509;&#36827;&#34892;&#20803;&#23398;&#20064;&#65307;&#33021;&#22815;&#22312;&#26356;&#24191;&#27867;&#30340;&#40657;&#30418;API&#19978;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20803;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2305.18413</link><description>&lt;p&gt;
&#20174;API&#23398;&#20064;&#23398;&#20064;&#65306;&#40657;&#30418;&#25968;&#25454;&#26080;&#20851;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Learn from APIs: Black-Box Data-Free Meta-Learning. (arXiv:2305.18413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18413
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;BiDf-MKD&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;API&#24211;&#20013;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#30452;&#25509;&#36827;&#34892;&#20803;&#23398;&#20064;&#65307;&#33021;&#22815;&#22312;&#26356;&#24191;&#27867;&#30340;&#40657;&#30418;API&#19978;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20803;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#65288;DFML&#65289;&#26088;&#22312;&#36890;&#36807;&#20174;&#19968;&#32452;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20803;&#23398;&#20064;&#32780;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;DFML&#24037;&#20316;&#20165;&#33021;&#20174;&#65288;i&#65289;&#30333;&#30418;&#21644;&#65288;ii&#65289;&#23567;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;iii&#65289;&#30456;&#21516;&#30340;&#26550;&#26500;&#20013;&#20803;&#23398;&#20064;&#65292;&#24573;&#30053;&#20102;&#26356;&#23454;&#38469;&#30340;&#35774;&#32622;&#65292;&#21363;&#29992;&#25143;&#20165;&#33021;&#36890;&#36807;&#20219;&#24847;&#27169;&#22411;&#26550;&#26500;&#21644;&#35268;&#27169;&#30340;API&#36827;&#34892;&#25512;&#26029;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#25968;&#25454;&#26080;&#20851;&#20803;&#30693;&#35782;&#33976;&#39311;&#65288;BiDf-MKD&#65289;&#26694;&#26550;&#65292;&#23558;&#26356;&#36890;&#29992;&#30340;&#20803;&#30693;&#35782;&#20174;&#19968;&#32452;&#40657;&#30418;API&#36716;&#31227;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#20803;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-free meta-learning (DFML) aims to enable efficient learning of new tasks by meta-learning from a collection of pre-trained models without access to the training data. Existing DFML work can only meta-learn from (i) white-box and (ii) small-scale pre-trained models (iii) with the same architecture, neglecting the more practical setting where the users only have inference access to the APIs with arbitrary model architectures and model scale inside. To solve this issue, we propose a Bi-level Data-free Meta Knowledge Distillation (BiDf-MKD) framework to transfer more general meta knowledge from a collection of black-box APIs to one single meta model. Specifically, by just querying APIs, we inverse each API to recover its training data via a zero-order gradient estimator and then perform meta-learning via a novel bi-level meta knowledge distillation structure, in which we design a boundary query set recovery technique to recover a more informative query set near the decision boundary. 
&lt;/p&gt;</description></item><item><title>GPT-NAS&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#20248;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#25552;&#20986;&#36817;&#20284;&#30340;&#26550;&#26500;&#32452;&#20214;&#20943;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05351</link><description>&lt;p&gt;
GPT-NAS: &#20197;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
GPT-NAS: Neural Architecture Search with the Generative Pre-Trained Model. (arXiv:2305.05351v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05351
&lt;/p&gt;
&lt;p&gt;
GPT-NAS&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#20248;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#25552;&#20986;&#36817;&#20284;&#30340;&#26550;&#26500;&#32452;&#20214;&#20943;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#24050;&#32463;&#25104;&#20026;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#26368;&#20248;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#34429;&#28982;&#19968;&#20123;&#20154;&#24037;&#35774;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;NAS&#26041;&#27861;&#20013;&#24456;&#23569;&#20986;&#29616;&#36825;&#31867;&#25104;&#26524;&#65292;&#20027;&#35201;&#21407;&#22240;&#22312;&#20110;&#31070;&#32463;&#26550;&#26500;&#30340;&#25628;&#32034;&#31354;&#38388;&#22826;&#22823;&#20102;&#65292;&#23548;&#33268;NAS&#31639;&#27861;&#25928;&#29575;&#20302;&#19979;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;GPT-NAS&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#20248;&#21270;&#31070;&#32463;&#26550;&#26500;&#12290;&#22312;GPT-NAS&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#19968;&#20010;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#26500;&#24314;&#31070;&#32463;&#26550;&#26500;&#30340;&#22522;&#26412;&#35268;&#24459;&#12290;&#22240;&#27492;&#65292;GPT-NAS&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25552;&#20986;&#21512;&#29702;&#30340;&#26550;&#26500;&#32452;&#20214;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#25628;&#32034;&#31354;&#38388;&#65292;&#24341;&#20837;&#20102;&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;GPT-NAS&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) has emerged as one of the effective methods to design the optimal neural network architecture automatically. Although neural architectures have achieved human-level performances in several tasks, few of them are obtained from the NAS method. The main reason is the huge search space of neural architectures, making NAS algorithms inefficient. This work presents a novel architecture search algorithm, called GPT-NAS, that optimizes neural architectures by Generative Pre-Trained (GPT) model. In GPT-NAS, we assume that a generative model pre-trained on a large-scale corpus could learn the fundamental law of building neural architectures. Therefore, GPT-NAS leverages the generative pre-trained (GPT) model to propose reasonable architecture components given the basic one. Such an approach can largely reduce the search space by introducing prior knowledge in the search process. Extensive experimental results show that our GPT-NAS method significantly outperforms
&lt;/p&gt;</description></item><item><title>&#19981;&#21463;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35268;&#27169;&#38480;&#21046;&#30340;&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#26694;&#26550;PURER&#65292;&#36890;&#36807;ECI&#25191;&#34892;&#20266;&#21608;&#26399;&#35757;&#32451;&#20197;&#36866;&#24212;&#26032;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;ICFIL&#23545;&#21453;&#28436;&#26799;&#24230;&#36827;&#34892;&#26657;&#20934;&#26469;&#20248;&#21270;&#21453;&#28436;&#36807;&#31243;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11183</link><description>&lt;p&gt;
&#19981;&#21463;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35268;&#27169;&#38480;&#21046;&#30340;&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning. (arXiv:2303.11183v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11183
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21463;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35268;&#27169;&#38480;&#21046;&#30340;&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#26694;&#26550;PURER&#65292;&#36890;&#36807;ECI&#25191;&#34892;&#20266;&#21608;&#26399;&#35757;&#32451;&#20197;&#36866;&#24212;&#26032;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;ICFIL&#23545;&#21453;&#28436;&#26799;&#24230;&#36827;&#34892;&#26657;&#20934;&#26469;&#20248;&#21270;&#21453;&#28436;&#36807;&#31243;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#30340;&#30446;&#30340;&#26159;&#20174;&#19968;&#32452;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#20854;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#35299;&#20915;&#20102;&#35813;&#38382;&#39064;&#65292;&#24573;&#30053;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#34164;&#21547;&#30340;&#20016;&#23500;&#25968;&#25454;&#30693;&#35782;&#65292;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21482;&#33021;&#20803;&#23398;&#20064;&#20855;&#26377;&#30456;&#21516;&#32593;&#32476;&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#8212;&#8212;PURER&#65292;&#20854;&#20013;&#21253;&#21547;&#65306;&#65288;1&#65289;&#25968;&#25454;&#26080;&#20851;&#30340;&#20803;&#35757;&#32451;&#26399;&#38388;&#30340;&#33410;&#30446;&#35838;&#31243;&#21453;&#36716;&#65288;ECI&#65289;&#65307;&#65288;2&#65289;&#20803;&#27979;&#35797;&#26399;&#38388;&#20869;&#37096;&#24490;&#29615;&#21518;&#30340;&#21453;&#28436;&#26657;&#20934;&#65288;ICFIL&#65289;&#12290;&#22312;&#20803;&#35757;&#32451;&#26399;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ECI&#26469;&#25191;&#34892;&#20266;&#21608;&#26399;&#35757;&#32451;&#65292;&#20197;&#20415;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#30475;&#19981;&#35265;&#30340;&#20219;&#21153;&#12290;&#22312;&#20803;&#27979;&#35797;&#26399;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ICFIL&#26469;&#26657;&#20934;&#21453;&#28436;&#26799;&#24230;&#65292;&#20197;&#20943;&#23569;&#22522;&#20110;&#21453;&#28436;&#30340;&#20248;&#21270;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;PURER&#21487;&#20197;&#26377;&#25928;&#22320;&#20803;&#23398;&#20064;&#26469;&#33258;&#20855;&#26377;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#12289;&#25968;&#25454;&#38598;&#22495;&#29978;&#33267;&#19981;&#21516;&#22823;&#23567;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of data-free meta-learning is to learn useful prior knowledge from a collection of pre-trained models without accessing their training data. However, existing works only solve the problem in parameter space, which (i) ignore the fruitful data knowledge contained in the pre-trained models; (ii) can not scale to large-scale pre-trained models; (iii) can only meta-learn pre-trained models with the same network architecture. To address those issues, we propose a unified framework, dubbed PURER, which contains: (1) ePisode cUrriculum inveRsion (ECI) during data-free meta training; and (2) invErsion calibRation following inner loop (ICFIL) during meta testing. During meta training, we propose ECI to perform pseudo episode training for learning to adapt fast to new unseen tasks. Specifically, we progressively synthesize a sequence of pseudo episodes by distilling the training data from each pre-trained model. The ECI adaptively increases the difficulty level of pseudo episodes accord
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#19982;&#20154;&#31867;&#24515;&#29702;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#21457;&#29616;&#27169;&#22411;&#35268;&#27169;&#21644;&#20307;&#31995;&#32467;&#26500;&#23545;&#23545;&#40784;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#30446;&#26631;&#20989;&#25968;&#37117;&#23545;&#23545;&#40784;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#20174;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#32447;&#24615;&#21464;&#25442;&#33021;&#26174;&#33879;&#25552;&#39640;&#23545;&#21478;&#22806;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#30340;&#23545;&#40784;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.01201</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Human alignment of neural network representations. (arXiv:2211.01201v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#19982;&#20154;&#31867;&#24515;&#29702;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#21457;&#29616;&#27169;&#22411;&#35268;&#27169;&#21644;&#20307;&#31995;&#32467;&#26500;&#23545;&#23545;&#40784;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#30446;&#26631;&#20989;&#25968;&#37117;&#23545;&#23545;&#40784;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#20174;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#32447;&#24615;&#21464;&#25442;&#33021;&#26174;&#33879;&#25552;&#39640;&#23545;&#21478;&#22806;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#30340;&#23545;&#40784;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#20154;&#31867;&#25110;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#21644;&#23398;&#20064;&#31639;&#27861;&#19982;&#23548;&#33268;&#20154;&#31867;&#35270;&#35273;&#30340;&#26041;&#24335;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#20043;&#22788;&#12290;&#26412;&#25991;&#30740;&#31350;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#25152;&#23398;&#20064;&#30340;&#34920;&#31034;&#19982;&#36890;&#36807;&#34892;&#20026;&#21453;&#24212;&#25512;&#26029;&#20986;&#30340;&#20154;&#31867;&#24515;&#29702;&#34920;&#31034;&#20043;&#38388;&#23545;&#40784;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#22411;&#30340;&#35268;&#27169;&#21644;&#20307;&#31995;&#32467;&#26500;&#23545;&#19982;&#20154;&#31867;&#34892;&#20026;&#21453;&#24212;&#30340;&#23545;&#40784;&#22522;&#26412;&#19978;&#27809;&#26377;&#24433;&#21709;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#30446;&#26631;&#20989;&#25968;&#21017;&#20855;&#26377;&#26356;&#22823;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#22312;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#20219;&#21153;&#25910;&#38598;&#30340;&#19977;&#20010;&#20154;&#31867;&#30456;&#20284;&#24230;&#21028;&#26029;&#25968;&#25454;&#38598;&#20013;&#20445;&#25345;&#19968;&#33268;&#12290;&#20174;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#32447;&#24615;&#21464;&#25442;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#21478;&#22806;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#21028;&#26029;&#30340;&#23545;&#40784;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20123;&#20154;&#31867;&#27010;&#24565;...
&lt;/p&gt;
&lt;p&gt;
Today's computer vision models achieve human or near-human level performance across a wide variety of vision tasks. However, their architectures, data, and learning algorithms differ in numerous ways from those that give rise to human vision. In this paper, we investigate the factors that affect the alignment between the representations learned by neural networks and human mental representations inferred from behavioral responses. We find that model scale and architecture have essentially no effect on the alignment with human behavioral responses, whereas the training dataset and objective function both have a much larger impact. These findings are consistent across three datasets of human similarity judgments collected using two different tasks. Linear transformations of neural network representations learned from behavioral responses from one dataset substantially improve alignment with human similarity judgments on the other two datasets. In addition, we find that some human concept
&lt;/p&gt;</description></item></channel></rss>