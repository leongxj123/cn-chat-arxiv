<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#30740;&#31350;&#23545;&#21355;&#26143;&#22270;&#20687;&#20013;&#35782;&#21035;&#39134;&#26426;&#30340;&#20219;&#21153;&#33258;&#23450;&#20041;&#30340;&#19968;&#22871;&#20808;&#36827;&#23545;&#35937;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#21457;&#29616;YOLOv5&#26159;&#22312;&#19981;&#21516;&#25104;&#20687;&#26465;&#20214;&#19979;&#23637;&#29616;&#39640;&#31934;&#24230;&#21644;&#36866;&#24212;&#24615;&#30340;&#26368;&#20248;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2404.02877</link><description>&lt;p&gt;
FlightScope: &#21355;&#26143;&#22270;&#20687;&#20013;&#39134;&#34892;&#22120;&#26816;&#27979;&#31639;&#27861;&#30340;&#28145;&#24230;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FlightScope: A Deep Comprehensive Assessment of Aircraft Detection Algorithms in Satellite Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#21355;&#26143;&#22270;&#20687;&#20013;&#35782;&#21035;&#39134;&#26426;&#30340;&#20219;&#21153;&#33258;&#23450;&#20041;&#30340;&#19968;&#22871;&#20808;&#36827;&#23545;&#35937;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#21457;&#29616;YOLOv5&#26159;&#22312;&#19981;&#21516;&#25104;&#20687;&#26465;&#20214;&#19979;&#23637;&#29616;&#39640;&#31934;&#24230;&#21644;&#36866;&#24212;&#24615;&#30340;&#26368;&#20248;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02877v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22312;&#36965;&#24863;&#21355;&#26143;&#22270;&#20687;&#20013;&#36827;&#34892;&#23545;&#35937;&#26816;&#27979;&#23545;&#20110;&#35768;&#22810;&#39046;&#22495;&#65292;&#22914;&#29983;&#29289;&#29289;&#29702;&#23398;&#21644;&#29615;&#22659;&#30417;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#19981;&#26029;&#21457;&#23637;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#22312;&#24120;&#35265;&#30340;&#22522;&#20110;&#22320;&#38754;&#25293;&#25668;&#30340;&#29031;&#29255;&#19978;&#23454;&#26045;&#21644;&#27979;&#35797;&#12290;&#26412;&#25991;&#23545;&#19968;&#22871;&#38024;&#23545;&#22312;&#21355;&#26143;&#22270;&#20687;&#20013;&#35782;&#21035;&#39134;&#26426;&#36825;&#19968;&#20219;&#21153;&#23450;&#21046;&#30340;&#20808;&#36827;&#23545;&#35937;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#21033;&#29992;&#22823;&#22411;HRPlanesV2&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19982;GDIT&#25968;&#25454;&#38598;&#30340;&#20005;&#26684;&#39564;&#35777;&#65292;&#35813;&#30740;&#31350;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#21253;&#25324;YOLO&#29256;&#26412;5&#21644;8&#12289;Faster RCNN&#12289;CenterNet&#12289;RetinaNet&#12289;RTMDet&#21644;DETR&#65292;&#22343;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#12290;&#36825;&#39033;&#20840;&#38754;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;YOLOv5&#20316;&#20026;&#35782;&#21035;&#36965;&#24863;&#25968;&#25454;&#20013;&#30340;&#39134;&#26426;&#36825;&#19968;&#29305;&#23450;&#26696;&#20363;&#30340;&#21331;&#36234;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#25104;&#20687;&#26465;&#20214;&#19979;&#30340;&#39640;&#31934;&#24230;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02877v1 Announce Type: cross  Abstract: Object detection in remotely sensed satellite pictures is fundamental in many fields such as biophysical, and environmental monitoring. While deep learning algorithms are constantly evolving, they have been mostly implemented and tested on popular ground-based taken photos. This paper critically evaluates and compares a suite of advanced object detection algorithms customized for the task of identifying aircraft within satellite imagery. Using the large HRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset, this research encompasses an array of methodologies including YOLO versions 5 and 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from scratch. This exhaustive training and validation study reveal YOLOv5 as the preeminent model for the specific case of identifying airplanes from remote sensing data, showcasing high precision and adaptability across diverse imaging conditions. This research
&lt;/p&gt;</description></item><item><title>SportsNGEN&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#32463;&#36807;&#35757;&#32451;&#33021;&#25345;&#32493;&#29983;&#25104;&#36924;&#30495;&#30340;&#22810;&#20154;&#20307;&#32946;&#28216;&#25103;&#65292;&#21253;&#25324;&#27169;&#25311;&#25972;&#20010;&#32593;&#29699;&#27604;&#36187;&#21644;&#20026;&#25945;&#32451;&#21644;&#24191;&#25773;&#21592;&#25552;&#20379;&#27934;&#23519;&#21147;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12977</link><description>&lt;p&gt;
SportsNGEN: &#25345;&#32493;&#29983;&#25104;&#22810;&#20154;&#20307;&#32946;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
SportsNGEN: Sustained Generation of Multi-player Sports Gameplay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12977
&lt;/p&gt;
&lt;p&gt;
SportsNGEN&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#32463;&#36807;&#35757;&#32451;&#33021;&#25345;&#32493;&#29983;&#25104;&#36924;&#30495;&#30340;&#22810;&#20154;&#20307;&#32946;&#28216;&#25103;&#65292;&#21253;&#25324;&#27169;&#25311;&#25972;&#20010;&#32593;&#29699;&#27604;&#36187;&#21644;&#20026;&#25945;&#32451;&#21644;&#24191;&#25773;&#21592;&#25552;&#20379;&#27934;&#23519;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;SportsNGEN&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#20351;&#29992;&#36816;&#21160;&#21592;&#21644;&#29699;&#36861;&#36394;&#24207;&#21015;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#19988;&#25345;&#32493;&#30340;&#28216;&#25103;&#22330;&#26223;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#19987;&#19994;&#32593;&#29699;&#36861;&#36394;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;SportsNGEN&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#27169;&#25311;&#19982;&#23556;&#20987;&#20998;&#31867;&#22120;&#21644;&#36923;&#36753;&#30456;&#32467;&#21512;&#26469;&#24320;&#22987;&#21644;&#32467;&#26463;&#29699;&#36187;&#65292;&#31995;&#32479;&#33021;&#22815;&#27169;&#25311;&#25972;&#20010;&#32593;&#29699;&#27604;&#36187;&#12290;&#27492;&#22806;&#65292;SportsNGEN&#30340;&#36890;&#29992;&#29256;&#26412;&#21487;&#20197;&#36890;&#36807;&#22312;&#21253;&#21547;&#35813;&#29699;&#21592;&#30340;&#27604;&#36187;&#25968;&#25454;&#19978;&#24494;&#35843;&#26469;&#23450;&#21046;&#29305;&#23450;&#29699;&#21592;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#65292;&#21487;&#20197;&#36890;&#36807;&#35780;&#20272;&#21453;&#20107;&#23454;&#25110;&#20551;&#35774;&#36873;&#39033;&#20026;&#25945;&#32451;&#21644;&#24191;&#25773;&#21592;&#25552;&#20379;&#27934;&#23519;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36136;&#37327;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#36275;&#29699;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12977v1 Announce Type: cross  Abstract: We present a transformer decoder based model, SportsNGEN, that is trained on sports player and ball tracking sequences that is capable of generating realistic and sustained gameplay. We train and evaluate SportsNGEN on a large database of professional tennis tracking data and demonstrate that by combining the generated simulations with a shot classifier and logic to start and end rallies, the system is capable of simulating an entire tennis match. In addition, a generic version of SportsNGEN can be customized to a specific player by fine-tuning on match data that includes that player. We show that our model is well calibrated and can be used to derive insights for coaches and broadcasters by evaluating counterfactual or what if options. Finally, we show qualitative results indicating the same approach works for football.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;Nested Neural Feature Fields (N2F2) &#23454;&#29616;&#20102;&#23618;&#27425;&#21270;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#23545;&#29289;&#29702;&#32500;&#24230;&#25110;&#35821;&#20041;&#32500;&#24230;&#31561;&#19981;&#21516;&#31890;&#24230;&#30340;&#22330;&#26223;&#23646;&#24615;&#20840;&#38754;&#21644;&#32454;&#33268;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.10997</link><description>&lt;p&gt;
&#23884;&#22871;&#31070;&#32463;&#29305;&#24449;&#22330;&#30340;&#23618;&#27425;&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10997
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;Nested Neural Feature Fields (N2F2) &#23454;&#29616;&#20102;&#23618;&#27425;&#21270;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#23545;&#29289;&#29702;&#32500;&#24230;&#25110;&#35821;&#20041;&#32500;&#24230;&#31561;&#19981;&#21516;&#31890;&#24230;&#30340;&#22330;&#26223;&#23646;&#24615;&#20840;&#38754;&#21644;&#32454;&#33268;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#29702;&#35299;&#22810;&#23618;&#25277;&#35937;&#30340;&#22797;&#26434;&#22330;&#26223;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23884;&#22871;&#31070;&#32463;&#29305;&#24449;&#22330; (N2F2)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#23618;&#30417;&#30563;&#26469;&#23398;&#20064;&#21333;&#20010;&#29305;&#24449;&#22330;&#65292;&#22312;&#21516;&#19968;&#39640;&#32500;&#29305;&#24449;&#20013;&#30340;&#19981;&#21516;&#32500;&#24230;&#32534;&#30721;&#19981;&#21516;&#31890;&#24230;&#30340;&#22330;&#26223;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#28789;&#27963;&#23450;&#20041;&#23618;&#27425;&#65292;&#21487;&#20197;&#26681;&#25454;&#29289;&#29702;&#32500;&#24230;&#12289;&#35821;&#20041;&#32500;&#24230;&#25110;&#20004;&#32773;&#22343;&#21305;&#37197;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22330;&#26223;&#30340;&#20840;&#38754;&#21644;&#32454;&#33268;&#29702;&#35299;&#12290;&#25105;&#20204;&#21033;&#29992;2D&#31867;&#21035;&#26080;&#20851;&#20998;&#21106;&#27169;&#22411;&#22312;&#22270;&#20687;&#31354;&#38388;&#30340;&#20219;&#24847;&#23610;&#24230;&#25552;&#20379;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#20687;&#32032;&#20998;&#32452;&#65292;&#24182;&#26597;&#35810;CLIP&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#20026;&#36825;&#20123;&#27573;&#33853;&#20013;&#30340;&#27599;&#20010;&#37096;&#20998;&#33719;&#24471;&#19982;&#35821;&#35328;&#23545;&#40784;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20998;&#23618;&#30417;&#30563;&#26041;&#27861;&#23558;&#19981;&#21516;&#30340;&#23884;&#22871;&#29305;&#24449;&#22330;&#32500;&#24230;&#20998;&#37197;&#32473;&#25552;&#21462;C
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10997v1 Announce Type: cross  Abstract: Understanding complex scenes at multiple levels of abstraction remains a formidable challenge in computer vision. To address this, we introduce Nested Neural Feature Fields (N2F2), a novel approach that employs hierarchical supervision to learn a single feature field, wherein different dimensions within the same high-dimensional feature encode scene properties at varying granularities. Our method allows for a flexible definition of hierarchies, tailored to either the physical dimensions or semantics or both, thereby enabling a comprehensive and nuanced understanding of scenes. We leverage a 2D class-agnostic segmentation model to provide semantically meaningful pixel groupings at arbitrary scales in the image space, and query the CLIP vision-encoder to obtain language-aligned embeddings for each of these segments. Our proposed hierarchical supervision method then assigns different nested dimensions of the feature field to distill the C
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#32771;&#34385;&#29983;&#25104;&#22270;&#20687;&#26159;&#30001;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#26465;&#20214;&#19979;&#65292;&#37327;&#21270;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11789</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#20551;&#35774;&#30340;&#32479;&#35745;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Statistical Test for Generated Hypotheses by Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#32771;&#34385;&#29983;&#25104;&#22270;&#20687;&#26159;&#30001;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#26465;&#20214;&#19979;&#65292;&#37327;&#21270;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#30340;&#22686;&#24378;&#24615;&#33021;&#21152;&#36895;&#20102;&#20854;&#34701;&#20837;&#31185;&#23398;&#30740;&#31350;&#12290;&#29305;&#21035;&#26159;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#21019;&#24314;&#31185;&#23398;&#20551;&#35774;&#26159;&#24456;&#26377;&#21069;&#36884;&#30340;&#65292;&#24182;&#19988;&#27491;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;AI&#29983;&#25104;&#30340;&#20551;&#35774;&#36827;&#34892;&#20851;&#38190;&#20915;&#31574;&#65288;&#22914;&#21307;&#23398;&#35786;&#26029;&#65289;&#26102;&#65292;&#39564;&#35777;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#36827;&#34892;&#21307;&#23398;&#35786;&#26029;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#26469;&#37327;&#21270;&#20854;&#21487;&#38752;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#32479;&#35745;&#26816;&#39564;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#20351;&#29992;&#36873;&#25321;&#24615;&#25512;&#26029;&#26694;&#26550;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#26159;&#30001;&#32463;&#36807;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#36825;&#19968;&#20107;&#23454;&#26465;&#20214;&#19979;&#30340;&#32479;&#35745;&#26816;&#39564;&#12290;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#32467;&#26524;&#30340;&#32479;&#35745;&#21487;&#38752;&#24615;&#21487;&#20197;&#20197;p&#20540;&#30340;&#24418;&#24335;&#37327;&#21270;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#25511;&#21046;&#38169;&#35823;&#29575;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11789v1 Announce Type: cross  Abstract: The enhanced performance of AI has accelerated its integration into scientific research. In particular, the use of generative AI to create scientific hypotheses is promising and is increasingly being applied across various fields. However, when employing AI-generated hypotheses for critical decisions, such as medical diagnoses, verifying their reliability is crucial. In this study, we consider a medical diagnostic task using generated images by diffusion models, and propose a statistical test to quantify its reliability. The basic idea behind the proposed statistical test is to employ a selective inference framework, where we consider a statistical test conditional on the fact that the generated images are produced by a trained diffusion model. Using the proposed method, the statistical reliability of medical image diagnostic results can be quantified in the form of a p-value, allowing for decision-making with a controlled error rate. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20860;&#39038;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.08977</link><description>&lt;p&gt;
&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#25345;&#32493;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Weighted Ensemble Models Are Strong Continual Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08977
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20860;&#39038;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#20174;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#24471;&#20197;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#22312;&#23398;&#20064;&#24403;&#21069;&#20219;&#21153;&#25968;&#25454;&#26102;&#19981;&#21487;&#29992;&#12290;CL&#26412;&#36136;&#19978;&#26159;&#22312;&#33021;&#22815;&#23398;&#20064;&#26032;&#20219;&#21153;&#65288;&#21363;&#21487;&#22609;&#24615;&#65289;&#21644;&#20445;&#25345;&#20808;&#21069;&#23398;&#20064;&#27010;&#24565;&#30340;&#24615;&#33021;&#65288;&#21363;&#31283;&#23450;&#24615;&#65289;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23545;&#20808;&#21069;&#21644;&#24403;&#21069;&#20219;&#21153;&#30340;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21152;&#26435;&#38598;&#25104;&#12290;&#36825;&#31181;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25345;&#32493;&#27169;&#22411;&#24179;&#22343;&#65288;&#25110;CoMA&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#22609;&#24615;&#22312;&#24403;&#21069;&#20219;&#21153;&#19978;&#33719;&#24471;&#39640;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#20559;&#31163;&#22826;&#36828;&#30340;&#20808;&#21069;&#26435;&#37325;&#37197;&#32622;&#65292;&#20174;&#32780;&#30830;&#20445;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;CoMA&#30340;&#25913;&#36827;&#22411;&#21464;&#20307;&#65292;&#21517;&#20026;&#25345;&#32493;&#36153;&#33293;&#23572;&#21152;&#26435;&#27169;&#22411;&#24179;&#22343;&#65288;&#25110;CoFiMA&#65289;&#65292;&#35813;&#27169;&#22411;&#23545;&#27599;&#19968;&#20010;&#21442;&#25968;&#36827;&#34892;&#36873;&#25321;&#24615;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08977v2 Announce Type: replace-cross  Abstract: In this work, we study the problem of continual learning (CL) where the goal is to learn a model on a sequence of tasks, such that the data from the previous tasks becomes unavailable while learning on the current task data. CL is essentially a balancing act between being able to learn on the new task (i.e., plasticity) and maintaining the performance on the previously learned concepts (i.e., stability). Intending to address the stability-plasticity trade-off, we propose to perform weight-ensembling of the model parameters of the previous and current tasks. This weighted-ensembled model, which we call Continual Model Averaging (or CoMA), attains high accuracy on the current task by leveraging plasticity, while not deviating too far from the previous weight configuration, ensuring stability. We also propose an improved variant of CoMA, named Continual Fisher-weighted Model Averaging (or CoFiMA), that selectively weighs each para
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;&#20102;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;CATM&#21644;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;GSL&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09786</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Adaptive Self-training Framework for Fine-grained Scene Graph Generation. (arXiv:2401.09786v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;&#20102;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;CATM&#21644;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;GSL&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;SGG&#65289;&#27169;&#22411;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#38271;&#23614;&#35859;&#35789;&#20998;&#24067;&#21644;&#32570;&#22833;&#27880;&#37322;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;SGG&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#35757;&#32451;SGG&#65288;ST-SGG&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#20026;&#20854;&#20998;&#37197;&#20266;&#26631;&#31614;&#20197;&#35757;&#32451;SGG&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#22270;&#20687;&#35782;&#21035;&#26041;&#38754;&#30340;&#33258;&#35757;&#32451;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#35774;&#35745;&#36866;&#29992;&#20110;SGG&#20219;&#21153;&#30340;&#33258;&#35757;&#32451;&#26694;&#26550;&#26356;&#20855;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#22266;&#26377;&#29305;&#24615;&#65292;&#22914;&#35821;&#20041;&#27495;&#20041;&#21644;&#38271;&#23614;&#20998;&#24067;&#30340;&#35859;&#35789;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SGG&#20266;&#26631;&#31614;&#25216;&#26415;&#65292;&#31216;&#20026;&#20855;&#26377;&#21160;&#37327;&#30340;&#31867;&#21035;&#33258;&#36866;&#24212;&#38408;&#20540;&#21270;&#65288;CATM&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#29420;&#31435;&#20110;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#24050;&#26377;&#30340;SGG&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;&#65288;GSL&#65289;&#65292;&#20174;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scene graph generation (SGG) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. In this work, we aim to alleviate the long-tailed problem of SGG by utilizing unannotated triplets. To this end, we introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels for unannotated triplets based on which the SGG models are trained. While there has been significant progress in self-training for image recognition, designing a self-training framework for the SGG task is more challenging due to its inherent nature such as the semantic ambiguity and the long-tailed distribution of predicate classes. Hence, we propose a novel pseudo-labeling technique for SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is a model-agnostic framework that can be applied to any existing SGG models. Furthermore, we devise a graph structure learner (GSL) that is benefici
&lt;/p&gt;</description></item><item><title>CoSSegGaussians&#26159;&#19968;&#31181;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#31354;&#38388;&#21644;&#35821;&#20041;&#29305;&#24449;&#23454;&#29616;&#32039;&#20945;&#21644;&#21487;&#38752;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2401.05925</link><description>&lt;p&gt;
CoSSegGaussians&#65306;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians. (arXiv:2401.05925v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05925
&lt;/p&gt;
&lt;p&gt;
CoSSegGaussians&#26159;&#19968;&#31181;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#31354;&#38388;&#21644;&#35821;&#20041;&#29305;&#24449;&#23454;&#29616;&#32039;&#20945;&#21644;&#21487;&#38752;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;&#65288;CoSSegGaussians&#65289;&#65292;&#35813;&#26041;&#27861;&#20165;&#20351;&#29992;RGB&#22270;&#20687;&#36755;&#20837;&#65292;&#20197;&#24555;&#36895;&#30340;&#28210;&#26579;&#36895;&#24230;&#23454;&#29616;&#32039;&#20945;&#30340;3D&#19968;&#33268;&#24615;&#22330;&#26223;&#20998;&#21106;&#12290;&#20808;&#21069;&#22522;&#20110;NeRF&#30340;3D&#20998;&#21106;&#26041;&#27861;&#20381;&#36182;&#20110;&#38544;&#24335;&#25110;&#20307;&#32032;&#31070;&#32463;&#22330;&#34920;&#31034;&#21644;&#20809;&#32447;&#34892;&#36827;&#20307;&#31215;&#28210;&#26579;&#65292;&#36825;&#20123;&#26041;&#27861;&#32791;&#26102;&#36739;&#38271;&#12290;&#26368;&#36817;&#30340;3D&#39640;&#26031;&#22330;&#25237;&#24433;&#26174;&#33879;&#25552;&#39640;&#20102;&#28210;&#26579;&#36895;&#24230;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#20998;&#21106;&#26041;&#27861;&#65288;&#20363;&#22914;&#39640;&#26031;&#20998;&#32452;&#65289;&#22312;&#38646;&#26679;&#26412;&#20998;&#21106;&#20013;&#27809;&#26377;&#25552;&#20379;&#32039;&#20945;&#30340;&#20998;&#21106;&#25513;&#27169;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#22312;&#36935;&#21040;&#19981;&#19968;&#33268;&#30340;2D&#26426;&#22120;&#29983;&#25104;&#26631;&#31614;&#26102;&#65292;&#26080;&#27861;&#30452;&#25509;&#20026;&#27599;&#20010;&#39640;&#26031;&#20998;&#37197;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#32039;&#20945;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#27973;&#23618;&#35299;&#30721;&#32593;&#32476;&#23558;&#27599;&#20010;&#39640;&#26031;&#28857;&#30340;&#34701;&#21512;&#31354;&#38388;&#21644;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#26144;&#23556;&#65292;&#36805;&#36895;&#23454;&#29616;&#32039;&#20945;&#19988;&#21487;&#38752;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based 3D segmentation methods have relied on implicit or voxel neural scene representation and ray-marching volume rendering which are time consuming. Recent 3D Gaussian Splatting significantly improves the rendering speed, however, existing Gaussians-based segmentation methods(eg: Gaussian Grouping) fail to provide compact segmentation masks especially in zero-shot segmentation, which is mainly caused by the lack of robustness and compactness for straightforwardly assigning learnable parameters to each Gaussian when encountering inconsistent 2D machine-generated labels. Our method aims to achieve compact and reliable zero-shot scene segmentation swiftly by mapping fused spatial and semantically meaningful features for each Gaussian point with a shallow decoding network. Specifically, our method fi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#29420;&#31435;&#30340;&#19987;&#38376;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2310.03986</link><description>&lt;p&gt;
&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#65292;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation. (arXiv:2310.03986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03986
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#29420;&#31435;&#30340;&#19987;&#38376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#26469;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#19968;&#20123;&#30456;&#20851;&#30340;&#27169;&#24577;&#20013;&#35266;&#23519;&#21040;&#65292;&#22914;&#26524;&#22312;&#27979;&#35797;&#26102;&#38388;&#32570;&#23569;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#31616;&#21333;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#31243;&#24207;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#26469;&#34917;&#20607;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#31181;&#36866;&#24212;&#21487;&#20197;&#37096;&#20998;&#24357;&#34917;&#30001;&#20110;&#32570;&#22833;&#27169;&#24577;&#32780;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#38024;&#23545;&#21487;&#29992;&#27169;&#24577;&#32452;&#21512;&#36827;&#34892;&#35757;&#32451;&#30340;&#29420;&#31435;&#30340;&#12289;&#19987;&#38376;&#30340;&#32593;&#32476;&#12290;&#25152;&#25552;&#20986;&#30340;&#36866;&#24212;&#25152;&#38656;&#30340;&#21442;&#25968;&#38750;&#24120;&#23569;&#65288;&#20363;&#22914;&#65292;&#23569;&#20110;&#65289;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning seeks to utilize data from multiple sources to improve the overall performance of downstream tasks. It is desirable for redundancies in the data to make multimodal systems robust to missing or corrupted observations in some correlated modalities. However, we observe that the performance of several existing multimodal networks significantly deteriorates if one or multiple modalities are absent at test time. To enable robustness to missing modalities, we propose simple and parameter-efficient adaptation procedures for pretrained multimodal networks. In particular, we exploit low-rank adaptation and modulation of intermediate features to compensate for the missing modalities. We demonstrate that such adaptation can partially bridge performance drop due to missing modalities and outperform independent, dedicated networks trained for the available modality combinations in some cases. The proposed adaptation requires extremely small number of parameters (e.g., fewer than 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#25105;&#20204;&#36890;&#36807;&#24050;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23884;&#20837;&#24335;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#24182;&#20026;MVMR&#20219;&#21153;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;</title><link>http://arxiv.org/abs/2309.16701</link><description>&lt;p&gt;
MVMR: &#22312;&#22810;&#20010;&#21487;&#38752;&#35270;&#39057;&#38598;&#20013;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
MVMR: Evaluating Natural Language Video Localization Bias over Multiple Reliable Videos Pool. (arXiv:2309.16701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#25105;&#20204;&#36890;&#36807;&#24050;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23884;&#20837;&#24335;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#24182;&#20026;MVMR&#20219;&#21153;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#28608;&#22686;&#65292;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#23427;&#33268;&#21147;&#20110;&#26816;&#27979;&#19982;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#21305;&#37197;&#30340;&#35270;&#39057;&#29255;&#27573;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#37117;&#27809;&#26377;&#25506;&#32034;&#22312;&#23384;&#22312;&#22810;&#20010;&#27491;&#36127;&#35270;&#39057;&#30340;&#22823;&#37327;&#35821;&#26009;&#24211;&#20013;&#23450;&#20301;&#19968;&#20010;&#26102;&#21051;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#65288;Massive Videos Moment Retrieval&#65289;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#29616;&#26377;&#35270;&#39057;&#23450;&#20301;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#23884;&#20837;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30446;&#26631;&#26597;&#35810;&#19982;&#35270;&#39057;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#20174;&#32780;&#23450;&#20041;&#27491;&#36127;&#38598;&#12290;&#38024;&#23545;&#25552;&#20986;&#30340;MVMR&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;
&lt;/p&gt;
&lt;p&gt;
With the explosion of multimedia content in recent years, natural language video localization, which focuses on detecting video moment that matches a given natural language query, has become a critical problem. However, none of the previous research explores localizing a moment from a large corpus where multiple positive and negative videos exist. In this paper, we propose an MVMR (Massive Videos Moment Retrieval) task, which aims to localize video frames from a massive set of videos given a text query. For this task, we suggest methods for constructing datasets by employing similarity filtering on the existing video localization datasets and introduce three MVMR datasets. Specifically, we employ embedding-based text similarity matching and video-language grounding techniques to calculate the relevance score between a target query and videos to define positive and negative sets. For the proposed MVMR task, we further develop a strong model, Reliable Mutual Matching Network (RMMN), whic
&lt;/p&gt;</description></item><item><title>GNFactor&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20195;&#29702;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#21644;Perceiver Transformer&#27169;&#22359;&#65292;&#20197;&#21450;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#34920;&#31034;&#26469;&#23454;&#29616;&#23545;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#25805;&#20316;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#23427;&#36890;&#36807;&#23558;&#35270;&#35273;&#21644;&#35821;&#20041;&#20449;&#24687;&#32435;&#20837;&#19977;&#32500;&#34920;&#31034;&#26469;&#25552;&#39640;&#22330;&#26223;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.16891</link><description>&lt;p&gt;
GNFactor&#65306;&#20855;&#26377;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#30340;&#22810;&#20219;&#21153;&#30495;&#23454;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields. (arXiv:2308.16891v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16891
&lt;/p&gt;
&lt;p&gt;
GNFactor&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20195;&#29702;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#21644;Perceiver Transformer&#27169;&#22359;&#65292;&#20197;&#21450;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#34920;&#31034;&#26469;&#23454;&#29616;&#23545;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#25805;&#20316;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#23427;&#36890;&#36807;&#23558;&#35270;&#35273;&#21644;&#35821;&#20041;&#20449;&#24687;&#32435;&#20837;&#19977;&#32500;&#34920;&#31034;&#26469;&#25552;&#39640;&#22330;&#26223;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#32467;&#26500;&#30340;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#65292;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#24320;&#21457;&#33021;&#22815;&#25191;&#34892;&#22810;&#26679;&#21270;&#25805;&#20316;&#20219;&#21153;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#19968;&#30452;&#26159;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#19968;&#20010;&#38271;&#26399;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#20840;&#38754;&#29702;&#35299;&#22330;&#26223;&#30340;&#19977;&#32500;&#32467;&#26500;&#21644;&#35821;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GNFactor&#65292;&#19968;&#31181;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#21487;&#35270;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#65292;&#23427;&#21033;&#29992;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#65288;GNF&#65289;&#20316;&#20026;&#37325;&#24314;&#27169;&#22359;&#65292;Perceiver Transformer&#20316;&#20026;&#20915;&#31574;&#27169;&#22359;&#65292;&#20849;&#20139;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#34920;&#31034;&#12290;&#20026;&#20102;&#23558;&#35821;&#20041;&#32435;&#20837;&#19977;&#32500;&#34920;&#31034;&#65292;&#37325;&#24314;&#27169;&#22359;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#31283;&#23450;&#25193;&#25955;&#65289;&#23558;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#25552;&#21462;&#21040;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#20013;&#12290;&#25105;&#20204;&#22312;3&#20010;&#30495;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;GNFactor&#65292;&#24182;&#23545;10&#20010;RLBench&#20219;&#21153;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#21482;&#20351;&#29992;&#20102;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is a long-standing problem in robotics to develop agents capable of executing diverse manipulation tasks from visual observations in unstructured real-world environments. To achieve this goal, the robot needs to have a comprehensive understanding of the 3D structure and semantics of the scene. In this work, we present $\textbf{GNFactor}$, a visual behavior cloning agent for multi-task robotic manipulation with $\textbf{G}$eneralizable $\textbf{N}$eural feature $\textbf{F}$ields. GNFactor jointly optimizes a generalizable neural field (GNF) as a reconstruction module and a Perceiver Transformer as a decision-making module, leveraging a shared deep 3D voxel representation. To incorporate semantics in 3D, the reconstruction module utilizes a vision-language foundation model ($\textit{e.g.}$, Stable Diffusion) to distill rich semantic information into the deep 3D voxel. We evaluate GNFactor on 3 real robot tasks and perform detailed ablations on 10 RLBench tasks with a limited number of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#35282;&#28145;&#24230;&#32593;&#32476;&#26041;&#27861;&#35770;&#65292;&#24212;&#29992;&#20110;&#23454;&#39564;&#29289;&#29702;&#20013;&#30340;&#22810;&#31181;&#25104;&#20687;&#34920;&#36798;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#35770;&#35299;&#20915;&#20102;&#22810;&#35270;&#35282;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08206</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#35282;&#28145;&#24230;&#32593;&#32476;&#26041;&#27861;&#35770;&#22312;&#23454;&#39564;&#29289;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Explainable Multi-View Deep Networks Methodology for Experimental Physics. (arXiv:2308.08206v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08206
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#35282;&#28145;&#24230;&#32593;&#32476;&#26041;&#27861;&#35770;&#65292;&#24212;&#29992;&#20110;&#23454;&#39564;&#29289;&#29702;&#20013;&#30340;&#22810;&#31181;&#25104;&#20687;&#34920;&#36798;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#35770;&#35299;&#20915;&#20102;&#22810;&#35270;&#35282;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#23454;&#39564;&#24120;&#28041;&#21450;&#22810;&#31181;&#25104;&#20687;&#34920;&#36798;&#65292;&#22914;X&#23556;&#32447;&#25195;&#25551;&#21644;&#26174;&#24494;&#22270;&#20687;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#36825;&#20123;&#23454;&#39564;&#30340;&#30417;&#30563;&#20998;&#26512;&#20013;&#12290;&#21512;&#24182;&#19981;&#21516;&#30340;&#22270;&#20687;&#34920;&#36798;&#32463;&#24120;&#38656;&#35201;&#27491;&#30830;&#20998;&#26512;&#21644;&#20570;&#20986;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#22810;&#35270;&#35282;&#25968;&#25454;&#24212;&#36816;&#32780;&#29983; - &#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#26679;&#26412;&#30001;&#26469;&#33258;&#19981;&#21516;&#35282;&#24230;&#12289;&#26469;&#28304;&#25110;&#27169;&#24577;&#30340;&#35270;&#22270;&#25551;&#36848;&#12290;&#22810;&#35270;&#35282;&#23398;&#20064;&#30340;&#27010;&#24565;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#23545;&#20110;&#21487;&#38752;&#21644;&#21487;&#20449;&#30340;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22810;&#35270;&#35282;&#27169;&#22411;&#32570;&#20047;&#36866;&#24403;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#30001;&#20110;&#20854;&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#65292;&#38590;&#20197;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35270;&#35273;&#39046;&#22495;&#30340;&#19981;&#21516;&#22810;&#35270;&#35282;&#26550;&#26500;&#65292;&#27599;&#20010;&#26550;&#26500;&#37117;&#36866;&#21512;&#35299;&#20915;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#37322;&#22810;&#35270;&#35282;&#27169;&#22411;&#30340;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physical experiments often involve multiple imaging representations, such as X-ray scans and microscopic images. Deep learning models have been widely used for supervised analysis in these experiments. Combining different image representations is frequently required to analyze and make a decision properly. Consequently, multi-view data has emerged - datasets where each sample is described by views from different angles, sources, or modalities. These problems are addressed with the concept of multi-view learning. Understanding the decision-making process of deep learning models is essential for reliable and credible analysis. Hence, many explainability methods have been devised recently. Nonetheless, there is a lack of proper explainability in multi-view models, which are challenging to explain due to their architectures. In this paper, we suggest different multi-view architectures for the vision domain, each suited to another problem, and we also present a methodology for explaining th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#24494;&#35843;&#23545;&#23569;&#26679;&#26412;&#19979;&#28216;&#20219;&#21153;&#30340;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36866;&#24403;&#36873;&#25321;&#22806;&#20998;&#24067;&#20998;&#25968;&#23545;&#20110;CLIP-based &#24494;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#22823;&#27010;&#24565;&#21305;&#37197;&#65288;MCM&#65289;&#20998;&#25968;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.06048</link><description>&lt;p&gt;
&#24494;&#35843;&#23545;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#24433;&#21709;&#26159;&#24590;&#26679;&#30340;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Does Fine-Tuning Impact Out-of-Distribution Detection for Vision-Language Models?. (arXiv:2306.06048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#24494;&#35843;&#23545;&#23569;&#26679;&#26412;&#19979;&#28216;&#20219;&#21153;&#30340;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36866;&#24403;&#36873;&#25321;&#22806;&#20998;&#24067;&#20998;&#25968;&#23545;&#20110;CLIP-based &#24494;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#22823;&#27010;&#24565;&#21305;&#37197;&#65288;MCM&#65289;&#20998;&#25968;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;CLIP&#65292;&#22312;&#22806;&#20998;&#24067;&#26816;&#27979;&#21644;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#20869;&#20998;&#24067;&#20934;&#30830;&#24615;&#24448;&#24448;&#22312;&#19979;&#28216;&#25968;&#25454;&#38598;&#20013;&#21463;&#21040;&#38480;&#21046;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;CLIP&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#22914;&#25552;&#31034;&#23398;&#20064;&#65292;&#24050;&#32463;&#22312;&#23384;&#22312;&#22806;&#20998;&#24067;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25913;&#36827;&#20102;&#20869;&#20998;&#24067;&#20998;&#31867;&#21644;&#22806;&#20998;&#24067;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#23545;&#20110;&#27809;&#26377;&#22806;&#20998;&#24067;&#26631;&#31614;&#30340;&#35821;&#20041;&#36716;&#31227;&#26159;&#21542;&#21487;&#38752;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#26088;&#22312;&#23545;&#24494;&#35843;&#23545;&#20110;&#23569;&#26679;&#26412;&#19979;&#28216;&#20219;&#21153;&#30340;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#24433;&#21709;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;&#36890;&#36807;&#23558;&#22806;&#20998;&#24067;&#26816;&#27979;&#26694;&#26550;&#21270;&#20026;&#22810;&#27169;&#24335;&#27010;&#24565;&#21305;&#37197;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24494;&#35843;&#26041;&#27861;&#21644;&#21508;&#31181;&#22806;&#20998;&#24067;&#20998;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#22806;&#20998;&#24067;&#20998;&#25968;&#23545;&#20110;&#22522;&#20110;CLIP&#30340;&#24494;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#65292;&#26368;&#22823;&#27010;&#24565;&#21305;&#37197;&#65288;MCM&#65289;&#20998;&#25968;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large vision-language models such as CLIP have shown remarkable out-of-distribution (OOD) detection and generalization performance. However, their zero-shot in-distribution (ID) accuracy is often limited for downstream datasets. Recent CLIP-based fine-tuning methods such as prompt learning have demonstrated significant improvements in ID classification and OOD generalization where OOD labels are available. Nonetheless, it remains unclear whether the model is reliable to semantic shifts without OOD labels. In this paper, we aim to bridge the gap and present a comprehensive study to understand how fine-tuning impact OOD detection for few-shot downstream tasks. By framing OOD detection as multi-modal concept matching, we establish a connection between fine-tuning methods and various OOD scores. Our results suggest that a proper choice of OOD scores is essential for CLIP-based fine-tuning. In particular, the maximum concept matching (MCM) score provides a promising solution consiste
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#36965;&#25805;&#20316;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#20197;&#21450;&#23398;&#20064;&#25163;&#29289;&#20114;&#21160;&#20808;&#39564;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#32852;&#32467;&#30446;&#26631;&#21644;&#25163;&#37096;&#23039;&#24577;&#20272;&#35745;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#20851;&#38190;&#28857;&#23450;&#20301;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01618</link><description>&lt;p&gt;
ContactArt&#65306;&#23398;&#20064;&#31867;&#21035;&#32423;&#32852;&#32467;&#29289;&#20307;&#21644;&#25163;&#37096;&#23039;&#24577;&#20272;&#35745;&#30340;&#19977;&#32500;&#20132;&#20114;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation. (arXiv:2305.01618v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#36965;&#25805;&#20316;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#20197;&#21450;&#23398;&#20064;&#25163;&#29289;&#20114;&#21160;&#20808;&#39564;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#32852;&#32467;&#30446;&#26631;&#21644;&#25163;&#37096;&#23039;&#24577;&#20272;&#35745;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#20851;&#38190;&#28857;&#23450;&#20301;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#25163;&#37096;&#21644;&#32852;&#32467;&#30446;&#26631;&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#25163;&#29289;&#20114;&#21160;&#20808;&#39564;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35270;&#35273;&#36965;&#25805;&#20316;&#25910;&#38598;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#20154;&#31867;&#25805;&#20316;&#21592;&#21487;&#20197;&#30452;&#25509;&#22312;&#29289;&#29702;&#27169;&#25311;&#22120;&#20013;&#28216;&#25103;&#26469;&#25805;&#32437;&#32852;&#32467;&#23545;&#35937;&#12290; &#25105;&#20204;&#35760;&#24405;&#25968;&#25454;&#24182;&#20174;&#27169;&#25311;&#22120;&#33719;&#24471;&#26377;&#20851;&#30446;&#26631;&#23039;&#24577;&#21644;&#25509;&#35302;&#20449;&#24687;&#30340;&#20813;&#36153;&#21644;&#20934;&#30830;&#27880;&#37322;&#12290; &#25105;&#20204;&#30340;&#31995;&#32479;&#20165;&#38656;&#35201;&#20351;&#29992;iPhone&#26469;&#35760;&#24405;&#20154;&#25163;&#36816;&#21160;&#65292;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#24182;&#22823;&#22823;&#38477;&#20302;&#25968;&#25454;&#21644;&#27880;&#37322;&#25910;&#38598;&#30340;&#25104;&#26412;&#12290;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19977;&#32500;&#20132;&#20114;&#20808;&#39564;&#65292;&#21253;&#25324;&#25429;&#33719;&#23545;&#35937;&#37096;&#20214;&#25490;&#21015;&#20998;&#24067;&#30340;&#37492;&#21035;&#22120;&#65288;&#22312;GAN&#20013;&#65289;&#65292;&#20197;&#21450;&#29983;&#25104;&#32852;&#32467;&#23545;&#35937;&#19978;&#25509;&#35302;&#21306;&#22495;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25351;&#23548;&#25163;&#21183;&#20272;&#35745;&#12290;&#36825;&#20123;&#32467;&#26500;&#21644;&#25509;&#35302;&#20808;&#39564;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#39046;&#22495;&#24046;&#36317;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#21644;&#23398;&#20064;&#30340;&#20808;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20851;&#38190;&#28857;&#23450;&#20301;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new dataset and a novel approach to learning hand-object interaction priors for hand and articulated object pose estimation. We first collect a dataset using visual teleoperation, where the human operator can directly play within a physical simulator to manipulate the articulated objects. We record the data and obtain free and accurate annotations on object poses and contact information from the simulator. Our system only requires an iPhone to record human hand motion, which can be easily scaled up and largely lower the costs of data and annotation collection. With this data, we learn 3D interaction priors including a discriminator (in a GAN) capturing the distribution of how object parts are arranged, and a diffusion model which generates the contact regions on articulated objects, guiding the hand pose estimation. Such structural and contact priors can easily transfer to real-world data with barely any domain gap. By using our data and learned priors, our method signific
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#24494;&#25968;&#25454;&#30456;&#20851;&#23618;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#39640;&#26031;&#21270;&#28508;&#22312;&#24352;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#32422;&#26463;&#36870;&#38382;&#39064;&#20026;&#33719;&#24471;&#39640;&#20445;&#30495;&#24230;&#30340;&#20998;&#24067;&#20869;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#35299;&#20915;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36870;&#38382;&#39064;&#20013;&#28508;&#22312;&#24352;&#37327;&#20559;&#31163;&#26399;&#26395;&#39640;&#26031;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2112.03860</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#39640;&#26031;&#21270;&#23618;&#29992;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#27491;&#21017;&#21270;&#30340;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Differentiable Gaussianization Layers for Inverse Problems Regularized by Deep Generative Models. (arXiv:2112.03860v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#24494;&#25968;&#25454;&#30456;&#20851;&#23618;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#39640;&#26031;&#21270;&#28508;&#22312;&#24352;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#32422;&#26463;&#36870;&#38382;&#39064;&#20026;&#33719;&#24471;&#39640;&#20445;&#30495;&#24230;&#30340;&#20998;&#24067;&#20869;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#35299;&#20915;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36870;&#38382;&#39064;&#20013;&#28508;&#22312;&#24352;&#37327;&#20559;&#31163;&#26399;&#26395;&#39640;&#26031;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22914;GAN&#12289;&#26631;&#20934;&#21270;&#27969;&#21644;&#25193;&#25955;&#27169;&#22411;&#26159;&#36870;&#38382;&#39064;&#30340;&#24378;&#22823;&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#20197;&#24110;&#21161;&#20943;&#23567;&#19981;&#36866;&#23450;&#24615;&#24182;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#36870;&#25512;&#36807;&#31243;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#28508;&#22312;&#24352;&#37327;&#21487;&#33021;&#20250;&#20174;&#26399;&#26395;&#30340;&#39640;&#32500;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#20013;&#33073;&#31163;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#22122;&#22768;&#21644;&#19981;&#20934;&#30830;&#30340;&#27491;&#21521;&#27169;&#22411;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20250;&#23548;&#33268;&#20302;&#20445;&#30495;&#24230;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26032;&#39062;&#30340;&#21487;&#24494;&#25968;&#25454;&#30456;&#20851;&#23618;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#39640;&#26031;&#21270;&#28508;&#22312;&#24352;&#37327;&#65292;&#20854;&#20013;&#20351;&#29992;&#33258;&#23450;&#20041;&#25805;&#20316;&#31526;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#25311;&#35758;&#30340;&#23618;&#23558;&#36870;&#38382;&#39064;&#32422;&#26463;&#20026;&#33719;&#24471;&#39640;&#20445;&#30495;&#24230;&#30340;&#20998;&#24067;&#20869;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#21453;&#28436;&#20219;&#21153;&#65288;&#21387;&#32553;&#24863;&#30693;MRI&#12289;&#22270;&#20687;&#21435;&#27169;&#31946;&#21644;&#20934;&#30830;&#24230;&#21463;&#38480;&#30340;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#21453;&#28436;&#38382;&#39064;&#8220;eikonal tomography&#8221;&#65289;&#19978;&#20351;&#29992;&#20004;&#31181;&#20856;&#22411;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models such as GANs, normalizing flows, and diffusion models are powerful regularizers for inverse problems. They exhibit great potential for helping reduce ill-posedness and attain high-quality results. However, the latent tensors of such deep generative models can fall out of the desired high-dimensional standard Gaussian distribution during inversion, particularly in the presence of data noise and inaccurate forward models, leading to low-fidelity solutions. To address this issue, we propose to reparameterize and Gaussianize the latent tensors using novel differentiable data-dependent layers wherein custom operators are defined by solving optimization problems. These proposed layers constrain inverse problems to obtain high-fidelity in-distribution solutions. We validate our technique on three inversion tasks: compressive-sensing MRI, image deblurring, and eikonal tomography (a nonlinear PDE-constrained inverse problem) using two representative deep generative models
&lt;/p&gt;</description></item></channel></rss>