<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>CHOSEN&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#35270;&#35282;&#28145;&#24230;&#32454;&#21270;&#30340;&#23545;&#27604;&#20551;&#35774;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#20551;&#35774;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#22810;&#35270;&#35282;&#31435;&#20307;&#21305;&#37197;&#20013;&#23454;&#29616;&#36739;&#39640;&#36136;&#37327;&#30340;&#28145;&#24230;&#21644;&#27861;&#32447;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.02225</link><description>&lt;p&gt;
CHOSEN&#65306;&#29992;&#20110;&#22810;&#35270;&#35282;&#28145;&#24230;&#32454;&#21270;&#30340;&#23545;&#27604;&#20551;&#35774;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02225
&lt;/p&gt;
&lt;p&gt;
CHOSEN&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#35270;&#35282;&#28145;&#24230;&#32454;&#21270;&#30340;&#23545;&#27604;&#20551;&#35774;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#20551;&#35774;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#22810;&#35270;&#35282;&#31435;&#20307;&#21305;&#37197;&#20013;&#23454;&#29616;&#36739;&#39640;&#36136;&#37327;&#30340;&#28145;&#24230;&#21644;&#27861;&#32447;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CHOSEN&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#28789;&#27963;&#12289;&#24378;&#22823;&#19988;&#26377;&#25928;&#30340;&#22810;&#35270;&#35282;&#28145;&#24230;&#32454;&#21270;&#26694;&#26550;&#12290;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#22810;&#35270;&#35282;&#31435;&#20307;&#21305;&#37197;&#27969;&#31243;&#20013;&#65292;&#20855;&#26377;&#38024;&#23545;&#19981;&#21516;&#22810;&#35270;&#35282;&#37319;&#38598;&#31995;&#32479;&#65288;&#22914;&#30456;&#26426;&#30456;&#23545;&#23450;&#20301;&#21644;&#38236;&#22836;&#65289;&#30340;&#30452;&#35266;&#27867;&#21270;&#33021;&#21147;&#12290;&#32473;&#23450;&#21021;&#22987;&#28145;&#24230;&#20272;&#35745;&#65292;CHOSEN&#36845;&#20195;&#22320;&#37325;&#26032;&#37319;&#26679;&#24182;&#36873;&#25321;&#26368;&#20339;&#20551;&#35774;&#65292;&#24182;&#33258;&#21160;&#36866;&#24212;&#30001;&#37319;&#38598;&#31995;&#32479;&#30830;&#23450;&#30340;&#19981;&#21516;&#24230;&#37327;&#25110;&#22266;&#26377;&#23610;&#24230;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#22312;&#36866;&#24403;&#30340;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#20013;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#20197;&#21450;&#31934;&#24515;&#35774;&#35745;&#30340;&#20551;&#35774;&#29305;&#24449;&#65292;&#22522;&#20110;&#36825;&#20123;&#29305;&#24449;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#27491;&#36127;&#20551;&#35774;&#12290;&#23558;CHOSEN&#38598;&#25104;&#21040;&#31616;&#21333;&#30340;&#22522;&#32447;&#22810;&#35270;&#35282;&#31435;&#20307;&#21305;&#37197;&#27969;&#31243;&#20013;&#65292;&#22312;&#28145;&#24230;&#21644;&#27861;&#32447;&#31934;&#24230;&#26041;&#38754;&#25552;&#20379;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36136;&#37327;&#65292;&#19982;&#35768;&#22810;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#35270;&#35282;&#31435;&#20307;&#21305;&#37197;&#27969;&#31243;&#30456;&#27604;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02225v1 Announce Type: cross  Abstract: We propose CHOSEN, a simple yet flexible, robust and effective multi-view depth refinement framework. It can be employed in any existing multi-view stereo pipeline, with straightforward generalization capability for different multi-view capture systems such as camera relative positioning and lenses. Given an initial depth estimation, CHOSEN iteratively re-samples and selects the best hypotheses, and automatically adapts to different metric or intrinsic scales determined by the capture system. The key to our approach is the application of contrastive learning in an appropriate solution space and a carefully designed hypothesis feature, based on which positive and negative hypotheses can be effectively distinguished. Integrated in a simple baseline multi-view stereo pipeline, CHOSEN delivers impressive quality in terms of depth and normal accuracy compared to many current deep learning based multi-view stereo pipelines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#22330;&#22320;&#36816;&#21160;&#35760;&#24405;&#20013;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#35752;&#36741;&#21161;&#20449;&#24687;&#23545;&#27492;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.07569</link><description>&lt;p&gt;
&#25506;&#31350;&#21333;&#21488;&#22330;&#22320;&#36816;&#21160;&#35760;&#24405;&#30340;&#28145;&#24230;&#23398;&#20064;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring Challenges in Deep Learning of Single-Station Ground Motion Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#22330;&#22320;&#36816;&#21160;&#35760;&#24405;&#20013;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#35752;&#36741;&#21161;&#20449;&#24687;&#23545;&#27492;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22320;&#38663;&#23398;&#21644;&#22320;&#38663;&#24037;&#31243;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#20381;&#36182;&#21033;&#29992;&#22330;&#22320;&#36816;&#21160;&#35760;&#24405;&#36827;&#34892;&#22320;&#38663;&#20107;&#20214;&#20998;&#31867;&#12289;&#23450;&#20301;&#12289;&#22320;&#38663;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#21644;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20174;&#36825;&#20123;&#22797;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#20013;&#26377;&#25928;&#23398;&#20064;&#30340;&#31243;&#24230;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#36741;&#21161;&#20449;&#24687;&#65288;&#22914;&#22320;&#38663;&#30456;&#21040;&#36798;&#26102;&#38388;&#25110;&#32593;&#32476;&#20869;&#22320;&#38663;&#21488;&#31449;&#20998;&#24067;&#65289;&#22312;&#20174;&#22330;&#22320;&#36816;&#21160;&#35760;&#24405;&#20013;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#20027;&#23548;&#31243;&#24230;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#20174;&#22330;&#22320;&#36816;&#21160;&#35760;&#24405;&#20013;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#26816;&#26597;&#36741;&#21161;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07569v1 Announce Type: cross  Abstract: Contemporary deep learning models have demonstrated promising results across various applications within seismology and earthquake engineering. These models rely primarily on utilizing ground motion records for tasks such as earthquake event classification, localization, earthquake early warning systems, and structural health monitoring. However, the extent to which these models effectively learn from these complex time-series signals has not been thoroughly analyzed. In this study, our objective is to evaluate the degree to which auxiliary information, such as seismic phase arrival times or seismic station distribution within a network, dominates the process of deep learning from ground motion records, potentially hindering its effectiveness. We perform a hyperparameter search on two deep learning models to assess their effectiveness in deep learning from ground motion records while also examining the impact of auxiliary information o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.06869</link><description>&lt;p&gt;
&#22312;&#26377;&#22122;&#22768;&#22522;&#30784;&#27169;&#22411;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#35843;&#25972;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#24448;&#24448;&#26080;&#27861;&#33719;&#21462;&#25110;&#25104;&#26412;&#36807;&#39640;&#65292;&#21487;&#33021;&#21253;&#21547;&#26631;&#31614;&#22122;&#22768;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36896;&#25104;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#24102;&#26469;&#24847;&#24819;&#19981;&#21040;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#26159;&#39318;&#20010;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22122;&#22768;&#24615;&#36136;&#65292;&#24182;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#26377;&#22122;&#22768;&#30340;ImageNet-1K&#12289;YFCC15M&#21644;CC12M&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23436;&#20840;&#30417;&#30563;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#23613;&#31649;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#20351;&#21516;&#39046;&#22495;&#65288;ID&#65289;&#24615;&#33021;&#21463;&#30410;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20849;&#20139;&#31867;&#20284;&#20998;&#24067;&#65292;&#20294;&#23427;&#24635;&#26159;&#20250;&#30772;&#22351;&#36328;&#39046;&#22495;&#65288;OOD&#65289;&#24615;&#33021;&#65292;&#22312;&#37027;&#37324;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#26126;&#26174;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06869v1 Announce Type: cross  Abstract: Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are signific
&lt;/p&gt;</description></item></channel></rss>