<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#21547;&#35780;&#20272;&#23545;&#29031;&#22810;&#20010;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#21450;&#35780;&#20272;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20287</link><description>&lt;p&gt;
&#22522;&#20934;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Counterfactual Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20287
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#21547;&#35780;&#20272;&#23545;&#29031;&#22810;&#20010;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#21450;&#35780;&#20272;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#22312;&#29702;&#35299;&#21464;&#37327;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#20855;&#26377;&#20851;&#38190;&#20316;&#29992;&#65292;&#22312;&#35299;&#37322;&#24615;&#21644;&#29983;&#25104;&#26080;&#20559;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#26377;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#22270;&#20687;&#29983;&#25104;&#26412;&#36523;&#23601;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#23545;&#20110;&#35780;&#20272;&#23545;&#29031;&#29983;&#25104;&#30340;&#38656;&#27714;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#22240;&#20026;&#26681;&#25454;&#23450;&#20041;&#65292;&#23545;&#29031;&#24773;&#26223;&#26159;&#27809;&#26377;&#21487;&#35266;&#27979;&#22522;&#20934;&#20107;&#23454;&#30340;&#20551;&#35774;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#39062;&#32508;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#20391;&#37325;&#20110;&#35780;&#20272;&#23545;&#29031;&#30340;&#19981;&#21516;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20363;&#22914;&#32452;&#25104;&#12289;&#26377;&#25928;&#24615;&#12289;&#24178;&#39044;&#30340;&#26368;&#23567;&#24615;&#21644;&#22270;&#20687;&#36924;&#30495;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#33539;&#24335;&#30340;&#19977;&#31181;&#19981;&#21516;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#31867;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#37197;&#22791;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20287v1 Announce Type: cross  Abstract: Counterfactual image generation is pivotal for understanding the causal relations of variables, with applications in interpretability and generation of unbiased synthetic data. However, evaluating image generation is a long-standing challenge in itself. The need to evaluate counterfactual generation compounds on this challenge, precisely because counterfactuals, by definition, are hypothetical scenarios without observable ground truths. In this paper, we present a novel comprehensive framework aimed at benchmarking counterfactual image generation methods. We incorporate metrics that focus on evaluating diverse aspects of counterfactuals, such as composition, effectiveness, minimality of interventions, and image realism. We assess the performance of three distinct conditional image generation model types, based on the Structural Causal Model paradigm. Our work is accompanied by a user-friendly Python package which allows to further eval
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;LenCom-Eval&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#20173;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2403.16422</link><description>&lt;p&gt;
&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65306;&#21521;&#20934;&#30830;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#23383;&#24418;&#22686;&#24378;&#22270;&#20687;&#29983;&#25104;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Refining Text-to-Image Generation: Towards Accurate Training-Free Glyph-Enhanced Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16422
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;LenCom-Eval&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#20173;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#26041;&#27861;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#26222;&#36890;&#25193;&#25955;&#27169;&#22411;&#36890;&#24120;&#22312;&#29983;&#25104;&#22270;&#20687;&#20013;&#26174;&#31034;&#30340;&#25991;&#26412;&#20013;&#23384;&#22312;&#25340;&#20889;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#29983;&#25104;&#35270;&#35273;&#25991;&#26412;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#65292;&#19981;&#20165;&#20855;&#26377;&#23398;&#26415;&#20215;&#20540;&#65292;&#36824;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#29983;&#25104;&#20934;&#30830;&#30340;&#35270;&#35273;&#25991;&#26412;&#22270;&#20687;&#65292;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#37319;&#29992;&#20102;&#19968;&#31181;&#23383;&#24418;&#25511;&#21046;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#25991;&#26412;&#24067;&#23616;&#29983;&#25104;&#22120;&#65292;&#28982;&#21518;&#26159;&#19968;&#20010;&#22312;&#29983;&#25104;&#30340;&#25991;&#26412;&#24067;&#23616;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#22270;&#20687;&#30340;&#22270;&#20687;&#29983;&#25104;&#22120;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#26469;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;LenCom-Eval&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#19987;&#38376;&#29992;&#20110;&#27979;&#35797;&#27169;&#22411;&#22312;&#29983;&#25104;&#20855;&#26377;&#22797;&#26434;&#35270;&#35273;&#25991;&#26412;&#30340;&#22270;&#20687;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16422v1 Announce Type: cross  Abstract: Over the past few years, Text-to-Image (T2I) generation approaches based on diffusion models have gained significant attention. However, vanilla diffusion models often suffer from spelling inaccuracies in the text displayed within the generated images. The capability to generate visual text is crucial, offering both academic interest and a wide range of practical applications. To produce accurate visual text images, state-of-the-art techniques adopt a glyph-controlled image generation approach, consisting of a text layout generator followed by an image generator that is conditioned on the generated text layout. Nevertheless, our study reveals that these models still face three primary challenges, prompting us to develop a testbed to facilitate future research. We introduce a benchmark, LenCom-Eval, specifically designed for testing models' capability in generating images with Lengthy and Complex visual text. Subsequently, we introduce 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#31616;&#21270;&#21518;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#65288;DSB&#65289;&#65292;&#36890;&#36807;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#30340;&#32479;&#19968;&#35299;&#20915;&#20102;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.14623</link><description>&lt;p&gt;
&#31616;&#21270;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;
&lt;/p&gt;
&lt;p&gt;
Simplified Diffusion Schr\"odinger Bridge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14623
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#31616;&#21270;&#21518;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#65288;DSB&#65289;&#65292;&#36890;&#36807;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#30340;&#32479;&#19968;&#35299;&#20915;&#20102;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#31616;&#21270;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#65288;DSB&#65289;&#65292;&#20415;&#20110;&#23558;&#20854;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#32479;&#19968;&#36215;&#26469;&#65292;&#35299;&#20915;&#20102;DSB&#22312;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#22686;&#24378;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;SGM&#20316;&#20026;DSB&#30340;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36825;&#20004;&#20010;&#26694;&#26550;&#30340;&#20248;&#21183;&#65292;&#30830;&#20445;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25913;&#36827;&#20102;SGM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#26415;&#65292;&#23613;&#31649;&#23384;&#22312;&#29702;&#35770;&#36817;&#20284;&#65292;&#20294;&#23454;&#38469;&#19978;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#25311;&#21512;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#23454;&#65292;&#35777;&#23454;&#20102;&#31616;&#21270;&#30340;DSB&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#24037;&#20316;&#30340;&#36129;&#29486;&#20026;&#20808;&#36827;&#30340;&#29983;&#25104;&#24314;&#27169;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14623v1 Announce Type: new  Abstract: This paper introduces a novel theoretical simplification of the Diffusion Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based Generative Models (SGMs), addressing the limitations of DSB in complex data generation and enabling faster convergence and enhanced performance. By employing SGMs as an initial solution for DSB, our approach capitalizes on the strengths of both frameworks, ensuring a more efficient training process and improving the performance of SGM. We also propose a reparameterization technique that, despite theoretical approximations, practically improves the network's fitting capabilities. Our extensive experimental evaluations confirm the effectiveness of the simplified DSB, demonstrating its significant improvements. We believe the contributions of this work pave the way for advanced generative modeling. The code is available at https://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.
&lt;/p&gt;</description></item><item><title>KEBench&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#26032;&#22686;&#21152;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#21487;&#31227;&#26893;&#24615;&#65289;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.07350</link><description>&lt;p&gt;
KEBench: &#29992;&#20110;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07350
&lt;/p&gt;
&lt;p&gt;
KEBench&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#26032;&#22686;&#21152;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#21487;&#31227;&#26893;&#24615;&#65289;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07350v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#30446;&#21069;&#65292;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#30693;&#35782;&#32534;&#36753;&#30740;&#31350;&#24456;&#23569;&#12290;&#32534;&#36753;LVLMs&#38754;&#20020;&#30528;&#26377;&#25928;&#25972;&#21512;&#22810;&#31181;&#27169;&#24577;&#65288;&#22270;&#20687;&#21644;&#25991;&#26412;&#65289;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#30830;&#20445;&#20462;&#25913;&#36830;&#36143;&#19988;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#12290;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#20855;&#26377;&#19977;&#20010;&#24230;&#37327;&#26631;&#20934;&#65288;&#21487;&#38752;&#24615;&#12289;&#23616;&#37096;&#24615;&#21644;&#19968;&#33324;&#24615;&#65289;&#29992;&#20110;&#34913;&#37327;LVLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#22312;&#35780;&#20272;&#20013;&#20351;&#29992;&#30340;&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#19981;&#36275;&#65292;&#24182;&#19988;&#26080;&#27861;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#26377;&#25928;&#22320;&#21033;&#29992;&#19982;&#30456;&#20851;&#20869;&#23481;&#30456;&#20851;&#30340;&#32534;&#36753;&#30693;&#35782;&#12290;&#25105;&#20204;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;$\textbf{KEBench}$&#65292;&#24182;&#25193;&#23637;&#20102;&#26032;&#24230;&#37327;&#26631;&#20934;(&#21487;&#31227;&#26893;&#24615;)&#20197;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#20511;&#21161;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#65292;&#25105;&#20204;&#30340;&#22270;&#20687;&#25968;&#25454;&#21576;&#29616;&#20986;&#26126;&#30830;&#30340;&#32473;&#23454;&#20307;&#26041;&#21521;&#24615;&#12290;&#36825;&#31181;&#26041;&#21521;&#24615;&#21487;&#20197;&#36827;&#19968;&#27493;&#29992;&#20110;&#25552;&#21462;&#19982;&#23454;&#20307;&#30456;&#20851;&#30340;&#30693;&#35782;&#21644;&#36827;&#34892;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07350v1 Announce Type: cross  Abstract: Currently, little research has been done on knowledge editing for Large Vision-Language Models (LVLMs). Editing LVLMs faces the challenge of effectively integrating diverse modalities (image and text) while ensuring coherent and contextually relevant modifications. An existing benchmark has three metrics (Reliability, Locality and Generality) to measure knowledge editing for LVLMs. However, the benchmark falls short in the quality of generated images used in evaluation and cannot assess whether models effectively utilize edited knowledge in relation to the associated content. We adopt different data collection methods to construct a new benchmark, $\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive evaluation. Leveraging a multimodal knowledge graph, our image data exhibits clear directionality towards entities. This directional aspect can be further utilized to extract entity-related knowledge and form editing 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21069;&#21015;&#33146;MRI&#20559;&#32622;&#22330;&#26657;&#27491;&#30340;&#27010;&#29575;&#21704;&#36798;&#29595;U-Net&#65292;&#24341;&#20837;&#20102;&#21704;&#36798;&#29595;U-Net&#65288;HU-Net&#65289;&#36890;&#36807;&#21704;&#36798;&#29595;&#21464;&#25442;&#23558;&#36755;&#20837;&#22270;&#20687;&#20174;&#26102;&#22495;&#36716;&#25442;&#20026;&#39057;&#22495;&#65292;&#24182;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#28388;&#27874;&#22120;&#21644;&#30828;&#38408;&#20540;&#23618;&#28040;&#38500;&#39640;&#39057;&#37096;&#20998;&#12290;</title><link>https://arxiv.org/abs/2403.05024</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;MRI&#20559;&#32622;&#22330;&#26657;&#27491;&#30340;&#27010;&#29575;&#21704;&#36798;&#29595;U-Net
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Hadamard U-Net for MRI Bias Field Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05024
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21069;&#21015;&#33146;MRI&#20559;&#32622;&#22330;&#26657;&#27491;&#30340;&#27010;&#29575;&#21704;&#36798;&#29595;U-Net&#65292;&#24341;&#20837;&#20102;&#21704;&#36798;&#29595;U-Net&#65288;HU-Net&#65289;&#36890;&#36807;&#21704;&#36798;&#29595;&#21464;&#25442;&#23558;&#36755;&#20837;&#22270;&#20687;&#20174;&#26102;&#22495;&#36716;&#25442;&#20026;&#39057;&#22495;&#65292;&#24182;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#28388;&#27874;&#22120;&#21644;&#30828;&#38408;&#20540;&#23618;&#28040;&#38500;&#39640;&#39057;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#22330;&#19981;&#22343;&#21248;&#24615;&#26657;&#27491;&#22312;MRI&#20998;&#26512;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#24050;&#24314;&#31435;&#30340;&#25216;&#26415;&#26159;&#20026;&#33041;MRI&#35774;&#35745;&#30340;&#65292;&#20551;&#35774;&#30456;&#21516;&#32452;&#32455;&#20013;&#30340;&#22270;&#20687;&#24378;&#24230;&#36981;&#24490;&#22343;&#21248;&#20998;&#24067;&#12290;&#36825;&#31181;&#20551;&#35774;&#19981;&#26131;&#36866;&#29992;&#20110;&#20854;&#20182;&#22120;&#23448;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20307;&#31215;&#23567;&#65292;&#36136;&#22320;&#19981;&#22343;&#21248;&#65288;&#24378;&#24230;&#21464;&#21270;&#22823;&#65289;&#30340;&#22120;&#23448;&#65292;&#27604;&#22914;&#21069;&#21015;&#33146;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21069;&#21015;&#33146;MRI&#20559;&#32622;&#22330;&#26657;&#27491;&#30340;&#27010;&#29575;&#21704;&#36798;&#29595;U-Net&#65288;PHU-Net&#65289;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21704;&#36798;&#29595;U-Net&#65288;HU-Net&#65289;&#20197;&#25552;&#21462;&#20302;&#39057;&#26631;&#37327;&#22330;&#65292;&#23558;&#20854;&#20056;&#20197;&#21407;&#22987;&#36755;&#20837;&#20197;&#33719;&#24471;&#21407;&#22411;&#26657;&#27491;&#22270;&#20687;&#12290;HU-Net&#36890;&#36807;&#21704;&#36798;&#29595;&#21464;&#25442;&#23558;&#36755;&#20837;&#22270;&#20687;&#20174;&#26102;&#22495;&#36716;&#25442;&#20026;&#39057;&#22495;&#12290;&#22312;&#39057;&#22495;&#20013;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#28388;&#27874;&#22120;&#65288;&#32553;&#25918;&#23618;&#65289;&#12289;&#30828;&#38408;&#20540;&#23618;&#28040;&#38500;&#39640;&#39057;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05024v1 Announce Type: cross  Abstract: Magnetic field inhomogeneity correction remains a challenging task in MRI analysis. Most established techniques are designed for brain MRI by supposing that image intensities in the identical tissue follow a uniform distribution. Such an assumption cannot be easily applied to other organs, especially those that are small in size and heterogeneous in texture (large variations in intensity), such as the prostate. To address this problem, this paper proposes a probabilistic Hadamard U-Net (PHU-Net) for prostate MRI bias field correction. First, a novel Hadamard U-Net (HU-Net) is introduced to extract the low-frequency scalar field, multiplied by the original input to obtain the prototypical corrected image. HU-Net converts the input image from the time domain into the frequency domain via Hadamard transform. In the frequency domain, high-frequency components are eliminated using the trainable filter (scaling layer), hard-thresholding laye
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#34913;&#37327;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#20351;&#29992;&#22270;&#20687;-&#26631;&#39064;&#23545;&#36827;&#34892;&#35757;&#32451;&#30340;VLMs&#65292;&#27169;&#22411;&#30830;&#23454;&#20250;&#20445;&#30041;&#20851;&#20110;&#35757;&#32451;&#22270;&#20687;&#20013;&#30340;&#20010;&#21035;&#23545;&#35937;&#30340;&#20449;&#24687;&#65292;&#25991;&#26412;&#38543;&#26426;&#21270;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20943;&#36731;&#35760;&#24518;&#29616;&#35937;&#32780;&#23545;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>https://arxiv.org/abs/2402.02103</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24515;&#29702;&#29616;&#35937;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
D\'ej\`a Vu Memorization in Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02103
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#34913;&#37327;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#20351;&#29992;&#22270;&#20687;-&#26631;&#39064;&#23545;&#36827;&#34892;&#35757;&#32451;&#30340;VLMs&#65292;&#27169;&#22411;&#30830;&#23454;&#20250;&#20445;&#30041;&#20851;&#20110;&#35757;&#32451;&#22270;&#20687;&#20013;&#30340;&#20010;&#21035;&#23545;&#35937;&#30340;&#20449;&#24687;&#65292;&#25991;&#26412;&#38543;&#26426;&#21270;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20943;&#36731;&#35760;&#24518;&#29616;&#35937;&#32780;&#23545;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20316;&#20026;&#26368;&#20808;&#36827;&#30340;&#34920;&#31034;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#20855;&#26377;&#35832;&#22810;&#19979;&#28216;&#24212;&#29992;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#26816;&#32034;&#21644;&#29983;&#25104;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#20250;&#35760;&#24518;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20063;&#23545;&#27867;&#21270;&#26377;&#30528;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;VLMs&#20013;&#35760;&#24518;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#24515;&#29702;&#29616;&#35937;&#35760;&#24518;&#12290;&#23545;&#20110;&#22312;&#22270;&#20687;-&#26631;&#39064;&#23545;&#19978;&#35757;&#32451;&#30340;VLMs&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30830;&#23454;&#20445;&#30041;&#20102;&#20851;&#20110;&#35757;&#32451;&#22270;&#20687;&#20013;&#20010;&#21035;&#23545;&#35937;&#30340;&#20449;&#24687;&#65292;&#36229;&#20986;&#20102;&#20174;&#30456;&#20851;&#24615;&#25110;&#22270;&#20687;&#26631;&#39064;&#20013;&#21487;&#20197;&#25512;&#26029;&#20986;&#30340;&#33539;&#30068;&#12290;&#25105;&#20204;&#22312;&#26679;&#26412;&#21644;&#24635;&#20307;&#27700;&#24179;&#19978;&#35780;&#20272;&#20102;&#24515;&#29702;&#29616;&#35937;&#35760;&#24518;&#65292;&#24182;&#23637;&#31034;&#20102;OpenCLIP&#22312;&#22810;&#36798;5000&#19975;&#20010;&#22270;&#20687;-&#26631;&#39064;&#23545;&#19978;&#35757;&#32451;&#26102;&#30340;&#26174;&#33879;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25991;&#26412;&#38543;&#26426;&#21270;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20943;&#36731;&#20102;&#35760;&#24518;&#65292;&#21516;&#26102;&#23545;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#20135;&#29983;&#20102;&#36866;&#24230;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models (VLMs) have emerged as the state-of-the-art representation learning solution, with myriads of downstream applications such as image classification, retrieval and generation. A natural question is whether these models memorize their training data, which also has implications for generalization. We propose a new method for measuring memorization in VLMs, which we call d\'ej\`a vu memorization. For VLMs trained on image-caption pairs, we show that the model indeed retains information about individual objects in the training images beyond what can be inferred from correlations or the image caption. We evaluate d\'ej\`a vu memorization at both sample and population level, and show that it is significant for OpenCLIP trained on as many as 50M image-caption pairs. Finally, we show that text randomization considerably mitigates memorization while only moderately impacting the model's downstream task performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#32463;&#20856;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#25506;&#31350;&#65292;&#25581;&#31034;&#20102;&#20998;&#21106;TTA&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;&#32463;&#20856;TTA&#31574;&#30053;&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#24182;&#19981;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.05341</link><description>&lt;p&gt;
&#23545;&#35821;&#20041;&#20998;&#21106;&#20013;&#32463;&#20856;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#30340;&#25209;&#21028;&#24615;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Critical Look at Classic Test-Time Adaptation Methods in Semantic Segmentation. (arXiv:2310.05341v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05341
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#32463;&#20856;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#25506;&#31350;&#65292;&#25581;&#31034;&#20102;&#20998;&#21106;TTA&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;&#32463;&#20856;TTA&#31574;&#30053;&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#24182;&#19981;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;TTA&#65289;&#26088;&#22312;&#23558;&#26368;&#21021;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36866;&#24212;&#20110;&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#21487;&#33021;&#20998;&#24067;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;TTA&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#23545;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;TTA&#25506;&#32034;&#38750;&#24120;&#26377;&#38480;&#12290;&#36825;&#31181;&#23545;&#20998;&#31867;&#30340;&#31361;&#20986;&#37325;&#35270;&#21487;&#33021;&#23548;&#33268;&#35768;&#22810;&#26032;&#25163;&#21644;&#24037;&#31243;&#24072;&#38169;&#35823;&#22320;&#35748;&#20026;&#20026;&#20998;&#31867;&#35774;&#35745;&#30340;&#32463;&#20856;TTA&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#20998;&#21106;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20551;&#35774;&#20173;&#26410;&#32463;&#39564;&#35777;&#65292;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#20998;&#21106;TTA&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#30830;&#23450;&#32463;&#20856;TTA&#31574;&#30053;&#26159;&#21542;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#36825;&#19968;&#20219;&#21153;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#32467;&#26524;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#24120;&#29992;&#20110;&#20998;&#31867;TTA&#30340;&#32463;&#20856;&#25209;&#24402;&#19968;&#21270;&#26356;&#26032;&#31574;&#30053;&#21482;&#33021;&#24102;&#26469;&#36731;&#24494;&#30340;&#24615;&#33021;&#25913;&#21892;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#20250;&#23545;&#32467;&#26524;&#20135;&#29983;&#36870;&#21521;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-time adaptation (TTA) aims to adapt a model, initially trained on training data, to potential distribution shifts in the test data. Most existing TTA studies, however, focus on classification tasks, leaving a notable gap in the exploration of TTA for semantic segmentation. This pronounced emphasis on classification might lead numerous newcomers and engineers to mistakenly assume that classic TTA methods designed for classification can be directly applied to segmentation. Nonetheless, this assumption remains unverified, posing an open question. To address this, we conduct a systematic, empirical study to disclose the unique challenges of segmentation TTA, and to determine whether classic TTA strategies can effectively address this task. Our comprehensive results have led to three key observations. First, the classic batch norm updating strategy, commonly used in classification TTA, only brings slight performance improvement, and in some cases it might even adversely affect the resu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#25628;&#32034;&#22823;&#35268;&#27169;&#30340;LAION&#25968;&#25454;&#38598;&#65292;&#23581;&#35797;&#37325;&#26032;&#21019;&#24314;&#22270;&#20687;&#32593;&#65292;&#24182;&#21457;&#29616;&#19982;&#21407;&#22987;&#22270;&#20687;&#32593;&#30456;&#27604;&#65292;&#26032;&#24314;&#30340;LAIONet&#20855;&#26377;&#26126;&#26174;&#19981;&#21516;&#20043;&#22788;&#12290;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#26159;&#65292;&#22312;&#22522;&#20110;&#22270;&#20687;&#25551;&#36848;&#36827;&#34892;&#25628;&#32034;&#26102;&#65292;&#23384;&#22312;&#20449;&#24687;&#29942;&#39048;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#36873;&#25321;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.15769</link><description>&lt;p&gt;
&#22270;&#20687;&#32593;&#20026;&#20309;&#19982;LAION&#32593;&#32476;&#25130;&#28982;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
What Makes ImageNet Look Unlike LAION. (arXiv:2306.15769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#25628;&#32034;&#22823;&#35268;&#27169;&#30340;LAION&#25968;&#25454;&#38598;&#65292;&#23581;&#35797;&#37325;&#26032;&#21019;&#24314;&#22270;&#20687;&#32593;&#65292;&#24182;&#21457;&#29616;&#19982;&#21407;&#22987;&#22270;&#20687;&#32593;&#30456;&#27604;&#65292;&#26032;&#24314;&#30340;LAIONet&#20855;&#26377;&#26126;&#26174;&#19981;&#21516;&#20043;&#22788;&#12290;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#26159;&#65292;&#22312;&#22522;&#20110;&#22270;&#20687;&#25551;&#36848;&#36827;&#34892;&#25628;&#32034;&#26102;&#65292;&#23384;&#22312;&#20449;&#24687;&#29942;&#39048;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#36873;&#25321;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#32593;&#26159;&#36890;&#36807;Flickr&#22270;&#20687;&#25628;&#32034;&#32467;&#26524;&#21019;&#24314;&#30340;&#12290;&#22914;&#26524;&#25105;&#20204;&#20165;&#26681;&#25454;&#22270;&#20687;&#25551;&#36848;&#37325;&#26032;&#21019;&#24314;&#22270;&#20687;&#32593;&#65292;&#25628;&#32034;&#22823;&#35268;&#27169;&#30340;LAION&#25968;&#25454;&#38598;&#20250;&#21457;&#29983;&#20160;&#20040;&#21602;&#65311;&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#36825;&#20010;&#21453;&#20107;&#23454;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#21457;&#29616;&#37325;&#26032;&#21019;&#24314;&#30340;&#22270;&#20687;&#32593;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;LAIONet&#65292;&#19982;&#21407;&#22987;&#22270;&#20687;&#32593;&#26377;&#26126;&#26174;&#19981;&#21516;&#20043;&#22788;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21407;&#22987;&#22270;&#20687;&#32593;&#20013;&#22270;&#20687;&#30340;&#31867;&#20869;&#30456;&#20284;&#24615;&#36828;&#39640;&#20110;LAIONet&#12290;&#22240;&#27492;&#65292;&#22312;&#22270;&#20687;&#32593;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;LAIONet&#19978;&#34920;&#29616;&#26126;&#26174;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#26684;&#35299;&#37322;&#36825;&#31181;&#24046;&#24322;&#30340;&#35266;&#28857;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#24615;&#30340;&#23454;&#39564;&#20104;&#20197;&#25903;&#25345;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#20165;&#22522;&#20110;&#22270;&#20687;&#25551;&#36848;&#36827;&#34892;&#25628;&#32034;&#20250;&#20135;&#29983;&#20449;&#24687;&#29942;&#39048;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#22522;&#20110;&#22270;&#20687;&#36807;&#28388;&#26102;&#23384;&#22312;&#30340;&#36873;&#25321;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#35299;&#37322;&#24418;&#24335;&#21270;&#20102;&#19968;&#20010;&#38271;&#26399;&#30340;&#30452;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
ImageNet was famously created from Flickr image search results. What if we recreated ImageNet instead by searching the massive LAION dataset based on image captions alone? In this work, we carry out this counterfactual investigation. We find that the resulting ImageNet recreation, which we call LAIONet, looks distinctly unlike the original. Specifically, the intra-class similarity of images in the original ImageNet is dramatically higher than it is for LAIONet. Consequently, models trained on ImageNet perform significantly worse on LAIONet. We propose a rigorous explanation for the discrepancy in terms of a subtle, yet important, difference in two plausible causal data-generating processes for the respective datasets, that we support with systematic experimentation. In a nutshell, searching based on an image caption alone creates an information bottleneck that mitigates the selection bias otherwise present in image-based filtering. Our explanation formalizes a long-held intuition in th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;DINO&#22522;&#32447;&#27169;&#22411;RHINO&#12290;&#24182;&#36890;&#36807;&#21256;&#29273;&#21033;&#21305;&#37197;&#21644;&#26597;&#35810;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21160;&#24577;&#38477;&#22122;&#65292;&#35299;&#20915;&#20102;&#37325;&#22797;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.07598</link><description>&lt;p&gt;
RHINO&#65306;&#36890;&#36807;&#21256;&#29273;&#21033;&#21305;&#37197;&#23454;&#29616;&#21160;&#24577;&#38477;&#22122;&#30340;&#26059;&#36716;&#30446;&#26631;&#26816;&#27979;&#30340;&#26059;&#36716;DETR
&lt;/p&gt;
&lt;p&gt;
RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection. (arXiv:2305.07598v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;DINO&#22522;&#32447;&#27169;&#22411;RHINO&#12290;&#24182;&#36890;&#36807;&#21256;&#29273;&#21033;&#21305;&#37197;&#21644;&#26597;&#35810;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21160;&#24577;&#38477;&#22122;&#65292;&#35299;&#20915;&#20102;&#37325;&#22797;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;DINO&#30340;&#21457;&#24067;&#65292;&#19968;&#31181;DETR&#30340;&#21464;&#20307;&#65292;&#26816;&#27979;&#21464;&#21387;&#22120;&#27491;&#22312;&#36890;&#36807;&#20854;&#31471;&#21040;&#31471;&#35774;&#35745;&#21644;&#21487;&#25193;&#23637;&#24615;&#22312;&#30446;&#26631;&#26816;&#27979;&#22522;&#20934;&#20013;&#21047;&#26032;&#35760;&#24405;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#39044;&#35745;&#20174;&#20854;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#33719;&#24471;&#26356;&#22810;&#30340;&#22909;&#22788;&#65292;&#22914;&#28040;&#38500;NMS&#21644;&#19982;&#38170;&#30456;&#20851;&#30340;&#25104;&#26412;&#65292;&#20294;&#23578;&#26410;&#24443;&#24213;&#30740;&#31350;DETR&#22312;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#30340;&#25193;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#38754;&#21521;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;DINO&#22522;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30452;&#25509;&#20351;&#29992;DETR&#36827;&#34892;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#24182;&#19981;&#33021;&#20445;&#35777;&#19981;&#37325;&#22797;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25104;&#26412;&#26469;&#20943;&#36731;&#36825;&#31181;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#22122;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20351;&#29992;&#21256;&#29273;&#21033;&#21305;&#37197;&#26469;&#36807;&#28388;&#20887;&#20313;&#30340;&#24102;&#22122;&#22768;&#30340;&#26597;&#35810;&#65292;&#24182;&#20351;&#29992;&#26597;&#35810;&#23545;&#40784;&#26469;&#20445;&#25345;Transformer&#35299;&#30721;&#22120;&#23618;&#20043;&#38388;&#30340;&#21305;&#37197;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#20197;&#21069;&#30340;&#26059;&#36716;DETR&#21644;&#20854;&#20182;&#23545;&#25163;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the publication of DINO, a variant of the Detection Transformer (DETR), Detection Transformers are breaking the record in the object detection benchmark with the merits of their end-to-end design and scalability. However, the extension of DETR to oriented object detection has not been thoroughly studied although more benefits from its end-to-end architecture are expected such as removing NMS and anchor-related costs. In this paper, we propose a first strong DINO-based baseline for oriented object detection. We found that straightforward employment of DETRs for oriented object detection does not guarantee non-duplicate prediction, and propose a simple cost to mitigate this. Furthermore, we introduce a novel denoising strategy that uses Hungarian matching to filter redundant noised queries and query alignment to preserve matching consistency between Transformer decoder layers. Our proposed model outperforms previous rotated DETRs and other counterparts, achieving state-of-the-art pe
&lt;/p&gt;</description></item><item><title>GPT-NAS&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#20248;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#25552;&#20986;&#36817;&#20284;&#30340;&#26550;&#26500;&#32452;&#20214;&#20943;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05351</link><description>&lt;p&gt;
GPT-NAS: &#20197;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
GPT-NAS: Neural Architecture Search with the Generative Pre-Trained Model. (arXiv:2305.05351v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05351
&lt;/p&gt;
&lt;p&gt;
GPT-NAS&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#20248;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#25552;&#20986;&#36817;&#20284;&#30340;&#26550;&#26500;&#32452;&#20214;&#20943;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#24050;&#32463;&#25104;&#20026;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#26368;&#20248;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#34429;&#28982;&#19968;&#20123;&#20154;&#24037;&#35774;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;NAS&#26041;&#27861;&#20013;&#24456;&#23569;&#20986;&#29616;&#36825;&#31867;&#25104;&#26524;&#65292;&#20027;&#35201;&#21407;&#22240;&#22312;&#20110;&#31070;&#32463;&#26550;&#26500;&#30340;&#25628;&#32034;&#31354;&#38388;&#22826;&#22823;&#20102;&#65292;&#23548;&#33268;NAS&#31639;&#27861;&#25928;&#29575;&#20302;&#19979;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;GPT-NAS&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#20248;&#21270;&#31070;&#32463;&#26550;&#26500;&#12290;&#22312;GPT-NAS&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#19968;&#20010;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#26500;&#24314;&#31070;&#32463;&#26550;&#26500;&#30340;&#22522;&#26412;&#35268;&#24459;&#12290;&#22240;&#27492;&#65292;GPT-NAS&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25552;&#20986;&#21512;&#29702;&#30340;&#26550;&#26500;&#32452;&#20214;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#25628;&#32034;&#31354;&#38388;&#65292;&#24341;&#20837;&#20102;&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;GPT-NAS&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) has emerged as one of the effective methods to design the optimal neural network architecture automatically. Although neural architectures have achieved human-level performances in several tasks, few of them are obtained from the NAS method. The main reason is the huge search space of neural architectures, making NAS algorithms inefficient. This work presents a novel architecture search algorithm, called GPT-NAS, that optimizes neural architectures by Generative Pre-Trained (GPT) model. In GPT-NAS, we assume that a generative model pre-trained on a large-scale corpus could learn the fundamental law of building neural architectures. Therefore, GPT-NAS leverages the generative pre-trained (GPT) model to propose reasonable architecture components given the basic one. Such an approach can largely reduce the search space by introducing prior knowledge in the search process. Extensive experimental results show that our GPT-NAS method significantly outperforms
&lt;/p&gt;</description></item></channel></rss>