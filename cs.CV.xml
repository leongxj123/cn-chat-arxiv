<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#35757;&#32451;&#25216;&#26415;&#65292;&#21363;Attention Interpolation via Diffusion (AID)&#65292;&#36890;&#36807;&#20869;/&#22806;&#25554;&#20540;&#27880;&#24847;&#21147;&#23618;&#12289;&#25554;&#20540;&#27880;&#24847;&#21147;&#19982;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#20197;&#25552;&#39640;&#20445;&#30495;&#24230;&#65292;&#20197;&#21450;&#24212;&#29992;&#36125;&#22612;&#20998;&#24067;&#36827;&#34892;&#36873;&#25321;&#20197;&#22686;&#21152;&#24179;&#28369;&#24230;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25554;&#20540;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17924</link><description>&lt;p&gt;
AID: &#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#30340;&#27880;&#37325;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
AID: Attention Interpolation of Text-to-Image Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17924
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#35757;&#32451;&#25216;&#26415;&#65292;&#21363;Attention Interpolation via Diffusion (AID)&#65292;&#36890;&#36807;&#20869;/&#22806;&#25554;&#20540;&#27880;&#24847;&#21147;&#23618;&#12289;&#25554;&#20540;&#27880;&#24847;&#21147;&#19982;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#20197;&#25552;&#39640;&#20445;&#30495;&#24230;&#65292;&#20197;&#21450;&#24212;&#29992;&#36125;&#22612;&#20998;&#24067;&#36827;&#34892;&#36873;&#25321;&#20197;&#22686;&#21152;&#24179;&#28369;&#24230;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25554;&#20540;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17924v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#26377;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#21019;&#24314;&#30475;&#19981;&#35265;&#30340;&#22270;&#20687;&#65292;&#26377;&#21161;&#20110;&#22270;&#20687;&#25554;&#20540;&#12290;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25554;&#20540;&#24050;&#32463;&#24471;&#21040;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#26159;&#20855;&#26377;&#29305;&#23450;&#26465;&#20214;&#65288;&#22914;&#25991;&#26412;&#25110;&#23039;&#21183;&#65289;&#30340;&#25554;&#20540;&#21364;&#20102;&#35299;&#19981;&#22810;&#12290;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22914;&#22312;&#26465;&#20214;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#25554;&#20540;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#22270;&#20687;&#32570;&#20047;&#19968;&#33268;&#24615;&#12289;&#24179;&#28369;&#24230;&#21644;&#20445;&#30495;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;Attention Interpolation via Diffusion (AID)&#30340;&#26032;&#39062;&#26080;&#35757;&#32451;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;1&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#20869;/&#22806;&#25554;&#20540;&#27880;&#24847;&#21147;&#23618;&#65307;2&#65289;&#23558;&#25554;&#20540;&#27880;&#24847;&#21147;&#19982;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#20197;&#25552;&#39640;&#20445;&#30495;&#24230;&#65307;3&#65289;&#24212;&#29992;&#36125;&#22612;&#20998;&#24067;&#36827;&#34892;&#36873;&#25321;&#20197;&#22686;&#21152;&#24179;&#28369;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20307;&#65292;Prompt-guided Attention Interpolation via Diffusion (PAID)&#65292;&#23427;&#23558;&#25554;&#20540;&#35270;&#20026;&#20381;&#36182;&#20110;&#26465;&#20214;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21019;&#24314;&#20986;&#26356;&#20855;&#21019;&#36896;&#24615;&#30340;&#26032;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17924v1 Announce Type: cross  Abstract: Conditional diffusion models can create unseen images in various settings, aiding image interpolation. Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or poses is less understood. Simple approaches, such as linear interpolation in the space of conditions, often result in images that lack consistency, smoothness, and fidelity. To that end, we introduce a novel training-free technique named Attention Interpolation via Diffusion (AID). Our key contributions include 1) proposing an inner/outer interpolated attention layer; 2) fusing the interpolated attention with self-attention to boost fidelity; and 3) applying beta distribution to selection to increase smoothness. We also present a variant, Prompt-guided Attention Interpolation via Diffusion (PAID), that considers interpolation as a condition-dependent generative process. This method enables the creation of new images with greater con
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20934;&#30830;&#23450;&#20301;&#21644;&#24809;&#32602;&#24187;&#35273;&#26631;&#35760;&#65292;ESREAL&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#20041;&#37325;&#24314;&#26469;&#25233;&#21046;&#29983;&#25104;&#24187;&#35273;&#65292;&#35299;&#20915;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16167</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#37325;&#24314;&#20943;&#23569;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16167
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20934;&#30830;&#23450;&#20301;&#21644;&#24809;&#32602;&#24187;&#35273;&#26631;&#35760;&#65292;ESREAL&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#20041;&#37325;&#24314;&#26469;&#25233;&#21046;&#29983;&#25104;&#24187;&#35273;&#65292;&#35299;&#20915;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#23545;&#20854;&#21487;&#38752;&#24615;&#26500;&#25104;&#37325;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#38271;&#26631;&#39064;&#26102;&#12290;&#24403;&#21069;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#21644;&#20943;&#36731;&#36825;&#20123;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ESREAL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20934;&#30830;&#23450;&#20301;&#21644;&#24809;&#32602;&#24187;&#35273;&#26631;&#35760;&#26469;&#25233;&#21046;&#24187;&#35273;&#29983;&#25104;&#12290;&#26368;&#21021;&#65292;ESREAL&#26681;&#25454;&#29983;&#25104;&#30340;&#26631;&#39064;&#21019;&#24314;&#19968;&#20010;&#37325;&#24314;&#22270;&#20687;&#65292;&#24182;&#23558;&#20854;&#23545;&#24212;&#21306;&#22495;&#19982;&#21407;&#22987;&#22270;&#20687;&#30340;&#21306;&#22495;&#23545;&#40784;&#12290;&#36825;&#31181;&#35821;&#20041;&#37325;&#24314;&#26377;&#21161;&#20110;&#35782;&#21035;&#29983;&#25104;&#26631;&#39064;&#20013;&#30340;&#26631;&#35760;&#32423;&#24187;&#35273;&#30340;&#23384;&#22312;&#21644;&#31867;&#22411;&#12290;&#38543;&#21518;&#65292;ESREAL&#36890;&#36807;&#35780;&#20272;&#23545;&#40784;&#21306;&#22495;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#35745;&#31639;&#26631;&#35760;&#32423;&#24187;&#35273;&#20998;&#25968;&#65292;&#22522;&#20110;&#24187;&#35273;&#30340;&#31867;&#22411;&#12290;&#26368;&#21518;&#65292;ESREAL&#37319;&#29992;&#19968;&#31181;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#36827;&#34892;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16167v1 Announce Type: cross  Abstract: Hallucinations in vision-language models pose a significant challenge to their reliability, particularly in the generation of long captions. Current methods fall short of accurately identifying and mitigating these hallucinations. To address this issue, we introduce ESREAL, a novel unsupervised learning framework designed to suppress the generation of hallucinations through accurate localization and penalization of hallucinated tokens. Initially, ESREAL creates a reconstructed image based on the generated caption and aligns its corresponding regions with those of the original image. This semantic reconstruction aids in identifying both the presence and type of token-level hallucinations within the generated caption. Subsequently, ESREAL computes token-level hallucination scores by assessing the semantic similarity of aligned regions based on the type of hallucination. Finally, ESREAL employs a proximal policy optimization algorithm, wh
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#30340;&#25968;&#25454;&#20013;&#24515;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20869;&#26680;&#20989;&#25968;&#35782;&#21035;&#25552;&#20379;&#26368;&#20339;&#39044;&#27979;&#25903;&#25345;&#32473;&#27979;&#35797;&#28857;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15576</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#30340;&#25968;&#25454;&#20013;&#24515;&#39044;&#27979;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Data-centric Prediction Explanation via Kernelized Stein Discrepancy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15576
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#30340;&#25968;&#25454;&#20013;&#24515;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20869;&#26680;&#20989;&#25968;&#35782;&#21035;&#25552;&#20379;&#26368;&#20339;&#39044;&#27979;&#25903;&#25345;&#32473;&#27979;&#35797;&#28857;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#20110;&#31034;&#20363;&#30340;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#27169;&#22411;&#30340;&#21442;&#25968;&#25110;&#28508;&#22312;&#34920;&#31034;&#26469;&#36830;&#25509;&#27979;&#35797;&#21644;&#35757;&#32451;&#25968;&#25454;&#28857;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#20851;&#27169;&#22411;&#39044;&#27979;&#21407;&#22240;&#30340;&#32447;&#32034;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#34920;&#29616;&#20986;&#22266;&#26377;&#30340;&#32570;&#38519;&#65292;&#27604;&#22914;&#20135;&#29983;&#26174;&#30528;&#30340;&#35745;&#31639;&#24320;&#38144;&#25110;&#29983;&#25104;&#31895;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#31934;&#24230;&#21644;&#25968;&#25454;&#20013;&#24515;&#30340;&#35299;&#37322;&#65288;HD-Explain&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#65288;KSD&#65289;&#23646;&#24615;&#30340;&#31616;&#21333;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;KSD&#21807;&#19968;&#22320;&#20026;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#20869;&#26680;&#20989;&#25968;&#65292;&#29992;&#20110;&#32534;&#30721;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#25968;&#25454;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#20869;&#26680;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#25552;&#20379;&#26368;&#20339;&#39044;&#27979;&#25903;&#25345;&#32473;&#27979;&#35797;&#28857;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20998;&#31867;&#39046;&#22495;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;HD-Explain&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15576v1 Announce Type: new  Abstract: Existing example-based prediction explanation methods often bridge test and training data points through the model's parameters or latent representations. While these methods offer clues to the causes of model predictions, they often exhibit innate shortcomings, such as incurring significant computational overhead or producing coarse-grained explanations. This paper presents a Highly-precise and Data-centric Explanation (HD-Explain), a straightforward prediction explanation method exploiting properties of Kernelized Stein Discrepancy (KSD). Specifically, the KSD uniquely defines a parameterized kernel function for a trained model that encodes model-dependent data correlation. By leveraging the kernel function, one can identify training samples that provide the best predictive support to a test point efficiently. We conducted thorough analyses and experiments across multiple classification domains, where we show that HD-Explain outperform
&lt;/p&gt;</description></item><item><title>Finer&#24037;&#20316;&#25581;&#31034;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#19978;&#30340;&#30701;&#26495;&#65292;&#23588;&#20854;&#26159;&#38590;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#32454;&#33268;&#23646;&#24615;&#35299;&#37322;&#65292;&#23613;&#31649;&#20855;&#26377;&#29983;&#25104;&#39640;&#27700;&#24179;&#22270;&#20687;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16315</link><description>&lt;p&gt;
Finer: &#22312;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30740;&#31350;&#21644;&#22686;&#24378;&#32454;&#31890;&#24230;&#35270;&#35273;&#27010;&#24565;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16315
&lt;/p&gt;
&lt;p&gt;
Finer&#24037;&#20316;&#25581;&#31034;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#19978;&#30340;&#30701;&#26495;&#65292;&#23588;&#20854;&#26159;&#38590;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#32454;&#33268;&#23646;&#24615;&#35299;&#37322;&#65292;&#23613;&#31649;&#20855;&#26377;&#29983;&#25104;&#39640;&#27700;&#24179;&#22270;&#20687;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#27169;&#22411;&#33021;&#22815;&#36731;&#26494;&#29983;&#25104;&#39640;&#27700;&#24179;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#35299;&#37322;&#12290;&#23613;&#31649;&#36825;&#31181;&#33021;&#21147;&#20027;&#35201;&#24402;&#22240;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#21253;&#21547;&#30340;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20845;&#20010;&#19981;&#21516;&#22522;&#20934;&#35774;&#32622;&#19979;&#30340;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#65288;FGVC&#65289;&#19978;&#30340;&#32570;&#38519;&#12290;&#26368;&#36817;&#30340;LVLMs&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#22914;LLaVa-1.5&#65292;InstructBLIP&#21644;GPT-4V&#65292;&#22312;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#20005;&#37325;&#19979;&#38477;&#65292;&#20363;&#22914;&#65292;LLaVA-1.5&#22312;&#26031;&#22374;&#31119;&#29399;&#30340;EM&#24179;&#22343;&#19979;&#38477;&#20102;65.58&#65292;&#32780;&#19988;&#36824;&#38590;&#20197;&#26681;&#25454;&#20986;&#29616;&#22312;&#36755;&#20837;&#22270;&#20687;&#20013;&#30340;&#27010;&#24565;&#29983;&#25104;&#20855;&#26377;&#35814;&#32454;&#23646;&#24615;&#30340;&#20934;&#30830;&#35299;&#37322;&#65292;&#23613;&#31649;&#23427;&#20204;&#26377;&#29983;&#25104;&#25972;&#20307;&#22270;&#20687;&#32423;&#25551;&#36848;&#30340;&#33021;&#21147;&#12290;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#32463;&#36807;&#25351;&#23548;&#35843;&#25972;&#30340;LVLMs&#22312;&#32473;&#23450;&#25991;&#26412;&#26102;&#21576;&#29616;&#20986;&#27169;&#24577;&#24046;&#36317;&#65292;&#26174;&#31034;&#20986;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16315v1 Announce Type: cross  Abstract: Recent advances in instruction-tuned Large Vision-Language Models (LVLMs) have imbued the models with the ability to generate high-level, image-grounded explanations with ease. While such capability is largely attributed to the rich world knowledge contained within the Large Language Models (LLMs), our work reveals their shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings. Most recent state-of-the-art LVLMs like LLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of classification performance, e.g., average drop of 65.58 in EM for Stanford Dogs for LLaVA-1.5, but also struggle to generate an accurate explanation with detailed attributes based on the concept that appears within an input image despite their capability to generate holistic image-level descriptions. In-depth analyses show that instruction-tuned LVLMs exhibit modality gap, showing discrepancy when given tex
&lt;/p&gt;</description></item><item><title>CommVQA&#25968;&#25454;&#38598;&#23558;&#22270;&#20687;&#32622;&#20110;&#33258;&#28982;&#29615;&#22659;&#20013;&#65292;&#25361;&#25112;&#20102;&#24403;&#21069;&#30340;VQA&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#20026;&#27169;&#22411;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15002</link><description>&lt;p&gt;
&#23558;&#35270;&#35273;&#38382;&#31572;&#32622;&#20110;&#20132;&#38469;&#32972;&#26223;&#20013;&#30340;CommVQA
&lt;/p&gt;
&lt;p&gt;
CommVQA: Situating Visual Question Answering in Communicative Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15002
&lt;/p&gt;
&lt;p&gt;
CommVQA&#25968;&#25454;&#38598;&#23558;&#22270;&#20687;&#32622;&#20110;&#33258;&#28982;&#29615;&#22659;&#20013;&#65292;&#25361;&#25112;&#20102;&#24403;&#21069;&#30340;VQA&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#20026;&#27169;&#22411;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#24448;&#24448;&#22312;&#23396;&#31435;&#30340;&#22270;&#20687;-&#38382;&#39064;&#23545;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#25552;&#20986;&#30340;&#38382;&#39064;&#21462;&#20915;&#20110;&#20182;&#20204;&#30340;&#20449;&#24687;&#38656;&#27714;&#21644;&#23545;&#22270;&#20687;&#20869;&#23481;&#30340;&#20808;&#21069;&#20102;&#35299;&#12290;&#20026;&#20102;&#35780;&#20272;&#23558;&#22270;&#20687;&#32622;&#20110;&#33258;&#28982;&#29615;&#22659;&#20013;&#22914;&#20309;&#22609;&#36896;&#35270;&#35273;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CommVQA&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#22270;&#20687;&#12289;&#22270;&#20687;&#25551;&#36848;&#12289;&#22270;&#20687;&#21487;&#33021;&#20986;&#29616;&#30340;&#30495;&#23454;&#20132;&#38469;&#22330;&#26223;&#65288;&#20363;&#22914;&#26053;&#34892;&#32593;&#31449;&#65289;&#20197;&#21450;&#20381;&#36182;&#20110;&#22330;&#26223;&#30340;&#21518;&#32493;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;VQA&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CommVQA&#23545;&#24403;&#21069;&#27169;&#22411;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;VQA&#27169;&#22411;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#21487;&#24191;&#27867;&#25552;&#39640;&#24615;&#33021;&#65292;&#31361;&#26174;&#23558;&#31995;&#32479;&#32622;&#20110;&#20132;&#38469;&#22330;&#26223;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15002v1 Announce Type: new  Abstract: Current visual question answering (VQA) models tend to be trained and evaluated on image-question pairs in isolation. However, the questions people ask are dependent on their informational needs and prior knowledge about the image content. To evaluate how situating images within naturalistic contexts shapes visual questions, we introduce CommVQA, a VQA dataset consisting of images, image descriptions, real-world communicative scenarios where the image might appear (e.g., a travel website), and follow-up questions and answers conditioned on the scenario. We show that CommVQA poses a challenge for current models. Providing contextual information to VQA models improves performance broadly, highlighting the relevance of situating systems within a communicative scenario.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#32771;&#34385;&#29983;&#25104;&#22270;&#20687;&#26159;&#30001;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#26465;&#20214;&#19979;&#65292;&#37327;&#21270;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11789</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#20551;&#35774;&#30340;&#32479;&#35745;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Statistical Test for Generated Hypotheses by Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#32771;&#34385;&#29983;&#25104;&#22270;&#20687;&#26159;&#30001;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#26465;&#20214;&#19979;&#65292;&#37327;&#21270;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#30340;&#22686;&#24378;&#24615;&#33021;&#21152;&#36895;&#20102;&#20854;&#34701;&#20837;&#31185;&#23398;&#30740;&#31350;&#12290;&#29305;&#21035;&#26159;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#21019;&#24314;&#31185;&#23398;&#20551;&#35774;&#26159;&#24456;&#26377;&#21069;&#36884;&#30340;&#65292;&#24182;&#19988;&#27491;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;AI&#29983;&#25104;&#30340;&#20551;&#35774;&#36827;&#34892;&#20851;&#38190;&#20915;&#31574;&#65288;&#22914;&#21307;&#23398;&#35786;&#26029;&#65289;&#26102;&#65292;&#39564;&#35777;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#36827;&#34892;&#21307;&#23398;&#35786;&#26029;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#26469;&#37327;&#21270;&#20854;&#21487;&#38752;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#32479;&#35745;&#26816;&#39564;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#20351;&#29992;&#36873;&#25321;&#24615;&#25512;&#26029;&#26694;&#26550;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#26159;&#30001;&#32463;&#36807;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#36825;&#19968;&#20107;&#23454;&#26465;&#20214;&#19979;&#30340;&#32479;&#35745;&#26816;&#39564;&#12290;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#32467;&#26524;&#30340;&#32479;&#35745;&#21487;&#38752;&#24615;&#21487;&#20197;&#20197;p&#20540;&#30340;&#24418;&#24335;&#37327;&#21270;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#25511;&#21046;&#38169;&#35823;&#29575;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11789v1 Announce Type: cross  Abstract: The enhanced performance of AI has accelerated its integration into scientific research. In particular, the use of generative AI to create scientific hypotheses is promising and is increasingly being applied across various fields. However, when employing AI-generated hypotheses for critical decisions, such as medical diagnoses, verifying their reliability is crucial. In this study, we consider a medical diagnostic task using generated images by diffusion models, and propose a statistical test to quantify its reliability. The basic idea behind the proposed statistical test is to employ a selective inference framework, where we consider a statistical test conditional on the fact that the generated images are produced by a trained diffusion model. Using the proposed method, the statistical reliability of medical image diagnostic results can be quantified in the form of a p-value, allowing for decision-making with a controlled error rate. 
&lt;/p&gt;</description></item><item><title>MITS-GAN&#26159;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#21307;&#23398;&#24433;&#20687;&#20813;&#21463;&#31713;&#25913;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36866;&#24403;&#30340;&#39640;&#26031;&#22122;&#22768;&#20316;&#20026;&#38450;&#25252;&#25514;&#26045;&#65292;&#25171;&#20081;&#25915;&#20987;&#32773;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26550;&#26500;&#30340;&#36755;&#20986;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MITS-GAN&#33021;&#22815;&#29983;&#25104;&#32784;&#31713;&#25913;&#22270;&#20687;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09624</link><description>&lt;p&gt;
MITS-GAN: &#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20445;&#25252;&#21307;&#23398;&#24433;&#20687;&#20813;&#21463;&#31713;&#25913;
&lt;/p&gt;
&lt;p&gt;
MITS-GAN: Safeguarding Medical Imaging from Tampering with Generative Adversarial Networks. (arXiv:2401.09624v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09624
&lt;/p&gt;
&lt;p&gt;
MITS-GAN&#26159;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#21307;&#23398;&#24433;&#20687;&#20813;&#21463;&#31713;&#25913;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36866;&#24403;&#30340;&#39640;&#26031;&#22122;&#22768;&#20316;&#20026;&#38450;&#25252;&#25514;&#26045;&#65292;&#25171;&#20081;&#25915;&#20987;&#32773;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26550;&#26500;&#30340;&#36755;&#20986;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MITS-GAN&#33021;&#22815;&#29983;&#25104;&#32784;&#31713;&#25913;&#22270;&#20687;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#28508;&#22312;&#30340;&#24694;&#24847;&#20351;&#29992;&#30340;&#25285;&#24551;&#65292;&#23588;&#20854;&#26159;&#22312;&#21307;&#23398;&#24433;&#20687;&#31561;&#25935;&#24863;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;MITS-GAN&#65292;&#29992;&#20110;&#38450;&#27490;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#31713;&#25913;&#65292;&#29305;&#21035;&#20851;&#27880;CT&#25195;&#25551;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#19981;&#21487;&#23519;&#35273;&#20294;&#31934;&#30830;&#30340;&#25200;&#21160;&#26469;&#25171;&#20081;&#25915;&#20987;&#32773;&#30340;CT-GAN&#26550;&#26500;&#30340;&#36755;&#20986;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#23558;&#36866;&#24403;&#30340;&#39640;&#26031;&#22122;&#22768;&#24341;&#20837;&#21040;&#36755;&#20837;&#20013;&#20316;&#20026;&#23545;&#21508;&#31181;&#25915;&#20987;&#30340;&#20445;&#25252;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#38450;&#31713;&#25913;&#33021;&#21147;&#65292;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#20248;&#21183;&#12290;&#23545;CT&#25195;&#25551;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MITS-GAN&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24378;&#35843;&#20102;&#20854;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#21487;&#24573;&#30053;&#20266;&#24433;&#30340;&#32784;&#31713;&#25913;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#22270;&#20687;&#31713;&#25913;&#24102;&#26469;&#20102;&#21361;&#21450;&#29983;&#21629;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#30340;&#20027;&#21160;&#38450;&#25252;&#26041;&#27861;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress in generative models, particularly Generative Adversarial Networks (GANs), opened new possibilities for image generation but raised concerns about potential malicious uses, especially in sensitive areas like medical imaging. This study introduces MITS-GAN, a novel approach to prevent tampering in medical images, with a specific focus on CT scans. The approach disrupts the output of the attacker's CT-GAN architecture by introducing imperceptible but yet precise perturbations. Specifically, the proposed approach involves the introduction of appropriate Gaussian noise to the input as a protective measure against various attacks. Our method aims to enhance tamper resistance, comparing favorably to existing techniques. Experimental results on a CT scan dataset demonstrate MITS-GAN's superior performance, emphasizing its ability to generate tamper-resistant images with negligible artifacts. As image tampering in medical domains poses life-threatening risks, our proactive approac
&lt;/p&gt;</description></item><item><title>MosaicFusion&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.13042</link><description>&lt;p&gt;
MosaicFusion: &#23558;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#30340;&#25968;&#25454;&#22686;&#24378;&#22120;
&lt;/p&gt;
&lt;p&gt;
MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation. (arXiv:2309.13042v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13042
&lt;/p&gt;
&lt;p&gt;
MosaicFusion&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MosaicFusion&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#38656;&#35757;&#32451;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26631;&#31614;&#30417;&#30563;&#12290;&#20004;&#20010;&#20851;&#38190;&#35774;&#35745;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#29616;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#26377;&#29992;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#23545;&#35937;&#23454;&#20363;&#21644;&#33945;&#29256;&#27880;&#37322;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#30011;&#24067;&#20998;&#20026;&#20960;&#20010;&#21306;&#22495;&#65292;&#24182;&#25191;&#34892;&#19968;&#36718;&#25193;&#25955;&#36807;&#31243;&#65292;&#21516;&#26102;&#22522;&#20110;&#19981;&#21516;&#30340;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22810;&#20010;&#23454;&#20363;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#21512;&#19982;&#23545;&#35937;&#25552;&#31034;&#30456;&#20851;&#32852;&#30340;&#36328;&#27880;&#24847;&#21147;&#22270;&#22312;&#23618;&#21644;&#25193;&#25955;&#26102;&#38388;&#27493;&#19978;&#65292;&#28982;&#21518;&#36827;&#34892;&#31616;&#21333;&#30340;&#38408;&#20540;&#22788;&#29702;&#21644;&#36793;&#32536;&#24863;&#30693;&#30340;&#32454;&#21270;&#22788;&#29702;&#65292;&#24471;&#21040;&#30456;&#24212;&#30340;&#23454;&#20363;&#33945;&#29256;&#12290;&#25105;&#20204;&#30340;MosaicFusion&#21487;&#20197;&#20026;&#31232;&#32570;&#21644;&#26032;&#39062;&#31867;&#21035;&#20135;&#29983;&#22823;&#37327;&#30340;&#21512;&#25104;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#30340;&#22788;&#29702;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;LVIS&#38271;&#23614;&#21644;&#24320;&#25918;&#35789;&#27719;&#22522;&#20934;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#22343;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MosaicFusion, a simple yet effective diffusion-based data augmentation approach for large vocabulary instance segmentation. Our method is training-free and does not rely on any label supervision. Two key designs enable us to employ an off-the-shelf text-to-image diffusion model as a useful dataset generator for object instances and mask annotations. First, we divide an image canvas into several regions and perform a single round of diffusion process to generate multiple instances simultaneously, conditioning on different text prompts. Second, we obtain corresponding instance masks by aggregating cross-attention maps associated with object prompts across layers and diffusion time steps, followed by simple thresholding and edge-aware refinement processing. Without bells and whistles, our MosaicFusion can produce a significant amount of synthetic labeled data for both rare and novel categories. Experimental results on the challenging LVIS long-tailed and open-vocabulary benchma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#29289;&#29702;&#23398;&#20013;&#30340;&#25955;&#23556;&#35889;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#21508;&#31181;&#22330;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;&#25955;&#23556;&#31995;&#25968;&#30340;&#21327;&#26041;&#24046;&#65292;&#32467;&#21512;&#20102;&#22330;&#30340;&#23567;&#27874;&#20998;&#35299;&#21644;&#28857;&#20301;&#27169;&#65292;&#33021;&#22815;&#20934;&#30830;&#19988;&#31283;&#20581;&#22320;&#37325;&#29616;&#26631;&#20934;&#32479;&#35745;&#37327;&#65292;&#25429;&#25417;&#20102;&#20851;&#38190;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17210</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#30340;&#25955;&#23556;&#35889;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scattering Spectra Models for Physics. (arXiv:2306.17210v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29289;&#29702;&#23398;&#20013;&#30340;&#25955;&#23556;&#35889;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#21508;&#31181;&#22330;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;&#25955;&#23556;&#31995;&#25968;&#30340;&#21327;&#26041;&#24046;&#65292;&#32467;&#21512;&#20102;&#22330;&#30340;&#23567;&#27874;&#20998;&#35299;&#21644;&#28857;&#20301;&#27169;&#65292;&#33021;&#22815;&#20934;&#30830;&#19988;&#31283;&#20581;&#22320;&#37325;&#29616;&#26631;&#20934;&#32479;&#35745;&#37327;&#65292;&#25429;&#25417;&#20102;&#20851;&#38190;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#23398;&#23478;&#24120;&#24120;&#38656;&#35201;&#27010;&#29575;&#27169;&#22411;&#26469;&#36827;&#34892;&#21442;&#25968;&#25512;&#26029;&#25110;&#29983;&#25104;&#19968;&#20010;&#22330;&#30340;&#26032;&#23454;&#29616;&#12290;&#38024;&#23545;&#39640;&#24230;&#38750;&#39640;&#26031;&#22330;&#30340;&#24314;&#31435;&#36825;&#26679;&#30340;&#27169;&#22411;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24403;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25955;&#23556;&#35889;&#27169;&#22411;&#29992;&#20110;&#24179;&#31283;&#22330;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#29289;&#29702;&#23398;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#22330;&#30340;&#20934;&#30830;&#19988;&#31283;&#20581;&#30340;&#32479;&#35745;&#25551;&#36848;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;&#25955;&#23556;&#31995;&#25968;&#30340;&#21327;&#26041;&#24046;&#65292;&#21363;&#22330;&#30340;&#23567;&#27874;&#20998;&#35299;&#21644;&#28857;&#20301;&#27169;&#12290;&#22312;&#20171;&#32461;&#21033;&#29992;&#26059;&#36716;&#21644;&#32553;&#25918;&#19979;&#22330;&#30340;&#35268;&#24459;&#24615;&#36827;&#34892;&#26377;&#29992;&#30340;&#32500;&#24230;&#32422;&#31616;&#21518;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#22810;&#23610;&#24230;&#29289;&#29702;&#22330;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#37325;&#29616;&#26631;&#20934;&#32479;&#35745;&#37327;&#65292;&#21253;&#25324;&#22235;&#38454;&#31354;&#38388;&#30697;&#12290;&#36825;&#20123;&#25955;&#23556;&#35889;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#32500;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#25429;&#25417;&#20102;&#20851;&#38190;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physicists routinely need probabilistic models for a number of tasks such as parameter inference or the generation of new realizations of a field. Establishing such models for highly non-Gaussian fields is a challenge, especially when the number of samples is limited. In this paper, we introduce scattering spectra models for stationary fields and we show that they provide accurate and robust statistical descriptions of a wide range of fields encountered in physics. These models are based on covariances of scattering coefficients, i.e. wavelet decomposition of a field coupled with a point-wise modulus. After introducing useful dimension reductions taking advantage of the regularity of a field under rotation and scaling, we validate these models on various multi-scale physical fields and demonstrate that they reproduce standard statistics, including spatial moments up to 4th order. These scattering spectra provide us with a low-dimensional structured representation that captures key prop
&lt;/p&gt;</description></item></channel></rss>