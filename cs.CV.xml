<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#26354;&#29575;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#20013;&#23884;&#20837;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20197;&#20445;&#25345;&#27169;&#22411;&#39640;&#20934;&#30830;&#24615;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10045</link><description>&lt;p&gt;
&#36890;&#36807;&#26354;&#29575;&#27491;&#21017;&#21270;&#23454;&#29616;&#23545;&#25239;&#40065;&#26834;&#24615;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Towards Adversarially Robust Dataset Distillation by Curvature Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#26354;&#29575;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#20013;&#23884;&#20837;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20197;&#20445;&#25345;&#27169;&#22411;&#39640;&#20934;&#30830;&#24615;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;DD&#65289;&#20801;&#35768;&#23558;&#25968;&#25454;&#38598;&#31934;&#28860;&#20026;&#21407;&#22987;&#22823;&#23567;&#30340;&#20998;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20016;&#23500;&#30340;&#20998;&#24067;&#20449;&#24687;&#65292;&#20351;&#24471;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#33410;&#30465;&#26174;&#33879;&#35745;&#31639;&#36127;&#36733;&#30340;&#21516;&#26102;&#36798;&#21040;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;&#26368;&#36817;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#25552;&#39640;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;DD&#30340;&#19968;&#31181;&#26032;&#35270;&#35282;&#12290;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#20013;&#23884;&#20837;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20197;&#20351;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20445;&#25345;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#33719;&#24471;&#26356;&#22909;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#26354;&#29575;&#27491;&#21017;&#21270;&#32435;&#20837;&#21040;&#31934;&#28860;&#36807;&#31243;&#20013;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26032;&#26041;&#27861;&#65292;&#32780;&#36825;&#31181;&#26041;&#27861;&#30340;&#35745;&#31639;&#24320;&#38144;&#27604;&#26631;&#20934;&#30340;&#23545;&#25239;&#35757;&#32451;&#35201;&#23569;&#24471;&#22810;&#12290;&#22823;&#37327;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#20934;&#30830;&#24615;&#19978;&#20248;&#20110;&#26631;&#20934;&#23545;&#25239;&#35757;&#32451;&#65292;&#21516;&#26102;&#22312;&#23545;&#25239;&#24615;&#33021;&#26041;&#38754;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10045v1 Announce Type: new  Abstract: Dataset distillation (DD) allows datasets to be distilled to fractions of their original size while preserving the rich distributional information so that models trained on the distilled datasets can achieve a comparable accuracy while saving significant computational loads. Recent research in this area has been focusing on improving the accuracy of models trained on distilled datasets. In this paper, we aim to explore a new perspective of DD. We study how to embed adversarial robustness in distilled datasets, so that models trained on these datasets maintain the high accuracy and meanwhile acquire better adversarial robustness. We propose a new method that achieves this goal by incorporating curvature regularization into the distillation process with much less computational overhead than standard adversarial training. Extensive empirical experiments suggest that our method not only outperforms standard adversarial training on both accur
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#34987;&#27745;&#26579;&#26679;&#26412;&#20013;&#27880;&#20837;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#65292;&#24403;&#35302;&#21457;&#26102;&#21487;&#20197;&#25233;&#21046;&#25915;&#20987;&#32773;&#23545;&#27745;&#26579;&#25968;&#25454;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2307.15539</link><description>&lt;p&gt;
&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Backdoor Defense with Non-Adversarial Backdoor. (arXiv:2307.15539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15539
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#34987;&#27745;&#26579;&#26679;&#26412;&#20013;&#27880;&#20837;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#65292;&#24403;&#35302;&#21457;&#26102;&#21487;&#20197;&#25233;&#21046;&#25915;&#20987;&#32773;&#23545;&#27745;&#26579;&#25968;&#25454;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#25915;&#20987;&#24182;&#19981;&#20250;&#24433;&#21709;&#32593;&#32476;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24615;&#33021;&#65292;&#20294;&#19968;&#26086;&#28155;&#21152;&#35302;&#21457;&#27169;&#24335;&#65292;&#23601;&#20250;&#25805;&#32437;&#32593;&#32476;&#34892;&#20026;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#22823;&#22823;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#20294;&#23427;&#20204;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20173;&#28982;&#36828;&#36828;&#33853;&#21518;&#20110;&#24178;&#20928;&#27169;&#22411;&#12290;&#21463;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#38450;&#24481;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#27880;&#20837;&#20102;&#38024;&#23545;&#34987;&#27745;&#26579;&#26679;&#26412;&#30340;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#12290;&#25353;&#29031;&#21518;&#38376;&#25915;&#20987;&#30340;&#19968;&#33324;&#27493;&#39588;&#65292;&#25105;&#20204;&#26816;&#27979;&#19968;&#23567;&#32452;&#21487;&#30097;&#26679;&#26412;&#65292;&#28982;&#21518;&#23545;&#23427;&#20204;&#24212;&#29992;&#27602;&#21270;&#31574;&#30053;&#12290;&#19968;&#26086;&#35302;&#21457;&#65292;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#25233;&#21046;&#20102;&#25915;&#20987;&#32773;&#23545;&#27745;&#26579;&#25968;&#25454;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20294;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;&#38450;&#24481;&#21487;&#20197;&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#26399;&#38388;&#36827;&#34892;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#26631;&#20934;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27969;&#31243;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are vulnerable to backdoor attack, which does not affect the network's performance on clean data but would manipulate the network behavior once a trigger pattern is added. Existing defense methods have greatly reduced attack success rate, but their prediction accuracy on clean data still lags behind a clean model by a large margin. Inspired by the stealthiness and effectiveness of backdoor attack, we propose a simple but highly effective defense framework which injects non-adversarial backdoors targeting poisoned samples. Following the general steps in backdoor attack, we detect a small set of suspected samples and then apply a poisoning strategy to them. The non-adversarial backdoor, once triggered, suppresses the attacker's backdoor on poisoned data, but has limited influence on clean data. The defense can be carried out during data preprocessing, without any modification to the standard end-to-end training pipeline. We conduct extensive experiments on mul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#34987;&#20266;&#36896;&#30340;&#25511;&#21046;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#26041;&#27861;&#65292;&#38024;&#23545;&#36523;&#20221;&#20132;&#25442;&#38382;&#39064;&#35774;&#35745;&#20102;&#26816;&#27979;&#21644;&#20462;&#27491;&#27169;&#22359;&#65292;&#20197;&#21450;&#35299;&#20915;&#22806;&#35266;&#20449;&#24687;&#27169;&#31946;&#21305;&#37197;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#25928;&#26524;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14591</link><description>&lt;p&gt;
&#22522;&#20110;&#26410;&#34987;&#20266;&#36896;&#30340;&#25511;&#21046;&#30340;&#36523;&#20221;&#20132;&#25442;&#26816;&#27979;&#19982;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
The detection and rectification for identity-switch based on unfalsified control. (arXiv:2307.14591v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#34987;&#20266;&#36896;&#30340;&#25511;&#21046;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#26041;&#27861;&#65292;&#38024;&#23545;&#36523;&#20221;&#20132;&#25442;&#38382;&#39064;&#35774;&#35745;&#20102;&#26816;&#27979;&#21644;&#20462;&#27491;&#27169;&#22359;&#65292;&#20197;&#21450;&#35299;&#20915;&#22806;&#35266;&#20449;&#24687;&#27169;&#31946;&#21305;&#37197;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20986;&#33394;&#30340;&#25928;&#26524;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#30446;&#30340;&#26159;&#25345;&#32493;&#36319;&#36394;&#21644;&#35782;&#21035;&#35270;&#39057;&#20013;&#26816;&#27979;&#21040;&#30340;&#29289;&#20307;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#22810;&#30446;&#26631;&#36319;&#36394;&#26041;&#27861;&#37117;&#26159;&#36890;&#36807;&#24314;&#27169;&#36816;&#21160;&#20449;&#24687;&#24182;&#23558;&#20854;&#19982;&#22806;&#35266;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#26469;&#30830;&#23450;&#21644;&#36319;&#36394;&#29289;&#20307;&#12290;&#26412;&#25991;&#37319;&#29992;&#20102;&#26410;&#34987;&#20266;&#36896;&#30340;&#25511;&#21046;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;&#36319;&#36394;&#20013;&#30340;&#36523;&#20221;&#20132;&#25442;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#36319;&#36394;&#36807;&#31243;&#20013;&#24314;&#31435;&#20102;&#22806;&#35266;&#20449;&#24687;&#21464;&#21270;&#30340;&#24207;&#21015;&#65292;&#38024;&#23545;&#36523;&#20221;&#20132;&#25442;&#26816;&#27979;&#21644;&#24674;&#22797;&#35774;&#35745;&#20102;&#19968;&#20010;&#26816;&#27979;&#21644;&#20462;&#27491;&#27169;&#22359;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#25968;&#25454;&#20851;&#32852;&#36807;&#31243;&#20013;&#22806;&#35266;&#20449;&#24687;&#27169;&#31946;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#20844;&#24320;&#21487;&#29992;&#30340;&#22810;&#30446;&#26631;&#36319;&#36394;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#36319;&#36394;&#22120;&#22312;&#22788;&#29702;&#30001;&#36974;&#25377;&#21644;&#24555;&#36895;&#36816;&#21160;&#24341;&#36215;&#30340;&#36319;&#36394;&#38169;&#35823;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#25928;&#26524;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of multi-object tracking (MOT) is to continuously track and identify objects detected in videos. Currently, most methods for multi-object tracking model the motion information and combine it with appearance information to determine and track objects. In this paper, unfalsified control is employed to address the ID-switch problem in multi-object tracking. We establish sequences of appearance information variations for the trajectories during the tracking process and design a detection and rectification module specifically for ID-switch detection and recovery. We also propose a simple and effective strategy to address the issue of ambiguous matching of appearance information during the data association process. Experimental results on publicly available MOT datasets demonstrate that the tracker exhibits excellent effectiveness and robustness in handling tracking errors caused by occlusions and rapid movements.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InceptionNeXt&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#22823;&#20869;&#26680;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16900</link><description>&lt;p&gt;
InceptionNeXt&#65306;&#24403;Inception&#36935;&#21040;ConvNeXt
&lt;/p&gt;
&lt;p&gt;
InceptionNeXt: When Inception Meets ConvNeXt. (arXiv:2303.16900v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InceptionNeXt&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#22823;&#20869;&#26680;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;ViTs&#38271;&#31243;&#24314;&#27169;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#36817;&#26399;&#24191;&#27867;&#30740;&#31350;&#21644;&#37319;&#29992;&#20102;&#22823;&#20869;&#26680;&#21367;&#31215;&#26469;&#25193;&#22823;&#24863;&#21463;&#37326;&#21644;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20363;&#22914;ConvNeXt&#37319;&#29992;&#20102;7x7&#28145;&#24230;&#21367;&#31215;&#12290;&#34429;&#28982;&#36825;&#31181;&#28145;&#24230;&#25805;&#20316;&#20165;&#28040;&#32791;&#23569;&#37327;FLOPs&#65292;&#20294;&#30001;&#20110;&#39640;&#20869;&#23384;&#35775;&#38382;&#25104;&#26412;&#65292;&#36825;&#22312;&#21151;&#33021;&#24378;&#22823;&#30340;&#35745;&#31639;&#35774;&#22791;&#19978;&#22823;&#22823;&#25439;&#23475;&#20102;&#27169;&#22411;&#25928;&#29575;&#12290;&#23613;&#31649;&#32553;&#23567;ConvNeXt&#30340;&#20869;&#26680;&#22823;&#23567;&#33021;&#25552;&#39640;&#36895;&#24230;&#65292;&#20294;&#20250;&#23548;&#33268;&#24615;&#33021;&#26174;&#30528;&#19979;&#38477;&#12290;&#22914;&#20309;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21463;Inceptions&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22823;&#20869;&#26680;&#28145;&#24230;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#65292;&#21363;&#23567;&#26041;&#20869;&#26680;&#12289;&#20004;&#20010;&#27491;&#20132;&#24102;&#20869;&#26680;&#21644;&#19968;&#20010;&#20114;&#34917;&#20869;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the long-range modeling ability of ViTs, large-kernel convolutions are widely studied and adopted recently to enlarge the receptive field and improve model performance, like the remarkable work ConvNeXt which employs 7x7 depthwise convolution. Although such depthwise operator only consumes a few FLOPs, it largely harms the model efficiency on powerful computing devices due to the high memory access costs. For example, ConvNeXt-T has similar FLOPs with ResNet-50 but only achieves 60% throughputs when trained on A100 GPUs with full precision. Although reducing the kernel size of ConvNeXt can improve speed, it results in significant performance degradation. It is still unclear how to speed up large-kernel-based CNN models while preserving their performance. To tackle this issue, inspired by Inceptions, we propose to decompose large-kernel depthwise convolution into four parallel branches along channel dimension, i.e. small square kernel, two orthogonal band kernels, and an ide
&lt;/p&gt;</description></item></channel></rss>