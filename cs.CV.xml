<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36890;&#36807;&#22312;&#31070;&#32463;&#27169;&#22411;&#20013;&#24341;&#20837;&#19981;&#21516;iable&#21644;&#23436;&#20840;&#21367;&#31215;&#30340;&#21069;&#31471;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#36339;&#36291;&#36830;&#25509;&#65292;&#25104;&#21151;&#23454;&#29616;&#23545;&#26799;&#24230;&#25915;&#20987;&#30340;&#26174;&#33879;&#38887;&#24615;&#65292;&#24182;&#36890;&#36807;&#23558;&#27169;&#22411;&#32452;&#21512;&#25104;&#38543;&#26426;&#38598;&#21512;&#65292;&#26377;&#25928;&#23545;&#25239;&#40657;&#30418;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.17018</link><description>&lt;p&gt;
&#36890;&#36807;&#23436;&#20840;&#21367;&#31215;&#21644;&#21487;&#24494;&#30340;&#21069;&#31471;&#19982;&#36339;&#36291;&#36830;&#25509;&#23545;&#26799;&#24230;&#25915;&#20987;&#34920;&#29616;&#20986;&#26174;&#33879;&#38887;&#24615;&#30340;&#32784;&#20154;&#23547;&#21619;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Curious Case of Remarkable Resilience to Gradient Attacks via Fully Convolutional and Differentiable Front End with a Skip Connection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#31070;&#32463;&#27169;&#22411;&#20013;&#24341;&#20837;&#19981;&#21516;iable&#21644;&#23436;&#20840;&#21367;&#31215;&#30340;&#21069;&#31471;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#36339;&#36291;&#36830;&#25509;&#65292;&#25104;&#21151;&#23454;&#29616;&#23545;&#26799;&#24230;&#25915;&#20987;&#30340;&#26174;&#33879;&#38887;&#24615;&#65292;&#24182;&#36890;&#36807;&#23558;&#27169;&#22411;&#32452;&#21512;&#25104;&#38543;&#26426;&#38598;&#21512;&#65292;&#26377;&#25928;&#23545;&#25239;&#40657;&#30418;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27979;&#35797;&#20102;&#36890;&#36807;&#22312;&#19968;&#20010;&#20923;&#32467;&#30340;&#20998;&#31867;&#22120;&#20043;&#21069;&#22686;&#21152;&#19968;&#20010;&#21487;&#24494;&#19988;&#23436;&#20840;&#21367;&#31215;&#30340;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#21069;&#31471;&#22686;&#24378;&#31070;&#32463;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#36739;&#23567;&#30340;&#23398;&#20064;&#29575;&#36827;&#34892;&#22823;&#32422;&#19968;&#20010;epoch&#30340;&#35757;&#32451;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20123;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20445;&#25345;&#39592;&#24178;&#20998;&#31867;&#22120;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23545;&#21253;&#25324;AutoAttack&#36719;&#20214;&#21253;&#20013;&#30340;APGD&#21644;FAB-T&#25915;&#20987;&#22312;&#20869;&#30340;&#26799;&#24230;&#25915;&#20987;&#20855;&#26377;&#24322;&#24120;&#30340;&#25269;&#25239;&#21147;&#65292;&#36825;&#24402;&#22240;&#20110;&#26799;&#24230;&#25513;&#30422;&#12290;&#26799;&#24230;&#25513;&#30422;&#29616;&#35937;&#24182;&#19981;&#26032;&#40092;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#27809;&#26377;&#26799;&#24230;&#30772;&#22351;&#37096;&#20998;&#65288;&#22914;JPEG&#21387;&#32553;&#25110;&#39044;&#35745;&#23548;&#33268;&#26799;&#24230;&#20943;&#23567;&#30340;&#37096;&#20998;&#65289;&#30340;&#23436;&#20840;&#21487;&#24494;&#27169;&#22411;&#26469;&#35828;&#65292;&#25513;&#30422;&#30340;&#31243;&#24230;&#30456;&#24403;&#26174;&#33879;&#12290;&#23613;&#31649;&#40657;&#30418;&#25915;&#20987;&#23545;&#26799;&#24230;&#25513;&#30422;&#21487;&#33021;&#37096;&#20998;&#26377;&#25928;&#65292;&#20294;&#36890;&#36807;&#23558;&#27169;&#22411;&#32452;&#21512;&#25104;&#38543;&#26426;&#38598;&#21512;&#65292;&#21487;&#20197;&#36731;&#26494;&#20987;&#36133;&#23427;&#20204;&#12290;&#25105;&#20204;&#20272;&#35745;&#36825;&#26679;&#30340;&#38598;&#21512;&#22312;CIFAR10&#21644;CIF&#31561;&#19978;&#23454;&#29616;&#20102;&#20960;&#20046;SOTA&#32423;&#21035;&#30340;AutoAttack&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17018v1 Announce Type: cross  Abstract: We tested front-end enhanced neural models where a frozen classifier was prepended by a differentiable and fully convolutional model with a skip connection. By training them using a small learning rate for about one epoch, we obtained models that retained the accuracy of the backbone classifier while being unusually resistant to gradient attacks including APGD and FAB-T attacks from the AutoAttack package, which we attributed to gradient masking. The gradient masking phenomenon is not new, but the degree of masking was quite remarkable for fully differentiable models that did not have gradient-shattering components such as JPEG compression or components that are expected to cause diminishing gradients.   Though black box attacks can be partially effective against gradient masking, they are easily defeated by combining models into randomized ensembles. We estimate that such ensembles achieve near-SOTA AutoAttack accuracy on CIFAR10, CIF
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#27169;&#22411;&#21387;&#32553;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#36731;&#37327;&#32423;&#25945;&#24072;&#27169;&#22411;&#20013;&#25552;&#21462;&#24402;&#32435;&#20559;&#24046;&#65292;&#20351;Vision Transformers (ViTs) &#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#19968;&#32452;&#19981;&#21516;&#26550;&#26500;&#30340;&#25945;&#24072;&#27169;&#22411;&#26469;&#25351;&#23548;&#23398;&#29983;Transformer&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00369</link><description>&lt;p&gt;
&#25552;&#28860;&#24402;&#32435;&#20559;&#24046;&#65306;&#36229;&#36234;&#27169;&#22411;&#21387;&#32553;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Distilling Inductive Bias: Knowledge Distillation Beyond Model Compression. (arXiv:2310.00369v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#27169;&#22411;&#21387;&#32553;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#36731;&#37327;&#32423;&#25945;&#24072;&#27169;&#22411;&#20013;&#25552;&#21462;&#24402;&#32435;&#20559;&#24046;&#65292;&#20351;Vision Transformers (ViTs) &#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#19968;&#32452;&#19981;&#21516;&#26550;&#26500;&#30340;&#25945;&#24072;&#27169;&#22411;&#26469;&#25351;&#23548;&#23398;&#29983;Transformer&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;Vision Transformers (ViTs) &#25552;&#20379;&#20102;&#22312;&#35270;&#35273;&#21644;&#25991;&#26412;&#39046;&#22495;&#20013;&#23454;&#29616;&#32479;&#19968;&#20449;&#24687;&#22788;&#29702;&#30340;&#35825;&#20154;&#21069;&#26223;&#12290;&#20294;&#26159;&#30001;&#20110;ViTs&#32570;&#20047;&#22266;&#26377;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#20351;&#23427;&#20204;&#30340;&#24212;&#29992;&#23454;&#38469;&#21487;&#34892;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#38598;&#25104;&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#36731;&#37327;&#32423;&#30340;&#25945;&#24072;&#27169;&#22411;&#20013;&#25552;&#21462;&#24402;&#32435;&#20559;&#24046;&#12290;&#20197;&#21069;&#30340;&#31995;&#32479;&#20165;&#20381;&#38752;&#22522;&#20110;&#21367;&#31215;&#30340;&#25945;&#23398;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#19968;&#32452;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#20542;&#21521;&#30340;&#36731;&#37327;&#32423;&#25945;&#24072;&#27169;&#22411;&#65288;&#20363;&#22914;&#21367;&#31215;&#21644;&#38750;&#32447;&#24615;&#21367;&#31215;&#65289;&#21516;&#26102;&#29992;&#20110;&#25351;&#23548;&#23398;&#29983;Transformer&#12290;&#30001;&#20110;&#36825;&#20123;&#29420;&#29305;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#25945;&#24072;&#27169;&#22411;&#21487;&#20197;&#20174;&#21508;&#31181;&#23384;&#20648;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#24191;&#27867;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#36824;&#28041;&#21450;&#39044;&#20808;&#35745;&#31639;&#21644;&#23384;&#20648;logits&#65292;&#20174;&#26681;&#26412;&#19978;&#23454;&#29616;&#20102;&#38750;&#24402;&#19968;&#21270;&#30340;&#29366;&#24577;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of computer vision, Vision Transformers (ViTs) offer the tantalizing prospect of unified information processing across visual and textual domains. But due to the lack of inherent inductive biases in ViTs, they require enormous amount of data for training. To make their applications practical, we introduce an innovative ensemble-based distillation approach distilling inductive bias from complementary lightweight teacher models. Prior systems relied solely on convolution-based teaching. However, this method incorporates an ensemble of light teachers with different architectural tendencies, such as convolution and involution, to instruct the student transformer jointly. Because of these unique inductive biases, instructors can accumulate a wide range of knowledge, even from readily identifiable stored datasets, which leads to enhanced student performance. Our proposed framework also involves precomputing and storing logits in advance, essentially the unnormalize
&lt;/p&gt;</description></item></channel></rss>