<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28784;&#24230;&#20849;&#29983;&#30697;&#38453;&#30340;&#22810;&#23610;&#24230;&#32441;&#29702;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26356;&#22909;&#22320;&#25429;&#25417;&#22797;&#26434;&#30340;&#22270;&#20687;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.16640</link><description>&lt;p&gt;
&#20351;&#29992;GAN&#36827;&#34892;CT&#21435;&#22122;&#30340;&#22810;&#23610;&#24230;&#32441;&#29702;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Texture Loss for CT denoising with GANs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16640
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28784;&#24230;&#20849;&#29983;&#30697;&#38453;&#30340;&#22810;&#23610;&#24230;&#32441;&#29702;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26356;&#22909;&#22320;&#25429;&#25417;&#22797;&#26434;&#30340;&#22270;&#20687;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#21435;&#22122;&#24212;&#29992;&#20013;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;GAN&#30340;&#21435;&#22122;&#31639;&#27861;&#20173;&#28982;&#23384;&#22312;&#25429;&#25417;&#22270;&#20687;&#20869;&#22797;&#26434;&#20851;&#31995;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25484;&#25569;&#39640;&#24230;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30340;&#32441;&#29702;&#20851;&#31995;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28784;&#24230;&#20849;&#29983;&#30697;&#38453;&#65288;GLCM&#65289;&#22266;&#26377;&#30340;&#22810;&#23610;&#24230;&#24615;&#36136;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#20998;&#31867;&#21644;&#26816;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#25105;&#20204;&#20551;&#35774;&#23558;&#20854;&#20449;&#24687;&#20869;&#23481;&#25972;&#21512;&#21040;GANs&#30340;&#35757;&#32451;&#20013;&#20250;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#20248;&#21270;&#30340;GLCM&#30340;&#21487;&#24494;&#20998;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16640v1 Announce Type: cross  Abstract: Generative Adversarial Networks (GANs) have proved as a powerful framework for denoising applications in medical imaging. However, GAN-based denoising algorithms still suffer from limitations in capturing complex relationships within the images. In this regard, the loss function plays a crucial role in guiding the image generation process, encompassing how much a synthetic image differs from a real image. To grasp highly complex and non-linear textural relationships in the training process, this work presents a loss function that leverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence Matrix (GLCM). Although the recent advances in deep learning have demonstrated superior performance in classification and detection tasks, we hypothesize that its information content can be valuable when integrated into GANs' training. To this end, we propose a differentiable implementation of the GLCM suited for gradient-based optimiza
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#23545;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02302</link><description>&lt;p&gt;
&#36229;&#36234;&#19987;&#19994;&#21270;&#65306;&#35780;&#20272;MLLMs&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#23545;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21464;&#24471;&#24322;&#24120;&#27969;&#34892;&#12290;&#20687;ChatGPT-4V&#21644;Gemini&#36825;&#26679;&#21151;&#33021;&#24378;&#22823;&#30340;&#21830;&#29992;&#27169;&#22411;&#65292;&#20197;&#21450;&#20687;LLaVA&#36825;&#26679;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#26412;&#36136;&#19978;&#37117;&#26159;&#36890;&#29992;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#21508;&#26679;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#22914;&#27492;&#24378;&#22823;&#30340;&#36890;&#29992;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#33267;&#20110;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#22788;&#29702;&#29978;&#33267;&#26410;&#32463;&#19987;&#38376;&#35757;&#32451;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#36804;&#20170;&#20026;&#27490;&#26368;&#24378;&#22823;&#30340;MLLMs&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;ShareGPT4V&#12289;ChatGPT&#12289;LLaVA-Next &#36827;&#34892;&#20102;&#19987;&#38376;&#20219;&#21153;&#30340;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#65292;&#19982;&#25105;&#20204;&#30340;&#26368;&#26032;&#19987;&#19994;&#21270;&#27169;&#22411;MiVOLO&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#26356;&#26032;&#20102;MiVOLO&#65292;&#24182;&#22312;&#26412;&#25991;&#20013;&#25552;&#20379;&#20102;&#35814;&#32454;&#20449;&#24687;&#21644;&#26032;&#30340;&#25351;&#26631;&#12290;&#36825;&#31181;&#27604;&#36739;&#20135;&#29983;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#32467;&#26524;&#21644;&#20851;&#20110;&#21442;&#19982;&#27169;&#22411;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02302v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) have recently gained immense popularity. Powerful commercial models like ChatGPT-4V and Gemini, as well as open-source ones such as LLaVA, are essentially general-purpose models and are applied to solve a wide variety of tasks, including those in computer vision. These neural networks possess such strong general knowledge and reasoning abilities that they have proven capable of working even on tasks for which they were not specifically trained. We compared the capabilities of the most powerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized task of age and gender estimation with our state-of-the-art specialized model, MiVOLO. We also updated MiVOLO and provide details and new metrics in this article. This comparison has yielded some interesting results and insights about the strengths and weaknesses of the participating models. Furthermore, we attempted various ways to fine-tune 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;TSformer-VO&#26041;&#27861;&#65292;&#23558;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#20316;&#20026;&#19968;&#39033;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#24182;&#36890;&#36807;&#26102;&#31354;&#33258;&#27880;&#24847;&#26426;&#21046;&#20174;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#20272;&#35745;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06121</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#65306;&#19968;&#31181;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transformer-based model for monocular visual odometry: a video understanding approach. (arXiv:2305.06121v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;TSformer-VO&#26041;&#27861;&#65292;&#23558;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#20316;&#20026;&#19968;&#39033;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#24182;&#36890;&#36807;&#26102;&#31354;&#33258;&#27880;&#24847;&#26426;&#21046;&#20174;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#20272;&#35745;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31227;&#21160;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#36710;&#36742;&#20013;&#65292;&#32473;&#23450;&#21333;&#20010;&#25668;&#20687;&#26426;&#22270;&#20687;&#20272;&#35745;&#25668;&#20687;&#26426;&#23039;&#21183;&#26159;&#19968;&#39033;&#20256;&#32479;&#20219;&#21153;&#12290;&#36825;&#20010;&#38382;&#39064;&#31216;&#20026;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#38656;&#35201;&#38024;&#23545;&#29305;&#23450;&#22330;&#26223;&#36827;&#34892;&#24037;&#31243;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#12290;&#32463;&#36807;&#36866;&#24403;&#35757;&#32451;&#21644;&#36275;&#22815;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#12290;Transformer&#26550;&#26500;&#24050;&#32479;&#27835;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#26368;&#21069;&#27839;&#65292;&#20363;&#22914;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#12290;&#26412;&#25991;&#23558;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#20316;&#20026;&#19968;&#39033;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#36827;&#34892;&#22788;&#29702;&#65292;&#20197;&#20272;&#35745;6-DoF&#25668;&#20687;&#26426;&#30340;&#23039;&#21183;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#31354;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;TSformer-VO&#27169;&#22411;&#65292;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20174;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#20272;&#35745;&#36816;&#21160;&#65292;&#19982;&#20960;&#20309;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the camera pose given images of a single camera is a traditional task in mobile robots and autonomous vehicles. This problem is called monocular visual odometry and it often relies on geometric approaches that require engineering effort for a specific scenario. Deep learning methods have shown to be generalizable after proper training and a considerable amount of available data. Transformer-based architectures have dominated the state-of-the-art in natural language processing and computer vision tasks, such as image and video understanding. In this work, we deal with the monocular visual odometry as a video understanding task to estimate the 6-DoF camera's pose. We contribute by presenting the TSformer-VO model based on spatio-temporal self-attention mechanisms to extract features from clips and estimate the motions in an end-to-end manner. Our approach achieved competitive state-of-the-art performance compared with geometry-based and deep learning-based methods on the KITTI
&lt;/p&gt;</description></item></channel></rss>