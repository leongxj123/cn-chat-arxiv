<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#21516;&#26102;&#36873;&#25321;&#34701;&#21512;&#31574;&#30053;&#21644;&#32534;&#30721;&#22120;&#26550;&#26500;&#23545;&#20316;&#29289;&#20998;&#31867;&#20855;&#26377;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.16582</link><description>&lt;p&gt;
&#22312;&#21033;&#29992;&#20840;&#29699;&#36965;&#24863;&#25968;&#25454;&#36827;&#34892;&#20316;&#29289;&#20998;&#31867;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#20339;&#36873;&#25321;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
In the Search for Optimal Multi-view Learning Models for Crop Classification with Global Remote Sensing Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16582
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#21516;&#26102;&#36873;&#25321;&#34701;&#21512;&#31574;&#30053;&#21644;&#32534;&#30721;&#22120;&#26550;&#26500;&#23545;&#20316;&#29289;&#20998;&#31867;&#20855;&#26377;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#29289;&#20998;&#31867;&#22312;&#30740;&#31350;&#20316;&#29289;&#27169;&#24335;&#21464;&#21270;&#12289;&#36164;&#28304;&#31649;&#29702;&#21644;&#30899;&#22266;&#23384;&#20013;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#36827;&#34892;&#39044;&#27979;&#26102;&#65292;&#21033;&#29992;&#21508;&#31181;&#26102;&#38388;&#25968;&#25454;&#28304;&#26159;&#24517;&#35201;&#30340;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#23545;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26144;&#23556;&#21040;&#39640;&#32423;&#34920;&#31034;&#20197;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22810;&#20010;&#36755;&#20837;&#27169;&#24335;&#26102;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#25991;&#29486;&#23545;&#22810;&#35270;&#22270;&#23398;&#20064;&#65288;MVL&#65289;&#22330;&#26223;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#25351;&#23548;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#25506;&#32034;&#20855;&#26377;&#29305;&#23450;&#32534;&#30721;&#22120;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#24182;&#22312;&#23616;&#37096;&#22320;&#21306;&#23545;&#20854;&#36827;&#34892;&#39564;&#35777;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#23545;&#20892;&#30000;&#22303;&#22320;&#21644;&#20316;&#29289;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#26102;&#21516;&#26102;&#36873;&#25321;&#34701;&#21512;&#31574;&#30053;&#21644;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16582v1 Announce Type: cross  Abstract: Crop classification is of critical importance due to its role in studying crop pattern changes, resource management, and carbon sequestration. When employing data-driven techniques for its prediction, utilizing various temporal data sources is necessary. Deep learning models have proven to be effective for this task by mapping time series data to high-level representation for prediction. However, they face substantial challenges when dealing with multiple input patterns. The literature offers limited guidance for Multi-View Learning (MVL) scenarios, as it has primarily focused on exploring fusion strategies with specific encoders and validating them in local regions. In contrast, we investigate the impact of simultaneous selection of the fusion strategy and the encoder architecture evaluated on a global-scale cropland and crop-type classifications. We use a range of five fusion strategies (Input, Feature, Decision, Ensemble, Hybrid) an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#38271;&#35270;&#39057;&#24207;&#21015;&#26102;&#27880;&#24847;&#21147;&#23618;&#20869;&#23384;&#28040;&#32791;&#22686;&#38271;&#24555;&#12289;&#38480;&#21046;&#36739;&#22823;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.07711</link><description>&lt;p&gt;
SSM&#36935;&#19978;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;: &#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#19979;&#30340;&#39640;&#25928;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SSM Meets Video Diffusion Models: Efficient Video Generation with Structured State Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07711
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#38271;&#35270;&#39057;&#24207;&#21015;&#26102;&#27880;&#24847;&#21147;&#23618;&#20869;&#23384;&#28040;&#32791;&#22686;&#38271;&#24555;&#12289;&#38480;&#21046;&#36739;&#22823;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22270;&#20687;&#29983;&#25104;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#30340;&#26174;&#33879;&#25104;&#23601;&#65292;&#30740;&#31350;&#30028;&#23545;&#23558;&#36825;&#20123;&#27169;&#22411;&#25193;&#23637;&#21040;&#35270;&#39057;&#29983;&#25104;&#34920;&#29616;&#20986;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#12290;&#26368;&#36817;&#29992;&#20110;&#35270;&#39057;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20027;&#35201;&#21033;&#29992;&#27880;&#24847;&#21147;&#23618;&#26469;&#25552;&#21462;&#26102;&#38388;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#24847;&#21147;&#23618;&#30340;&#20869;&#23384;&#28040;&#32791;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#20108;&#27425;&#22686;&#38271;&#65292;&#36825;&#31181;&#38480;&#21046;&#22312;&#23581;&#35797;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26356;&#38271;&#35270;&#39057;&#24207;&#21015;&#26102;&#20250;&#24102;&#26469;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#12290;&#30001;&#20110;&#30456;&#23545;&#20110;&#24207;&#21015;&#38271;&#24230;&#65292;SSMs&#20855;&#26377;&#32447;&#24615;&#20869;&#23384;&#28040;&#32791;&#65292;&#26368;&#36817;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20351;&#29992;UCF101&#36825;&#19968;&#35270;&#39057;&#29983;&#25104;&#30340;&#26631;&#20934;&#22522;&#20934;&#26469;&#35780;&#20272;&#25105;&#20204;&#22522;&#20110;SSM&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20026;&#25506;&#35752;SSMs&#22312;&#26356;&#38271;&#35270;&#39057;&#29983;&#25104;&#20013;&#30340;&#28508;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07711v1 Announce Type: cross  Abstract: Given the remarkable achievements in image generation through diffusion models, the research community has shown increasing interest in extending these models to video generation. Recent diffusion models for video generation have predominantly utilized attention layers to extract temporal features. However, attention layers are limited by their memory consumption, which increases quadratically with the length of the sequence. This limitation presents significant challenges when attempting to generate longer video sequences using diffusion models. To overcome this challenge, we propose leveraging state-space models (SSMs). SSMs have recently gained attention as viable alternatives due to their linear memory consumption relative to sequence length. In the experiments, we first evaluate our SSM-based model with UCF101, a standard benchmark of video generation. In addition, to investigate the potential of SSMs for longer video generation, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24320;&#25918;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#65292;&#22312;&#22810;&#31181;&#22320;&#24418;&#21644;&#20256;&#24863;&#22120;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#28155;&#21152;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#65292;&#24182;&#25253;&#21578;&#20912;&#24029;&#33539;&#22260;&#30340;&#26657;&#20934;&#32622;&#20449;&#24230;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15113</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24320;&#25918;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#23454;&#29616;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;
&lt;/p&gt;
&lt;p&gt;
Towards Global Glacier Mapping with Deep Learning and Open Earth Observation Data. (arXiv:2401.15113v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24320;&#25918;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#65292;&#22312;&#22810;&#31181;&#22320;&#24418;&#21644;&#20256;&#24863;&#22120;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#28155;&#21152;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#65292;&#24182;&#25253;&#21578;&#20912;&#24029;&#33539;&#22260;&#30340;&#26657;&#20934;&#32622;&#20449;&#24230;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#23545;&#20110;&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20010;&#36807;&#31243;&#21463;&#21040;&#20912;&#24029;&#22810;&#26679;&#24615;&#12289;&#38590;&#20197;&#20998;&#31867;&#30340;&#30862;&#30707;&#21644;&#22823;&#25968;&#25454;&#22788;&#29702;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Glacier-VisionTransformer-U-Net (GlaViTU)&#65292;&#19968;&#20010;&#21367;&#31215;-Transformer&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20116;&#31181;&#21033;&#29992;&#24320;&#25918;&#21355;&#26143;&#24433;&#20687;&#36827;&#34892;&#22810;&#26102;&#30456;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#30340;&#31574;&#30053;&#12290;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#36328;&#20256;&#24863;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31574;&#30053;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;IoU&#65288;&#20132;&#24182;&#27604;&#65289;&gt; 0.85&#65292;&#24182;&#19988;&#22312;&#20197;&#20912;&#38634;&#20026;&#20027;&#30340;&#22320;&#21306;&#22686;&#21152;&#21040;&#20102;&gt; 0.90&#65292;&#32780;&#22312;&#39640;&#23665;&#20122;&#27954;&#31561;&#30862;&#30707;&#20016;&#23500;&#30340;&#21306;&#22495;&#21017;&#38477;&#33267;&gt; 0.75&#12290;&#27492;&#22806;&#65292;&#28155;&#21152;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#65292;&#21363;&#22238;&#27874;&#21644;&#24178;&#28041;&#30456;&#24178;&#24230;&#65292;&#21487;&#20197;&#25552;&#39640;&#25152;&#26377;&#21487;&#29992;&#22320;&#21306;&#30340;&#20934;&#30830;&#24615;&#12290;&#25253;&#21578;&#20912;&#24029;&#33539;&#22260;&#30340;&#26657;&#20934;&#32622;&#20449;&#24230;&#20351;&#39044;&#27979;&#26356;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate global glacier mapping is critical for understanding climate change impacts. It is challenged by glacier diversity, difficult-to-classify debris and big data processing. Here we propose Glacier-VisionTransformer-U-Net (GlaViTU), a convolutional-transformer deep learning model, and five strategies for multitemporal global-scale glacier mapping using open satellite imagery. Assessing the spatial, temporal and cross-sensor generalisation shows that our best strategy achieves intersection over union &gt;0.85 on previously unobserved images in most cases, which drops to &gt;0.75 for debris-rich areas such as High-Mountain Asia and increases to &gt;0.90 for regions dominated by clean ice. Additionally, adding synthetic aperture radar data, namely, backscatter and interferometric coherence, increases the accuracy in all regions where available. The calibrated confidence for glacier extents is reported making the predictions more reliable and interpretable. We also release a benchmark dataset 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20223;&#21046;&#35895;&#27468;&#30524;&#21160;&#35770;&#25991;&#30340;&#24320;&#28304;&#23454;&#29616;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#23454;&#29616;&#19982;&#35895;&#27468;&#35770;&#25991;&#30456;&#24403;&#30340;&#20934;&#30830;&#30524;&#21160;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.13495</link><description>&lt;p&gt;
&#24320;&#25918;&#27880;&#35270;&#65306;&#19968;&#20010;&#20223;&#21046;&#35895;&#27468;&#30524;&#21160;&#35770;&#25991;&#30340;&#24320;&#28304;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Open Gaze: An Open-Source Implementation Replicating Google's Eye Tracking Paper. (arXiv:2308.13495v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20223;&#21046;&#35895;&#27468;&#30524;&#21160;&#35770;&#25991;&#30340;&#24320;&#28304;&#23454;&#29616;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#23454;&#29616;&#19982;&#35895;&#27468;&#35770;&#25991;&#30456;&#24403;&#30340;&#20934;&#30830;&#30524;&#21160;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30524;&#21160;&#24050;&#32463;&#25104;&#20026;&#35270;&#35273;&#30740;&#31350;&#12289;&#35821;&#35328;&#20998;&#26512;&#21644;&#21487;&#29992;&#24615;&#35780;&#20272;&#31561;&#19981;&#21516;&#39046;&#22495;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;&#19987;&#38376;&#30340;&#12289;&#26114;&#36149;&#30340;&#30524;&#21160;&#36861;&#36394;&#30828;&#20214;&#30340;&#25193;&#23637;&#24335;&#26700;&#38754;&#26174;&#31034;&#22120;&#19978;&#12290;&#23613;&#31649;&#26234;&#33021;&#25163;&#26426;&#30340;&#26222;&#21450;&#29575;&#21644;&#20351;&#29992;&#39057;&#29575;&#24456;&#39640;&#65292;&#20294;&#23545;&#20110;&#26234;&#33021;&#25163;&#26426;&#19978;&#30340;&#30524;&#29699;&#31227;&#21160;&#27169;&#24335;&#21364;&#40092;&#26377;&#35265;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#24320;&#28304;&#27880;&#35270;&#36861;&#36394;&#23454;&#29616;&#65292;&#27169;&#25311;&#20102;&#35895;&#27468;&#35770;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#35770;&#65288;&#20854;&#28304;&#20195;&#30721;&#20173;&#28982;&#26159;&#19987;&#26377;&#30340;&#65289;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#30828;&#20214;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#19982;&#35895;&#27468;&#35770;&#25991;&#26041;&#27861;&#30456;&#24403;&#30340;&#20934;&#30830;&#24230;&#12290;&#36890;&#36807;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#31181;&#26412;&#22320;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#20934;&#30830;&#30524;&#21160;&#36861;&#36394;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#31227;&#21160;&#30524;&#21160;&#36861;&#36394;&#22120;&#30456;&#24403;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eye tracking has been a pivotal tool in diverse fields such as vision research, language analysis, and usability assessment. The majority of prior investigations, however, have concentrated on expansive desktop displays employing specialized, costly eye tracking hardware that lacks scalability. Remarkably little insight exists into ocular movement patterns on smartphones, despite their widespread adoption and significant usage. In this manuscript, we present an open-source implementation of a smartphone-based gaze tracker that emulates the methodology proposed by a GooglePaper (whose source code remains proprietary). Our focus is on attaining accuracy comparable to that attained through the GooglePaper's methodology, without the necessity for supplementary hardware. Through the integration of machine learning techniques, we unveil an accurate eye tracking solution that is native to smartphones. Our approach demonstrates precision akin to the state-of-the-art mobile eye trackers, which 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#40657;&#30418;&#35299;&#37322;&#26041;&#27861;&#8212;&#8212;BODEM&#65292;&#23427;&#37319;&#29992;&#20102;&#23616;&#37096;&#21644;&#36828;&#31243;&#25513;&#34109;&#29983;&#25104;&#22810;&#20010;&#29256;&#26412;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#20174;&#32780;&#27604;&#30446;&#21069;&#29992;&#20110;&#35299;&#37322;&#23545;&#35937;&#26816;&#27979;&#30340;&#20854;&#20182;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#35814;&#32454;&#21644;&#26377;&#29992;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.17249</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#35937;&#26816;&#27979;&#30340;&#27169;&#22411;&#26080;&#20851;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Model-agnostic explainable artificial intelligence for object detection in image data. (arXiv:2303.17249v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#40657;&#30418;&#35299;&#37322;&#26041;&#27861;&#8212;&#8212;BODEM&#65292;&#23427;&#37319;&#29992;&#20102;&#23616;&#37096;&#21644;&#36828;&#31243;&#25513;&#34109;&#29983;&#25104;&#22810;&#20010;&#29256;&#26412;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#20174;&#32780;&#27604;&#30446;&#21069;&#29992;&#20110;&#35299;&#37322;&#23545;&#35937;&#26816;&#27979;&#30340;&#20854;&#20182;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#35814;&#32454;&#21644;&#26377;&#29992;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#26816;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#36890;&#36807;&#24320;&#21457;&#22823;&#22411;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#36879;&#26126;&#24230;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21487;&#33021;&#22952;&#30861;&#36825;&#20123;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#20854;&#20013;&#24320;&#21457;&#26041;&#27861;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#30340;&#34892;&#20026;&#12289;&#20915;&#31574;&#36923;&#36753;&#21644;&#28431;&#27934;&#12290;&#26412;&#25991;&#20026;&#20102;&#35299;&#37322;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#23545;&#35937;&#26816;&#27979;&#31995;&#32479;&#35774;&#35745;&#21644;&#23454;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;Black-box Object Detection Explanation by Masking&#65288;BODEM&#65289;&#30340;&#40657;&#30418;&#35828;&#26126;&#26041;&#27861;&#65292;&#37319;&#29992;&#26032;&#30340;&#25513;&#34109;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23616;&#37096;&#21644;&#36828;&#31243;&#25513;&#34109;&#26469;&#29983;&#25104;&#36755;&#20837;&#22270;&#20687;&#30340;&#22810;&#20010;&#29256;&#26412;&#12290;&#23616;&#37096;&#25513;&#34109;&#29992;&#20110;&#24178;&#25200;&#30446;&#26631;&#23545;&#35937;&#20869;&#30340;&#20687;&#32032;&#65292;&#20197;&#20102;&#35299;&#23545;&#35937;&#26816;&#27979;&#22120;&#23545;&#36825;&#20123;&#21464;&#21270;&#30340;&#21453;&#24212;&#65292;&#32780;&#36828;&#31243;&#25513;&#34109;&#21017;&#29992;&#20110;&#30740;&#31350;&#23545;&#35937;&#26816;&#27979;&#22120;&#22312;&#22270;&#20687;&#32972;&#26223;&#19978;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29992;&#20110;&#35299;&#37322;&#23545;&#35937;&#26816;&#27979;&#30340;&#20854;&#20182;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;BODEM&#25552;&#20379;&#20102;&#26356;&#35814;&#32454;&#21644;&#26377;&#29992;&#30340;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object detection is a fundamental task in computer vision, which has been greatly progressed through developing large and intricate deep learning models. However, the lack of transparency is a big challenge that may not allow the widespread adoption of these models. Explainable artificial intelligence is a field of research where methods are developed to help users understand the behavior, decision logics, and vulnerabilities of AI-based systems. Black-box explanation refers to explaining decisions of an AI system without having access to its internals. In this paper, we design and implement a black-box explanation method named Black-box Object Detection Explanation by Masking (BODEM) through adopting a new masking approach for AI-based object detection systems. We propose local and distant masking to generate multiple versions of an input image. Local masks are used to disturb pixels within a target object to figure out how the object detector reacts to these changes, while distant ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FaceRNET&#30340;&#38754;&#37096;&#34920;&#24773;&#24378;&#24230;&#20272;&#35745;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#37319;&#29992;&#20102;&#34920;&#31034;&#25552;&#21462;&#22120;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#32452;&#21512;&#65292;&#33021;&#22815;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#21508;&#31181;&#24773;&#24863;&#25551;&#36848;&#31526;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#36335;&#30001;&#22788;&#29702;&#19981;&#21516;&#38271;&#24230;&#30340;&#36755;&#20837;&#35270;&#39057;&#12290;&#22312;Hume-Reaction&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.00180</link><description>&lt;p&gt;
FaceRNET: &#19968;&#31181;&#38754;&#37096;&#34920;&#24773;&#24378;&#24230;&#20272;&#35745;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
FaceRNET: a Facial Expression Intensity Estimation Network. (arXiv:2303.00180v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FaceRNET&#30340;&#38754;&#37096;&#34920;&#24773;&#24378;&#24230;&#20272;&#35745;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#37319;&#29992;&#20102;&#34920;&#31034;&#25552;&#21462;&#22120;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#32452;&#21512;&#65292;&#33021;&#22815;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#21508;&#31181;&#24773;&#24863;&#25551;&#36848;&#31526;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#36335;&#30001;&#22788;&#29702;&#19981;&#21516;&#38271;&#24230;&#30340;&#36755;&#20837;&#35270;&#39057;&#12290;&#22312;Hume-Reaction&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#35270;&#39057;&#20013;&#36827;&#34892;&#38754;&#37096;&#34920;&#24773;&#24378;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;&#65306;i) &#19968;&#20010;&#34920;&#31034;&#25552;&#21462;&#22120;&#32593;&#32476;&#65292;&#20174;&#27599;&#20010;&#35270;&#39057;&#24103;&#20013;&#25552;&#21462;&#21508;&#31181;&#24773;&#24863;&#25551;&#36848;&#31526;&#65288;&#20215;&#20540;-&#21796;&#37266;&#12289;&#21160;&#20316;&#21333;&#20803;&#21644;&#22522;&#26412;&#34920;&#24773;&#65289;&#65307;ii) &#19968;&#20010;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#65292;&#28982;&#21518;&#26159;&#19968;&#20010;&#25513;&#30721;&#23618;&#65292;&#36890;&#36807;&#21160;&#24577;&#36335;&#30001;&#23454;&#29616;&#23545;&#19981;&#21516;&#36755;&#20837;&#35270;&#39057;&#38271;&#24230;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#22312;Hume-Reaction&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our approach for Facial Expression Intensity Estimation from videos. It includes two components: i) a representation extractor network that extracts various emotion descriptors (valence-arousal, action units and basic expressions) from each videoframe; ii) a RNN that captures temporal information in the data, followed by a mask layer which enables handling varying input video lengths through dynamic routing. This approach has been tested on the Hume-Reaction dataset yielding excellent results.
&lt;/p&gt;</description></item></channel></rss>