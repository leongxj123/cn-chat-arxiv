<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#20197;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#32593;&#26684;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#24179;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36817;&#20284;&#20132;&#28857;&#30340;&#26041;&#27861;&#65292;&#25299;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.10403</link><description>&lt;p&gt;
&#20174;&#20998;&#27573;&#19977;&#32447;&#24615;&#32593;&#32476;&#20013;&#23548;&#20986;&#22810;&#38754;&#20307;&#22797;&#21512;&#20307;
&lt;/p&gt;
&lt;p&gt;
Polyhedral Complex Derivation from Piecewise Trilinear Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#32593;&#26684;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#24179;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36817;&#20284;&#20132;&#28857;&#30340;&#26041;&#27861;&#65292;&#25299;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#35270;&#21270;&#30340;&#36827;&#23637;&#25581;&#31034;&#20102;&#23427;&#20204;&#32467;&#26500;&#30340;&#35265;&#35299;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#36830;&#32493;&#20998;&#27573;&#20223;&#23556;&#65288;CPWA&#65289;&#20989;&#25968;&#20013;&#25552;&#21462;&#32593;&#26684;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31070;&#32463;&#34920;&#38754;&#34920;&#31034;&#23398;&#20064;&#30340;&#21457;&#23637;&#21253;&#25324;&#38750;&#32447;&#24615;&#20301;&#32622;&#32534;&#30721;&#65292;&#35299;&#20915;&#20102;&#35832;&#22914;&#35889;&#20559;&#24046;&#20043;&#31867;&#30340;&#38382;&#39064;&#65307;&#28982;&#32780;&#65292;&#36825;&#22312;&#24212;&#29992;&#22522;&#20110;CPWA&#20989;&#25968;&#30340;&#32593;&#26684;&#25552;&#21462;&#25216;&#26415;&#26041;&#38754;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#30340;&#32593;&#26684;&#25552;&#21462;&#65292;&#23637;&#31034;&#20102;&#22312;&#22855;&#25343;&#23572;&#32422;&#26463;&#19979;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#19977;&#32447;&#24615;&#21306;&#22495;&#20869;&#30340;&#24179;&#38754;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36817;&#20284;&#19977;&#20010;&#39640;&#32500;&#26354;&#38754;&#20043;&#38388;&#30340;&#20132;&#28857;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#27721;&#26126;&#36317;&#31163;&#21644;&#25928;&#29575;&#20197;&#21450;&#35282;&#36317;&#31163;&#26469;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#27491;&#30830;&#24615;&#21644;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#26816;&#26597;&#20102;t&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10403v1 Announce Type: cross  Abstract: Recent advancements in visualizing deep neural networks provide insights into their structures and mesh extraction from Continuous Piecewise Affine (CPWA) functions. Meanwhile, developments in neural surface representation learning incorporate non-linear positional encoding, addressing issues like spectral bias; however, this poses challenges in applying mesh extraction techniques based on CPWA functions. Focusing on trilinear interpolating methods as positional encoding, we present theoretical insights and an analytical mesh extraction, showing the transformation of hypersurfaces to flat planes within the trilinear region under the eikonal constraint. Moreover, we introduce a method for approximating intersecting points among three hypersurfaces contributing to broader applications. We empirically validate correctness and parsimony through chamfer distance and efficiency, and angular distance, while examining the correlation between t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#24320;&#25918;&#35789;&#27719;&#35774;&#32622;&#20013;&#35299;&#20915;&#22270;&#20687;&#23383;&#24149;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MOCHa&#26469;&#32531;&#35299;&#24187;&#35273;</title><link>https://arxiv.org/abs/2312.03631</link><description>&lt;p&gt;
&#32531;&#35299;&#24320;&#25918;&#35789;&#27719;&#25551;&#36848;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Mitigating Open-Vocabulary Caption Hallucinations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#24320;&#25918;&#35789;&#27719;&#35774;&#32622;&#20013;&#35299;&#20915;&#22270;&#20687;&#23383;&#24149;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MOCHa&#26469;&#32531;&#35299;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#26465;&#20214;&#30340;&#25991;&#26412;&#29983;&#25104;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#22270;&#20687;&#23383;&#24149;&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#19982;&#32473;&#23450;&#22270;&#20687;&#26080;&#27861;&#25512;&#26029;&#30340;&#34394;&#20551;&#32454;&#33410;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#22270;&#20687;&#23383;&#24149;&#20013;&#22823;&#22810;&#20351;&#29992;&#23553;&#38381;&#35789;&#27719;&#23545;&#35937;&#21015;&#34920;&#26469;&#32531;&#35299;&#25110;&#35780;&#20272;&#24187;&#35273;&#65292;&#24573;&#30053;&#20102;&#23454;&#36341;&#20013;&#21457;&#29983;&#30340;&#22823;&#22810;&#25968;&#24187;&#35273;&#31867;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#24212;&#23545;&#24320;&#25918;&#35789;&#27719;&#35774;&#32622;&#20013;&#22270;&#20687;&#23383;&#24149;&#20013;&#30340;&#24187;&#35273;&#65292;&#21253;&#25324;&#37327;&#21270;&#23427;&#20204;&#30340;&#23384;&#22312;&#24182;&#20248;&#21270;&#20197;&#20943;&#36731;&#36825;&#31181;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;OpenCHAIR&#22522;&#20934;&#21033;&#29992;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#26469;&#35780;&#20272;&#24320;&#25918;&#35789;&#27719;&#25551;&#36848;&#24187;&#35273;&#65292;&#22312;&#22810;&#26679;&#24615;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#37117;&#36229;&#36807;&#20102;&#27969;&#34892;&#30340;CHAIR&#22522;&#20934;&#12290;&#20026;&#20102;&#22312;&#24207;&#21015;&#32423;&#21035;&#19978;&#32531;&#35299;&#24320;&#25918;&#35789;&#27719;&#30340;&#24187;&#35273;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MOCHa&#65292;&#19968;&#31181;&#21033;&#29992;&#36827;&#23637;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03631v2 Announce Type: replace-cross  Abstract: While recent years have seen rapid progress in image-conditioned text generation, image captioning still suffers from the fundamental issue of hallucinations, namely, the generation of spurious details that cannot be inferred from the given image. Existing methods largely use closed-vocabulary object lists to mitigate or evaluate hallucinations in image captioning, ignoring most types of hallucinations that occur in practice. To this end, we propose a framework for addressing hallucinations in image captioning in the open-vocabulary setting, including quantifying their presence and optimizing to mitigate such hallucinations. Our OpenCHAIR benchmark leverages generative foundation models to evaluate open-vocabulary caption hallucinations, surpassing the popular CHAIR benchmark in both diversity and accuracy. To mitigate open-vocabulary hallucinations at the sequence level, we propose MOCHa, an approach harnessing advancements in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#21644;&#21487;&#29992;&#30340;&#34920;&#38754;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.17329</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Efficient Anatomical labeling of Pulmonary Tree Structures via Implicit Point-Graph Networks. (arXiv:2309.17329v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#21644;&#21487;&#29992;&#30340;&#34920;&#38754;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#37096;&#30142;&#30149;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#26159;&#23548;&#33268;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#27835;&#24840;&#32954;&#37096;&#30142;&#30149;&#38656;&#35201;&#26356;&#22909;&#22320;&#29702;&#35299;&#32954;&#37096;&#31995;&#32479;&#20869;&#30340;&#35768;&#22810;&#22797;&#26434;&#30340;3D&#26641;&#29366;&#32467;&#26500;&#65292;&#22914;&#27668;&#36947;&#12289;&#21160;&#33033;&#21644;&#38745;&#33033;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#22534;&#26632;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#23494;&#38598;&#20307;&#32032;&#32593;&#26684;&#30340;&#26631;&#20934;CNN&#26041;&#27861;&#20195;&#20215;&#36807;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#30340;&#26041;&#27861;&#65292;&#20445;&#30041;&#20102;&#26641;&#39592;&#26550;&#30340;&#22270;&#36830;&#36890;&#24615;&#65292;&#24182;&#32467;&#21512;&#20102;&#38544;&#24335;&#34920;&#38754;&#34920;&#31034;&#12290;&#23427;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#20855;&#26377;&#21487;&#29992;&#30340;&#34920;&#38754;&#12290;&#30001;&#20110;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#25968;&#25454;&#31232;&#32570;&#65292;&#25105;&#20204;&#36824;&#25972;&#29702;&#20102;&#19968;&#22871;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pulmonary diseases rank prominently among the principal causes of death worldwide. Curing them will require, among other things, a better understanding of the many complex 3D tree-shaped structures within the pulmonary system, such as airways, arteries, and veins. In theory, they can be modeled using high-resolution image stacks. Unfortunately, standard CNN approaches operating on dense voxel grids are prohibitively expensive. To remedy this, we introduce a point-based approach that preserves graph connectivity of tree skeleton and incorporates an implicit surface representation. It delivers SOTA accuracy at a low computational cost and the resulting models have usable surfaces. Due to the scarcity of publicly accessible data, we have also curated an extensive dataset to evaluate our approach and will make it public.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#32534;&#30721;&#35270;&#39057;&#25968;&#25454;&#24182;&#23558;&#20854;&#23384;&#20648;&#22312;&#21521;&#37327;&#25968;&#25454;&#24211;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#36827;&#34892;&#35821;&#35328;&#32534;&#30721;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.05822</link><description>&lt;p&gt;
&#32534;&#30721;-&#23384;&#20648;-&#26816;&#32034;&#65306;&#36890;&#36807;&#35821;&#35328;&#32534;&#30721;&#30340;&#33258;&#25105;&#20013;&#24515;&#24863;&#30693;&#22686;&#24378;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Encode-Store-Retrieve: Enhancing Memory Augmentation through Language-Encoded Egocentric Perception. (arXiv:2308.05822v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#32534;&#30721;&#35270;&#39057;&#25968;&#25454;&#24182;&#23558;&#20854;&#23384;&#20648;&#22312;&#21521;&#37327;&#25968;&#25454;&#24211;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#36827;&#34892;&#35821;&#35328;&#32534;&#30721;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20381;&#36182;&#20110;&#33258;&#24049;&#30340;&#35760;&#24518;&#26469;&#32534;&#30721;&#12289;&#23384;&#20648;&#21644;&#26816;&#32034;&#25105;&#20204;&#30340;&#32463;&#21382;&#12290;&#28982;&#32780;&#65292;&#35760;&#24518;&#38388;&#38548;&#26377;&#26102;&#20250;&#21457;&#29983;&#12290;&#23454;&#29616;&#35760;&#24518;&#22686;&#24378;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;&#22686;&#24378;&#29616;&#23454;&#22836;&#25140;&#24335;&#26174;&#31034;&#35774;&#22791;&#26469;&#25429;&#25417;&#21644;&#20445;&#30041;&#33258;&#25105;&#20013;&#24515;&#30340;&#35270;&#39057;&#65292;&#36825;&#31181;&#20570;&#27861;&#36890;&#24120;&#34987;&#31216;&#20026;&#29983;&#27963;&#35760;&#24405;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#25216;&#26415;&#32570;&#20047;&#39640;&#25928;&#32534;&#30721;&#21644;&#23384;&#20648;&#22914;&#27492;&#22823;&#37327;&#30340;&#35270;&#39057;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20174;&#24222;&#22823;&#30340;&#35270;&#39057;&#23384;&#26723;&#20013;&#26816;&#32034;&#29305;&#23450;&#20449;&#24687;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#22797;&#26434;&#20102;&#24555;&#36895;&#35775;&#38382;&#25152;&#38656;&#20869;&#23481;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We depend on our own memory to encode, store, and retrieve our experiences. However, memory lapses can occur. One promising avenue for achieving memory augmentation is through the use of augmented reality head-mounted displays to capture and preserve egocentric videos, a practice commonly referred to as life logging. However, a significant challenge arises from the sheer volume of video data generated through life logging, as the current technology lacks the capability to encode and store such large amounts of data efficiently. Further, retrieving specific information from extensive video archives requires substantial computational power, further complicating the task of quickly accessing desired content. To address these challenges, we propose a memory augmentation system that involves leveraging natural language encoding for video data and storing them in a vector database. This approach harnesses the power of large vision language models to perform the language encoding process. Add
&lt;/p&gt;</description></item></channel></rss>