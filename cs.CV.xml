<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;FishNet&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#65292;&#21033;&#29992;&#20302;&#25104;&#26412;&#25968;&#30721;&#30456;&#26426;&#22270;&#20687;&#25191;&#34892;&#40060;&#31867;&#20998;&#31867;&#21644;&#22823;&#23567;&#20272;&#31639;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.10916</link><description>&lt;p&gt;
FishNet:&#29992;&#20110;&#20302;&#25104;&#26412;&#40060;&#31867;&#23384;&#26639;&#20272;&#31639;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
FishNet: Deep Neural Networks for Low-Cost Fish Stock Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10916
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FishNet&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#65292;&#21033;&#29992;&#20302;&#25104;&#26412;&#25968;&#30721;&#30456;&#26426;&#22270;&#20687;&#25191;&#34892;&#40060;&#31867;&#20998;&#31867;&#21644;&#22823;&#23567;&#20272;&#31639;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40060;&#31867;&#24211;&#23384;&#35780;&#20272;&#36890;&#24120;&#38656;&#35201;&#30001;&#20998;&#31867;&#19987;&#23478;&#36827;&#34892;&#25163;&#24037;&#40060;&#31867;&#35745;&#25968;&#65292;&#36825;&#26082;&#32791;&#26102;&#21448;&#26114;&#36149;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#20351;&#29992;&#20302;&#25104;&#26412;&#25968;&#30721;&#30456;&#26426;&#25293;&#25668;&#30340;&#22270;&#20687;&#20013;&#25191;&#34892;&#20998;&#31867;&#21644;&#40060;&#31867;&#22823;&#23567;&#20272;&#31639;&#12290;&#35813;&#31995;&#32479;&#39318;&#20808;&#21033;&#29992;Mask R-CNN&#25191;&#34892;&#30446;&#26631;&#26816;&#27979;&#21644;&#20998;&#21106;&#65292;&#20197;&#35782;&#21035;&#21253;&#21547;&#22810;&#26465;&#40060;&#30340;&#22270;&#20687;&#20013;&#30340;&#21333;&#26465;&#40060;&#65292;&#36825;&#20123;&#40060;&#21487;&#33021;&#30001;&#19981;&#21516;&#29289;&#31181;&#32452;&#25104;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#40060;&#31867;&#34987;&#20998;&#31867;&#24182;&#20351;&#29992;&#21333;&#29420;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#38271;&#24230;&#12290;&#36825;&#20123;&#27169;&#22411;&#35757;&#32451;&#20110;&#21253;&#21547;50,000&#24352;&#25163;&#24037;&#27880;&#37322;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;163&#31181;&#19981;&#21516;&#38271;&#24230;&#20174;10&#21400;&#31859;&#21040;250&#21400;&#31859;&#30340;&#40060;&#31867;&#12290;&#22312;&#20445;&#30041;&#30340;&#27979;&#35797;&#25968;&#25454;&#19978;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#40060;&#31867;&#20998;&#21106;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;92%&#30340;&#20132;&#24182;&#27604;&#65292;&#21333;&#19968;&#40060;&#31867;&#20998;&#31867;&#20934;&#30830;&#29575;&#20026;89%&#65292;&#24179;&#22343;&#35823;&#24046;&#20026;2.3&#21400;&#31859;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10916v1 Announce Type: cross  Abstract: Fish stock assessment often involves manual fish counting by taxonomy specialists, which is both time-consuming and costly. We propose an automated computer vision system that performs both taxonomic classification and fish size estimation from images taken with a low-cost digital camera. The system first performs object detection and segmentation using a Mask R-CNN to identify individual fish from images containing multiple fish, possibly consisting of different species. Then each fish species is classified and the predicted length using separate machine learning models. These models are trained on a dataset of 50,000 hand-annotated images containing 163 different fish species, ranging in length from 10cm to 250cm. Evaluated on held-out test data, our system achieves a $92\%$ intersection over union on the fish segmentation task, a $89\%$ top-1 classification accuracy on single fish species classification, and a $2.3$~cm mean error on
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#38381;&#29615;&#30340;&#26694;&#26550;&#65288;LogicCheckGPT&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.11622</link><description>&lt;p&gt;
&#36923;&#36753;&#38381;&#29615;&#65306;&#25581;&#31034;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23545;&#35937;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11622
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#38381;&#29615;&#30340;&#26694;&#26550;&#65288;LogicCheckGPT&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#24187;&#35273;&#19968;&#30452;&#26159;&#38459;&#30861;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#26356;&#24191;&#27867;&#24212;&#29992;&#30340;&#36719;&#32907;&#12290;&#23545;&#35937;&#24187;&#35273;&#26159;&#25351;LVLMs&#22312;&#22270;&#20687;&#20013;&#22768;&#31216;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30340;&#29616;&#35937;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#25351;&#23548;&#35843;&#25972;&#21644;&#22522;&#20110;&#22806;&#37096;&#27169;&#22411;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#22806;&#37096;&#27169;&#22411;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#26410;&#28145;&#20837;&#25506;&#35752;&#30340;&#39046;&#22495;&#65292;&#21363;&#21033;&#29992;LVLM&#26412;&#36523;&#26469;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36825;&#26679;&#30340;&#30452;&#35273;&#65292;&#21363;LVLM&#20542;&#21521;&#20110;&#23545;&#23384;&#22312;&#30340;&#23545;&#35937;&#20570;&#20986;&#36923;&#36753;&#19968;&#33268;&#30340;&#21453;&#24212;&#65292;&#20294;&#23545;&#24187;&#35273;&#23545;&#35937;&#20570;&#20986;&#19981;&#19968;&#33268;&#30340;&#21453;&#24212;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36923;&#36753;&#38381;&#29615;&#30340;&#23545;&#35937;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#36731;&#26694;&#26550;&#65292;&#21363;LogicCheckGPT&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36923;&#36753;&#19968;&#33268;&#24615;&#25506;&#27979;&#26469;&#25552;&#20986;&#20855;&#26377;&#36923;&#36753;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11622v1 Announce Type: cross  Abstract: Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical corr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#33322;&#29677;&#28369;&#34892;&#23433;&#20840;&#30340;&#36305;&#36947;&#29289;&#20307;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#35780;&#20272;&#20102;&#35813;&#20998;&#31867;&#22120;&#23545;&#19977;&#31181;&#24120;&#35265;&#22270;&#20687;&#25200;&#21160;&#31867;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21333;&#35843;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.00035</link><description>&lt;p&gt;
&#33322;&#29677;&#28369;&#34892;&#23433;&#20840;&#30340;&#36305;&#36947;&#29289;&#20307;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Robustness Assessment of a Runway Object Classifier for Safe Aircraft Taxiing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#33322;&#29677;&#28369;&#34892;&#23433;&#20840;&#30340;&#36305;&#36947;&#29289;&#20307;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#35780;&#20272;&#20102;&#35813;&#20998;&#31867;&#22120;&#23545;&#19977;&#31181;&#24120;&#35265;&#22270;&#20687;&#25200;&#21160;&#31867;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21333;&#35843;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#35768;&#22810;&#35745;&#31639;&#38382;&#39064;&#19978;&#25104;&#20026;&#20027;&#35201;&#35299;&#20915;&#26041;&#26696;&#65292;&#33322;&#31354;&#19994;&#24076;&#26395;&#25506;&#32034;&#23427;&#20204;&#22312;&#20943;&#36731;&#39134;&#34892;&#21592;&#36127;&#25285;&#21644;&#25913;&#21892;&#36816;&#33829;&#23433;&#20840;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31867;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#20351;&#29992;DNNs&#38656;&#35201;&#36827;&#34892;&#24443;&#24213;&#30340;&#35748;&#35777;&#36807;&#31243;&#12290;&#36825;&#19968;&#38656;&#27714;&#21487;&#20197;&#36890;&#36807;&#24418;&#24335;&#39564;&#35777;&#26469;&#35299;&#20915;&#65292;&#24418;&#24335;&#39564;&#35777;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#65292;&#20363;&#22914;&#35777;&#26126;&#26576;&#20123;&#35823;&#21028;&#30340;&#19981;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;Airbus&#24403;&#21069;&#27491;&#22312;&#24320;&#21457;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;DNN&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#26088;&#22312;&#22312;&#39134;&#26426;&#28369;&#34892;&#38454;&#27573;&#20351;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#26469;&#35780;&#20272;&#36825;&#20010;DNN&#23545;&#19977;&#31181;&#24120;&#35265;&#22270;&#20687;&#25200;&#21160;&#31867;&#22411;&#30340;&#40065;&#26834;&#24615;&#65306;&#22122;&#22768;&#12289;&#20142;&#24230;&#21644;&#23545;&#27604;&#24230;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#37096;&#20998;&#32452;&#21512;&#12290;&#36825;&#20010;&#36807;&#31243;&#28041;&#21450;&#22810;&#27425;&#35843;&#29992;&#24213;&#23618;&#39564;&#35777;&#22120;&#65292;&#36825;&#21487;&#33021;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65307;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21333;&#35843;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks (DNNs) are becoming the prominent solution for many computational problems, the aviation industry seeks to explore their potential in alleviating pilot workload and in improving operational safety. However, the use of DNNs in this type of safety-critical applications requires a thorough certification process. This need can be addressed through formal verification, which provides rigorous assurances -- e.g.,~by proving the absence of certain mispredictions. In this case-study paper, we demonstrate this process using an image-classifier DNN currently under development at Airbus and intended for use during the aircraft taxiing phase. We use formal methods to assess this DNN's robustness to three common image perturbation types: noise, brightness and contrast, and some of their combinations. This process entails multiple invocations of the underlying verifier, which might be computationally expensive; and we therefore propose a method that leverages the monotonicity
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20048;&#39640;&#31215;&#26408;&#65292;&#36890;&#36807;&#38598;&#25104;&#23616;&#37096;&#29305;&#24449;&#20016;&#23500;&#21644;&#20840;&#23616;&#20869;&#23481;&#21327;&#35843;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#21487;&#33258;&#36866;&#24212;&#30340;&#36845;&#20195;&#32454;&#21270;&#25193;&#25955;&#24314;&#27169;&#12290;&#36825;&#20123;&#31215;&#26408;&#21487;&#20197;&#22534;&#21472;&#22312;&#19968;&#36215;&#65292;&#29992;&#20110;&#22312;&#27979;&#35797;&#26102;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#37325;&#26500;&#65292;&#20174;&#32780;&#20943;&#23569;&#37319;&#26679;&#25104;&#26412;&#24182;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2310.06389</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#22534;&#21472;&#21644;&#21487;&#36339;&#36807;&#30340;&#20048;&#39640;&#31215;&#26408;&#20197;&#23454;&#29616;&#39640;&#25928;&#12289;&#21487;&#37325;&#26500;&#21644;&#21487;&#21464;&#20998;&#36776;&#29575;&#30340;&#25193;&#25955;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Learning Stackable and Skippable LEGO Bricks for Efficient, Reconfigurable, and Variable-Resolution Diffusion Modeling. (arXiv:2310.06389v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20048;&#39640;&#31215;&#26408;&#65292;&#36890;&#36807;&#38598;&#25104;&#23616;&#37096;&#29305;&#24449;&#20016;&#23500;&#21644;&#20840;&#23616;&#20869;&#23481;&#21327;&#35843;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#21487;&#33258;&#36866;&#24212;&#30340;&#36845;&#20195;&#32454;&#21270;&#25193;&#25955;&#24314;&#27169;&#12290;&#36825;&#20123;&#31215;&#26408;&#21487;&#20197;&#22534;&#21472;&#22312;&#19968;&#36215;&#65292;&#29992;&#20110;&#22312;&#27979;&#35797;&#26102;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#37325;&#26500;&#65292;&#20174;&#32780;&#20943;&#23569;&#37319;&#26679;&#25104;&#26412;&#24182;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#30495;&#23454;&#24863;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35757;&#32451;&#21644;&#37319;&#26679;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20123;&#35745;&#31639;&#25361;&#25112;&#65292;&#20294;&#19968;&#20010;&#36739;&#23569;&#25506;&#32034;&#30340;&#38382;&#39064;&#26159;&#35774;&#35745;&#19968;&#20010;&#39640;&#25928;&#19988;&#36866;&#24212;&#24615;&#24378;&#30340;&#32593;&#32476;&#39592;&#24178;&#65292;&#29992;&#20110;&#36845;&#20195;&#32454;&#21270;&#12290;&#24403;&#21069;&#30340;&#36873;&#39033;&#22914;U-Net&#21644;Vision Transformer&#36890;&#24120;&#20381;&#36182;&#20110;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#32570;&#20047;&#22312;&#21464;&#37327;&#20998;&#36776;&#29575;&#19979;&#29983;&#25104;&#22270;&#20687;&#25110;&#20351;&#29992;&#27604;&#35757;&#32451;&#20013;&#26356;&#23567;&#30340;&#32593;&#32476;&#25152;&#38656;&#30340;&#28789;&#27963;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20048;&#39640;&#31215;&#26408;&#65292;&#23427;&#20204;&#26080;&#32541;&#38598;&#25104;&#20102;&#23616;&#37096;&#29305;&#24449;&#20016;&#23500;&#21644;&#20840;&#23616;&#20869;&#23481;&#21327;&#35843;&#12290;&#36825;&#20123;&#31215;&#26408;&#21487;&#20197;&#22534;&#21472;&#22312;&#19968;&#36215;&#65292;&#21019;&#24314;&#19968;&#20010;&#27979;&#35797;&#26102;&#21487;&#37325;&#26500;&#30340;&#25193;&#25955;&#39592;&#24178;&#65292;&#20801;&#35768;&#36873;&#25321;&#24615;&#36339;&#36807;&#31215;&#26408;&#20197;&#20943;&#23569;&#37319;&#26679;&#25104;&#26412;&#65292;&#24182;&#29983;&#25104;&#27604;&#35757;&#32451;&#25968;&#25454;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#12290;&#20048;&#39640;&#31215;&#26408;&#36890;&#36807;MLP&#23545;&#23616;&#37096;&#21306;&#22495;&#36827;&#34892;&#20016;&#23500;&#65292;&#24182;&#20351;&#29992;Transformer&#22359;&#36827;&#34892;&#21464;&#25442;&#65292;&#21516;&#26102;&#20445;&#25345;&#19968;&#33268;&#30340;&#20840;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Diffusion models excel at generating photo-realistic images but come with significant computational costs in both training and sampling. While various techniques address these computational challenges, a less-explored issue is designing an efficient and adaptable network backbone for iterative refinement. Current options like U-Net and Vision Transformer often rely on resource-intensive deep networks and lack the flexibility needed for generating images at variable resolutions or with a smaller network than used in training. This study introduces LEGO bricks, which seamlessly integrate Local-feature Enrichment and Global-content Orchestration. These bricks can be stacked to create a test-time reconfigurable diffusion backbone, allowing selective skipping of bricks to reduce sampling costs and generate higher-resolution images than the training data. LEGO bricks enrich local regions with an MLP and transform them using a Transformer block while maintaining a consistent full-resolution i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35299;&#35835;&#33258;&#32534;&#30721;&#22120;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20026;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#25968;&#25454;&#28857;&#20998;&#37197;&#29420;&#29305;&#30340;&#38543;&#26426;&#20002;&#24323;&#27169;&#24335;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#21482;&#20381;&#38752;&#37325;&#26500;&#35823;&#24046;&#26469;&#25552;&#20379;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#24615;&#33021;&#65292;&#24182;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#19982;DCGAN&#30456;&#23218;&#32654;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.01712</link><description>&lt;p&gt;
&#20002;&#24323;&#27169;&#24335;&#30340;&#29983;&#25104;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Generative Autoencoding of Dropout Patterns. (arXiv:2310.01712v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35299;&#35835;&#33258;&#32534;&#30721;&#22120;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20026;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#25968;&#25454;&#28857;&#20998;&#37197;&#29420;&#29305;&#30340;&#38543;&#26426;&#20002;&#24323;&#27169;&#24335;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#21482;&#20381;&#38752;&#37325;&#26500;&#35823;&#24046;&#26469;&#25552;&#20379;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#24615;&#33021;&#65292;&#24182;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#19982;DCGAN&#30456;&#23218;&#32654;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35299;&#35835;&#33258;&#32534;&#30721;&#22120;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#20026;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#25968;&#25454;&#28857;&#20998;&#37197;&#19968;&#20010;&#21807;&#19968;&#30340;&#38543;&#26426;&#20002;&#24323;&#27169;&#24335;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20010;&#27169;&#24335;&#20316;&#20026;&#34987;&#32534;&#30721;&#30340;&#20449;&#24687;&#26469;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#26469;&#37325;&#26500;&#30456;&#24212;&#30340;&#25968;&#25454;&#28857;&#12290;&#30001;&#20110;&#35299;&#35835;&#33258;&#32534;&#30721;&#22120;&#30340;&#35757;&#32451;&#20165;&#20381;&#36182;&#20110;&#37325;&#26500;&#35823;&#24046;&#65292;&#25152;&#20197;&#30456;&#27604;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#23613;&#31649;&#23427;&#24456;&#31616;&#21333;&#65292;&#20294;&#35299;&#35835;&#33258;&#32534;&#30721;&#22120;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#19982;DCGAN&#30456;&#23218;&#32654;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a generative model termed Deciphering Autoencoders. In this model, we assign a unique random dropout pattern to each data point in the training dataset and then train an autoencoder to reconstruct the corresponding data point using this pattern as information to be encoded. Since the training of Deciphering Autoencoders relies solely on reconstruction error, it offers more stable training than other generative models. Despite its simplicity, Deciphering Autoencoders show comparable sampling quality to DCGAN on the CIFAR-10 dataset.
&lt;/p&gt;</description></item></channel></rss>