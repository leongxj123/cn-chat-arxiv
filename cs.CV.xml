<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#23567;&#26377;&#25928;&#35270;&#22270;&#65288;MSVs&#65289;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#31867;&#20284;&#20110;&#22810;&#35270;&#22270;&#65292;&#20294;&#36866;&#29992;&#20110;&#23454;&#38469;&#22270;&#20687;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;MSV&#30340;&#25968;&#37327;&#19982;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01095</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20351;&#29992;&#22810;&#23569;&#20010;&#35270;&#22270;&#65311;
&lt;/p&gt;
&lt;p&gt;
How many views does your deep neural network use for prediction?
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26368;&#23567;&#26377;&#25928;&#35270;&#22270;&#65288;MSVs&#65289;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#31867;&#20284;&#20110;&#22810;&#35270;&#22270;&#65292;&#20294;&#36866;&#29992;&#20110;&#23454;&#38469;&#22270;&#20687;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;MSV&#30340;&#25968;&#37327;&#19982;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36827;&#34892;&#20102;&#35768;&#22810;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#20294;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#20173;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#26368;&#36817;&#65292;Allen-Zhu&#21644;Li&#65288;2023&#65289;&#24341;&#20837;&#20102;&#22810;&#35270;&#22270;&#30340;&#27010;&#24565;&#26469;&#35299;&#37322;DNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#20182;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#38598;&#25104;&#25110;&#33976;&#39311;&#27169;&#22411;&#65292;&#24182;&#26410;&#35752;&#35770;&#29992;&#20110;&#29305;&#23450;&#36755;&#20837;&#39044;&#27979;&#30340;&#22810;&#35270;&#22270;&#20272;&#35745;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#23567;&#26377;&#25928;&#35270;&#22270;&#65288;MSVs&#65289;&#65292;&#23427;&#31867;&#20284;&#20110;&#22810;&#35270;&#22270;&#65292;&#20294;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#30495;&#23454;&#22270;&#20687;&#12290;MSVs&#26159;&#36755;&#20837;&#20013;&#30340;&#19968;&#32452;&#26368;&#23567;&#19988;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#27599;&#20010;&#29305;&#24449;&#20445;&#30041;&#20102;&#27169;&#22411;&#23545;&#35813;&#36755;&#20837;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#27169;&#22411;&#65288;&#21253;&#25324;&#21367;&#31215;&#21644;&#36716;&#25442;&#27169;&#22411;&#65289;&#30340;MSV&#25968;&#37327;&#19982;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#26126;&#30830;&#30340;&#20851;&#31995;&#65292;&#36825;&#34920;&#26126;&#22810;&#35270;&#22270;&#30340;&#35282;&#24230;&#23545;&#20110;&#29702;&#35299;&#65288;&#38750;&#38598;&#25104;&#25110;&#38750;&#33976;&#39311;&#65289;DNN&#30340;&#27867;&#21270;&#33021;&#21147;&#20063;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization ability of Deep Neural Networks (DNNs) is still not fully understood, despite numerous theoretical and empirical analyses. Recently, Allen-Zhu &amp; Li (2023) introduced the concept of multi-views to explain the generalization ability of DNNs, but their main target is ensemble or distilled models, and no method for estimating multi-views used in a prediction of a specific input is discussed. In this paper, we propose Minimal Sufficient Views (MSVs), which is similar to multi-views but can be efficiently computed for real images. MSVs is a set of minimal and distinct features in an input, each of which preserves a model's prediction for the input. We empirically show that there is a clear relationship between the number of MSVs and prediction accuracy across models, including convolutional and transformer models, suggesting that a multi-view like perspective is also important for understanding the generalization ability of (non-ensemble or non-distilled) DNNs.
&lt;/p&gt;</description></item><item><title>NACHOS &#26159;&#19968;&#31181;&#38754;&#21521;&#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#35774;&#35745;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#24182;&#32771;&#34385;&#39592;&#24178;&#21644;&#26089;&#26399;&#36864;&#20986;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.13330</link><description>&lt;p&gt;
NACHOS: &#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
NACHOS: Neural Architecture Search for Hardware Constrained Early Exit Neural Networks. (arXiv:2401.13330v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13330
&lt;/p&gt;
&lt;p&gt;
NACHOS &#26159;&#19968;&#31181;&#38754;&#21521;&#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#35774;&#35745;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#24182;&#32771;&#34385;&#39592;&#24178;&#21644;&#26089;&#26399;&#36864;&#20986;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#65288;EENNs&#65289;&#20026;&#26631;&#20934;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#37197;&#22791;&#26089;&#26399;&#36864;&#20986;&#20998;&#31867;&#22120;&#65288;EECs&#65289;&#65292;&#22312;&#22788;&#29702;&#30340;&#20013;&#38388;&#28857;&#19978;&#25552;&#20379;&#36275;&#22815;&#30340;&#20998;&#31867;&#32622;&#20449;&#24230;&#26102;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#26041;&#38754;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#12290;&#30446;&#21069;&#65292;EENNs&#30340;&#35774;&#35745;&#26159;&#30001;&#19987;&#23478;&#25163;&#21160;&#23436;&#25104;&#30340;&#65292;&#36825;&#26159;&#19968;&#39033;&#22797;&#26434;&#21644;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#32771;&#34385;&#35768;&#22810;&#26041;&#38754;&#65292;&#21253;&#25324;&#27491;&#30830;&#30340;&#25918;&#32622;&#12289;&#38408;&#20540;&#35774;&#32622;&#21644;EECs&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#27491;&#22312;&#25506;&#32034;&#20351;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#33258;&#21160;&#21270;&#35774;&#35745;EENNs&#12290;&#30446;&#21069;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20960;&#20010;&#23436;&#25972;&#30340;NAS&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;EENNs&#65292;&#24182;&#19988;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#32508;&#21512;&#35774;&#35745;&#31574;&#30053;&#65292;&#21516;&#26102;&#32771;&#34385;&#39592;&#24178;&#21644;EECs&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#21576;&#29616;&#20102;&#38754;&#21521;&#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NACHOS&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN) with Early Exit Classifiers (EECs), to provide predictions at intermediate points of the processing when enough confidence in classification is achieved. This leads to many benefits in terms of effectiveness and efficiency. Currently, the design of EENNs is carried out manually by experts, a complex and time-consuming task that requires accounting for many aspects, including the correct placement, the thresholding, and the computational overhead of the EECs. For this reason, the research is exploring the use of Neural Architecture Search (NAS) to automatize the design of EENNs. Currently, few comprehensive NAS solutions for EENNs have been proposed in the literature, and a fully automated, joint design strategy taking into consideration both the backbone and the EECs remains an open problem. To this end, this work presents Neural Architecture Search for Hardware Constrained Early Exit Neural Networks (NACHOS),
&lt;/p&gt;</description></item></channel></rss>