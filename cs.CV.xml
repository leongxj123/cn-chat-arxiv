<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;ICON&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#25913;&#21892;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#25253;&#21578;&#38388;&#19968;&#33268;&#24615;&#65292;&#25552;&#21319;&#31995;&#32479;&#25429;&#25417;&#35821;&#20041;&#31561;&#25928;&#30149;&#21464;&#30456;&#20284;&#24615;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.12844</link><description>&lt;p&gt;
ICON&#65306;&#36890;&#36807;&#30149;&#21464;&#24863;&#30693;&#28151;&#21512;&#22686;&#24378;&#25913;&#21892;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#25253;&#21578;&#38388;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
ICON: Improving Inter-Report Consistency of Radiology Report Generation via Lesion-aware Mix-up Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;ICON&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#25913;&#21892;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#25253;&#21578;&#38388;&#19968;&#33268;&#24615;&#65292;&#25552;&#21319;&#31995;&#32479;&#25429;&#25417;&#35821;&#20041;&#31561;&#25928;&#30149;&#21464;&#30456;&#20284;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#20808;&#21069;&#30740;&#31350;&#22312;&#22686;&#21152;&#29983;&#25104;&#25253;&#21578;&#30340;&#20020;&#24202;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#20854;&#24212;&#20855;&#22791;&#30340;&#21478;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#29305;&#36136;&#65292;&#21363;&#25253;&#21578;&#38388;&#19968;&#33268;&#24615;&#65292;&#25351;&#30340;&#26159;&#23545;&#35821;&#20041;&#19978;&#31561;&#25928;&#30340;X&#23556;&#32447;&#29031;&#29255;&#29983;&#25104;&#19968;&#33268;&#24615;&#25253;&#21578;&#30340;&#33021;&#21147;&#12290;ICON&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#25913;&#21892;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#25253;&#21578;&#38388;&#19968;&#33268;&#24615;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12844v1 Announce Type: cross  Abstract: Previous research on radiology report generation has made significant progress in terms of increasing the clinical accuracy of generated reports. In this paper, we emphasize another crucial quality that it should possess, i.e., inter-report consistency, which refers to the capability of generating consistent reports for semantically equivalent radiographs. This quality is even of greater significance than the overall report accuracy in terms of ensuring the system's credibility, as a system prone to providing conflicting results would severely erode users' trust. Regrettably, existing approaches struggle to maintain inter-report consistency, exhibiting biases towards common patterns and susceptibility to lesion variants. To address this issue, we propose ICON, which improves the inter-report consistency of radiology report generation. Aiming at enhancing the system's ability to capture the similarities in semantically equivalent lesion
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#25351;&#20196;&#26469;&#25351;&#23548;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#24674;&#22797;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20026;&#22522;&#20110;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.16468</link><description>&lt;p&gt;
&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
High-Quality Image Restoration Following Human Instructions. (arXiv:2401.16468v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#25351;&#20196;&#26469;&#25351;&#23548;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#24674;&#22797;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20026;&#22522;&#20110;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24674;&#22797;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#28041;&#21450;&#20174;&#36864;&#21270;&#35266;&#27979;&#20013;&#24674;&#22797;&#20986;&#39640;&#36136;&#37327;&#30340;&#24178;&#20928;&#22270;&#20687;&#12290;&#20840;&#33021;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36864;&#21270;&#31867;&#22411;&#30340;&#20449;&#24687;&#20316;&#20026;&#25552;&#31034;&#26469;&#26377;&#25928;&#22320;&#24674;&#22797;&#21508;&#31181;&#31867;&#22411;&#21644;&#32423;&#21035;&#30340;&#36864;&#21270;&#22270;&#20687;&#65292;&#24182;&#24341;&#23548;&#24674;&#22797;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#25351;&#20196;&#26469;&#25351;&#23548;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20174;&#36864;&#21270;&#22270;&#20687;&#20013;&#24674;&#22797;&#20986;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#32771;&#34385;&#22810;&#31181;&#36864;&#21270;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;InstructIR&#22312;&#22270;&#20687;&#21435;&#22122;&#12289;&#38632;&#27700;&#21435;&#38500;&#12289;&#21435;&#27169;&#31946;&#12289;&#21435;&#38654;&#21644;(&#20302;&#20809;)&#22270;&#20687;&#22686;&#24378;&#31561;&#22810;&#20010;&#24674;&#22797;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;InstructIR&#22312;&#20043;&#21069;&#30340;&#20840;&#33021;&#24674;&#22797;&#26041;&#27861;&#19978;&#25552;&#39640;&#20102;1dB&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#32467;&#26524;&#20026;&#22522;&#20110;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30340;&#26032;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20195;&#30721;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image restoration is a fundamental problem that involves recovering a high-quality clean image from its degraded observation. All-In-One image restoration models can effectively restore images from various types and levels of degradation using degradation-specific information as prompts to guide the restoration model. In this work, we present the first approach that uses human-written instructions to guide the image restoration model. Given natural language prompts, our model can recover high-quality images from their degraded counterparts, considering multiple degradation types. Our method, InstructIR, achieves state-of-the-art results on several restoration tasks including image denoising, deraining, deblurring, dehazing, and (low-light) image enhancement. InstructIR improves +1dB over previous all-in-one restoration methods. Moreover, our dataset and results represent a novel benchmark for new research on text-guided image restoration and enhancement. Our code, datasets and models a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20998;&#24067;&#23545;&#40784;&#26041;&#27861;&#20197;&#35299;&#20915;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#20110;&#24357;&#25955;&#27169;&#22411;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#24310;&#36831;&#24212;&#29992;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#33021;&#26377;&#25928;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04585</link><description>&lt;p&gt;
&#25193;&#23637;&#20998;&#24067;&#23545;&#40784;&#26469;&#23454;&#29616;&#24357;&#25955;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models. (arXiv:2401.04585v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20998;&#24067;&#23545;&#40784;&#26041;&#27861;&#20197;&#35299;&#20915;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#20110;&#24357;&#25955;&#27169;&#22411;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#24310;&#36831;&#24212;&#29992;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#33021;&#26377;&#25928;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#22122;&#22768;&#20272;&#35745;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#32321;&#37325;&#30340;&#21435;&#22122;&#36807;&#31243;&#21644;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#20302;&#24310;&#36831;&#24212;&#29992;&#12290;&#37327;&#21270;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24230;&#65292;&#32780;&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#22312;&#21152;&#36895;&#21435;&#22122;&#36807;&#31243;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#24494;&#35843;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#30001;&#20110;&#19981;&#21516;&#21435;&#22122;&#27493;&#39588;&#20013;&#28608;&#27963;&#30340;&#39640;&#24230;&#21160;&#24577;&#20998;&#24067;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;PTQ&#26041;&#27861;&#22312;&#26657;&#20934;&#26679;&#26412;&#21644;&#37325;&#26500;&#36755;&#20986;&#20004;&#20010;&#23618;&#38754;&#19978;&#37117;&#23384;&#22312;&#20998;&#24067;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#24615;&#33021;&#36828;&#20302;&#20110;&#20196;&#20154;&#28385;&#24847;&#30340;&#27700;&#24179;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#20301;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#30340;&#20998;&#24067;&#23545;&#40784;&#29992;&#20110;&#24357;&#25955;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;(EDA-DM)&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#26657;&#20934;&#26679;&#26412;&#23618;&#38754;&#65292;&#25105;&#20204;&#22522;&#20110;...[&#32570;&#30465;]
&lt;/p&gt;
&lt;p&gt;
Diffusion models have achieved great success in image generation tasks through iterative noise estimation. However, the heavy denoising process and complex neural networks hinder their low-latency applications in real-world scenarios. Quantization can effectively reduce model complexity, and post-training quantization (PTQ), which does not require fine-tuning, is highly promising in accelerating the denoising process. Unfortunately, we find that due to the highly dynamic distribution of activations in different denoising steps, existing PTQ methods for diffusion models suffer from distribution mismatch issues at both calibration sample level and reconstruction output level, which makes the performance far from satisfactory, especially in low-bit cases. In this paper, we propose Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models (EDA-DM) to address the above issues. Specifically, at the calibration sample level, we select calibration samples based on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#23454;&#29616;Text-to-image diffusion models&#20013;&#30340;&#24555;&#36895;&#25512;&#29702;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.01008</link><description>&lt;p&gt;
Text-to-image diffusion models&#20013;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#23454;&#29616;&#24555;&#36895;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Fast Inference Through The Reuse Of Attention Maps In Diffusion Models. (arXiv:2401.01008v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#23454;&#29616;Text-to-image diffusion models&#20013;&#30340;&#24555;&#36895;&#25512;&#29702;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#28789;&#27963;&#21644;&#36924;&#30495;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#21333;&#20010;&#22270;&#20687;&#25152;&#38656;&#30340;&#36845;&#20195;&#36807;&#31243;&#26082;&#26114;&#36149;&#21448;&#20855;&#26377;&#36739;&#39640;&#30340;&#24310;&#36831;&#65292;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#36827;&#19968;&#27493;&#30740;&#31350;&#20854;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#37319;&#26679;&#27493;&#38271;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#37325;&#22797;&#35745;&#31639;&#27880;&#24847;&#21147;&#26144;&#23556;&#26082;&#32791;&#26102;&#21448;&#20887;&#20313;&#65292;&#22240;&#27492;&#25105;&#20204;&#24314;&#35758;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#32467;&#26500;&#21270;&#22320;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#37325;&#29992;&#31574;&#30053;&#21463;&#21040;&#21021;&#32423;ODE&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#35813;&#29702;&#35770;&#35748;&#20026;&#22312;&#37319;&#26679;&#36807;&#31243;&#30340;&#21518;&#26399;&#37325;&#29992;&#26368;&#21512;&#36866;&#12290;&#22312;&#27880;&#24847;&#21040;&#36825;&#31181;&#29702;&#35770;&#26041;&#27861;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models have demonstrated unprecedented abilities at flexible and realistic image synthesis. However, the iterative process required to produce a single image is costly and incurs a high latency, prompting researchers to further investigate its efficiency. Typically, improvements in latency have been achieved in two ways: (1) training smaller models through knowledge distillation (KD); and (2) adopting techniques from ODE-theory to facilitate larger step sizes. In contrast, we propose a training-free approach that does not alter the step-size of the sampler. Specifically, we find the repeated calculation of attention maps to be both costly and redundant; therefore, we propose a structured reuse of attention maps during sampling. Our initial reuse policy is motivated by rudimentary ODE-theory, which suggests that reuse is most suitable late in the sampling procedure. After noting a number of limitations in this theoretical approach, we empirically search for a bet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EPTQ&#30340;&#22686;&#24378;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#21152;&#26435;&#23618;&#21644;&#26080;&#26631;&#31614;Hessian&#36817;&#20284;&#25216;&#26415;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11531</link><description>&lt;p&gt;
EPTQ:&#36890;&#36807;&#26080;&#26631;&#31614;Hessian&#22686;&#24378;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
EPTQ: Enhanced Post-Training Quantization via Label-Free Hessian. (arXiv:2309.11531v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EPTQ&#30340;&#22686;&#24378;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#21152;&#26435;&#23618;&#21644;&#26080;&#26631;&#31614;Hessian&#36817;&#20284;&#25216;&#26415;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#37327;&#21270;&#24050;&#25104;&#20026;&#23558;&#36825;&#20123;&#32593;&#32476;&#23884;&#20837;&#21040;&#26368;&#32456;&#29992;&#25143;&#35774;&#22791;&#19978;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#37327;&#21270;&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#20005;&#37325;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EPTQ&#30340;&#22686;&#24378;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#21152;&#26435;&#23618;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#26631;&#31614;Hessian&#36817;&#20284;&#25216;&#26415;&#65292;&#21517;&#20026;Label-Free Hessian&#12290;&#36825;&#31181;&#25216;&#26415;&#28040;&#38500;&#20102;&#35745;&#31639;Hessian&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#35201;&#27714;&#12290;&#33258;&#36866;&#24212;&#30693;&#35782;&#33976;&#39311;&#21033;&#29992;Label-Free Hessian&#25216;&#26415;&#65292;&#22312;&#36827;&#34892;&#20248;&#21270;&#26102;&#26356;&#21152;&#20851;&#27880;&#27169;&#22411;&#30340;&#25935;&#24863;&#37096;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;EPTQ&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#27169;&#22411;&#12289;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;ImageNet&#20998;&#31867;&#12289;COCO&#30446;&#26631;&#26816;&#27979;&#21644;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;Pascal-VOC&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization of deep neural networks (DNN) has become a key element in the efforts of embedding such networks on end-user devices. However, current quantization methods usually suffer from costly accuracy degradation. In this paper, we propose a new method for Enhanced Post Training Quantization named EPTQ. The method is based on knowledge distillation with an adaptive weighting of layers. In addition, we introduce a new label-free technique for approximating the Hessian trace of the task loss, named Label-Free Hessian. This technique removes the requirement of a labeled dataset for computing the Hessian. The adaptive knowledge distillation uses the Label-Free Hessian technique to give greater attention to the sensitive parts of the model while performing the optimization. Empirically, by employing EPTQ we achieve state-of-the-art results on a wide variety of models, tasks, and datasets, including ImageNet classification, COCO object detection, and Pascal-VOC for semantic segmentation.
&lt;/p&gt;</description></item><item><title>&#22791;&#36873;&#30340;&#21464;&#28966;&#20301;&#31227;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#31227;&#21160;&#21644;&#25193;&#23637;&#29305;&#24449;&#20449;&#24687;&#26469;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21487;&#20197;&#31283;&#20581;&#22320;&#25429;&#25417;&#19981;&#21516;&#27169;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#39640;&#32423;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#27969;&#34892;&#30340;&#22810;&#27169;&#24577;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.16950</link><description>&lt;p&gt;
&#22791;&#36873;&#30340;&#21464;&#28966;&#20301;&#31227;&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Alternative Telescopic Displacement: An Efficient Multimodal Alignment Method. (arXiv:2306.16950v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16950
&lt;/p&gt;
&lt;p&gt;
&#22791;&#36873;&#30340;&#21464;&#28966;&#20301;&#31227;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#31227;&#21160;&#21644;&#25193;&#23637;&#29305;&#24449;&#20449;&#24687;&#26469;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21487;&#20197;&#31283;&#20581;&#22320;&#25429;&#25417;&#19981;&#21516;&#27169;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#39640;&#32423;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#27969;&#34892;&#30340;&#22810;&#27169;&#24577;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#23545;&#40784;&#26159;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#65292;&#21487;&#20197;&#23436;&#20840;&#34701;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#20132;&#26367;&#31227;&#21160;&#21644;&#25193;&#23637;&#26469;&#23454;&#29616;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#19968;&#33268;&#34920;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#31283;&#20581;&#22320;&#25429;&#25417;&#19981;&#21516;&#27169;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#39640;&#32423;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#27969;&#34892;&#30340;&#22810;&#27169;&#24577;&#26041;&#26696;&#12290;&#23545;ETT&#21644;MIT-BIH-Arrhythmia&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature alignment is the primary means of fusing multimodal data. We propose a feature alignment method that fully fuses multimodal information, which alternately shifts and expands feature information from different modalities to have a consistent representation in a feature space. The proposed method can robustly capture high-level interactions between features of different modalities, thus significantly improving the performance of multimodal learning. We also show that the proposed method outperforms other popular multimodal schemes on multiple tasks. Experimental evaluation of ETT and MIT-BIH-Arrhythmia, datasets shows that the proposed method achieves state of the art performance.
&lt;/p&gt;</description></item></channel></rss>