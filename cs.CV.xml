<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#25509;&#21463;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#27169;&#24577;&#20998;&#31163;&#21644;&#37325;&#26500;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20058</link><description>&lt;p&gt;
&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#21644;&#28145;&#24230;&#25972;&#21512;&#30340;&#33041;&#20195;&#35874;&#12289;&#34880;&#28082;&#21160;&#21147;&#23398;&#21644;&#28748;&#27880;&#32593;&#32476;&#24443;&#24213;&#25913;&#21464;&#30142;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Disease Diagnosis with simultaneous functional PET/MR and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20058
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#25509;&#21463;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#27169;&#24577;&#20998;&#31163;&#21644;&#37325;&#26500;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#21151;&#33021;PET/MR&#65288;sf-PET/MR&#65289;&#26159;&#19968;&#31181;&#23574;&#31471;&#30340;&#22810;&#27169;&#24335;&#31070;&#32463;&#24433;&#20687;&#25216;&#26415;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#65292;&#21487;&#20197;&#21516;&#26102;&#30417;&#27979;&#21644;&#25972;&#21512;&#30001;&#26102;&#31354;&#21327;&#21464;&#20195;&#35874;&#27963;&#21160;&#12289;&#31070;&#32463;&#27963;&#21160;&#21644;&#33041;&#34880;&#27969;&#65288;&#28748;&#27880;&#65289;&#26500;&#24314;&#30340;&#22810;&#26041;&#38754;&#22823;&#33041;&#32593;&#32476;&#12290;&#34429;&#28982;&#22312;&#31185;&#23398;/&#20020;&#24202;&#20215;&#20540;&#19978;&#24456;&#39640;&#65292;&#20294;PET/MR&#30828;&#20214;&#30340;&#21487;&#21450;&#24615;&#19981;&#36275;&#38459;&#30861;&#20102;&#20854;&#24212;&#29992;&#65292;&#26356;&#19981;&#29992;&#35828;&#29616;&#20195;&#22522;&#20110;AI&#30340;PET/MR&#34701;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;AI&#30340;&#20020;&#24202;&#21487;&#34892;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#20840;&#38754;&#30340;sf-PET/MR&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20855;&#26377;&#20801;&#35768;&#21333;&#27169;&#24577;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#20165;PET&#65289;&#20197;&#21450;&#24378;&#21046;&#22810;&#27169;&#24577;&#20934;&#30830;&#24615;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#23545;&#40784;&#21644;&#37325;&#26500;&#27169;&#22411;&#12290;&#23427;&#26159;&#27169;&#24577;&#21487;&#20998;&#31163;&#21644;&#21487;&#20132;&#25442;&#30340;&#65292;&#21160;&#24577;&#20998;&#37197;&#19981;&#21516;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;"&#28151;&#21512;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20058v1 Announce Type: cross  Abstract: Simultaneous functional PET/MR (sf-PET/MR) presents a cutting-edge multimodal neuroimaging technique. It provides an unprecedented opportunity for concurrently monitoring and integrating multifaceted brain networks built by spatiotemporally covaried metabolic activity, neural activity, and cerebral blood flow (perfusion). Albeit high scientific/clinical values, short in hardware accessibility of PET/MR hinders its applications, let alone modern AI-based PET/MR fusion models. Our objective is to develop a clinically feasible AI-based disease diagnosis model trained on comprehensive sf-PET/MR data with the power of, during inferencing, allowing single modality input (e.g., PET only) as well as enforcing multimodal-based accuracy. To this end, we propose MX-ARM, a multimodal MiXture-of-experts Alignment and Reconstruction Model. It is modality detachable and exchangeable, allocating different multi-layer perceptrons dynamically ("mixture 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20351;&#29992;&#26469;&#33258;&#21363;&#25554;&#21363;&#29992;&#30340;&#38388;&#25509;&#39134;&#34892;&#26102;&#38388;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#20809;&#32447;&#21453;&#23556;&#21457;&#29983;&#30340;&#34920;&#38754;&#37325;&#26032;&#26500;&#24314;&#20026;&#34394;&#25311;&#38236;&#23376;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#33719;&#21462;&#38544;&#34255;&#22330;&#26223;&#28145;&#24230;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19376</link><description>&lt;p&gt;
NIGHT -- &#38388;&#25509;&#39134;&#34892;&#26102;&#38388;&#25968;&#25454;&#30340;&#38750;&#35270;&#36317;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
NIGHT -- Non-Line-of-Sight Imaging from Indirect Time of Flight Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20351;&#29992;&#26469;&#33258;&#21363;&#25554;&#21363;&#29992;&#30340;&#38388;&#25509;&#39134;&#34892;&#26102;&#38388;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#20809;&#32447;&#21453;&#23556;&#21457;&#29983;&#30340;&#34920;&#38754;&#37325;&#26032;&#26500;&#24314;&#20026;&#34394;&#25311;&#38236;&#23376;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#33719;&#21462;&#38544;&#34255;&#22330;&#26223;&#28145;&#24230;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38750;&#35270;&#35282;&#30456;&#26426;&#22806;&#37096;&#33719;&#21462;&#29289;&#20307;&#26159;&#19968;&#20010;&#38750;&#24120;&#24341;&#20154;&#27880;&#30446;&#20294;&#20063;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#21033;&#29992;&#23450;&#21046;&#30340;&#30452;&#25509;&#39134;&#34892;&#26102;&#38388;&#20256;&#24863;&#22120;&#20135;&#29983;&#30340;&#30636;&#26102;&#25104;&#20687;&#25968;&#25454;&#65292;&#36825;&#20010;&#24819;&#27861;&#26159;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20351;&#29992;&#26469;&#33258;&#21363;&#25554;&#21363;&#29992;&#30340;&#38388;&#25509;&#39134;&#34892;&#26102;&#38388;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#30828;&#20214;&#35201;&#27714;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#20809;&#32447;&#21453;&#23556;&#21457;&#29983;&#30340;&#34920;&#38754;&#37325;&#26032;&#26500;&#24314;&#20026;&#34394;&#25311;&#38236;&#23376;&#12290;&#36825;&#31181;&#24314;&#27169;&#20351;&#24471;&#20219;&#21153;&#26356;&#23481;&#26131;&#22788;&#29702;&#65292;&#20063;&#26377;&#21161;&#20110;&#26500;&#24314;&#24102;&#26377;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20174;&#33719;&#24471;&#30340;&#25968;&#25454;&#20013;&#65292;&#21487;&#20197;&#24674;&#22797;&#38544;&#34255;&#22330;&#26223;&#30340;&#28145;&#24230;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#39318;&#21019;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#24819;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19376v1 Announce Type: cross  Abstract: The acquisition of objects outside the Line-of-Sight of cameras is a very intriguing but also extremely challenging research topic. Recent works showed the feasibility of this idea exploiting transient imaging data produced by custom direct Time of Flight sensors. In this paper, for the first time, we tackle this problem using only data from an off-the-shelf indirect Time of Flight sensor without any further hardware requirement. We introduced a Deep Learning model able to re-frame the surfaces where light bounces happen as a virtual mirror. This modeling makes the task easier to handle and also facilitates the construction of annotated training data. From the obtained data it is possible to retrieve the depth information of the hidden scene. We also provide a first-in-its-kind synthetic dataset for the task and demonstrate the feasibility of the proposed idea over it.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65288;NSI&#65289;&#65292;&#36890;&#36807;&#27133;&#34920;&#31034;&#23398;&#20064;&#25509;&#22320;&#21644;&#29983;&#25104;&#29289;&#20307;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#20307;&#35821;&#20041;&#32467;&#21512;&#21040;&#25277;&#35937;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.07887</link><description>&lt;p&gt;
&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65306;&#22312;&#26032;&#20852;&#30340;&#27133;&#34920;&#31034;&#20013;&#25509;&#22320;&#23545;&#35937;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Neural Slot Interpreters: Grounding Object Semantics in Emergent Slot Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07887
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65288;NSI&#65289;&#65292;&#36890;&#36807;&#27133;&#34920;&#31034;&#23398;&#20064;&#25509;&#22320;&#21644;&#29983;&#25104;&#29289;&#20307;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#20307;&#35821;&#20041;&#32467;&#21512;&#21040;&#25277;&#35937;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#20013;&#24515;&#26041;&#27861;&#22312;&#23558;&#21407;&#22987;&#24863;&#30693;&#26080;&#30417;&#30563;&#20998;&#35299;&#20026;&#20016;&#23500;&#30340;&#31867;&#20284;&#29289;&#20307;&#30340;&#25277;&#35937;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#20307;&#35821;&#20041;&#25509;&#22320;&#21040;&#23398;&#21040;&#30340;&#25277;&#35937;&#20013;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#19979;&#28216;&#29702;&#35299;&#24212;&#29992;&#20013;&#30340;&#37319;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65288;NSI&#65289;&#65292;&#23427;&#36890;&#36807;&#27133;&#34920;&#31034;&#23398;&#20064;&#25509;&#22320;&#21644;&#29983;&#25104;&#29289;&#20307;&#35821;&#20041;&#12290;NSI&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#31867;&#20284;XML&#30340;&#32534;&#31243;&#35821;&#35328;&#65292;&#23427;&#20351;&#29992;&#31616;&#21333;&#30340;&#35821;&#27861;&#35268;&#21017;&#23558;&#22330;&#26223;&#30340;&#29289;&#20307;&#35821;&#20041;&#32452;&#32455;&#25104;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#31243;&#24207;&#21407;&#35821;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#23545;&#40784;&#27169;&#22411;&#23398;&#20064;&#36890;&#36807;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#19978;&#30340;&#21452;&#23618;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#23558;&#31243;&#24207;&#21407;&#35821;&#25509;&#22320;&#21040;&#27133;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;NSI&#31243;&#24207;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#40784;&#27169;&#22411;&#25512;&#26029;&#30340;&#23494;&#38598;&#20851;&#32852;&#20174;&#27133;&#29983;&#25104;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#31243;&#24207;&#12290;&#22312;&#21452;&#27169;&#24335;&#26816;&#32034;&#23454;&#39564;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07887v1 Announce Type: cross  Abstract: Object-centric methods have seen significant progress in unsupervised decomposition of raw perception into rich object-like abstractions. However, limited ability to ground object semantics of the real world into the learned abstractions has hindered their adoption in downstream understanding applications. We present the Neural Slot Interpreter (NSI) that learns to ground and generate object semantics via slot representations. At the core of NSI is an XML-like programming language that uses simple syntax rules to organize the object semantics of a scene into object-centric program primitives. Then, an alignment model learns to ground program primitives into slots through a bi-level contrastive learning objective over a shared embedding space. Finally, we formulate the NSI program generator model to use the dense associations inferred from the alignment model to generate object-centric programs from slots. Experiments on bi-modal retrie
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#38754;&#21521;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#20219;&#21153;&#21644;&#23545;&#40784;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.19404</link><description>&lt;p&gt;
&#38754;&#21521;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26694;&#26550;&#29992;&#20110;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Entity-Aware Multimodal Alignment Framework for News Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19404
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#38754;&#21521;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#20219;&#21153;&#21644;&#23545;&#40784;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#26159;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#19968;&#20010;&#21464;&#20307;&#65292;&#35201;&#27714;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#23383;&#24149;&#65292;&#20854;&#20013;&#21253;&#21547;&#26032;&#38395;&#22270;&#20687;&#21644;&#30456;&#20851;&#26032;&#38395;&#25991;&#31456;&#12290;&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#36805;&#36895;&#65292;&#24182;&#22312;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#24120;&#35265;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35774;&#23450;&#19979;&#29983;&#25104;&#23454;&#20307;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#21363;&#20351;&#22312;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#31616;&#21333;&#24494;&#35843;&#65292;&#23427;&#20204;&#22788;&#29702;&#23454;&#20307;&#20449;&#24687;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#33719;&#24471;&#19968;&#20010;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#23454;&#20307;&#20449;&#24687;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#22810;&#27169;&#24577;&#23454;&#20307;&#24863;&#30693;&#23545;&#40784;&#20219;&#21153;&#21644;&#19968;&#20010;&#23545;&#40784;&#26694;&#26550;&#65292;&#20197;&#23545;&#40784;&#27169;&#22411;&#24182;&#29983;&#25104;&#26032;&#38395;&#22270;&#20687;&#23383;&#24149;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;GoodNews&#25968;&#25454;&#38598;&#19978;&#23558;CIDEr&#20998;&#25968;&#25552;&#39640;&#21040;86.29&#65288;&#20174;72.33&#65289;&#65292;&#22312;NYTimes800k&#25968;&#25454;&#38598;&#19978;&#23558;&#20854;&#25552;&#39640;&#21040;85.61&#65288;&#20174;70.83&#65289;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19404v1 Announce Type: cross  Abstract: News image captioning task is a variant of image captioning task which requires model to generate a more informative caption with news image and the associated news article. Multimodal Large Language models have developed rapidly in recent years and is promising in news image captioning task. However, according to our experiments, common MLLMs are not good at generating the entities in zero-shot setting. Their abilities to deal with the entities information are still limited after simply fine-tuned on news image captioning dataset. To obtain a more powerful model to handle the multimodal entity information, we design two multimodal entity-aware alignment tasks and an alignment framework to align the model and generate the news image captions. Our method achieves better results than previous state-of-the-art models in CIDEr score (72.33 -&gt; 86.29) on GoodNews dataset and (70.83 -&gt; 85.61) on NYTimes800k dataset.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#32676;&#20307;&#27169;&#22411;&#65292;&#26377;&#25928;&#35299;&#24320;&#24739;&#32773;&#23646;&#24615;&#19982;&#30456;&#26426;&#25928;&#26524;&#65292;&#23454;&#29616;&#21487;&#25511;&#19988;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.19186</link><description>&lt;p&gt;
&#29992;&#29983;&#25104;&#27169;&#22411;&#35299;&#24320;&#35270;&#32593;&#33180;&#22270;&#20687;&#30340;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Disentangling representations of retinal images with generative models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19186
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#32676;&#20307;&#27169;&#22411;&#65292;&#26377;&#25928;&#35299;&#24320;&#24739;&#32773;&#23646;&#24615;&#19982;&#30456;&#26426;&#25928;&#26524;&#65292;&#23454;&#29616;&#21487;&#25511;&#19988;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#22312;&#26089;&#26399;&#26816;&#27979;&#30524;&#37096;&#30142;&#30149;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#29978;&#33267;&#34920;&#26126;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#20123;&#22270;&#20687;&#36824;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;&#24515;&#34880;&#31649;&#39118;&#38505;&#22240;&#32032;&#21644;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22270;&#20687;&#21463;&#25216;&#26415;&#22240;&#32032;&#30340;&#24433;&#21709;&#21487;&#33021;&#23545;&#30524;&#31185;&#39046;&#22495;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26500;&#25104;&#25361;&#25112;&#12290;&#20363;&#22914;&#65292;&#22823;&#22411;&#24213;&#22270;&#38431;&#21015;&#24448;&#24448;&#21463;&#21040;&#30456;&#26426;&#31867;&#22411;&#12289;&#22270;&#20687;&#36136;&#37327;&#25110;&#29031;&#26126;&#27700;&#24179;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#23384;&#22312;&#23398;&#20064;&#24555;&#25463;&#26041;&#24335;&#32780;&#19981;&#26159;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#32972;&#21518;&#22240;&#26524;&#20851;&#31995;&#30340;&#39118;&#38505;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#32676;&#20307;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35299;&#24320;&#20102;&#24739;&#32773;&#23646;&#24615;&#19982;&#30456;&#26426;&#25928;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21487;&#25511;&#19988;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36317;&#31163;&#30456;&#20851;&#24615;&#30340;&#26032;&#39062;&#35299;&#24320;&#25439;&#22833;&#12290;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19186v1 Announce Type: cross  Abstract: Retinal fundus images play a crucial role in the early detection of eye diseases and, using deep learning approaches, recent studies have even demonstrated their potential for detecting cardiovascular risk factors and neurological disorders. However, the impact of technical factors on these images can pose challenges for reliable AI applications in ophthalmology. For example, large fundus cohorts are often confounded by factors like camera type, image quality or illumination level, bearing the risk of learning shortcuts rather than the causal relationships behind the image generation process. Here, we introduce a novel population model for retinal fundus images that effectively disentangles patient attributes from camera effects, thus enabling controllable and highly realistic image generation. To achieve this, we propose a novel disentanglement loss based on distance correlation. Through qualitative and quantitative analyses, we demon
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#39532;&#27663;&#36317;&#31163;&#24230;&#37327;&#65292;&#35299;&#20915;&#24191;&#20041;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#25237;&#24433;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25193;&#23637;VAEGAN&#26550;&#26500;&#21644;&#24341;&#20837;&#26032;&#25439;&#22833;&#20989;&#25968;&#20197;&#23454;&#29616;&#26356;&#31283;&#20581;&#30340;&#36317;&#31163;&#23398;&#20064;</title><link>https://arxiv.org/abs/2309.01390</link><description>&lt;p&gt;
&#24357;&#21512;&#25237;&#24433;&#24046;&#36317;&#65306;&#36890;&#36807;&#21442;&#25968;&#21270;&#36317;&#31163;&#23398;&#20064;&#20811;&#26381;&#25237;&#24433;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Bridging the Projection Gap: Overcoming Projection Bias Through Parameterized Distance Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01390
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#39532;&#27663;&#36317;&#31163;&#24230;&#37327;&#65292;&#35299;&#20915;&#24191;&#20041;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#25237;&#24433;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25193;&#23637;VAEGAN&#26550;&#26500;&#21644;&#24341;&#20837;&#26032;&#25439;&#22833;&#20989;&#25968;&#20197;&#23454;&#29616;&#26356;&#31283;&#20581;&#30340;&#36317;&#31163;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;GZSL&#65289;&#26088;&#22312;&#20165;&#21033;&#29992;&#24050;&#30693;&#31867;&#21035;&#26679;&#26412;&#35757;&#32451;&#26469;&#35782;&#21035;&#26469;&#33258;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#30001;&#20110;&#25237;&#24433;&#20989;&#25968;&#26159;&#20174;&#24050;&#30693;&#31867;&#21035;&#20013;&#23398;&#20064;&#30340;&#65292;GZSL&#26041;&#27861;&#24456;&#23481;&#26131;&#20559;&#21521;&#24050;&#30693;&#31867;&#21035;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#33268;&#21147;&#20110;&#23398;&#20064;&#20934;&#30830;&#30340;&#25237;&#24433;&#65292;&#20294;&#25237;&#24433;&#20013;&#30340;&#20559;&#24046;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#39532;&#27663;&#36317;&#31163;&#24230;&#37327;&#26469;&#35299;&#20915;&#35813;&#25237;&#24433;&#20559;&#24046;&#65292;&#20851;&#38190;&#27934;&#23519;&#26159;&#23613;&#31649;&#25237;&#24433;&#23384;&#22312;&#20559;&#24046;&#65292;&#20294;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#36317;&#31163;&#35745;&#31639;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20316;&#20986;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486; - (1)&#25105;&#20204;&#36890;&#36807;&#22686;&#21152;&#20004;&#20010;&#20998;&#25903;&#25193;&#23637;&#20102;VAEGAN&#65288;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65289;&#26550;&#26500;&#65292;&#20998;&#21035;&#36755;&#20986;&#26469;&#33258;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#30340;&#25237;&#24433;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31283;&#20581;&#30340;&#36317;&#31163;&#23398;&#20064;&#12290; (2)&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#39532;&#27663;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01390v2 Announce Type: replace-cross  Abstract: Generalized zero-shot learning (GZSL) aims to recognize samples from both seen and unseen classes using only seen class samples for training. However, GZSL methods are prone to bias towards seen classes during inference due to the projection function being learned from seen classes. Most methods focus on learning an accurate projection, but bias in the projection is inevitable. We address this projection bias by proposing to learn a parameterized Mahalanobis distance metric for robust inference. Our key insight is that the distance computation during inference is critical, even with a biased projection. We make two main contributions - (1) We extend the VAEGAN (Variational Autoencoder \&amp; Generative Adversarial Networks) architecture with two branches to separately output the projection of samples from seen and unseen classes, enabling more robust distance learning. (2) We introduce a novel loss function to optimize the Mahalano
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#32852;&#21512;&#28304;-&#20449;&#36947;&#32534;&#30721;&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#22122;&#22768;&#26080;&#32447;&#20449;&#36947;&#19978;&#36827;&#34892;&#22270;&#20687;&#20256;&#36755;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#33539;&#22260;-&#38646;&#31354;&#38388;&#20998;&#35299;&#21644;&#36880;&#27493;&#20248;&#21270;&#38646;&#31354;&#38388;&#20869;&#23481;&#65292;&#23454;&#29616;&#20102;&#22312;&#22833;&#30495;&#21644;&#24863;&#30693;&#36136;&#37327;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.15889</link><description>&lt;p&gt;
&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#39640;&#24863;&#30693;&#36136;&#37327;&#30340;&#26080;&#32447;&#22270;&#20687;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
High Perceptual Quality Wireless Image Delivery with Denoising Diffusion Models. (arXiv:2309.15889v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#32852;&#21512;&#28304;-&#20449;&#36947;&#32534;&#30721;&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#22122;&#22768;&#26080;&#32447;&#20449;&#36947;&#19978;&#36827;&#34892;&#22270;&#20687;&#20256;&#36755;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#33539;&#22260;-&#38646;&#31354;&#38388;&#20998;&#35299;&#21644;&#36880;&#27493;&#20248;&#21270;&#38646;&#31354;&#38388;&#20869;&#23481;&#65292;&#23454;&#29616;&#20102;&#22312;&#22833;&#30495;&#21644;&#24863;&#30693;&#36136;&#37327;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32852;&#21512;&#28304;-&#20449;&#36947;&#32534;&#30721;&#65288;DeepJSCC&#65289;&#20197;&#21450;&#25509;&#25910;&#31471;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#22312;&#22122;&#22768;&#26080;&#32447;&#20449;&#36947;&#19978;&#36827;&#34892;&#22270;&#20687;&#20256;&#36755;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#22312;&#23454;&#38469;&#26377;&#38480;&#22359;&#38271;&#24230;&#30340;&#24773;&#20917;&#19979;&#30340;&#24863;&#30693;&#22833;&#30495;&#26435;&#34913;&#38382;&#39064;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20998;&#31163;&#30340;&#28304;&#32534;&#30721;&#21644;&#20449;&#36947;&#32534;&#30721;&#21487;&#33021;&#20250;&#39640;&#24230;&#19981;&#29702;&#24819;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#30446;&#26631;&#22270;&#20687;&#30340;&#33539;&#22260;-&#38646;&#31354;&#38388;&#20998;&#35299;&#30340;&#26032;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#32534;&#30721;&#21518;&#20256;&#36755;&#22270;&#20687;&#30340;&#33539;&#22260;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;DDPM&#36880;&#27493;&#20248;&#21270;&#20854;&#38646;&#31354;&#38388;&#20869;&#23481;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#26631;&#20934;&#30340;DeepJSCC&#21644;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#24335;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#37325;&#26500;&#22270;&#20687;&#30340;&#22833;&#30495;&#21644;&#24863;&#30693;&#36136;&#37327;&#26041;&#38754;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#20026;&#20102;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#21487;&#37325;&#29616;&#24615;&#65292;&#25105;&#20204;&#23558;&#20844;&#24320;&#20998;&#20139;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the image transmission problem over a noisy wireless channel via deep learning-based joint source-channel coding (DeepJSCC) along with a denoising diffusion probabilistic model (DDPM) at the receiver. Specifically, we are interested in the perception-distortion trade-off in the practical finite block length regime, in which separate source and channel coding can be highly suboptimal. We introduce a novel scheme that utilizes the range-null space decomposition of the target image. We transmit the range-space of the image after encoding and employ DDPM to progressively refine its null space contents. Through extensive experiments, we demonstrate significant improvements in distortion and perceptual quality of reconstructed images compared to standard DeepJSCC and the state-of-the-art generative learning-based method. We will publicly share our source code to facilitate further research and reproducibility.
&lt;/p&gt;</description></item><item><title>&#22791;&#36873;&#30340;&#21464;&#28966;&#20301;&#31227;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#31227;&#21160;&#21644;&#25193;&#23637;&#29305;&#24449;&#20449;&#24687;&#26469;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21487;&#20197;&#31283;&#20581;&#22320;&#25429;&#25417;&#19981;&#21516;&#27169;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#39640;&#32423;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#27969;&#34892;&#30340;&#22810;&#27169;&#24577;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.16950</link><description>&lt;p&gt;
&#22791;&#36873;&#30340;&#21464;&#28966;&#20301;&#31227;&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Alternative Telescopic Displacement: An Efficient Multimodal Alignment Method. (arXiv:2306.16950v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16950
&lt;/p&gt;
&lt;p&gt;
&#22791;&#36873;&#30340;&#21464;&#28966;&#20301;&#31227;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#31227;&#21160;&#21644;&#25193;&#23637;&#29305;&#24449;&#20449;&#24687;&#26469;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21487;&#20197;&#31283;&#20581;&#22320;&#25429;&#25417;&#19981;&#21516;&#27169;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#39640;&#32423;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#27969;&#34892;&#30340;&#22810;&#27169;&#24577;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#23545;&#40784;&#26159;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#65292;&#21487;&#20197;&#23436;&#20840;&#34701;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#20132;&#26367;&#31227;&#21160;&#21644;&#25193;&#23637;&#26469;&#23454;&#29616;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#19968;&#33268;&#34920;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#31283;&#20581;&#22320;&#25429;&#25417;&#19981;&#21516;&#27169;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#39640;&#32423;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#27969;&#34892;&#30340;&#22810;&#27169;&#24577;&#26041;&#26696;&#12290;&#23545;ETT&#21644;MIT-BIH-Arrhythmia&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature alignment is the primary means of fusing multimodal data. We propose a feature alignment method that fully fuses multimodal information, which alternately shifts and expands feature information from different modalities to have a consistent representation in a feature space. The proposed method can robustly capture high-level interactions between features of different modalities, thus significantly improving the performance of multimodal learning. We also show that the proposed method outperforms other popular multimodal schemes on multiple tasks. Experimental evaluation of ETT and MIT-BIH-Arrhythmia, datasets shows that the proposed method achieves state of the art performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26631;&#23450;&#26041;&#27861;&#65292;&#20351;&#29992;&#28909;&#24230;&#22270;&#22238;&#24402;&#26469;&#28040;&#38500;&#26364;&#21704;&#39039;&#19990;&#30028;&#20551;&#35774;&#19979;&#40060;&#30524;&#22270;&#29255;&#20013;&#27178;&#21521;&#35282;&#24230;&#27495;&#20041;&#65292;&#21516;&#26102;&#24674;&#22797;&#26059;&#36716;&#21644;&#28040;&#38500;&#40060;&#30524;&#22833;&#30495;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20248;&#21270;&#30340;&#23545;&#35282;&#32447;&#28857;&#32531;&#35299;&#22270;&#20687;&#20013;&#32570;&#20047;&#28040;&#22833;&#28857;&#30340;&#24773;&#20917;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.17166</link><description>&lt;p&gt;
&#20351;&#29992;&#28909;&#24230;&#22270;&#22238;&#24402;&#36827;&#34892;&#28145;&#24230;&#21333;&#24352;&#22270;&#29255;&#25668;&#20687;&#26426;&#26631;&#23450;&#65292;&#22312;&#26364;&#21704;&#39039;&#19990;&#30028;&#20551;&#35774;&#19979;&#22312;&#19981;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#36824;&#21407;&#40060;&#30524;&#22270;&#29255;
&lt;/p&gt;
&lt;p&gt;
Deep Single Image Camera Calibration by Heatmap Regression to Recover Fisheye Images Under ManhattanWorld AssumptionWithout Ambiguity. (arXiv:2303.17166v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26631;&#23450;&#26041;&#27861;&#65292;&#20351;&#29992;&#28909;&#24230;&#22270;&#22238;&#24402;&#26469;&#28040;&#38500;&#26364;&#21704;&#39039;&#19990;&#30028;&#20551;&#35774;&#19979;&#40060;&#30524;&#22270;&#29255;&#20013;&#27178;&#21521;&#35282;&#24230;&#27495;&#20041;&#65292;&#21516;&#26102;&#24674;&#22797;&#26059;&#36716;&#21644;&#28040;&#38500;&#40060;&#30524;&#22833;&#30495;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20248;&#21270;&#30340;&#23545;&#35282;&#32447;&#28857;&#32531;&#35299;&#22270;&#20687;&#20013;&#32570;&#20047;&#28040;&#22833;&#28857;&#30340;&#24773;&#20917;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27491;&#20132;&#19990;&#30028;&#22352;&#26631;&#31995;&#20013;&#65292;&#26364;&#21704;&#39039;&#19990;&#30028;&#27839;&#30528;&#38271;&#26041;&#20307;&#24314;&#31569;&#29289;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26364;&#21704;&#39039;&#19990;&#30028;&#38656;&#35201;&#25913;&#36827;&#65292;&#22240;&#20026;&#22270;&#20687;&#20013;&#30340;&#27178;&#21521;&#35282;&#24230;&#30340;&#21407;&#28857;&#26159;&#20219;&#24847;&#30340;&#65292;&#21363;&#20855;&#26377;&#22235;&#20493;&#36718;&#25442;&#23545;&#31216;&#30340;&#27178;&#21521;&#35282;&#24230;&#30340;&#27495;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25668;&#20687;&#26426;&#21644;&#34892;&#39542;&#26041;&#21521;&#30340;&#36947;&#36335;&#26041;&#21521;&#30340;&#24179;&#35282;&#23450;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26631;&#23450;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#28909;&#24230;&#22270;&#22238;&#24402;&#26469;&#28040;&#38500;&#27495;&#20041;&#65292;&#31867;&#20284;&#20110;&#23039;&#24577;&#20272;&#35745;&#20851;&#38190;&#28857;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#20004;&#20010;&#20998;&#25903;&#32593;&#32476;&#24674;&#22797;&#26059;&#36716;&#24182;&#20174;&#19968;&#33324;&#22330;&#26223;&#22270;&#20687;&#20013;&#28040;&#38500;&#40060;&#30524;&#22833;&#30495;&#12290;&#20026;&#20102;&#32531;&#35299;&#22270;&#20687;&#20013;&#32570;&#20047;&#28040;&#22833;&#28857;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20855;&#26377;&#31354;&#38388;&#22343;&#21248;&#24615;&#26368;&#20339;&#30340;&#23545;&#35282;&#32447;&#28857;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26364;&#21704;&#39039;&#19990;&#30028;&#20551;&#35774;&#19979;&#23545;&#40060;&#30524;&#22270;&#20687;&#30340;&#28145;&#24230;&#21333;&#24352;&#22270;&#29255;&#25668;&#20687;&#26426;&#26631;&#23450;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#27809;&#26377;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
In orthogonal world coordinates, a Manhattan world lying along cuboid buildings is widely useful for various computer vision tasks. However, the Manhattan world has much room for improvement because the origin of pan angles from an image is arbitrary, that is, four-fold rotational symmetric ambiguity of pan angles. To address this problem, we propose a definition for the pan-angle origin based on the directions of the roads with respect to a camera and the direction of travel. We propose a learning-based calibration method that uses heatmap regression to remove the ambiguity by each direction of labeled image coordinates, similar to pose estimation keypoints. Simultaneously, our two-branched network recovers the rotation and removes fisheye distortion from a general scene image. To alleviate the lack of vanishing points in images, we introduce auxiliary diagonal points that have the optimal 3D arrangement of spatial uniformity. Extensive experiments demonstrated that our method outperf
&lt;/p&gt;</description></item></channel></rss>