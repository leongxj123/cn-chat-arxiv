<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>InkSight&#26159;&#19968;&#20010;&#21487;&#20197;&#23558;&#31163;&#32447;&#25163;&#20889;&#36716;&#25442;&#20026;&#22312;&#32447;&#25163;&#20889;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#38405;&#35835;&#21644;&#20070;&#20889;&#20808;&#39564;&#30693;&#35782;&#65292;&#22312;&#22810;&#26679;&#21270;&#30340;&#29031;&#29255;&#20013;&#26377;&#25928;&#22320;Derendering&#25163;&#20889;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.05804</link><description>&lt;p&gt;
InkSight&#65306;&#36890;&#36807;&#23398;&#20064;&#38405;&#35835;&#21644;&#20070;&#20889;&#23454;&#29616;&#31163;&#32447;&#21040;&#22312;&#32447;&#25163;&#20889;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
InkSight: Offline-to-Online Handwriting Conversion by Learning to Read and Write
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05804
&lt;/p&gt;
&lt;p&gt;
InkSight&#26159;&#19968;&#20010;&#21487;&#20197;&#23558;&#31163;&#32447;&#25163;&#20889;&#36716;&#25442;&#20026;&#22312;&#32447;&#25163;&#20889;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#38405;&#35835;&#21644;&#20070;&#20889;&#20808;&#39564;&#30693;&#35782;&#65292;&#22312;&#22810;&#26679;&#21270;&#30340;&#29031;&#29255;&#20013;&#26377;&#25928;&#22320;Derendering&#25163;&#20889;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#31508;&#35760;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32784;&#29992;&#12289;&#21487;&#32534;&#36753;&#21644;&#26131;&#20110;&#32034;&#24341;&#30340;&#23384;&#20648;&#31508;&#35760;&#30340;&#26041;&#24335;&#65292;&#21363;&#30690;&#37327;&#21270;&#24418;&#24335;&#30340;&#25968;&#23383;&#22696;&#27700;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31508;&#35760;&#26041;&#24335;&#19982;&#20256;&#32479;&#30340;&#32440;&#31508;&#35760;&#26041;&#24335;&#20043;&#38388;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#32780;&#20256;&#32479;&#32440;&#31508;&#35760;&#26041;&#24335;&#20173;&#21463;&#21040;&#32477;&#22823;&#22810;&#25968;&#20154;&#30340;&#38738;&#30544;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;InkSight&#26088;&#22312;&#24357;&#21512;&#36825;&#31181;&#24046;&#36317;&#65292;&#20351;&#23454;&#20307;&#31508;&#35760;&#32773;&#33021;&#22815;&#36731;&#26494;&#22320;&#23558;&#20182;&#20204;&#30340;&#20316;&#21697;&#65288;&#31163;&#32447;&#25163;&#20889;&#65289;&#36716;&#25442;&#20026;&#25968;&#23383;&#22696;&#27700;&#65288;&#22312;&#32447;&#25163;&#20889;&#65289;&#65292;&#36825;&#20010;&#36807;&#31243;&#25105;&#20204;&#31216;&#20043;&#20026;Derendering&#12290;&#20043;&#21069;&#20851;&#20110;&#27492;&#20027;&#39064;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#22270;&#20687;&#30340;&#20960;&#20309;&#23646;&#24615;&#19978;&#65292;&#23548;&#33268;&#20102;&#22312;&#35757;&#32451;&#39046;&#22495;&#20043;&#22806;&#30340;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#38405;&#35835;&#21644;&#20070;&#20889;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20801;&#35768;&#22312;&#32570;&#20047;&#22823;&#37327;&#37197;&#23545;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#37197;&#23545;&#26679;&#26412;&#24456;&#38590;&#33719;&#21462;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26377;&#25928;&#22320;&#23545;&#20855;&#26377;&#22810;&#26679;&#21270;&#35270;&#35273;&#29305;&#24449;&#21644;&#32972;&#26223;&#30340;&#20219;&#24847;&#29031;&#29255;&#20013;&#30340;&#25163;&#20889;&#25991;&#26412;&#36827;&#34892;Derendering&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital note-taking is gaining popularity, offering a durable, editable, and easily indexable way of storing notes in the vectorized form, known as digital ink. However, a substantial gap remains between this way of note-taking and traditional pen-and-paper note-taking, a practice still favored by a vast majority. Our work, InkSight, aims to bridge the gap by empowering physical note-takers to effortlessly convert their work (offline handwriting) to digital ink (online handwriting), a process we refer to as Derendering. Prior research on the topic has focused on the geometric properties of images, resulting in limited generalization beyond their training domains. Our approach combines reading and writing priors, allowing training a model in the absence of large amounts of paired samples, which are difficult to obtain. To our knowledge, this is the first work that effectively derenders handwritten text in arbitrary photos with diverse visual characteristics and backgrounds. Furthermore,
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#31354;&#24179;&#21488;&#31449;&#65288;HAPS&#65289;&#20351;&#33021;&#30340;&#22402;&#30452;&#24322;&#26500;&#32593;&#32476;&#20013;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#38382;&#39064;&#30340;&#25112;&#30053;&#23458;&#25143;&#36873;&#25321;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#30340;&#32593;&#32476;&#27969;&#37327;&#34892;&#20026;&#39044;&#27979;&#21644;&#20998;&#31867;&#65292;&#20248;&#20808;&#36873;&#25321;&#25968;&#25454;&#21576;&#29616;&#30456;&#20284;&#27169;&#24335;&#30340;&#23458;&#25143;&#21442;&#19982;&#65292;&#20197;&#25552;&#39640;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.05308</link><description>&lt;p&gt;
&#38754;&#23545;HAPS&#20351;&#33021;&#30340;FL&#32593;&#32476;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#65292;&#25112;&#30053;&#23458;&#25143;&#36873;&#25321;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Strategic Client Selection to Address Non-IIDness in HAPS-enabled FL Networks. (arXiv:2401.05308v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05308
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#31354;&#24179;&#21488;&#31449;&#65288;HAPS&#65289;&#20351;&#33021;&#30340;&#22402;&#30452;&#24322;&#26500;&#32593;&#32476;&#20013;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#38382;&#39064;&#30340;&#25112;&#30053;&#23458;&#25143;&#36873;&#25321;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#30340;&#32593;&#32476;&#27969;&#37327;&#34892;&#20026;&#39044;&#27979;&#21644;&#20998;&#31867;&#65292;&#20248;&#20808;&#36873;&#25321;&#25968;&#25454;&#21576;&#29616;&#30456;&#20284;&#27169;&#24335;&#30340;&#23458;&#25143;&#21442;&#19982;&#65292;&#20197;&#25552;&#39640;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30001;&#39640;&#31354;&#24179;&#21488;&#31449;&#65288;HAPS&#65289;&#20351;&#33021;&#30340;&#22402;&#30452;&#24322;&#26500;&#32593;&#32476;&#20013;&#37096;&#32626;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#20026;&#21508;&#31181;&#19981;&#21516;&#36890;&#20449;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#23458;&#25143;&#25552;&#20379;&#20102;&#21442;&#19982;&#30340;&#26426;&#20250;&#12290;&#36825;&#31181;&#22810;&#26679;&#24615;&#19981;&#20165;&#25552;&#39640;&#20102;FL&#27169;&#22411;&#30340;&#35757;&#32451;&#31934;&#24230;&#65292;&#36824;&#21152;&#24555;&#20102;&#20854;&#25910;&#25947;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#24191;&#38420;&#30340;&#32593;&#32476;&#20013;&#24212;&#29992;FL&#23384;&#22312;&#26174;&#33879;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#12290;&#36825;&#31181;&#25968;&#25454;&#24322;&#36136;&#24615;&#24448;&#24448;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#21644;&#27169;&#22411;&#35757;&#32451;&#24615;&#33021;&#30340;&#38477;&#20302;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#27492;&#38382;&#39064;&#30340;&#23458;&#25143;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#29992;&#25143;&#32593;&#32476;&#27969;&#37327;&#34892;&#20026;&#36827;&#34892;&#39044;&#27979;&#21644;&#20998;&#31867;&#12290;&#35813;&#31574;&#30053;&#36890;&#36807;&#25112;&#30053;&#24615;&#36873;&#25321;&#25968;&#25454;&#21576;&#29616;&#30456;&#20284;&#27169;&#24335;&#30340;&#23458;&#25143;&#21442;&#19982;&#65292;&#21516;&#26102;&#20248;&#20808;&#32771;&#34385;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of federated learning (FL) within vertical heterogeneous networks, such as those enabled by high-altitude platform station (HAPS), offers the opportunity to engage a wide array of clients, each endowed with distinct communication and computational capabilities. This diversity not only enhances the training accuracy of FL models but also hastens their convergence. Yet, applying FL in these expansive networks presents notable challenges, particularly the significant non-IIDness in client data distributions. Such data heterogeneity often results in slower convergence rates and reduced effectiveness in model training performance. Our study introduces a client selection strategy tailored to address this issue, leveraging user network traffic behaviour. This strategy involves the prediction and classification of clients based on their network usage patterns while prioritizing user privacy. By strategically selecting clients whose data exhibit similar patterns for participation
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#30475;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;SurgVLP&#65292;&#36890;&#36807;&#21033;&#29992;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#30340;&#35821;&#38899;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.15220</link><description>&lt;p&gt;
&#36890;&#36807;&#35266;&#30475;&#25968;&#30334;&#20010;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures. (arXiv:2307.15220v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15220
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#30475;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;SurgVLP&#65292;&#36890;&#36807;&#21033;&#29992;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#30340;&#35821;&#38899;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22806;&#31185;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#20381;&#38752;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#65292;&#20027;&#35201;&#20351;&#29992;&#35270;&#35273;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#25163;&#26415;&#35270;&#39057;&#26469;&#39044;&#27979;&#19968;&#32452;&#22266;&#23450;&#30340;&#23545;&#35937;&#31867;&#21035;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26410;&#35265;&#25163;&#26415;&#31243;&#24207;&#21644;&#21518;&#32493;&#20219;&#21153;&#19978;&#30340;&#36890;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;&#36890;&#36807;&#24320;&#25918;&#30340;&#25163;&#26415;&#30005;&#23376;&#23398;&#20064;&#24179;&#21488;&#25552;&#20379;&#30340;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#21487;&#20197;&#20026;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#26377;&#25928;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#25163;&#21160;&#27880;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#20114;&#34917;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#29983;&#25104;&#25991;&#26412;&#36716;&#24405;&#26469;&#35299;&#20915;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#23384;&#22312;&#30340;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;SurgVLP - &#25163;&#26415;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#12290;SurgVLP&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#23558;&#35270;&#39057;&#21098;&#36753;&#23884;&#20837;&#19982;&#30456;&#24212;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in surgical computer vision applications have been driven by fully-supervised methods, primarily using only visual data. These methods rely on manually annotated surgical videos to predict a fixed set of object categories, limiting their generalizability to unseen surgical procedures and downstream tasks. In this work, we put forward the idea that the surgical video lectures available through open surgical e-learning platforms can provide effective supervisory signals for multi-modal representation learning without relying on manual annotations. We address the surgery-specific linguistic challenges present in surgical video lectures by employing multiple complementary automatic speech recognition systems to generate text transcriptions. We then present a novel method, SurgVLP - Surgical Vision Language Pre-training, for multi-modal representation learning. SurgVLP constructs a new contrastive learning objective to align video clip embeddings with the corresponding m
&lt;/p&gt;</description></item><item><title>FigCaps-HF&#26159;&#19968;&#20010;&#22270;&#20687;&#29983;&#25104;&#26631;&#39064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#34701;&#20837;&#39046;&#22495;&#19987;&#23478;&#30340;&#21453;&#39304;&#24847;&#35265;&#65292;&#29983;&#25104;&#31526;&#21512;&#35835;&#32773;&#20559;&#22909;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#26631;&#39064;&#12290;&#23558;&#33258;&#21160;&#35780;&#20272;&#21644;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#30340;&#26631;&#39064;&#19982;&#35835;&#32773;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10867</link><description>&lt;p&gt;
FigCaps-HF:&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#22270;&#20687;&#29983;&#25104;&#26631;&#39064;&#26694;&#26550;&#21644;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback. (arXiv:2307.10867v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10867
&lt;/p&gt;
&lt;p&gt;
FigCaps-HF&#26159;&#19968;&#20010;&#22270;&#20687;&#29983;&#25104;&#26631;&#39064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#34701;&#20837;&#39046;&#22495;&#19987;&#23478;&#30340;&#21453;&#39304;&#24847;&#35265;&#65292;&#29983;&#25104;&#31526;&#21512;&#35835;&#32773;&#20559;&#22909;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#26631;&#39064;&#12290;&#23558;&#33258;&#21160;&#35780;&#20272;&#21644;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#30340;&#26631;&#39064;&#19982;&#35835;&#32773;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#39064;&#23545;&#20110;&#29702;&#35299;&#31185;&#23398;&#21487;&#35270;&#21270;&#21644;&#25991;&#26723;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#31185;&#23398;&#22270;&#20687;&#29983;&#25104;&#26631;&#39064;&#26041;&#27861;&#20381;&#36182;&#20110;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#30340;&#22270;&#20687;-&#26631;&#39064;&#37197;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#20854;&#20013;&#35768;&#22810;&#37197;&#23545;&#22312;&#24110;&#21161;&#24615;&#12289;&#35299;&#37322;&#24615;&#21644;&#35270;&#35273;&#25551;&#36848;&#24615;&#31561;&#25351;&#26631;&#19978;&#23384;&#22312;&#19981;&#36275;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#26631;&#39064;&#19982;&#35835;&#32773;&#20559;&#22909;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#26631;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FigCaps-HF&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22270;&#20687;&#29983;&#25104;&#26631;&#39064;&#26694;&#26550;&#65292;&#21487;&#20197;&#34701;&#20837;&#39046;&#22495;&#19987;&#23478;&#30340;&#21453;&#39304;&#24847;&#35265;&#65292;&#20197;&#29983;&#25104;&#20248;&#21270;&#20102;&#35835;&#32773;&#20559;&#22909;&#30340;&#26631;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#21547;1&#65289;&#19968;&#31181;&#35780;&#20272;&#22270;&#20687;-&#26631;&#39064;&#37197;&#23545;&#36136;&#37327;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;2&#65289;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#29983;&#25104;&#24335;&#22270;&#20687;&#29983;&#25104;&#26631;&#39064;&#27169;&#22411;&#20197;&#31526;&#21512;&#35835;&#32773;&#20559;&#22909;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#27169;&#22411;&#19978;&#25913;&#36827;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#31616;&#21333;&#30340;&#23398;&#20064;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Captions are crucial for understanding scientific visualizations and documents. Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness [15] leading to generated captions being misaligned with reader preferences. To enable the generation of high-quality figure captions, we introduce FigCaps-HF a new framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences. Our framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences. We demonstrate the effectiveness of our simple learning framework by improving performance over standard fine-tuning across different types of mod
&lt;/p&gt;</description></item></channel></rss>