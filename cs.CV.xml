<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;</title><link>https://arxiv.org/abs/2403.12027</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#21040;&#27934;&#23519;: &#22312;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12027
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21487;&#35270;&#21270;&#20197;&#22270;&#34920;&#24418;&#24335;&#22312;&#25968;&#25454;&#20998;&#26512;&#20013;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#65292;&#25552;&#20379;&#20851;&#38190;&#27934;&#23519;&#24182;&#24110;&#21161;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#38543;&#30528;&#36817;&#24180;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38761;&#21629;&#65292;&#24182;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#20219;&#21153;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#25506;&#35752;&#20102;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12027v1 Announce Type: cross  Abstract: Data visualization in the form of charts plays a pivotal role in data analysis, offering critical insights and aiding in informed decision-making. Automatic chart understanding has witnessed significant advancements with the rise of large foundation models in recent years. Foundation models, such as large language models (LLMs), have revolutionized various natural language processing (NLP) tasks and are increasingly being applied to chart understanding tasks. This survey paper provides a comprehensive overview of the recent developments, challenges, and future directions in chart understanding within the context of these foundation models. The paper begins by defining chart understanding, outlining problem formulations, and discussing fundamental building blocks crucial for studying chart understanding tasks. In the section on tasks and datasets, we explore various tasks within chart understanding and discuss their evaluation metrics a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23454;&#26102;&#21464;&#21387;&#22120;&#30340;&#24320;&#35789;&#27719;&#26816;&#27979;&#27169;&#22411;OmDet-Turbo&#65292;&#20855;&#26377;&#39640;&#25928;&#34701;&#21512;&#22836;&#27169;&#22359;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#30417;&#30563;&#27169;&#22411;&#20960;&#20046;&#25345;&#24179;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.06892</link><description>&lt;p&gt;
&#22522;&#20110;&#23454;&#26102;&#21464;&#21387;&#22120;&#30340;&#39640;&#25928;&#34701;&#21512;&#22836;&#24320;&#35789;&#27719;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23454;&#26102;&#21464;&#21387;&#22120;&#30340;&#24320;&#35789;&#27719;&#26816;&#27979;&#27169;&#22411;OmDet-Turbo&#65292;&#20855;&#26377;&#39640;&#25928;&#34701;&#21512;&#22836;&#27169;&#22359;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#30417;&#30563;&#27169;&#22411;&#20960;&#20046;&#25345;&#24179;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31471;&#21040;&#31471;&#21464;&#21387;&#22120;&#30340;&#26816;&#27979;&#22120;&#65288;DETRs&#65289;&#36890;&#36807;&#25972;&#21512;&#35821;&#35328;&#27169;&#24577;&#65292;&#22312;&#23553;&#38381;&#38598;&#21644;&#24320;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#65288;OVD&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#39640;&#35201;&#27714;&#30340;&#35745;&#31639;&#38656;&#27714;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#65288;OD&#65289;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#23457;&#26597;&#20102;OVDEval&#22522;&#20934;&#27979;&#35797;&#20013;&#20004;&#20010;&#39046;&#20808;&#27169;&#22411;OmDet&#21644;Grounding-DINO&#30340;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;OmDet-Turbo&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#23454;&#26102;OVD&#27169;&#22411;&#20855;&#26377;&#21019;&#26032;&#30340;&#39640;&#25928;&#34701;&#21512;&#22836;&#65288;EFH&#65289;&#27169;&#22359;&#65292;&#26088;&#22312;&#32531;&#35299;OmDet&#21644;Grounding-DINO&#20013;&#35266;&#23519;&#21040;&#30340;&#29942;&#39048;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;OmDet-Turbo-Base&#22312;&#24212;&#29992;TensorRT&#21644;&#35821;&#35328;&#32531;&#23384;&#25216;&#26415;&#21518;&#23454;&#29616;&#20102;100.2&#24103;&#27599;&#31186;&#65288;FPS&#65289;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;COCO&#21644;LVIS&#25968;&#25454;&#38598;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#20013;&#65292;OmDet-Turbo&#30340;&#24615;&#33021;&#20960;&#20046;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#27169;&#22411;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06892v1 Announce Type: cross  Abstract: End-to-end transformer-based detectors (DETRs) have shown exceptional performance in both closed-set and open-vocabulary object detection (OVD) tasks through the integration of language modalities. However, their demanding computational requirements have hindered their practical application in real-time object detection (OD) scenarios. In this paper, we scrutinize the limitations of two leading models in the OVDEval benchmark, OmDet and Grounding-DINO, and introduce OmDet-Turbo. This novel transformer-based real-time OVD model features an innovative Efficient Fusion Head (EFH) module designed to alleviate the bottlenecks observed in OmDet and Grounding-DINO. Notably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with TensorRT and language cache techniques applied. Notably, in zero-shot scenarios on COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on par with current state-of-the-art supervised models. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22343;&#36136;&#21644;&#24322;&#36136;&#23454;&#20363;&#32423;&#21035;&#32467;&#26500;&#65292;&#26500;&#24314;&#39640;&#36136;&#37327;&#36328;&#27169;&#24577;&#26631;&#31614;&#20851;&#32852;&#30340;&#27169;&#24577;&#32479;&#19968;&#26631;&#31614;&#20256;&#36755;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.00672</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#30340;&#22343;&#36136;&#21644;&#24322;&#36136;&#19968;&#33268;&#26631;&#31614;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised Visible-Infrared Person ReID
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22343;&#36136;&#21644;&#24322;&#36136;&#23454;&#20363;&#32423;&#21035;&#32467;&#26500;&#65292;&#26500;&#24314;&#39640;&#36136;&#37327;&#36328;&#27169;&#24577;&#26631;&#31614;&#20851;&#32852;&#30340;&#27169;&#24577;&#32479;&#19968;&#26631;&#31614;&#20256;&#36755;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#29289;&#37325;&#26032;&#35782;&#21035;&#65288;USL-VI-ReID&#65289;&#26088;&#22312;&#26080;&#38656;&#27880;&#37322;&#20174;&#19981;&#21516;&#27169;&#24577;&#20013;&#26816;&#32034;&#30456;&#21516;&#36523;&#20221;&#30340;&#34892;&#20154;&#22270;&#20687;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#24314;&#31435;&#36328;&#27169;&#24577;&#30340;&#20266;&#26631;&#31614;&#20851;&#32852;&#20197;&#24357;&#21512;&#27169;&#24577;&#38388;&#30340;&#24046;&#24322;&#65292;&#20294;&#24573;&#30053;&#20102;&#22312;&#20266;&#26631;&#31614;&#31354;&#38388;&#20013;&#20445;&#25345;&#23454;&#20363;&#32423;&#21035;&#30340;&#22343;&#36136;&#21644;&#24322;&#36136;&#19968;&#33268;&#24615;&#65292;&#23548;&#33268;&#20851;&#32852;&#31895;&#31961;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#24577;&#32479;&#19968;&#26631;&#31614;&#20256;&#36755;&#65288;MULT&#65289;&#27169;&#22359;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22343;&#36136;&#21644;&#24322;&#36136;&#32454;&#31890;&#24230;&#23454;&#20363;&#32423;&#32467;&#26500;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#36328;&#27169;&#24577;&#26631;&#31614;&#20851;&#32852;&#12290;&#23427;&#24314;&#27169;&#20102;&#22343;&#36136;&#21644;&#24322;&#36136;&#30340;&#20851;&#32852;&#24615;&#65292;&#21033;&#29992;&#23427;&#20204;&#23450;&#20041;&#20266;&#26631;&#31614;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#28982;&#21518;&#26368;&#23567;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#32500;&#25345;&#20102;&#36328;&#27169;&#24577;&#30340;&#23545;&#40784;&#24182;&#20445;&#25345;&#20102;&#20869;&#37096;&#27169;&#24577;&#32467;&#26500;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#26377;&#19968;&#20010;&#31616;&#21333;&#26131;&#29992;&#30340;&#22312;&#32447;&#20132;&#21449;&#35760;&#24518;&#26631;&#31614;&#24341;&#29992;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to retrieve pedestrian images of the same identity from different modalities without annotations. While prior work focuses on establishing cross-modality pseudo-label associations to bridge the modality-gap, they ignore maintaining the instance-level homogeneous and heterogeneous consistency in pseudo-label space, resulting in coarse associations. In response, we introduce a Modality-Unified Label Transfer (MULT) module that simultaneously accounts for both homogeneous and heterogeneous fine-grained instance-level structures, yielding high-quality cross-modality label associations. It models both homogeneous and heterogeneous affinities, leveraging them to define the inconsistency for the pseudo-labels and then minimize it, leading to pseudo-labels that maintain alignment across modalities and consistency within intra-modality structures. Additionally, a straightforward plug-and-play Online Cross-memory Label Ref
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#24341;&#20837;&#22122;&#22768;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#22312;&#38477;&#20302;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#26465;&#20214;&#19979;&#25552;&#21319;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.10625</link><description>&lt;p&gt;
&#25506;&#32034;&#23398;&#20064;&#31995;&#32479;&#20013;&#20449;&#24687;&#29109;&#21464;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Influence of Information Entropy Change in Learning Systems. (arXiv:2309.10625v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#24341;&#20837;&#22122;&#22768;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#22312;&#38477;&#20302;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#26465;&#20214;&#19979;&#25552;&#21319;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21521;&#36755;&#20837;/&#38544;&#21547;&#29305;&#24449;&#28155;&#21152;&#22122;&#22768;&#26469;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#29109;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#30340;&#24212;&#29992;&#37325;&#28857;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#65292;&#20294;&#25152;&#25552;&#20986;&#30340;&#29702;&#35770;&#21487;&#20197;&#36827;&#19968;&#27493;&#24212;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#12290;&#22122;&#22768;&#36890;&#24120;&#34987;&#35270;&#20026;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#35270;&#35273;&#21464;&#25442;&#22120;&#65289;&#20197;&#21450;&#22270;&#20687;&#20998;&#31867;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#19981;&#21516;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#23475;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#26088;&#22312;&#37325;&#26032;&#24605;&#32771;&#20256;&#32479;&#21629;&#39064;&#26159;&#21542;&#24635;&#26159;&#25104;&#31435;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#29305;&#23450;&#22122;&#22768;&#21487;&#20197;&#25552;&#21319;&#21508;&#31181;&#28145;&#24230;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20449;&#24687;&#29109;&#23450;&#20041;&#30340;&#20219;&#21153;&#22797;&#26434;&#24615;&#20943;&#23569;&#26041;&#38754;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#27491;&#22122;&#22768;&#30340;&#22686;&#24378;&#25928;&#26524;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#65289;&#20013;&#23454;&#39564;&#35777;&#26126;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore the influence of entropy change in deep learning systems by adding noise to the inputs/latent features. The applications in this paper focus on deep learning tasks within computer vision, but the proposed theory can be further applied to other fields. Noise is conventionally viewed as a harmful perturbation in various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers (ViTs), as well as different learning tasks like image classification and transfer learning. However, this paper aims to rethink whether the conventional proposition always holds. We demonstrate that specific noise can boost the performance of various deep architectures under certain conditions. We theoretically prove the enhancement gained from positive noise by reducing the task complexity defined by information entropy and experimentally show the significant performance gain in large image datasets, such as the ImageNet. Herein, we use the informat
&lt;/p&gt;</description></item><item><title>Bongard&#38382;&#39064;&#26159;&#19968;&#31181;&#38656;&#35201;&#20174;&#19968;&#32452;&#27491;&#36127;&#22270;&#20687;&#20013;&#25512;&#23548;&#20986;&#25277;&#35937;&#27010;&#24565;&#24182;&#36827;&#34892;&#20998;&#31867;&#30340;&#26234;&#21147;&#27979;&#35797;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;Bongard&#38382;&#39064;&#20013;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#26159;&#22240;&#20026;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#25972;&#21512;&#25903;&#25345;&#38598;&#21512;&#20013;&#30340;&#20449;&#24687;&#65292;&#32780;&#26159;&#20165;&#20381;&#36182;&#20110;&#21333;&#20010;&#25903;&#25345;&#22270;&#20687;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.03468</link><description>&lt;p&gt;
&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#23545;&#20110;Bongard&#38382;&#39064;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Cross-Image Context Matters for Bongard Problems. (arXiv:2309.03468v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03468
&lt;/p&gt;
&lt;p&gt;
Bongard&#38382;&#39064;&#26159;&#19968;&#31181;&#38656;&#35201;&#20174;&#19968;&#32452;&#27491;&#36127;&#22270;&#20687;&#20013;&#25512;&#23548;&#20986;&#25277;&#35937;&#27010;&#24565;&#24182;&#36827;&#34892;&#20998;&#31867;&#30340;&#26234;&#21147;&#27979;&#35797;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;Bongard&#38382;&#39064;&#20013;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#26159;&#22240;&#20026;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#25972;&#21512;&#25903;&#25345;&#38598;&#21512;&#20013;&#30340;&#20449;&#24687;&#65292;&#32780;&#26159;&#20165;&#20381;&#36182;&#20110;&#21333;&#20010;&#25903;&#25345;&#22270;&#20687;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;Bongard&#38382;&#39064;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;Bongard&#38382;&#39064;&#26159;&#19968;&#31181;&#38656;&#35201;&#20174;&#19968;&#32452;&#27491;&#36127;&#8220;&#25903;&#25345;&#8221;&#22270;&#20687;&#20013;&#25512;&#23548;&#20986;&#25277;&#35937;&#8220;&#27010;&#24565;&#8221;&#65292;&#28982;&#21518;&#23545;&#20110;&#26032;&#30340;&#26597;&#35810;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#21028;&#26029;&#23427;&#26159;&#21542;&#25551;&#36848;&#20102;&#20851;&#38190;&#27010;&#24565;&#30340;&#26234;&#21147;&#27979;&#35797;&#12290;&#22312;&#29992;&#20110;&#33258;&#28982;&#22270;&#20687;Bongard&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;Bongard-HOI&#20013;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#20165;&#36798;&#21040;&#20102;66%&#65288;&#20598;&#28982;&#20934;&#30830;&#29575;&#20026;50%&#65289;&#12290;&#20302;&#20934;&#30830;&#29575;&#36890;&#24120;&#24402;&#22240;&#20110;&#31070;&#32463;&#32593;&#32476;&#32570;&#20047;&#21457;&#29616;&#31867;&#20284;&#20154;&#31867;&#31526;&#21495;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#38382;&#39064;&#32780;&#22833;&#21435;&#20102;&#20934;&#30830;&#24615;&#65306;&#23427;&#20204;&#27809;&#26377;&#23558;&#25903;&#25345;&#38598;&#21512;&#20013;&#30340;&#20449;&#24687;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#21152;&#20837;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#20174;&#21333;&#20010;&#25903;&#25345;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#12290;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22240;&#20026;&#19982;&#28041;&#21450;&#23545;&#35937;&#20998;&#31867;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#19981;&#21516;&#65292;&#19968;&#20010;&#20856;&#22411;&#30340;Bongard&#38382;&#39064;&#20013;&#30340;&#8220;&#20851;&#38190;&#27010;&#24565;&#8221;&#21482;&#33021;&#20351;&#29992;&#22810;&#20010;&#27491;&#20363;&#21644;&#22810;&#20010;&#21453;&#20363;&#26469;&#21306;&#20998;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current machine learning methods struggle to solve Bongard problems, which are a type of IQ test that requires deriving an abstract "concept" from a set of positive and negative "support" images, and then classifying whether or not a new query image depicts the key concept. On Bongard-HOI, a benchmark for natural-image Bongard problems, existing methods have only reached 66% accuracy (where chance is 50%). Low accuracy is often attributed to neural nets' lack of ability to find human-like symbolic rules. In this work, we point out that many existing methods are forfeiting accuracy due to a much simpler problem: they do not incorporate information contained in the support set as a whole, and rely instead on information extracted from individual supports. This is a critical issue, because unlike in few-shot learning tasks concerning object classification, the "key concept" in a typical Bongard problem can only be distinguished using multiple positives and multiple negatives. We explore a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#23545;&#20110;&#24066;&#22330;&#33829;&#38144;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#19968;&#30452;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#21442;&#19982;&#32773;&#21644;&#24191;&#21578;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#20160;&#20040;&#20351;&#24191;&#21578;&#35760;&#24518;&#28145;&#21051;&#30340;&#26377;&#36259;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.00378</link><description>&lt;p&gt;
&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Long-Term Memorability On Advertisements. (arXiv:2309.00378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#23545;&#20110;&#24066;&#22330;&#33829;&#38144;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#19968;&#30452;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#21442;&#19982;&#32773;&#21644;&#24191;&#21578;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#20160;&#20040;&#20351;&#24191;&#21578;&#35760;&#24518;&#28145;&#21051;&#30340;&#26377;&#36259;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24066;&#22330;&#33829;&#38144;&#20154;&#21592;&#33457;&#36153;&#25968;&#21313;&#20159;&#32654;&#20803;&#22312;&#24191;&#21578;&#19978;&#65292;&#20294;&#26159;&#25237;&#20837;&#21040;&#24191;&#21578;&#19978;&#30340;&#37329;&#38065;&#33021;&#36215;&#22810;&#22823;&#20316;&#29992;&#21602;&#65311;&#24403;&#39038;&#23458;&#22312;&#36141;&#20080;&#26102;&#26080;&#27861;&#36776;&#35748;&#20986;&#20182;&#20204;&#30475;&#36807;&#30340;&#21697;&#29260;&#30340;&#35805;&#65292;&#33457;&#22312;&#24191;&#21578;&#19978;&#30340;&#38065;&#22522;&#26412;&#19978;&#23601;&#34987;&#28010;&#36153;&#20102;&#12290;&#23613;&#31649;&#22312;&#33829;&#38144;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#25991;&#29486;&#20013;&#36824;&#27809;&#26377;&#20851;&#20110;&#24191;&#21578;&#35760;&#24518;&#21147;&#30340;&#30740;&#31350;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#26159;&#23545;&#29305;&#23450;&#20869;&#23481;&#31867;&#22411;&#65288;&#22914;&#29289;&#20307;&#21644;&#21160;&#20316;&#35270;&#39057;&#65289;&#36827;&#34892;&#30701;&#26399;&#22238;&#24518;&#65288;&lt;5&#20998;&#38047;&#65289;&#30340;&#30740;&#31350;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24191;&#21578;&#34892;&#19994;&#21482;&#20851;&#24515;&#38271;&#26399;&#35760;&#24518;&#65288;&#20960;&#20010;&#23567;&#26102;&#25110;&#26356;&#38271;&#26102;&#38388;&#65289;&#65292;&#32780;&#19988;&#24191;&#21578;&#20960;&#20046;&#24635;&#26159;&#39640;&#24230;&#22810;&#27169;&#24335;&#21270;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#24418;&#24335;&#65288;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#26469;&#35762;&#25925;&#20107;&#12290;&#22522;&#20110;&#36825;&#19968;&#21160;&#26426;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#20010;&#22823;&#35268;&#27169;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#20849;&#26377;1203&#21517;&#21442;&#19982;&#32773;&#21644;2205&#20010;&#24191;&#21578;&#28085;&#30422;&#20102;276&#20010;&#21697;&#29260;&#12290;&#22312;&#19981;&#21516;&#21442;&#19982;&#32773;&#23376;&#32676;&#20307;&#21644;&#24191;&#21578;&#31867;&#22411;&#19978;&#36827;&#34892;&#32479;&#35745;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#35768;&#22810;&#26377;&#20851;&#20160;&#20040;&#20351;&#24191;&#21578;&#38590;&#24536;&#30340;&#26377;&#36259;&#35265;&#35299;-&#26080;&#35770;&#26159;&#20869;&#23481;&#36824;&#26159;
&lt;/p&gt;
&lt;p&gt;
Marketers spend billions of dollars on advertisements but to what end? At the purchase time, if customers cannot recognize a brand for which they saw an ad, the money spent on the ad is essentially wasted. Despite its importance in marketing, until now, there has been no study on the memorability of ads in the ML literature. Most studies have been conducted on short-term recall (&lt;5 mins) on specific content types like object and action videos. On the other hand, the advertising industry only cares about long-term memorability (a few hours or longer), and advertisements are almost always highly multimodal, depicting a story through its different modalities (text, images, and videos). With this motivation, we conduct the first large scale memorability study consisting of 1203 participants and 2205 ads covering 276 brands. Running statistical tests over different participant subpopulations and ad-types, we find many interesting insights into what makes an ad memorable - both content and h
&lt;/p&gt;</description></item><item><title>MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02490</link><description>&lt;p&gt;
MM-Vet: &#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#32508;&#21512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02490
&lt;/p&gt;
&lt;p&gt;
MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#26816;&#26597;&#22312;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#34920;&#29616;&#12290;&#26368;&#36817;&#30340;LMM&#23637;&#31034;&#20102;&#21508;&#31181;&#26377;&#36259;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#35299;&#20915;&#20070;&#20889;&#22312;&#40657;&#26495;&#19978;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#25512;&#29702;&#26032;&#38395;&#22270;&#29255;&#20013;&#30340;&#20107;&#20214;&#21644;&#21517;&#20154;&#65292;&#20197;&#21450;&#35299;&#37322;&#35270;&#35273;&#31505;&#35805;&#12290;&#24555;&#36895;&#30340;&#27169;&#22411;&#36827;&#27493;&#32473;&#35780;&#20272;&#26631;&#20934;&#30340;&#24320;&#21457;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#38382;&#39064;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22914;&#20309;&#31995;&#32479;&#22320;&#26500;&#24314;&#21644;&#35780;&#20272;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65307;&#65288;2&#65289;&#22914;&#20309;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#21644;&#22238;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#65307;&#65288;3&#65289;&#22914;&#20309;&#32473;&#20986;&#36229;&#20986;&#31616;&#21333;&#24615;&#33021;&#25490;&#21517;&#30340;&#27169;&#22411;&#27934;&#23519;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#27934;&#23519;&#65306;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26377;&#36259;&#33021;&#21147;&#36890;&#24120;&#36890;&#36807;&#19968;&#31181;&#36890;&#25165;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;MM-Vet&#23450;&#20041;&#20102;6&#20010;&#26680;&#24515;VL&#33021;&#21147;&#65292;&#24182;&#26816;&#26597;&#20102;&#20174;&#36825;&#20123;&#33021;&#21147;&#32452;&#21512;&#20013;&#24471;&#20986;&#30340;16&#31181;&#26377;&#36259;&#30340;&#25972;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30452;&#25509;&#35780;&#20272;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#65292;&#24182;&#20998;&#26512;&#23427;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24863;&#30693;&#12290;&#36890;&#36807;&#23637;&#31034;&#20855;&#26377;&#26356;&#20016;&#23500;&#32479;&#35745;&#29305;&#24449;&#30340;&#33258;&#28982;&#22270;&#20687;&#34987;&#24863;&#30693;&#20026;&#20855;&#26377;&#26356;&#22823;&#30340;&#26174;&#30528;&#24615;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#30452;&#25509;&#25903;&#25345;Barlow&#21644;Attneave&#29702;&#35770;&#30340;&#35777;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22270;&#20687;&#32479;&#35745;&#19982;&#30693;&#35273;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.09874</link><description>&lt;p&gt;
&#22270;&#20687;&#32479;&#35745;&#19982;&#20154;&#31867;&#24863;&#30693;&#20043;&#38388;&#30340;&#20851;&#32852;&#20851;&#31995;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Disentangling the Link Between Image Statistics and Human Perception. (arXiv:2303.09874v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30452;&#25509;&#35780;&#20272;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#65292;&#24182;&#20998;&#26512;&#23427;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24863;&#30693;&#12290;&#36890;&#36807;&#23637;&#31034;&#20855;&#26377;&#26356;&#20016;&#23500;&#32479;&#35745;&#29305;&#24449;&#30340;&#33258;&#28982;&#22270;&#20687;&#34987;&#24863;&#30693;&#20026;&#20855;&#26377;&#26356;&#22823;&#30340;&#26174;&#30528;&#24615;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#30452;&#25509;&#25903;&#25345;Barlow&#21644;Attneave&#29702;&#35770;&#30340;&#35777;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22270;&#20687;&#32479;&#35745;&#19982;&#30693;&#35273;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;20&#19990;&#32426;50&#24180;&#20195;&#65292;&#38669;&#21202;&#26031;&#24052;&#27931;&#21644;&#24343;&#38647;&#24503;&#38463;&#29305;&#32435;&#22827;&#25552;&#20986;&#20102;&#24863;&#23448;&#31995;&#32479;&#21644;&#23427;&#20204;&#22914;&#20309;&#36866;&#24212;&#29615;&#22659;&#20043;&#38388;&#30340;&#20851;&#31995;&#65306;&#26089;&#26399;&#35270;&#35273;&#30340;&#36827;&#21270;&#26159;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20256;&#36882;&#20851;&#20110;&#36755;&#20837;&#20449;&#21495;&#30340;&#20449;&#24687;&#12290;&#25353;&#29031;&#39321;&#20892;&#30340;&#23450;&#20041;&#65292;&#36825;&#20123;&#20449;&#24687;&#26159;&#36890;&#36807;&#33258;&#28982;&#22330;&#26223;&#20013;&#25293;&#25668;&#30340;&#22270;&#20687;&#30340;&#27010;&#29575;&#26469;&#25551;&#36848;&#30340;&#12290;&#30001;&#20110;&#35745;&#31639;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#20197;&#21069;&#26080;&#27861;&#30452;&#25509;&#20934;&#30830;&#22320;&#39044;&#27979;&#22270;&#20687;&#30340;&#27010;&#29575;&#12290;&#23613;&#31649;&#36825;&#31181;&#24819;&#27861;&#30340;&#25506;&#32034;&#26159;&#38388;&#25509;&#30340;&#65292;&#20027;&#35201;&#22522;&#20110;&#22270;&#20687;&#23494;&#24230;&#30340;&#36807;&#24230;&#31616;&#21270;&#27169;&#22411;&#25110;&#31995;&#32479;&#35774;&#35745;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#37325;&#29616;&#21508;&#31181;&#29983;&#29702;&#21644;&#24515;&#29702;&#29289;&#29702;&#29616;&#35937;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30452;&#25509;&#35780;&#20272;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#65292;&#24182;&#20998;&#26512;&#23427;&#22914;&#20309;&#30830;&#23450;&#30693;&#35273;&#28789;&#25935;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;&#20154;&#31867;&#24847;&#35265;&#30456;&#20851;&#24615;&#24456;&#39640;&#30340;&#22270;&#20687;&#36136;&#37327;&#25351;&#26631;&#20316;&#20026;&#20154;&#31867;&#35270;&#35273;&#30340;&#20195;&#29702;&#65292;&#20197;&#21450;&#19968;&#20010;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#30452;&#25509;&#20272;&#35745;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26681;&#25454;Barlow&#21644;Attneave&#29702;&#35770;&#39044;&#27979;&#30340;&#22270;&#20687;&#32479;&#35745;&#19982;&#20154;&#31867;&#30693;&#35273;&#20043;&#38388;&#23384;&#22312;&#31995;&#32479;&#24615;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#20855;&#26377;&#26356;&#20016;&#23500;&#32479;&#35745;&#29305;&#24449;&#30340;&#33258;&#28982;&#22270;&#20687;&#34987;&#24863;&#30693;&#20026;&#20855;&#26377;&#26356;&#22823;&#30340;&#26174;&#30528;&#24615;&#26469;&#35828;&#26126;&#36825;&#19968;&#21457;&#29616;&#65292;&#36825;&#26159;&#36890;&#36807;&#35270;&#35273;&#25628;&#32034;&#23454;&#39564;&#27979;&#37327;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#30452;&#25509;&#25903;&#25345;Barlow&#21644;Attneave&#29702;&#35770;&#30340;&#35777;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22270;&#20687;&#32479;&#35745;&#19982;&#30693;&#35273;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the 1950s Horace Barlow and Fred Attneave suggested a connection between sensory systems and how they are adapted to the environment: early vision evolved to maximise the information it conveys about incoming signals. Following Shannon's definition, this information was described using the probability of the images taken from natural scenes. Previously, direct accurate predictions of image probabilities were not possible due to computational limitations. Despite the exploration of this idea being indirect, mainly based on oversimplified models of the image density or on system design methods, these methods had success in reproducing a wide range of physiological and psychophysical phenomena. In this paper, we directly evaluate the probability of natural images and analyse how it may determine perceptual sensitivity. We employ image quality metrics that correlate well with human opinion as a surrogate of human vision, and an advanced generative model to directly estimate the probabil
&lt;/p&gt;</description></item></channel></rss>