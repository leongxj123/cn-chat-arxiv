<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26032;&#39062;&#30340;&#26694;&#26550;SFFormer&#65292;&#32467;&#21512;&#20102;&#22810;&#22836;&#20132;&#21449;&#27880;&#24847;&#21147;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#22522;&#20110;dMRI&#32420;&#32500;&#26463;&#36861;&#36394;&#65292;&#39044;&#27979;&#20102;&#20027;&#35266;&#35821;&#35328;&#34920;&#29616;&#65292;&#25299;&#23637;&#20102;&#33041;&#32467;&#26500;&#19982;&#20154;&#31867;&#35748;&#30693;&#21151;&#33021;&#30340;&#20851;&#32852;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.19001</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#30340;&#32420;&#32500;&#31751;&#24418;&#29366;&#20998;&#26512;&#29992;&#20110;&#35821;&#35328;&#34920;&#29616;&#35748;&#30693;&#20998;&#25968;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cross--domain Fiber Cluster Shape Analysis for Language Performance Cognitive Score Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26032;&#39062;&#30340;&#26694;&#26550;SFFormer&#65292;&#32467;&#21512;&#20102;&#22810;&#22836;&#20132;&#21449;&#27880;&#24847;&#21147;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#22522;&#20110;dMRI&#32420;&#32500;&#26463;&#36861;&#36394;&#65292;&#39044;&#27979;&#20102;&#20027;&#35266;&#35821;&#35328;&#34920;&#29616;&#65292;&#25299;&#23637;&#20102;&#33041;&#32467;&#26500;&#19982;&#20154;&#31867;&#35748;&#30693;&#21151;&#33021;&#30340;&#20851;&#32852;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#23545;&#35937;&#24418;&#24577;&#21644;&#21151;&#33021;&#30340;&#20449;&#24687;&#29305;&#24449;&#12290;&#33041;&#25104;&#20687;&#20013;&#30340;&#24418;&#29366;&#20998;&#26512;&#21487;&#24110;&#21161;&#35299;&#37322;&#20154;&#33041;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#33041;&#30340;3D&#30333;&#36136;&#36830;&#25509;&#30340;&#24418;&#29366;&#21450;&#20854;&#19982;&#20154;&#31867;&#35748;&#30693;&#21151;&#33021;&#30340;&#28508;&#22312;&#39044;&#27979;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#25193;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#32420;&#32500;&#26463;&#36861;&#36394;&#23558;&#22823;&#33041;&#36830;&#25509;&#37325;&#24314;&#20026;3D&#28857;&#24207;&#21015;&#12290;&#20026;&#20102;&#25551;&#36848;&#27599;&#20010;&#36830;&#25509;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;12&#20010;&#24418;&#29366;&#25551;&#36848;&#31526;&#20197;&#21450;&#20256;&#32479;&#30340;dMRI&#36830;&#25509;&#21644;&#32452;&#32455;&#24494;&#32467;&#26500;&#29305;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#24418;&#29366;&#34701;&#21512;&#32420;&#32500;&#31751;&#21464;&#25442;&#22120;&#65288;SFFormer&#65289;&#65292;&#21033;&#29992;&#22810;&#22836;&#20132;&#21449;&#27880;&#24847;&#21147;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#22522;&#20110;dMRI&#32420;&#32500;&#26463;&#36861;&#36394;&#26469;&#39044;&#27979;&#29305;&#23450;&#20010;&#20307;&#30340;&#35821;&#35328;&#34920;&#29616;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19001v1 Announce Type: cross  Abstract: Shape plays an important role in computer graphics, offering informative features to convey an object's morphology and functionality. Shape analysis in brain imaging can help interpret structural and functionality correlations of the human brain. In this work, we investigate the shape of the brain's 3D white matter connections and its potential predictive relationship to human cognitive function. We reconstruct brain connections as sequences of 3D points using diffusion magnetic resonance imaging (dMRI) tractography. To describe each connection, we extract 12 shape descriptors in addition to traditional dMRI connectivity and tissue microstructure features. We introduce a novel framework, Shape--fused Fiber Cluster Transformer (SFFormer), that leverages a multi-head cross-attention feature fusion module to predict subject-specific language performance based on dMRI tractography. We assess the performance of the method on a large dataset
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cobweb4V&#30340;&#26032;&#39062;&#35270;&#35273;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#31867;&#31867;&#20284;&#23398;&#20064;&#31995;&#32479;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#25928;&#24212;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#25104;&#26524;&#65292;&#24182;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16933</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#27010;&#24565;&#24418;&#25104;&#36991;&#20813;&#35270;&#35273;&#20998;&#31867;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Avoiding Catastrophic Forgetting in Visual Classification Using Human Concept Formation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16933
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cobweb4V&#30340;&#26032;&#39062;&#35270;&#35273;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#31867;&#31867;&#20284;&#23398;&#20064;&#31995;&#32479;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#25928;&#24212;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#25104;&#26524;&#65292;&#24182;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#28982;&#32780;&#65292;&#24403;&#25353;&#39034;&#24207;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#23427;&#20204;&#32463;&#24120;&#38754;&#20020;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Cobweb4V&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;&#20998;&#31867;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;Cobweb&#65292;&#36825;&#26159;&#19968;&#31181;&#20154;&#31867;&#31867;&#20284;&#30340;&#23398;&#20064;&#31995;&#32479;&#65292;&#21463;&#21040;&#20154;&#31867;&#38543;&#26102;&#38388;&#36880;&#28176;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;Cobweb4V&#22312;&#23398;&#20064;&#35270;&#35273;&#27010;&#24565;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#23398;&#20064;&#25104;&#26524;&#65292;&#38543;&#26102;&#38388;&#20445;&#25345;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#25928;&#24212;&#12290;&#36825;&#20123;&#29305;&#24449;&#19982;&#20154;&#31867;&#35748;&#30693;&#20013;&#30340;&#23398;&#20064;&#31574;&#30053;&#19968;&#33268;&#65292;&#23558;Cobweb4V&#23450;&#20301;&#20026;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16933v1 Announce Type: cross  Abstract: Deep neural networks have excelled in machine learning, particularly in vision tasks, however, they often suffer from catastrophic forgetting when learning new tasks sequentially. In this work, we propose Cobweb4V, a novel visual classification approach that builds on Cobweb, a human like learning system that is inspired by the way humans incrementally learn new concepts over time. In this research, we conduct a comprehensive evaluation, showcasing the proficiency of Cobweb4V in learning visual concepts, requiring less data to achieve effective learning outcomes compared to traditional methods, maintaining stable performance over time, and achieving commendable asymptotic behavior, without catastrophic forgetting effects. These characteristics align with learning strategies in human cognition, positioning Cobweb4V as a promising alternative to neural network approaches.
&lt;/p&gt;</description></item><item><title /><link>https://arxiv.org/abs/2402.07127</link><description>&lt;p&gt;
&#35266;&#23519;&#23398;&#20064;&#65306;&#22522;&#20110;&#35270;&#39057;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#23398;&#20064;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07127
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#25805;&#20316;&#25216;&#33021;&#21463;&#21040;&#22810;&#26679;&#21270;&#12289;&#26080;&#20559;&#30340;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#38382;&#39064;&#65292;&#20294;&#22312;&#27867;&#21270;&#24615;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#8220;&#37326;&#22806;&#8221;&#35270;&#39057;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#23384;&#22312;&#36890;&#36807;&#33258;&#30417;&#30563;&#25216;&#26415;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#36827;&#23637;&#12290;&#23558;&#36825;&#19968;&#28857;&#24212;&#29992;&#21040;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#34987;&#21160;&#35266;&#23519;&#26469;&#23398;&#20064;&#20016;&#23500;&#30340;&#22312;&#32447;&#35270;&#39057;&#20013;&#30340;&#25805;&#20316;&#25216;&#33021;&#12290;&#36825;&#31181;&#22522;&#20110;&#35270;&#39057;&#30340;&#23398;&#20064;&#33539;&#24335;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#23427;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#12290;&#26412;&#32508;&#36848;&#22238;&#39038;&#20102;&#35270;&#39057;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#12289;&#29289;&#20307;&#21487;&#34892;&#24615;&#29702;&#35299;&#12289;&#19977;&#32500;&#25163;&#37096;/&#36523;&#20307;&#24314;&#27169;&#21644;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#36164;&#28304;&#31561;&#22522;&#30784;&#30693;&#35782;&#65292;&#20197;&#21450;&#20174;&#19981;&#21463;&#25511;&#21046;&#30340;&#35270;&#39057;&#28436;&#31034;&#20013;&#33719;&#21462;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30340;&#26032;&#20852;&#25216;&#26415;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20165;&#20174;&#35266;&#23519;&#22823;&#35268;&#27169;&#20154;&#31867;&#35270;&#39057;&#20013;&#23398;&#20064;&#22914;&#20309;&#22686;&#24378;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot learning of manipulation skills is hindered by the scarcity of diverse, unbiased datasets. While curated datasets can help, challenges remain in generalizability and real-world transfer. Meanwhile, large-scale "in-the-wild" video datasets have driven progress in computer vision through self-supervised techniques. Translating this to robotics, recent works have explored learning manipulation skills by passively watching abundant videos sourced online. Showing promising results, such video-based learning paradigms provide scalable supervision while reducing dataset bias. This survey reviews foundations such as video feature representation learning techniques, object affordance understanding, 3D hand/body modeling, and large-scale robot resources, as well as emerging techniques for acquiring robot manipulation skills from uncontrolled video demonstrations. We discuss how learning only from observing large-scale human videos can enhance generalization and sample efficiency for roboti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#32467;&#21512;Copula&#29702;&#35770;&#26469;&#35299;&#20915;&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.17448</link><description>&lt;p&gt;
NN-Copula-CD&#65306;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#21464;&#21270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
NN-Copula-CD: A Copula-Guided Interpretable Neural Network for Change Detection in Heterogeneous Remote Sensing Images. (arXiv:2303.17448v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17448
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#32467;&#21512;Copula&#29702;&#35770;&#26469;&#35299;&#20915;&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#26159;&#19968;&#20010;&#23454;&#38469;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36807;&#21435;&#21313;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#21457;&#23637;&#35753;&#24322;&#26500;&#21464;&#21270;&#26816;&#27979;&#38382;&#39064;&#21463;&#30410;&#21290;&#27973;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;DNN&#22987;&#32456;&#20687;&#40657;&#21283;&#23376;&#19968;&#26679;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;DNN&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#21464;&#21270;&#26816;&#27979;&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#24322;&#26500;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;(NN-Copula-CD)&#12290;&#22312;NN-Copula-CD&#20013;&#65292;Copula&#30340;&#25968;&#23398;&#29305;&#24449;&#34987;&#35774;&#35745;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#30417;&#30563;&#19968;&#20010;&#31616;&#21333;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Change detection (CD) in heterogeneous remote sensing images is a practical and challenging issue for real-life emergencies. In the past decade, the heterogeneous CD problem has significantly benefited from the development of deep neural networks (DNN). However, the data-driven DNNs always perform like a black box where the lack of interpretability limits the trustworthiness and controllability of DNNs in most practical CD applications. As a strong knowledge-driven tool to measure correlation between random variables, Copula theory has been introduced into CD, yet it suffers from non-robust CD performance without manual prior selection for Copula functions. To address the above issues, we propose a knowledge-data-driven heterogeneous CD method (NN-Copula-CD) based on the Copula-guided interpretable neural network. In our NN-Copula-CD, the mathematical characteristics of Copula are designed as the losses to supervise a simple fully connected neural network to learn the correlation betwe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.16199</link><description>&lt;p&gt;
LLaMA-Adapter: &#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#31934;&#32454;&#35843;&#25972;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. (arXiv:2303.16199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Adapter&#36825;&#19968;&#36731;&#37327;&#32423;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;LLaMA&#39640;&#25928;&#22320;&#24494;&#35843;&#20026;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;&#21033;&#29992;52K&#20010;&#33258;&#25105;&#25351;&#23548;&#31034;&#33539;&#65292;LLaMA-Adapter&#20165;&#22312;&#20923;&#32467;&#30340;LLaMA 7B&#27169;&#22411;&#19978;&#24341;&#20837;&#20102;1.2M&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;8&#20010;A100 GPU&#19978;&#20165;&#32791;&#26102;&#19981;&#21040;&#19968;&#20010;&#23567;&#26102;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#36866;&#24212;&#25552;&#31034;&#65292;&#24182;&#22312;&#36739;&#39640;&#30340;&#21464;&#21387;&#22120;&#23618;&#20013;&#23558;&#23427;&#20204;&#39044;&#32622;&#20110;&#36755;&#20837;&#25991;&#26412;&#20196;&#29260;&#20043;&#21069;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#38646;&#38376;&#25511;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#26032;&#30340;&#25351;&#20196;&#25552;&#31034;&#27880;&#20837;LLaMA&#65292;&#24182;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#20854;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#39640;&#25928;&#35757;&#32451;&#65292;LLaMA-Adapter&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;7B&#21442;&#25968;&#30340;Alpaca&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#31616;&#21333;&#22320;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20363;&#22914;&#22270;&#20687;&#65292;&#29992;&#20110;&#22270;&#20687;&#30456;&#20851;&#30340;LLaMA&#65292;&#22312;ScienceQA&#19978;&#23454;&#29616;&#20102;&#26356;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/ZrrSkywalker/LLaMA-Adapt&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapt
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24615;&#33021;&#19981;&#21487;&#30693;&#30340;&#22810;&#27169;&#24577;&#24471;&#20998;&#26041;&#27861;MM-SHAP&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#37327;&#21270;&#22810;&#27169;&#24577;&#27169;&#22411;&#20351;&#29992;&#21508;&#33258;&#27169;&#24577;&#30340;&#27604;&#20363;&#65292;&#24182;&#24212;&#29992;&#20110;&#27604;&#36739;&#27169;&#22411;&#30340;&#24179;&#22343;&#22810;&#27169;&#24577;&#31243;&#24230;&#21644;&#34913;&#37327;&#20010;&#20307;&#27169;&#22411;&#30340;&#36129;&#29486;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21333;&#27169;&#24577;&#23849;&#28291;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#26356;&#20026;&#26222;&#36941;&#65292;&#32780;MM-SHAP&#26159;&#20998;&#26512;VL&#27169;&#22411;&#22810;&#27169;&#24577;&#34892;&#20026;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2212.08158</link><description>&lt;p&gt;
MM-SHAP&#65306;&#19968;&#31181;&#29992;&#20110;&#34913;&#37327;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#36129;&#29486;&#30340;&#24615;&#33021;&#19981;&#21487;&#30693;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal Contributions in Vision and Language Models &amp; Tasks. (arXiv:2212.08158v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24615;&#33021;&#19981;&#21487;&#30693;&#30340;&#22810;&#27169;&#24577;&#24471;&#20998;&#26041;&#27861;MM-SHAP&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#37327;&#21270;&#22810;&#27169;&#24577;&#27169;&#22411;&#20351;&#29992;&#21508;&#33258;&#27169;&#24577;&#30340;&#27604;&#20363;&#65292;&#24182;&#24212;&#29992;&#20110;&#27604;&#36739;&#27169;&#22411;&#30340;&#24179;&#22343;&#22810;&#27169;&#24577;&#31243;&#24230;&#21644;&#34913;&#37327;&#20010;&#20307;&#27169;&#22411;&#30340;&#36129;&#29486;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21333;&#27169;&#24577;&#23849;&#28291;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#26356;&#20026;&#26222;&#36941;&#65292;&#32780;MM-SHAP&#26159;&#20998;&#26512;VL&#27169;&#22411;&#22810;&#27169;&#24577;&#34892;&#20026;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;VL&#65289;&#24448;&#24448;&#21033;&#29992;&#21508;&#33258;&#27169;&#24577;&#20013;&#30340;&#19981;&#31283;&#23450;&#25351;&#26631;&#65288;&#20363;&#22914;&#30001;&#20998;&#24067;&#20559;&#24046;&#24341;&#20837;&#65289;&#32780;&#19981;&#26159;&#19987;&#27880;&#20110;&#27599;&#20010;&#27169;&#24577;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#22914;&#26524;&#21333;&#27169;&#24577;&#27169;&#22411;&#22312;VL&#20219;&#21153;&#19978;&#36798;&#21040;&#31867;&#20284;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#65292;&#21017;&#34920;&#26126;&#25152;&#35859;&#30340;&#21333;&#27169;&#24577;&#23849;&#28291;&#24050;&#32463;&#21457;&#29983;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#20934;&#30830;&#24230;&#30340;&#27979;&#35797;&#26080;&#27861;&#26816;&#27979;&#20363;&#22914;&#27169;&#22411;&#39044;&#27979;&#38169;&#35823;&#20294;&#27169;&#22411;&#20351;&#29992;&#20102;&#19968;&#20010;&#27169;&#24577;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MM-SHAP&#65292;&#19968;&#31181;&#22522;&#20110;Shapley&#20540;&#30340;&#24615;&#33021;&#19981;&#21487;&#30693;&#22810;&#27169;&#24577;&#24471;&#20998;&#65292;&#21487;&#21487;&#38752;&#22320;&#37327;&#21270;&#22810;&#27169;&#24577;&#27169;&#22411;&#20351;&#29992;&#21508;&#33258;&#27169;&#24577;&#30340;&#27604;&#20363;&#12290;&#25105;&#20204;&#23558;MM-SHAP&#24212;&#29992;&#20110;&#20004;&#31181;&#26041;&#24335;&#65306;&#65288;1&#65289;&#27604;&#36739;&#27169;&#22411;&#30340;&#24179;&#22343;&#22810;&#27169;&#24577;&#31243;&#24230;&#65292;&#65288;2&#65289;&#34913;&#37327;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20010;&#20307;&#27169;&#22411;&#23545;&#21508;&#33258;&#27169;&#24577;&#30340;&#36129;&#29486;&#12290;&#20845;&#20010;VL&#27169;&#22411;&#30340;&#23454;&#39564;&#65288;LXMERT&#12289;CLIP&#21644;&#22235;&#20010;ALBEF&#21464;&#20307;&#65289;&#34920;&#26126;&#21333;&#27169;&#24577;&#23849;&#28291;&#27604;&#25105;&#20204;&#20197;&#21069;&#35748;&#20026;&#30340;&#26356;&#20026;&#26222;&#36941;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;MM-SHAP&#26159;&#25581;&#31034;&#21644;&#20998;&#26512;VL&#27169;&#22411;&#22810;&#27169;&#24577;&#34892;&#20026;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision and language models (VL) are known to exploit unrobust indicators in individual modalities (e.g., introduced by distributional biases) instead of focusing on relevant information in each modality. That a unimodal model achieves similar accuracy on a VL task to a multimodal one, indicates that so-called unimodal collapse occurred. However, accuracy-based tests fail to detect e.g., when the model prediction is wrong, while the model used relevant information from a modality. Instead, we propose MM-SHAP, a performance-agnostic multimodality score based on Shapley values that reliably quantifies in which proportions a multimodal model uses individual modalities. We apply MM-SHAP in two ways: (1) to compare models for their average degree of multimodality, and (2) to measure for individual models the contribution of individual modalities for different tasks and datasets. Experiments with six VL models -- LXMERT, CLIP and four ALBEF variants -- on four VL tasks highlight that unimodal
&lt;/p&gt;</description></item></channel></rss>