<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38024;&#23545;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40065;&#26834;&#24615;&#24314;&#27169;&#12290;&#30740;&#31350;&#21457;&#29616;&#38544;&#34255;&#23618;&#25968;&#37327;&#23545;&#27169;&#22411;&#30340;&#25512;&#24191;&#24615;&#33021;&#26377;&#24433;&#21709;&#65292;&#21516;&#26102;&#36824;&#27979;&#35797;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#28014;&#28857;&#31934;&#24230;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#36755;&#20986;&#30340;&#22122;&#22768;&#27700;&#24179;&#31561;&#21442;&#25968;&#12290;&#20026;&#20102;&#25913;&#36827;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35825;&#21457;&#25925;&#38556;&#26469;&#24314;&#27169;&#25925;&#38556;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13751</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#24314;&#27169;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Systematic Approach to Robustness Modelling for Deep Convolutional Neural Networks. (arXiv:2401.13751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38024;&#23545;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40065;&#26834;&#24615;&#24314;&#27169;&#12290;&#30740;&#31350;&#21457;&#29616;&#38544;&#34255;&#23618;&#25968;&#37327;&#23545;&#27169;&#22411;&#30340;&#25512;&#24191;&#24615;&#33021;&#26377;&#24433;&#21709;&#65292;&#21516;&#26102;&#36824;&#27979;&#35797;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#28014;&#28857;&#31934;&#24230;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#36755;&#20986;&#30340;&#22122;&#22768;&#27700;&#24179;&#31561;&#21442;&#25968;&#12290;&#20026;&#20102;&#25913;&#36827;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35825;&#21457;&#25925;&#38556;&#26469;&#24314;&#27169;&#25925;&#38556;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26377;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#39046;&#22495;&#37117;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#36235;&#21183;&#26159;&#20351;&#29992;&#20855;&#26377;&#36234;&#26469;&#36234;&#22810;&#21487;&#35843;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#27169;&#22411;&#25439;&#22833;&#25110;&#21019;&#24314;&#26356;&#20855;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#30446;&#26631;&#36890;&#24120;&#30456;&#20114;&#30683;&#30462;&#12290;&#29305;&#21035;&#26159;&#65292;&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#26356;&#22823;&#27169;&#22411;&#33021;&#21542;&#25512;&#24191;&#21040;&#21463;&#25511;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#20043;&#22806;&#30340;&#25968;&#25454;&#30340;&#30097;&#38382;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ResNet&#27169;&#22411;&#20013;&#38544;&#34255;&#23618;&#30340;&#25968;&#37327;&#22312;MNIST&#12289;CIFAR10&#21644;CIFAR100&#25968;&#25454;&#38598;&#19978;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#21442;&#25968;&#65292;&#21253;&#25324;&#27169;&#22411;&#30340;&#22823;&#23567;&#12289;&#28014;&#28857;&#31934;&#24230;&#65292;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#36755;&#20986;&#30340;&#22122;&#22768;&#27700;&#24179;&#12290;&#20026;&#20102;&#25913;&#36827;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#35825;&#21457;&#25925;&#38556;&#26469;&#24314;&#27169;&#25925;&#38556;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks have shown to be widely applicable to a large number of fields when large amounts of labelled data are available. The recent trend has been to use models with increasingly larger sets of tunable parameters to increase model accuracy, reduce model loss, or create more adversarially robust models -- goals that are often at odds with one another. In particular, recent theoretical work raises questions about the ability for even larger models to generalize to data outside of the controlled train and test sets. As such, we examine the role of the number of hidden layers in the ResNet model, demonstrated on the MNIST, CIFAR10, CIFAR100 datasets. We test a variety of parameters including the size of the model, the floating point precision, and the noise level of both the training data and the model output. To encapsulate the model's predictive power and computational cost, we provide a method that uses induced failures to model the probability of failure as a fun
&lt;/p&gt;</description></item><item><title>CoFiI2P&#26159;&#19968;&#31181;&#31895;&#21040;&#31934;&#30340;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#21644;&#29305;&#24449;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.14660</link><description>&lt;p&gt;
CoFiI2P: &#31895;&#21040;&#31934;&#30340;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#30340;&#23545;&#24212;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
CoFiI2P: Coarse-to-Fine Correspondences for Image-to-Point Cloud Registration. (arXiv:2309.14660v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14660
&lt;/p&gt;
&lt;p&gt;
CoFiI2P&#26159;&#19968;&#31181;&#31895;&#21040;&#31934;&#30340;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#21644;&#29305;&#24449;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21040;&#28857;&#20113;&#65288;I2P&#65289;&#27880;&#20876;&#26159;&#26426;&#22120;&#20154;&#23548;&#33322;&#21644;&#31227;&#21160;&#24314;&#22270;&#39046;&#22495;&#20013;&#30340;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;I2P&#27880;&#20876;&#26041;&#27861;&#22312;&#28857;&#21040;&#20687;&#32032;&#32423;&#21035;&#19978;&#20272;&#35745;&#23545;&#24212;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#20840;&#23616;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#26469;&#33258;&#20840;&#23616;&#32422;&#26463;&#30340;&#39640;&#32423;&#24341;&#23548;&#30340;I2P&#21305;&#37197;&#23481;&#26131;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;I2P&#27880;&#20876;&#32593;&#32476;CoFiI2P&#65292;&#36890;&#36807;&#31895;&#21040;&#31934;&#30340;&#26041;&#24335;&#25552;&#21462;&#23545;&#24212;&#20851;&#31995;&#65292;&#20197;&#24471;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#39318;&#20808;&#65292;&#23558;&#22270;&#20687;&#21644;&#28857;&#20113;&#36755;&#20837;&#21040;&#19968;&#20010;&#20849;&#20139;&#32534;&#30721;-&#35299;&#30721;&#32593;&#32476;&#20013;&#36827;&#34892;&#23618;&#27425;&#21270;&#29305;&#24449;&#25552;&#21462;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31895;&#21040;&#31934;&#30340;&#21305;&#37197;&#27169;&#22359;&#65292;&#21033;&#29992;&#29305;&#24449;&#24314;&#31435;&#31283;&#20581;&#30340;&#29305;&#24449;&#23545;&#24212;&#20851;&#31995;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#31895;&#21305;&#37197;&#22359;&#20013;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;I2P&#21464;&#25442;&#27169;&#22359;&#65292;&#20174;&#22270;&#20687;&#21644;&#28857;&#20113;&#20013;&#25429;&#25417;&#21516;&#36136;&#21644;&#24322;&#36136;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#36890;&#36807;&#21028;&#21035;&#25551;&#36848;&#23376;&#65292;&#23436;&#25104;&#31895;-&#32454;&#29305;&#24449;&#21305;&#37197;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#32454;&#21270;&#21305;&#37197;&#27169;&#22359;&#36827;&#19968;&#27493;&#25552;&#21319;&#23545;&#24212;&#20851;&#31995;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-to-point cloud (I2P) registration is a fundamental task in the fields of robot navigation and mobile mapping. Existing I2P registration works estimate correspondences at the point-to-pixel level, neglecting the global alignment. However, I2P matching without high-level guidance from global constraints may converge to the local optimum easily. To solve the problem, this paper proposes CoFiI2P, a novel I2P registration network that extracts correspondences in a coarse-to-fine manner for the global optimal solution. First, the image and point cloud are fed into a Siamese encoder-decoder network for hierarchical feature extraction. Then, a coarse-to-fine matching module is designed to exploit features and establish resilient feature correspondences. Specifically, in the coarse matching block, a novel I2P transformer module is employed to capture the homogeneous and heterogeneous global information from image and point cloud. With the discriminate descriptors, coarse super-point-to-su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#21644;&#20256;&#24863;&#22120;&#24863;&#30693;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07808</link><description>&lt;p&gt;
&#25552;&#21319;&#27169;&#20223;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#30340;&#20851;&#38190;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving. (arXiv:2309.07808v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#21644;&#20256;&#24863;&#22120;&#24863;&#30693;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#20840;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#65292;&#22312;&#36825;&#31181;&#25216;&#26415;&#20013;&#65292;&#25972;&#20010;&#39550;&#39542;&#27969;&#31243;&#34987;&#26367;&#25442;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#30001;&#20110;&#20854;&#32467;&#26500;&#31616;&#21333;&#21644;&#25512;&#29702;&#26102;&#38388;&#24555;&#65292;&#22240;&#27492;&#21464;&#24471;&#38750;&#24120;&#21560;&#24341;&#20154;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23569;&#20102;&#39550;&#39542;&#27969;&#31243;&#20013;&#30340;&#32452;&#20214;&#65292;&#20294;&#20854;&#31616;&#21333;&#24615;&#20063;&#23548;&#33268;&#35299;&#37322;&#24615;&#38382;&#39064;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#35757;&#32451;&#24471;&#21040;&#30340;&#31574;&#30053;&#24182;&#19981;&#24635;&#26159;&#31526;&#21512;&#20132;&#36890;&#35268;&#21017;&#65292;&#21516;&#26102;&#20063;&#24456;&#38590;&#21457;&#29616;&#20854;&#38169;&#35823;&#30340;&#21407;&#22240;&#65292;&#22240;&#20026;&#32570;&#20047;&#20013;&#38388;&#36755;&#20986;&#12290;&#21516;&#26102;&#65292;&#20256;&#24863;&#22120;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#34892;&#24615;&#20063;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#24110;&#21161;&#24863;&#30693;&#22797;&#26434;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#21608;&#22260;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
More research attention has recently been given to end-to-end autonomous driving technologies where the entire driving pipeline is replaced with a single neural network because of its simpler structure and faster inference time. Despite this appealing approach largely reducing the components in driving pipeline, its simplicity also leads to interpretability problems and safety issues arXiv:2003.06404. The trained policy is not always compliant with the traffic rules and it is also hard to discover the reason for the misbehavior because of the lack of intermediate outputs. Meanwhile, Sensors are also critical to autonomous driving's security and feasibility to perceive the surrounding environment under complex driving scenarios. In this paper, we proposed P-CSG, a novel penalty-based imitation learning approach with cross semantics generation sensor fusion technologies to increase the overall performance of End-to-End Autonomous Driving. We conducted an assessment of our model's perform
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36890;&#36807;&#23545;&#29289;&#20307;&#36755;&#20837;&#34920;&#31034;&#36827;&#34892;&#36523;&#20221;&#20445;&#25345;&#30340;&#21464;&#25442;&#65292;&#19981;&#20165;&#26377;&#21161;&#20110;&#29289;&#20307;&#30340;&#20998;&#31867;&#65292;&#36824;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#23646;&#24615;&#30340;&#26377;&#26080;&#20915;&#31574;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2302.10763</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#19982;&#23646;&#24615;&#20851;&#32852;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning and the Emergence of Attributes Associations. (arXiv:2302.10763v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10763
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36890;&#36807;&#23545;&#29289;&#20307;&#36755;&#20837;&#34920;&#31034;&#36827;&#34892;&#36523;&#20221;&#20445;&#25345;&#30340;&#21464;&#25442;&#65292;&#19981;&#20165;&#26377;&#21161;&#20110;&#29289;&#20307;&#30340;&#20998;&#31867;&#65292;&#36824;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#23646;&#24615;&#30340;&#26377;&#26080;&#20915;&#31574;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29289;&#20307;&#21576;&#29616;&#65292;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#36890;&#24120;&#20250;&#32473;&#20986;&#19968;&#20010;&#31616;&#27905;&#30340;&#26631;&#31614;&#12290;&#32780;&#20154;&#31867;&#22312;&#31867;&#20284;&#30340;&#21576;&#29616;&#19979;&#65292;&#38500;&#20102;&#32473;&#20986;&#19968;&#20010;&#26631;&#31614;&#22806;&#65292;&#36824;&#20250;&#34987;&#22823;&#37327;&#30340;&#20851;&#32852;&#20449;&#24687;&#25152;&#28153;&#27809;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#21576;&#29616;&#29289;&#20307;&#30340;&#23646;&#24615;&#12290;&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#65292;&#22522;&#20110;&#23545;&#29289;&#20307;&#36755;&#20837;&#34920;&#31034;&#36827;&#34892;&#20445;&#25345;&#36523;&#20221;&#30340;&#21464;&#25442;&#12290;&#26412;&#30740;&#31350;&#25512;&#27979;&#65292;&#36825;&#20123;&#21464;&#25442;&#19981;&#20165;&#21487;&#20197;&#20445;&#25345;&#21576;&#29616;&#29289;&#20307;&#30340;&#36523;&#20221;&#65292;&#36824;&#21487;&#20197;&#20445;&#25345;&#20854;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#23646;&#24615;&#30340;&#36523;&#20221;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#30340;&#36755;&#20986;&#34920;&#31034;&#19981;&#20165;&#23545;&#20110;&#21576;&#29616;&#29289;&#20307;&#30340;&#20998;&#31867;&#26377;&#20215;&#20540;&#65292;&#36824;&#23545;&#20110;&#20219;&#20309;&#24863;&#20852;&#36259;&#23646;&#24615;&#30340;&#26377;&#26080;&#20915;&#31574;&#26377;&#20215;&#20540;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#35266;&#28857;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to an object presentation, supervised learning schemes generally respond with a parsimonious label. Upon a similar presentation we humans respond again with a label, but are flooded, in addition, by a myriad of associations. A significant portion of these consist of the presented object attributes. Contrastive learning is a semi-supervised learning scheme based on the application of identity preserving transformations on the object input representations. It is conjectured in this work that these same applied transformations preserve, in addition to the identity of the presented object, also the identity of its semantically meaningful attributes. The corollary of this is that the output representations of such a contrastive learning scheme contain valuable information not only for the classification of the presented object, but also for the presence or absence decision of any attribute of interest. Simulation results which demonstrate this idea and the feasibility of this co
&lt;/p&gt;</description></item></channel></rss>