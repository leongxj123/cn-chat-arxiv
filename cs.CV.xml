<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#32508;&#36848;&#35843;&#30740;&#20102;&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;</title><link>https://arxiv.org/abs/2402.02242</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#30740;&#20102;&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#65288;PVMs&#65289;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26368;&#20808;&#36827;&#30340;PVMs&#36798;&#21040;&#25968;&#21313;&#20159;&#29978;&#33267;&#25968;&#19975;&#20159;&#20010;&#21442;&#25968;&#65292;&#26631;&#20934;&#30340;&#20840;&#38754;&#24494;&#35843;&#33539;&#24335;&#30001;&#20110;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#21464;&#24471;&#19981;&#21487;&#25345;&#32493;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#25506;&#32034;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#65292;&#26088;&#22312;&#20197;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#35270;&#35273;PEFT&#30340;&#20840;&#38754;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#23545;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#23457;&#26597;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;PEFT&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#27169;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#26041;&#27861;&#20998;&#20026;&#19977;&#31867;&#65306;&#22522;&#20110;&#28155;&#21152;&#30340;&#12289;&#22522;&#20110;&#37096;&#20998;&#30340;&#21644;&#22522;&#20110;&#32479;&#19968;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#25361;&#25112;&#12290;&#35813;&#32508;&#36848;&#36824;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained vision models (PVMs) have shown great potential for adaptability across various downstream vision tasks. However, with state-of-the-art PVMs growing to billions or even trillions of parameters, the standard full fine-tuning paradigm is becoming unsustainable due to high computational and storage demands. In response, researchers are exploring parameter-efficient fine-tuning (PEFT), which seeks to exceed the performance of full fine-tuning with minimal parameter modifications. This survey provides a comprehensive overview and future directions for visual PEFT, offering a systematic review of the latest advancements. First, we provide a formal definition of PEFT and discuss model pre-training methods. We then categorize existing methods into three categories: addition-based, partial-based, and unified-based. Finally, we introduce the commonly used datasets and applications and suggest potential future research challenges. A comprehensive collection of resources is
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#19981;&#21516;&#23616;&#37096;&#24615;&#23545;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;SNN&#30340;&#38544;&#24335;&#24490;&#29615;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01782</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;&#23616;&#37096;&#24615;&#23545;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Spiking Neural Network Learning Methods with Varying Locality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#19981;&#21516;&#23616;&#37096;&#24615;&#23545;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;SNN&#30340;&#38544;&#24335;&#24490;&#29615;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#25552;&#20379;&#26356;&#30495;&#23454;&#30340;&#31070;&#32463;&#21160;&#21147;&#23398;&#65292;&#22312;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#20449;&#24687;&#22312;SNN&#20013;&#20197;&#33033;&#20914;&#24418;&#24335;&#36827;&#34892;&#22788;&#29702;&#65292;&#37319;&#29992;&#20107;&#20214;&#39537;&#21160;&#26426;&#21046;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#33021;&#28304;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33033;&#20914;&#26426;&#21046;&#30340;&#38750;&#21487;&#24494;&#24615;&#65292;&#35757;&#32451;SNN&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#65288;BPTT&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#19968;&#23450;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#29983;&#29289;&#23398;&#19978;&#19981;&#21487;&#34892;&#12290;&#30456;&#21453;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20855;&#26377;&#19981;&#21516;&#23616;&#37096;&#24615;&#30340;&#26367;&#20195;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#21516;&#26102;&#22312;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;SNN&#30340;&#38544;&#24335;&#24490;&#29615;&#29305;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs), providing more realistic neuronal dynamics, have shown to achieve performance comparable to Artificial Neural Networks (ANNs) in several machine learning tasks. Information is processed as spikes within SNNs in an event-based mechanism that significantly reduces energy consumption. However, training SNNs is challenging due to the non-differentiable nature of the spiking mechanism. Traditional approaches, such as Backpropagation Through Time (BPTT), have shown effectiveness but comes with additional computational and memory costs and are biologically implausible. In contrast, recent works propose alternative learning methods with varying degrees of locality, demonstrating success in classification tasks. In this work, we show that these methods share similarities during the training process, while they present a trade-off between biological plausibility and performance. Further, this research examines the implicitly recurrent nature of SNNs and investigat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#25442;&#22120;&#21644;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#25913;&#21892;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#24120;&#35265;&#25104;&#20687;&#21464;&#24322;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15952</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#24341;&#23548;&#25193;&#25955;&#21644;&#23884;&#22871;&#38598;&#25104;&#25913;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness and Reliability in Medical Image Classification with Latent-Guided Diffusion and Nested-Ensembles. (arXiv:2310.15952v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#25442;&#22120;&#21644;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#25913;&#21892;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#24120;&#35265;&#25104;&#20687;&#21464;&#24322;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#30495;&#23454;&#20020;&#24202;&#29615;&#22659;&#20013;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#23427;&#20204;&#23545;&#25152;&#33719;&#21462;&#30340;&#22270;&#20687;&#30340;&#21464;&#24322;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#35768;&#22810;&#26041;&#27861;&#20250;&#23545;&#35757;&#32451;&#25968;&#25454;&#24212;&#29992;&#39044;&#23450;&#20041;&#30340;&#36716;&#25442;&#65292;&#20197;&#22686;&#24378;&#27979;&#35797;&#26102;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#36825;&#20123;&#36716;&#25442;&#21487;&#33021;&#26080;&#27861;&#30830;&#20445;&#27169;&#22411;&#23545;&#24739;&#32773;&#22270;&#20687;&#20013;&#30340;&#22810;&#26679;&#24615;&#21464;&#24322;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#22120;&#21644;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#19977;&#38454;&#27573;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#23454;&#36341;&#20013;&#24120;&#35265;&#30340;&#25104;&#20687;&#21464;&#24322;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#39044;&#20808;&#30830;&#23450;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#22810;&#20010;&#22270;&#20687;&#32534;&#30721;&#22120;&#39318;&#20808;&#23398;&#20064;&#20998;&#23618;&#29305;&#24449;&#34920;&#31034;&#26469;&#26500;&#24314;&#36776;&#21035;&#28508;&#22312;&#31354;&#38388;&#12290;&#25509;&#19979;&#26469;&#65292;&#19968;&#20010;&#30001;&#28508;&#22312;&#20195;&#30721;&#24341;&#23548;&#30340;&#36870;&#25193;&#25955;&#36807;&#31243;&#20316;&#29992;&#20110;&#26377;&#20449;&#24687;&#20808;&#39564;&#65292;&#24182;&#25552;&#20986;&#39044;&#27979;&#20505;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning models have achieved remarkable success across a range of medical image analysis tasks, deployment of these models in real clinical contexts requires that they be robust to variability in the acquired images. While many methods apply predefined transformations to augment the training data to enhance test-time robustness, these transformations may not ensure the model's robustness to the diverse variability seen in patient images. In this paper, we introduce a novel three-stage approach based on transformers coupled with conditional diffusion models, with the goal of improving model robustness to the kinds of imaging variability commonly encountered in practice without the need for pre-determined data augmentation strategies. To this end, multiple image encoders first learn hierarchical feature representations to build discriminative latent spaces. Next, a reverse diffusion process, guided by the latent code, acts on an informative prior and proposes prediction candi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#22788;&#29702;&#23398;&#20064;&#31639;&#27861;&#65288;CBAGAN-RRT&#65289;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#29992;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25214;&#21040;&#26356;&#20248;&#30340;&#26368;&#20339;&#36335;&#24452;&#24182;&#25552;&#39640;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#27604;&#65292;&#22312;&#22270;&#20687;&#36136;&#37327;&#29983;&#25104;&#25351;&#26631;&#21644;&#36335;&#24452;&#35268;&#21010;&#25351;&#26631;&#26041;&#38754;&#37117;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.10442</link><description>&lt;p&gt;
CBAGAN-RRT: &#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
CBAGAN-RRT: Convolutional Block Attention Generative Adversarial Network for Sampling-Based Path Planning. (arXiv:2305.10442v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#22788;&#29702;&#23398;&#20064;&#31639;&#27861;&#65288;CBAGAN-RRT&#65289;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#29992;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25214;&#21040;&#26356;&#20248;&#30340;&#26368;&#20339;&#36335;&#24452;&#24182;&#25552;&#39640;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#27604;&#65292;&#22312;&#22270;&#20687;&#36136;&#37327;&#29983;&#25104;&#25351;&#26631;&#21644;&#36335;&#24452;&#35268;&#21010;&#25351;&#26631;&#26041;&#38754;&#37117;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#22312;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;RRT&#31639;&#27861;&#30340;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#29983;&#25104;&#30340;&#21021;&#22987;&#36335;&#24452;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#32780;&#19988;&#25910;&#25947;&#36895;&#24230;&#36807;&#24930;&#65292;&#26080;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#22270;&#20687;&#22788;&#29702;&#23398;&#20064;&#31639;&#27861;&#65288;CBAGAN-RRT&#65289;&#65292;&#20197;&#35774;&#35745;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#25214;&#21040;&#26356;&#20248;&#30340;&#26368;&#20339;&#36335;&#24452;&#65292;&#24182;&#25552;&#39640;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;GAN&#27169;&#22411;&#29983;&#25104;&#30340;&#36335;&#24452;&#27010;&#29575;&#20998;&#24067;&#29992;&#20110;&#24341;&#23548;RRT&#31639;&#27861;&#30340;&#37319;&#26679;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#30001; \cite {zhang2021generative} &#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22270;&#20687;&#36136;&#37327;&#29983;&#25104;&#25351;&#26631;&#65288;&#22914;IOU&#20998;&#25968;&#65292;Dice&#20998;&#25968;&#65289;&#21644;&#36335;&#24452;&#35268;&#21010;&#25351;&#26631;&#65288;&#22914;&#36335;&#24452;&#38271;&#24230;&#21644;&#25104;&#21151;&#29575;&#65289;&#26041;&#38754;&#22343;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling-based path planning algorithms play an important role in autonomous robotics. However, a common problem among the RRT-based algorithms is that the initial path generated is not optimal and the convergence is too slow to be used in real-world applications. In this paper, we propose a novel image-based learning algorithm (CBAGAN-RRT) using a Convolutional Block Attention Generative Adversarial Network with a combination of spatial and channel attention and a novel loss function to design the heuristics, find a better optimal path, and improve the convergence of the algorithm both concerning time and speed. The probability distribution of the paths generated from our GAN model is used to guide the sampling process for the RRT algorithm. We train and test our network on the dataset generated by \cite{zhang2021generative} and demonstrate that our algorithm outperforms the previous state-of-the-art algorithms using both the image quality generation metrics like IOU Score, Dice Score
&lt;/p&gt;</description></item></channel></rss>