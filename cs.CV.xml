<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#35760;&#24518;&#38382;&#39064;&#12290;&#36890;&#36807;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#31243;&#24230;&#20197;&#21450;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#35760;&#24518;&#30340;&#22240;&#32032;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#21644;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01054</link><description>&lt;p&gt;
&#26080;&#26465;&#20214;&#30340;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#35760;&#24518;&#24739;&#32773;&#24433;&#20687;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Unconditional Latent Diffusion Models Memorize Patient Imaging Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#35760;&#24518;&#38382;&#39064;&#12290;&#36890;&#36807;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#31243;&#24230;&#20197;&#21450;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#35760;&#24518;&#30340;&#22240;&#32032;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#21644;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#30340;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#24212;&#29992;&#26159;&#36890;&#36807;&#25552;&#20986;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#30340;&#26367;&#20195;&#21697;&#26469;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#24320;&#25918;&#25968;&#25454;&#20849;&#20139;&#12290;&#23613;&#31649;&#26377;&#36825;&#20010;&#24212;&#29992;&#30340;&#21069;&#26223;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#24739;&#32773;&#25968;&#25454;&#30340;&#35760;&#24518;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#29983;&#25104;&#24739;&#32773;&#25968;&#25454;&#30340;&#21103;&#26412;&#32780;&#19981;&#26159;&#26032;&#30340;&#21512;&#25104;&#26679;&#26412;&#12290;&#36825;&#30772;&#22351;&#20102;&#20445;&#25252;&#24739;&#32773;&#25968;&#25454;&#30340;&#25972;&#20010;&#30446;&#30340;&#65292;&#29978;&#33267;&#21487;&#33021;&#23548;&#33268;&#24739;&#32773;&#34987;&#37325;&#26032;&#35782;&#21035;&#12290;&#32771;&#34385;&#21040;&#36825;&#20010;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#21307;&#23398;&#24433;&#20687;&#30028;&#20013;&#36825;&#20010;&#38382;&#39064;&#24182;&#27809;&#26377;&#24471;&#21040;&#22826;&#22810;&#20851;&#27880;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#35760;&#24518;&#38382;&#39064;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;2D&#21644;3D&#30340;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#20351;&#29992;CT&#12289;MR&#21644;X&#20809;&#25968;&#25454;&#38598;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#30340;&#29983;&#25104;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#26469;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#34987;&#35760;&#24518;&#30340;&#31243;&#24230;&#65292;&#24182;&#36827;&#19968;&#27493;&#30740;&#31350;&#21487;&#33021;&#23548;&#33268;&#35760;&#24518;&#30340;&#21508;&#31181;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative latent diffusion models hold a wide range of applications in the medical imaging domain. A noteworthy application is privacy-preserved open-data sharing by proposing synthetic data as surrogates of real patient data. Despite the promise, these models are susceptible to patient data memorization, where models generate patient data copies instead of novel synthetic samples. This undermines the whole purpose of preserving patient data and may even result in patient re-identification. Considering the importance of the problem, surprisingly it has received relatively little attention in the medical imaging community. To this end, we assess memorization in latent diffusion models for medical image synthesis. We train 2D and 3D latent diffusion models on CT, MR, and X-ray datasets for synthetic data generation. Afterwards, we examine the amount of training data memorized utilizing self-supervised models and further investigate various factors that can possibly lead to memorization 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;CG-like-Adam&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#22312;&#25910;&#25947;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;</title><link>https://arxiv.org/abs/2404.01714</link><description>&lt;p&gt;
&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#30340;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#20248;&#21270;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01714
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;CG-like-Adam&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#22312;&#25910;&#25947;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#21152;&#24555;&#22521;&#35757;&#36895;&#24230;&#24182;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;&#20256;&#32479;&#30340;&#20849;&#36717;&#26799;&#24230;&#20462;&#27491;&#20026;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#65292;&#24182;&#23558;&#20854;&#24182;&#20837;&#36890;&#29992;Adam&#20013;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CG-like-Adam&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#29992;Adam&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#20272;&#35745;&#22343;&#30001;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#26367;&#25442;&#12290;&#25910;&#25947;&#20998;&#26512;&#22788;&#29702;&#20102;&#19968;&#38454;&#30697;&#20272;&#35745;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#31995;&#25968;&#20026;&#24120;&#25968;&#19988;&#19968;&#38454;&#30697;&#20272;&#35745;&#26080;&#20559;&#30340;&#24773;&#20917;&#12290;&#25968;&#20540;&#23454;&#39564;&#26174;&#31034;&#20102;&#22522;&#20110;CIFAR10/100&#25968;&#25454;&#38598;&#30340;&#25152;&#25552;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01714v1 Announce Type: cross  Abstract: Training deep neural networks is a challenging task. In order to speed up training and enhance the performance of deep neural networks, we rectify the vanilla conjugate gradient as conjugate-gradient-like and incorporate it into the generic Adam, and thus propose a new optimization algorithm named CG-like-Adam for deep learning. Specifically, both the first-order and the second-order moment estimation of generic Adam are replaced by the conjugate-gradient-like. Convergence analysis handles the cases where the exponential moving average coefficient of the first-order moment estimation is constant and the first-order moment estimation is unbiased. Numerical experiments show the superiority of the proposed algorithm based on the CIFAR10/100 dataset.
&lt;/p&gt;</description></item><item><title>&#35813;&#25945;&#31243;&#35752;&#35770;&#20102;&#22270;&#20687;&#21644;&#35270;&#35273;&#39046;&#22495;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#29702;&#24565;&#65292;&#36866;&#21512;&#23545;&#25193;&#25955;&#27169;&#22411;&#30740;&#31350;&#25110;&#24212;&#29992;&#24863;&#20852;&#36259;&#30340;&#26412;&#31185;&#29983;&#21644;&#30740;&#31350;&#29983;&#12290;</title><link>https://arxiv.org/abs/2403.18103</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#20687;&#21644;&#35270;&#35273;&#25193;&#25955;&#27169;&#22411;&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Tutorial on Diffusion Models for Imaging and Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18103
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25945;&#31243;&#35752;&#35770;&#20102;&#22270;&#20687;&#21644;&#35270;&#35273;&#39046;&#22495;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#29702;&#24565;&#65292;&#36866;&#21512;&#23545;&#25193;&#25955;&#27169;&#22411;&#30740;&#31350;&#25110;&#24212;&#29992;&#24863;&#20852;&#36259;&#30340;&#26412;&#31185;&#29983;&#21644;&#30740;&#31350;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#29983;&#25104;&#24037;&#20855;&#30340;&#24778;&#20154;&#22686;&#38271;&#20351;&#24471;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#31561;&#35768;&#22810;&#20196;&#20154;&#20852;&#22859;&#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#20123;&#29983;&#25104;&#24037;&#20855;&#32972;&#21518;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#25193;&#25955;&#27010;&#24565;&#65292;&#19968;&#31181;&#29305;&#27530;&#30340;&#37319;&#26679;&#26426;&#21046;&#65292;&#20811;&#26381;&#20102;&#20197;&#21069;&#26041;&#27861;&#20013;&#34987;&#35748;&#20026;&#22256;&#38590;&#30340;&#19968;&#20123;&#32570;&#28857;&#12290;&#26412;&#25945;&#31243;&#30340;&#30446;&#26631;&#26159;&#35752;&#35770;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#29702;&#24565;&#12290;&#26412;&#25945;&#31243;&#30340;&#30446;&#26631;&#21463;&#20247;&#21253;&#25324;&#23545;&#30740;&#31350;&#25193;&#25955;&#27169;&#22411;&#25110;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#35299;&#20915;&#20854;&#20182;&#38382;&#39064;&#24863;&#20852;&#36259;&#30340;&#26412;&#31185;&#29983;&#21644;&#30740;&#31350;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18103v1 Announce Type: new  Abstract: The astonishing growth of generative tools in recent years has empowered many exciting applications in text-to-image generation and text-to-video generation. The underlying principle behind these generative tools is the concept of diffusion, a particular sampling mechanism that has overcome some shortcomings that were deemed difficult in the previous approaches. The goal of this tutorial is to discuss the essential ideas underlying the diffusion models. The target audience of this tutorial includes undergraduate and graduate students who are interested in doing research on diffusion models or applying these models to solve other problems.
&lt;/p&gt;</description></item><item><title>NeuralDiffuser&#24341;&#20837;&#20027;&#35270;&#35273;&#29305;&#24449;&#24341;&#23548;&#65292;&#25193;&#23637;&#20102;LDM&#26041;&#27861;&#30340;&#33258;&#19979;&#32780;&#19978;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#24544;&#23454;&#30340;&#35821;&#20041;&#21644;&#32454;&#33410;&#12290;</title><link>https://arxiv.org/abs/2402.13809</link><description>&lt;p&gt;
NeuralDiffuser&#65306;&#20855;&#26377;&#20027;&#35270;&#35273;&#29305;&#24449;&#24341;&#23548;&#25193;&#25955;&#30340;&#21487;&#25511;fMRI&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
NeuralDiffuser: Controllable fMRI Reconstruction with Primary Visual Feature Guided Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13809
&lt;/p&gt;
&lt;p&gt;
NeuralDiffuser&#24341;&#20837;&#20027;&#35270;&#35273;&#29305;&#24449;&#24341;&#23548;&#65292;&#25193;&#23637;&#20102;LDM&#26041;&#27861;&#30340;&#33258;&#19979;&#32780;&#19978;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#24544;&#23454;&#30340;&#35821;&#20041;&#21644;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#20174;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;(fMRI)&#20013;&#37325;&#24314;&#35270;&#35273;&#21050;&#28608;&#65292;&#20026;&#22823;&#33041;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#26816;&#32034;&#12290;&#19968;&#20010;&#25361;&#25112;&#22312;&#20110;&#37325;&#24314;&#32454;&#33410;&#30340;&#36830;&#36143;&#23545;&#40784;&#65288;&#22914;&#32467;&#26500;&#12289;&#32972;&#26223;&#12289;&#32441;&#29702;&#12289;&#39068;&#33394;&#31561;&#65289;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#65292;LDM&#20063;&#20250;&#29983;&#25104;&#19981;&#21516;&#30340;&#22270;&#20687;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25581;&#31034;&#20102;&#22522;&#20110;LDM&#30340;&#31070;&#32463;&#31185;&#23398;&#35270;&#35282;&#65292;&#21363;&#22522;&#20110;&#26469;&#33258;&#28023;&#37327;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#36827;&#34892;&#33258;&#19978;&#32780;&#19979;&#30340;&#21019;&#24314;&#65292;&#20294;&#32570;&#20047;&#22522;&#20110;&#32454;&#33410;&#39537;&#21160;&#30340;&#33258;&#19979;&#32780;&#19978;&#24863;&#30693;&#65292;&#23548;&#33268;&#32454;&#33410;&#19981;&#24544;&#23454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NeuralDiffuser&#65292;&#24341;&#20837;&#20027;&#35270;&#35273;&#29305;&#24449;&#24341;&#23548;&#65292;&#20197;&#28176;&#21464;&#24418;&#24335;&#25552;&#20379;&#32454;&#33410;&#32447;&#32034;&#65292;&#25193;&#23637;&#20102;LDM&#26041;&#27861;&#30340;&#33258;&#19979;&#32780;&#19978;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#24544;&#23454;&#30340;&#35821;&#20041;&#21644;&#32454;&#33410;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#37325;&#22797;&#37325;&#24314;&#30340;&#19968;&#33268;&#24615;&#65292;&#32780;&#19981;&#26159;&#38543;&#26426;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13809v1 Announce Type: cross  Abstract: Reconstructing visual stimuli from functional Magnetic Resonance Imaging (fMRI) based on Latent Diffusion Models (LDM) provides a fine-grained retrieval of the brain. A challenge persists in reconstructing a cohesive alignment of details (such as structure, background, texture, color, etc.). Moreover, LDMs would generate different image results even under the same conditions. For these, we first uncover the neuroscientific perspective of LDM-based methods that is top-down creation based on pre-trained knowledge from massive images but lack of detail-driven bottom-up perception resulting in unfaithful details. We propose NeuralDiffuser which introduces primary visual feature guidance to provide detail cues in the form of gradients, extending the bottom-up process for LDM-based methods to achieve faithful semantics and details. We also developed a novel guidance strategy to ensure the consistency of repeated reconstructions rather than a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer&#65288;DEFormer&#65289;&#65292;&#21487;&#20197;&#22312;&#20302;&#20809;&#22270;&#20687;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#32454;&#33410;&#65292;&#36890;&#36807;&#24341;&#20837;&#39057;&#29575;&#20316;&#20026;&#26032;&#30340;&#32447;&#32034;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#39057;&#29575;&#20998;&#25903;&#65288;LFB&#65289;&#21644;&#22522;&#20110;&#26354;&#29575;&#30340;&#39057;&#29575;&#22686;&#24378;&#65288;CFE&#65289;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20132;&#21449;&#22495;&#34701;&#21512;&#65288;CDF&#65289;&#26469;&#20943;&#23569;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;DEFormer&#36824;&#21487;&#20197;&#20316;&#20026;&#26263;&#37096;&#26816;&#27979;&#30340;&#39044;&#22788;&#29702;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06941</link><description>&lt;p&gt;
DEFormer: &#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#21644;&#26263;&#35270;&#35273;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer
&lt;/p&gt;
&lt;p&gt;
DEFormer: DCT-driven Enhancement Transformer for Low-light Image and Dark Vision. (arXiv:2309.06941v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06941
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer&#65288;DEFormer&#65289;&#65292;&#21487;&#20197;&#22312;&#20302;&#20809;&#22270;&#20687;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#32454;&#33410;&#65292;&#36890;&#36807;&#24341;&#20837;&#39057;&#29575;&#20316;&#20026;&#26032;&#30340;&#32447;&#32034;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#39057;&#29575;&#20998;&#25903;&#65288;LFB&#65289;&#21644;&#22522;&#20110;&#26354;&#29575;&#30340;&#39057;&#29575;&#22686;&#24378;&#65288;CFE&#65289;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20132;&#21449;&#22495;&#34701;&#21512;&#65288;CDF&#65289;&#26469;&#20943;&#23569;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;DEFormer&#36824;&#21487;&#20197;&#20316;&#20026;&#26263;&#37096;&#26816;&#27979;&#30340;&#39044;&#22788;&#29702;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#30446;&#26631;&#26159;&#24674;&#22797;&#22270;&#20687;&#30340;&#39068;&#33394;&#21644;&#32454;&#33410;&#65292;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#39640;&#32423;&#35270;&#35273;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#38752;RGB&#39046;&#22495;&#24456;&#38590;&#24674;&#22797;&#26263;&#21306;&#22495;&#30340;&#20002;&#22833;&#32454;&#33410;&#12290;&#26412;&#25991;&#23558;&#39057;&#29575;&#20316;&#20026;&#32593;&#32476;&#30340;&#26032;&#32447;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer&#65288;DEFormer&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#39057;&#29575;&#20998;&#25903;&#65288;LFB&#65289;&#29992;&#20110;&#39057;&#29575;&#22686;&#24378;&#65292;&#21253;&#25324;DCT&#22788;&#29702;&#21644;&#22522;&#20110;&#26354;&#29575;&#30340;&#39057;&#29575;&#22686;&#24378;&#65288;CFE&#65289;&#12290;CFE&#35745;&#31639;&#27599;&#20010;&#36890;&#36947;&#30340;&#26354;&#29575;&#20197;&#34920;&#31034;&#19981;&#21516;&#39057;&#29575;&#24102;&#30340;&#32454;&#33410;&#20016;&#23500;&#24230;&#65292;&#28982;&#21518;&#25105;&#20204;&#23558;&#39057;&#29575;&#29305;&#24449;&#21010;&#20998;&#20026;&#26356;&#20016;&#23500;&#32441;&#29702;&#30340;&#39057;&#29575;&#24102;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#21449;&#22495;&#34701;&#21512;&#65288;CDF&#65289;&#26469;&#20943;&#23569;RGB&#39046;&#22495;&#21644;&#39057;&#29575;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23558;DEFormer&#20316;&#20026;&#26263;&#37096;&#26816;&#27979;&#30340;&#39044;&#22788;&#29702;&#65292;DEFormer&#26377;&#25928;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of low-light image enhancement is to restore the color and details of the image and is of great significance for high-level visual tasks in autonomous driving. However, it is difficult to restore the lost details in the dark area by relying only on the RGB domain. In this paper we introduce frequency as a new clue into the network and propose a novel DCT-driven enhancement transformer (DEFormer). First, we propose a learnable frequency branch (LFB) for frequency enhancement contains DCT processing and curvature-based frequency enhancement (CFE). CFE calculates the curvature of each channel to represent the detail richness of different frequency bands, then we divides the frequency features, which focuses on frequency bands with richer textures. In addition, we propose a cross domain fusion (CDF) for reducing the differences between the RGB domain and the frequency domain. We also adopt DEFormer as a preprocessing in dark detection, DEFormer effectively improves the performance
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#24515;&#33039;MRI&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#35299;&#38145;&#24515;&#30005;&#22270;&#30340;&#35786;&#26029;&#28508;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;CMR&#22270;&#20687;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#36716;&#31227;&#21040;ECG&#23884;&#20837;&#20013;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#20165;&#26681;&#25454;ECG&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#24515;&#33039;&#31579;&#26597;&#65292;&#24182;&#33021;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#20010;&#20307;&#39118;&#38505;&#21644;&#30830;&#23450;&#24515;&#33039;&#34920;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.05764</link><description>&lt;p&gt;
&#36890;&#36807;&#20174;&#24515;&#33039;MRI&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#35299;&#38145;&#24515;&#30005;&#22270;&#30340;&#35786;&#26029;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Diagnostic Potential of ECG through Knowledge Transfer from Cardiac MRI. (arXiv:2308.05764v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#24515;&#33039;MRI&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#35299;&#38145;&#24515;&#30005;&#22270;&#30340;&#35786;&#26029;&#28508;&#21147;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;CMR&#22270;&#20687;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#36716;&#31227;&#21040;ECG&#23884;&#20837;&#20013;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#20165;&#26681;&#25454;ECG&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#24515;&#33039;&#31579;&#26597;&#65292;&#24182;&#33021;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#20010;&#20307;&#39118;&#38505;&#21644;&#30830;&#23450;&#24515;&#33039;&#34920;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270; (ECG) &#26159;&#19968;&#31181;&#24191;&#27867;&#21487;&#29992;&#30340;&#35786;&#26029;&#24037;&#20855;&#65292;&#21487;&#20197;&#24555;&#36895;&#21644;&#32463;&#27982;&#39640;&#25928;&#22320;&#35780;&#20272;&#24515;&#34880;&#31649;&#20581;&#24247;&#29366;&#20917;&#12290;&#28982;&#32780;&#65292;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#35786;&#26029;&#20013;&#65292;&#36890;&#24120;&#26356;&#21916;&#27426;&#20351;&#29992;&#26114;&#36149;&#30340;&#24515;&#33039;&#30913;&#20849;&#25391; (CMR) &#25104;&#20687;&#36827;&#34892;&#26356;&#35814;&#32454;&#30340;&#26816;&#26597;&#12290;&#34429;&#28982; CMR &#25104;&#20687;&#21487;&#20197;&#25552;&#20379;&#35814;&#32454;&#30340;&#24515;&#33039;&#35299;&#21078;&#21487;&#35270;&#21270;&#65292;&#20294;&#30001;&#20110;&#38271;&#26102;&#38388;&#25195;&#25551;&#21644;&#39640;&#26114;&#30340;&#36153;&#29992;&#65292;&#23427;&#24182;&#19981;&#24191;&#27867;&#21487;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19968;&#31181;&#33258;&#30417;&#30563;&#23545;&#27604;&#26041;&#27861;&#65292;&#23558;CMR&#22270;&#20687;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#36716;&#31227;&#21040;ECG&#23884;&#20837;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#19982;&#23631;&#34109;&#25968;&#25454;&#24314;&#27169;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20165;&#26681;&#25454;ECG&#25968;&#25454;&#36827;&#34892;&#20840;&#38754;&#30340;&#24515;&#33039;&#31579;&#26597;&#12290;&#22312;&#20351;&#29992;&#26469;&#33258;40044&#21517;UK Biobank&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#12290;&#25105;&#20204;&#39044;&#27979;&#20102;&#21508;&#31181;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#20010;&#20307;&#39118;&#38505;&#65292;&#24182;&#20165;&#26681;&#25454;ECG&#25968;&#25454;&#30830;&#23450;&#20102;&#19981;&#21516;&#30340;&#24515;&#33039;&#34920;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The electrocardiogram (ECG) is a widely available diagnostic tool that allows for a cost-effective and fast assessment of the cardiovascular health. However, more detailed examination with expensive cardiac magnetic resonance (CMR) imaging is often preferred for the diagnosis of cardiovascular diseases. While providing detailed visualization of the cardiac anatomy, CMR imaging is not widely available due to long scan times and high costs. To address this issue, we propose the first self-supervised contrastive approach that transfers domain-specific information from CMR images to ECG embeddings. Our approach combines multimodal contrastive learning with masked data modeling to enable holistic cardiac screening solely from ECG data. In extensive experiments using data from 40,044 UK Biobank subjects, we demonstrate the utility and generalizability of our method. We predict the subject-specific risk of various cardiovascular diseases and determine distinct cardiac phenotypes solely from E
&lt;/p&gt;</description></item></channel></rss>