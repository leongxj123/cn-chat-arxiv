<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#21160;&#24577;&#22522;&#20934;NPHardEval4V&#65292;&#21457;&#29616;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#19981;&#21516;&#27169;&#22411;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#25581;&#31034;&#20102;&#30456;&#23545;&#20110;LLMs&#65292;MLLMs&#30340;&#25512;&#29702;&#24615;&#33021;&#36739;&#24369;&#12290;</title><link>https://arxiv.org/abs/2403.01777</link><description>&lt;p&gt;
NPHardEval4V: &#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#25512;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01777
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#21160;&#24577;&#22522;&#20934;NPHardEval4V&#65292;&#21457;&#29616;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#19981;&#21516;&#27169;&#22411;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#25581;&#31034;&#20102;&#30456;&#23545;&#20110;LLMs&#65292;MLLMs&#30340;&#25512;&#29702;&#24615;&#33021;&#36739;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#24577;&#22522;&#20934;&#65292;NPHardEval4V&#65292;&#26088;&#22312;&#35299;&#20915;&#35780;&#20272;MLLM&#32431;&#31929;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#29616;&#26377;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#24179;&#21488;&#65292;&#20197;&#35299;&#24320;&#35832;&#22810;&#22240;&#32032;&#65288;&#22914;&#22270;&#20687;&#35782;&#21035;&#21644;&#25351;&#20196;&#36981;&#24490;&#65289;&#23545;&#27169;&#22411;&#25972;&#20307;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#19987;&#27880;&#20110;&#35780;&#20272;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#36739;&#20110;LLMs&#65292;MLLMs&#22312;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#30456;&#23545;&#36739;&#24369;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#25552;&#31034;&#26679;&#24335;&#65288;&#21253;&#25324;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#32467;&#21512;&#35270;&#35273;&#19982;&#25991;&#26412;&#25552;&#31034;&#65289;&#23545;MLLM&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;&#36755;&#20837;&#22312;&#27169;&#22411;&#24615;&#33021;&#20013;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01777v1 Announce Type: new  Abstract: Understanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V, aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall performance of the models, allowing us to focus solely on evaluating their reasoning abilities. Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles, including visual, text, and combined vision and text prompts, on the reasoning abilities of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. U
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21387;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#20195;&#26367;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#20687;&#24674;&#22797;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.17744</link><description>&lt;p&gt;
&#20351;&#29992;&#21387;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Variational Bayes image restoration with compressive autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17744
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21387;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#20195;&#26367;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#20687;&#24674;&#22797;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#38382;&#39064;&#30340;&#27491;&#21017;&#21270;&#22312;&#35745;&#31639;&#25104;&#20687;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26377;&#25928;&#22270;&#20687;&#34920;&#31034;&#30340;&#33021;&#21147;&#24050;&#34987;&#21033;&#29992;&#26469;&#35774;&#35745;&#24378;&#22823;&#30340;&#25968;&#25454;&#39537;&#21160;&#27491;&#21017;&#21270;&#22120;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20351;&#29992;&#21387;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#12290;&#36825;&#20123;&#32593;&#32476;&#21487;&#20197;&#34987;&#30475;&#20316;&#20855;&#26377;&#28789;&#27963;&#28508;&#22312;&#20808;&#39564;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#27604;&#36215;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#26356;&#23567;&#26356;&#23481;&#26131;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17744v2 Announce Type: replace-cross  Abstract: Regularization of inverse problems is of paramount importance in computational imaging. The ability of neural networks to learn efficient image representations has been recently exploited to design powerful data-driven regularizers. While state-of-the-art plug-and-play methods rely on an implicit regularization provided by neural denoisers, alternative Bayesian approaches consider Maximum A Posteriori (MAP) estimation in the latent space of a generative model, thus with an explicit regularization. However, state-of-the-art deep generative models require a huge amount of training data compared to denoisers. Besides, their complexity hampers the optimization involved in latent MAP derivation. In this work, we first propose to use compressive autoencoders instead. These networks, which can be seen as variational autoencoders with a flexible latent prior, are smaller and easier to train than state-of-the-art generative models. As a
&lt;/p&gt;</description></item></channel></rss>