<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26410;&#32463;&#35757;&#32451;&#30340;&#38543;&#26426;&#26435;&#37325;&#32593;&#32476;&#65292;&#21457;&#29616;&#21363;&#20351;&#31616;&#21333;&#30340;MLPs&#20063;&#20855;&#26377;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#35266;&#28857;&#30340;&#26159;&#65292;NNs&#24182;&#19981;&#20855;&#26377;&#22266;&#26377;&#30340;&#8220;&#31616;&#21333;&#20559;&#35265;&#8221;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.02241</link><description>&lt;p&gt;
&#31070;&#32463;&#32418;&#31227;&#65306;&#38543;&#26426;&#32593;&#32476;&#24182;&#38750;&#38543;&#26426;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Neural Redshift: Random Networks are not Random Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26410;&#32463;&#35757;&#32451;&#30340;&#38543;&#26426;&#26435;&#37325;&#32593;&#32476;&#65292;&#21457;&#29616;&#21363;&#20351;&#31616;&#21333;&#30340;MLPs&#20063;&#20855;&#26377;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#35266;&#28857;&#30340;&#26159;&#65292;NNs&#24182;&#19981;&#20855;&#26377;&#22266;&#26377;&#30340;&#8220;&#31616;&#21333;&#20559;&#35265;&#8221;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35299;&#20173;&#19981;&#23436;&#25972;&#12290;&#30446;&#21069;&#30340;&#35299;&#37322;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30340;&#38544;&#21547;&#20559;&#35265;&#65292;&#20294;&#26080;&#27861;&#35299;&#37322;&#26799;&#24230;&#33258;&#30001;&#26041;&#27861;&#20013;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20063;&#26080;&#27861;&#35299;&#37322;&#26368;&#36817;&#35266;&#23519;&#21040;&#30340;&#26410;&#32463;&#35757;&#32451;&#32593;&#32476;&#30340;&#31616;&#21333;&#20559;&#35265;&#12290;&#26412;&#25991;&#23547;&#25214;NNs&#20013;&#30340;&#20854;&#20182;&#27867;&#21270;&#28304;&#12290;&#20026;&#20102;&#29420;&#31435;&#20110;GD&#29702;&#35299;&#20307;&#31995;&#32467;&#26500;&#25552;&#20379;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#25105;&#20204;&#30740;&#31350;&#26410;&#32463;&#35757;&#32451;&#30340;&#38543;&#26426;&#26435;&#37325;&#32593;&#32476;&#12290;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;MLPs&#20063;&#34920;&#29616;&#20986;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65306;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#36827;&#34892;&#22343;&#21248;&#25277;&#26679;&#20250;&#20135;&#29983;&#19968;&#20010;&#38750;&#24120;&#20559;&#21521;&#20110;&#22797;&#26434;&#24615;&#30340;&#20989;&#25968;&#20998;&#24067;&#12290;&#20294;&#19982;&#24120;&#35268;&#26234;&#24935;&#19981;&#21516;&#65292;NNs&#24182;&#19981;&#20855;&#26377;&#22266;&#26377;&#30340;&#8220;&#31616;&#21333;&#20559;&#35265;&#8221;&#12290;&#36825;&#19968;&#29305;&#24615;&#21462;&#20915;&#20110;&#32452;&#20214;&#65292;&#22914;ReLU&#12289;&#27531;&#24046;&#36830;&#25509;&#21644;&#23618;&#24402;&#19968;&#21270;&#12290;&#21487;&#21033;&#29992;&#26367;&#20195;&#20307;&#31995;&#32467;&#26500;&#26500;&#24314;&#20559;&#21521;&#20110;&#20219;&#20309;&#22797;&#26434;&#24615;&#27700;&#24179;&#30340;&#20559;&#35265;&#12290;Transformers&#20063;&#20855;&#26377;&#36825;&#19968;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02241v1 Announce Type: cross  Abstract: Our understanding of the generalization capabilities of neural networks (NNs) is still incomplete. Prevailing explanations are based on implicit biases of gradient descent (GD) but they cannot account for the capabilities of models from gradient-free methods nor the simplicity bias recently observed in untrained networks. This paper seeks other sources of generalization in NNs.   Findings. To understand the inductive biases provided by architectures independently from GD, we examine untrained, random-weight networks. Even simple MLPs show strong inductive biases: uniform sampling in weight space yields a very biased distribution of functions in terms of complexity. But unlike common wisdom, NNs do not have an inherent "simplicity bias". This property depends on components such as ReLUs, residual connections, and layer normalizations. Alternative architectures can be built with a bias for any level of complexity. Transformers also inher
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#20445;&#25345;&#36523;&#20221;&#30340;&#12289;&#30643;&#23380;&#23610;&#23544;&#21464;&#21270;&#30340;&#34425;&#33180;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21512;&#25104;&#19981;&#21516;&#30643;&#23380;&#23610;&#23544;&#30340;&#34425;&#33180;&#22270;&#20687;&#65292;&#20195;&#34920;&#19981;&#23384;&#22312;&#30340;&#36523;&#20221;&#65292;&#24182;&#33021;&#22815;&#22312;&#20445;&#25345;&#36523;&#20221;&#30340;&#21516;&#26102;&#36827;&#34892;&#38750;&#32447;&#24615;&#32441;&#29702;&#21464;&#24418;&#12290;</title><link>http://arxiv.org/abs/2312.12028</link><description>&lt;p&gt;
EyePreserve: &#20445;&#25345;&#36523;&#20221;&#30340;&#34425;&#33180;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
EyePreserve: Identity-Preserving Iris Synthesis. (arXiv:2312.12028v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#20445;&#25345;&#36523;&#20221;&#30340;&#12289;&#30643;&#23380;&#23610;&#23544;&#21464;&#21270;&#30340;&#34425;&#33180;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21512;&#25104;&#19981;&#21516;&#30643;&#23380;&#23610;&#23544;&#30340;&#34425;&#33180;&#22270;&#20687;&#65292;&#20195;&#34920;&#19981;&#23384;&#22312;&#30340;&#36523;&#20221;&#65292;&#24182;&#33021;&#22815;&#22312;&#20445;&#25345;&#36523;&#20221;&#30340;&#21516;&#26102;&#36827;&#34892;&#38750;&#32447;&#24615;&#32441;&#29702;&#21464;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#30340;&#30643;&#23380;&#23610;&#23544;&#33539;&#22260;&#20869;&#20445;&#25345;&#36523;&#20221;&#30340;&#21516;&#36523;&#20221;&#29983;&#29289;&#29305;&#24449;&#34425;&#33180;&#22270;&#20687;&#30340;&#21512;&#25104;&#26159;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#34425;&#33180;&#32908;&#32905;&#25910;&#32553;&#26426;&#21046;&#65292;&#38656;&#35201;&#23558;&#34425;&#33180;&#38750;&#32447;&#24615;&#32441;&#29702;&#21464;&#24418;&#27169;&#22411;&#23884;&#20837;&#21040;&#21512;&#25104;&#27969;&#31243;&#20013;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#20445;&#25345;&#36523;&#20221;&#30340;&#12289;&#30643;&#23380;&#23610;&#23544;&#21464;&#21270;&#30340;&#34425;&#33180;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#19981;&#21516;&#30643;&#23380;&#23610;&#23544;&#30340;&#34425;&#33180;&#22270;&#20687;&#65292;&#20195;&#34920;&#19981;&#23384;&#22312;&#30340;&#36523;&#20221;&#65292;&#24182;&#33021;&#22815;&#22312;&#32473;&#23450;&#30446;&#26631;&#34425;&#33180;&#22270;&#20687;&#30340;&#20998;&#21106;&#25513;&#33180;&#19979;&#38750;&#32447;&#24615;&#22320;&#21464;&#24418;&#29616;&#26377;&#20027;&#20307;&#30340;&#34425;&#33180;&#22270;&#20687;&#32441;&#29702;&#12290;&#34425;&#33180;&#35782;&#21035;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#21464;&#24418;&#27169;&#22411;&#19981;&#20165;&#22312;&#25913;&#21464;&#30643;&#23380;&#23610;&#23544;&#26102;&#20445;&#25345;&#36523;&#20221;&#65292;&#32780;&#19988;&#22312;&#30643;&#23380;&#23610;&#23544;&#26377;&#26174;&#33879;&#24046;&#24322;&#30340;&#21516;&#36523;&#20221;&#34425;&#33180;&#26679;&#26412;&#20043;&#38388;&#25552;&#20379;&#26356;&#22909;&#30340;&#30456;&#20284;&#24230;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#32447;&#24615;&#26041;&#27861;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesis of same-identity biometric iris images, both for existing and non-existing identities while preserving the identity across a wide range of pupil sizes, is complex due to intricate iris muscle constriction mechanism, requiring a precise model of iris non-linear texture deformations to be embedded into the synthesis pipeline. This paper presents the first method of fully data-driven, identity-preserving, pupil size-varying s ynthesis of iris images. This approach is capable of synthesizing images of irises with different pupil sizes representing non-existing identities as well as non-linearly deforming the texture of iris images of existing subjects given the segmentation mask of the target iris image. Iris recognition experiments suggest that the proposed deformation model not only preserves the identity when changing the pupil size but offers better similarity between same-identity iris samples with significant differences in pupil size, compared to state-of-the-art linear an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEyes&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#30524;&#21160;&#36319;&#36394;&#26694;&#26550;&#65292;&#21033;&#29992;&#21512;&#25104;&#30524;&#37096;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#19981;&#36275;&#21644;&#30524;&#37096;&#22270;&#20687;&#21464;&#24322;&#23548;&#33268;&#30340;&#27169;&#22411;&#27867;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LEyes&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30643;&#23380;&#21644;CR&#23450;&#20301;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.06129</link><description>&lt;p&gt;
LEyes&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#30524;&#21160;&#36319;&#36394;&#26694;&#26550;&#65292;&#20351;&#29992;&#21512;&#25104;&#30524;&#37096;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LEyes: A Lightweight Framework for Deep Learning-Based Eye Tracking using Synthetic Eye Images. (arXiv:2309.06129v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEyes&#30340;&#36731;&#37327;&#32423;&#28145;&#24230;&#23398;&#20064;&#30524;&#21160;&#36319;&#36394;&#26694;&#26550;&#65292;&#21033;&#29992;&#21512;&#25104;&#30524;&#37096;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#19981;&#36275;&#21644;&#30524;&#37096;&#22270;&#20687;&#21464;&#24322;&#23548;&#33268;&#30340;&#27169;&#22411;&#27867;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LEyes&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30643;&#23380;&#21644;CR&#23450;&#20301;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#21152;&#24378;&#20102;&#20957;&#35270;&#20272;&#35745;&#25216;&#26415;&#65292;&#20294;&#23454;&#38469;&#37096;&#32626;&#21463;&#21040;&#19981;&#36275;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#12290;&#30524;&#37096;&#22270;&#20687;&#30340;&#30828;&#20214;&#24341;&#36215;&#30340;&#21464;&#24322;&#20197;&#21450;&#35760;&#24405;&#30340;&#21442;&#19982;&#32773;&#20043;&#38388;&#22266;&#26377;&#30340;&#29983;&#29289;&#24046;&#24322;&#20250;&#23548;&#33268;&#29305;&#24449;&#21644;&#20687;&#32032;&#32423;&#21035;&#30340;&#24046;&#24322;&#65292;&#38459;&#30861;&#20102;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#34394;&#25311;&#25968;&#25454;&#38598;&#21487;&#20197;&#26159;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#21019;&#24314;&#34394;&#25311;&#25968;&#25454;&#38598;&#26082;&#38656;&#35201;&#26102;&#38388;&#21448;&#38656;&#35201;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Light Eyes or "LEyes"&#30340;&#26694;&#26550;&#65292;&#19982;&#20256;&#32479;&#30340;&#36924;&#30495;&#26041;&#27861;&#19981;&#21516;&#65292;LEyes&#20165;&#27169;&#25311;&#35270;&#39057;&#30524;&#21160;&#36319;&#36394;&#25152;&#38656;&#30340;&#20851;&#38190;&#22270;&#20687;&#29305;&#24449;&#12290;LEyes&#20415;&#20110;&#22312;&#22810;&#26679;&#21270;&#30340;&#20957;&#35270;&#20272;&#35745;&#20219;&#21153;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;LEyes&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#30524;&#30555;&#30643;&#23380;&#21644;CR&#23450;&#20301;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has bolstered gaze estimation techniques, but real-world deployment has been impeded by inadequate training datasets. This problem is exacerbated by both hardware-induced variations in eye images and inherent biological differences across the recorded participants, leading to both feature and pixel-level variance that hinders the generalizability of models trained on specific datasets. While synthetic datasets can be a solution, their creation is both time and resource-intensive. To address this problem, we present a framework called Light Eyes or "LEyes" which, unlike conventional photorealistic methods, only models key image features required for video-based eye tracking using simple light distributions. LEyes facilitates easy configuration for training neural networks across diverse gaze-estimation tasks. We demonstrate that models trained using LEyes outperform other state-of-the-art algorithms in terms of pupil and CR localization across well-known datasets. In addit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#27491;&#21521;&#23545;&#38598;&#21512;&#65288;SPPS&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#20013;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#65292;&#24182;&#23558;&#23427;&#20204;&#35270;&#20026;&#27491;&#21521;&#23454;&#20363;&#65292;&#20174;&#32780;&#20943;&#23569;&#20002;&#24323;&#37325;&#35201;&#29305;&#24449;&#30340;&#39118;&#38505;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16122</link><description>&lt;p&gt;
&#22686;&#24378;&#23545;&#27604;&#23454;&#20363;&#21306;&#20998;&#30340;&#35821;&#20041;&#27491;&#21521;&#23545;
&lt;/p&gt;
&lt;p&gt;
Semantic Positive Pairs for Enhancing Contrastive Instance Discrimination. (arXiv:2306.16122v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16122
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#27491;&#21521;&#23545;&#38598;&#21512;&#65288;SPPS&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#20013;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#65292;&#24182;&#23558;&#23427;&#20204;&#35270;&#20026;&#27491;&#21521;&#23454;&#20363;&#65292;&#20174;&#32780;&#20943;&#23569;&#20002;&#24323;&#37325;&#35201;&#29305;&#24449;&#30340;&#39118;&#38505;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23454;&#20363;&#21306;&#20998;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#26377;&#25928;&#22320;&#38450;&#27490;&#34920;&#31034;&#22349;&#32553;&#65292;&#24182;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#20135;&#29983;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#21560;&#24341;&#27491;&#21521;&#23545;&#65288;&#21363;&#30456;&#21516;&#23454;&#20363;&#30340;&#20004;&#20010;&#35270;&#22270;&#65289;&#24182;&#25490;&#26021;&#25152;&#26377;&#20854;&#20182;&#23454;&#20363;&#65288;&#21363;&#36127;&#21521;&#23545;&#65289;&#65292;&#26080;&#35770;&#23427;&#20204;&#30340;&#31867;&#21035;&#65292;&#21487;&#33021;&#23548;&#33268;&#20002;&#24323;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#35821;&#20041;&#20869;&#23481;&#30340;&#22270;&#20687;&#65292;&#24182;&#23558;&#23427;&#20204;&#35270;&#20026;&#27491;&#21521;&#23454;&#20363;&#65292;&#21629;&#21517;&#20026;&#35821;&#20041;&#27491;&#21521;&#23545;&#38598;&#21512;&#65288;SPPS&#65289;&#65292;&#20174;&#32780;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#20943;&#23569;&#20102;&#20002;&#24323;&#37325;&#35201;&#29305;&#24449;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#19982;&#20219;&#20309;&#23545;&#27604;&#23454;&#20363;&#21306;&#20998;&#26694;&#26550;&#65288;&#22914;SimCLR&#25110;MOCO&#65289;&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;ImageNet&#12289;STL-10&#21644;CIFAR-10&#65289;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;vanilla&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning algorithms based on instance discrimination effectively prevent representation collapse and produce promising results in representation learning. However, the process of attracting positive pairs (i.e., two views of the same instance) in the embedding space and repelling all other instances (i.e., negative pairs) irrespective of their categories could result in discarding important features. To address this issue, we propose an approach to identifying those images with similar semantic content and treating them as positive instances, named semantic positive pairs set (SPPS), thereby reducing the risk of discarding important features during representation learning. Our approach could work with any contrastive instance discrimination framework such as SimCLR or MOCO. We conduct experiments on three datasets: ImageNet, STL-10 and CIFAR-10 to evaluate our approach. The experimental results show that our approach consistently outperforms the baseline method vanilla 
&lt;/p&gt;</description></item></channel></rss>