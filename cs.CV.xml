<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.19888</link><description>&lt;p&gt;
MambaMixer&#65306;&#20855;&#26377;&#21452;&#37325;&#26631;&#35760;&#21644;&#36890;&#36947;&#36873;&#25321;&#30340;&#39640;&#25928;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19888
&lt;/p&gt;
&lt;p&gt;
MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;Transformers&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#24615;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#22823;&#35268;&#27169;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26550;&#26500;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#23637;&#29616;&#20986;&#36755;&#20837;&#22823;&#23567;&#30340;&#20108;&#27425;&#26102;&#38388;&#21644;&#31354;&#38388;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#23581;&#35797;&#20026;&#22810;&#32500;&#25968;&#25454;&#35774;&#35745;&#39640;&#25928;&#26377;&#25928;&#30340;&#26550;&#26500;&#20027;&#24178;&#65292;&#20363;&#22914;&#22270;&#20687;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#35201;&#20040;&#26159;&#25968;&#25454;&#29420;&#31435;&#30340;&#65292;&#35201;&#20040;&#26080;&#27861;&#20801;&#35768;&#36328;&#32500;&#24230;&#21644;&#20869;&#37096;&#32500;&#24230;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#26368;&#36817;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#39640;&#25928;&#30828;&#20214;&#24863;&#30693;&#23454;&#29616;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#23637;&#29616;&#20986;&#20102;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#21463;&#21040;SSMs&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MambaMixer&#65292;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#36328;&#26631;&#35760;&#21644;&#36890;&#36947;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19888v1 Announce Type: cross  Abstract: Recent advances in deep learning have mainly relied on Transformers due to their data dependency and ability to learn at scale. The attention module in these architectures, however, exhibits quadratic time and space in input size, limiting their scalability for long-sequence modeling. Despite recent attempts to design efficient and effective architecture backbone for multi-dimensional data, such as images and multivariate time series, existing models are either data independent, or fail to allow inter- and intra-dimension communication. Recently, State Space Models (SSMs), and more specifically Selective State Space Models, with efficient hardware-aware implementation, have shown promising potential for long sequence modeling. Motivated by the success of SSMs, we present MambaMixer, a new architecture with data-dependent weights that uses a dual selection mechanism across tokens and channels, called Selective Token and Channel Mixer. M
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181; PathM3 &#26694;&#26550;&#65292;&#29992;&#20110;WSI&#20998;&#31867;&#21644;&#23383;&#24149;&#29983;&#25104;&#65292;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26597;&#35810;&#24335;transformer&#26377;&#25928;&#23545;&#40784;WSIs&#19982;&#35786;&#26029;&#24615;&#23383;&#24149;&#12290;</title><link>https://arxiv.org/abs/2403.08967</link><description>&lt;p&gt;
PathM3: &#19968;&#31181;&#29992;&#20110;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#21644;&#23383;&#24149;&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#22810;&#23454;&#20363;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PathM3: A Multimodal Multi-Task Multiple Instance Learning Framework for Whole Slide Image Classification and Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08967
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181; PathM3 &#26694;&#26550;&#65292;&#29992;&#20110;WSI&#20998;&#31867;&#21644;&#23383;&#24149;&#29983;&#25104;&#65292;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26597;&#35810;&#24335;transformer&#26377;&#25928;&#23545;&#40784;WSIs&#19982;&#35786;&#26029;&#24615;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#32452;&#32455;&#30149;&#29702;&#23398;&#39046;&#22495;&#65292;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#21644;&#35786;&#26029;&#24615;&#23383;&#24149;&#37117;&#20026;&#20570;&#20986;&#35786;&#26029;&#20915;&#31574;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#23558;WSIs&#19982;&#35786;&#26029;&#24615;&#23383;&#24149;&#23545;&#40784;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#33258;&#20004;&#20010;&#22240;&#32032;&#65306;1&#65289;&#24040;&#22411;&#20687;&#32032;WSIs&#19981;&#36866;&#21512;&#30452;&#25509;&#36755;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#22270;&#22359;&#38388;&#30340;&#20887;&#20313;&#24615;&#21644;&#30456;&#20851;&#24615;&#35201;&#27714;&#26356;&#22810;&#27880;&#24847;&#65307;2&#65289;&#30495;&#23454;&#30340;WSI&#35786;&#26029;&#24615;&#23383;&#24149;&#26497;&#20854;&#26377;&#38480;&#65292;&#20351;&#24471;&#38590;&#20197;&#35757;&#32451;&#20986;&#26377;&#25928;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08967v1 Announce Type: cross  Abstract: In the field of computational histopathology, both whole slide images (WSIs) and diagnostic captions provide valuable insights for making diagnostic decisions. However, aligning WSIs with diagnostic captions presents a significant challenge. This difficulty arises from two main factors: 1) Gigapixel WSIs are unsuitable for direct input into deep learning models, and the redundancy and correlation among the patches demand more attention; and 2) Authentic WSI diagnostic captions are extremely limited, making it difficult to train an effective model. To overcome these obstacles, we present PathM3, a multimodal, multi-task, multiple instance learning (MIL) framework for WSI classification and captioning. PathM3 adapts a query-based transformer to effectively align WSIs with diagnostic captions. Given that histopathology visual patterns are redundantly distributed across WSIs, we aggregate each patch feature with MIL method that considers t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;MM-Soc&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#23545;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#29702;&#35299;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#36890;&#36807;&#23545;&#21313;&#31181;&#22823;&#23567;&#21464;&#20307;&#30340;&#22235;&#20010;&#24320;&#28304;MLLMs&#36827;&#34892;&#35814;&#23613;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.14154</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14154
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;MM-Soc&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#23545;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#29702;&#35299;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#36890;&#36807;&#23545;&#21313;&#31181;&#22823;&#23567;&#21464;&#20307;&#30340;&#22235;&#20010;&#24320;&#28304;MLLMs&#36827;&#34892;&#35814;&#23613;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26159;&#22810;&#27169;&#24577;&#20449;&#24687;&#20132;&#27969;&#30340;&#20013;&#24515;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#29255;&#21644;&#35270;&#39057;&#65292;&#36825;&#20351;&#24471;&#26426;&#22120;&#38590;&#20197;&#29702;&#35299;&#22312;&#32447;&#31354;&#38388;&#20013;&#20132;&#20114;&#25152;&#20851;&#32852;&#30340;&#20449;&#24687;&#25110;&#24773;&#32490;&#12290;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#20934;&#30830;&#35299;&#37322;&#20154;&#31867;&#24773;&#32490;&#21644;&#35832;&#22914;&#34394;&#20551;&#20449;&#24687;&#31561;&#22797;&#26434;&#20869;&#23481;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MM-Soc&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;MLLMs&#23545;&#22810;&#27169;&#24577;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#29702;&#35299;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;MM-Soc&#25972;&#21512;&#20102;&#33879;&#21517;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#24182;&#34701;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#35268;&#27169;YouTube&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#38024;&#23545;&#20174;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#12289;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21040;&#31038;&#20132;&#19978;&#19979;&#25991;&#29983;&#25104;&#31561;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#24320;&#28304;MLLMs&#30340;&#21313;&#31181;&#19981;&#21516;&#35268;&#27169;&#21464;&#20307;&#36827;&#34892;&#35814;&#23613;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#20984;&#26174;&#20986;&#20102;&#23545;&#24615;&#33021;&#24179;&#34913;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14154v1 Announce Type: new  Abstract: Social media platforms are hubs for multimodal information exchange, encompassing text, images, and videos, making it challenging for machines to comprehend the information or emotions associated with interactions in online spaces. Multimodal Large Language Models (MLLMs) have emerged as a promising solution to address these challenges, yet struggle with accurately interpreting human emotions and complex contents like misinformation. This paper introduces MM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of multimodal social media content. MM-Soc compiles prominent multimodal datasets and incorporates a novel large-scale YouTube tagging dataset, targeting a range of tasks from misinformation detection, hate speech detection, and social context generation. Through our exhaustive evaluation on ten size-variants of four open-source MLLMs, we have identified significant performance disparities, highlighting the need
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#25216;&#26415;&#35282;&#24230;&#20840;&#38754;&#27010;&#36848;&#20102;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#29256;&#26435;&#20445;&#25252;&#38382;&#39064;&#65292;&#21253;&#25324;&#25968;&#25454;&#29256;&#26435;&#21644;&#27169;&#22411;&#29256;&#26435;&#20004;&#20010;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21019;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.02333</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#29256;&#26435;&#20445;&#25252;&#65306;&#25216;&#26415;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Copyright Protection in Generative AI: A Technical Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#25216;&#26415;&#35282;&#24230;&#20840;&#38754;&#27010;&#36848;&#20102;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#29256;&#26435;&#20445;&#25252;&#38382;&#39064;&#65292;&#21253;&#25324;&#25968;&#25454;&#29256;&#26435;&#21644;&#27169;&#22411;&#29256;&#26435;&#20004;&#20010;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21019;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;Generative AI&#65289;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#25193;&#23637;&#20102;&#20854;&#21019;&#24314;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#20195;&#30721;&#31561;&#21512;&#25104;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;Deep Generative Models&#65292;DGMs&#65289;&#29983;&#25104;&#30340;&#20869;&#23481;&#39640;&#20445;&#30495;&#24230;&#21644;&#30495;&#23454;&#24615;&#24341;&#21457;&#20102;&#37325;&#22823;&#30340;&#29256;&#26435;&#38382;&#39064;&#12290;&#20851;&#20110;&#22914;&#20309;&#26377;&#25928;&#20445;&#25252;DGMs&#20013;&#30340;&#29256;&#26435;&#38382;&#39064;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#21508;&#31181;&#27861;&#24459;&#36777;&#35770;&#12290;&#26412;&#25991;&#20174;&#25216;&#26415;&#35282;&#24230;&#25552;&#20379;&#20102;&#29256;&#26435;&#20445;&#25252;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#20174;&#20004;&#20010;&#19981;&#21516;&#30340;&#35270;&#35282;&#26469;&#36827;&#34892;&#30740;&#31350;&#65306;&#19968;&#26159;&#19982;&#25968;&#25454;&#25152;&#26377;&#32773;&#25152;&#25345;&#26377;&#30340;&#28304;&#25968;&#25454;&#30456;&#20851;&#30340;&#29256;&#26435;&#65292;&#20108;&#26159;&#19982;&#27169;&#22411;&#26500;&#24314;&#32773;&#25152;&#32500;&#25252;&#30340;&#29983;&#25104;&#27169;&#22411;&#30456;&#20851;&#30340;&#29256;&#26435;&#12290;&#23545;&#20110;&#25968;&#25454;&#29256;&#26435;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#25152;&#26377;&#32773;&#22914;&#20309;&#20445;&#25252;&#20854;&#20869;&#23481;&#65292;&#24182;&#22312;&#19981;&#20405;&#29359;&#36825;&#20123;&#26435;&#21033;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;DGMs&#12290;&#23545;&#20110;&#27169;&#22411;&#29256;&#26435;&#65292;&#25105;&#20204;&#30340;&#35752;&#35770;&#24310;&#20280;&#21040;&#38450;&#27490;&#27169;&#22411;&#30423;&#31363;&#21644;&#35782;&#21035;&#29305;&#23450;&#27169;&#22411;&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#20123;&#21019;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#26469;&#22788;&#29702;&#36825;&#20123;&#29256;&#26435;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Vid-LLMs&#65289;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;Vid-LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#21253;&#25324;&#24320;&#25918;&#24335;&#26102;&#31354;&#25512;&#29702;&#21644;&#24120;&#35782;&#30693;&#35782;&#65292;&#20026;&#26410;&#26469;&#30340;&#35270;&#39057;&#29702;&#35299;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2312.17432</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Video Understanding with Large Language Models: A Survey. (arXiv:2312.17432v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17432
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Vid-LLMs&#65289;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;Vid-LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#21253;&#25324;&#24320;&#25918;&#24335;&#26102;&#31354;&#25512;&#29702;&#21644;&#24120;&#35782;&#30693;&#35782;&#65292;&#20026;&#26410;&#26469;&#30340;&#35270;&#39057;&#29702;&#35299;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#32447;&#35270;&#39057;&#24179;&#21488;&#30340;&#19981;&#26029;&#22686;&#38271;&#21644;&#35270;&#39057;&#20869;&#23481;&#30340;&#19981;&#26029;&#22686;&#22810;&#65292;&#23545;&#29087;&#32451;&#30340;&#35270;&#39057;&#29702;&#35299;&#24037;&#20855;&#30340;&#38656;&#27714;&#26174;&#33879;&#22686;&#21152;&#12290;&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Vid-LLMs&#65289;&#25216;&#26415;&#36827;&#34892;&#35270;&#39057;&#29702;&#35299;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;Vid-LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#20196;&#20154;&#24778;&#35766;&#65292;&#23588;&#20854;&#26159;&#23427;&#20204;&#22312;&#24320;&#25918;&#24335;&#26102;&#31354;&#25512;&#29702;&#21644;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20026;&#26410;&#26469;&#30340;&#35270;&#39057;&#29702;&#35299;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#26412;&#35843;&#26597;&#23545;Vid-LLMs&#30340;&#29420;&#29305;&#29305;&#28857;&#21644;&#33021;&#21147;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#20998;&#20026;&#22235;&#31181;&#20027;&#35201;&#31867;&#22411;&#65306;&#22522;&#20110;LLM&#30340;&#35270;&#39057;&#20195;&#29702;&#12289;Vid-LLMs&#30340;&#39044;&#35757;&#32451;&#12289;Vid-LLMs&#30340;&#25351;&#20196;&#35843;&#25972;&#21644;&#28151;&#21512;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#35843;&#26597;&#23545;Vid-LLMs&#30340;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#21478;&#22806;&#65292;&#23427;&#36824;&#25506;&#35752;&#20102;Vid-LLMs&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly. Given the remarkable capabilities of Large Language Models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of the recent advancements in video understanding harnessing the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended spatial-temporal reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into four main types: LLM-based Video Agents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods. Furthermore, this survey presents a comprehensive study of the tasks, datasets, and evaluation methodologies for Vid-LLMs. Additionally, it explores 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#20219;&#21153;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#24615;&#65292;&#20248;&#21270;&#26399;&#26395;&#20934;&#30830;&#29575;&#65292;&#21462;&#24471;&#20102;&#24378;&#26377;&#21147;&#30340;&#26367;&#20195;&#20998;&#31867;&#25439;&#22833;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.09615</link><description>&lt;p&gt;
EXACT: &#22914;&#20309;&#25552;&#39640;&#20934;&#30830;&#29575;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
EXACT: How to Train Your Accuracy. (arXiv:2205.09615v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#20219;&#21153;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#24615;&#65292;&#20248;&#21270;&#26399;&#26395;&#20934;&#30830;&#29575;&#65292;&#21462;&#24471;&#20102;&#24378;&#26377;&#21147;&#30340;&#26367;&#20195;&#20998;&#31867;&#25439;&#22833;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#20219;&#21153;&#36890;&#24120;&#20250;&#20197;&#20934;&#30830;&#29575;&#20316;&#20026;&#35780;&#20272;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#29575;&#26159;&#19981;&#36830;&#32493;&#30340;&#65292;&#26080;&#27861;&#30452;&#25509;&#20351;&#29992;&#26799;&#24230;&#19978;&#21319;&#36827;&#34892;&#20248;&#21270;&#12290;&#27969;&#34892;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#12289;&#38128;&#38142;&#25439;&#22833;&#25110;&#20854;&#20182;&#26367;&#20195;&#25439;&#22833;&#26469;&#20248;&#21270;&#65292;&#20294;&#36825;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27169;&#22411;&#30340;&#36755;&#20986;&#24341;&#20837;&#38543;&#26426;&#24615;&#24182;&#20248;&#21270;&#26399;&#26395;&#20934;&#30830;&#29575;&#65292;&#21363;&#38543;&#26426;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#12290;&#23545;&#32447;&#24615;&#27169;&#22411;&#21644;&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#20248;&#21270;&#26041;&#27861;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#20998;&#31867;&#25439;&#22833;&#30340;&#24378;&#26377;&#21147;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification tasks are usually evaluated in terms of accuracy. However, accuracy is discontinuous and cannot be directly optimized using gradient ascent. Popular methods minimize cross-entropy, hinge loss, or other surrogate losses, which can lead to suboptimal results. In this paper, we propose a new optimization framework by introducing stochasticity to a model's output and optimizing expected accuracy, i.e. accuracy of the stochastic model. Extensive experiments on linear models and deep image classification show that the proposed optimization method is a powerful alternative to widely used classification losses.
&lt;/p&gt;</description></item></channel></rss>