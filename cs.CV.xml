<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;ReMamber&#65292;&#19968;&#31181;&#25972;&#21512;&#20102;Mamba&#21644;&#22810;&#27169;&#24577;Mamba Twister&#22359;&#30340;&#26032;&#22411;RIS&#26550;&#26500;&#65292;&#36890;&#36807;&#20854;&#29420;&#29305;&#30340;&#36890;&#36947;&#21644;&#31354;&#38388;&#25197;&#26354;&#26426;&#21046;&#23454;&#29616;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#65292;&#21462;&#24471;&#20102;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;</title><link>https://arxiv.org/abs/2403.17839</link><description>&lt;p&gt;
ReMamber&#65306;&#20351;&#29992;Mamba Twister&#23454;&#29616;&#24341;&#29992;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
ReMamber: Referring Image Segmentation with Mamba Twister
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17839
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ReMamber&#65292;&#19968;&#31181;&#25972;&#21512;&#20102;Mamba&#21644;&#22810;&#27169;&#24577;Mamba Twister&#22359;&#30340;&#26032;&#22411;RIS&#26550;&#26500;&#65292;&#36890;&#36807;&#20854;&#29420;&#29305;&#30340;&#36890;&#36947;&#21644;&#31354;&#38388;&#25197;&#26354;&#26426;&#21046;&#23454;&#29616;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#65292;&#21462;&#24471;&#20102;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#29992;&#22270;&#20687;&#20998;&#21106;&#65288;RIS&#65289;&#21033;&#29992;&#21464;&#25442;&#22120;&#22312;&#35299;&#37322;&#22797;&#26434;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20108;&#27425;&#35745;&#31639;&#25104;&#26412;&#20351;&#20854;&#22312;&#25429;&#25417;&#36828;&#31243;&#35270;&#35273;-&#35821;&#35328;&#20381;&#36182;&#24615;&#26041;&#38754;&#28040;&#32791;&#36164;&#28304;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;Mamba&#36890;&#36807;&#39640;&#25928;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#22312;&#22788;&#29702;&#26041;&#38754;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23558;Mamba&#30452;&#25509;&#24212;&#29992;&#20110;&#22810;&#27169;&#24577;&#20132;&#20114;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#22240;&#20026;&#36890;&#36947;&#20132;&#20114;&#19981;&#36275;&#65292;&#26080;&#27861;&#26377;&#25928;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReMamber&#65292;&#36825;&#26159;&#19968;&#31181;&#25972;&#21512;&#20102;Mamba&#21644;&#22810;&#27169;&#24577;Mamba Twister&#22359;&#24378;&#22823;&#21151;&#33021;&#30340;&#26032;&#22411;RIS&#26550;&#26500;&#12290;Mamba Twister&#36890;&#36807;&#20854;&#29420;&#29305;&#30340;&#36890;&#36947;&#21644;&#31354;&#38388;&#25197;&#26354;&#26426;&#21046;&#26126;&#30830;&#24314;&#27169;&#22270;&#20687;-&#25991;&#26412;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#20854;&#29420;&#29305;&#30340;&#36890;&#36947;&#21644;&#31354;&#38388;&#25197;&#26354;&#26426;&#21046;&#34701;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;ReMamber&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20854;&#20182;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17839v1 Announce Type: cross  Abstract: Referring Image Segmentation (RIS) leveraging transformers has achieved great success on the interpretation of complex visual-language tasks. However, the quadratic computation cost makes it resource-consuming in capturing long-range visual-language dependencies. Fortunately, Mamba addresses this with efficient linear complexity in processing. However, directly applying Mamba to multi-modal interactions presents challenges, primarily due to inadequate channel interactions for the effective fusion of multi-modal data. In this paper, we propose ReMamber, a novel RIS architecture that integrates the power of Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly models image-text interaction, and fuses textual and visual features through its unique channel and spatial twisting mechanism. We achieve the state-of-the-art on three challenging benchmarks. Moreover, we conduct thorough analyses of ReMamber and discuss other
&lt;/p&gt;</description></item><item><title>DiCoM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#20803;&#27010;&#24565;&#65292;&#26377;&#25928;&#34920;&#31034;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#65292;&#20197;&#24212;&#23545;&#21307;&#23398;&#25104;&#20687;&#39044;&#35757;&#32451;&#20013;&#19982;&#33258;&#28982;&#22270;&#20687;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15534</link><description>&lt;p&gt;
DiCoM -- &#22810;&#20803;&#27010;&#24565;&#24314;&#27169;&#20197;&#22686;&#24378;&#33016;&#37096;X&#23556;&#32447;&#30740;&#31350;&#30340;&#26222;&#36866;&#24615;
&lt;/p&gt;
&lt;p&gt;
DiCoM -- Diverse Concept Modeling towards Enhancing Generalizability in Chest X-Ray Studies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15534
&lt;/p&gt;
&lt;p&gt;
DiCoM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#20803;&#27010;&#24565;&#65292;&#26377;&#25928;&#34920;&#31034;&#33016;&#37096;X&#23556;&#32447;&#25968;&#25454;&#65292;&#20197;&#24212;&#23545;&#21307;&#23398;&#25104;&#20687;&#39044;&#35757;&#32451;&#20013;&#19982;&#33258;&#28982;&#22270;&#20687;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#20020;&#24202;&#25104;&#20687;&#27169;&#24577;&#65292;&#22312;&#21508;&#31181;&#32954;&#37096;&#21644;&#24515;&#33039;&#30456;&#20851;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20256;&#32479;&#30340;&#20381;&#36182;&#25918;&#23556;&#23398;&#35835;&#29255;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#20020;&#24202;&#35786;&#26029;&#24037;&#20855;&#35774;&#35745;&#31574;&#30053;&#38656;&#35201;&#39640;&#36136;&#37327;&#27880;&#37322;&#35757;&#32451;&#25968;&#25454;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#20013;&#32988;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20195;&#34920;&#20102;&#35813;&#39046;&#22495;&#30340;&#37325;&#22823;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#25104;&#20687;&#39044;&#35757;&#32451;&#19982;&#33258;&#28982;&#22270;&#20687;&#65288;&#20363;&#22914;ImageNet&#65289;&#30340;&#39044;&#35757;&#32451;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19981;&#21516;&#65292;&#22240;&#20026;&#20020;&#24202;&#22270;&#20687;&#20855;&#26377;&#29420;&#29305;&#23646;&#24615;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22810;&#20803;&#27010;&#24565;&#24314;&#27169;&#65288;DiCoM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;&#20102;&#23398;&#29983;&#25945;&#24072;&#26694;&#26550;&#26469;&#23398;&#20064;&#22810;&#20803;&#27010;&#24565;&#65292;&#20174;&#32780;&#26377;&#25928;&#34920;&#31034;CXR&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15534v1 Announce Type: cross  Abstract: Chest X-Ray (CXR) is a widely used clinical imaging modality and has a pivotal role in the diagnosis and prognosis of various lung and heart related conditions. Conventional automated clinical diagnostic tool design strategies relying on radiology reads and supervised learning, entail the cumbersome requirement of high quality annotated training data. To address this challenge, self-supervised pre-training has proven to outperform supervised pre-training in numerous downstream vision tasks, representing a significant breakthrough in the field. However, medical imaging pre-training significantly differs from pre-training with natural images (e.g., ImageNet) due to unique attributes of clinical images. In this context, we introduce Diverse Concept Modeling (DiCoM), a novel self-supervised training paradigm that leverages a student teacher framework for learning diverse concepts and hence effective representation of the CXR data. Hence, e
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;3D Diffuser Actor&#65292;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#26500;&#24314;3D&#35270;&#35273;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#12290;</title><link>https://arxiv.org/abs/2402.10885</link><description>&lt;p&gt;
&#22522;&#20110;3D&#22330;&#26223;&#34920;&#31034;&#30340;3D&#25193;&#25955;&#22120;Actor&#65306;&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
3D Diffuser Actor: Policy Diffusion with 3D Scene Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10885
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;3D Diffuser Actor&#65292;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#26500;&#24314;3D&#35270;&#35273;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#25193;&#25955;&#31574;&#30053;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#25193;&#25955;&#31574;&#30053;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#22522;&#20110;&#26426;&#22120;&#20154;&#21644;&#29615;&#22659;&#29366;&#24577;&#30340;&#21160;&#20316;&#20998;&#24067;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#24050;&#32463;&#34920;&#29616;&#20986;&#20248;&#20110;&#30830;&#23450;&#24615;&#21644;&#20854;&#20182;&#22522;&#20110;&#29366;&#24577;&#30340;&#21160;&#20316;&#20998;&#24067;&#23398;&#20064;&#26041;&#27861;&#12290;3D&#26426;&#22120;&#20154;&#31574;&#30053;&#20351;&#29992;&#20174;&#21333;&#20010;&#25110;&#22810;&#20010;&#25668;&#20687;&#22836;&#35270;&#35282;&#33719;&#21462;&#30340;&#24863;&#24212;&#28145;&#24230;&#32858;&#21512;&#30340;3D&#22330;&#26223;&#29305;&#24449;&#34920;&#31034;&#12290;&#23427;&#20204;&#24050;&#32463;&#35777;&#26126;&#27604;&#20854;2D&#23545;&#24212;&#29289;&#22312;&#25668;&#20687;&#26426;&#35270;&#35282;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#32479;&#19968;&#20102;&#36825;&#20004;&#26465;&#32447;&#36335;&#30340;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;3D&#25193;&#25955;&#22120;Actor&#65292;&#36825;&#26159;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#23427;&#22312;&#32473;&#23450;&#35821;&#35328;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#26500;&#24314;&#35270;&#35273;&#22330;&#26223;&#30340;3D&#34920;&#31034;&#65292;&#24182;&#22312;&#20854;&#19978;&#36827;&#34892;&#26465;&#20214;&#36845;&#20195;&#21435;&#22122;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#12290;&#22312;&#27599;&#20010;&#21435;&#22122;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#20272;&#35745;&#34920;&#31034;&#20026;3D&#22330;&#26223;&#20196;&#29260;&#65292;&#24182;&#39044;&#27979;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10885v1 Announce Type: cross  Abstract: We marry diffusion policies and 3D scene representations for robot manipulation. Diffusion policies learn the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative state-conditioned action distribution learning methods. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor, a neural policy architecture that, given a language instruction, builds a 3D representation of the visual scene and conditions on it to iteratively denoise 3D rotations and translations for the robot's end-effector. At each denoising iteration, our model represents end-effector pose estimates as 3D scene tokens and predicts t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20799;&#31461;&#30340;&#35270;&#35282;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#38271;&#26102;&#38388;&#30340;&#22836;&#25140;&#24335;&#25668;&#20687;&#35760;&#24405;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#20419;&#36827;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#21160;&#27010;&#24565;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.00300</link><description>&lt;p&gt;
&#20174;&#20799;&#31461;&#35270;&#35282;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35270;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning of video representations from a child's perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20799;&#31461;&#30340;&#35270;&#35282;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#38271;&#26102;&#38388;&#30340;&#22836;&#25140;&#24335;&#25668;&#20687;&#35760;&#24405;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#20419;&#36827;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#21160;&#27010;&#24565;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#36890;&#36807;&#20960;&#24180;&#30340;&#33258;&#25105;&#35270;&#35273;&#32463;&#39564;&#23398;&#20064;&#21040;&#20102;&#24378;&#22823;&#30340;&#19990;&#30028;&#20869;&#37096;&#27169;&#22411;&#12290;&#36825;&#20123;&#20869;&#37096;&#27169;&#22411;&#33021;&#21542;&#36890;&#36807;&#20799;&#31461;&#30340;&#35270;&#35273;&#20307;&#39564;&#21644;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#65292;&#36824;&#26159;&#38656;&#35201;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#65311;&#26368;&#36817;&#65292;&#22312;&#25910;&#38598;&#22823;&#35268;&#27169;&#12289;&#32437;&#21521;&#30340;&#21457;&#23637;&#29616;&#23454;&#35270;&#39057;&#25968;&#25454;&#38598;&#20197;&#21450;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#36827;&#23637;&#20351;&#25105;&#20204;&#33021;&#22815;&#24320;&#22987;&#25506;&#35752;&#36825;&#20010;&#26412;&#36136;&#19982;&#20859;&#32946;&#20043;&#38388;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20851;&#27880;&#22522;&#20110;&#22270;&#20687;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#21644;&#21487;&#20197;&#20174;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#30340;&#35270;&#35273;&#33021;&#21147;&#65288;&#20363;&#22914;&#30446;&#26631;&#35782;&#21035;&#65289;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#19990;&#30028;&#30340;&#26102;&#38388;&#24615;&#36136;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20799;&#31461;&#26089;&#26399;&#21457;&#23637;&#38454;&#27573;&#65288;6-31&#20010;&#26376;&#65289;&#20174;&#20799;&#31461;&#30340;&#22836;&#25140;&#24335;&#25668;&#20687;&#35760;&#24405;&#20013;&#35757;&#32451;&#33258;&#30417;&#30563;&#35270;&#39057;&#27169;&#22411;&#12290;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#20419;&#36827;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#21160;&#27010;&#24565;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children learn powerful internal models of the world around them from a few years of egocentric visual experience. Can such internal models be learned from a child's visual experience with highly generic learning algorithms or do they require strong inductive biases? Recent advances in collecting large-scale, longitudinal, developmentally realistic video datasets and generic self-supervised learning (SSL) algorithms are allowing us to begin to tackle this nature vs. nurture question. However, existing work typically focuses on image-based SSL algorithms and visual capabilities that can be learned from static images (e.g. object recognition), thus ignoring temporal aspects of the world. To close this gap, here we train self-supervised video models on longitudinal, egocentric headcam recordings collected from a child over a two year period in their early development (6-31 months). The resulting models are highly effective at facilitating the learning of action concepts from a small numbe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20013;&#21516;&#26102;&#22788;&#29702;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#21033;&#29992;&#25345;&#32493;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#21518;&#32493;&#20219;&#21153;&#20013;&#30340;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.12112</link><description>&lt;p&gt;
&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Generalized Continual Category Discovery. (arXiv:2308.12112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20013;&#21516;&#26102;&#22788;&#29702;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#21033;&#29992;&#25345;&#32493;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#21518;&#32493;&#20219;&#21153;&#20013;&#30340;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#25512;&#21160;&#30528;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#30340;&#26497;&#38480;&#65292;&#20854;&#20013;&#19968;&#20010;&#26234;&#33021;&#20307;&#26399;&#26395;&#23398;&#20064;&#26032;&#30340;&#26631;&#35760;&#20219;&#21153;&#32780;&#19981;&#20250;&#24536;&#35760;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35774;&#32622;&#19982;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#19981;&#22826;&#21563;&#21512;&#65292;&#20854;&#20013;&#23398;&#20064;&#26234;&#33021;&#20307;&#21487;&#20197;&#35775;&#38382;&#22823;&#37327;&#30340;&#26080;&#26631;&#35760;&#25968;&#25454;&#65292;&#21253;&#25324;&#20840;&#26032;&#65288;&#23436;&#20840;&#26080;&#26631;&#35760;&#65289;&#31867;&#21035;&#21644;&#24050;&#30693;&#31867;&#21035;&#30340;&#31034;&#20363;&#12290;&#21463;&#21040;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#25918;&#26494;&#36825;&#20010;&#20551;&#35774;&#12290;&#30830;&#20999;&#22320;&#35828;&#65292;&#22312;&#20219;&#20309;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20801;&#35768;&#23384;&#22312;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#24517;&#39035;&#20351;&#29992;&#25345;&#32493;&#29256;&#26412;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#35774;&#32622;&#20026;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#12290;&#23427;&#32479;&#19968;&#20102;CL&#21644;GCD&#65292;&#24357;&#21512;&#20102;&#21512;&#25104;&#22522;&#20934;&#21644;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#26080;&#27861;&#20174;&#21518;&#32493;&#20219;&#21153;&#20013;&#31215;&#32047;&#30693;&#35782;&#65292;&#20854;&#20013;&#21253;&#21547;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of Continual Learning (CL) methods push the limit of supervised learning settings, where an agent is expected to learn new labeled tasks and not forget previous knowledge. However, these settings are not well aligned with real-life scenarios, where a learning agent has access to a vast amount of unlabeled data encompassing both novel (entirely unlabeled) classes and examples from known classes. Drawing inspiration from Generalized Category Discovery (GCD), we introduce a novel framework that relaxes this assumption. Precisely, in any task, we allow for the existence of novel and known classes, and one must use continual version of unsupervised learning methods to discover them. We call this setting Generalized Continual Category Discovery (GCCD). It unifies CL and GCD, bridging the gap between synthetic benchmarks and real-life scenarios. With a series of experiments, we present that existing methods fail to accumulate knowledge from subsequent tasks in which unlabeled samples of 
&lt;/p&gt;</description></item></channel></rss>