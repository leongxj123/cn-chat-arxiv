<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20302;&#28201;&#33976;&#39311;&#65288;LTD&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20462;&#25913;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#29983;&#25104;&#36719;&#26631;&#31614;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#24120;&#29992;&#30340;&#29420;&#28909;&#21521;&#37327;&#26631;&#31614;&#24102;&#26469;&#30340;&#23398;&#20064;&#22256;&#38590;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.02331</link><description>&lt;p&gt;
&#20302;&#28201;&#33976;&#39311;&#65306;&#29992;&#20110;&#31283;&#20581;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LTD: Low Temperature Distillation for Robust Adversarial Training. (arXiv:2111.02331v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.02331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20302;&#28201;&#33976;&#39311;&#65288;LTD&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20462;&#25913;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#29983;&#25104;&#36719;&#26631;&#31614;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#24120;&#29992;&#30340;&#29420;&#28909;&#21521;&#37327;&#26631;&#31614;&#24102;&#26469;&#30340;&#23398;&#20064;&#22256;&#38590;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#26159;&#36825;&#20123;&#27169;&#22411;&#30340;&#33258;&#28982;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#30528;&#26174;&#33879;&#24046;&#36317;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#21457;&#29616;&#20102;&#36825;&#20010;&#24046;&#36317;&#30340;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#24120;&#29992;&#30340;&#29420;&#28909;&#21521;&#37327;&#20316;&#20026;&#26631;&#31614;&#65292;&#36825;&#38459;&#30861;&#20102;&#22270;&#20687;&#35782;&#21035;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#29992;&#29420;&#28909;&#21521;&#37327;&#34920;&#31034;&#27169;&#31946;&#22270;&#20687;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#24471;&#21040;&#27425;&#20248;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#20302;&#28201;&#33976;&#39311;&#65288;LTD&#65289;&#65292;&#23427;&#20351;&#29992;&#20462;&#25913;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#29983;&#25104;&#36719;&#26631;&#31614;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;LTD&#22312;&#25945;&#24072;&#27169;&#22411;&#20013;&#20351;&#29992;&#30456;&#23545;&#36739;&#20302;&#30340;&#28201;&#24230;&#65292;&#32780;&#23545;&#25945;&#24072;&#21644;&#23398;&#29983;&#27169;&#22411;&#20351;&#29992;&#22266;&#23450;&#20294;&#19981;&#21516;&#30340;&#28201;&#24230;&#12290;&#36825;&#20010;&#20462;&#25913;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#36935;&#21040;&#24050;&#32463;&#22312;&#20808;&#21069;&#24037;&#20316;&#20013;&#35299;&#20915;&#30340;&#26799;&#24230;&#25513;&#30721;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training has been widely used to enhance the robustness of neural network models against adversarial attacks. Despite the popularity of neural network models, a significant gap exists between the natural and robust accuracy of these models. In this paper, we identify one of the primary reasons for this gap is the common use of one-hot vectors as labels, which hinders the learning process for image recognition. Representing ambiguous images with one-hot vectors is imprecise and may lead the model to suboptimal solutions. To overcome this issue, we propose a novel method called Low Temperature Distillation (LTD) that generates soft labels using the modified knowledge distillation framework. Unlike previous approaches, LTD uses a relatively low temperature in the teacher model and fixed, but different temperatures for the teacher and student models. This modification boosts the model's robustness without encountering the gradient masking problem that has been addressed in defe
&lt;/p&gt;</description></item></channel></rss>