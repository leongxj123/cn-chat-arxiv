<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#23569;&#26679;&#26412;&#26102;&#24207;&#21160;&#20316;&#23450;&#20301;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#21644;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#30340;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#36890;&#29992;&#29305;&#24449;&#21644;&#20943;&#36731;&#36807;&#25311;&#21512;&#39118;&#38505;</title><link>https://arxiv.org/abs/2403.18915</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#23569;&#26679;&#26412;&#26102;&#24207;&#21160;&#20316;&#23450;&#20301;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PLOT-TAL -- Prompt Learning with Optimal Transport for Few-Shot Temporal Action Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18915
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#23569;&#26679;&#26412;&#26102;&#24207;&#21160;&#20316;&#23450;&#20301;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#21644;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#30340;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#36890;&#29992;&#29305;&#24449;&#21644;&#20943;&#36731;&#36807;&#25311;&#21512;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#26102;&#24207;&#21160;&#20316;&#23450;&#20301;&#65288;TAL&#65289;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#20256;&#32479;&#21333;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#30001;&#20110;&#26080;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#39057;&#20013;&#36328;&#19981;&#21516;&#19978;&#19979;&#25991;&#36827;&#34892;&#27867;&#21270;&#32780;&#23548;&#33268;&#36807;&#25311;&#21512;&#12290;&#37492;&#20110;&#35270;&#39057;&#20013;&#25668;&#20687;&#26426;&#35270;&#35282;&#12289;&#32972;&#26223;&#21644;&#29289;&#20307;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#20102;&#26368;&#20248;&#20256;&#36755;&#30340;&#22810;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#36825;&#20010;&#35774;&#35745;&#20801;&#35768;&#27169;&#22411;&#20026;&#27599;&#20010;&#21160;&#20316;&#23398;&#20064;&#19968;&#32452;&#22810;&#26679;&#30340;&#25552;&#31034;&#65292;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#36890;&#29992;&#29305;&#24449;&#24182;&#20998;&#24067;&#34920;&#31034;&#20197;&#20943;&#36731;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#37319;&#29992;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#25552;&#31034;&#19982;&#21160;&#20316;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#65292;&#20248;&#21270;&#20197;&#33719;&#24471;&#36866;&#24212;&#35270;&#39057;&#25968;&#25454;&#22810;&#38754;&#24615;&#30340;&#32508;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21160;&#20316;&#23450;&#20301;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18915v1 Announce Type: cross  Abstract: This paper introduces a novel approach to temporal action localization (TAL) in few-shot learning. Our work addresses the inherent limitations of conventional single-prompt learning methods that often lead to overfitting due to the inability to generalize across varying contexts in real-world videos. Recognizing the diversity of camera views, backgrounds, and objects in videos, we propose a multi-prompt learning framework enhanced with optimal transport. This design allows the model to learn a set of diverse prompts for each action, capturing general characteristics more effectively and distributing the representation to mitigate the risk of overfitting. Furthermore, by employing optimal transport theory, we efficiently align these prompts with action features, optimizing for a comprehensive representation that adapts to the multifaceted nature of video data. Our experiments demonstrate significant improvements in action localization a
&lt;/p&gt;</description></item><item><title>I-CEE&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#23450;&#21046;&#20102;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#36890;&#36807;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#31034;&#20363;&#22270;&#20687;&#12289;&#23616;&#37096;&#35299;&#37322;&#21644;&#27169;&#22411;&#20915;&#31574;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2312.12102</link><description>&lt;p&gt;
I-CEE: &#23558;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#35299;&#37322;&#23450;&#21046;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
I-CEE: Tailoring Explanations of Image Classification Models to User Expertise. (arXiv:2312.12102v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12102
&lt;/p&gt;
&lt;p&gt;
I-CEE&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#23450;&#21046;&#20102;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#36890;&#36807;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#31034;&#20363;&#22270;&#20687;&#12289;&#23616;&#37096;&#35299;&#37322;&#21644;&#27169;&#22411;&#20915;&#31574;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#35299;&#37322;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#23545;&#20110;&#20381;&#36182;&#23427;&#20204;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36127;&#36131;&#20219;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#35782;&#21035;&#21040;&#20854;&#37325;&#35201;&#24615;&#65292;&#21487;&#20197;&#29983;&#25104;&#36825;&#20123;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#25552;&#20379;&#20102;&#20960;&#31181;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#19981;&#26029;&#21457;&#23637;&#30340;&#24037;&#20316;&#20013;&#65292;&#23545;&#29992;&#25143;&#65288;&#35299;&#37322;&#23545;&#35937;&#65289;&#30340;&#20851;&#27880;&#30456;&#23545;&#36739;&#23569;&#65292;&#22823;&#22810;&#25968;XAI&#25216;&#26415;&#20135;&#29983;&#30340;&#26159;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#23454;&#29616;&#26356;&#21152;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;XAI&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;I-CEE&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#23450;&#21046;&#22270;&#20687;&#20998;&#31867;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;&#21463;&#21040;&#29616;&#26377;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;I-CEE&#36890;&#36807;&#20026;&#29992;&#25143;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#65288;&#21363;&#31034;&#20363;&#22270;&#20687;&#65289;&#12289;&#30456;&#24212;&#30340;&#23616;&#37096;&#35299;&#37322;&#21644;&#27169;&#22411;&#20915;&#31574;&#26469;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#19982;&#27492;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;I-CEE&#27169;&#25311;&#20102;&#31034;&#20363;&#22270;&#20687;&#30340;&#20449;&#24687;&#37327;&#20381;&#36182;&#20110;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#20026;&#19981;&#21516;&#30340;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effectively explaining decisions of black-box machine learning models is critical to responsible deployment of AI systems that rely on them. Recognizing their importance, the field of explainable AI (XAI) provides several techniques to generate these explanations. Yet, there is relatively little emphasis on the user (the explainee) in this growing body of work and most XAI techniques generate "one-size-fits-all" explanations. To bridge this gap and achieve a step closer towards human-centered XAI, we present I-CEE, a framework that provides Image Classification Explanations tailored to User Expertise. Informed by existing work, I-CEE explains the decisions of image classification models by providing the user with an informative subset of training data (i.e., example images), corresponding local explanations, and model decisions. However, unlike prior work, I-CEE models the informativeness of the example images to depend on user expertise, resulting in different examples for different u
&lt;/p&gt;</description></item></channel></rss>