<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CF-SimCLR&#65292;&#19968;&#31181;&#21453;&#20107;&#23454;&#23545;&#29031;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36817;&#20284;&#21453;&#20107;&#23454;&#25512;&#26029;&#21019;&#36896;&#27491;&#26679;&#26412;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#37319;&#38598;&#20559;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09605</link><description>&lt;p&gt;
&#21453;&#20107;&#23454;&#23545;&#29031;&#23398;&#20064;&#65306;&#36890;&#36807;&#22240;&#26524;&#22270;&#20687;&#21512;&#25104;&#33719;&#24471;&#31283;&#20581;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Counterfactual contrastive learning: robust representations via causal image synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CF-SimCLR&#65292;&#19968;&#31181;&#21453;&#20107;&#23454;&#23545;&#29031;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36817;&#20284;&#21453;&#20107;&#23454;&#25512;&#26029;&#21019;&#36896;&#27491;&#26679;&#26412;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#37319;&#38598;&#20559;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#39044;&#35757;&#32451;&#24050;&#34987;&#24191;&#27867;&#35748;&#20026;&#33021;&#22815;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#38480;&#26631;&#31614;&#35774;&#32622;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#23545;&#22686;&#24378;&#31649;&#36947;&#30340;&#36873;&#25321;&#25935;&#24863;&#12290;&#27491;&#26679;&#26412;&#24212;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#21516;&#26102;&#30772;&#22351;&#22495;&#29305;&#23450;&#20449;&#24687;&#12290;&#26631;&#20934;&#22686;&#24378;&#31649;&#36947;&#36890;&#36807;&#39044;&#23450;&#20041;&#30340;&#20809;&#24230;&#21464;&#25442;&#27169;&#25311;&#22495;&#29305;&#23450;&#21464;&#21270;&#65292;&#20294;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#27169;&#25311;&#30495;&#23454;&#30340;&#39046;&#22495;&#21464;&#21270;&#21602;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26368;&#36817;&#22312;&#21453;&#20107;&#23454;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CF-SimCLR&#65292;&#19968;&#31181;&#21453;&#20107;&#23454;&#23545;&#29031;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#36817;&#20284;&#21453;&#20107;&#23454;&#25512;&#26029;&#36827;&#34892;&#27491;&#26679;&#26412;&#21019;&#24314;&#12290;&#23545;&#33016;&#37096;X&#20809;&#21644;&#20083;&#33146;X&#20809;&#31561;&#20116;&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;CF-SimCLR&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#33719;&#21462;&#20559;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#22312;&#20004;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#19979;&#28216;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09605v1 Announce Type: cross  Abstract: Contrastive pretraining is well-known to improve downstream task performance and model generalisation, especially in limited label settings. However, it is sensitive to the choice of augmentation pipeline. Positive pairs should preserve semantic information while destroying domain-specific information. Standard augmentation pipelines emulate domain-specific changes with pre-defined photometric transformations, but what if we could simulate realistic domain changes instead? In this work, we show how to utilise recent progress in counterfactual image generation to this effect. We propose CF-SimCLR, a counterfactual contrastive learning approach which leverages approximate counterfactual inference for positive pair creation. Comprehensive evaluation across five datasets, on chest radiography and mammography, demonstrates that CF-SimCLR substantially improves robustness to acquisition shift with higher downstream performance on both in- an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Vid-LLMs&#65289;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;Vid-LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#21253;&#25324;&#24320;&#25918;&#24335;&#26102;&#31354;&#25512;&#29702;&#21644;&#24120;&#35782;&#30693;&#35782;&#65292;&#20026;&#26410;&#26469;&#30340;&#35270;&#39057;&#29702;&#35299;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2312.17432</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Video Understanding with Large Language Models: A Survey. (arXiv:2312.17432v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17432
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Vid-LLMs&#65289;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;Vid-LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#21253;&#25324;&#24320;&#25918;&#24335;&#26102;&#31354;&#25512;&#29702;&#21644;&#24120;&#35782;&#30693;&#35782;&#65292;&#20026;&#26410;&#26469;&#30340;&#35270;&#39057;&#29702;&#35299;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#32447;&#35270;&#39057;&#24179;&#21488;&#30340;&#19981;&#26029;&#22686;&#38271;&#21644;&#35270;&#39057;&#20869;&#23481;&#30340;&#19981;&#26029;&#22686;&#22810;&#65292;&#23545;&#29087;&#32451;&#30340;&#35270;&#39057;&#29702;&#35299;&#24037;&#20855;&#30340;&#38656;&#27714;&#26174;&#33879;&#22686;&#21152;&#12290;&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Vid-LLMs&#65289;&#25216;&#26415;&#36827;&#34892;&#35270;&#39057;&#29702;&#35299;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;Vid-LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#20196;&#20154;&#24778;&#35766;&#65292;&#23588;&#20854;&#26159;&#23427;&#20204;&#22312;&#24320;&#25918;&#24335;&#26102;&#31354;&#25512;&#29702;&#21644;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20026;&#26410;&#26469;&#30340;&#35270;&#39057;&#29702;&#35299;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#26412;&#35843;&#26597;&#23545;Vid-LLMs&#30340;&#29420;&#29305;&#29305;&#28857;&#21644;&#33021;&#21147;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#20998;&#20026;&#22235;&#31181;&#20027;&#35201;&#31867;&#22411;&#65306;&#22522;&#20110;LLM&#30340;&#35270;&#39057;&#20195;&#29702;&#12289;Vid-LLMs&#30340;&#39044;&#35757;&#32451;&#12289;Vid-LLMs&#30340;&#25351;&#20196;&#35843;&#25972;&#21644;&#28151;&#21512;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#35843;&#26597;&#23545;Vid-LLMs&#30340;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#21478;&#22806;&#65292;&#23427;&#36824;&#25506;&#35752;&#20102;Vid-LLMs&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly. Given the remarkable capabilities of Large Language Models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of the recent advancements in video understanding harnessing the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended spatial-temporal reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into four main types: LLM-based Video Agents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods. Furthermore, this survey presents a comprehensive study of the tasks, datasets, and evaluation methodologies for Vid-LLMs. Additionally, it explores 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#30001;&#20154;&#31867;&#26631;&#27880;&#30340;&#20114;&#34917;&#26631;&#31614;&#65292;&#21019;&#36896;&#20102;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;CLL&#25968;&#25454;&#38598;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#29616;&#23454;&#34920;&#29616;&#19979;CLL&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#23454;&#38469;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2305.08295</link><description>&lt;p&gt;
CLCIFAR&#65306;&#24102;&#20154;&#31867;&#26631;&#27880;&#20114;&#34917;&#26631;&#31614;&#30340;CIFAR&#27966;&#29983;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CLCIFAR: CIFAR-Derived Benchmark Datasets with Human Annotated Complementary Labels. (arXiv:2305.08295v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#30001;&#20154;&#31867;&#26631;&#27880;&#30340;&#20114;&#34917;&#26631;&#31614;&#65292;&#21019;&#36896;&#20102;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;CLL&#25968;&#25454;&#38598;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#29616;&#23454;&#34920;&#29616;&#19979;CLL&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#23454;&#38469;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#65288;CLL&#65289;&#26159;&#19968;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#20165;&#20351;&#29992;&#20114;&#34917;&#26631;&#31614;&#65288;&#26631;&#31034;&#23454;&#20363;&#19981;&#23646;&#20110;&#21738;&#20123;&#31867;&#21035;&#65289;&#26469;&#35757;&#32451;&#22810;&#31867;&#20998;&#31867;&#22120;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;CLL&#31639;&#27861;&#65292;&#20294;&#30001;&#20110;&#20004;&#20010;&#21407;&#22240;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#34920;&#29616;&#20173;&#19981;&#28165;&#26970;&#12290;&#39318;&#20808;&#65292;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#20114;&#34917;&#26631;&#31614;&#29983;&#25104;&#30340;&#20551;&#35774;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#20165;&#38480;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#33719;&#21462;&#26377;&#20851;CLL&#31639;&#27861;&#30340;&#30495;&#23454;&#19990;&#30028;&#34920;&#29616;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21327;&#35758;&#26469;&#25910;&#38598;&#30001;&#20154;&#31867;&#27880;&#37322;&#32773;&#27880;&#37322;&#30340;&#20114;&#34917;&#26631;&#31614;&#12290;&#36825;&#19968;&#21162;&#21147;&#23548;&#33268;&#21019;&#24314;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;CLCIFAR10&#21644;CLCIFAR20&#65292;&#20998;&#21035;&#30001;CIFAR10&#21644;CIFAR100&#27966;&#29983;&#32780;&#26469;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;https://github.com/ntucllab/complementary_cifar&#19978;&#20844;&#24320;&#21457;&#24067;&#65292;&#20195;&#34920;&#20102;&#31532;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;CLL&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#22522;&#20934;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#36739;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24403;&#20351;&#29992;&#20154;&#31867;&#27880;&#37322;&#30340;&#20114;&#34917;&#26631;&#31614;&#26102;&#65292;&#24615;&#33021;&#26377;&#26126;&#26174;&#19979;&#38477;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#20063;&#35266;&#23519;&#21040;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;CLL&#25968;&#25454;&#38598;&#20351;&#24471;&#22312;&#26356;&#25509;&#36817;&#23454;&#38469;&#24212;&#29992;&#26465;&#20214;&#19979;&#35780;&#20272;&#31639;&#27861;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#26356;&#30495;&#23454;&#22320;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complementary-label learning (CLL) is a weakly-supervised learning paradigm that aims to train a multi-class classifier using only complementary labels, which indicate classes to which an instance does not belong. Despite numerous algorithmic proposals for CLL, their practical performance remains unclear for two reasons. Firstly, these algorithms often rely on assumptions about the generation of complementary labels. Secondly, their evaluation has been limited to synthetic datasets. To gain insights into the real-world performance of CLL algorithms, we developed a protocol to collect complementary labels annotated by human annotators. This effort resulted in the creation of two datasets, CLCIFAR10 and CLCIFAR20, derived from CIFAR10 and CIFAR100, respectively. These datasets, publicly released at https://github.com/ntucllab/complementary_cifar, represent the very first real-world CLL datasets. Through extensive benchmark experiments, we discovered a notable decline in performance when 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#24605;&#24819;&#30340;&#28508;&#22312;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;Barrett&#39135;&#31649;&#30142;&#30149;&#36827;&#31243;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2303.12711</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#28508;&#22312;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#24314;&#27169;Barrett&#39135;&#31649;&#30142;&#30149;&#36827;&#31243;
&lt;/p&gt;
&lt;p&gt;
Geometry-Aware Latent Representation Learning for Modeling Disease Progression of Barrett's Esophagus. (arXiv:2303.12711v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#24605;&#24819;&#30340;&#28508;&#22312;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;Barrett&#39135;&#31649;&#30142;&#30149;&#36827;&#31243;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Barrett&#39135;&#31649;&#26159;&#39135;&#31649;&#33146;&#30284;&#30340;&#21807;&#19968;&#20808;&#39537;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#35786;&#26029;&#26102;&#39044;&#21518;&#19981;&#33391;&#30340;&#39135;&#31649;&#30284;&#30151;&#12290;&#22240;&#27492;&#65292;&#35786;&#26029;Barrett&#39135;&#31649;&#23545;&#20110;&#39044;&#38450;&#21644;&#27835;&#30103;&#39135;&#31649;&#30284;&#33267;&#20851;&#37325;&#35201;&#12290;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25903;&#25345;Barrett&#39135;&#31649;&#35786;&#26029;&#65292;&#20294;&#32452;&#32455;&#30149;&#29702;&#23398;&#35757;&#32451;&#25968;&#25454;&#30340;&#39640;&#35266;&#23519;&#32773;&#21464;&#24322;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#12290;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAEs)&#36827;&#34892;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26174;&#31034;&#20986;&#28508;&#22312;&#20248;&#21183;&#65292;&#22240;&#20026;&#23427;&#20204;&#23558;&#36755;&#20837;&#25968;&#25454;&#26144;&#23556;&#21040;&#20855;&#26377;&#20165;&#26377;&#29992;&#29305;&#24449;&#30340;&#20302;&#32500;&#27969;&#24418;&#65292;&#20026;&#25913;&#36827;&#19979;&#28216;&#20219;&#21153;&#21644;&#35265;&#35299;&#23558;Barrett&#39135;&#31649;&#30149;&#31243;&#34920;&#24449;&#12290;&#28982;&#32780;&#65292;VAE&#30340;&#27431;&#20960;&#37324;&#24471;&#28508;&#22312;&#31354;&#38388;&#25197;&#26354;&#20102;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#30142;&#30149;&#36827;&#23637;&#24314;&#27169;&#12290;&#20960;&#20309;VAEs&#20026;&#28508;&#22312;&#31354;&#38388;&#25552;&#20379;&#38468;&#21152;&#20960;&#20309;&#32467;&#26500;&#65292;RHVAE&#20551;&#35774;&#20026;&#40654;&#26364;&#27969;&#24418;&#65292;$\mathcal{S}$-VAE&#20551;&#35774;&#20026;&#36229;&#29699;&#38754;&#27969;&#24418;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;$\mathcal{S}$-VAE&#20248;&#20110;&#24120;&#35268;VAE&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Barrett's Esophagus (BE) is the only precursor known to Esophageal Adenocarcinoma (EAC), a type of esophageal cancer with poor prognosis upon diagnosis. Therefore, diagnosing BE is crucial in preventing and treating esophageal cancer. While supervised machine learning supports BE diagnosis, high interobserver variability in histopathological training data limits these methods. Unsupervised representation learning via Variational Autoencoders (VAEs) shows promise, as they map input data to a lower-dimensional manifold with only useful features, characterizing BE progression for improved downstream tasks and insights. However, the VAE's Euclidean latent space distorts point relationships, hindering disease progression modeling. Geometric VAEs provide additional geometric structure to the latent space, with RHVAE assuming a Riemannian manifold and $\mathcal{S}$-VAE a hyperspherical manifold. Our study shows that $\mathcal{S}$-VAE outperforms vanilla VAE with better reconstruction losses, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;&#20351;&#29992;&#22810;&#38454;&#21338;&#24328;&#35770;&#20132;&#20114;&#30340;&#26032;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32431;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;MogaNet&#65292;&#23427;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;&#22810;&#31181;&#20856;&#22411;&#35270;&#35273;&#22522;&#20934;&#20013;&#20197;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#21033;&#29992;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.03295</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#22810;&#38454;&#38376;&#25511;&#32858;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Efficient Multi-order Gated Aggregation Network. (arXiv:2211.03295v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;&#20351;&#29992;&#22810;&#38454;&#21338;&#24328;&#35770;&#20132;&#20114;&#30340;&#26032;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32431;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;MogaNet&#65292;&#23427;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;&#22810;&#31181;&#20856;&#22411;&#35270;&#35273;&#22522;&#20934;&#20013;&#20197;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#21033;&#29992;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#21462;&#24471;&#26368;&#36817;&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#23545;ViT&#39118;&#26684;&#26550;&#26500;&#30340;&#25506;&#32034;&#24341;&#21457;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#20852;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#22810;&#38454;&#21338;&#24328;&#35770;&#20132;&#20114;&#30340;&#26032;&#35270;&#35282;&#25506;&#32034;&#20102;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;&#36825;&#31181;&#20132;&#20114;&#21453;&#26144;&#20102;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;&#19981;&#21516;&#23610;&#24230;&#19978;&#19979;&#25991;&#30340;&#21464;&#37327;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#12290;&#22312;&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#20351;&#29992;&#27010;&#24565;&#19978;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#26469;&#23450;&#21046;&#20004;&#20010;&#29305;&#24449;&#28151;&#21512;&#22120;&#65292;&#20197;&#20419;&#36827;&#36328;&#31354;&#38388;&#21644;&#36890;&#36947;&#31354;&#38388;&#30340;&#20013;&#38454;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32431;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;MogaNet&#65292;&#23427;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;ImageNet&#21644;&#21253;&#25324;COCO&#30446;&#26631;&#26816;&#27979;&#12289;ADE20K&#35821;&#20041;&#20998;&#21106;&#12289;2D&amp;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20197;&#21450;&#35270;&#39057;&#39044;&#27979;&#31561;&#22810;&#31181;&#20856;&#22411;&#35270;&#35273;&#22522;&#20934;&#20013;&#20197;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#21033;&#29992;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the recent success of Vision Transformers (ViTs), explorations toward ViT-style architectures have triggered the resurgence of ConvNets. In this work, we explore the representation ability of modern ConvNets from a novel view of multi-order game-theoretic interaction, which reflects inter-variable interaction effects w.r.t.~contexts of different scales based on game theory. Within the modern ConvNet framework, we tailor the two feature mixers with conceptually simple yet effective depthwise convolutions to facilitate middle-order information across spatial and channel spaces respectively. In this light, a new family of pure ConvNet architecture, dubbed MogaNet, is proposed, which shows excellent scalability and attains competitive results among state-of-the-art models with more efficient use of parameters on ImageNet and multifarious typical vision benchmarks, including COCO object detection, ADE20K semantic segmentation, 2D\&amp;3D human pose estimation, and video prediction. Typica
&lt;/p&gt;</description></item></channel></rss>