<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36890;&#36807;EC-IoU&#24230;&#37327;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#23450;&#21521;&#23433;&#20840;&#24615;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#25552;&#39640;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;IoU&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15474</link><description>&lt;p&gt;
EC-IoU: &#36890;&#36807;&#33258;&#25105;&#20013;&#24515;&#20132;&#24182;&#32852;&#35843;&#25972;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
EC-IoU: Orienting Safety for Object Detectors via Ego-Centric Intersection-over-Union
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15474
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;EC-IoU&#24230;&#37327;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#23450;&#21521;&#23433;&#20840;&#24615;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#25552;&#39640;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;IoU&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#20013;&#24515;&#20132;&#24182;&#32852;&#65288;EC-IoU&#65289;&#24230;&#37327;&#26469;&#23450;&#21521;&#23433;&#20840;&#24615;&#29289;&#20307;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#26426;&#21046;&#26469;&#20248;&#21270;&#24191;&#27867;&#20351;&#29992;&#30340;IoU&#24230;&#37327;&#65292;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#33258;&#25105;&#20195;&#29702;&#20154;&#30340;&#35270;&#35282;&#35206;&#30422;&#26356;&#36817;&#30340;&#22320;&#38754;&#30495;&#23454;&#23545;&#35937;&#28857;&#30340;&#39044;&#27979;&#20998;&#37197;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;EC-IoU&#24230;&#37327;&#21487;&#20197;&#29992;&#20110;&#20856;&#22411;&#30340;&#35780;&#20272;&#36807;&#31243;&#65292;&#36873;&#25321;&#26377;&#26356;&#39640;&#23433;&#20840;&#24615;&#34920;&#29616;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#23427;&#36824;&#21487;&#20197;&#38598;&#25104;&#21040;&#24120;&#35265;&#25439;&#22833;&#20989;&#25968;&#20013;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#12290;&#23613;&#31649;&#38754;&#21521;&#23433;&#20840;&#24615;&#65292;&#20294;&#25105;&#20204;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;EC-IoU&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22343;&#20540;&#24179;&#22343;&#31934;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#20248;&#20110;&#20351;&#29992;IoU&#35757;&#32451;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15474v1 Announce Type: cross  Abstract: This paper presents safety-oriented object detection via a novel Ego-Centric Intersection-over-Union (EC-IoU) measure, addressing practical concerns when applying state-of-the-art learning-based perception models in safety-critical domains such as autonomous driving. Concretely, we propose a weighting mechanism to refine the widely used IoU measure, allowing it to assign a higher score to a prediction that covers closer points of a ground-truth object from the ego agent's perspective. The proposed EC-IoU measure can be used in typical evaluation processes to select object detectors with higher safety-related performance for downstream tasks. It can also be integrated into common loss functions for model fine-tuning. While geared towards safety, our experiment with the KITTI dataset demonstrates the performance of a model trained on EC-IoU can be better than that of a variant trained on IoU in terms of mean Average Precision as well.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#12289;&#20934;&#30830;&#22320;&#38477;&#23610;&#24230;&#20219;&#24847;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#65292;&#24182;&#20135;&#29983;&#27010;&#29575;&#24615;&#38477;&#23610;&#24230;&#22330;&#12290;</title><link>https://arxiv.org/abs/2403.02774</link><description>&lt;p&gt;
&#24555;&#36895;&#12289;&#33258;&#36866;&#24212;&#23610;&#24230;&#21644;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#22330;&#38477;&#23610;&#24230;&#19982;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System Model Fields with Generative Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02774
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#12289;&#20934;&#30830;&#22320;&#38477;&#23610;&#24230;&#20219;&#24847;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#65292;&#24182;&#20135;&#29983;&#27010;&#29575;&#24615;&#38477;&#23610;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#21644;&#39640;&#20998;&#36776;&#29575;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;(ESM)&#27169;&#25311;&#23545;&#20110;&#35780;&#20272;&#20154;&#20026;&#27668;&#20505;&#21464;&#21270;&#23545;&#29983;&#24577;&#21644;&#31038;&#20250;&#32463;&#27982;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;ESM&#27169;&#25311;&#30340;&#38477;&#23610;&#24230;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#32479;&#35745;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23545;&#27599;&#20010;ESM&#37117;&#38656;&#35201;&#35745;&#31639;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#27668;&#20505;&#39044;&#27979;&#25928;&#26524;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#19968;&#33268;&#24615;&#27169;&#22411;(CM)&#65292;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#39640;&#25928;&#20934;&#30830;&#22320;&#38477;&#23610;&#24230;&#20219;&#24847;ESM&#27169;&#25311;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#12290;&#25105;&#20204;&#30340;&#22522;&#30784;&#27169;&#22411;&#26041;&#27861;&#20197;&#21482;&#21463;&#35266;&#27979;&#21442;&#32771;&#25968;&#25454;&#38480;&#21046;&#30340;&#20998;&#36776;&#29575;&#20135;&#29983;&#27010;&#29575;&#24615;&#38477;&#23610;&#24230;&#22330;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CM&#22312;&#32500;&#25345;&#39640;&#21487;&#25511;&#24615;&#30340;&#21516;&#26102;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02774v1 Announce Type: cross  Abstract: Accurate and high-resolution Earth system model (ESM) simulations are essential to assess the ecological and socio-economic impacts of anthropogenic climate change, but are computationally too expensive. Recent machine learning approaches have shown promising results in downscaling ESM simulations, outperforming state-of-the-art statistical approaches. However, existing methods require computationally costly retraining for each ESM and extrapolate poorly to climates unseen during training. We address these shortcomings by learning a consistency model (CM) that efficiently and accurately downscales arbitrary ESM simulations without retraining in a zero-shot manner. Our foundation model approach yields probabilistic downscaled fields at resolution only limited by the observational reference data. We show that the CM outperforms state-of-the-art diffusion models at a fraction of computational cost while maintaining high controllability on
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#34920;&#29616;&#20248;&#24322;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#35757;&#32451;&#32593;&#32476;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#65292;&#19988;&#25104;&#26412;&#26497;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.13144</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Neural Network Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13144
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#34920;&#29616;&#20248;&#24322;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#35757;&#32451;&#32593;&#32476;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#65292;&#19988;&#25104;&#26412;&#26497;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#20063;&#21487;&#20197;\textit{&#29983;&#25104;&#34920;&#29616;&#20248;&#24322;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;}&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24456;&#31616;&#21333;&#65292;&#21033;&#29992;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#26631;&#20934;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;&#33258;&#21160;&#32534;&#30721;&#22120;&#25552;&#21462;&#20102;&#37096;&#20998;&#21463;&#35757;&#32593;&#32476;&#21442;&#25968;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#28982;&#21518;&#35757;&#32451;&#20102;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#26469;&#20174;&#38543;&#26426;&#22122;&#22768;&#20013;&#21512;&#25104;&#36825;&#20123;&#28508;&#22312;&#21442;&#25968;&#34920;&#31034;&#12290;&#23427;&#29983;&#25104;&#20102;&#26032;&#30340;&#34920;&#31034;&#65292;&#32463;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35299;&#30721;&#22120;&#65292;&#36755;&#20986;&#20934;&#22791;&#29992;&#20316;&#26032;&#30340;&#32593;&#32476;&#21442;&#25968;&#23376;&#38598;&#12290;&#22312;&#21508;&#31181;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#25193;&#25955;&#36807;&#31243;&#22987;&#32456;&#29983;&#25104;&#24615;&#33021;&#19982;&#32463;&#36807;&#35757;&#32451;&#30340;&#32593;&#32476;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#38468;&#21152;&#25104;&#26412;&#26497;&#23567;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#19982;&#32463;&#36807;&#35757;&#32451;&#30340;&#32593;&#32476;&#34920;&#29616;&#20986;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13144v1 Announce Type: new  Abstract: Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts latent representations of a subset of the trained network parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, our diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models perform differently with the trained networks. Our results en
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.14142</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65306;&#32479;&#19968;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations. (arXiv:2401.14142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14142
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411; (CBM)&#65292;&#22312;&#20026;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#23427;&#20204;&#36890;&#24120;&#36890;&#36807;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#27010;&#24565;&#65292;&#28982;&#21518;&#22312;&#32473;&#23450;&#39044;&#27979;&#30340;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26368;&#32456;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20363;&#22914;&#32416;&#27491;&#19968;&#20010;&#39044;&#27979;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33016;&#37096;&#8221;&#65289;&#26080;&#27861;&#24110;&#21161;&#32416;&#27491;&#39640;&#24230;&#30456;&#20851;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33145;&#37096;&#8221;&#65289;&#65292;&#23548;&#33268;&#26368;&#32456;&#20934;&#30830;&#29575;&#19981;&#29702;&#24819;&#65307;&#23427;&#20204;&#26080;&#27861;&#33258;&#28982;&#22320;&#37327;&#21270;&#19981;&#21516;&#27010;&#24565;&#21644;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65288;&#20363;&#22914;&#23545;&#20110;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#26631;&#31614;&#8220;Kentucky Warbler&#8221;&#21644;&#27010;&#24565;&#8220;&#40657;&#33394;&#22068;&#24052;&#8221;&#30340;&#22270;&#20687;&#65292;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#39044;&#27979;&#21478;&#19968;&#20010;&#27010;&#24565;&#8220;&#40657;&#33394;&#20896;&#8221;&#30340;&#27010;&#29575;&#26159;&#22810;&#23569;&#65289;&#65292;&#22240;&#27492;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#40657;&#30418;&#27169;&#22411;&#24037;&#20316;&#21407;&#29702;&#26356;&#28145;&#23618;&#27425;&#30340;&#27934;&#23519;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;Energy-based Concept Bottleneck Models&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not help correct highly correlated concepts (e.g., "yellow belly"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label "Kentucky Warbler" and a concept "black bill", what is the probability that the model correctly predicts another concept "black crown"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bot
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InceptionNeXt&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#22823;&#20869;&#26680;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16900</link><description>&lt;p&gt;
InceptionNeXt&#65306;&#24403;Inception&#36935;&#21040;ConvNeXt
&lt;/p&gt;
&lt;p&gt;
InceptionNeXt: When Inception Meets ConvNeXt. (arXiv:2303.16900v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InceptionNeXt&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#22823;&#20869;&#26680;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;ViTs&#38271;&#31243;&#24314;&#27169;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#36817;&#26399;&#24191;&#27867;&#30740;&#31350;&#21644;&#37319;&#29992;&#20102;&#22823;&#20869;&#26680;&#21367;&#31215;&#26469;&#25193;&#22823;&#24863;&#21463;&#37326;&#21644;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20363;&#22914;ConvNeXt&#37319;&#29992;&#20102;7x7&#28145;&#24230;&#21367;&#31215;&#12290;&#34429;&#28982;&#36825;&#31181;&#28145;&#24230;&#25805;&#20316;&#20165;&#28040;&#32791;&#23569;&#37327;FLOPs&#65292;&#20294;&#30001;&#20110;&#39640;&#20869;&#23384;&#35775;&#38382;&#25104;&#26412;&#65292;&#36825;&#22312;&#21151;&#33021;&#24378;&#22823;&#30340;&#35745;&#31639;&#35774;&#22791;&#19978;&#22823;&#22823;&#25439;&#23475;&#20102;&#27169;&#22411;&#25928;&#29575;&#12290;&#23613;&#31649;&#32553;&#23567;ConvNeXt&#30340;&#20869;&#26680;&#22823;&#23567;&#33021;&#25552;&#39640;&#36895;&#24230;&#65292;&#20294;&#20250;&#23548;&#33268;&#24615;&#33021;&#26174;&#30528;&#19979;&#38477;&#12290;&#22914;&#20309;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21463;Inceptions&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22823;&#20869;&#26680;&#28145;&#24230;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#65292;&#21363;&#23567;&#26041;&#20869;&#26680;&#12289;&#20004;&#20010;&#27491;&#20132;&#24102;&#20869;&#26680;&#21644;&#19968;&#20010;&#20114;&#34917;&#20869;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the long-range modeling ability of ViTs, large-kernel convolutions are widely studied and adopted recently to enlarge the receptive field and improve model performance, like the remarkable work ConvNeXt which employs 7x7 depthwise convolution. Although such depthwise operator only consumes a few FLOPs, it largely harms the model efficiency on powerful computing devices due to the high memory access costs. For example, ConvNeXt-T has similar FLOPs with ResNet-50 but only achieves 60% throughputs when trained on A100 GPUs with full precision. Although reducing the kernel size of ConvNeXt can improve speed, it results in significant performance degradation. It is still unclear how to speed up large-kernel-based CNN models while preserving their performance. To tackle this issue, inspired by Inceptions, we propose to decompose large-kernel depthwise convolution into four parallel branches along channel dimension, i.e. small square kernel, two orthogonal band kernels, and an ide
&lt;/p&gt;</description></item></channel></rss>