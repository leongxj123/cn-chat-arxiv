<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25972;&#21512;&#27491;&#24358;&#20989;&#25968;&#21040;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19243</link><description>&lt;p&gt;
&#29992;&#27491;&#24358;&#28608;&#27963;&#30340;&#20302;&#31209;&#30697;&#38453;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sine Activated Low-Rank Matrices for Parameter Efficient Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19243
&lt;/p&gt;
&lt;p&gt;
&#25972;&#21512;&#27491;&#24358;&#20989;&#25968;&#21040;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#20998;&#35299;&#24050;&#32463;&#25104;&#20026;&#22312;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#22686;&#24378;&#21442;&#25968;&#25928;&#29575;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#20123;&#25216;&#26415;&#26174;&#33879;&#38477;&#20302;&#20102;&#21442;&#25968;&#25968;&#37327;&#65292;&#21462;&#24471;&#20102;&#31616;&#27905;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#26159;&#22312;&#21442;&#25968;&#25928;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#20570;&#20986;&#22949;&#21327;&#65292;&#21442;&#25968;&#20943;&#23569;&#24448;&#24448;&#23548;&#33268;&#20934;&#30830;&#24615;&#19981;&#21450;&#23436;&#25972;&#31209;&#23545;&#24212;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#22312;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#25972;&#21512;&#20102;&#19968;&#20010;&#27491;&#24358;&#20989;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20445;&#30041;&#20102;&#20302;&#31209;&#26041;&#27861;&#30340;&#21442;&#25968;&#25928;&#29575;&#29305;&#24615;&#30340;&#22909;&#22788;&#65292;&#36824;&#22686;&#21152;&#20102;&#20998;&#35299;&#30340;&#31209;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#29616;&#26377;&#20302;&#31209;&#27169;&#22411;&#30340;&#19968;&#31181;&#36866;&#24212;&#24615;&#22686;&#24378;&#65292;&#27491;&#22914;&#20854;&#25104;&#21151;&#35777;&#23454;&#30340;&#37027;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19243v1 Announce Type: new  Abstract: Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and performance. However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts. In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process. This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition's rank, thereby enhancing model accuracy. Our method proves to be an adaptable enhancement for existing low-rank models, as evidenced by its successful 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#28151;&#20081;&#29615;&#22659;&#20013;&#25235;&#21462;&#24050;&#37319;&#25688;&#30340;&#35199;&#32418;&#26623;&#31319;&#26524;&#30340;&#35270;&#35273;&#24341;&#23548;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35270;&#35273;&#31995;&#32479;&#26469;&#35782;&#21035;&#31319;&#26524;&#24182;&#30830;&#23450;&#36866;&#21512;&#25235;&#21462;&#30340;&#20301;&#32622;&#65292;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#26469;&#25490;&#24207;&#25235;&#21462;&#23039;&#21183;&#65292;&#24182;&#23454;&#29616;&#26080;&#35302;&#35273;&#20256;&#24863;&#22120;&#25110;&#20960;&#20309;&#27169;&#22411;&#30340;&#22841;&#25345;&#25235;&#21462;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#20855;&#26377;100%&#30340;&#28165;&#29702;&#29575;&#21644;93%&#30340;&#19968;&#27425;&#24615;&#25104;&#21151;&#25235;&#21462;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.17170</link><description>&lt;p&gt;
&#29992;&#20110;&#22312;&#28151;&#20081;&#29615;&#22659;&#20013;&#25235;&#21462;&#24050;&#37319;&#25688;&#30340;&#35199;&#32418;&#26623;&#31319;&#26524;&#30340;&#35270;&#35273;&#24341;&#23548;&#26426;&#22120;&#20154;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Vision-Guided Robotic System for Grasping Harvested Tomato Trusses in Cluttered Environments. (arXiv:2309.17170v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17170
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#28151;&#20081;&#29615;&#22659;&#20013;&#25235;&#21462;&#24050;&#37319;&#25688;&#30340;&#35199;&#32418;&#26623;&#31319;&#26524;&#30340;&#35270;&#35273;&#24341;&#23548;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35270;&#35273;&#31995;&#32479;&#26469;&#35782;&#21035;&#31319;&#26524;&#24182;&#30830;&#23450;&#36866;&#21512;&#25235;&#21462;&#30340;&#20301;&#32622;&#65292;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#26469;&#25490;&#24207;&#25235;&#21462;&#23039;&#21183;&#65292;&#24182;&#23454;&#29616;&#26080;&#35302;&#35273;&#20256;&#24863;&#22120;&#25110;&#20960;&#20309;&#27169;&#22411;&#30340;&#22841;&#25345;&#25235;&#21462;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#20855;&#26377;100%&#30340;&#28165;&#29702;&#29575;&#21644;93%&#30340;&#19968;&#27425;&#24615;&#25104;&#21151;&#25235;&#21462;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#23545;&#20110;&#35199;&#32418;&#26623;&#30340;&#31216;&#37325;&#21644;&#21253;&#35013;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#25805;&#20316;&#12290;&#33258;&#21160;&#21270;&#30340;&#20027;&#35201;&#38556;&#30861;&#22312;&#20110;&#24320;&#21457;&#19968;&#20010;&#21487;&#38752;&#30340;&#29992;&#20110;&#24050;&#37319;&#25688;&#30340;&#31319;&#26524;&#30340;&#26426;&#22120;&#20154;&#25235;&#21462;&#31995;&#32479;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25235;&#21462;&#22534;&#25918;&#22312;&#35013;&#31665;&#20013;&#30340;&#31319;&#26524;&#65292;&#36825;&#26159;&#23427;&#20204;&#22312;&#37319;&#25688;&#21518;&#24120;&#35265;&#30340;&#23384;&#20648;&#21644;&#36816;&#36755;&#26041;&#24335;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35270;&#35273;&#31995;&#32479;&#65292;&#39318;&#20808;&#35782;&#21035;&#20986;&#35013;&#31665;&#20013;&#30340;&#21333;&#20010;&#31319;&#26524;&#65292;&#28982;&#21518;&#30830;&#23450;&#33550;&#37096;&#30340;&#36866;&#21512;&#25235;&#21462;&#30340;&#20301;&#32622;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#22312;&#32447;&#23398;&#20064;&#33021;&#21147;&#30340;&#25235;&#21462;&#23039;&#21183;&#25490;&#24207;&#31639;&#27861;&#12290;&#22312;&#36873;&#25321;&#20102;&#26368;&#26377;&#21069;&#26223;&#30340;&#25235;&#21462;&#23039;&#21183;&#20043;&#21518;&#65292;&#26426;&#22120;&#20154;&#25191;&#34892;&#19968;&#31181;&#26080;&#38656;&#35302;&#35273;&#20256;&#24863;&#22120;&#25110;&#20960;&#20309;&#27169;&#22411;&#30340;&#22841;&#25345;&#25235;&#21462;&#12290;&#23454;&#39564;&#23460;&#23454;&#39564;&#35777;&#26126;&#65292;&#37197;&#22791;&#20102;&#19968;&#20010;&#25163;&#30524;&#19968;&#20307;&#30340;RGB-D&#30456;&#26426;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#20174;&#22534;&#20013;&#25441;&#36215;&#25152;&#26377;&#30340;&#31319;&#26524;&#30340;&#28165;&#29702;&#29575;&#36798;&#21040;100%&#12290;93%&#30340;&#31319;&#26524;&#22312;&#31532;&#19968;&#27425;&#23581;&#35797;&#26102;&#25104;&#21151;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, truss tomato weighing and packaging require significant manual work. The main obstacle to automation lies in the difficulty of developing a reliable robotic grasping system for already harvested trusses. We propose a method to grasp trusses that are stacked in a crate with considerable clutter, which is how they are commonly stored and transported after harvest. The method consists of a deep learning-based vision system to first identify the individual trusses in the crate and then determine a suitable grasping location on the stem. To this end, we have introduced a grasp pose ranking algorithm with online learning capabilities. After selecting the most promising grasp pose, the robot executes a pinch grasp without needing touch sensors or geometric models. Lab experiments with a robotic manipulator equipped with an eye-in-hand RGB-D camera showed a 100% clearance rate when tasked to pick all trusses from a pile. 93% of the trusses were successfully grasped on the first try,
&lt;/p&gt;</description></item></channel></rss>