<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Hyper-STTN&#65292;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#65292;&#29992;&#20110;&#20154;&#32676;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#36229;&#22270;&#26469;&#25429;&#25417;&#25317;&#25380;&#22330;&#26223;&#20013;&#30340;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#36716;&#25442;&#22120;&#26469;&#25429;&#25417;&#34892;&#20154;&#30340;&#25104;&#23545;&#28508;&#22312;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20123;&#24322;&#26500;&#30340;&#32676;&#20307;&#38388;&#21644;&#25104;&#23545;&#38388;&#30456;&#20114;&#20316;&#29992;&#36890;&#36807;&#19968;&#20010;&#22810;&#27169;&#24577;&#36716;&#25442;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#21644;&#23545;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.06344</link><description>&lt;p&gt;
&#36229;&#32423;-STTN&#65306;&#31038;&#20132;&#32676;&#20307;&#24863;&#30693;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#29992;&#20110;&#20154;&#20307;&#36712;&#36857;&#39044;&#27979;&#19982;&#36229;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Hyper-STTN: Social Group-aware Spatial-Temporal Transformer Network for Human Trajectory Prediction with Hypergraph Reasoning. (arXiv:2401.06344v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Hyper-STTN&#65292;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#65292;&#29992;&#20110;&#20154;&#32676;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#36229;&#22270;&#26469;&#25429;&#25417;&#25317;&#25380;&#22330;&#26223;&#20013;&#30340;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#36716;&#25442;&#22120;&#26469;&#25429;&#25417;&#34892;&#20154;&#30340;&#25104;&#23545;&#28508;&#22312;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20123;&#24322;&#26500;&#30340;&#32676;&#20307;&#38388;&#21644;&#25104;&#23545;&#38388;&#30456;&#20114;&#20316;&#29992;&#36890;&#36807;&#19968;&#20010;&#22810;&#27169;&#24577;&#36716;&#25442;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#21644;&#23545;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#21253;&#25324;&#26381;&#21153;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65292;&#39044;&#27979;&#25317;&#25380;&#30340;&#24847;&#22270;&#21644;&#36712;&#36857;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#29702;&#35299;&#29615;&#22659;&#21160;&#24577;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#19981;&#20165;&#22240;&#20026;&#23545;&#24314;&#27169;&#25104;&#23545;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#24615;&#65292;&#36824;&#22240;&#20026;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#30721;&#25317;&#25380;&#22330;&#26223;&#20013;&#20840;&#38754;&#30340;&#25104;&#23545;&#21644;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Hyper-STTN&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#65292;&#29992;&#20110;&#20154;&#32676;&#36712;&#36857;&#39044;&#27979;&#12290;&#22312;Hyper-STTN&#20013;&#65292;&#36890;&#36807;&#19968;&#32452;&#22810;&#23610;&#24230;&#36229;&#22270;&#26500;&#24314;&#20102;&#25317;&#25380;&#30340;&#32676;&#20307;&#38388;&#30456;&#20851;&#24615;&#65292;&#36825;&#20123;&#36229;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#32676;&#20307;&#22823;&#23567;&#65292;&#36890;&#36807;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#27010;&#29575;&#30340;&#36229;&#22270;&#35889;&#21367;&#31215;&#36827;&#34892;&#25429;&#25417;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#20102;&#31354;&#38388;-&#26102;&#38388;&#36716;&#25442;&#22120;&#26469;&#25429;&#25417;&#34892;&#20154;&#22312;&#31354;&#38388;-&#26102;&#38388;&#32500;&#24230;&#19978;&#30340;&#23545;&#29031;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#24322;&#26500;&#30340;&#32676;&#20307;&#38388;&#21644;&#25104;&#23545;&#38388;&#30456;&#20114;&#20316;&#29992;&#36890;&#36807;&#19968;&#20010;&#22810;&#27169;&#24577;&#36716;&#25442;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#21644;&#23545;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting crowded intents and trajectories is crucial in varouls real-world applications, including service robots and autonomous vehicles. Understanding environmental dynamics is challenging, not only due to the complexities of modeling pair-wise spatial and temporal interactions but also the diverse influence of group-wise interactions. To decode the comprehensive pair-wise and group-wise interactions in crowded scenarios, we introduce Hyper-STTN, a Hypergraph-based Spatial-Temporal Transformer Network for crowd trajectory prediction. In Hyper-STTN, crowded group-wise correlations are constructed using a set of multi-scale hypergraphs with varying group sizes, captured through random-walk robability-based hypergraph spectral convolution. Additionally, a spatial-temporal transformer is adapted to capture pedestrians' pair-wise latent interactions in spatial-temporal dimensions. These heterogeneous group-wise and pair-wise are then fused and aligned though a multimodal transformer net
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21512;&#25104;&#20266;&#35013;&#25968;&#25454;&#20197;&#25913;&#21892;&#23545;&#33258;&#28982;&#22330;&#26223;&#20013;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#30340;&#20266;&#35013;&#22270;&#20687;&#65292;&#24182;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.06701</link><description>&lt;p&gt;
&#20266;&#35013;&#22270;&#20687;&#21512;&#25104;&#26159;&#25552;&#39640;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection. (arXiv:2308.06701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06701
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21512;&#25104;&#20266;&#35013;&#25968;&#25454;&#20197;&#25913;&#21892;&#23545;&#33258;&#28982;&#22330;&#26223;&#20013;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#30340;&#20266;&#35013;&#22270;&#20687;&#65292;&#24182;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#20837;&#33258;&#28982;&#22330;&#26223;&#30340;&#20266;&#35013;&#29289;&#20307;&#32473;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#21644;&#21512;&#25104;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#28982;&#32780;&#30001;&#20110;&#25968;&#25454;&#26377;&#38480;&#65292;&#35813;&#30740;&#31350;&#35838;&#39064;&#19968;&#30452;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21512;&#25104;&#20266;&#35013;&#25968;&#25454;&#20197;&#22686;&#24378;&#23545;&#33258;&#28982;&#22330;&#26223;&#20013;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#30340;&#20266;&#35013;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#29616;&#26377;&#30340;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20266;&#35013;&#29615;&#22659;&#29983;&#25104;&#22120;&#65292;&#30001;&#20266;&#35013;&#20998;&#24067;&#20998;&#31867;&#22120;&#36827;&#34892;&#30417;&#30563;&#65292;&#21512;&#25104;&#20266;&#35013;&#22270;&#20687;&#65292;&#28982;&#21518;&#23558;&#20854;&#36755;&#20837;&#25105;&#20204;&#30340;&#29983;&#25104;&#22120;&#20197;&#25193;&#23637;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;COD10k&#12289;CAMO&#21644;CHAMELEON&#65289;&#19978;&#30340;&#25928;&#26524;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#25913;&#21892;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Camouflaged objects that blend into natural scenes pose significant challenges for deep-learning models to detect and synthesize. While camouflaged object detection is a crucial task in computer vision with diverse real-world applications, this research topic has been constrained by limited data availability. We propose a framework for synthesizing camouflage data to enhance the detection of camouflaged objects in natural scenes. Our approach employs a generative model to produce realistic camouflage images, which can be used to train existing object detection models. Specifically, we use a camouflage environment generator supervised by a camouflage distribution classifier to synthesize the camouflage images, which are then fed into our generator to expand the dataset. Our framework outperforms the current state-of-the-art method on three datasets (COD10k, CAMO, and CHAMELEON), demonstrating its effectiveness in improving camouflaged object detection. This approach can serve as a plug-
&lt;/p&gt;</description></item></channel></rss>