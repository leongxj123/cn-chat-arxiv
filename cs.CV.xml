<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#26354;&#29575;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#20013;&#23884;&#20837;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20197;&#20445;&#25345;&#27169;&#22411;&#39640;&#20934;&#30830;&#24615;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10045</link><description>&lt;p&gt;
&#36890;&#36807;&#26354;&#29575;&#27491;&#21017;&#21270;&#23454;&#29616;&#23545;&#25239;&#40065;&#26834;&#24615;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Towards Adversarially Robust Dataset Distillation by Curvature Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#26354;&#29575;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#20013;&#23884;&#20837;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20197;&#20445;&#25345;&#27169;&#22411;&#39640;&#20934;&#30830;&#24615;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;DD&#65289;&#20801;&#35768;&#23558;&#25968;&#25454;&#38598;&#31934;&#28860;&#20026;&#21407;&#22987;&#22823;&#23567;&#30340;&#20998;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20016;&#23500;&#30340;&#20998;&#24067;&#20449;&#24687;&#65292;&#20351;&#24471;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#33410;&#30465;&#26174;&#33879;&#35745;&#31639;&#36127;&#36733;&#30340;&#21516;&#26102;&#36798;&#21040;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;&#26368;&#36817;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#25552;&#39640;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;DD&#30340;&#19968;&#31181;&#26032;&#35270;&#35282;&#12290;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#20013;&#23884;&#20837;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20197;&#20351;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20445;&#25345;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#33719;&#24471;&#26356;&#22909;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#26354;&#29575;&#27491;&#21017;&#21270;&#32435;&#20837;&#21040;&#31934;&#28860;&#36807;&#31243;&#20013;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26032;&#26041;&#27861;&#65292;&#32780;&#36825;&#31181;&#26041;&#27861;&#30340;&#35745;&#31639;&#24320;&#38144;&#27604;&#26631;&#20934;&#30340;&#23545;&#25239;&#35757;&#32451;&#35201;&#23569;&#24471;&#22810;&#12290;&#22823;&#37327;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#20934;&#30830;&#24615;&#19978;&#20248;&#20110;&#26631;&#20934;&#23545;&#25239;&#35757;&#32451;&#65292;&#21516;&#26102;&#22312;&#23545;&#25239;&#24615;&#33021;&#26041;&#38754;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10045v1 Announce Type: new  Abstract: Dataset distillation (DD) allows datasets to be distilled to fractions of their original size while preserving the rich distributional information so that models trained on the distilled datasets can achieve a comparable accuracy while saving significant computational loads. Recent research in this area has been focusing on improving the accuracy of models trained on distilled datasets. In this paper, we aim to explore a new perspective of DD. We study how to embed adversarial robustness in distilled datasets, so that models trained on these datasets maintain the high accuracy and meanwhile acquire better adversarial robustness. We propose a new method that achieves this goal by incorporating curvature regularization into the distillation process with much less computational overhead than standard adversarial training. Extensive empirical experiments suggest that our method not only outperforms standard adversarial training on both accur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#21333;&#24352;&#22270;&#20687;&#20013;&#29983;&#25104;&#20855;&#26377;&#36924;&#30495;&#31227;&#21160;&#30340;&#20154;&#29289;&#21160;&#30011;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#20154;&#20307;&#19981;&#21487;&#35265;&#37096;&#20998;&#30340;&#21512;&#25104;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10889</link><description>&lt;p&gt;
&#20351;&#29992;3D&#25511;&#21046;&#21512;&#25104;&#31227;&#21160;&#20154;&#29289;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Moving People with 3D Control. (arXiv:2401.10889v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#21333;&#24352;&#22270;&#20687;&#20013;&#29983;&#25104;&#20855;&#26377;&#36924;&#30495;&#31227;&#21160;&#30340;&#20154;&#29289;&#21160;&#30011;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#20154;&#20307;&#19981;&#21487;&#35265;&#37096;&#20998;&#30340;&#21512;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#21333;&#24352;&#22270;&#20687;&#20013;&#20026;&#32473;&#23450;&#30340;&#30446;&#26631;3D&#36816;&#21160;&#24207;&#21015;&#29983;&#25104;&#20154;&#29289;&#21160;&#30011;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#21547;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65306;a) &#23398;&#20064;&#20851;&#20110;&#20154;&#20307;&#21644;&#26381;&#35013;&#19981;&#21487;&#35265;&#37096;&#20998;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;b) &#20197;&#36866;&#24403;&#30340;&#26381;&#35013;&#21644;&#32441;&#29702;&#28210;&#26579;&#26032;&#30340;&#20154;&#20307;&#23039;&#21183;&#12290;&#23545;&#20110;&#31532;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#31181;&#22635;&#20805;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#32473;&#23450;&#21333;&#24352;&#22270;&#20687;&#29983;&#25104;&#20154;&#29289;&#30340;&#19981;&#21487;&#35265;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#32441;&#29702;&#26144;&#23556;&#31354;&#38388;&#19978;&#35757;&#32451;&#36825;&#20010;&#27169;&#22411;&#65292;&#20351;&#20854;&#23545;&#23039;&#21183;&#21644;&#35270;&#35282;&#19981;&#21464;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#28210;&#26579;&#27969;&#27700;&#32447;&#65292;&#30001;3D&#20154;&#20307;&#23039;&#21183;&#25511;&#21046;&#12290;&#36825;&#21487;&#20197;&#20135;&#29983;&#36924;&#30495;&#30340;&#20154;&#29289;&#26032;&#23039;&#21183;&#30340;&#28210;&#26579;&#22270;&#20687;&#65292;&#21253;&#25324;&#26381;&#35013;&#12289;&#22836;&#21457;&#21644;&#26410;&#30693;&#21306;&#22495;&#30340;&#21512;&#29702;&#34917;&#20805;&#12290;&#36825;&#31181;&#20998;&#35299;&#30340;&#26041;&#27861;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#19968;&#31995;&#21015;&#22270;&#20687;&#65292;&#26082;&#31526;&#21512;3D&#23039;&#21183;&#20013;&#30340;&#30446;&#26631;&#36816;&#21160;&#65292;&#20063;&#31526;&#21512;&#35270;&#35273;&#19978;&#19982;&#36755;&#20837;&#22270;&#20687;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a diffusion model-based framework for animating people from a single image for a given target 3D motion sequence. Our approach has two core components: a) learning priors about invisible parts of the human body and clothing, and b) rendering novel body poses with proper clothing and texture. For the first part, we learn an in-filling diffusion model to hallucinate unseen parts of a person given a single image. We train this model on texture map space, which makes it more sample-efficient since it is invariant to pose and viewpoint. Second, we develop a diffusion-based rendering pipeline, which is controlled by 3D human poses. This produces realistic renderings of novel poses of the person, including clothing, hair, and plausible in-filling of unseen regions. This disentangled approach allows our method to generate a sequence of images that are faithful to the target motion in the 3D pose and, to the input image in terms of visual similarity. In addition to tha
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#20381;&#38752;&#29702;&#35770;&#29289;&#29702;&#30340;&#24605;&#24819;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#26500;&#24314;&#32039;&#20945;&#30340;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#25968;&#25454;&#30340;&#22522;&#26412;&#32467;&#26500;&#21644;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#30452;&#35266;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#39564;&#35777;&#24615;&#65292;&#24182;&#21487;&#20197;&#22312;&#24191;&#20041;&#35774;&#32622;&#20013;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.01930</link><description>&lt;p&gt;
&#23398;&#20064;ECG&#20449;&#21495;&#29305;&#24449;&#30340;&#38750;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning ECG signal features without backpropagation. (arXiv:2307.01930v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01930
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#65292;&#20381;&#38752;&#29702;&#35770;&#29289;&#29702;&#30340;&#24605;&#24819;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#26500;&#24314;&#32039;&#20945;&#30340;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#25968;&#25454;&#30340;&#22522;&#26412;&#32467;&#26500;&#21644;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#30452;&#35266;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#39564;&#35777;&#24615;&#65292;&#24182;&#21487;&#20197;&#22312;&#24191;&#20041;&#35774;&#32622;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#65292;&#23427;&#26088;&#22312;&#21457;&#29616;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#21644;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#21407;&#22987;&#25968;&#25454;&#30340;&#26377;&#25928;&#29305;&#24449;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#31867;&#22411;&#25968;&#25454;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#20381;&#38752;&#29702;&#35770;&#29289;&#29702;&#30340;&#24605;&#24819;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#26500;&#24314;&#32039;&#20945;&#30340;&#34920;&#31034;&#65292;&#24182;&#21487;&#20197;&#25429;&#25417;&#21040;&#25968;&#25454;&#30340;&#22522;&#26412;&#32467;&#26500;&#21644;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#30452;&#35266;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;&#36825;&#20010;&#26032;&#26041;&#27861;&#26088;&#22312;&#35782;&#21035;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#23646;&#20110;&#29305;&#23450;&#31867;&#21035;&#30340;&#26679;&#26412;&#20043;&#38388;&#20849;&#20139;&#29305;&#24449;&#30340;&#32447;&#24615;&#35268;&#24459;&#12290;&#36890;&#36807;&#38543;&#21518;&#21033;&#29992;&#36825;&#20123;&#35268;&#24459;&#22312;&#21069;&#21521;&#26041;&#24335;&#19979;&#29983;&#25104;&#19968;&#20010;&#19982;&#20998;&#31867;&#22120;&#26080;&#20851;&#30340;&#34920;&#31034;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#24191;&#20041;&#35774;&#32622;&#20013;&#24212;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning has become a crucial area of research in machine learning, as it aims to discover efficient ways of representing raw data with useful features to increase the effectiveness, scope and applicability of downstream tasks such as classification and prediction. In this paper, we propose a novel method to generate representations for time series-type data. This method relies on ideas from theoretical physics to construct a compact representation in a data-driven way, and it can capture both the underlying structure of the data and task-specific information while still remaining intuitive, interpretable and verifiable. This novel methodology aims to identify linear laws that can effectively capture a shared characteristic among samples belonging to a specific class. By subsequently utilizing these laws to generate a classifier-agnostic representation in a forward manner, they become applicable in a generalized setting. We demonstrate the effectiveness of our approach o
&lt;/p&gt;</description></item><item><title>&#21435;&#22122;&#22768;&#26356;&#22909;&#65292;&#34920;&#29616;&#26356;&#22909;&#30340;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;</title><link>http://arxiv.org/abs/2306.04940</link><description>&lt;p&gt;
&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Layer-level activation mechanism. (arXiv:2306.04940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04940
&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#22768;&#26356;&#22909;&#65292;&#34920;&#29616;&#26356;&#22909;&#30340;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28608;&#27963;&#26426;&#21046;&#65292;&#26088;&#22312;&#24314;&#31435;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#21151;&#33021;&#65288;LayerAct&#65289;&#12290;&#36825;&#20123;&#21151;&#33021;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36755;&#20837;&#20559;&#31227;&#25152;&#23548;&#33268;&#30340;&#28608;&#27963;&#36755;&#20986;&#30340;&#20998;&#23618;&#32423;&#27874;&#21160;&#26469;&#38477;&#20302;&#20256;&#32479;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#30340;&#22122;&#38899;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;LayerAct&#21151;&#33021;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#38646;&#30340;&#24179;&#22343;&#28608;&#27963;&#36755;&#20986;&#65292;&#32780;&#19981;&#38480;&#21046;&#28608;&#27963;&#36755;&#20986;&#31354;&#38388;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#35777;&#26126;LayerAct&#21151;&#33021;&#22312;&#22122;&#22768;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#65292;&#24182;&#19988;&#32463;&#39564;&#35777;&#26126;&#36825;&#20123;&#21151;&#33021;&#30340;&#24179;&#22343;&#28608;&#27963;&#32467;&#26524;&#31867;&#20284;&#20110;&#38646;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22788;&#29702;&#22024;&#26434;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#26102;&#65292;LayerAct&#21151;&#33021;&#27604;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#28165;&#27905;&#25968;&#25454;&#38598;&#30340;&#34920;&#29616;&#20063;&#26159;&#20248;&#36234;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel activation mechanism aimed at establishing layer-level activation (LayerAct) functions. These functions are designed to be more noise-robust compared to traditional element-level activation functions by reducing the layer-level fluctuation of the activation outputs due to shift in inputs. Moreover, the LayerAct functions achieve a zero-like mean activation output without restricting the activation output space. We present an analysis and experiments demonstrating that LayerAct functions exhibit superior noise-robustness compared to element-level activation functions, and empirically show that these functions have a zero-like mean activation. Experimental results on three benchmark image classification tasks show that LayerAct functions excel in handling noisy image datasets, outperforming element-level activation functions, while the performance on clean datasets is also superior in most cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23558;&#19968;&#20123;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#65292;&#22312;MS COCO&#25968;&#25454;&#38598;&#19978;&#20026;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2304.10727</link><description>&lt;p&gt;
RoCOCO&#65306;&#31283;&#20581;&#30340;&#22522;&#20934;MS-COCO&#35780;&#20272;&#22270;&#25991;&#21305;&#37197;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
RoCOCO: Robust Benchmark MS-COCO to Stress-test Robustness of Image-Text Matching Models. (arXiv:2304.10727v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23558;&#19968;&#20123;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#65292;&#22312;MS COCO&#25968;&#25454;&#38598;&#19978;&#20026;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#20041;&#23884;&#20837;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;MS COCO 5K&#27979;&#35797;&#38598;&#19978;&#22270;&#25991;&#21305;&#37197;&#65288;ITM&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#26102;&#65292;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25554;&#20837;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#26469;&#26356;&#25913;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#21517;&#35789;&#26469;&#26356;&#25913;&#26631;&#39064;&#65292;&#20174;&#32780;&#25913;&#21464;&#21477;&#23376;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#20165;&#23558;&#36825;&#20123;&#26032;&#21019;&#24314;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#27979;&#35797;&#38598;&#20013;&#23601;&#21487;&#20197;&#38477;&#20302;&#21508;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65288;&#20363;&#22914;&#65292;&#22312;BLIP&#20013;&#20174;81.9&#65285;&#38477;&#33267;64.5&#65285;&#65292;&#22312;VSE&#8734;&#20013;&#20174;66.1&#65285;&#38477;&#33267;37.5&#65285;&#65289;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#21457;&#29616;&#33021;&#20026;&#25552;&#39640;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#35774;&#35745;&#26356;&#22810;&#26679;&#21270;&#30340;&#21387;&#21147;&#27979;&#35797;&#25552;&#20379;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large-scale vision-language pre-training models and visual semantic embedding methods have significantly improved image-text matching (ITM) accuracy on MS COCO 5K test set. However, it is unclear how robust these state-of-the-art (SOTA) models are when using them in the wild. In this paper, we propose a novel evaluation benchmark to stress-test the robustness of ITM models. To this end, we add various fooling images and captions to a retrieval pool. Specifically, we change images by inserting unrelated images, and change captions by substituting a noun, which can change the meaning of a sentence. We discover that just adding these newly created images and captions to the test set can degrade performances (i.e., Recall@1) of a wide range of SOTA models (e.g., 81.9% $\rightarrow$ 64.5% in BLIP, 66.1% $\rightarrow$ 37.5% in VSE$\infty$). We expect that our findings can provide insights for improving the robustness of the vision-language models and devising more diverse stress-te
&lt;/p&gt;</description></item><item><title>EDO-Net&#26159;&#19968;&#20010;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#24377;&#24615;&#23646;&#24615;&#30340;&#22270;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#25552;&#21462;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#29289;&#29702;&#23646;&#24615;&#65292;&#23454;&#29616;&#23545;&#31867;&#20284;&#24067;&#26009;&#30340;&#23545;&#35937;&#26410;&#26469;&#29366;&#24577;&#30340;&#39044;&#27979;&#21644;&#36716;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2209.08996</link><description>&lt;p&gt;
EDO-Net: &#20174;&#22270;&#21160;&#21147;&#23398;&#20013;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#24377;&#24615;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
EDO-Net: Learning Elastic Properties of Deformable Objects from Graph Dynamics. (arXiv:2209.08996v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08996
&lt;/p&gt;
&lt;p&gt;
EDO-Net&#26159;&#19968;&#20010;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#24377;&#24615;&#23646;&#24615;&#30340;&#22270;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#25552;&#21462;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#29289;&#29702;&#23646;&#24615;&#65292;&#23454;&#29616;&#23545;&#31867;&#20284;&#24067;&#26009;&#30340;&#23545;&#35937;&#26410;&#26469;&#29366;&#24577;&#30340;&#39044;&#27979;&#21644;&#36716;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#22270;&#21160;&#21147;&#23398;&#30340;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#29289;&#29702;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#21033;&#29992;&#21487;&#25552;&#21462;&#30340;&#31867;&#20284;&#24067;&#26009;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#24377;&#24615;&#29289;&#29702;&#23646;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20363;&#22914;&#20174;&#25289;&#20280;&#20132;&#20114;&#20013;&#25552;&#21462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EDO-Net&#65288;&#24377;&#24615;&#21487;&#21464;&#24418;&#23545;&#35937;-Net&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#20855;&#26377;&#19981;&#21516;&#24377;&#24615;&#23646;&#24615;&#30340;&#22823;&#37327;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#22270;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#19981;&#20381;&#36182;&#20110;&#23646;&#24615;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;EDO-Net&#20849;&#21516;&#23398;&#20064;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27169;&#22359;&#21644;&#19968;&#20010;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22359;&#12290;&#21069;&#32773;&#36127;&#36131;&#25552;&#21462;&#23545;&#35937;&#30340;&#29289;&#29702;&#29305;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#32780;&#21518;&#32773;&#21033;&#29992;&#28508;&#22312;&#34920;&#31034;&#26469;&#39044;&#27979;&#20197;&#22270;&#24418;&#34920;&#31034;&#30340;&#31867;&#20284;&#24067;&#26009;&#30340;&#23545;&#35937;&#30340;&#26410;&#26469;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#35780;&#20272;&#20102;EDO-Net&#30340;&#33021;&#21147;&#65306;1&#65289;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#29289;&#29702;&#23646;&#24615;&#65292;2&#65289;&#36716;&#31227;&#23398;&#20064;&#25152;&#23398;&#21040;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning graph dynamics of deformable objects that generalizes to unknown physical properties. Our key insight is to leverage a latent representation of elastic physical properties of cloth-like deformable objects that can be extracted, for example, from a pulling interaction. In this paper we propose EDO-Net (Elastic Deformable Object - Net), a model of graph dynamics trained on a large variety of samples with different elastic properties that does not rely on ground-truth labels of the properties. EDO-Net jointly learns an adaptation module, and a forward-dynamics module. The former is responsible for extracting a latent representation of the physical properties of the object, while the latter leverages the latent representation to predict future states of cloth-like objects represented as graphs. We evaluate EDO-Net both in simulation and real world, assessing its capabilities of: 1) generalizing to unknown physical properties, 2) transferring the learned rep
&lt;/p&gt;</description></item></channel></rss>