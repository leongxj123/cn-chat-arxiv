<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#23558;&#19968;&#33324;&#20154;&#21475;&#22270;&#35889;&#36716;&#21464;&#20026;&#29305;&#23450;&#23376;&#20154;&#21475;&#30340;&#22270;&#35889;&#65292;&#30830;&#20445;&#32467;&#26500;&#21512;&#29702;&#24615;&#65292;&#36991;&#20813;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2403.16776</link><description>&lt;p&gt;
Diff-Def: &#36890;&#36807;&#25193;&#25955;&#29983;&#25104;&#30340;&#24418;&#21464;&#22330;&#36827;&#34892;&#26377;&#26465;&#20214;&#30340;&#22270;&#35889;&#21046;&#20316;
&lt;/p&gt;
&lt;p&gt;
Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16776
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#23558;&#19968;&#33324;&#20154;&#21475;&#22270;&#35889;&#36716;&#21464;&#20026;&#29305;&#23450;&#23376;&#20154;&#21475;&#30340;&#22270;&#35889;&#65292;&#30830;&#20445;&#32467;&#26500;&#21512;&#29702;&#24615;&#65292;&#36991;&#20813;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#21078;&#22270;&#35889;&#24191;&#27867;&#24212;&#29992;&#20110;&#20154;&#21475;&#20998;&#26512;&#12290;&#26377;&#26465;&#20214;&#30340;&#22270;&#35889;&#38024;&#23545;&#36890;&#36807;&#29305;&#23450;&#26465;&#20214;&#65288;&#22914;&#20154;&#21475;&#32479;&#35745;&#23398;&#25110;&#30149;&#29702;&#23398;&#65289;&#23450;&#20041;&#30340;&#29305;&#23450;&#23376;&#20154;&#21475;&#65292;&#24182;&#20801;&#35768;&#30740;&#31350;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#24418;&#24577;&#23398;&#24046;&#24322;&#31561;&#32454;&#31890;&#24230;&#35299;&#21078;&#23398;&#24046;&#24322;&#12290;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#37197;&#20934;&#30340;&#26041;&#27861;&#25110;&#29983;&#25104;&#27169;&#22411;&#65292;&#21069;&#32773;&#26080;&#27861;&#22788;&#29702;&#22823;&#30340;&#35299;&#21078;&#23398;&#21464;&#24322;&#65292;&#21518;&#32773;&#21487;&#33021;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#19981;&#31283;&#23450;&#21644;&#24187;&#35273;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#23558;&#19968;&#20010;&#24120;&#35268;&#20154;&#21475;&#22270;&#35889;&#36716;&#21464;&#20026;&#20195;&#34920;&#29305;&#23450;&#23376;&#20154;&#21475;&#30340;&#22270;&#35889;&#12290;&#36890;&#36807;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#24182;&#23558;&#26377;&#26465;&#20214;&#30340;&#22270;&#35889;&#27880;&#20876;&#21040;&#19968;&#32452;&#22270;&#20687;&#38468;&#36817;&#65292;&#25105;&#20204;&#30830;&#20445;&#32467;&#26500;&#30340;&#21512;&#29702;&#24615;&#65292;&#36991;&#20813;&#30452;&#25509;&#22270;&#20687;&#21512;&#25104;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16776v1 Announce Type: cross  Abstract: Anatomical atlases are widely used for population analysis. Conditional atlases target a particular sub-population defined via certain conditions (e.g. demographics or pathologies) and allow for the investigation of fine-grained anatomical differences - such as morphological changes correlated with age. Existing approaches use either registration-based methods that are unable to handle large anatomical variations or generative models, which can suffer from training instabilities and hallucinations. To overcome these limitations, we use latent diffusion models to generate deformation fields, which transform a general population atlas into one representing a specific sub-population. By generating a deformation field and registering the conditional atlas to a neighbourhood of images, we ensure structural plausibility and avoid hallucinations, which can occur during direct image synthesis. We compare our method to several state-of-the-art 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;REAL&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21452;&#27969;&#22522;&#30784;&#39044;&#35757;&#32451;&#21644;&#34920;&#31034;&#22686;&#24378;&#33976;&#39311;&#36807;&#31243;&#26469;&#22686;&#24378;&#25552;&#21462;&#22120;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13522</link><description>&lt;p&gt;
REAL&#65306;&#29992;&#20110;&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#34920;&#31034;&#22686;&#24378;&#20998;&#26512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
REAL: Representation Enhanced Analytic Learning for Exemplar-free Class-incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;REAL&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21452;&#27969;&#22522;&#30784;&#39044;&#35757;&#32451;&#21644;&#34920;&#31034;&#22686;&#24378;&#33976;&#39311;&#36807;&#31243;&#26469;&#22686;&#24378;&#25552;&#21462;&#22120;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#33539;&#20363;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;(EFCIL)&#26088;&#22312;&#20943;&#36731;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32780;&#27809;&#26377;&#21487;&#29992;&#30340;&#21382;&#21490;&#25968;&#25454;&#12290;&#19982;&#23384;&#20648;&#21382;&#21490;&#26679;&#26412;&#30340;&#22238;&#25918;&#24335;CIL&#30456;&#27604;&#65292;EFCIL&#22312;&#26080;&#33539;&#20363;&#32422;&#26463;&#19979;&#26356;&#23481;&#26131;&#36951;&#24536;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#26368;&#36817;&#21457;&#23637;&#30340;&#22522;&#20110;&#20998;&#26512;&#23398;&#20064;(AL)&#30340;CIL&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;EFCIL&#30340;&#34920;&#31034;&#22686;&#24378;&#20998;&#26512;&#23398;&#20064;(REAL)&#12290;REAL&#26500;&#24314;&#20102;&#19968;&#20010;&#21452;&#27969;&#22522;&#30784;&#39044;&#35757;&#32451;(DS-BPT)&#21644;&#19968;&#20010;&#34920;&#31034;&#22686;&#24378;&#33976;&#39311;(RED)&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#25552;&#21462;&#22120;&#30340;&#34920;&#31034;&#12290;DS-BPT&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;(SSCL)&#20004;&#20010;&#27969;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#30784;&#30693;&#35782;&#25552;&#21462;&#12290;RED&#36807;&#31243;&#23558;&#30417;&#30563;&#30693;&#35782;&#25552;&#28860;&#21040;SSCL&#39044;&#35757;&#32451;&#39592;&#24178;&#37096;&#20998;&#65292;&#20419;&#36827;&#21518;&#32493;&#30340;&#22522;&#20110;AL&#30340;CIL&#65292;&#23558;CIL&#36716;&#25442;&#20026;&#36882;&#24402;&#26368;&#23567;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13522v1 Announce Type: new  Abstract: Exemplar-free class-incremental learning (EFCIL) aims to mitigate catastrophic forgetting in class-incremental learning without available historical data. Compared with its counterpart (replay-based CIL) that stores historical samples, the EFCIL suffers more from forgetting issues under the exemplar-free constraint. In this paper, inspired by the recently developed analytic learning (AL) based CIL, we propose a representation enhanced analytic learning (REAL) for EFCIL. The REAL constructs a dual-stream base pretraining (DS-BPT) and a representation enhancing distillation (RED) process to enhance the representation of the extractor. The DS-BPT pretrains model in streams of both supervised learning and self-supervised contrastive learning (SSCL) for base knowledge extraction. The RED process distills the supervised knowledge to the SSCL pretrained backbone and facilitates a subsequent AL-basd CIL that converts the CIL to a recursive least
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03299</link><description>&lt;p&gt;
GUARD: &#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#26469;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#21335;&#30340;&#21512;&#35268;&#24615;
&lt;/p&gt;
&lt;p&gt;
GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#32469;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#21644;&#26377;&#23475;&#22238;&#24212;&#30340;"&#36234;&#29425;"&#24050;&#32463;&#40723;&#21169;&#31038;&#21306;&#37319;&#21462;&#23433;&#20840;&#25514;&#26045;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#23433;&#20840;&#25514;&#26045;&#26159;&#22312;&#21457;&#24067;&#20043;&#21069;&#29992;&#36234;&#29425;&#20027;&#21160;&#27979;&#35797;LLM&#12290;&#22240;&#27492;&#65292;&#36825;&#26679;&#30340;&#27979;&#35797;&#23558;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#22823;&#35268;&#27169;&#19988;&#39640;&#25928;&#22320;&#29983;&#25104;&#36234;&#29425;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#36861;&#38543;&#19968;&#31181;&#26032;&#39062;&#32780;&#30452;&#35266;&#30340;&#31574;&#30053;&#19979;&#65292;&#20197;&#20154;&#31867;&#29983;&#25104;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#36234;&#29425;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35282;&#33394;&#25198;&#28436;&#31995;&#32479;&#65292;&#23558;&#22235;&#31181;&#19981;&#21516;&#35282;&#33394;&#20998;&#37197;&#32473;&#29992;&#25143;LLM&#65292;&#20197;&#20415;&#21327;&#20316;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25910;&#38598;&#29616;&#26377;&#30340;&#36234;&#29425;&#65292;&#24182;&#36890;&#36807;&#21477;&#23376;&#36880;&#21477;&#36827;&#34892;&#32858;&#31867;&#39057;&#29575;&#21644;&#35821;&#20041;&#27169;&#24335;&#30340;&#21010;&#20998;&#65292;&#23558;&#23427;&#20204;&#20998;&#25104;&#19981;&#21516;&#30340;&#29420;&#31435;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#29305;&#24449;&#32452;&#32455;&#25104;&#19968;&#20010;&#30693;&#35782;&#22270;&#65292;&#20351;&#20854;&#26356;&#26131;&#20110;&#35775;&#38382;&#21644;&#26816;&#32034;&#12290;&#25105;&#20204;&#30340;&#35282;&#33394;&#31995;&#32479;&#23558;&#21033;&#29992;&#36825;&#20010;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effec
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#29992;&#25143;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#22810;&#20010;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#36825;&#20123;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#21306;&#20998;&#22270;&#20687;&#23545;&#24182;&#33258;&#21160;&#26631;&#27880;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2312.03187</link><description>&lt;p&gt;
FERGI&#65306;&#26469;&#33258;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#29992;&#25143;&#20559;&#22909;&#30340;&#33258;&#21160;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
FERGI: Automatic Annotation of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03187
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#29992;&#25143;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#22810;&#20010;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#36825;&#20123;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#21306;&#20998;&#22270;&#20687;&#23545;&#24182;&#33258;&#21160;&#26631;&#27880;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20351;&#29992;&#20154;&#31867;&#20559;&#22909;&#21453;&#39304;&#25968;&#25454;&#26469;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#65292;&#20154;&#31867;&#21453;&#39304;&#25910;&#38598;&#30340;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20174;&#29992;&#25143;&#30340;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#20854;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#21040;&#29983;&#25104;&#22270;&#20687;&#65288;FERGI&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#20010;&#38754;&#37096;&#36816;&#21160;&#21333;&#20803;&#65288;AUs&#65289;&#30340;&#28608;&#27963;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AU4&#65288;&#30473;&#27611;&#19979;&#22402;&#32773;&#65289;&#21453;&#26144;&#20102;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#36127;&#38754;&#35780;&#20215;&#65292;&#32780;AU12&#65288;&#22068;&#35282;&#25289;&#21160;&#32773;&#65289;&#21453;&#26144;&#20102;&#27491;&#38754;&#35780;&#20215;&#12290;&#36825;&#20004;&#32773;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#24456;&#26377;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#22320;&#20351;&#29992;&#36825;&#20123;AU&#21709;&#24212;&#23384;&#22312;&#23454;&#36136;&#24046;&#24322;&#30340;&#22270;&#20687;&#23545;&#20043;&#38388;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03187v2 Announce Type: replace-cross  Abstract: Researchers have proposed to use data of human preference feedback to fine-tune text-to-image generative models. However, the scalability of human feedback collection has been limited by its reliance on manual annotation. Therefore, we develop and test a method to automatically annotate user preferences from their spontaneous facial expression reaction to the generated images. We collect a dataset of Facial Expression Reaction to Generated Images (FERGI) and show that the activations of multiple facial action units (AUs) are highly correlated with user evaluations of the generated images. Specifically, AU4 (brow lowerer) is reflective of negative evaluations of the generated image whereas AU12 (lip corner puller) is reflective of positive evaluations. These can be useful in two ways. Firstly, we can automatically annotate user preferences between image pairs with substantial difference in these AU responses with an accuracy sig
&lt;/p&gt;</description></item><item><title>RefinedFields&#26159;&#31532;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25913;&#21892;&#26080;&#32422;&#26463;&#22330;&#26223;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20248;&#21270;&#25351;&#23548;&#21644;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#30340;&#20808;&#39564;&#26465;&#20214;&#20013;&#25552;&#21462;&#26356;&#20016;&#23500;&#30340;&#32454;&#33410;&#65292;&#24182;&#22312;&#26032;&#35270;&#35282;&#21512;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.00639</link><description>&lt;p&gt;
RefinedFields: &#23545;&#26080;&#32422;&#26463;&#22330;&#26223;&#30340;&#36752;&#23556;&#22330;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
RefinedFields: Radiance Fields Refinement for Unconstrained Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00639
&lt;/p&gt;
&lt;p&gt;
RefinedFields&#26159;&#31532;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25913;&#21892;&#26080;&#32422;&#26463;&#22330;&#26223;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20248;&#21270;&#25351;&#23548;&#21644;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#30340;&#20808;&#39564;&#26465;&#20214;&#20013;&#25552;&#21462;&#26356;&#20016;&#23500;&#30340;&#32454;&#33410;&#65292;&#24182;&#22312;&#26032;&#35270;&#35282;&#21512;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26080;&#32422;&#26463;&#30340;&#22270;&#20687;&#20013;&#24314;&#27169;&#22823;&#22330;&#26223;&#34987;&#35777;&#26126;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#22788;&#29702;&#37326;&#22806;&#22330;&#26223;&#24314;&#27169;&#26159;&#22312;&#23553;&#38381;&#30340;&#29615;&#22659;&#20013;&#65292;&#27809;&#26377;&#23545;&#20174;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#33719;&#24471;&#30340;&#20808;&#39564;&#26465;&#20214;&#36827;&#34892;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RefinedFields&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25913;&#21892;&#37326;&#22806;&#22330;&#26223;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#32593;&#32476;&#36890;&#36807;&#20248;&#21270;&#25351;&#23548;&#20351;&#29992;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#26469;&#32454;&#21270;K-Planes&#34920;&#31034;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#25105;&#20204;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#26053;&#28216;&#29031;&#29255;&#38598;&#19978;&#30340;&#20248;&#28857;&#12290;RefinedFields&#22686;&#24378;&#20102;&#28210;&#26579;&#22330;&#26223;&#30340;&#32454;&#33410;&#65292;&#20248;&#20110;&#20197;&#24448;&#22312;&#37326;&#22806;&#36827;&#34892;&#26032;&#35270;&#35282;&#21512;&#25104;&#20219;&#21153;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#39029;&#38754;&#21487;&#20197;&#22312;https://refinedfields.github.io&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling large scenes from unconstrained images has proven to be a major challenge in computer vision. Existing methods tackling in-the-wild scene modeling operate in closed-world settings, where no conditioning on priors acquired from real-world images is present. We propose RefinedFields, which is, to the best of our knowledge, the first method leveraging pre-trained models to improve in-the-wild scene modeling. We employ pre-trained networks to refine K-Planes representations via optimization guidance using an alternating training procedure. We carry out extensive experiments and verify the merit of our method on synthetic data and real tourism photo collections. RefinedFields enhances rendered scenes with richer details and outperforms previous work on the task of novel view synthesis in the wild. Our project page can be found at https://refinedfields.github.io .
&lt;/p&gt;</description></item></channel></rss>