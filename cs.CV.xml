<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#30340;&#25968;&#25454;&#20013;&#24515;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20869;&#26680;&#20989;&#25968;&#35782;&#21035;&#25552;&#20379;&#26368;&#20339;&#39044;&#27979;&#25903;&#25345;&#32473;&#27979;&#35797;&#28857;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15576</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#30340;&#25968;&#25454;&#20013;&#24515;&#39044;&#27979;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Data-centric Prediction Explanation via Kernelized Stein Discrepancy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15576
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#30340;&#25968;&#25454;&#20013;&#24515;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20869;&#26680;&#20989;&#25968;&#35782;&#21035;&#25552;&#20379;&#26368;&#20339;&#39044;&#27979;&#25903;&#25345;&#32473;&#27979;&#35797;&#28857;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#20110;&#31034;&#20363;&#30340;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#27169;&#22411;&#30340;&#21442;&#25968;&#25110;&#28508;&#22312;&#34920;&#31034;&#26469;&#36830;&#25509;&#27979;&#35797;&#21644;&#35757;&#32451;&#25968;&#25454;&#28857;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#20851;&#27169;&#22411;&#39044;&#27979;&#21407;&#22240;&#30340;&#32447;&#32034;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#34920;&#29616;&#20986;&#22266;&#26377;&#30340;&#32570;&#38519;&#65292;&#27604;&#22914;&#20135;&#29983;&#26174;&#30528;&#30340;&#35745;&#31639;&#24320;&#38144;&#25110;&#29983;&#25104;&#31895;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#31934;&#24230;&#21644;&#25968;&#25454;&#20013;&#24515;&#30340;&#35299;&#37322;&#65288;HD-Explain&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#65288;KSD&#65289;&#23646;&#24615;&#30340;&#31616;&#21333;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;KSD&#21807;&#19968;&#22320;&#20026;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#20869;&#26680;&#20989;&#25968;&#65292;&#29992;&#20110;&#32534;&#30721;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#25968;&#25454;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#20869;&#26680;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#25552;&#20379;&#26368;&#20339;&#39044;&#27979;&#25903;&#25345;&#32473;&#27979;&#35797;&#28857;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20998;&#31867;&#39046;&#22495;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;HD-Explain&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15576v1 Announce Type: new  Abstract: Existing example-based prediction explanation methods often bridge test and training data points through the model's parameters or latent representations. While these methods offer clues to the causes of model predictions, they often exhibit innate shortcomings, such as incurring significant computational overhead or producing coarse-grained explanations. This paper presents a Highly-precise and Data-centric Explanation (HD-Explain), a straightforward prediction explanation method exploiting properties of Kernelized Stein Discrepancy (KSD). Specifically, the KSD uniquely defines a parameterized kernel function for a trained model that encodes model-dependent data correlation. By leveraging the kernel function, one can identify training samples that provide the best predictive support to a test point efficiently. We conducted thorough analyses and experiments across multiple classification domains, where we show that HD-Explain outperform
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25511;&#21046;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#21453;&#39304;&#26426;&#21046;&#65292;&#19968;&#26041;&#38754;&#20351;&#29992;&#30417;&#30563;&#27169;&#22411;&#21453;&#39304;&#25214;&#21040;&#23545;&#25239;&#24615;&#25552;&#31034;&#35789;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#65292;&#21478;&#19968;&#26041;&#38754;&#36890;&#36807;&#24341;&#23548;&#20351;&#29983;&#25104;&#36807;&#31243;&#26397;&#21521;&#29305;&#23450;&#30446;&#26631;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2403.15309</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25511;&#21046;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Controlled Training Data Generation with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15309
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25511;&#21046;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#21453;&#39304;&#26426;&#21046;&#65292;&#19968;&#26041;&#38754;&#20351;&#29992;&#30417;&#30563;&#27169;&#22411;&#21453;&#39304;&#25214;&#21040;&#23545;&#25239;&#24615;&#25552;&#31034;&#35789;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#65292;&#21478;&#19968;&#26041;&#38754;&#36890;&#36807;&#24341;&#23548;&#20351;&#29983;&#25104;&#36807;&#31243;&#26397;&#21521;&#29305;&#23450;&#30446;&#26631;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20197;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#19987;&#38376;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#12290;&#19982;&#20043;&#21069;&#37027;&#20123;&#37319;&#29992;&#24320;&#29615;&#26041;&#27861;&#24182;&#39044;&#20808;&#23450;&#20041;&#25552;&#31034;&#35789;&#26469;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25110;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#29983;&#25104;&#26032;&#25968;&#25454;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#38381;&#29615;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#21453;&#39304;&#26426;&#21046;&#12290;&#31532;&#19968;&#20010;&#26426;&#21046;&#20351;&#29992;&#26469;&#33258;&#32473;&#23450;&#30417;&#30563;&#27169;&#22411;&#30340;&#21453;&#39304;&#65292;&#24182;&#25214;&#21040;&#23548;&#33268;&#22270;&#20687;&#29983;&#25104;&#26368;&#22823;&#21270;&#27169;&#22411;&#25439;&#22833;&#30340;&#23545;&#25239;&#25552;&#31034;&#35789;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#25239;&#25552;&#31034;&#35789;&#23548;&#33268;&#20102;&#32463;&#36807;&#27169;&#22411;&#35757;&#32451;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#29983;&#25104;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#30693;&#36947;&#30446;&#26631;&#20998;&#24067;&#65292;&#36825;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#31532;&#20108;&#20010;&#21453;&#39304;&#26426;&#21046;&#65292;&#23558;&#29983;&#25104;&#36807;&#31243;&#24341;&#23548;&#21040;&#29305;&#23450;&#30446;&#26631;&#20998;&#24067;&#12290;&#25105;&#20204;&#31216;&#23558;&#36825;&#20004;&#20010;&#26426;&#21046;&#32467;&#21512;&#36215;&#26469;&#30340;&#26041;&#27861;&#20026;&#24341;&#23548;&#23545;&#25239;&#25552;&#31034;&#35789;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15309v1 Announce Type: cross  Abstract: In this work, we present a method to control a text-to-image generative model to produce training data specifically "useful" for supervised learning. Unlike previous works that employ an open-loop approach and pre-define prompts to generate new data using either a language model or human expertise, we develop an automated closed-loop system which involves two feedback mechanisms. The first mechanism uses feedback from a given supervised model and finds adversarial prompts that result in image generations that maximize the model loss. While these adversarial prompts result in diverse data informed by the model, they are not informed of the target distribution, which can be inefficient. Therefore, we introduce the second feedback mechanism that guides the generation process towards a certain target distribution. We call the method combining these two mechanisms Guided Adversarial Prompts. We perform our evaluations on different tasks, da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#35821;&#35328;-Only VLM&#36873;&#25321;&#20013;&#30340;&#20004;&#20010;&#22266;&#26377;&#25361;&#25112;&#65306;&#12300;&#27169;&#24577;&#24046;&#36317;&#12301;&#21644;&#12300;&#33021;&#21147;&#24046;&#36317;&#12301;&#65292;&#24182;&#25552;&#20986;&#20102;VLM&#36873;&#25321;&#20013;&#24357;&#21512;&#36825;&#20004;&#20010;&#24046;&#36317;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.13797</link><description>&lt;p&gt;
&#24357;&#21512;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#21644;&#33021;&#21147;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridge the Modality and Capacity Gaps in Vision-Language Model Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#35821;&#35328;-Only VLM&#36873;&#25321;&#20013;&#30340;&#20004;&#20010;&#22266;&#26377;&#25361;&#25112;&#65306;&#12300;&#27169;&#24577;&#24046;&#36317;&#12301;&#21644;&#12300;&#33021;&#21147;&#24046;&#36317;&#12301;&#65292;&#24182;&#25552;&#20986;&#20102;VLM&#36873;&#25321;&#20013;&#24357;&#21512;&#36825;&#20004;&#20010;&#24046;&#36317;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#36890;&#36807;&#23558;&#22270;&#20687;&#19982;&#25991;&#26412;&#31867;&#21035;&#21517;&#31216;&#37197;&#23545;&#65292;&#22312;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#39044;&#35757;&#32451;&#30340;VLMs&#30340;&#19981;&#26029;&#22686;&#21152;&#20351;&#24471;&#29305;&#23450;&#20219;&#21153;&#30340;VLM&#36873;&#25321;&#26356;&#26377;&#21487;&#33021;&#26631;&#35782;&#20986;&#36866;&#21512;&#30340;VLM&#12290;&#22240;&#27492;&#65292;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#31574;&#30053;&#26159;&#20174;VLM&#21160;&#29289;&#22253;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;VLM&#65292;&#20165;&#20381;&#36182;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#25968;&#25454;&#32780;&#26080;&#38656;&#35775;&#38382;&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#36825;&#31181;&#20165;&#35821;&#35328;VLM&#36873;&#25321;&#20013;&#20004;&#20010;&#22266;&#26377;&#25361;&#25112;&#65306;&#12300;&#27169;&#24577;&#24046;&#36317;&#12301;&#8212;&#8212;VLM&#22312;&#20004;&#20010;&#19981;&#21516;&#27169;&#24577;&#19979;&#30340;&#23884;&#20837;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20351;&#24471;&#25991;&#26412;&#25104;&#20026;&#22270;&#20687;&#30340;&#19968;&#20010;&#19981;&#22826;&#21487;&#38752;&#30340;&#26367;&#20195;&#21697;&#65307;&#12300;&#33021;&#21147;&#24046;&#36317;&#12301;&#8212;&#8212;VLM&#30340;&#25972;&#20307;&#25490;&#21517;&#19982;&#20854;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#25490;&#21517;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#38459;&#30861;&#20102;&#30452;&#25509;&#20174;&#27169;&#22411;&#30340;&#25972;&#20307;&#34920;&#29616;&#26469;&#39044;&#27979;&#20854;&#25968;&#25454;&#38598;&#29305;&#23450;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;VLM&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13797v1 Announce Type: new  Abstract: Vision Language Models (VLMs) excel in zero-shot image classification by pairing images with textual category names. The expanding variety of Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for specific tasks. Thus, a promising zero-shot image classification strategy is selecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely on the text data of the target dataset without access to the dataset's images. In this paper, we analyze two inherent challenges in assessing the ability of a VLM in this Language-Only VLM selection: the "Modality Gap" -- the disparity in VLM's embeddings across two different modalities, making text a less reliable substitute for images; and the "Capability Gap" -- the discrepancy between the VLM's overall ranking and its ranking for target dataset, hindering direct prediction of a model's dataset-specific performance from its general performance. We propose VLM Selectio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#21644;&#20998;&#23618;&#23545;&#40784;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#22810;&#27169;&#24577;&#32479;&#19968;&#34920;&#31034;&#30340;&#32454;&#31890;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36328;&#27169;&#24577;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.05168</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#21644;&#20998;&#23618;&#23545;&#40784;&#35299;&#38145;&#22810;&#27169;&#24577;&#32479;&#19968;&#31163;&#25955;&#34920;&#31034;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of Multimodal Unified Discrete Representation through Training-Free Codebook Optimization and Hierarchical Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#21644;&#20998;&#23618;&#23545;&#40784;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#22810;&#27169;&#24577;&#32479;&#19968;&#34920;&#31034;&#30340;&#32454;&#31890;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36328;&#27169;&#24577;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#34920;&#26126;&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#32479;&#19968;&#30721;&#26412;&#30340;&#21452;&#20132;&#21449;&#27169;&#24577;&#20449;&#24687;&#35299;&#32544;&#65288;DCID&#65289;&#27169;&#22411;&#22312;&#23454;&#29616;&#32454;&#31890;&#24230;&#34920;&#31034;&#21644;&#36328;&#27169;&#24577;&#27867;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20173;&#21463;&#21040;&#23545;&#25152;&#26377;&#36890;&#36947;&#30340;&#22343;&#31561;&#23545;&#24453;&#20197;&#21450;&#24573;&#35270;&#27425;&#35201;&#20107;&#20214;&#20449;&#24687;&#30340;&#38459;&#30861;&#65292;&#23548;&#33268;&#26469;&#33258;&#26080;&#20851;&#36890;&#36947;&#30340;&#24178;&#25200;&#24182;&#22312;&#32454;&#31890;&#24230;&#20219;&#21153;&#20013;&#34920;&#29616;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#65288;TOC&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32479;&#19968;&#31354;&#38388;&#20013;&#36873;&#25321;&#37325;&#35201;&#36890;&#36947;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#21452;&#20132;&#21449;&#27169;&#24577;&#20449;&#24687;&#35299;&#32544;&#65288;H-DCID&#65289;&#26041;&#27861;&#23558;&#20449;&#24687;&#20998;&#31163;&#21644;&#23545;&#40784;&#25193;&#23637;&#21040;&#20004;&#20010;&#32423;&#21035;&#65292;&#25429;&#25417;&#26356;&#22810;&#36328;&#27169;&#24577;&#32454;&#33410;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05168v1 Announce Type: cross  Abstract: Recent advances in representation learning have demonstrated the significance of multimodal alignment. The Dual Cross-modal Information Disentanglement (DCID) model, utilizing a unified codebook, shows promising results in achieving fine-grained representation and cross-modal generalization. However, it is still hindered by equal treatment of all channels and neglect of minor event information, resulting in interference from irrelevant channels and limited performance in fine-grained tasks. Thus, in this work, We propose a Training-free Optimization of Codebook (TOC) method to enhance model performance by selecting important channels in the unified space without retraining. Additionally, we introduce the Hierarchical Dual Cross-modal Information Disentanglement (H-DCID) approach to extend information separation and alignment to two levels, capturing more cross-modal details. The experiment results demonstrate significant improvements a
&lt;/p&gt;</description></item><item><title>sDBSCAN&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#36827;&#34892;&#39640;&#32500;&#23494;&#24230;&#32858;&#31867;&#30340;&#31639;&#27861;&#65292;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15679</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#30340;&#21487;&#25193;&#23637;&#23494;&#24230;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Scalable Density-based Clustering with Random Projections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15679
&lt;/p&gt;
&lt;p&gt;
sDBSCAN&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#36827;&#34892;&#39640;&#32500;&#23494;&#24230;&#32858;&#31867;&#30340;&#31639;&#27861;&#65292;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;sDBSCAN&#30340;&#31639;&#27861;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20351;&#29992;&#20313;&#24358;&#36317;&#31163;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#23494;&#24230;&#32858;&#31867;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#30340;&#20445;&#37051;&#29305;&#24615;&#65292;sDBSCAN&#33021;&#22815;&#24555;&#36895;&#35782;&#21035;&#26680;&#24515;&#28857;&#21450;&#20854;&#37051;&#22495;&#65292;&#36825;&#26159;&#23494;&#24230;&#32858;&#31867;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;sDBSCAN&#22312;&#36739;&#36731;&#30340;&#26465;&#20214;&#19979;&#20197;&#39640;&#27010;&#29575;&#36755;&#20986;&#31867;&#20284;&#20110;DBSCAN&#30340;&#32858;&#31867;&#32467;&#26500;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20419;&#36827;sDBSCAN&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;sOPTICS&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20132;&#20114;&#24335;&#25506;&#32034;&#20869;&#22312;&#32858;&#31867;&#32467;&#26500;&#30340;&#21487;&#25193;&#23637;OPTICS&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#38543;&#26426;&#26680;&#29305;&#24449;&#23558;sDBSCAN&#21644;sOPTICS&#25193;&#23637;&#21040;L2&#12289;L1&#12289;$\chi^2$&#21644;Jensen-Shannon&#36317;&#31163;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;sDBSCAN&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;&#19978;&#27604;&#35768;&#22810;&#20854;&#20182;&#32858;&#31867;&#31639;&#27861;&#26174;&#33879;&#26356;&#24555;&#65292;&#24182;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#65292;sDBSCAN&#21644;sOPTICS&#22312;&#20960;&#20998;&#38047;&#20869;&#36816;&#34892;&#65292;&#32780;scikit-learn&#30340;&#23545;&#24212;&#31639;&#27861;&#38656;&#35201;&#25968;&#23567;&#26102;&#25110;&#30001;&#20110;&#20869;&#23384;&#19981;&#36275;&#32780;&#26080;&#27861;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15679v1 Announce Type: new  Abstract: We present sDBSCAN, a scalable density-based clustering algorithm in high dimensions with cosine distance. Utilizing the neighborhood-preserving property of random projections, sDBSCAN can quickly identify core points and their neighborhoods, the primary hurdle of density-based clustering. Theoretically, sDBSCAN outputs a clustering structure similar to DBSCAN under mild conditions with high probability. To further facilitate sDBSCAN, we present sOPTICS, a scalable OPTICS for interactive exploration of the intrinsic clustering structure. We also extend sDBSCAN and sOPTICS to L2, L1, $\chi^2$, and Jensen-Shannon distances via random kernel features. Empirically, sDBSCAN is significantly faster and provides higher accuracy than many other clustering algorithms on real-world million-point data sets. On these data sets, sDBSCAN and sOPTICS run in a few minutes, while the scikit-learn's counterparts demand several hours or cannot run due to m
&lt;/p&gt;</description></item><item><title>SVQ&#26159;&#19968;&#31181;&#21033;&#29992;&#31232;&#30095;&#22238;&#24402;&#23454;&#29616;&#31616;&#26126;&#34920;&#31034;&#30340;&#31232;&#30095;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20445;&#30041;&#20851;&#38190;&#32454;&#33410;&#21644;&#28388;&#38500;&#22122;&#22768;&#26469;&#25552;&#39640;&#26102;&#31354;&#39044;&#27979;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;SVQ&#22312;&#20116;&#20010;&#31354;&#38388;-&#26102;&#38388;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.03406</link><description>&lt;p&gt;
SVQ: &#31232;&#30095;&#21521;&#37327;&#37327;&#21270;&#29992;&#20110;&#26102;&#31354;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SVQ: Sparse Vector Quantization for Spatiotemporal Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03406
&lt;/p&gt;
&lt;p&gt;
SVQ&#26159;&#19968;&#31181;&#21033;&#29992;&#31232;&#30095;&#22238;&#24402;&#23454;&#29616;&#31616;&#26126;&#34920;&#31034;&#30340;&#31232;&#30095;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20445;&#30041;&#20851;&#38190;&#32454;&#33410;&#21644;&#28388;&#38500;&#22122;&#22768;&#26469;&#25552;&#39640;&#26102;&#31354;&#39044;&#27979;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;SVQ&#22312;&#20116;&#20010;&#31354;&#38388;-&#26102;&#38388;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#39044;&#27979;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#26159;&#20851;&#38190;&#65292;&#21462;&#24471;&#22909;&#30340;&#39044;&#27979;&#32467;&#26524;&#38656;&#35201;&#25214;&#21040;&#24494;&#22937;&#30340;&#27169;&#24335;&#24182;&#28388;&#38500;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31232;&#30095;&#22238;&#24402;&#21521;&#37327;&#37327;&#21270;&#65288;SVQ&#65289;&#36825;&#19968;&#26032;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#31232;&#30095;&#22238;&#24402;&#26469;&#23454;&#29616;&#31616;&#26126;&#34920;&#31034;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#19978;&#37117;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#32858;&#31867;&#30340;&#21521;&#37327;&#37327;&#21270;&#26041;&#27861;&#26356;&#26377;&#20248;&#21183;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#22238;&#24402;&#27169;&#22411;&#20445;&#30041;&#21407;&#22987;&#21521;&#37327;&#30340;&#20851;&#38190;&#32454;&#33410;&#65292;&#21516;&#26102;&#36890;&#36807;&#31232;&#30095;&#35774;&#35745;&#28388;&#38500;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#23618;MLP&#21644;&#19968;&#20010;&#24191;&#27867;&#30340;&#30721;&#26412;&#26469;&#36817;&#20284;&#31232;&#30095;&#22238;&#24402;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#36824;&#20351;&#24471;SVQ&#20855;&#26377;&#21487;&#24494;&#24615;&#21644;&#35757;&#32451;&#31616;&#26131;&#24615;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#31354;&#38388;-&#26102;&#38388;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;SVQ&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal forecasting, pivotal in numerous fields, hinges on the delicate equilibrium between isolating nuanced patterns and sifting out noise. To tackle this, we introduce Sparse Regression-based Vector Quantization (SVQ), a novel technique that leverages sparse regression for succinct representation, an approach theoretically and practically favored over classical clustering-based vector quantization methods. This approach preserves critical details from the original vectors using a regression model while filtering out noise via sparse design. Moreover, we approximate the sparse regression process using a blend of a two-layer MLP and an extensive codebook. This approach not only substantially cuts down on computational costs but also grants SVQ differentiability and training simplicity, resulting in a notable enhancement of performance. Our empirical studies on five spatial-temporal benchmark datasets demonstrate that SVQ achieves state-of-the-art results. Specifically, on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;&#26469;&#20445;&#25252;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65292;&#33021;&#22815;&#38450;&#24481;&#23616;&#37096;&#25200;&#21160;&#20026;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#19988;&#33021;&#22815;&#22312;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.07992</link><description>&lt;p&gt;
&#23433;&#20840;&#30340;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Securing Visually-Aware Recommender Systems: An Adversarial Image Reconstruction and Detection Framework. (arXiv:2306.07992v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;&#26469;&#20445;&#25252;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65292;&#33021;&#22815;&#38450;&#24481;&#23616;&#37096;&#25200;&#21160;&#20026;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#19988;&#33021;&#22815;&#22312;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23500;&#21547;&#22270;&#29255;&#31561;&#35270;&#35273;&#25968;&#25454;&#19982;&#29289;&#21697;&#20851;&#32852;&#24230;&#22686;&#21152;&#65292;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65288;VARS&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;VARS&#26131;&#21463;&#21040;&#29289;&#21697;-&#22270;&#20687;&#23545;&#25239;&#25915;&#20987;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#21521;&#19982;&#36825;&#20123;&#29289;&#21697;&#20851;&#32852;&#30340;&#24178;&#20928;&#22270;&#20687;&#28155;&#21152;&#20154;&#31867;&#26080;&#27861;&#24863;&#30693;&#30340;&#25200;&#21160;&#12290;&#23545;VARS&#30340;&#25915;&#20987;&#20026;&#24191;&#27867;&#20351;&#29992;VARS&#30340;&#35768;&#22810;&#24212;&#29992;&#65288;&#22914;&#30005;&#23376;&#21830;&#21153;&#21644;&#31038;&#20132;&#32593;&#32476;&#65289;&#24102;&#26469;&#26032;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;&#22914;&#20309;&#20445;&#25252;VARS&#20813;&#21463;&#27492;&#31867;&#23545;&#25239;&#25915;&#20987;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;&#23578;&#32570;&#20047;&#31995;&#32479;&#22320;&#30740;&#31350;&#22914;&#20309;&#35774;&#35745;&#38024;&#23545;VARS&#35270;&#35273;&#25915;&#20987;&#30340;&#23433;&#20840;&#38450;&#24481;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;&#26469;&#20445;&#25252;VARS&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;(1)&#36890;&#36807;&#22522;&#20110;&#20840;&#23616;&#35270;&#35273;&#20256;&#36755;&#30340;&#22270;&#20687;&#37325;&#26500;&#26469;&#38450;&#24481;&#20197;&#23616;&#37096;&#25200;&#21160;&#20026;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;(2)&#20351;&#29992;&#22312;&#23569;&#37327;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#26816;&#27979;&#27169;&#22411;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#24481;&#21508;&#31181;&#29289;&#21697;-&#22270;&#20687;&#23545;&#25239;&#25915;&#20987;&#23545;VARS&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With rich visual data, such as images, becoming readily associated with items, visually-aware recommendation systems (VARS) have been widely used in different applications. Recent studies have shown that VARS are vulnerable to item-image adversarial attacks, which add human-imperceptible perturbations to the clean images associated with those items. Attacks on VARS pose new security challenges to a wide range of applications such as e-Commerce and social networks where VARS are widely used. How to secure VARS from such adversarial attacks becomes a critical problem. Currently, there is still a lack of systematic study on how to design secure defense strategies against visual attacks on VARS. In this paper, we attempt to fill this gap by proposing an adversarial image reconstruction and detection framework to secure VARS. Our proposed method can simultaneously (1) secure VARS from adversarial attacks characterized by local perturbations by image reconstruction based on global vision tra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;API&#30340;&#26041;&#27861;&#29983;&#25104;&#23494;&#20999;&#31867;&#20284;&#20110;&#21407;&#22987;&#31169;&#26377;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#37096;&#32626;&#12290;&#20351;&#29992;Private Evolution&#65288;PE&#65289;&#26694;&#26550;&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#65292;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#36827;&#21270;&#31639;&#27861;&#21644;&#20803;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#26082;&#20026;DP&#21448;&#19982;&#21407;&#22987;&#22270;&#20687;&#22806;&#35266;&#30456;&#20284;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#22312;&#27969;&#34892;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.15560</link><description>&lt;p&gt;
&#22522;&#20110; Foundation Model APIs &#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#65306;&#22270;&#29255;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Synthetic Data via Foundation Model APIs 1: Images. (arXiv:2305.15560v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;API&#30340;&#26041;&#27861;&#29983;&#25104;&#23494;&#20999;&#31867;&#20284;&#20110;&#21407;&#22987;&#31169;&#26377;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#37096;&#32626;&#12290;&#20351;&#29992;Private Evolution&#65288;PE&#65289;&#26694;&#26550;&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#65292;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#36827;&#21270;&#31639;&#27861;&#21644;&#20803;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#26082;&#20026;DP&#21448;&#19982;&#21407;&#22987;&#22270;&#20687;&#22806;&#35266;&#30456;&#20284;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#22312;&#27969;&#34892;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#25968;&#25454;&#39537;&#21160;&#30340;&#19990;&#30028;&#20013;&#65292;&#29983;&#25104;&#23494;&#20999;&#31867;&#20284;&#20110;&#21407;&#22987;&#31169;&#26377;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21512;&#25104;&#25968;&#25454;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21487;&#20943;&#36731;&#38544;&#31169;&#38382;&#39064;&#12290;&#19982;&#24403;&#21069;&#20026;&#27492;&#20219;&#21153;&#35757;&#32451;&#23450;&#21046;&#27169;&#22411;&#30340;&#20570;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;API&#29983;&#25104;DP&#21512;&#25104;&#25968;&#25454;&#65288;DPSDA&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#23558;&#22522;&#30784;&#27169;&#22411;&#35270;&#20026;&#40657;&#30418;&#24182;&#21482;&#21033;&#29992;&#20854;&#25512;&#29702;API&#12290;&#36825;&#20123;&#22522;&#20110;API&#30340;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#26356;&#23481;&#26131;&#37096;&#32626;&#65292;&#22914;&#26368;&#36817; API &#24212;&#29992;&#31243;&#24207;&#30340;&#28608;&#22686;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#12290;&#36825;&#20123;&#26041;&#27861;&#36824;&#21487;&#20197;&#21033;&#29992;&#21487;&#36890;&#36807;&#20854;&#25512;&#29702;API&#35775;&#38382;&#20854;&#26435;&#37325;&#26410;&#21457;&#24067;&#30340;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#27169;&#22411;&#35775;&#38382;&#26356;&#21152;&#20005;&#26684;&#65292;&#36824;&#38656;&#20445;&#25252;API&#25552;&#20379;&#21830;&#30340;&#38544;&#31169;&#65292;&#36825;&#23558;&#24102;&#26469;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026; Private Evolution&#65288;PE&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;API&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#26041;&#38754;&#30340;&#21021;&#22987;&#23454;&#29616;&#12290;PE&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#36827;&#21270;&#31639;&#27861;&#21644;&#20803;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#26082;&#20026;DP&#21448;&#19982;&#21407;&#22987;&#22270;&#20687;&#22806;&#35266;&#30456;&#20284;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#22312;&#27969;&#34892;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#22914;CIFAR-10&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#29992;&#21644;&#38544;&#31169;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;DP&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating differentially private (DP) synthetic data that closely resembles the original private data without leaking sensitive user information is a scalable way to mitigate privacy concerns in the current data-driven world. In contrast to current practices that train customized models for this task, we aim to generate DP Synthetic Data via APIs (DPSDA), where we treat foundation models as blackboxes and only utilize their inference APIs. Such API-based, training-free approaches are easier to deploy as exemplified by the recent surge in the number of API-based apps. These approaches can also leverage the power of large foundation models which are accessible via their inference APIs while the model weights are unreleased. However, this comes with greater challenges due to strictly more restrictive model access and the additional need to protect privacy from the API provider.  In this paper, we present a new framework called Private Evolution (PE) to solve this problem and show its ini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;AUTO&#30340;&#26041;&#27861;&#65292;&#22312;&#22312;&#32447;&#27979;&#35797;&#26102;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#22312;&#32447;&#25968;&#25454;&#30452;&#25509;&#25552;&#39640;OOD&#26816;&#27979;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#24182;&#22312;&#32447;&#26816;&#27979;OOD&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.12267</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#31163;&#32676;&#20540;&#20248;&#21270;&#65306;&#29992;&#20110;&#22312;&#32447;&#27979;&#35797;&#26102;OOD&#26816;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AUTO: Adaptive Outlier Optimization for Online Test-Time OOD Detection. (arXiv:2303.12267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;AUTO&#30340;&#26041;&#27861;&#65292;&#22312;&#22312;&#32447;&#27979;&#35797;&#26102;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#22312;&#32447;&#25968;&#25454;&#30452;&#25509;&#25552;&#39640;OOD&#26816;&#27979;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#24182;&#22312;&#32447;&#26816;&#27979;OOD&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;OOD&#65288;out-of-distribution&#65289;&#26816;&#27979;&#26159;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#36741;&#21161;&#31163;&#32676;&#20540;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;OOD&#26816;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31163;&#32676;&#20540;&#36890;&#24120;&#19982;&#27979;&#35797;OOD&#25968;&#25454;&#23384;&#22312;&#20998;&#24067;&#24046;&#36317;&#65292;&#24182;&#19988;&#19981;&#33021;&#35206;&#30422;&#25152;&#26377;&#21487;&#33021;&#30340;&#27979;&#35797;OOD&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#32467;&#21512;&#36825;&#20123;&#31163;&#32676;&#20540;&#36824;&#20250;&#22686;&#21152;&#35757;&#32451;&#30340;&#36127;&#25285;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#27979;&#35797;&#26102;OOD&#26816;&#27979;&#30340;&#26032;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;&#30452;&#25509;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#22312;&#32447;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;OOD&#26816;&#27979;&#24615;&#33021;&#12290;&#34429;&#28982;&#36825;&#31181;&#33539;&#24335;&#24456;&#39640;&#25928;&#65292;&#20294;&#23427;&#20063;&#38754;&#20020;&#30528;&#35832;&#22914;&#28798;&#38590;&#24615;&#36951;&#24536;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#31163;&#32676;&#20540;&#20248;&#21270;&#65288;AUTO&#65289;&#65292;&#23427;&#30001;&#20869;&#22806;&#24863;&#30693;&#28388;&#27874;&#22120;&#12289;ID&#23384;&#20648;&#22120;&#21644;&#35821;&#20041;&#19968;&#33268;&#30340;&#30446;&#26631;&#32452;&#25104;&#12290;AUTO&#33258;&#36866;&#24212;&#22320;&#20174;&#27979;&#35797;&#25968;&#25454;&#20013;&#25366;&#25496;&#20266;ID&#21644;&#20266;OOD&#26679;&#26412;&#65292;&#21033;&#29992;&#23427;&#20204;&#26469;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#24182;&#22312;&#32447;&#26816;&#27979;OOD&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AUTO&#22312;&#21508;&#31181;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is a crucial aspect of deploying machine learning models in open-world applications. Empirical evidence suggests that training with auxiliary outliers substantially improves OOD detection. However, such outliers typically exhibit a distribution gap compared to the test OOD data and do not cover all possible test OOD scenarios. Additionally, incorporating these outliers introduces additional training burdens. In this paper, we introduce a novel paradigm called test-time OOD detection, which utilizes unlabeled online data directly at test time to improve OOD detection performance. While this paradigm is efficient, it also presents challenges such as catastrophic forgetting. To address these challenges, we propose adaptive outlier optimization (AUTO), which consists of an in-out-aware filter, an ID memory bank, and a semantically-consistent objective. AUTO adaptively mines pseudo-ID and pseudo-OOD samples from test data, utilizing them to optimize netwo
&lt;/p&gt;</description></item><item><title>DPM-Solver++&#26159;&#19968;&#31181;&#24555;&#36895;&#27714;&#35299;&#22120;&#65292;&#22312;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#24341;&#23548;&#37319;&#26679;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#21152;&#24555;&#26679;&#26412;&#29983;&#25104;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.01095</link><description>&lt;p&gt;
DPM-Solver++&#65306;&#29992;&#20110;&#24341;&#23548;&#37319;&#26679;&#30340;&#24555;&#36895;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models. (arXiv:2211.01095v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01095
&lt;/p&gt;
&lt;p&gt;
DPM-Solver++&#26159;&#19968;&#31181;&#24555;&#36895;&#27714;&#35299;&#22120;&#65292;&#22312;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#24341;&#23548;&#37319;&#26679;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#21152;&#24555;&#26679;&#26412;&#29983;&#25104;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411; (DPMs) &#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21512;&#25104;&#31561;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780; DPM &#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#25152;&#38656;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#26159;&#24341;&#23548;&#37319;&#26679;&#12290;&#29616;&#26377;&#30340;&#24555;&#36895;&#37319;&#26679;&#22120; DDIM &#22312;&#24341;&#23548;&#37319;&#26679;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#38656;&#35201; 100 &#33267; 250 &#27493;&#25165;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; DPM-Solver++&#65292;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#24341;&#23548;&#37319;&#26679;&#30340;&#39640;&#38454;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models (DPMs) have achieved impressive success in high-resolution image synthesis, especially in recent large-scale text-to-image generation applications. An essential technique for improving the sample quality of DPMs is guided sampling, which usually needs a large guidance scale to obtain the best sample quality. The commonly-used fast sampler for guided sampling is DDIM, a first-order diffusion ODE solver that generally needs 100 to 250 steps for high-quality samples. Although recent works propose dedicated high-order solvers and achieve a further speedup for sampling without guidance, their effectiveness for guided sampling has not been well-tested before. In this work, we demonstrate that previous high-order fast samplers suffer from instability issues, and they even become slower than DDIM when the guidance scale grows large. To further speed up guided sampling, we propose DPM-Solver++, a high-order solver for the guided sampling of DPMs. DPM-Solver++ solv
&lt;/p&gt;</description></item></channel></rss>