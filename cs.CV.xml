<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#22270;&#20687;&#35270;&#35273;&#24120;&#35782;&#21457;&#29616;&#65288;VCD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#24120;&#35782;&#31867;&#22411;&#20197;&#21450;&#26500;&#24314;&#21253;&#25324;&#36229;&#36807;10&#19975;&#24352;&#22270;&#20687;&#21644;1400&#19975;&#20010;&#23545;&#35937;-&#24120;&#35782;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#21319;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17213</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#22270;&#20687;&#35270;&#35273;&#24120;&#35782;&#21457;&#29616;&#65288;VCD&#65289;
&lt;/p&gt;
&lt;p&gt;
VCD: Knowledge Base Guided Visual Commonsense Discovery in Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17213
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#22270;&#20687;&#35270;&#35273;&#24120;&#35782;&#21457;&#29616;&#65288;VCD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#24120;&#35782;&#31867;&#22411;&#20197;&#21450;&#26500;&#24314;&#21253;&#25324;&#36229;&#36807;10&#19975;&#24352;&#22270;&#20687;&#21644;1400&#19975;&#20010;&#23545;&#35937;-&#24120;&#35782;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#21319;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20013;&#30340;&#35270;&#35273;&#24120;&#35782;&#21253;&#21547;&#26377;&#20851;&#23545;&#35937;&#23646;&#24615;&#12289;&#20851;&#31995;&#21644;&#34892;&#20026;&#30340;&#30693;&#35782;&#12290;&#21457;&#29616;&#35270;&#35273;&#24120;&#35782;&#21487;&#20197;&#25552;&#20379;&#23545;&#22270;&#20687;&#30340;&#26356;&#20840;&#38754;&#21644;&#20016;&#23500;&#30340;&#29702;&#35299;&#65292;&#24182;&#22686;&#24378;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35270;&#35273;&#24120;&#35782;&#21457;&#29616;&#30740;&#31350;&#20013;&#25152;&#23450;&#20041;&#30340;&#35270;&#35273;&#24120;&#35782;&#26159;&#31895;&#31890;&#24230;&#19988;&#19981;&#23436;&#25972;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24120;&#35782;&#30693;&#35782;&#24211;ConceptNet&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24182;&#31995;&#32479;&#22320;&#23450;&#20041;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#35270;&#35273;&#24120;&#35782;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#35270;&#35273;&#24120;&#35782;&#21457;&#29616;&#65288;VCD&#65289;&#65292;&#26088;&#22312;&#25552;&#21462;&#22270;&#20687;&#20013;&#19981;&#21516;&#23545;&#35937;&#25152;&#21253;&#21547;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#32454;&#31890;&#24230;&#24120;&#35782;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20174;Visual Genome&#21644;ConceptNet&#20013;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;VCDD&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;10&#19975;&#24352;&#22270;&#20687;&#21644;1400&#19975;&#20010;&#23545;&#35937;-&#24120;&#35782;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17213v1 Announce Type: cross  Abstract: Visual commonsense contains knowledge about object properties, relationships, and behaviors in visual data. Discovering visual commonsense can provide a more comprehensive and richer understanding of images, and enhance the reasoning and decision-making capabilities of computer vision systems. However, the visual commonsense defined in existing visual commonsense discovery studies is coarse-grained and incomplete. In this work, we draw inspiration from a commonsense knowledge base ConceptNet in natural language processing, and systematically define the types of visual commonsense. Based on this, we introduce a new task, Visual Commonsense Discovery (VCD), aiming to extract fine-grained commonsense of different types contained within different objects in the image. We accordingly construct a dataset (VCDD) from Visual Genome and ConceptNet for VCD, featuring over 100,000 images and 14 million object-commonsense pairs. We furthermore pro
&lt;/p&gt;</description></item><item><title>ViewFusion &#26159;&#19968;&#31181;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#30340;&#26368;&#26032;&#31471;&#21040;&#31471;&#29983;&#25104;&#26041;&#27861;&#65292;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#28789;&#27963;&#24615;&#65292;&#36890;&#36807;&#21516;&#26102;&#24212;&#29992;&#25193;&#25955;&#21435;&#22122;&#21644;&#20687;&#32032;&#21152;&#26435;&#25513;&#27169;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02906</link><description>&lt;p&gt;
ViewFusion: &#23398;&#20064;&#21487;&#32452;&#21512;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
ViewFusion: Learning Composable Diffusion Models for Novel View Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02906
&lt;/p&gt;
&lt;p&gt;
ViewFusion &#26159;&#19968;&#31181;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#30340;&#26368;&#26032;&#31471;&#21040;&#31471;&#29983;&#25104;&#26041;&#27861;&#65292;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#28789;&#27963;&#24615;&#65292;&#36890;&#36807;&#21516;&#26102;&#24212;&#29992;&#25193;&#25955;&#21435;&#22122;&#21644;&#20687;&#32032;&#21152;&#26435;&#25513;&#27169;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20026;&#26032;&#35270;&#35282;&#21512;&#25104;&#36825;&#20010;&#32769;&#38382;&#39064;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#30340;&#26041;&#27861;&#21040;&#31471;&#21040;&#31471;&#30340;&#39118;&#26684;&#26550;&#26500;&#12290;&#27599;&#31181;&#26041;&#27861;&#37117;&#20855;&#26377;&#29305;&#23450;&#30340;&#20248;&#21183;&#65292;&#20294;&#20063;&#20855;&#26377;&#29305;&#23450;&#30340;&#36866;&#29992;&#24615;&#38480;&#21046;&#12290;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;ViewFusion&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#28789;&#27963;&#24615;&#30340;&#26368;&#26032;&#31471;&#21040;&#31471;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#12290;ViewFusion&#21516;&#26102;&#23545;&#22330;&#26223;&#30340;&#20219;&#24847;&#25968;&#37327;&#30340;&#36755;&#20837;&#35270;&#35282;&#24212;&#29992;&#25193;&#25955;&#21435;&#22122;&#27493;&#39588;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#35270;&#35282;&#24471;&#21040;&#30340;&#22122;&#22768;&#26799;&#24230;&#19982;&#65288;&#25512;&#26029;&#24471;&#21040;&#30340;&#65289;&#20687;&#32032;&#21152;&#26435;&#25513;&#27169;&#30456;&#32467;&#21512;&#65292;&#30830;&#20445;&#23545;&#20110;&#30446;&#26631;&#22330;&#26223;&#30340;&#27599;&#20010;&#21306;&#22495;&#65292;&#21482;&#32771;&#34385;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36755;&#20837;&#35270;&#35282;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#35299;&#20915;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#20960;&#20010;&#23616;&#38480;&#24615;&#65306;&#65288;1&#65289;&#21487;&#35757;&#32451;&#19988;&#33021;&#22815;&#27867;&#21270;&#21040;&#22810;&#20010;&#22330;&#26223;&#21644;&#29289;&#20307;&#31867;&#21035;&#65292;&#65288;2&#65289;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#22320;&#37319;&#29992;&#21487;&#21464;&#25968;&#37327;&#30340;&#26080;&#23039;&#24577;&#35270;&#22270;&#65292;&#65288;3&#65289;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is providing a wealth of new approaches to the old problem of novel view synthesis, from Neural Radiance Field (NeRF) based approaches to end-to-end style architectures. Each approach offers specific strengths but also comes with specific limitations in their applicability. This work introduces ViewFusion, a state-of-the-art end-to-end generative approach to novel view synthesis with unparalleled flexibility. ViewFusion consists in simultaneously applying a diffusion denoising step to any number of input views of a scene, then combining the noise gradients obtained for each view with an (inferred) pixel-weighting mask, ensuring that for each region of the target scene only the most informative input views are taken into account. Our approach resolves several limitations of previous approaches by (1) being trainable and generalizing across multiple scenes and object classes, (2) adaptively taking in a variable number of pose-free views at both train and test time, (3) gene
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;AbdGen&#65292;&#29992;&#20110;&#23558;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#19982;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#12290;&#23427;&#35299;&#20915;&#20102;&#31526;&#21495;&#36171;&#20540;&#21644;&#35268;&#21017;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;&#31526;&#21495;&#36171;&#20540;&#65292;&#36890;&#36807;&#23545;&#27604;&#20803;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#31934;&#30830;&#30340;&#35268;&#21017;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.17451</link><description>&lt;p&gt;
&#36890;&#36807;&#29702;&#35299;&#29983;&#25104;&#65306;&#20855;&#26377;&#36923;&#36753;&#31526;&#21495;&#22522;&#30784;&#30340;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Generating by Understanding: Neural Visual Generation with Logical Symbol Groundings. (arXiv:2310.17451v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17451
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;AbdGen&#65292;&#29992;&#20110;&#23558;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#19982;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#12290;&#23427;&#35299;&#20915;&#20102;&#31526;&#21495;&#36171;&#20540;&#21644;&#35268;&#21017;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;&#31526;&#21495;&#36171;&#20540;&#65292;&#36890;&#36807;&#23545;&#27604;&#20803;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#31934;&#30830;&#30340;&#35268;&#21017;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23558;&#20854;&#19982;&#24378;&#22823;&#30340;&#31526;&#21495;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#38598;&#25104;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20027;&#35201;&#25361;&#25112;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#19968;&#20010;&#26159;&#31526;&#21495;&#36171;&#20540;&#65292;&#21363;&#23558;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#22120;&#30340;&#28508;&#22312;&#22240;&#32032;&#19982;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#20013;&#30340;&#26377;&#24847;&#20041;&#30340;&#31526;&#21495;&#36827;&#34892;&#32465;&#23450;&#12290;&#21478;&#19968;&#20010;&#26159;&#35268;&#21017;&#23398;&#20064;&#65292;&#21363;&#23398;&#20064;&#26032;&#30340;&#35268;&#21017;&#65292;&#36825;&#20123;&#35268;&#21017;&#25511;&#21046;&#25968;&#25454;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#31526;&#21495;&#22522;&#30784;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;Abductive Visual Generation (AbdGen)&#65292;&#29992;&#20110;&#22522;&#20110;&#35825;&#23548;&#23398;&#20064;&#26694;&#26550;&#23558;&#36923;&#36753;&#32534;&#31243;&#31995;&#32479;&#19982;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#36215;&#26469;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;&#31526;&#21495;&#36171;&#20540;&#65292;&#24341;&#20837;&#20102;&#37327;&#21270;&#35825;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#32534;&#30721;&#26412;&#20013;&#30340;&#26368;&#36817;&#37051;&#26597;&#25214;&#29983;&#25104;&#35825;&#23548;&#25552;&#26696;&#12290;&#20026;&#20102;&#23454;&#29616;&#31934;&#30830;&#30340;&#35268;&#21017;&#23398;&#20064;&#65292;&#24341;&#20837;&#20102;&#23545;&#27604;&#20803;&#35825;&#23548;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great success of neural visual generative models in recent years, integrating them with strong symbolic knowledge reasoning systems remains a challenging task. The main challenges are two-fold: one is symbol assignment, i.e. bonding latent factors of neural visual generators with meaningful symbols from knowledge reasoning systems. Another is rule learning, i.e. learning new rules, which govern the generative process of the data, to augment the knowledge reasoning systems. To deal with these symbol grounding problems, we propose a neural-symbolic learning approach, Abductive Visual Generation (AbdGen), for integrating logic programming systems with neural visual generative models based on the abductive learning framework. To achieve reliable and efficient symbol assignment, the quantized abduction method is introduced for generating abduction proposals by the nearest-neighbor lookups within semantic codebooks. To achieve precise rule learning, the contrastive meta-abduction
&lt;/p&gt;</description></item></channel></rss>