<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22235;&#38454;&#27573;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;MV&#19968;&#33268;&#30340;&#25193;&#25955;&#36807;&#31243;&#12289;&#21322;&#23450;&#32534;&#31243;&#38382;&#39064;&#35299;&#20915;&#12289;&#38750;&#21018;&#24615;&#23545;&#40784;&#21644;MRF&#38382;&#39064;&#35299;&#20915;&#31561;&#27493;&#39588;&#26469;&#23454;&#29616;&#23545;3D&#32593;&#26684;&#36827;&#34892;&#32441;&#29702;&#36148;&#22270;&#30340;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15559</link><description>&lt;p&gt;
&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#24378;&#21046;&#23454;&#29616;&#23545;3D&#32593;&#26684;&#36827;&#34892;&#32441;&#29702;&#36148;&#22270;&#30340;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
An Optimization Framework to Enforce Multi-View Consistency for Texturing 3D Meshes Using Pre-Trained Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15559
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22235;&#38454;&#27573;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;MV&#19968;&#33268;&#30340;&#25193;&#25955;&#36807;&#31243;&#12289;&#21322;&#23450;&#32534;&#31243;&#38382;&#39064;&#35299;&#20915;&#12289;&#38750;&#21018;&#24615;&#23545;&#40784;&#21644;MRF&#38382;&#39064;&#35299;&#20915;&#31561;&#27493;&#39588;&#26469;&#23454;&#29616;&#23545;3D&#32593;&#26684;&#36827;&#34892;&#32441;&#29702;&#36148;&#22270;&#30340;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23545;3D&#32593;&#26684;&#36827;&#34892;&#32441;&#29702;&#36148;&#22270;&#26102;&#65292;&#30830;&#20445;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#22235;&#20010;&#38454;&#27573;&#23454;&#29616;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;MV&#19968;&#33268;&#30340;&#25193;&#25955;&#36807;&#31243;&#20174;&#39044;&#23450;&#20041;&#30340;&#35270;&#28857;&#38598;&#29983;&#25104;2D&#32441;&#29702;&#30340;&#36807;&#23436;&#22791;&#38598;&#12290;&#31532;&#20108;&#38454;&#27573;&#36890;&#36807;&#35299;&#20915;&#21322;&#23450;&#32534;&#31243;&#38382;&#39064;&#36873;&#25321;&#30456;&#20114;&#19968;&#33268;&#19988;&#35206;&#30422;&#22522;&#30784;3D&#27169;&#22411;&#30340;&#35270;&#22270;&#23376;&#38598;&#12290;&#31532;&#19977;&#38454;&#27573;&#25191;&#34892;&#38750;&#21018;&#24615;&#23545;&#40784;&#65292;&#20351;&#36873;&#23450;&#30340;&#35270;&#22270;&#22312;&#37325;&#21472;&#21306;&#22495;&#23545;&#40784;&#12290;&#31532;&#22235;&#38454;&#27573;&#35299;&#20915;MRF&#38382;&#39064;&#20197;&#20851;&#32852;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15559v1 Announce Type: cross  Abstract: A fundamental problem in the texturing of 3D meshes using pre-trained text-to-image models is to ensure multi-view consistency. State-of-the-art approaches typically use diffusion models to aggregate multi-view inputs, where common issues are the blurriness caused by the averaging operation in the aggregation step or inconsistencies in local features. This paper introduces an optimization framework that proceeds in four stages to achieve multi-view consistency. Specifically, the first stage generates an over-complete set of 2D textures from a predefined set of viewpoints using an MV-consistent diffusion process. The second stage selects a subset of views that are mutually consistent while covering the underlying 3D model. We show how to achieve this goal by solving semi-definite programs. The third stage performs non-rigid alignment to align the selected views across overlapping regions. The fourth stage solves an MRF problem to associ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;&#20102;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;CATM&#21644;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;GSL&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09786</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Adaptive Self-training Framework for Fine-grained Scene Graph Generation. (arXiv:2401.09786v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;&#20102;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;CATM&#21644;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;GSL&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;SGG&#65289;&#27169;&#22411;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#38271;&#23614;&#35859;&#35789;&#20998;&#24067;&#21644;&#32570;&#22833;&#27880;&#37322;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;SGG&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#35757;&#32451;SGG&#65288;ST-SGG&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#20026;&#20854;&#20998;&#37197;&#20266;&#26631;&#31614;&#20197;&#35757;&#32451;SGG&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#22270;&#20687;&#35782;&#21035;&#26041;&#38754;&#30340;&#33258;&#35757;&#32451;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#35774;&#35745;&#36866;&#29992;&#20110;SGG&#20219;&#21153;&#30340;&#33258;&#35757;&#32451;&#26694;&#26550;&#26356;&#20855;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#22266;&#26377;&#29305;&#24615;&#65292;&#22914;&#35821;&#20041;&#27495;&#20041;&#21644;&#38271;&#23614;&#20998;&#24067;&#30340;&#35859;&#35789;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SGG&#20266;&#26631;&#31614;&#25216;&#26415;&#65292;&#31216;&#20026;&#20855;&#26377;&#21160;&#37327;&#30340;&#31867;&#21035;&#33258;&#36866;&#24212;&#38408;&#20540;&#21270;&#65288;CATM&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#29420;&#31435;&#20110;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#24050;&#26377;&#30340;SGG&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;&#65288;GSL&#65289;&#65292;&#20174;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scene graph generation (SGG) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. In this work, we aim to alleviate the long-tailed problem of SGG by utilizing unannotated triplets. To this end, we introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels for unannotated triplets based on which the SGG models are trained. While there has been significant progress in self-training for image recognition, designing a self-training framework for the SGG task is more challenging due to its inherent nature such as the semantic ambiguity and the long-tailed distribution of predicate classes. Hence, we propose a novel pseudo-labeling technique for SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is a model-agnostic framework that can be applied to any existing SGG models. Furthermore, we devise a graph structure learner (GSL) that is benefici
&lt;/p&gt;</description></item><item><title>SynthoGestures&#26159;&#19968;&#31181;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;&#21512;&#25104;&#36924;&#30495;&#25163;&#21183;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#21160;&#24577;&#20154;&#26426;&#30028;&#38754;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#29983;&#25104;&#22810;&#31181;&#21464;&#20307;&#21644;&#27169;&#25311;&#19981;&#21516;&#25668;&#20687;&#26426;&#31867;&#22411;&#65292;&#25552;&#39640;&#20102;&#25163;&#21183;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#24182;&#33410;&#30465;&#20102;&#25968;&#25454;&#38598;&#21019;&#24314;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04421</link><description>&lt;p&gt;
SynthoGestures&#65306;&#19968;&#31181;&#29992;&#20110;&#39550;&#39542;&#22330;&#26223;&#30340;&#21512;&#25104;&#21160;&#24577;&#25163;&#21183;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SynthoGestures: A Novel Framework for Synthetic Dynamic Hand Gesture Generation for Driving Scenarios. (arXiv:2309.04421v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04421
&lt;/p&gt;
&lt;p&gt;
SynthoGestures&#26159;&#19968;&#31181;&#20351;&#29992;&#34394;&#24187;&#24341;&#25806;&#21512;&#25104;&#36924;&#30495;&#25163;&#21183;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#21160;&#24577;&#20154;&#26426;&#30028;&#38754;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#29983;&#25104;&#22810;&#31181;&#21464;&#20307;&#21644;&#27169;&#25311;&#19981;&#21516;&#25668;&#20687;&#26426;&#31867;&#22411;&#65292;&#25552;&#39640;&#20102;&#25163;&#21183;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#24182;&#33410;&#30465;&#20102;&#25968;&#25454;&#38598;&#21019;&#24314;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27773;&#36710;&#39046;&#22495;&#20013;&#65292;&#20026;&#21160;&#24577;&#20154;&#26426;&#30028;&#38754;&#21019;&#24314;&#22810;&#26679;&#21270;&#21644;&#20840;&#38754;&#30340;&#25163;&#21183;&#25968;&#25454;&#38598;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#34394;&#25311;3D&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25163;&#21183;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#34394;&#24187;&#24341;&#25806;&#21512;&#25104;&#36924;&#30495;&#30340;&#25163;&#21183;&#65292;&#25552;&#20379;&#23450;&#21046;&#36873;&#39033;&#24182;&#38477;&#20302;&#36807;&#25311;&#21512;&#39118;&#38505;&#12290;&#29983;&#25104;&#22810;&#31181;&#21464;&#20307;&#65292;&#21253;&#25324;&#25163;&#21183;&#36895;&#24230;&#12289;&#24615;&#33021;&#21644;&#25163;&#24418;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27169;&#25311;&#19981;&#21516;&#30340;&#25668;&#20687;&#26426;&#20301;&#32622;&#21644;&#31867;&#22411;&#65292;&#22914;RGB&#12289;&#32418;&#22806;&#21644;&#28145;&#24230;&#25668;&#20687;&#26426;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#26102;&#38388;&#21644;&#36153;&#29992;&#33719;&#21462;&#36825;&#20123;&#25668;&#20687;&#26426;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#26694;&#26550;SynthoGestures&#25552;&#39640;&#20102;&#25163;&#21183;&#35782;&#21035;&#20934;&#30830;&#29575;&#65292;&#21487;&#20197;&#26367;&#20195;&#25110;&#22686;&#24378;&#30495;&#25163;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#33410;&#30465;&#25968;&#25454;&#38598;&#21019;&#24314;&#30340;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#25105;&#20204;&#30340;&#24037;&#20855;&#20419;&#36827;&#20102;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating a diverse and comprehensive dataset of hand gestures for dynamic human-machine interfaces in the automotive domain can be challenging and time-consuming. To overcome this challenge, we propose using synthetic gesture datasets generated by virtual 3D models. Our framework utilizes Unreal Engine to synthesize realistic hand gestures, offering customization options and reducing the risk of overfitting. Multiple variants, including gesture speed, performance, and hand shape, are generated to improve generalizability. In addition, we simulate different camera locations and types, such as RGB, infrared, and depth cameras, without incurring additional time and cost to obtain these cameras. Experimental results demonstrate that our proposed framework, SynthoGestures\footnote{\url{https://github.com/amrgomaaelhady/SynthoGestures}}, improves gesture recognition accuracy and can replace or augment real-hand datasets. By saving time and effort in the creation of the data set, our tool acc
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#36741;&#21161;&#32593;&#32476;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#34920;&#31034;&#26469;&#35757;&#32451;&#26356;&#22909;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#65292;&#35299;&#20915;&#20102;&#25512;&#26029;&#36807;&#31243;&#20013;&#27169;&#24577;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03452</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#36741;&#21161;&#32593;&#32476;&#29992;&#20110;&#25512;&#26029;&#32570;&#22833;&#27169;&#24577;
&lt;/p&gt;
&lt;p&gt;
Multi-Modality Guidance Network For Missing Modality Inference. (arXiv:2309.03452v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03452
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#36741;&#21161;&#32593;&#32476;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#34920;&#31034;&#26469;&#35757;&#32451;&#26356;&#22909;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#65292;&#35299;&#20915;&#20102;&#25512;&#26029;&#36807;&#31243;&#20013;&#27169;&#24577;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#26368;&#36817;&#20960;&#24180;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#26631;&#20934;&#22810;&#27169;&#24577;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#22312;&#35757;&#32451;&#38454;&#27573;&#21644;&#25512;&#26029;&#38454;&#27573;&#27169;&#24577;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;&#35768;&#22810;&#22330;&#26223;&#26080;&#27861;&#28385;&#36275;&#36825;&#26679;&#30340;&#20551;&#35774;&#65292;&#25512;&#26029;&#36807;&#31243;&#20013;&#20250;&#20986;&#29616;&#32570;&#22833;&#27169;&#24577;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#37325;&#24314;&#32570;&#22833;&#27169;&#24577;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#22686;&#21152;&#20102;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#23588;&#20854;&#23545;&#20110;&#22823;&#22411;&#37096;&#32626;&#31995;&#32479;&#32780;&#35328;&#21487;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#20174;&#20004;&#20010;&#26041;&#38754;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#32593;&#32476;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#34920;&#31034;&#26469;&#35757;&#32451;&#26356;&#22909;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#12290;&#22312;&#26292;&#21147;&#26816;&#27979;&#30340;&#29616;&#23454;&#29983;&#27963;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#25512;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal models have gained significant success in recent years. Standard multimodal approaches often assume unchanged modalities from training stage to inference stage. In practice, however, many scenarios fail to satisfy such assumptions with missing modalities during inference, leading to limitations on where multimodal models can be applied. While existing methods mitigate the problem through reconstructing the missing modalities, it increases unnecessary computational cost, which could be just as critical, especially for large, deployed systems. To solve the problem from both sides, we propose a novel guidance network that promotes knowledge sharing during training, taking advantage of the multimodal representations to train better single-modality models for inference. Real-life experiment in violence detection shows that our proposed framework trains single-modality models that significantly outperform its traditionally trained counterparts while maintaining the same inference 
&lt;/p&gt;</description></item></channel></rss>