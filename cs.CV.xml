<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25972;&#21512;&#27491;&#24358;&#20989;&#25968;&#21040;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19243</link><description>&lt;p&gt;
&#29992;&#27491;&#24358;&#28608;&#27963;&#30340;&#20302;&#31209;&#30697;&#38453;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sine Activated Low-Rank Matrices for Parameter Efficient Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19243
&lt;/p&gt;
&lt;p&gt;
&#25972;&#21512;&#27491;&#24358;&#20989;&#25968;&#21040;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#20998;&#35299;&#24050;&#32463;&#25104;&#20026;&#22312;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#22686;&#24378;&#21442;&#25968;&#25928;&#29575;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#20123;&#25216;&#26415;&#26174;&#33879;&#38477;&#20302;&#20102;&#21442;&#25968;&#25968;&#37327;&#65292;&#21462;&#24471;&#20102;&#31616;&#27905;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#26159;&#22312;&#21442;&#25968;&#25928;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#20570;&#20986;&#22949;&#21327;&#65292;&#21442;&#25968;&#20943;&#23569;&#24448;&#24448;&#23548;&#33268;&#20934;&#30830;&#24615;&#19981;&#21450;&#23436;&#25972;&#31209;&#23545;&#24212;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#22312;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#25972;&#21512;&#20102;&#19968;&#20010;&#27491;&#24358;&#20989;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20445;&#30041;&#20102;&#20302;&#31209;&#26041;&#27861;&#30340;&#21442;&#25968;&#25928;&#29575;&#29305;&#24615;&#30340;&#22909;&#22788;&#65292;&#36824;&#22686;&#21152;&#20102;&#20998;&#35299;&#30340;&#31209;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#29616;&#26377;&#20302;&#31209;&#27169;&#22411;&#30340;&#19968;&#31181;&#36866;&#24212;&#24615;&#22686;&#24378;&#65292;&#27491;&#22914;&#20854;&#25104;&#21151;&#35777;&#23454;&#30340;&#37027;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19243v1 Announce Type: new  Abstract: Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and performance. However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts. In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process. This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition's rank, thereby enhancing model accuracy. Our method proves to be an adaptable enhancement for existing low-rank models, as evidenced by its successful 
&lt;/p&gt;</description></item><item><title>DreamArtist&#37319;&#29992;&#27491;&#36127;prompt-tuning&#23398;&#20064;&#31574;&#30053;&#26469;&#29983;&#25104;&#21487;&#25511;&#30340;&#19968;&#27425;&#24615;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.11337</link><description>&lt;p&gt;
DreamArtist: &#36890;&#36807;&#23545;&#27604;prompt-tuning&#23454;&#29616;&#21487;&#25511;&#30340;&#19968;&#27425;&#24615;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DreamArtist: Towards Controllable One-Shot Text-to-Image Generation via Contrastive Prompt-Tuning. (arXiv:2211.11337v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11337
&lt;/p&gt;
&lt;p&gt;
DreamArtist&#37319;&#29992;&#27491;&#36127;prompt-tuning&#23398;&#20064;&#31574;&#30053;&#26469;&#29983;&#25104;&#21487;&#25511;&#30340;&#19968;&#27425;&#24615;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#25991;&#26412;&#25351;&#23548;&#21512;&#25104;&#39640;&#36136;&#37327;&#12289;&#29305;&#24449;&#20016;&#23500;&#12289;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#26032;&#27010;&#24565;&#65288;&#20363;&#22914;&#26032;&#39118;&#26684;&#12289;&#29289;&#20307;&#23454;&#20307;&#31561;&#65289;&#26102;&#24120;&#24120;&#38754;&#20020;&#22256;&#38590;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#23581;&#35797;&#37319;&#29992;&#24494;&#35843;&#25110;prompt-tuning&#31574;&#30053;&#26469;&#25945;&#25480;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20174;&#21442;&#32771;&#22270;&#20687;&#38598;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#36807;&#24230;&#25311;&#21512;&#32473;&#23450;&#30340;&#21442;&#32771;&#22270;&#20687;&#65292;&#29305;&#21035;&#26159;&#22312;&#21333;&#27425;&#24212;&#29992;&#20013;&#65292;&#36825;&#23545;&#20110;&#20445;&#25345;&#29983;&#25104;&#21487;&#25511;&#24615;&#24182;&#20135;&#29983;&#22810;&#26679;&#21270;&#12289;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;DreamArtist&#65292;&#23427;&#37319;&#29992;&#20102;&#27491;&#36127;prompt-tuning&#23398;&#20064;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DreamArtist&#32467;&#21512;&#20102;&#27491;&#36127;&#23884;&#20837;&#24182;&#32852;&#21512;&#35757;&#32451;&#23427;&#20204;&#12290;&#27491;&#23884;&#20837;&#31215;&#26497;&#22320;&#25429;&#25417;&#21442;&#32771;&#22270;&#20687;&#30340;&#26174;&#30528;&#29305;&#24449;&#26469;&#39537;&#21160;&#22270;&#20687;&#29983;&#25104;&#65292;&#32780;&#36127;&#23884;&#20837;&#21017;&#24378;&#21046;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#24615;&#22270;&#20687;&#20197;&#38477;&#20302;&#36807;&#24230;&#25311;&#21512;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale text-to-image generation models have achieved remarkable progress in synthesizing high-quality, feature-rich images with high resolution guided by texts. However, these models often struggle with novel concepts, eg, new styles, object entities, etc. Although recent attempts have employed fine-tuning or prompt-tuning strategies to teach the pre-trained diffusion model novel concepts from a reference image set,they have the drawback of overfitting to the given reference images, particularly in one-shot applications, which is harmful to generate diverse and high-quality images while maintaining generation controllability.  To tackle this challenge, we present a simple yet effective method called DreamArtist, which employs a positive-negative prompt-tuning learning strategy. Specifically, DreamArtist incorporates both positive and negative embeddings and jointly trains them. The positive embedding aggressively captures the salient characteristics of the reference image to drive
&lt;/p&gt;</description></item></channel></rss>