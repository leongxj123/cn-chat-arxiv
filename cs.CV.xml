<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2402.17376</link><description>&lt;p&gt;
&#20248;&#21270;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Accelerating Diffusion Sampling with Optimized Time Steps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17376
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21512;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#37319;&#26679;&#27493;&#39588;&#65292;&#20854;&#37319;&#26679;&#25928;&#29575;&#20173;&#26377;&#24453;&#25552;&#39640;&#12290;&#36817;&#26399;&#39640;&#38454;&#25968;&#20540;ODE&#27714;&#35299;&#22120;&#22312;DPMs&#20013;&#30340;&#24212;&#29992;&#20351;&#24471;&#29992;&#26356;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#36825;&#26159;&#19968;&#39033;&#37325;&#22823;&#36827;&#23637;&#65292;&#22823;&#22810;&#25968;&#37319;&#26679;&#26041;&#27861;&#20173;&#28982;&#37319;&#29992;&#22343;&#21248;&#26102;&#38388;&#27493;&#38271;&#65292;&#32780;&#22312;&#37319;&#26679;&#27493;&#39588;&#36739;&#23569;&#26102;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#20248;&#21270;&#38382;&#39064;&#26088;&#22312;&#20026;DPMs&#30340;&#29305;&#23450;&#25968;&#20540;ODE&#27714;&#35299;&#22120;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;&#27492;&#20248;&#21270;&#38382;&#39064;&#26088;&#22312;&#26368;&#23567;&#21270;&#22320;&#23454;&#29616;&#22320;&#30495;&#23454;&#35299;&#19982;&#19982;&#25968;&#20540;&#27714;&#35299;&#22120;&#23545;&#24212;&#30340;&#36817;&#20284;&#35299;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#21463;&#38480;&#20449;&#36182;&#22495;&#26041;&#27861;&#36827;&#34892;&#39640;&#25928;&#27714;&#35299;&#65292;&#26102;&#38388;&#23569;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17376v1 Announce Type: cross  Abstract: Diffusion probabilistic models (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps. Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps. While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps. To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs. This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver. It can be efficiently solved using the constrained trust region method, taking less than 
&lt;/p&gt;</description></item><item><title>&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#23545;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#20449;&#20219;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30340;&#26041;&#24335;&#65292;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#22312;&#28385;&#36275;&#33258;&#21160;&#39550;&#39542;&#35201;&#27714;&#26041;&#38754;&#30340;&#20851;&#38190;&#36129;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#35774;&#35745;&#12289;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#12289;&#21487;&#35299;&#37322;&#30340;&#30417;&#25511;&#12289;&#36741;&#21161;&#25216;&#26415;&#21644;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#31561;&#20116;&#20010;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.10086</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#23433;&#20840;&#21487;&#20449;&#30340;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#35780;&#36848;
&lt;/p&gt;
&lt;p&gt;
Explainable AI for Safe and Trustworthy Autonomous Driving: A Systematic Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10086
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#23545;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#20449;&#20219;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30340;&#26041;&#24335;&#65292;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#22312;&#28385;&#36275;&#33258;&#21160;&#39550;&#39542;&#35201;&#27714;&#26041;&#38754;&#30340;&#20851;&#38190;&#36129;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#35774;&#35745;&#12289;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#12289;&#21487;&#35299;&#37322;&#30340;&#30417;&#25511;&#12289;&#36741;&#21161;&#25216;&#26415;&#21644;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#31561;&#20116;&#20010;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20854;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#20219;&#21153;&#20013;&#30456;&#23545;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#65288;AD&#65289;&#30340;&#24212;&#29992;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#38590;&#20197;&#29702;&#35299;&#30340;AI&#31995;&#32479;&#21152;&#21095;&#20102;&#23545;AD&#23433;&#20840;&#20445;&#35777;&#30340;&#29616;&#26377;&#25361;&#25112;&#12290;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#65288;XAI&#65289;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#20851;&#20110;&#21487;&#35299;&#37322;&#26041;&#27861;&#22312;&#23433;&#20840;&#21487;&#20449;&#30340;AD&#20013;&#30340;&#20840;&#38754;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#22312;AD&#32972;&#26223;&#19979;AI&#30340;&#35201;&#27714;&#65292;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#26426;&#26500;&#36825;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;XAI&#23545;&#20110;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;AI&#20013;&#35299;&#37322;&#30340;&#26469;&#28304;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;XAI&#30340;&#20998;&#31867;&#23398;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;XAI&#22312;&#23433;&#20840;&#21487;&#20449;&#30340;AD&#20013;&#30340;&#20116;&#20010;&#20027;&#35201;&#36129;&#29486;&#65292;&#21253;&#25324;&#21487;&#35299;&#37322;&#30340;&#35774;&#35745;&#12289;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#12289;&#21487;&#35299;&#37322;&#30340;&#30417;&#25511;&#65292;&#36741;&#21161;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10086v1 Announce Type: cross  Abstract: Artificial Intelligence (AI) shows promising applications for the perception and planning tasks in autonomous driving (AD) due to its superior performance compared to conventional methods. However, inscrutable AI systems exacerbate the existing challenge of safety assurance of AD. One way to mitigate this challenge is to utilize explainable AI (XAI) techniques. To this end, we present the first comprehensive systematic literature review of explainable methods for safe and trustworthy AD. We begin by analyzing the requirements for AI in the context of AD, focusing on three key aspects: data, model, and agency. We find that XAI is fundamental to meeting these requirements. Based on this, we explain the sources of explanations in AI and describe a taxonomy of XAI. We then identify five key contributions of XAI for safe and trustworthy AI in AD, which are interpretable design, interpretable surrogate models, interpretable monitoring, auxil
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12289;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#20197;&#21450;&#19987;&#19994;&#33402;&#26415;&#23478;&#30340;&#30693;&#35782;&#12290;&#36825;&#23545;&#38450;&#27490;&#27450;&#35784;&#12289;&#36981;&#23432;&#25919;&#31574;&#20197;&#21450;&#36991;&#20813;&#27169;&#22411;&#23849;&#28291;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.03214</link><description>&lt;p&gt;
&#26377;&#26426;&#25110;&#25193;&#25955;&#65306;&#25105;&#20204;&#33021;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03214
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12289;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#20197;&#21450;&#19987;&#19994;&#33402;&#26415;&#23478;&#30340;&#30693;&#35782;&#12290;&#36825;&#23545;&#38450;&#27490;&#27450;&#35784;&#12289;&#36981;&#23432;&#25919;&#31574;&#20197;&#21450;&#36991;&#20813;&#27169;&#22411;&#23849;&#28291;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#22270;&#20687;&#30340;&#20986;&#29616;&#23436;&#20840;&#39072;&#35206;&#20102;&#33402;&#26415;&#30028;&#12290;&#20174;&#20154;&#31867;&#33402;&#26415;&#20013;&#35782;&#21035;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20854;&#24433;&#21709;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#19981;&#26029;&#22686;&#21152;&#12290;&#26410;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#20250;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#32773;&#27450;&#35784;&#37027;&#20123;&#25903;&#20184;&#39640;&#20215;&#36141;&#20080;&#20154;&#31867;&#33402;&#26415;&#21697;&#30340;&#20010;&#20154;&#21644;&#31105;&#27490;&#20351;&#29992;AI&#22270;&#20687;&#30340;&#20844;&#21496;&#12290;&#36825;&#23545;&#20110;&#38656;&#35201;&#36807;&#28388;&#35757;&#32451;&#25968;&#25454;&#20197;&#36991;&#20813;&#28508;&#22312;&#27169;&#22411;&#23849;&#28291;&#30340;AI&#27169;&#22411;&#35757;&#32451;&#32773;&#26469;&#35828;&#20063;&#33267;&#20851;&#37325;&#35201;&#12290;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#22270;&#20687;&#30340;&#26041;&#27861;&#26377;&#22810;&#31181;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#65292;&#20197;&#21450;&#36890;&#36807;&#19987;&#19994;&#33402;&#26415;&#23478;&#21033;&#29992;&#20182;&#20204;&#23545;&#33402;&#26415;&#25216;&#24039;&#30340;&#30693;&#35782;&#36827;&#34892;&#35782;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#33391;&#24615;&#21644;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;7&#31181;&#39118;&#26684;&#30340;&#30495;&#23454;&#20154;&#31867;&#33402;&#26415;&#65292;&#20174;5&#20010;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#20102;&#19982;&#20043;&#21305;&#37197;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20102;8&#20010;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of generative AI images has completely disrupted the art world. Identifying AI generated images from human art is a challenging problem whose impact is growing over time. The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery. This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse. There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors 
&lt;/p&gt;</description></item><item><title>RIDGE&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#21487;&#37325;&#22797;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#21487;&#38752;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#25928;&#29575;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#24037;&#20316;&#36136;&#37327;&#21644;&#36879;&#26126;&#24230;&#65292;&#30830;&#20445;&#20998;&#21106;&#27169;&#22411;&#22312;&#31185;&#23398;&#21487;&#38752;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#19978;&#37117;&#20855;&#22791;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.08847</link><description>&lt;p&gt;
RIDGE: &#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#21487;&#37325;&#22797;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#21487;&#38752;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#25928;&#29575;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RIDGE: Reproducibility, Integrity, Dependability, Generalizability, and Efficiency Assessment of Medical Image Segmentation Models. (arXiv:2401.08847v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08847
&lt;/p&gt;
&lt;p&gt;
RIDGE&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#21487;&#37325;&#22797;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#21487;&#38752;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#25928;&#29575;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#24037;&#20316;&#36136;&#37327;&#21644;&#36879;&#26126;&#24230;&#65292;&#30830;&#20445;&#20998;&#21106;&#27169;&#22411;&#22312;&#31185;&#23398;&#21487;&#38752;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#19978;&#37117;&#20855;&#22791;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#24448;&#24448;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#20020;&#24202;&#20013;&#30340;&#24212;&#29992;&#12290;&#22270;&#20687;&#20998;&#21106;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#20043;&#19968;&#65292;&#38656;&#35201;&#23545;&#19968;&#20010;&#25110;&#22810;&#20010;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;/&#20307;&#31215;&#36827;&#34892;&#27880;&#37322;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;RIDGE&#28165;&#21333;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#21487;&#37325;&#22797;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#21487;&#38752;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#25928;&#29575;&#30340;&#26694;&#26550;&#12290;&#35813;&#28165;&#21333;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#25351;&#23548;&#65292;&#20197;&#25552;&#39640;&#20854;&#24037;&#20316;&#30340;&#36136;&#37327;&#21644;&#36879;&#26126;&#24230;&#65292;&#30830;&#20445;&#20998;&#21106;&#27169;&#22411;&#19981;&#20165;&#20855;&#26377;&#31185;&#23398;&#30340;&#21487;&#38752;&#24615;&#65292;&#36824;&#20855;&#26377;&#20020;&#24202;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning techniques, despite their potential, often suffer from a lack of reproducibility and generalizability, impeding their clinical adoption. Image segmentation is one of the critical tasks in medical image analysis, in which one or several regions/volumes of interest should be annotated. This paper introduces the RIDGE checklist, a framework for assessing the Reproducibility, Integrity, Dependability, Generalizability, and Efficiency of deep learning-based medical image segmentation models. The checklist serves as a guide for researchers to enhance the quality and transparency of their work, ensuring that segmentation models are not only scientifically sound but also clinically relevant.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Mixture-of-Experts&#29992;&#20110;&#24320;&#25918;&#22495;&#36866;&#24212;&#30340;&#21452;&#31354;&#38388;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#29305;&#24449;&#31354;&#38388;&#21644;&#36335;&#30001;&#29305;&#24449;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#65292;&#26080;&#38656;&#25163;&#21160;&#35843;&#33410;&#38408;&#20540;&#12290;</title><link>http://arxiv.org/abs/2311.00285</link><description>&lt;p&gt;
Mixture-of-Experts&#29992;&#20110;&#24320;&#25918;&#22495;&#36866;&#24212;&#30340;&#21452;&#31354;&#38388;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Experts for Open Set Domain Adaptation: A Dual-Space Detection Approach. (arXiv:2311.00285v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00285
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Mixture-of-Experts&#29992;&#20110;&#24320;&#25918;&#22495;&#36866;&#24212;&#30340;&#21452;&#31354;&#38388;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#29305;&#24449;&#31354;&#38388;&#21644;&#36335;&#30001;&#29305;&#24449;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#65292;&#26080;&#38656;&#25163;&#21160;&#35843;&#33410;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#36866;&#24212;&#65288;OSDA&#65289;&#26088;&#22312;&#21516;&#26102;&#22788;&#29702;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#21644;&#26631;&#31614;&#20559;&#31227;&#65292;&#23454;&#29616;&#23545;&#24050;&#30693;&#31867;&#21035;&#30340;&#31934;&#30830;&#20998;&#31867;&#65292;&#21516;&#26102;&#22312;&#30446;&#26631;&#22495;&#20013;&#35782;&#21035;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;OSDA&#26041;&#27861;&#20381;&#36182;&#20110;&#28145;&#24230;&#27169;&#22411;&#30340;&#26368;&#32456;&#22270;&#20687;&#29305;&#24449;&#31354;&#38388;&#65292;&#38656;&#35201;&#25163;&#21160;&#35843;&#33410;&#38408;&#20540;&#65292;&#24182;&#19988;&#21487;&#33021;&#23558;&#26410;&#30693;&#26679;&#26412;&#38169;&#35823;&#20998;&#31867;&#20026;&#24050;&#30693;&#31867;&#21035;&#12290;Mixture-of-Expert&#65288;MoE&#65289;&#21487;&#33021;&#26159;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#12290;&#22312;MoE&#20013;&#65292;&#19981;&#21516;&#30340;&#19987;&#23478;&#22788;&#29702;&#19981;&#21516;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#22312;&#36335;&#30001;&#29305;&#24449;&#31354;&#38388;&#20013;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#29983;&#25104;&#29420;&#29305;&#30340;&#19987;&#23478;&#36335;&#30001;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#20063;&#21487;&#20197;&#26174;&#31034;&#19982;&#24050;&#30693;&#31867;&#21035;&#19981;&#21516;&#30340;&#19987;&#23478;&#36335;&#30001;&#27169;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21452;&#31354;&#38388;&#26816;&#27979;&#65292;&#21033;&#29992;&#22270;&#20687;&#29305;&#24449;&#31354;&#38388;&#21644;&#36335;&#30001;&#29305;&#24449;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#65292;&#26080;&#38656;&#20219;&#20309;&#38408;&#20540;&#12290;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#22270;&#24418;&#36335;&#30001;&#22120;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#25688;&#35201;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open Set Domain Adaptation (OSDA) aims to cope with the distribution and label shifts between the source and target domains simultaneously, performing accurate classification for known classes while identifying unknown class samples in the target domain. Most existing OSDA approaches, depending on the final image feature space of deep models, require manually-tuned thresholds, and may easily misclassify unknown samples as known classes. Mixture-of-Expert (MoE) could be a remedy. Within an MoE, different experts address different input features, producing unique expert routing patterns for different classes in a routing feature space. As a result, unknown class samples may also display different expert routing patterns to known classes. This paper proposes Dual-Space Detection, which exploits the inconsistencies between the image feature space and the routing feature space to detect unknown class samples without any threshold. Graph Router is further introduced to better make use of the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31264;&#23494;&#19977;&#32500;&#24341;&#29992;&#32593;&#32476;ConcreteNet&#65292;&#21253;&#21547;&#19977;&#20010;&#26032;&#27169;&#22359;&#65292;&#26088;&#22312;&#25913;&#21892;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#31867;&#21035;&#24178;&#25200;&#22240;&#32032;&#30340;&#37325;&#22797;&#23454;&#20363;&#30340;&#24341;&#29992;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04561</link><description>&lt;p&gt;
&#25913;&#36827;&#31264;&#23494;&#19977;&#32500;&#35270;&#35273;&#24341;&#29992;&#30340;&#19977;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Three Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding. (arXiv:2309.04561v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04561
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31264;&#23494;&#19977;&#32500;&#24341;&#29992;&#32593;&#32476;ConcreteNet&#65292;&#21253;&#21547;&#19977;&#20010;&#26032;&#27169;&#22359;&#65292;&#26088;&#22312;&#25913;&#21892;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#31867;&#21035;&#24178;&#25200;&#22240;&#32032;&#30340;&#37325;&#22797;&#23454;&#20363;&#30340;&#24341;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#35270;&#35273;&#24341;&#29992;&#26159;&#25351;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#23450;&#20301;&#19977;&#32500;&#22330;&#26223;&#20013;&#34987;&#24341;&#29992;&#30340;&#29289;&#20307;&#30340;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#22312;&#33258;&#20027;&#23460;&#20869;&#26426;&#22120;&#20154;&#21040;AR/VR&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#30446;&#21069;&#19968;&#31181;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#26816;&#27979;&#26469;&#23436;&#25104;&#19977;&#32500;&#35270;&#35273;&#24341;&#29992;&#65292;&#21363;&#36890;&#36807;&#36793;&#30028;&#26694;&#26469;&#23450;&#20301;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#36827;&#34892;&#29289;&#29702;&#20132;&#20114;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36793;&#30028;&#26694;&#19981;&#36275;&#20197;&#25551;&#36848;&#29289;&#20307;&#30340;&#20960;&#20309;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#31264;&#23494;&#19977;&#32500;&#35270;&#35273;&#24341;&#29992;&#30340;&#38382;&#39064;&#65292;&#21363;&#22522;&#20110;&#24341;&#29992;&#30340;&#19977;&#32500;&#23454;&#20363;&#20998;&#21106;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31264;&#23494;&#19977;&#32500;&#24341;&#29992;&#32593;&#32476;ConcreteNet&#65292;&#20854;&#20013;&#21253;&#21547;&#19977;&#20010;&#29420;&#31435;&#30340;&#26032;&#27169;&#22359;&#65292;&#26088;&#22312;&#25913;&#36827;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#31867;&#21035;&#24178;&#25200;&#22240;&#32032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37325;&#22797;&#23454;&#20363;&#30340;&#24341;&#29992;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#19979;&#32780;&#19978;&#30340;&#27880;&#24847;&#21147;&#34701;&#21512;&#27169;&#22359;&#65292;&#26088;&#22312;&#28040;&#38500;&#23454;&#20363;&#38388;&#20851;&#31995;&#32447;&#32034;&#30340;&#27495;&#20041;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#26500;&#36896;&#19968;&#20010;cont
&lt;/p&gt;
&lt;p&gt;
3D visual grounding is the task of localizing the object in a 3D scene which is referred by a description in natural language. With a wide range of applications ranging from autonomous indoor robotics to AR/VR, the task has recently risen in popularity. A common formulation to tackle 3D visual grounding is grounding-by-detection, where localization is done via bounding boxes. However, for real-life applications that require physical interactions, a bounding box insufficiently describes the geometry of an object. We therefore tackle the problem of dense 3D visual grounding, i.e. referral-based 3D instance segmentation. We propose a dense 3D grounding network ConcreteNet, featuring three novel stand-alone modules which aim to improve grounding performance for challenging repetitive instances, i.e. instances with distractors of the same semantic class. First, we introduce a bottom-up attentive fusion module that aims to disambiguate inter-instance relational cues, next we construct a cont
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#20998;&#21106;&#22270;&#20013;&#26631;&#31614;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;S4MC&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#36890;&#36807;&#22686;&#24378;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#24182;&#25552;&#39640;&#20102;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20351;&#29992;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.13900</link><description>&lt;p&gt;
&#36890;&#36807;&#36793;&#38469;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Semantic Segmentation via Marginal Contextual Information. (arXiv:2308.13900v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13900
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20998;&#21106;&#22270;&#20013;&#26631;&#31614;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;S4MC&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#36890;&#36807;&#22686;&#24378;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#24182;&#25552;&#39640;&#20102;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20351;&#29992;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32622;&#20449;&#24230;&#31934;&#21270;&#26041;&#26696;&#65292;&#22686;&#24378;&#20102;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#20266;&#26631;&#31614;&#12290;&#19982;&#24403;&#21069;&#20027;&#27969;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#30456;&#37051;&#20687;&#32032;&#20998;&#32452;&#24182;&#20849;&#21516;&#32771;&#34385;&#23427;&#20204;&#30340;&#20266;&#26631;&#31614;&#65292;&#21033;&#29992;&#20998;&#21106;&#22270;&#20013;&#26631;&#31614;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#20511;&#21161;&#36825;&#31181;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;S4MC&#65292;&#22312;&#20445;&#25345;&#20266;&#26631;&#31614;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#22686;&#21152;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#25968;&#37327;&#65292;&#19988;&#35745;&#31639;&#24320;&#38144;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#36890;&#36807;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;S4MC&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20026;&#38477;&#20302;&#33719;&#24471;&#31264;&#23494;&#26631;&#27880;&#25104;&#26412;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20363;&#22914;&#65292;&#22312;PASCAL VOC 12&#19978;&#20351;&#29992;366&#20010;&#24102;&#27880;&#37322;&#22270;&#20687;&#65292;S4MC&#27604;&#21069;&#19968;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;1.29&#20010;mIoU&#12290;&#26377;&#20851;&#37325;&#29616;&#25105;&#20204;&#23454;&#39564;&#30340;&#20195;&#30721;&#21442;&#35265;...
&lt;/p&gt;
&lt;p&gt;
We present a novel confidence refinement scheme that enhances pseudo-labels in semi-supervised semantic segmentation. Unlike current leading methods, which filter pixels with low-confidence predictions in isolation, our approach leverages the spatial correlation of labels in segmentation maps by grouping neighboring pixels and considering their pseudo-labels collectively. With this contextual information, our method, named S4MC, increases the amount of unlabeled data used during training while maintaining the quality of the pseudo-labels, all with negligible computational overhead. Through extensive experiments on standard benchmarks, we demonstrate that S4MC outperforms existing state-of-the-art semi-supervised learning approaches, offering a promising solution for reducing the cost of acquiring dense annotations. For example, S4MC achieves a 1.29 mIoU improvement over the prior state-of-the-art method on PASCAL VOC 12 with 366 annotated images. The code to reproduce our experiments i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#32452;&#21512;&#24615;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#65292;&#21457;&#29616;&#28145;&#24230;CNN&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#38543;&#30528;&#31867;&#21035;&#25968;&#12289;&#32452;&#21512;&#25968;&#21644;&#36845;&#20195;&#27425;&#25968;&#30340;&#22686;&#21152;&#32780;&#28176;&#36827;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2307.02129</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#32452;&#21512;&#24615;&#25968;&#25454;&#65306;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model. (arXiv:2307.02129v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#32452;&#21512;&#24615;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#65292;&#21457;&#29616;&#28145;&#24230;CNN&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#38543;&#30528;&#31867;&#21035;&#25968;&#12289;&#32452;&#21512;&#25968;&#21644;&#36845;&#20195;&#27425;&#25968;&#30340;&#22686;&#21152;&#32780;&#28176;&#36827;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19968;&#33324;&#39640;&#32500;&#20219;&#21153;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#19982;&#32500;&#24230;&#25104;&#25351;&#25968;&#22686;&#38271;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#25104;&#21151;&#12290;&#19968;&#31181;&#26222;&#36941;&#30340;&#20551;&#35774;&#26159;&#21487;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;CNN&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#24314;&#31435;&#20102;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#38656;&#35201;&#22810;&#23569;&#35757;&#32451;&#25968;&#25454;&#20197;&#21450;&#36825;&#20010;&#25968;&#23383;&#22914;&#20309;&#21462;&#20915;&#20110;&#25968;&#25454;&#32467;&#26500;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#22238;&#31572;&#20102;&#38024;&#23545;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#30340;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#25429;&#25417;&#30495;&#23454;&#25968;&#25454;&#30340;&#30456;&#20851;&#26041;&#38754;&#65306;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;$n_c$&#20010;&#31867;&#21035;&#20013;&#30340;&#27599;&#19968;&#20010;&#23545;&#24212;&#20110;$m$&#20010;&#21516;&#20041;&#32452;&#21512;&#30340;&#39640;&#23618;&#27425;&#29305;&#24449;&#65292;&#24182;&#19988;&#36825;&#20123;&#29305;&#24449;&#21448;&#36890;&#36807;&#19968;&#20010;&#37325;&#22797;$L$&#27425;&#30340;&#36845;&#20195;&#36807;&#31243;&#30001;&#23376;&#29305;&#24449;&#32452;&#25104;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38656;&#35201;&#28145;&#24230;CNN&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;$P^*$&#65288;i&#65289;&#38543;&#30528;$n_c m^L$&#30340;&#22686;&#38271;&#32780;&#28176;&#36827;&#22320;&#22686;&#38271;&#65292;&#36825;&#21482;&#26377;...
&lt;/p&gt;
&lt;p&gt;
Learning generic high-dimensional tasks is notably hard, as it requires a number of training data exponential in the dimension. Yet, deep convolutional neural networks (CNNs) have shown remarkable success in overcoming this challenge. A popular hypothesis is that learnable tasks are highly structured and that CNNs leverage this structure to build a low-dimensional representation of the data. However, little is known about how much training data they require, and how this number depends on the data structure. This paper answers this question for a simple classification task that seeks to capture relevant aspects of real data: the Random Hierarchy Model. In this model, each of the $n_c$ classes corresponds to $m$ synonymic compositions of high-level features, which are in turn composed of sub-features through an iterative process repeated $L$ times. We find that the number of training data $P^*$ required by deep CNNs to learn this task (i) grows asymptotically as $n_c m^L$, which is only
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17401</link><description>&lt;p&gt;
&#19968;&#31181;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework For Refining Text Classification and Object Recognition from Academic Articles. (arXiv:2305.17401v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#39640;&#25928;&#22320;&#20174;&#22823;&#37327;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#21462;&#29305;&#23450;&#20449;&#24687;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#36890;&#24120;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25366;&#25496;&#23398;&#26415;&#35770;&#25991;&#30340;&#25968;&#25454;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#33258;&#21160;&#20174;&#22797;&#26434;&#30340;&#38750;&#32467;&#26500;&#21270;&#24067;&#23616;&#25991;&#26723;&#20013;&#25552;&#21462;&#29305;&#23450;&#27169;&#24335;&#12290;&#24403;&#21069;&#30340;&#23398;&#26415;&#35770;&#25991;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#65288;RB&#65289;&#25110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#38656;&#35201;&#32534;&#20889;&#22797;&#26434;&#25490;&#29256;&#35770;&#25991;&#30340;&#39640;&#26114;&#25104;&#26412;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20165;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#23545;&#25991;&#31456;&#20013;&#22797;&#26434;&#20869;&#23481;&#31867;&#22411;&#36827;&#34892;&#27880;&#37322;&#24037;&#20316;&#65292;&#36825;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#12290;&#27492;&#22806;&#65292;&#20165;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#23481;&#26131;&#35782;&#21035;&#30340;&#27169;&#24335;&#34987;&#38169;&#35823;&#25552;&#21462;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20174;&#20998;&#26512;&#25351;&#23450;&#33879;&#20316;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#24067;&#23616;&#21644;&#25490;&#29256;&#35282;&#24230;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread use of the internet, it has become increasingly crucial to extract specific information from vast amounts of academic articles efficiently. Data mining techniques are generally employed to solve this issue. However, data mining for academic articles is challenging since it requires automatically extracting specific patterns in complex and unstructured layout documents. Current data mining methods for academic articles employ rule-based(RB) or machine learning(ML) approaches. However, using rule-based methods incurs a high coding cost for complex typesetting articles. On the other hand, simply using machine learning methods requires annotation work for complex content types within the paper, which can be costly. Furthermore, only using machine learning can lead to cases where patterns easily recognized by rule-based methods are mistakenly extracted. To overcome these issues, from the perspective of analyzing the standard layout and typesetting used in the specified p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;MSc-iNCD&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#30340;&#23398;&#20064;&#20013;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.15975</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#20855;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery. (arXiv:2303.15975v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;MSc-iNCD&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#30340;&#23398;&#20064;&#20013;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#21629;&#38271;&#23398;&#20064;&#32773;&#20013;&#65292;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#36830;&#32493;&#22320;&#21457;&#29616;&#26032;&#27010;&#24565;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26399;&#26395;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#36825;&#31867;&#38382;&#39064;&#22312;&#38750;&#24120;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#37096;&#20998;&#35299;&#20915;&#65292;&#20854;&#20013;&#35201;&#20040;&#20026;&#21457;&#29616;&#26032;&#27010;&#24565;&#25552;&#20379;&#26377;&#26631;&#21495;&#30340;&#25968;&#25454;&#65288;&#20363;&#22914; NCD&#65289;&#65292;&#35201;&#20040;&#23398;&#20064;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#22686;&#37327;&#27493;&#39588;&#20013;&#21457;&#29983;&#65288;&#20363;&#22914;&#31867; iNCD&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026; MSc-iNCD&#65292;&#20854;&#20013;&#23398;&#20064;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#65292;&#24182;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#22522;&#32447;&#65292;&#19981;&#20165;&#22312;&#36739;&#38271;&#30340;&#23398;&#20064;&#24773;&#22659;&#19979;&#20855;&#26377;&#24377;&#24615;&#65292;&#32780;&#19988;&#19982;&#22797;&#26434;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#32447;&#30340;&#26377;&#25928;&#24615;&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#22522;&#20934;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering novel concepts from unlabelled data and in a continuous manner is an important desideratum of lifelong learners. In the literature such problems have been partially addressed under very restricted settings, where either access to labelled data is provided for discovering novel concepts (e.g., NCD) or learning occurs for a limited number of incremental steps (e.g., class-iNCD). In this work we challenge the status quo and propose a more challenging and practical learning paradigm called MSc-iNCD, where learning occurs continuously and unsupervisedly, while exploiting the rich priors from large-scale pre-trained models. To this end, we propose simple baselines that are not only resilient under longer learning scenarios, but are surprisingly strong when compared with sophisticated state-of-the-art methods. We conduct extensive empirical evaluation on a multitude of benchmarks and show the effectiveness of our proposed baselines, which significantly raises the bar.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;&#22810;&#31181;&#24120;&#29992;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#21450;&#20854;&#19981;&#21516;&#29305;&#24449;&#25552;&#21462;&#28857;&#65292;&#22312;&#22235;&#20010;&#28145;&#24230;&#24863;&#30693;&#25439;&#22833;&#29992;&#20363;&#19978;&#35299;&#20915;&#20102;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.04032</link><description>&lt;p&gt;
&#28145;&#24230;&#24863;&#30693;&#25439;&#22833;&#32593;&#32476;&#30340;&#31995;&#32479;&#24615;&#33021;&#20998;&#26512;&#65306;&#25171;&#30772;&#36801;&#31227;&#23398;&#20064;&#30340;&#32422;&#23450;
&lt;/p&gt;
&lt;p&gt;
A Systematic Performance Analysis of Deep Perceptual Loss Networks: Breaking Transfer Learning Conventions. (arXiv:2302.04032v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04032
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;&#22810;&#31181;&#24120;&#29992;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#21450;&#20854;&#19981;&#21516;&#29305;&#24449;&#25552;&#21462;&#28857;&#65292;&#22312;&#22235;&#20010;&#28145;&#24230;&#24863;&#30693;&#25439;&#22833;&#29992;&#20363;&#19978;&#35299;&#20915;&#20102;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24863;&#30693;&#25439;&#22833;&#26159;&#19968;&#31181;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#20351;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#20174;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#30340;&#28145;&#24230;&#29305;&#24449;&#26469;&#27169;&#20223;&#20154;&#31867;&#24863;&#30693;&#12290;&#36817;&#24180;&#26469;&#65292;&#35813;&#26041;&#27861;&#22312;&#35768;&#22810;&#26377;&#36259;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#22270;&#20687;&#25110;&#31867;&#20284;&#22270;&#20687;&#36755;&#20986;&#30340;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#21512;&#25104;&#12289;&#20998;&#21106;&#12289;&#28145;&#24230;&#39044;&#27979;&#31561;&#12290;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#36890;&#24120;&#26159;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#25439;&#22833;&#35745;&#31639;&#12290;&#23613;&#31649;&#23545;&#35813;&#26041;&#27861;&#30340;&#20852;&#36259;&#21644;&#24191;&#27867;&#20351;&#29992;&#22686;&#21152;&#20102;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#22810;&#30340;&#21162;&#21147;&#26469;&#25506;&#32034;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#24863;&#30693;&#25439;&#22833;&#30340;&#32593;&#32476;&#20197;&#21450;&#20174;&#21738;&#20123;&#23618;&#25552;&#21462;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#31995;&#32479;&#22320;&#35780;&#20272;&#22810;&#31181;&#24120;&#29992;&#19988;&#26131;&#20110;&#33719;&#21462;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#65292;&#20197;&#21450;&#38024;&#23545;&#22235;&#20010;&#29616;&#26377;&#28145;&#24230;&#24863;&#30693;&#25439;&#22833;&#29992;&#20363;&#30340;&#19981;&#21516;&#29305;&#24449;&#25552;&#21462;&#28857;&#26469;&#32416;&#27491;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep perceptual loss is a type of loss function in computer vision that aims to mimic human perception by using the deep features extracted from neural networks. In recent years, the method has been applied to great effect on a host of interesting computer vision tasks, especially for tasks with image or image-like outputs, such as image synthesis, segmentation, depth prediction, and more. Many applications of the method use pretrained networks, often convolutional networks, for loss calculation. Despite the increased interest and broader use, more effort is needed toward exploring which networks to use for calculating deep perceptual loss and from which layers to extract the features.  This work aims to rectify this by systematically evaluating a host of commonly used and readily available, pretrained networks for a number of different feature extraction points on four existing use cases of deep perceptual loss. The use cases of perceptual similarity, super-resolution, image segmentat
&lt;/p&gt;</description></item></channel></rss>