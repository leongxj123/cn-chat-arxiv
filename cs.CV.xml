<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;REAL&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21452;&#27969;&#22522;&#30784;&#39044;&#35757;&#32451;&#21644;&#34920;&#31034;&#22686;&#24378;&#33976;&#39311;&#36807;&#31243;&#26469;&#22686;&#24378;&#25552;&#21462;&#22120;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13522</link><description>&lt;p&gt;
REAL&#65306;&#29992;&#20110;&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#34920;&#31034;&#22686;&#24378;&#20998;&#26512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
REAL: Representation Enhanced Analytic Learning for Exemplar-free Class-incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;REAL&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21452;&#27969;&#22522;&#30784;&#39044;&#35757;&#32451;&#21644;&#34920;&#31034;&#22686;&#24378;&#33976;&#39311;&#36807;&#31243;&#26469;&#22686;&#24378;&#25552;&#21462;&#22120;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#33539;&#20363;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;(EFCIL)&#26088;&#22312;&#20943;&#36731;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32780;&#27809;&#26377;&#21487;&#29992;&#30340;&#21382;&#21490;&#25968;&#25454;&#12290;&#19982;&#23384;&#20648;&#21382;&#21490;&#26679;&#26412;&#30340;&#22238;&#25918;&#24335;CIL&#30456;&#27604;&#65292;EFCIL&#22312;&#26080;&#33539;&#20363;&#32422;&#26463;&#19979;&#26356;&#23481;&#26131;&#36951;&#24536;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#26368;&#36817;&#21457;&#23637;&#30340;&#22522;&#20110;&#20998;&#26512;&#23398;&#20064;(AL)&#30340;CIL&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;EFCIL&#30340;&#34920;&#31034;&#22686;&#24378;&#20998;&#26512;&#23398;&#20064;(REAL)&#12290;REAL&#26500;&#24314;&#20102;&#19968;&#20010;&#21452;&#27969;&#22522;&#30784;&#39044;&#35757;&#32451;(DS-BPT)&#21644;&#19968;&#20010;&#34920;&#31034;&#22686;&#24378;&#33976;&#39311;(RED)&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#25552;&#21462;&#22120;&#30340;&#34920;&#31034;&#12290;DS-BPT&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;(SSCL)&#20004;&#20010;&#27969;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#30784;&#30693;&#35782;&#25552;&#21462;&#12290;RED&#36807;&#31243;&#23558;&#30417;&#30563;&#30693;&#35782;&#25552;&#28860;&#21040;SSCL&#39044;&#35757;&#32451;&#39592;&#24178;&#37096;&#20998;&#65292;&#20419;&#36827;&#21518;&#32493;&#30340;&#22522;&#20110;AL&#30340;CIL&#65292;&#23558;CIL&#36716;&#25442;&#20026;&#36882;&#24402;&#26368;&#23567;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13522v1 Announce Type: new  Abstract: Exemplar-free class-incremental learning (EFCIL) aims to mitigate catastrophic forgetting in class-incremental learning without available historical data. Compared with its counterpart (replay-based CIL) that stores historical samples, the EFCIL suffers more from forgetting issues under the exemplar-free constraint. In this paper, inspired by the recently developed analytic learning (AL) based CIL, we propose a representation enhanced analytic learning (REAL) for EFCIL. The REAL constructs a dual-stream base pretraining (DS-BPT) and a representation enhancing distillation (RED) process to enhance the representation of the extractor. The DS-BPT pretrains model in streams of both supervised learning and self-supervised contrastive learning (SSCL) for base knowledge extraction. The RED process distills the supervised knowledge to the SSCL pretrained backbone and facilitates a subsequent AL-basd CIL that converts the CIL to a recursive least
&lt;/p&gt;</description></item></channel></rss>