<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;ModTr&#26041;&#27861;&#65292;&#36890;&#36807;&#23567;&#22411;&#36716;&#25442;&#32593;&#32476;&#35843;&#25972;&#36755;&#20837;&#20197;&#26368;&#23567;&#21270;&#26816;&#27979;&#25439;&#22833;&#65292;&#23454;&#29616;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#20174;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#21040;&#21478;&#19968;&#20010;&#30340;&#26377;&#25928;&#36866;&#24212;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2404.01492</link><description>&lt;p&gt;
&#19981;&#36951;&#24536;&#20808;&#39564;&#30693;&#35782;&#30340;&#30446;&#26631;&#26816;&#27979;&#36866;&#24212;&#27169;&#24577;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Modality Translation for Object Detection Adaptation Without Forgetting Prior Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;ModTr&#26041;&#27861;&#65292;&#36890;&#36807;&#23567;&#22411;&#36716;&#25442;&#32593;&#32476;&#35843;&#25972;&#36755;&#20837;&#20197;&#26368;&#23567;&#21270;&#26816;&#27979;&#25439;&#22833;&#65292;&#23454;&#29616;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#20174;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#21040;&#21478;&#19968;&#20010;&#30340;&#26377;&#25928;&#36866;&#24212;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#20934;&#30830;&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#21482;&#36866;&#29992;&#20110;&#36328;&#27169;&#24577;&#65292;&#22240;&#20026;&#20351;&#29992;&#19981;&#21516;&#20256;&#24863;&#22120;&#25429;&#33719;&#30340;&#25968;&#25454;&#23384;&#22312;&#26356;&#22823;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#23558;&#22823;&#22411;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#35843;&#25972;&#21040;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ModTr&#20316;&#20026;&#26222;&#36941;&#20570;&#27861;&#24494;&#35843;&#22823;&#22411;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;ModTr&#21253;&#25324;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#36716;&#25442;&#32593;&#32476;&#35843;&#25972;&#36755;&#20837;&#65292;&#35813;&#32593;&#32476;&#32463;&#36807;&#35757;&#32451;&#65292;&#30452;&#25509;&#20351;&#26816;&#27979;&#25439;&#22833;&#26368;&#23567;&#21270;&#12290;&#22240;&#27492;&#65292;&#21407;&#22987;&#27169;&#22411;&#21487;&#20197;&#22312;&#36716;&#25442;&#21518;&#30340;&#36755;&#20837;&#19978;&#24037;&#20316;&#65292;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#36827;&#19968;&#27493;&#30340;&#26356;&#25913;&#25110;&#21442;&#25968;&#24494;&#35843;&#12290;&#23545;&#20004;&#20010;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#20174;&#32418;&#22806;&#21040;RGB&#22270;&#20687;&#30340;&#36716;&#25442;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;ModTr&#26041;&#27861;&#25552;&#20379;&#20102;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01492v1 Announce Type: cross  Abstract: A common practice in deep learning consists of training large neural networks on massive datasets to perform accurately for different domains and tasks. While this methodology may work well in numerous application areas, it only applies across modalities due to a larger distribution shift in data captured using different sensors. This paper focuses on the problem of adapting a large object detection model to one or multiple modalities while being efficient. To do so, we propose ModTr as an alternative to the common approach of fine-tuning large models. ModTr consists of adapting the input with a small transformation network trained to minimize the detection loss directly. The original model can therefore work on the translated inputs without any further change or fine-tuning to its parameters. Experimental results on translating from IR to RGB images on two well-known datasets show that this simple ModTr approach provides detectors tha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65292;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.09918</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Attention-based Class-Conditioned Alignment for Multi-Source Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09918
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65292;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#65288;OD&#65289;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#33268;&#21147;&#20110;&#36890;&#36807;&#20419;&#36827;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#29305;&#24449;&#23545;&#40784;&#26469;&#32531;&#35299;&#20998;&#24067;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#20801;&#35768;&#21033;&#29992;&#22810;&#20010;&#24102;&#27880;&#37322;&#30340;&#28304;&#25968;&#25454;&#38598;&#21644;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#26469;&#25552;&#39640;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;OD MSDA&#26041;&#27861;&#20197;&#19968;&#31181;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#26041;&#24335;&#25191;&#34892;&#29305;&#24449;&#23545;&#40784;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#25353;&#31867;&#21035;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#22024;&#26434;&#30340;&#20266;&#26631;&#31614;&#32780;&#23548;&#33268;&#38169;&#35823;&#31215;&#32047;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;MSDA&#65292;&#35813;&#26041;&#26696;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09918v1 Announce Type: cross  Abstract: Domain adaptation methods for object detection (OD) strive to mitigate the impact of distribution shifts by promoting feature alignment across source and target domains. Multi-source domain adaptation (MSDA) allows leveraging multiple annotated source datasets, and unlabeled target data to improve the accuracy and robustness of the detection model. Most state-of-the-art MSDA methods for OD perform feature alignment in a class-agnostic manner. This is challenging since the objects have unique modal information due to variations in object appearance across domains. A recent prototype-based approach proposed a class-wise alignment, yet it suffers from error accumulation due to noisy pseudo-labels which can negatively affect adaptation with imbalanced data. To overcome these limitations, we propose an attention-based class-conditioned alignment scheme for MSDA that aligns instances of each object category across domains. In particular, an 
&lt;/p&gt;</description></item><item><title>&#22312;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#21512;&#25104;&#25463;&#24452;&#26469;&#25506;&#31350;&#23545;&#27604;&#35757;&#32451;&#26159;&#21542;&#36275;&#20197;&#23398;&#20064;&#21040;&#21253;&#21547;&#25152;&#26377;&#20449;&#24687;&#30340;&#20219;&#21153;&#26368;&#20248;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.17510</link><description>&lt;p&gt;
&#31034;&#33539;&#21644;&#20943;&#23569;&#35270;&#35273;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#25463;&#24452;
&lt;/p&gt;
&lt;p&gt;
Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17510
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#21512;&#25104;&#25463;&#24452;&#26469;&#25506;&#31350;&#23545;&#27604;&#35757;&#32451;&#26159;&#21542;&#36275;&#20197;&#23398;&#20064;&#21040;&#21253;&#21547;&#25152;&#26377;&#20449;&#24687;&#30340;&#20219;&#21153;&#26368;&#20248;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17510v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#20027;&#35201;&#20381;&#36182;&#23545;&#27604;&#35757;&#32451;&#26469;&#23398;&#20064;&#22270;&#20687;&#21644;&#26631;&#39064;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#24773;&#20917;&#26159;&#24403;&#19968;&#20010;&#22270;&#20687;&#19982;&#22810;&#20010;&#26631;&#39064;&#30456;&#20851;&#32852;&#26102;&#65292;&#27599;&#20010;&#26631;&#39064;&#26082;&#21253;&#21547;&#25152;&#26377;&#26631;&#39064;&#20849;&#20139;&#30340;&#20449;&#24687;&#65292;&#21448;&#21253;&#21547;&#20851;&#20110;&#22270;&#20687;&#22330;&#26223;&#30340;&#27599;&#20010;&#26631;&#39064;&#29420;&#29305;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23578;&#19981;&#28165;&#26970;&#23545;&#27604;&#25439;&#22833;&#26159;&#21542;&#36275;&#20197;&#23398;&#20064;&#21253;&#21547;&#26631;&#39064;&#25552;&#20379;&#30340;&#25152;&#26377;&#20449;&#24687;&#30340;&#20219;&#21153;&#26368;&#20248;&#34920;&#31034;&#65292;&#36824;&#26159;&#23545;&#27604;&#23398;&#20064;&#35774;&#32622;&#26159;&#21542;&#40723;&#21169;&#23398;&#20064;&#26368;&#23567;&#21270;&#23545;&#27604;&#25439;&#22833;&#30340;&#31616;&#21333;&#25463;&#24452;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#35270;&#35273;-&#35821;&#35328;&#30340;&#21512;&#25104;&#25463;&#24452;&#65306;&#19968;&#31181;&#35757;&#32451;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#21521;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#27880;&#20837;&#21512;&#25104;&#25463;&#24452;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#25110;&#29992;&#21253;&#21547;&#36825;&#20123;&#21512;&#25104;&#25463;&#24452;&#30340;&#25968;&#25454;&#24494;&#35843;&#30340;&#23545;&#27604;VLMs&#20027;&#35201;&#23398;&#20064;&#20195;&#34920;&#25463;&#24452;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17510v1 Announce Type: cross  Abstract: Vision-language models (VLMs) mainly rely on contrastive training to learn general-purpose representations of images and captions. We focus on the situation when one image is associated with several captions, each caption containing both information shared among all captions and unique information per caption about the scene depicted in the image. In such cases, it is unclear whether contrastive losses are sufficient for learning task-optimal representations that contain all the information provided by the captions or whether the contrastive learning setup encourages the learning of a simple shortcut that minimizes contrastive loss. We introduce synthetic shortcuts for vision-language: a training and evaluation framework where we inject synthetic shortcuts into image-text data. We show that contrastive VLMs trained from scratch or fine-tuned with data containing these synthetic shortcuts mainly learn features that represent the shortcu
&lt;/p&gt;</description></item><item><title>Lite-Mind&#26088;&#22312;&#35299;&#20915;fMRI&#35299;&#30721;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#31283;&#20581;&#30340;&#33041;&#34920;&#31034;&#32593;&#32476;&#65292;&#36991;&#20813;&#20102;&#22312;&#23454;&#36341;&#35774;&#22791;&#19978;&#20026;&#27599;&#20010;&#21463;&#35797;&#32773;&#37096;&#32626;&#29305;&#23450;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.03781</link><description>&lt;p&gt;
Lite-Mind: &#39640;&#25928;&#31283;&#20581;&#30340;&#33041;&#34920;&#31034;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Lite-Mind: Towards Efficient and Robust Brain Representation Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03781
&lt;/p&gt;
&lt;p&gt;
Lite-Mind&#26088;&#22312;&#35299;&#20915;fMRI&#35299;&#30721;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#31283;&#20581;&#30340;&#33041;&#34920;&#31034;&#32593;&#32476;&#65292;&#36991;&#20813;&#20102;&#22312;&#23454;&#36341;&#35774;&#22791;&#19978;&#20026;&#27599;&#20010;&#21463;&#35797;&#32773;&#37096;&#32626;&#29305;&#23450;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#30340;fMRI&#26041;&#27861;&#35299;&#30721;&#22823;&#33041;&#20013;&#30340;&#35270;&#35273;&#20449;&#24687;&#30340;&#30740;&#31350;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#12290;&#25361;&#25112;&#22312;&#20110;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;fMRI&#20449;&#21495;&#30340;&#20302;&#20449;&#22122;&#27604;&#65292;&#23548;&#33268;fMRI&#21040;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#30340;&#20302;&#31934;&#24230;&#12290;MindEye&#25216;&#26415;&#36890;&#36807;&#21033;&#29992;&#39640;&#21442;&#25968;&#35745;&#25968;&#30340;&#28145;&#24230;MLP&#65288;&#27599;&#20010;&#21463;&#35797;&#32773;&#30340;996M MLP&#20027;&#24178;&#65289;&#23558;fMRI&#23884;&#20837;&#23545;&#40784;&#21040;CLIP&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#26368;&#32456;&#38544;&#34255;&#23618;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;fMRI&#21040;&#22270;&#20687;&#26816;&#32034;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#30456;&#21516;&#30340;&#23454;&#39564;&#35774;&#32622;&#20869;&#65292;&#21463;&#35797;&#32773;&#20043;&#38388;&#23384;&#22312;&#26174;&#30528;&#30340;&#20010;&#20307;&#24046;&#24322;&#65292;&#38656;&#35201;&#35757;&#32451;&#29305;&#23450;&#20110;&#21463;&#35797;&#32773;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#22823;&#37327;&#30340;&#21442;&#25968;&#22312;&#23454;&#38469;&#35774;&#22791;&#19978;&#37096;&#32626;fMRI&#35299;&#30721;&#26102;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#38656;&#35201;&#20026;&#27599;&#20010;&#21463;&#35797;&#32773;&#25552;&#20379;&#29305;&#23450;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03781v2 Announce Type: replace-cross  Abstract: Research in decoding visual information from the brain, particularly through the non-invasive fMRI method, is rapidly progressing. The challenge arises from the limited data availability and the low signal-to-noise ratio of fMRI signals, leading to a low-precision task of fMRI-to-image retrieval. State-of-the-art MindEye remarkably improves fMRI-to-image retrieval performance by leveraging a deep MLP with a high parameter count orders of magnitude, i.e., a 996M MLP Backbone per subject, to align fMRI embeddings to the final hidden layer of CLIP's vision transformer. However, significant individual variations exist among subjects, even within identical experimental setups, mandating the training of subject-specific models. The substantial parameters pose significant challenges in deploying fMRI decoding on practical devices, especially with the necessitating of specific models for each subject. To this end, we propose Lite-Mind,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;&#20102;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;CATM&#21644;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;GSL&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09786</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Adaptive Self-training Framework for Fine-grained Scene Graph Generation. (arXiv:2401.09786v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;&#20102;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;CATM&#21644;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;GSL&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;SGG&#65289;&#27169;&#22411;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#38271;&#23614;&#35859;&#35789;&#20998;&#24067;&#21644;&#32570;&#22833;&#27880;&#37322;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;SGG&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#35757;&#32451;SGG&#65288;ST-SGG&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#20026;&#20854;&#20998;&#37197;&#20266;&#26631;&#31614;&#20197;&#35757;&#32451;SGG&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#22270;&#20687;&#35782;&#21035;&#26041;&#38754;&#30340;&#33258;&#35757;&#32451;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#35774;&#35745;&#36866;&#29992;&#20110;SGG&#20219;&#21153;&#30340;&#33258;&#35757;&#32451;&#26694;&#26550;&#26356;&#20855;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#22266;&#26377;&#29305;&#24615;&#65292;&#22914;&#35821;&#20041;&#27495;&#20041;&#21644;&#38271;&#23614;&#20998;&#24067;&#30340;&#35859;&#35789;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SGG&#20266;&#26631;&#31614;&#25216;&#26415;&#65292;&#31216;&#20026;&#20855;&#26377;&#21160;&#37327;&#30340;&#31867;&#21035;&#33258;&#36866;&#24212;&#38408;&#20540;&#21270;&#65288;CATM&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#29420;&#31435;&#20110;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#24050;&#26377;&#30340;SGG&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;&#65288;GSL&#65289;&#65292;&#20174;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scene graph generation (SGG) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. In this work, we aim to alleviate the long-tailed problem of SGG by utilizing unannotated triplets. To this end, we introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels for unannotated triplets based on which the SGG models are trained. While there has been significant progress in self-training for image recognition, designing a self-training framework for the SGG task is more challenging due to its inherent nature such as the semantic ambiguity and the long-tailed distribution of predicate classes. Hence, we propose a novel pseudo-labeling technique for SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is a model-agnostic framework that can be applied to any existing SGG models. Furthermore, we devise a graph structure learner (GSL) that is benefici
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SC-MIL&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#26469;&#21516;&#26102;&#25913;&#36827;&#29305;&#24449;&#23884;&#20837;&#21644;&#23454;&#20363;&#30456;&#20851;&#24615;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00048</link><description>&lt;p&gt;
SC-MIL: &#29992;&#20110;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#31232;&#30095;&#32534;&#30721;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2311.00048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SC-MIL&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#26469;&#21516;&#26102;&#25913;&#36827;&#29305;&#24449;&#23884;&#20837;&#21644;&#23454;&#20363;&#30456;&#20851;&#24615;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#22312;&#24369;&#30417;&#30563;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSI&#65289;&#20998;&#31867;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#20856;&#22411;&#30340;MIL&#26041;&#27861;&#21253;&#25324;&#29305;&#24449;&#23884;&#20837;&#37096;&#20998;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#23558;&#23454;&#20363;&#23884;&#20837;&#21040;&#29305;&#24449;&#20013;&#65292;&#20197;&#21450;MIL&#32858;&#21512;&#22120;&#65292;&#23558;&#23454;&#20363;&#23884;&#20837;&#32452;&#21512;&#25104;&#39044;&#27979;&#32467;&#26524;&#12290;&#30446;&#21069;&#30340;&#37325;&#28857;&#26159;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;&#36825;&#20123;&#37096;&#20998;&#65292;&#24182;&#21333;&#29420;&#24314;&#27169;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#32534;&#30721;&#30340;MIL&#65288;SC-MIL&#65289;&#65292;&#21516;&#26102;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#26041;&#38754;&#12290;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#36890;&#36807;&#23558;&#23454;&#20363;&#34920;&#31034;&#20026;&#36807;&#23436;&#22791;&#23383;&#20856;&#20013;&#21407;&#23376;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#26469;&#25429;&#25417;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#31232;&#30095;&#24615;&#21487;&#20197;&#36890;&#36807;&#25233;&#21046;&#19981;&#30456;&#20851;&#30340;&#23454;&#20363;&#32780;&#20445;&#30041;&#26368;&#30456;&#20851;&#30340;&#23454;&#20363;&#65292;&#20174;&#32780;&#22686;&#24378;&#23454;&#20363;&#30340;&#29305;&#24449;&#23884;&#20837;&#12290;&#20026;&#20102;&#25913;&#21892;&#20256;&#32479;&#30340;&#29305;&#24449;&#23884;&#20837;&#21644;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;we proposed a sparsely coded MIL.
&lt;/p&gt;
&lt;p&gt;
Multiple Instance Learning (MIL) has been widely used in weakly supervised whole slide image (WSI) classification. Typical MIL methods include a feature embedding part that embeds the instances into features via a pre-trained feature extractor and the MIL aggregator that combines instance embeddings into predictions. The current focus has been directed toward improving these parts by refining the feature embeddings through self-supervised pre-training and modeling the correlations between instances separately. In this paper, we proposed a sparsely coded MIL (SC-MIL) that addresses those two aspects at the same time by leveraging sparse dictionary learning. The sparse dictionary learning captures the similarities of instances by expressing them as a sparse linear combination of atoms in an over-complete dictionary. In addition, imposing sparsity help enhance the instance feature embeddings by suppressing irrelevant instances while retaining the most relevant ones. To make the convention
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prototype-based Mean-Teacher (PMT)&#30340;&#26032;&#22411;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31867;&#21407;&#22411;&#32780;&#19981;&#26159;&#22495;&#29305;&#23450;&#23376;&#32593;&#32476;&#26469;&#20445;&#30041;&#22495;&#29305;&#23450;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14950</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#21407;&#22411;&#30340;&#22343;&#20540;&#25945;&#24072;&#30340;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Domain Adaptation for Object Detection with Prototype-based Mean-teacher. (arXiv:2309.14950v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14950
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prototype-based Mean-Teacher (PMT)&#30340;&#26032;&#22411;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31867;&#21407;&#22411;&#32780;&#19981;&#26159;&#22495;&#29305;&#23450;&#23376;&#32593;&#32476;&#26469;&#20445;&#30041;&#22495;&#29305;&#23450;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35270;&#35273;&#30446;&#26631;&#26816;&#27979;&#22120;&#36866;&#24212;&#20110;&#25805;&#20316;&#30446;&#26631;&#39046;&#22495;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#20351;&#29992;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;&#24403;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26469;&#33258;&#22810;&#20010;&#28304;&#22495;&#26102;&#65292;&#23558;&#23427;&#20204;&#35270;&#20026;&#21333;&#29420;&#30340;&#22495;&#24182;&#36827;&#34892;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#65292;&#30456;&#27604;&#23558;&#36825;&#20123;&#28304;&#22495;&#28151;&#21512;&#24182;&#36827;&#34892;UDA&#65292;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#36817;&#26399;&#30340;&#30740;&#31350;&#20063;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#29616;&#26377;&#30340;MSDA&#26041;&#27861;&#23398;&#20064;&#22495;&#19981;&#21464;&#21644;&#22495;&#29305;&#23450;&#21442;&#25968;&#65288;&#23545;&#20110;&#27599;&#20010;&#28304;&#22495;&#65289;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#28982;&#32780;&#65292;&#19982;&#21333;&#28304;UDA&#26041;&#27861;&#19981;&#21516;&#65292;&#23398;&#20064;&#22495;&#29305;&#23450;&#21442;&#25968;&#20351;&#23427;&#20204;&#19982;&#20351;&#29992;&#30340;&#28304;&#22495;&#25968;&#37327;&#25104;&#27491;&#27604;&#22686;&#38271;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#21407;&#22411;&#30340;&#22343;&#20540;&#25945;&#24072;&#65288;PMT&#65289;&#30340;&#26032;&#22411;MSDA&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#31867;&#21407;&#22411;&#32780;&#19981;&#26159;&#22495;&#29305;&#23450;&#23376;&#32593;&#32476;&#26469;&#20445;&#30041;&#22495;&#29305;&#23450;&#20449;&#24687;&#12290;&#36825;&#20123;&#21407;&#22411;&#26159;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#23398;&#20064;&#30340;&#65292;&#23545;&#40784;&#30456;&#21516;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting visual object detectors to operational target domains is a challenging task, commonly achieved using unsupervised domain adaptation (UDA) methods. When the labeled dataset is coming from multiple source domains, treating them as separate domains and performing a multi-source domain adaptation (MSDA) improves the accuracy and robustness over mixing these source domains and performing a UDA, as observed by recent studies in MSDA. Existing MSDA methods learn domain invariant and domain-specific parameters (for each source domain) for the adaptation. However, unlike single-source UDA methods, learning domain-specific parameters makes them grow significantly proportional to the number of source domains used. This paper proposes a novel MSDA method called Prototype-based Mean-Teacher (PMT), which uses class prototypes instead of domain-specific subnets to preserve domain-specific information. These prototypes are learned using a contrastive loss, aligning the same categories across 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39046;&#22495;&#38543;&#26426;&#21270;&#21644;&#30446;&#26631;&#36319;&#36394;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#31181;&#23376;&#26680;&#35745;&#25968;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#30340;&#26367;&#20195;&#21697;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26377;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#20302;&#25104;&#26412;&#20272;&#35745;&#35895;&#29289;&#20135;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.05846</link><description>&lt;p&gt;
&#20351;&#29992;&#39046;&#22495;&#38543;&#26426;&#21270;&#21644;&#30446;&#26631;&#36319;&#36394;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31181;&#23376;&#26680;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
Seed Kernel Counting using Domain Randomization and Object Tracking Neural Networks. (arXiv:2308.05846v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39046;&#22495;&#38543;&#26426;&#21270;&#21644;&#30446;&#26631;&#36319;&#36394;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#31181;&#23376;&#26680;&#35745;&#25968;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#30340;&#26367;&#20195;&#21697;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26377;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#20302;&#25104;&#26412;&#20272;&#35745;&#35895;&#29289;&#20135;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36890;&#37327;&#34920;&#22411;&#65288;HTP&#65289;&#23545;&#31181;&#23376;&#30340;&#35780;&#20272;&#26159;&#23545;&#29983;&#38271;&#12289;&#21457;&#32946;&#12289;&#32784;&#21463;&#24615;&#12289;&#25239;&#24615;&#12289;&#29983;&#24577;&#12289;&#20135;&#37327;&#31561;&#22797;&#26434;&#31181;&#23376;&#29305;&#24615;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#20197;&#21450;&#34913;&#37327;&#24418;&#25104;&#26356;&#22797;&#26434;&#29305;&#24615;&#30340;&#21442;&#25968;&#12290;&#31181;&#23376;&#34920;&#22411;&#30340;&#20851;&#38190;&#20043;&#19968;&#26159;&#35895;&#29289;&#20135;&#37327;&#20272;&#35745;&#65292;&#31181;&#23376;&#29983;&#20135;&#34892;&#19994;&#20381;&#36182;&#20110;&#36825;&#19968;&#20272;&#35745;&#26469;&#36827;&#34892;&#19994;&#21153;&#36816;&#20316;&#12290;&#30446;&#21069;&#24066;&#22330;&#19978;&#24050;&#26377;&#26426;&#26800;&#21270;&#30340;&#31181;&#23376;&#26680;&#35745;&#25968;&#22120;&#65292;&#20294;&#20215;&#26684;&#24448;&#24448;&#24456;&#39640;&#65292;&#26377;&#26102;&#36229;&#20986;&#23567;&#35268;&#27169;&#31181;&#23376;&#29983;&#20135;&#20225;&#19994;&#30340;&#25215;&#21463;&#33539;&#22260;&#12290;&#30446;&#26631;&#36319;&#36394;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;(&#22914;YOLO)&#30340;&#21457;&#23637;&#20351;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#33021;&#22815;&#35774;&#35745;&#20986;&#21487;&#20197;&#20302;&#25104;&#26412;&#20272;&#35745;&#35895;&#29289;&#20135;&#37327;&#30340;&#31639;&#27861;&#12290;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20851;&#38190;&#29942;&#39048;&#26159;&#38656;&#35201;&#22823;&#37327;&#26377;&#26631;&#31614;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#25237;&#20837;&#20351;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#20316;&#20026;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-throughput phenotyping (HTP) of seeds, also known as seed phenotyping, is the comprehensive assessment of complex seed traits such as growth, development, tolerance, resistance, ecology, yield, and the measurement of parameters that form more complex traits. One of the key aspects of seed phenotyping is cereal yield estimation that the seed production industry relies upon to conduct their business. While mechanized seed kernel counters are available in the market currently, they are often priced high and sometimes outside the range of small scale seed production firms' affordability. The development of object tracking neural network models such as You Only Look Once (YOLO) enables computer scientists to design algorithms that can estimate cereal yield inexpensively. The key bottleneck with neural network models is that they require a plethora of labelled training data before they can be put to task. We demonstrate that the use of synthetic imagery serves as a feasible substitute t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;U-Net&#36827;&#34892;&#32974;&#20799;&#22836;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;MobileNet&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#24182;&#23545;&#26377;&#38480;&#30340;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30456;&#23218;&#32654;&#30340;&#20998;&#21106;&#24615;&#33021;&#65292;&#19988;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.09067</link><description>&lt;p&gt;
&#35780;&#20272;&#20351;&#29992;U-Net&#36827;&#34892;&#32974;&#20799;&#22836;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24494;&#35843;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image Segmentation with U-Net. (arXiv:2307.09067v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;U-Net&#36827;&#34892;&#32974;&#20799;&#22836;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;MobileNet&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#24182;&#23545;&#26377;&#38480;&#30340;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30456;&#23218;&#32654;&#30340;&#20998;&#21106;&#24615;&#33021;&#65292;&#19988;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32974;&#20799;&#22836;&#20998;&#21106;&#26159;&#27979;&#37327;&#22922;&#23072;&#26399;&#38388;&#32974;&#20799;&#22836;&#22260;(HC)&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#26159;&#30417;&#27979;&#32974;&#20799;&#29983;&#38271;&#30340;&#37325;&#35201;&#29983;&#29289;&#27979;&#23450;&#23398;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#29983;&#25104;&#29983;&#29289;&#23398;&#27979;&#23450;&#26159;&#32791;&#26102;&#19988;&#32467;&#26524;&#19981;&#19968;&#33268;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32454;&#35843;(U-Net&#32593;&#32476;&#21644;&#36731;&#37327;&#32423;&#30340;MobileNet&#20316;&#20026;&#32534;&#30721;&#22120;)&#23545;&#19968;&#32452;&#26377;&#38480;&#30340;&#32974;&#20799;&#22836;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;CNN&#32593;&#32476;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#32454;&#35843;&#31574;&#30053;&#22312;&#35757;&#32451;&#21442;&#25968;&#20943;&#23569;85.8%&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;&#24182;&#19988;&#65292;&#25105;&#20204;&#30340;&#32454;&#35843;&#31574;&#30053;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fetal head segmentation is a crucial step in measuring the fetal head circumference (HC) during gestation, an important biometric in obstetrics for monitoring fetal growth. However, manual biometry generation is time-consuming and results in inconsistent accuracy. To address this issue, convolutional neural network (CNN) models have been utilized to improve the efficiency of medical biometry. But training a CNN network from scratch is a challenging task, we proposed a Transfer Learning (TL) method. Our approach involves fine-tuning (FT) a U-Net network with a lightweight MobileNet as the encoder to perform segmentation on a set of fetal head ultrasound (US) images with limited effort. This method addresses the challenges associated with training a CNN network from scratch. It suggests that our proposed FT strategy yields segmentation performance that is comparable when trained with a reduced number of parameters by 85.8%. And our proposed FT strategy outperforms other strategies with s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#32534;&#30721;&#20132;&#20114;&#26469;&#20934;&#30830;&#19988;&#31616;&#26126;&#22320;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#25512;&#29702;&#36923;&#36753;&#30340;&#26041;&#27861;&#12290;&#38024;&#23545;&#27492;&#30446;&#30340;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21363;AND&#20132;&#20114;&#21644;OR&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#35774;&#35745;&#20986;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13312</link><description>&lt;p&gt;
&#25216;&#26415;&#31508;&#35760;&#65306;&#23450;&#20041;&#21644;&#37327;&#21270;DNN&#30340;AND-OR&#20132;&#20114;&#20197;&#36827;&#34892;&#20934;&#30830;&#21644;&#31616;&#26126;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Technical Note: Defining and Quantifying AND-OR Interactions for Faithful and Concise Explanation of DNNs. (arXiv:2304.13312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#32534;&#30721;&#20132;&#20114;&#26469;&#20934;&#30830;&#19988;&#31616;&#26126;&#22320;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#25512;&#29702;&#36923;&#36753;&#30340;&#26041;&#27861;&#12290;&#38024;&#23545;&#27492;&#30446;&#30340;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21363;AND&#20132;&#20114;&#21644;OR&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#35774;&#35745;&#20986;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#32534;&#30721;&#20132;&#20114;&#26469;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#25512;&#29702;&#36923;&#36753;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#37325;&#26032;&#24605;&#32771;&#20132;&#20114;&#30340;&#23450;&#20041;&#65292;&#28982;&#21518;&#27491;&#24335;&#23450;&#20041;&#20102;&#22522;&#20110;&#20132;&#20114;&#30340;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#31616;&#27905;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21363;AND&#20132;&#20114;&#21644;OR&#20132;&#20114;&#12290;&#38024;&#23545;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;AND&#65288;OR&#65289;&#20132;&#20114;&#22312;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;AND&#65288;OR&#65289;&#20851;&#31995;&#25928;&#24212;&#26041;&#38754;&#30340;&#21807;&#19968;&#24615;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;AND-OR&#20132;&#20114;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#25216;&#26415;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;DNN&#30340;&#25512;&#29702;&#36923;&#36753;&#21487;&#20197;&#36890;&#36807;&#19968;&#32452;&#31526;&#21495;&#27010;&#24565;&#20934;&#30830;&#32780;&#31616;&#26126;&#22320;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this technical note, we aim to explain a deep neural network (DNN) by quantifying the encoded interactions between input variables, which reflects the DNN's inference logic. Specifically, we first rethink the definition of interactions, and then formally define faithfulness and conciseness for interaction-based explanation. To this end, we propose two kinds of interactions, i.e., the AND interaction and the OR interaction. For faithfulness, we prove the uniqueness of the AND (OR) interaction in quantifying the effect of the AND (OR) relationship between input variables. Besides, based on AND-OR interactions, we design techniques to boost the conciseness of the explanation, while not hurting the faithfulness. In this way, the inference logic of a DNN can be faithfully and concisely explained by a set of symbolic concepts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25506;&#27979;&#22120;&#27169;&#25311;&#26041;&#27861;IEA-GAN&#65292;&#36890;&#36807;&#20135;&#29983;&#19982;&#22270;&#23618;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#21270;&#30340;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#36229;&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#21709;&#24212;&#30340;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#26032;&#30340;&#20107;&#20214;&#24863;&#30693;&#25439;&#22833;&#21644;&#32479;&#19968;&#24615;&#25439;&#22833;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08046</link><description>&lt;p&gt;
&#22522;&#20110;&#20107;&#20214;&#24863;&#30693;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#33258;&#30417;&#30563;&#20851;&#31995;&#25512;&#29702;&#30340;&#36229;&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Ultra-High-Resolution Detector Simulation with Intra-Event Aware GAN and Self-Supervised Relational Reasoning. (arXiv:2303.08046v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25506;&#27979;&#22120;&#27169;&#25311;&#26041;&#27861;IEA-GAN&#65292;&#36890;&#36807;&#20135;&#29983;&#19982;&#22270;&#23618;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#21270;&#30340;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#36229;&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#21709;&#24212;&#30340;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#26032;&#30340;&#20107;&#20214;&#24863;&#30693;&#25439;&#22833;&#21644;&#32479;&#19968;&#24615;&#25439;&#22833;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31890;&#23376;&#29289;&#29702;&#23398;&#20013;&#65292;&#27169;&#25311;&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#21709;&#24212;&#19968;&#30452;&#26159;&#19968;&#20010;&#23384;&#20648;&#25104;&#26412;&#39640;&#12289;&#35745;&#31639;&#23494;&#38598;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#20351;&#36825;&#20010;&#36807;&#31243;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#65292;&#20294;&#36229;&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#27169;&#25311;&#20173;&#28982;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#21253;&#21547;&#20102;&#20107;&#20214;&#20869;&#30456;&#20851;&#21644;&#32454;&#31890;&#24230;&#30340;&#30456;&#20114;&#20449;&#24687;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26041;&#27861;&#65288;IEA-GAN&#65289;&#65292;&#34701;&#21512;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#20851;&#31995;&#25512;&#29702;&#27169;&#22411;&#12290;IEA-GAN&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#31995;&#25512;&#29702;&#27169;&#22359;&#65292;&#36817;&#20284;&#20110;&#25506;&#27979;&#22120;&#27169;&#25311;&#20013;&#8220;&#20107;&#20214;&#8221;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#29983;&#25104;&#19982;&#22270;&#23618;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#21270;&#30340;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#36229;&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#21709;&#24212;&#30340;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;IEA-GAN&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#20107;&#20214;&#24863;&#30693;&#25439;&#22833;&#21644;&#32479;&#19968;&#24615;&#25439;&#22833;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;IEA-GAN&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating high-resolution detector responses is a storage-costly and computationally intensive process that has long been challenging in particle physics. Despite the ability of deep generative models to make this process more cost-efficient, ultra-high-resolution detector simulation still proves to be difficult as it contains correlated and fine-grained mutual information within an event. To overcome these limitations, we propose Intra-Event Aware GAN (IEA-GAN), a novel fusion of Self-Supervised Learning and Generative Adversarial Networks. IEA-GAN presents a Relational Reasoning Module that approximates the concept of an ''event'' in detector simulation, allowing for the generation of correlated layer-dependent contextualized images for high-resolution detector responses with a proper relational inductive bias. IEA-GAN also introduces a new intra-event aware loss and a Uniformity loss, resulting in significant enhancements to image fidelity and diversity. We demonstrate IEA-GAN's ap
&lt;/p&gt;</description></item></channel></rss>