<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#35268;&#21010;&#12289;&#20223;&#30495;&#21644;&#20851;&#38190;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#35201;&#36129;&#29486;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#32763;&#35793;&#33021;&#21147;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#39550;&#39542;&#22330;&#26223;&#21019;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01105</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey for Foundation Models in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#35268;&#21010;&#12289;&#20223;&#30495;&#21644;&#20851;&#38190;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#35201;&#36129;&#29486;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#32763;&#35793;&#33021;&#21147;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#39550;&#39542;&#22330;&#26223;&#21019;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21457;&#29983;&#20102;&#38761;&#21629;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#23545;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#23637;&#31034;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#20316;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#35268;&#21010;&#21644;&#20223;&#30495;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#22312;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#20851;&#38190;&#20219;&#21153;&#20013;&#24471;&#21040;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#20197;&#21450;&#20026;&#20223;&#30495;&#21644;&#27979;&#35797;&#21019;&#24314;&#36924;&#30495;&#30340;&#39550;&#39542;&#22330;&#26223;&#12290;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#25972;&#21512;&#22810;&#26679;&#30340;&#36755;&#20837;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#65292;&#23545;&#20110;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20998;&#31867;&#65292;&#26681;&#25454;&#27169;&#24577;&#21644;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#21151;&#33021;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of foundation models has revolutionized the fields of natural language processing and computer vision, paving the way for their application in autonomous driving (AD). This survey presents a comprehensive review of more than 40 research papers, demonstrating the role of foundation models in enhancing AD. Large language models contribute to planning and simulation in AD, particularly through their proficiency in reasoning, code generation and translation. In parallel, vision foundation models are increasingly adapted for critical tasks such as 3D object detection and tracking, as well as creating realistic driving scenarios for simulation and testing. Multi-modal foundation models, integrating diverse inputs, exhibit exceptional visual understanding and spatial reasoning, crucial for end-to-end AD. This survey not only provides a structured taxonomy, categorizing foundation models based on their modalities and functionalities within the AD domain but also delves into the meth
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GiCiSAD&#30340;&#22522;&#20110;&#22270;&#25340;&#22270;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#39592;&#26550;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.12172</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#25340;&#22270;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#20110;&#39592;&#26550;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12172
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GiCiSAD&#30340;&#22522;&#20110;&#22270;&#25340;&#22270;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#39592;&#26550;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39592;&#26550;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#65288;SVAD&#65289;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#20934;&#30830;&#35782;&#21035;&#24322;&#24120;&#27169;&#24335;&#25110;&#20107;&#20214;&#20351;&#25805;&#20316;&#21592;&#33021;&#22815;&#21450;&#26102;&#26816;&#27979;&#21487;&#30097;&#27963;&#21160;&#65292;&#20174;&#32780;&#22686;&#24378;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#26410;&#33021;&#21516;&#26102;&#35299;&#20915;&#36825;&#20123;&#20851;&#38190;&#29305;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#23454;&#29992;&#19988;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#22270;&#25340;&#22270;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#20110;&#39592;&#26550;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#65288;GiCiSAD&#65289;&#65292;&#20197;&#20811;&#26381;&#19982;SVAD&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12172v1 Announce Type: cross  Abstract: Skeleton-based video anomaly detection (SVAD) is a crucial task in computer vision. Accurately identifying abnormal patterns or events enables operators to promptly detect suspicious activities, thereby enhancing safety. Achieving this demands a comprehensive understanding of human motions, both at body and region levels, while also accounting for the wide variations of performing a single action. However, existing studies fail to simultaneously address these crucial properties. This paper introduces a novel, practical and lightweight framework, namely Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection (GiCiSAD) to overcome the challenges associated with SVAD. GiCiSAD consists of three novel modules: the Graph Attention-based Forecasting module to capture the spatio-temporal dependencies inherent in the data, the Graph-level Jigsaw Puzzle Maker module to distinguish subtle region-level discrepancies bet
&lt;/p&gt;</description></item><item><title>Tur[k]ingBench&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22788;&#29702;&#21253;&#21547;&#25991;&#26412;&#25351;&#31034;&#21644;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11905</link><description>&lt;p&gt;
Tur[k]ingBench&#65306;&#29992;&#20110;&#32593;&#32476;&#20195;&#29702;&#30340;&#25361;&#25112;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Tur[k]ingBench: A Challenge Benchmark for Web Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11905
&lt;/p&gt;
&lt;p&gt;
Tur[k]ingBench&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22788;&#29702;&#21253;&#21547;&#25991;&#26412;&#25351;&#31034;&#21644;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#23637;&#31034;&#20102;&#22312;&#21407;&#22987;&#25991;&#26412;&#24418;&#24335;&#19979;&#29702;&#35299;&#21644;&#20132;&#27969;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19990;&#30028;&#19978;&#19981;&#20165;&#20165;&#26159;&#21407;&#22987;&#25991;&#26412;&#12290;&#20363;&#22914;&#65292;&#20154;&#20204;&#22312;&#32593;&#39029;&#19978;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#65292;&#22312;&#36825;&#20123;&#32593;&#39029;&#19978;&#65292;&#25991;&#26412;&#19982;&#20854;&#20182;&#24418;&#24335;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#24182;&#20197;&#21508;&#31181;&#22797;&#26434;&#20114;&#21160;&#30340;&#24418;&#24335;&#23436;&#25104;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25512;&#24191;&#21040;&#36825;&#31181;&#22797;&#26434;&#30340;&#39046;&#22495;&#21602;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TurkingBench&#65292;&#19968;&#20010;&#30001;&#21253;&#21547;&#22810;&#27169;&#24577;&#32972;&#26223;&#30340;&#25991;&#26412;&#35828;&#26126;&#21046;&#23450;&#30340;&#20219;&#21153;&#22522;&#20934;&#12290;&#19982;&#29616;&#26377;&#30340;&#20351;&#29992;&#20154;&#24037;&#21512;&#25104;&#30340;&#32593;&#39029;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#36825;&#37324;&#25105;&#20204;&#20351;&#29992;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#21508;&#31181;&#27880;&#37322;&#30446;&#30340;&#30340;&#33258;&#28982;HTML&#39029;&#38754;&#12290;&#27599;&#20010;&#20219;&#21153;&#30340;HTML&#35828;&#26126;&#20063;&#34987;&#23454;&#20363;&#21270;&#20026;&#21508;&#31181;&#20540;&#65288;&#20174;&#20247;&#21253;&#20219;&#21153;&#33719;&#24471;&#65289;&#20197;&#24418;&#25104;&#20219;&#21153;&#30340;&#26032;&#23454;&#20363;&#12290;&#36825;&#20010;&#22522;&#20934;&#21253;&#21547;32.2K&#20010;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11905v1 Announce Type: new  Abstract: Recent chatbots have demonstrated impressive ability to understand and communicate in raw-text form. However, there is more to the world than raw text. For example, humans spend long hours of their time on web pages, where text is intertwined with other modalities and tasks are accomplished in the form of various complex interactions. Can state-of-the-art multi-modal models generalize to such complex domains?   To address this question, we introduce TurkingBench, a benchmark of tasks formulated as web pages containing textual instructions with multi-modal context. Unlike existing work which employs artificially synthesized web pages, here we use natural HTML pages that were originally designed for crowdsourcing workers for various annotation purposes. The HTML instructions of each task are also instantiated with various values (obtained from the crowdsourcing tasks) to form new instances of the task. This benchmark contains 32.2K instanc
&lt;/p&gt;</description></item><item><title>FastV&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#24182;&#22312;&#21518;&#32493;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06764</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#29255;&#22312;&#31532;&#20108;&#23618;&#20043;&#21518;&#20215;&#20540;1/2&#20195;&#24065;&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21363;&#25554;&#21363;&#29992;&#25512;&#29702;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06764
&lt;/p&gt;
&lt;p&gt;
FastV&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#24182;&#22312;&#21518;&#32493;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#20013;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#23384;&#22312;&#20302;&#25928;&#29616;&#35937;&#65292;&#23588;&#20854;&#26159;&#22312;&#30693;&#21517;&#27169;&#22411;&#22914;LLaVA-1.5&#12289;QwenVL-Chat&#21644;Video-LLaVA&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27969;&#34892;&#30340;LVLMs&#30340;&#28145;&#23618;&#20013;&#65292;&#23545;&#35270;&#35273;&#20195;&#24065;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26497;&#20854;&#20302;&#25928;&#65292;&#26263;&#31034;&#30456;&#36739;&#20110;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#65292;&#38656;&#35201;&#26356;&#31232;&#30095;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FastV&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#26089;&#26399;&#23618;&#20013;&#30340;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#21644;&#22312;&#38543;&#21518;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#26469;&#20248;&#21270;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;FastV&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;LLaVA-1.5-13B&#30340;FLOP&#20943;&#23569;&#20102;45%&#65289;&#65292;&#32780;&#19981;&#20250;&#22312;&#24191;&#27867;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#29306;&#29298;&#24615;&#33021;&#12290;FastV&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#26435;&#34913;&#26159;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#65292;&#24182;&#19988;&#26159;&#24085;&#32047;&#25176;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06764v1 Announce Type: cross  Abstract: In this study, we identify the inefficient attention phenomena in Large Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5, QwenVL-Chat and Video-LLaVA. We find out that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs, suggesting a need for a sparser approach compared to textual data handling. To this end, we introduce FastV, a versatile plug-and-play method designed to optimize computational efficiency by learning adaptive attention patterns in early layers and pruning visual tokens in subsequent ones. Our evaluations demonstrate FastV's ability to dramatically reduce computational costs (e.g., a 45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a wide range of image and video understanding tasks. The computational efficiency and performance trade-off of FastV are highly customizable and pareto-efficient. It can compress t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;MM-Soc&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#23545;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#29702;&#35299;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#36890;&#36807;&#23545;&#21313;&#31181;&#22823;&#23567;&#21464;&#20307;&#30340;&#22235;&#20010;&#24320;&#28304;MLLMs&#36827;&#34892;&#35814;&#23613;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.14154</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#23545;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14154
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;MM-Soc&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#23545;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#29702;&#35299;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#36890;&#36807;&#23545;&#21313;&#31181;&#22823;&#23567;&#21464;&#20307;&#30340;&#22235;&#20010;&#24320;&#28304;MLLMs&#36827;&#34892;&#35814;&#23613;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26159;&#22810;&#27169;&#24577;&#20449;&#24687;&#20132;&#27969;&#30340;&#20013;&#24515;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#29255;&#21644;&#35270;&#39057;&#65292;&#36825;&#20351;&#24471;&#26426;&#22120;&#38590;&#20197;&#29702;&#35299;&#22312;&#32447;&#31354;&#38388;&#20013;&#20132;&#20114;&#25152;&#20851;&#32852;&#30340;&#20449;&#24687;&#25110;&#24773;&#32490;&#12290;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#20934;&#30830;&#35299;&#37322;&#20154;&#31867;&#24773;&#32490;&#21644;&#35832;&#22914;&#34394;&#20551;&#20449;&#24687;&#31561;&#22797;&#26434;&#20869;&#23481;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MM-Soc&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;MLLMs&#23545;&#22810;&#27169;&#24577;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#29702;&#35299;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;MM-Soc&#25972;&#21512;&#20102;&#33879;&#21517;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#24182;&#34701;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#35268;&#27169;YouTube&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#38024;&#23545;&#20174;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#12289;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21040;&#31038;&#20132;&#19978;&#19979;&#25991;&#29983;&#25104;&#31561;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#24320;&#28304;MLLMs&#30340;&#21313;&#31181;&#19981;&#21516;&#35268;&#27169;&#21464;&#20307;&#36827;&#34892;&#35814;&#23613;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#20984;&#26174;&#20986;&#20102;&#23545;&#24615;&#33021;&#24179;&#34913;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14154v1 Announce Type: new  Abstract: Social media platforms are hubs for multimodal information exchange, encompassing text, images, and videos, making it challenging for machines to comprehend the information or emotions associated with interactions in online spaces. Multimodal Large Language Models (MLLMs) have emerged as a promising solution to address these challenges, yet struggle with accurately interpreting human emotions and complex contents like misinformation. This paper introduces MM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of multimodal social media content. MM-Soc compiles prominent multimodal datasets and incorporates a novel large-scale YouTube tagging dataset, targeting a range of tasks from misinformation detection, hate speech detection, and social context generation. Through our exhaustive evaluation on ten size-variants of four open-source MLLMs, we have identified significant performance disparities, highlighting the need
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#25216;&#26415;FRDiff&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26102;&#38388;&#20887;&#20313;&#24615;&#65292;&#37325;&#26032;&#20351;&#29992;&#20855;&#26377;&#39640;&#26102;&#38388;&#30456;&#20284;&#24615;&#30340;&#29305;&#24449;&#22270;&#65292;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#32780;&#19981;&#24433;&#21709;&#36755;&#20986;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2312.03517</link><description>&lt;p&gt;
FRDiff&#65306;&#29305;&#24449;&#37325;&#29992;&#29992;&#20110;&#26080;&#35757;&#32451;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FRDiff : Feature Reuse for Universal Training-free Acceleration of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03517
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#25216;&#26415;FRDiff&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26102;&#38388;&#20887;&#20313;&#24615;&#65292;&#37325;&#26032;&#20351;&#29992;&#20855;&#26377;&#39640;&#26102;&#38388;&#30456;&#20284;&#24615;&#30340;&#29305;&#24449;&#22270;&#65292;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#32780;&#19981;&#24433;&#21709;&#36755;&#20986;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#36739;&#22823;&#35745;&#31639;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#39640;&#36136;&#37327;&#22270;&#20687;&#29983;&#25104;&#25152;&#24517;&#38656;&#30340;&#37325;&#22797;&#21435;&#22122;&#27493;&#39588;&#32780;&#20135;&#29983;&#30340;&#65292;&#36825;&#26159;&#38459;&#30861;&#23427;&#20204;&#24471;&#21040;&#24191;&#27867;&#37319;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#39640;&#32423;&#21152;&#36895;&#25216;&#26415;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#22266;&#26377;&#30340;&#26102;&#38388;&#20887;&#20313;&#24615;&#26469;&#37325;&#26032;&#20351;&#29992;&#20855;&#26377;&#39640;&#26102;&#38388;&#30456;&#20284;&#24615;&#30340;&#29305;&#24449;&#22270;&#65292;&#20174;&#32780;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#32780;&#19981;&#24433;&#21709;&#36755;&#20986;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03517v2 Announce Type: replace-cross  Abstract: The substantial computational costs of diffusion models, especially due to the repeated denoising steps necessary for high-quality image generation, present a major obstacle to their widespread adoption. While several studies have attempted to address this issue by reducing the number of score function evaluations (NFE) using advanced ODE solvers without fine-tuning, the decreased number of denoising iterations misses the opportunity to update fine details, resulting in noticeable quality degradation. In our work, we introduce an advanced acceleration technique that leverages the temporal redundancy inherent in diffusion models. Reusing feature maps with high temporal similarity opens up a new opportunity to save computation resources without compromising output quality. To realize the practical benefits of this intuition, we conduct an extensive analysis and propose a novel method, FRDiff. FRDiff is designed to harness the adv
&lt;/p&gt;</description></item><item><title>GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2401.08396</link><description>&lt;p&gt;
GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#32972;&#21518;&#30340;&#38544;&#34255;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine. (arXiv:2401.08396v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08396
&lt;/p&gt;
&lt;p&gt;
GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;Vision&#21151;&#33021;&#30340;GPT-4&#22312;&#21307;&#23398;&#25361;&#25112;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#21307;&#29983;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35780;&#20272;&#20027;&#35201;&#20851;&#27880;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#20934;&#30830;&#24230;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;GPT-4V&#22312;&#35299;&#20915;&#26032;&#33521;&#26684;&#20848;&#21307;&#23398;&#26434;&#24535;&#22270;&#20687;&#25361;&#25112;&#20013;&#30340;&#22270;&#20687;&#29702;&#35299;&#12289;&#21307;&#23398;&#30693;&#35782;&#22238;&#24518;&#21644;&#36880;&#27493;&#22810;&#27169;&#24577;&#25512;&#29702;&#30340;&#21407;&#29702;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#23454;&#65292;GPT-4V&#22312;&#22810;&#39033;&#36873;&#25321;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20154;&#31867;&#21307;&#29983;&#65288;88.0% vs. 77.0%&#65292;p=0.034&#65289;&#12290;GPT-4V&#22312;&#21307;&#29983;&#22238;&#31572;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#34920;&#29616;&#20986;&#36229;&#36807;80%&#30340;&#20934;&#30830;&#24230;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-4V&#22312;&#26368;&#32456;&#20570;&#20986;&#27491;&#30830;&#36873;&#25321;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#24120;&#25552;&#20379;&#26377;&#32570;&#38519;&#30340;&#25512;&#29702;&#65288;27.3%&#65289;&#65292;&#20854;&#20013;&#26368;&#31361;&#20986;&#30340;&#26159;&#22270;&#20687;&#29702;&#35299;&#65288;21.6%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies indicate that Generative Pre-trained Transformer 4 with Vision (GPT-4V) outperforms human physicians in medical challenge tasks. However, these evaluations primarily focused on the accuracy of multi-choice questions alone. Our study extends the current scope by conducting a comprehensive analysis of GPT-4V's rationales of image comprehension, recall of medical knowledge, and step-by-step multimodal reasoning when solving New England Journal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test the knowledge and diagnostic capabilities of medical professionals. Evaluation results confirmed that GPT-4V outperforms human physicians regarding multi-choice accuracy (88.0% vs. 77.0%, p=0.034). GPT-4V also performs well in cases where physicians incorrectly answer, with over 80% accuracy. However, we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (27.3%), most prominent in image comprehension (21.6
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;NutritionVerse-Synth&#65292;&#36825;&#26159;&#19968;&#20010;&#25317;&#26377;&#22823;&#35268;&#27169;&#21512;&#25104;&#39135;&#29289;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#22810;&#31181;&#35270;&#35282;&#12289;&#27169;&#24577;&#21644;&#39278;&#39135;&#27880;&#37322;&#65292;&#26088;&#22312;&#35299;&#20915;&#30446;&#21069;&#39278;&#39135;&#25668;&#20837;&#20272;&#35745;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#30495;&#23454;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07704</link><description>&lt;p&gt;
NutritionVerse: &#21508;&#31181;&#39278;&#39135;&#25668;&#20837;&#20272;&#35745;&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
NutritionVerse: Empirical Study of Various Dietary Intake Estimation Approaches. (arXiv:2309.07704v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07704
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;NutritionVerse-Synth&#65292;&#36825;&#26159;&#19968;&#20010;&#25317;&#26377;&#22823;&#35268;&#27169;&#21512;&#25104;&#39135;&#29289;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#22810;&#31181;&#35270;&#35282;&#12289;&#27169;&#24577;&#21644;&#39278;&#39135;&#27880;&#37322;&#65292;&#26088;&#22312;&#35299;&#20915;&#30446;&#21069;&#39278;&#39135;&#25668;&#20837;&#20272;&#35745;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#30495;&#23454;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#39278;&#39135;&#25668;&#20837;&#20272;&#35745;&#23545;&#20110;&#25903;&#25345;&#20581;&#24247;&#39278;&#39135;&#30340;&#25919;&#31574;&#21644;&#31243;&#24207;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#33829;&#20859;&#19981;&#33391;&#19982;&#29983;&#27963;&#36136;&#37327;&#19979;&#38477;&#30452;&#25509;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#35832;&#22914;&#39135;&#29289;&#26085;&#35760;&#20043;&#31867;&#30340;&#33258;&#25105;&#25253;&#21578;&#26041;&#27861;&#23384;&#22312;&#26174;&#33879;&#20559;&#24046;&#12290;&#20854;&#20182;&#20256;&#32479;&#30340;&#39278;&#39135;&#35780;&#20272;&#25216;&#26415;&#21644;&#26032;&#20852;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#22914;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#32791;&#26102;&#38271;&#65292;&#24182;&#19988;&#21487;&#33021;&#38656;&#35201;&#21463;&#36807;&#35757;&#32451;&#30340;&#20154;&#21592;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#26469;&#20174;&#39135;&#29289;&#22270;&#20687;&#20013;&#33258;&#21160;&#20272;&#35745;&#39278;&#39135;&#25668;&#20837;&#37327;&#65292;&#20294;&#32570;&#20047;&#20855;&#26377;&#22810;&#26679;&#35270;&#35282;&#12289;&#27169;&#24577;&#21644;&#39135;&#29289;&#27880;&#37322;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#38480;&#21046;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NutritionVerse-Synth&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25317;&#26377;84,984&#20010;&#36924;&#30495;&#30340;&#21512;&#25104;2D&#39135;&#29289;&#22270;&#20687;&#21450;&#30456;&#20851;&#39278;&#39135;&#20449;&#24687;&#21644;&#22810;&#27169;&#24577;&#26631;&#27880;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#28145;&#24230;&#22270;&#20687;&#12289;&#23454;&#20363;&#25513;&#33180;&#21644;&#35821;&#20041;&#25513;&#33180;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate dietary intake estimation is critical for informing policies and programs to support healthy eating, as malnutrition has been directly linked to decreased quality of life. However self-reporting methods such as food diaries suffer from substantial bias. Other conventional dietary assessment techniques and emerging alternative approaches such as mobile applications incur high time costs and may necessitate trained personnel. Recent work has focused on using computer vision and machine learning to automatically estimate dietary intake from food images, but the lack of comprehensive datasets with diverse viewpoints, modalities and food annotations hinders the accuracy and realism of such methods. To address this limitation, we introduce NutritionVerse-Synth, the first large-scale dataset of 84,984 photorealistic synthetic 2D food images with associated dietary information and multimodal annotations (including depth images, instance masks, and semantic masks). Additionally, we col
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;RefSAM&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#32447;&#26041;&#24335;&#20174;&#19981;&#21516;&#26102;&#38388;&#25139;&#30340;&#22810;&#35270;&#22270;&#20449;&#24687;&#20013;&#21152;&#20837;SAM&#30340;&#28508;&#21147;&#65292;&#25506;&#32034;&#20854;&#22312;&#25351;&#20195;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#65288;RVOS&#65289;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#36328;&#27169;&#24577;MLP&#21644;&#20998;&#23618;&#31264;&#23494;&#27880;&#24847;&#27169;&#22359;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;SAM&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#24418;&#24577;&#30340;&#31934;&#30830;&#29702;&#35299;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.00997</link><description>&lt;p&gt;
RefSAM&#65306;&#39640;&#25928;&#36866;&#24212;&#20219;&#20309;&#27169;&#22411;&#30340;&#25351;&#20195;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation. (arXiv:2307.00997v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;RefSAM&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#32447;&#26041;&#24335;&#20174;&#19981;&#21516;&#26102;&#38388;&#25139;&#30340;&#22810;&#35270;&#22270;&#20449;&#24687;&#20013;&#21152;&#20837;SAM&#30340;&#28508;&#21147;&#65292;&#25506;&#32034;&#20854;&#22312;&#25351;&#20195;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#65288;RVOS&#65289;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#36328;&#27169;&#24577;MLP&#21644;&#20998;&#23618;&#31264;&#23494;&#27880;&#24847;&#27169;&#22359;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;SAM&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#24418;&#24577;&#30340;&#31934;&#30830;&#29702;&#35299;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM)&#22240;&#20854;&#22312;&#22270;&#20687;&#20998;&#21106;&#20013;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#25351;&#20195;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#65288;RVOS&#65289;&#26041;&#38754;&#65292;&#30001;&#20110;&#38656;&#35201;&#31934;&#30830;&#30340;&#29992;&#25143;&#20132;&#20114;&#25552;&#31034;&#20197;&#21450;&#23545;&#35821;&#35328;&#21644;&#35270;&#35273;&#31561;&#19981;&#21516;&#24418;&#24577;&#30340;&#26377;&#38480;&#29702;&#35299;&#33021;&#21147;&#65292;SAM&#32570;&#20047;&#29087;&#32451;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;RefSAM&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#32447;&#26041;&#24335;&#20174;&#19981;&#21516;&#26102;&#38388;&#25139;&#30340;&#22810;&#35270;&#22270;&#20449;&#24687;&#20013;&#21152;&#20837;SAM&#30340;&#28508;&#21147;&#65292;&#25506;&#32034;&#20854;&#22312;RVOS&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#21407;&#22987;SAM&#27169;&#22411;&#36827;&#34892;&#20102;&#36866;&#24212;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#36328;&#27169;&#24577;MLP&#23558;&#25351;&#20195;&#34920;&#36798;&#30340;&#25991;&#26412;&#23884;&#20837;&#25237;&#24433;&#20026;&#31232;&#30095;&#21644;&#23494;&#38598;&#23884;&#20837;&#65292;&#20316;&#20026;&#29992;&#25143;&#20132;&#20114;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#36328;&#27169;&#24577;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20998;&#23618;&#31264;&#23494;&#27880;&#24847;&#27169;&#22359;&#65292;&#20197;&#23558;&#20998;&#23618;&#35270;&#35273;&#35821;&#20041;&#20449;&#24687;&#19982;&#31232;&#30095;&#23884;&#20837;&#34701;&#21512;&#65292;&#20197;&#33719;&#24471;&#32454;&#31890;&#24230;&#30340;&#23494;&#38598;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) has gained significant attention for its impressive performance in image segmentation. However, it lacks proficiency in referring video object segmentation (RVOS) due to the need for precise user-interactive prompts and a limited understanding of different modalities, such as language and vision. This paper presents the RefSAM model, which explores the potential of SAM for RVOS by incorporating multi-view information from diverse modalities and successive frames at different timestamps in an online manner. Our proposed approach adapts the original SAM model to enhance cross-modality learning by employing a lightweight Cross-Modal MLP that projects the text embedding of the referring expression into sparse and dense embeddings, serving as user-interactive prompts. Additionally, we have introduced the hierarchical dense attention module to fuse hierarchical visual semantic information with sparse embeddings in order to obtain fine-grained dense embeddings
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Di-Long&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#20013;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#23398;&#29983;&#32593;&#32476;&#35266;&#23519;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#38271;&#36712;&#36857;&#65292;&#25945;&#24072;&#32593;&#32476;&#35266;&#23519;&#26356;&#38271;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#20313;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2305.08553</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#30701;&#26399;&#21040;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Distilling Knowledge for Short-to-Long Term Trajectory Prediction. (arXiv:2305.08553v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Di-Long&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#20013;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#23398;&#29983;&#32593;&#32476;&#35266;&#23519;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#38271;&#36712;&#36857;&#65292;&#25945;&#24072;&#32593;&#32476;&#35266;&#23519;&#26356;&#38271;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#20313;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#19968;&#20010;&#22522;&#26412;&#22256;&#38590;&#22312;&#20110;&#38543;&#30528;&#26102;&#38388;&#33539;&#22260;&#30340;&#22686;&#38271;&#65292;&#36712;&#36857;&#30340;&#28436;&#21464;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Di-Long&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#32473;&#23450;&#19968;&#20010;&#21253;&#21547;&#23398;&#29983;&#32593;&#32476;&#20801;&#35768;&#30340;&#35266;&#27979;&#24207;&#21015;&#21644;&#34917;&#20805;&#30446;&#26631;&#24207;&#21015;&#30340;&#24635;&#24207;&#21015;&#38271;&#24230;&#65292;&#25105;&#20204;&#35753;&#23398;&#29983;&#21644;&#25945;&#24072;&#23545;&#21516;&#19968;&#20010;&#23436;&#25972;&#36712;&#36857;&#23450;&#20041;&#20004;&#20010;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#65306;&#23398;&#29983;&#35266;&#23519;&#19968;&#20010;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#19968;&#20010;&#38271;&#36712;&#36857;&#65292;&#32780;&#25945;&#24072;&#35266;&#23519;&#19968;&#20010;&#26356;&#38271;&#30340;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#19979;&#30340;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term trajectory forecasting is an important and challenging problem in the fields of computer vision, machine learning, and robotics. One fundamental difficulty stands in the evolution of the trajectory that becomes more and more uncertain and unpredictable as the time horizon grows, subsequently increasing the complexity of the problem. To overcome this issue, in this paper, we propose Di-Long, a new method that employs the distillation of a short-term trajectory model forecaster that guides a student network for long-term trajectory prediction during the training process. Given a total sequence length that comprehends the allowed observation for the student network and the complementary target sequence, we let the student and the teacher solve two different related tasks defined over the same full trajectory: the student observes a short sequence and predicts a long trajectory, whereas the teacher observes a longer sequence and predicts the remaining short target trajectory. The
&lt;/p&gt;</description></item></channel></rss>