<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#35813;&#25945;&#31243;&#35752;&#35770;&#20102;&#22270;&#20687;&#21644;&#35270;&#35273;&#39046;&#22495;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#29702;&#24565;&#65292;&#36866;&#21512;&#23545;&#25193;&#25955;&#27169;&#22411;&#30740;&#31350;&#25110;&#24212;&#29992;&#24863;&#20852;&#36259;&#30340;&#26412;&#31185;&#29983;&#21644;&#30740;&#31350;&#29983;&#12290;</title><link>https://arxiv.org/abs/2403.18103</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#20687;&#21644;&#35270;&#35273;&#25193;&#25955;&#27169;&#22411;&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Tutorial on Diffusion Models for Imaging and Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18103
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25945;&#31243;&#35752;&#35770;&#20102;&#22270;&#20687;&#21644;&#35270;&#35273;&#39046;&#22495;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#29702;&#24565;&#65292;&#36866;&#21512;&#23545;&#25193;&#25955;&#27169;&#22411;&#30740;&#31350;&#25110;&#24212;&#29992;&#24863;&#20852;&#36259;&#30340;&#26412;&#31185;&#29983;&#21644;&#30740;&#31350;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#29983;&#25104;&#24037;&#20855;&#30340;&#24778;&#20154;&#22686;&#38271;&#20351;&#24471;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#31561;&#35768;&#22810;&#20196;&#20154;&#20852;&#22859;&#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#20123;&#29983;&#25104;&#24037;&#20855;&#32972;&#21518;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#25193;&#25955;&#27010;&#24565;&#65292;&#19968;&#31181;&#29305;&#27530;&#30340;&#37319;&#26679;&#26426;&#21046;&#65292;&#20811;&#26381;&#20102;&#20197;&#21069;&#26041;&#27861;&#20013;&#34987;&#35748;&#20026;&#22256;&#38590;&#30340;&#19968;&#20123;&#32570;&#28857;&#12290;&#26412;&#25945;&#31243;&#30340;&#30446;&#26631;&#26159;&#35752;&#35770;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#29702;&#24565;&#12290;&#26412;&#25945;&#31243;&#30340;&#30446;&#26631;&#21463;&#20247;&#21253;&#25324;&#23545;&#30740;&#31350;&#25193;&#25955;&#27169;&#22411;&#25110;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#35299;&#20915;&#20854;&#20182;&#38382;&#39064;&#24863;&#20852;&#36259;&#30340;&#26412;&#31185;&#29983;&#21644;&#30740;&#31350;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18103v1 Announce Type: new  Abstract: The astonishing growth of generative tools in recent years has empowered many exciting applications in text-to-image generation and text-to-video generation. The underlying principle behind these generative tools is the concept of diffusion, a particular sampling mechanism that has overcome some shortcomings that were deemed difficult in the previous approaches. The goal of this tutorial is to discuss the essential ideas underlying the diffusion models. The target audience of this tutorial includes undergraduate and graduate students who are interested in doing research on diffusion models or applying these models to solve other problems.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#19968;&#25597;&#23376;&#25216;&#24039;&#26694;&#26550;&#65292;&#23558;&#20843;&#31181;&#20851;&#38190;&#25216;&#26415;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#25913;&#36827;&#20102;&#31283;&#23450;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#25972;&#20307;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.14392</link><description>&lt;p&gt;
&#29992;&#20110;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#19968;&#25597;&#23376;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
A Bag of Tricks for Few-Shot Class-Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14392
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#19968;&#25597;&#23376;&#25216;&#24039;&#26694;&#26550;&#65292;&#23558;&#20843;&#31181;&#20851;&#38190;&#25216;&#26415;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#25913;&#36827;&#20102;&#31283;&#23450;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#25972;&#20307;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19968;&#25597;&#23376;&#25216;&#24039;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#23398;&#20064;&#24418;&#24335;&#65292;&#28041;&#21450;&#23545;&#26032;&#20219;&#21153;&#36827;&#34892;&#36830;&#32493;&#36866;&#24212;&#65292;&#24182;&#19988;&#26679;&#26412;&#26377;&#38480;&#12290; FSCIL &#38656;&#35201;&#20445;&#25345;&#31283;&#23450;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#21363;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20445;&#25345;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#19968;&#25597;&#23376;&#25216;&#24039;&#23558;&#20843;&#31181;&#20851;&#38190;&#19988;&#20855;&#26377;&#39640;&#24433;&#21709;&#21147;&#30340;&#25216;&#26415;&#27719;&#38598;&#22312;&#19968;&#36215;&#65292;&#38024;&#23545; FSCIL &#22312;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#19979;&#25913;&#36827;&#31283;&#23450;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25216;&#24039;&#32452;&#32455;&#25104;&#19977;&#31867;&#65306;&#31283;&#23450;&#24615;&#25216;&#24039;&#12289;&#36866;&#24212;&#24615;&#25216;&#24039;&#21644;&#35757;&#32451;&#25216;&#24039;&#12290;&#31283;&#23450;&#24615;&#25216;&#24039;&#26088;&#22312;&#36890;&#36807;&#22686;&#24378;&#24050;&#23398;&#20064;&#31867;&#21035;&#30340;&#23884;&#20837;&#20043;&#38388;&#30340;&#20998;&#31163;&#21644;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#26102;&#26368;&#23567;&#21270;&#24178;&#25200;&#26469;&#20943;&#36731;&#20808;&#21069;&#23398;&#20064;&#31867;&#21035;&#30340;&#36951;&#24536;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36866;&#24212;&#24615;&#25216;&#24039;&#20391;&#37325;&#20110;&#26377;&#25928;&#23398;&#20064;&#26032;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14392v1 Announce Type: cross  Abstract: We present a bag of tricks framework for few-shot class-incremental learning (FSCIL), which is a challenging form of continual learning that involves continuous adaptation to new tasks with limited samples. FSCIL requires both stability and adaptability, i.e., preserving proficiency in previously learned tasks while learning new ones. Our proposed bag of tricks brings together eight key and highly influential techniques that improve stability, adaptability, and overall performance under a unified framework for FSCIL. We organize these tricks into three categories: stability tricks, adaptability tricks, and training tricks. Stability tricks aim to mitigate the forgetting of previously learned classes by enhancing the separation between the embeddings of learned classes and minimizing interference when learning new ones. On the other hand, adaptability tricks focus on the effective learning of new classes. Finally, training tricks improv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#36127;&#23545;&#35937;&#24615;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;K+1&#20010;logits&#24182;&#22312;&#23494;&#38598;&#39044;&#27979;&#32467;&#26500;&#20013;&#23884;&#20837;&#65292;&#21487;&#29420;&#31435;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.15374</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#36127;&#23545;&#35937;&#24615;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Outlier detection by ensembling uncertainty with negative objectness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15374
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#36127;&#23545;&#35937;&#24615;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;K+1&#20010;logits&#24182;&#22312;&#23494;&#38598;&#39044;&#27979;&#32467;&#26500;&#20013;&#23884;&#20837;&#65292;&#21487;&#29420;&#31435;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#30417;&#30563;&#24335;&#35270;&#35273;&#35782;&#21035;&#20013;&#20851;&#38190;&#30340;&#21151;&#33021;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#36807;&#40723;&#21169;&#26631;&#20934;&#23553;&#38381;&#38598;&#27169;&#22411;&#22312;&#36127;&#35757;&#32451;&#25968;&#25454;&#20013;&#20135;&#29983;&#20302;&#32622;&#20449;&#24230;&#39044;&#27979;&#26469;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#28151;&#28102;&#20102;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#36127;&#31867;&#21035;&#30340;&#35782;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#30452;&#25509;&#39044;&#27979;K+1&#20010;logits&#65292;&#36825;&#20123;logits&#23545;&#24212;&#20110;K&#20010;&#22522;&#26412;&#30495;&#23454;&#31867;&#21035;&#21644;&#19968;&#20010;&#24322;&#24120;&#31867;&#21035;&#12290;&#36825;&#31181;&#35774;&#32622;&#20801;&#35768;&#25105;&#20204;&#21046;&#23450;&#19968;&#31181;&#26032;&#22855;&#30340;&#24322;&#24120;&#24471;&#20998;&#65292;&#20316;&#20026;&#20998;&#24067;&#20869;&#19981;&#30830;&#23450;&#24615;&#21644;&#24322;&#24120;&#31867;&#21035;&#30340;&#21518;&#39564;&#30340;&#38598;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#36127;&#23545;&#35937;&#24615;&#12290;&#29616;&#22312;&#65292;&#24322;&#24120;&#20540;&#21487;&#20197;&#36890;&#36807;&#39640;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25110;&#19982;&#36127;&#25968;&#25454;&#30456;&#20284;&#20043;&#22788;&#29420;&#31435;&#26816;&#27979;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#23884;&#20837;&#21040;&#19968;&#20010;&#23494;&#38598;&#39044;&#27979;&#32467;&#26500;&#20013;&#65292;&#35813;&#32467;&#26500;&#20855;&#26377;K+2&#20010;&#31867;&#21035;&#30340;&#25513;&#30721;&#32423;&#21035;&#35782;&#21035;&#12290;&#35757;&#32451;&#36807;&#31243;&#40723;&#21169;&#26032;&#39062;&#30340;K+2-th&#31867;&#21035;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15374v1 Announce Type: cross  Abstract: Outlier detection is an essential capability in safety-critical applications of supervised visual recognition. Most of the existing methods deliver best results by encouraging standard closed-set models to produce low-confidence predictions in negative training data. However, that approach conflates prediction uncertainty with recognition of the negative class. We therefore reconsider direct prediction of K+1 logits that correspond to K groundtruth classes and one outlier class. This setup allows us to formulate a novel anomaly score as an ensemble of in-distribution uncertainty and the posterior of the outlier class which we term negative objectness. Now outliers can be independently detected due to i) high prediction uncertainty or ii) similarity with negative data. We embed our method into a dense prediction architecture with mask-level recognition over K+2 classes. The training procedure encourages the novel K+2-th class to learn n
&lt;/p&gt;</description></item><item><title>MIM-Refiner&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;MIM&#27169;&#22411;&#20013;&#30340;&#20013;&#38388;&#23618;&#34920;&#31034;&#21644;&#22810;&#20010;&#23545;&#27604;&#22836;&#65292;&#33021;&#22815;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#65292;&#24182;&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10093</link><description>&lt;p&gt;
MIM-Refiner&#65306;&#19968;&#31181;&#20174;&#20013;&#38388;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#33719;&#24471;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10093
&lt;/p&gt;
&lt;p&gt;
MIM-Refiner&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;MIM&#27169;&#22411;&#20013;&#30340;&#20013;&#38388;&#23618;&#34920;&#31034;&#21644;&#22810;&#20010;&#23545;&#27604;&#22836;&#65292;&#33021;&#22815;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#65292;&#24182;&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;MIM-Refiner&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#35757;&#32451;MIM&#27169;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#12290;MIM-Refiner&#30340;&#21160;&#26426;&#22312;&#20110;MIM&#27169;&#22411;&#20013;&#30340;&#26368;&#20339;&#34920;&#31034;&#36890;&#24120;&#20301;&#20110;&#20013;&#38388;&#23618;&#12290;&#22240;&#27492;&#65292;MIM-Refiner&#21033;&#29992;&#36830;&#25509;&#21040;&#19981;&#21516;&#20013;&#38388;&#23618;&#30340;&#22810;&#20010;&#23545;&#27604;&#22836;&#12290;&#22312;&#27599;&#20010;&#22836;&#20013;&#65292;&#20462;&#25913;&#21518;&#30340;&#26368;&#36817;&#37051;&#30446;&#26631;&#24110;&#21161;&#26500;&#24314;&#30456;&#24212;&#30340;&#35821;&#20041;&#32858;&#31867;&#12290;&#27492;&#36807;&#31243;&#30701;&#32780;&#26377;&#25928;&#65292;&#22312;&#20960;&#20010;epochs&#20869;&#65292;&#25105;&#20204;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#12290;&#20351;&#29992;data2vec 2.0&#22312;ImageNet-1K&#19978;&#39044;&#35757;&#32451;&#30340;ViT-H&#32463;&#36807;&#25913;&#36827;&#21518;&#65292;&#22312;&#32447;&#24615;&#25506;&#27979;&#21644;&#20302;&#26679;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65288;&#20998;&#21035;&#20026;84.7%&#21644;64.2%&#65289;&#65292;&#36229;&#36807;&#20102;&#22312;ImageNet-1K&#19978;&#39044;&#35757;&#32451;&#30340;&#20854;&#20182;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10093v1 Announce Type: cross  Abstract: We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted in the insight that optimal representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are connected to diverse intermediate layers. In each head, a modified nearest neighbor objective helps to construct respective semantic clusters.   The refinement process is short but effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, achieves new state-of-the-art results in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K 1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%, outperforming larger mo
&lt;/p&gt;</description></item><item><title>MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.02263</link><description>&lt;p&gt;
MixedNUTS: &#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#23454;&#29616;&#26080;&#38656;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02263
&lt;/p&gt;
&lt;p&gt;
MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#24448;&#24448;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#65292;&#38459;&#30861;&#20102;&#40065;&#26834;&#20998;&#31867;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#19982;&#24050;&#35757;&#32451;&#30340;&#22823;&#22411;&#39640;&#24615;&#33021;&#27169;&#22411;&#20860;&#23481;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#27492;&#38656;&#35201;&#25506;&#32034;&#26080;&#38656;&#35757;&#32451;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#40065;&#26834;&#27169;&#22411;&#22312;&#24178;&#20928;&#25968;&#25454;&#21644;&#23545;&#25239;&#25968;&#25454;&#19978;&#30340;&#27491;&#30830;&#39044;&#27979;&#27604;&#38169;&#35823;&#39044;&#27979;&#26356;&#33258;&#20449;&#65292;&#25105;&#20204;&#25512;&#27979;&#36890;&#36807;&#22686;&#24378;&#36825;&#31181;&#8220;&#33391;&#24615;&#32622;&#20449;&#24230;&#29305;&#24615;&#8221;&#21487;&#20197;&#22312;&#38598;&#25104;&#29615;&#22659;&#20013;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;MixedNUTS&#8221;&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20165;&#26377;&#19977;&#20010;&#21442;&#25968;&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#26469;&#22788;&#29702;&#40065;&#26834;&#20998;&#31867;&#22120;&#21644;&#26631;&#20934;&#38750;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;Logits&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#21518;&#65292;MixedNUTS&#23558;&#36716;&#25442;&#21518;&#30340;Logits&#36716;&#25442;&#20026;&#27010;&#29575;&#65292;&#24182;&#23558;&#23427;&#20204;&#28151;&#21512;&#20316;&#20026;&#26368;&#32456;&#30340;&#36755;&#20986;&#12290;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness often comes at the cost of degraded accuracy, impeding the real-life application of robust classification models. Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches. Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this "benign confidence property" can reconcile accuracy and robustness in an ensemble setting. To achieve so, we propose "MixedNUTS", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm. MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet da
&lt;/p&gt;</description></item><item><title>CMMMU&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#20026;&#22635;&#34917;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#35780;&#20272;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31354;&#30333;&#32780;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2401.11944</link><description>&lt;p&gt;
CMMMU&#65306;&#19968;&#20010;&#20013;&#22269;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11944
&lt;/p&gt;
&lt;p&gt;
CMMMU&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#20026;&#22635;&#34917;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#35780;&#20272;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31354;&#30333;&#32780;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;(LMMs)&#30340;&#33021;&#21147;&#19981;&#26029;&#25552;&#21319;&#65292;&#35780;&#20272;LMMs&#30340;&#34920;&#29616;&#26085;&#30410;&#25104;&#20026;&#19968;&#20010;&#36843;&#20999;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#22312;&#35780;&#20272;LMMs&#22312;&#20013;&#25991;&#31561;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26356;&#22823;&#24046;&#36317;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CMMMU&#65292;&#19968;&#20010;&#26032;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;LMMs&#22312;&#38656;&#35201;&#22823;&#23398;&#27700;&#24179;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;CMMMU&#21463;&#21040;&#20102;MMMUs&#30340;&#26631;&#27880;&#21644;&#20998;&#26512;&#27169;&#24335;&#30340;&#21551;&#21457;&#24182;&#20005;&#26684;&#36981;&#24490;&#12290;CMMMU&#21253;&#25324;&#26469;&#33258;&#22823;&#23398;&#32771;&#35797;&#12289;&#27979;&#39564;&#21644;&#25945;&#31185;&#20070;&#30340;1.2&#19975;&#20010;&#25163;&#21160;&#25910;&#38598;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#28085;&#30422;&#20845;&#20010;&#26680;&#24515;&#23398;&#31185;&#65306;&#33402;&#26415;&#19982;&#35774;&#35745;&#12289;&#21830;&#19994;&#12289;&#31185;&#23398;&#12289;&#20581;&#24247;&#19982;&#21307;&#23398;&#12289;&#20154;&#25991;&#31038;&#31185;&#20197;&#21450;&#25216;&#26415;&#19982;&#24037;&#31243;&#65292;&#23601;&#20687;&#20854;&#20249;&#20276;MMMMU&#19968;&#26679;&#12290;&#36825;&#20123;&#38382;&#39064;&#28085;&#30422;30&#20010;&#23398;&#31185;&#65292;&#21253;&#25324;39&#20010;&#39640;&#24230;&#24322;&#36136;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11944v2 Announce Type: replace-cross  Abstract: As the capabilities of large multimodal models (LMMs) continue to advance, evaluating the performance of LMMs emerges as an increasing need. Additionally, there is an even larger gap in evaluating the advanced knowledge and reasoning abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU, a new Chinese Massive Multi-discipline Multimodal Understanding benchmark designed to evaluate LMMs on tasks demanding college-level subject knowledge and deliberate reasoning in a Chinese context. CMMMU is inspired by and strictly follows the annotation and analysis pattern of MMMU.   CMMMU includes 12k manually collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art &amp; Design, Business, Science, Health &amp; Medicine, Humanities &amp; Social Science, and Tech &amp; Engineering, like its companion, MMMU. These questions span 30 subjects and comprise 39 highly heterogeneous image 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer&#65288;DEFormer&#65289;&#65292;&#21487;&#20197;&#22312;&#20302;&#20809;&#22270;&#20687;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#32454;&#33410;&#65292;&#36890;&#36807;&#24341;&#20837;&#39057;&#29575;&#20316;&#20026;&#26032;&#30340;&#32447;&#32034;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#39057;&#29575;&#20998;&#25903;&#65288;LFB&#65289;&#21644;&#22522;&#20110;&#26354;&#29575;&#30340;&#39057;&#29575;&#22686;&#24378;&#65288;CFE&#65289;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20132;&#21449;&#22495;&#34701;&#21512;&#65288;CDF&#65289;&#26469;&#20943;&#23569;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;DEFormer&#36824;&#21487;&#20197;&#20316;&#20026;&#26263;&#37096;&#26816;&#27979;&#30340;&#39044;&#22788;&#29702;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06941</link><description>&lt;p&gt;
DEFormer: &#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#21644;&#26263;&#35270;&#35273;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer
&lt;/p&gt;
&lt;p&gt;
DEFormer: DCT-driven Enhancement Transformer for Low-light Image and Dark Vision. (arXiv:2309.06941v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06941
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer&#65288;DEFormer&#65289;&#65292;&#21487;&#20197;&#22312;&#20302;&#20809;&#22270;&#20687;&#20013;&#24674;&#22797;&#20002;&#22833;&#30340;&#32454;&#33410;&#65292;&#36890;&#36807;&#24341;&#20837;&#39057;&#29575;&#20316;&#20026;&#26032;&#30340;&#32447;&#32034;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#39057;&#29575;&#20998;&#25903;&#65288;LFB&#65289;&#21644;&#22522;&#20110;&#26354;&#29575;&#30340;&#39057;&#29575;&#22686;&#24378;&#65288;CFE&#65289;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#20132;&#21449;&#22495;&#34701;&#21512;&#65288;CDF&#65289;&#26469;&#20943;&#23569;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;DEFormer&#36824;&#21487;&#20197;&#20316;&#20026;&#26263;&#37096;&#26816;&#27979;&#30340;&#39044;&#22788;&#29702;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#30446;&#26631;&#26159;&#24674;&#22797;&#22270;&#20687;&#30340;&#39068;&#33394;&#21644;&#32454;&#33410;&#65292;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#39640;&#32423;&#35270;&#35273;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#38752;RGB&#39046;&#22495;&#24456;&#38590;&#24674;&#22797;&#26263;&#21306;&#22495;&#30340;&#20002;&#22833;&#32454;&#33410;&#12290;&#26412;&#25991;&#23558;&#39057;&#29575;&#20316;&#20026;&#32593;&#32476;&#30340;&#26032;&#32447;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DCT&#39537;&#21160;&#22686;&#24378;Transformer&#65288;DEFormer&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#39057;&#29575;&#20998;&#25903;&#65288;LFB&#65289;&#29992;&#20110;&#39057;&#29575;&#22686;&#24378;&#65292;&#21253;&#25324;DCT&#22788;&#29702;&#21644;&#22522;&#20110;&#26354;&#29575;&#30340;&#39057;&#29575;&#22686;&#24378;&#65288;CFE&#65289;&#12290;CFE&#35745;&#31639;&#27599;&#20010;&#36890;&#36947;&#30340;&#26354;&#29575;&#20197;&#34920;&#31034;&#19981;&#21516;&#39057;&#29575;&#24102;&#30340;&#32454;&#33410;&#20016;&#23500;&#24230;&#65292;&#28982;&#21518;&#25105;&#20204;&#23558;&#39057;&#29575;&#29305;&#24449;&#21010;&#20998;&#20026;&#26356;&#20016;&#23500;&#32441;&#29702;&#30340;&#39057;&#29575;&#24102;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#21449;&#22495;&#34701;&#21512;&#65288;CDF&#65289;&#26469;&#20943;&#23569;RGB&#39046;&#22495;&#21644;&#39057;&#29575;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23558;DEFormer&#20316;&#20026;&#26263;&#37096;&#26816;&#27979;&#30340;&#39044;&#22788;&#29702;&#65292;DEFormer&#26377;&#25928;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of low-light image enhancement is to restore the color and details of the image and is of great significance for high-level visual tasks in autonomous driving. However, it is difficult to restore the lost details in the dark area by relying only on the RGB domain. In this paper we introduce frequency as a new clue into the network and propose a novel DCT-driven enhancement transformer (DEFormer). First, we propose a learnable frequency branch (LFB) for frequency enhancement contains DCT processing and curvature-based frequency enhancement (CFE). CFE calculates the curvature of each channel to represent the detail richness of different frequency bands, then we divides the frequency features, which focuses on frequency bands with richer textures. In addition, we propose a cross domain fusion (CDF) for reducing the differences between the RGB domain and the frequency domain. We also adopt DEFormer as a preprocessing in dark detection, DEFormer effectively improves the performance
&lt;/p&gt;</description></item><item><title>PointLLM&#26159;&#19968;&#31181;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#24378;&#22823;&#30340;LLM&#23558;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16911</link><description>&lt;p&gt;
PointLLM&#65306;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PointLLM: Empowering Large Language Models to Understand Point Clouds. (arXiv:2308.16911v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16911
&lt;/p&gt;
&lt;p&gt;
PointLLM&#26159;&#19968;&#31181;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#24378;&#22823;&#30340;LLM&#23558;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#65292;&#20294;&#22312;3D&#29702;&#35299;&#39046;&#22495;&#20173;&#26377;&#24453;&#23436;&#20840;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PointLLM&#65292;&#36825;&#26159;&#19968;&#39033;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#30340;&#21021;&#27493;&#24037;&#20316;&#65292;&#20351;LLM&#33021;&#22815;&#29702;&#35299;&#28857;&#20113;&#65292;&#24182;&#25552;&#20379;&#20102;&#36229;&#36234;2D&#35270;&#35273;&#25968;&#25454;&#30340;&#26032;&#36884;&#24452;&#12290;PointLLM&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#22788;&#29702;&#24102;&#26377;&#39068;&#33394;&#30340;&#29289;&#20307;&#28857;&#20113;&#65292;&#24182;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#28857;&#20113;&#21644;&#24120;&#35782;&#30340;&#25484;&#25569;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#20010;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#24378;&#22823;&#30340;LLM&#65292;&#26377;&#25928;&#22320;&#34701;&#21512;&#20102;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;66&#19975;&#20010;&#31616;&#21333;&#21644;7&#19975;&#20010;&#22797;&#26434;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#65292;&#20197;&#23454;&#29616;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65306;&#39318;&#20808;&#23545;&#40784;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#23545;&#32479;&#19968;&#27169;&#22411;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#12290;&#20026;&#20102;&#20005;&#26684;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unprecedented advancements in Large Language Models (LLMs) have created a profound impact on natural language processing but are yet to fully embrace the realm of 3D understanding. This paper introduces PointLLM, a preliminary effort to fill this gap, thereby enabling LLMs to understand point clouds and offering a new avenue beyond 2D visual data. PointLLM processes colored object point clouds with human instructions and generates contextually appropriate responses, illustrating its grasp of point clouds and common sense. Specifically, it leverages a point cloud encoder with a powerful LLM to effectively fuse geometric, appearance, and linguistic information. We collect a novel dataset comprising 660K simple and 70K complex point-text instruction pairs to enable a two-stage training strategy: initially aligning latent spaces and subsequently instruction-tuning the unified model. To rigorously evaluate our model's perceptual abilities and its generalization capabilities, we establis
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#35813;&#21407;&#29702;&#21487;&#20197;&#22788;&#29702;&#27169;&#22411;&#20803;&#32032;&#19981;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#65292;&#24182;&#20248;&#20110;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#22823;&#29109;&#26041;&#27861;&#12290;&#21516;&#26102;&#23558;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#29992;&#20316;&#19981;&#30830;&#23450;&#26426;&#22120;&#29109;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.09868</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;
&lt;/p&gt;
&lt;p&gt;
The Principle of Uncertain Maximum Entropy. (arXiv:2305.09868v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09868
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#35813;&#21407;&#29702;&#21487;&#20197;&#22788;&#29702;&#27169;&#22411;&#20803;&#32032;&#19981;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#65292;&#24182;&#20248;&#20110;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#22823;&#29109;&#26041;&#27861;&#12290;&#21516;&#26102;&#23558;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#29992;&#20316;&#19981;&#30830;&#23450;&#26426;&#22120;&#29109;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#29109;&#21407;&#29702;&#22312;&#20449;&#24687;&#29702;&#35770;&#20013;&#30340;&#24341;&#20837;&#65292;&#20026;&#32479;&#35745;&#21147;&#23398;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#29983;&#24577;&#23398;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#20854;&#24471;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20652;&#21270;&#21058;&#65292;&#20419;&#36827;&#30740;&#31350;&#20154;&#21592;&#23558;&#20182;&#20204;&#30340;&#32463;&#39564;&#35266;&#23519;&#26144;&#23556;&#21040;&#33719;&#21462;&#26080;&#20559;&#27169;&#22411;&#65292;&#21516;&#26102;&#21152;&#28145;&#20102;&#23545;&#22797;&#26434;&#31995;&#32479;&#21644;&#29616;&#35937;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#27169;&#22411;&#20803;&#32032;&#19981;&#30452;&#25509;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#23384;&#22312;&#22122;&#22768;&#25110;&#30524;&#37096;&#36974;&#25377;&#30340;&#24773;&#20917;&#19979;&#65292;&#26631;&#20934;&#26368;&#22823;&#29109;&#26041;&#27861;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#21305;&#37197;&#29305;&#24449;&#32422;&#26463;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;&#20316;&#20026;&#19968;&#31181;&#26041;&#27861;&#65292;&#23613;&#31649;&#23384;&#22312;&#20219;&#24847;&#22122;&#22768;&#35266;&#23519;&#65292;&#23427;&#21516;&#26102;&#23558;&#25152;&#26377;&#21487;&#29992;&#20449;&#24687;&#32534;&#30721;&#65292;&#32780;&#19988;&#20248;&#20110;&#19968;&#20123;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#22823;&#29109;&#26041;&#27861;&#30340;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#29992;&#20316;&#19981;&#30830;&#23450;&#26426;&#22120;&#29109;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#22312;&#19982;&#26368;&#22823;&#20284;&#28982;&#31639;&#27861;&#30456;&#27604;&#26102;&#24314;&#31435;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The principle of maximum entropy, as introduced by Jaynes in information theory, has contributed to advancements in various domains such as Statistical Mechanics, Machine Learning, and Ecology. Its resultant solutions have served as a catalyst, facilitating researchers in mapping their empirical observations to the acquisition of unbiased models, whilst deepening the understanding of complex systems and phenomena. However, when we consider situations in which the model elements are not directly observable, such as when noise or ocular occlusion is present, possibilities arise for which standard maximum entropy approaches may fail, as they are unable to match feature constraints. Here we show the Principle of Uncertain Maximum Entropy as a method that both encodes all available information in spite of arbitrarily noisy observations while surpassing the accuracy of some ad-hoc methods. Additionally, we utilize the output of a black-box machine learning model as input into an uncertain ma
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#38376;&#25915;&#20987;&#19968;&#30452;&#26159;&#19968;&#20010;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#25913;&#36827;&#26041;&#27861;&#38656;&#35201;&#24378;&#22823;&#30340;&#25915;&#20987;&#32773;&#33021;&#21147;&#65292;&#22312;&#33021;&#21147;&#21463;&#38480;&#22330;&#26223;&#19979;&#36824;&#27809;&#26377;&#25214;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#21150;&#27861;&#65292;&#27492;&#22806;&#65292;&#27169;&#22411;&#40065;&#26834;&#24615;&#20173;&#28982;&#20540;&#24471;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2304.10985</link><description>&lt;p&gt;
&#22312;&#33021;&#21147;&#21463;&#38480;&#22330;&#26223;&#19979;&#21551;&#21160;&#24378;&#38887;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Launching a Robust Backdoor Attack under Capability Constrained Scenarios. (arXiv:2304.10985v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10985
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#38376;&#25915;&#20987;&#19968;&#30452;&#26159;&#19968;&#20010;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#25913;&#36827;&#26041;&#27861;&#38656;&#35201;&#24378;&#22823;&#30340;&#25915;&#20987;&#32773;&#33021;&#21147;&#65292;&#22312;&#33021;&#21147;&#21463;&#38480;&#22330;&#26223;&#19979;&#36824;&#27809;&#26377;&#25214;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#21150;&#27861;&#65292;&#27492;&#22806;&#65292;&#27169;&#22411;&#40065;&#26834;&#24615;&#20173;&#28982;&#20540;&#24471;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20851;&#38190;&#39046;&#22495;&#30340;&#24212;&#29992;&#19981;&#26029;&#22686;&#21152;&#65292;&#20154;&#20204;&#24320;&#22987;&#25285;&#24515;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#12290;&#30001;&#20110;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#27745;&#26579;&#30340;&#21518;&#38376;&#27169;&#22411;&#22312;&#26222;&#36890;&#29615;&#22659;&#19979;&#21487;&#33021;&#34920;&#29616;&#27491;&#24120;&#65292;&#20294;&#24403;&#36755;&#20837;&#21253;&#21547;&#35302;&#21457;&#22120;&#26102;&#65292;&#20250;&#26174;&#31034;&#20986;&#24694;&#24847;&#34892;&#20026;&#12290;&#30446;&#21069;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#25913;&#21892;&#35302;&#21457;&#22120;&#30340;&#31192;&#23494;&#24615;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#38656;&#35201;&#24378;&#22823;&#30340;&#25915;&#20987;&#32773;&#33021;&#21147;&#65292;&#20363;&#22914;&#23545;&#27169;&#22411;&#32467;&#26500;&#30340;&#20102;&#35299;&#25110;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#25511;&#21046;&#12290;&#30001;&#20110;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#25915;&#20987;&#32773;&#30340;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#65292;&#36825;&#20123;&#25915;&#20987;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#36824;&#26410;&#24471;&#21040;&#20805;&#20998;&#20851;&#27880;&#12290;&#20363;&#22914;&#65292;&#27169;&#22411;&#33976;&#39311;&#24120;&#29992;&#20110;&#31616;&#21270;&#27169;&#22411;&#22823;&#23567;&#65292;&#20294;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20197;&#21069;&#30340;&#35768;&#22810;&#21518;&#38376;&#25915;&#20987;&#22312;&#27169;&#22411;&#33976;&#39311;&#21518;&#22343;&#22833;&#36133;;&#22270;&#20687;&#22686;&#24378;&#25805;&#20316;&#21487;&#20197;&#30772;&#22351;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#20351;&#21518;&#38376;&#25915;&#20987;&#22833;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks continue to be used in critical domains, concerns over their security have emerged. Deep learning models are vulnerable to backdoor attacks due to the lack of transparency. A poisoned backdoor model may perform normally in routine environments, but exhibit malicious behavior when the input contains a trigger. Current research on backdoor attacks focuses on improving the stealthiness of triggers, and most approaches require strong attacker capabilities, such as knowledge of the model structure or control over the training process. These attacks are impractical since in most cases the attacker's capabilities are limited. Additionally, the issue of model robustness has not received adequate attention. For instance, model distillation is commonly used to streamline model size as the number of parameters grows exponentially, and most of previous backdoor attacks failed after model distillation; the image augmentation operations can destroy the trigger and thus disabl
&lt;/p&gt;</description></item></channel></rss>