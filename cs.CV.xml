<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#21551;&#31034;&#65288;Counterfactual Inception&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21453;&#20107;&#23454;&#24605;&#24819;&#26893;&#20837;&#21040;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#20013;&#65292;&#21487;&#20197;&#20943;&#36731;&#24187;&#35273;&#25928;&#24212;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.13513</link><description>&lt;p&gt;
&#22914;&#26524;......&#20250;&#24590;&#26679;&#65311;&#65306;&#21453;&#20107;&#23454;&#21551;&#31034;&#22312;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#20943;&#36731;&#24187;&#35273;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
What if...?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#21551;&#31034;&#65288;Counterfactual Inception&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21453;&#20107;&#23454;&#24605;&#24819;&#26893;&#20837;&#21040;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#20013;&#65292;&#21487;&#20197;&#20943;&#36731;&#24187;&#35273;&#25928;&#24212;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25552;&#39640;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#22788;&#29702;&#24187;&#35273;&#25928;&#24212;&#26041;&#38754;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#27169;&#22411;&#20250;&#29983;&#25104;&#19981;&#27491;&#30830;&#25110;&#26080;&#20851;&#30340;&#21709;&#24212;&#12290;&#27809;&#26377;&#39069;&#22806;&#30340;&#25351;&#23548;&#35843;&#25972;&#33539;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#21551;&#31034;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#24515;&#36873;&#25321;&#30340;&#12289;&#19981;&#23545;&#40784;&#30340;&#21453;&#20107;&#23454;&#20851;&#38190;&#35789;&#23558;&#21453;&#20107;&#23454;&#24605;&#24819;&#26893;&#20837;&#21040;LMMs&#20013;&#12290;&#35813;&#26041;&#27861;&#26681;&#26893;&#20110;&#21453;&#20107;&#23454;&#24605;&#32500;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#35748;&#30693;&#36807;&#31243;&#65292;&#20154;&#31867;&#22312;&#20854;&#20013;&#32771;&#34385;&#26367;&#20195;&#29616;&#23454;&#21644;&#32467;&#26524;&#12290;&#36890;&#36807;&#23558;&#36825;&#31181;&#31867;&#20284;&#20154;&#31867;&#30340;&#25512;&#29702;&#26426;&#21046;&#24212;&#29992;&#21040;LMMs&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20943;&#23569;&#24187;&#35273;&#25928;&#24212;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21452;&#27169;&#24577;&#39564;&#35777;&#36807;&#31243;&#65288;DVP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20005;&#26684;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36873;&#25321;&#35302;&#21457;LMMs&#20013;&#21453;&#20107;&#23454;&#24605;&#32500;&#30340;&#26368;&#20339;&#21453;&#20107;&#23454;&#20851;&#38190;&#35789;&#65292;&#21516;&#26102;&#32771;&#34385;&#35270;&#35273;&#21644;&#35821;&#35328;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;LMMs&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13513v1 Announce Type: cross  Abstract: This paper presents a way of enhancing the reliability of Large Multimodal Models (LMMs) in addressing hallucination effects, where models generate incorrect or unrelated responses. Without additional instruction tuning paradigm, we introduce Counterfactual Inception, a novel method that implants counterfactual thoughts into LMMs using carefully chosen, misaligned counterfactual keywords. This method is grounded in the concept of counterfactual thinking, a cognitive process where humans consider alternative realities and outcomes. By applying this human-like reasoning mechanism to LMMs, we aim to reduce hallucination effects and improve the models' trustworthiness. We also propose Dual-modality Verification Process (DVP), a rigorous framework for selecting optimal counterfactual keywords to trigger counterfactual thinking into LMMs, concurrently considering visual and linguistic context. Our extensive experiments across various LMMs, i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26631;&#35760;&#30340;&#28304;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#37096;&#32626;&#21040;&#20998;&#24067;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#26102;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36830;&#25509;&#24310;&#36831;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#23450;&#21521;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03325</link><description>&lt;p&gt;
&#36830;&#25509;&#24310;&#36831;&#65306;&#21033;&#29992;&#23450;&#21521;&#22686;&#24378;&#26041;&#27861;&#25552;&#39640;&#40065;&#26834;&#24615;&#30340;&#24494;&#35843;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Connect Later: Improving Fine-tuning for Robustness with Targeted Augmentations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26631;&#35760;&#30340;&#28304;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#37096;&#32626;&#21040;&#20998;&#24067;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#26102;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36830;&#25509;&#24310;&#36831;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#23450;&#21521;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#35760;&#30340;&#28304;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;&#37326;&#29983;&#21160;&#29289;&#30456;&#26426;&#38519;&#38449;&#30340;&#26631;&#35760;&#22270;&#20687;&#65289;&#36890;&#24120;&#22312;&#37096;&#32626;&#21040;&#20998;&#24067;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#65288;&#20363;&#22914;&#26032;&#30340;&#30456;&#26426;&#38519;&#38449;&#20301;&#32622;&#30340;&#22270;&#20687;&#65289;&#26102;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#22312;&#23384;&#22312;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#30340;&#22495;&#36866;&#24212;&#35774;&#32622;&#20013;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;&#20363;&#22914;&#36974;&#34109;&#33258;&#32534;&#30721;&#25110;&#23545;&#27604;&#23398;&#20064;&#65289;&#26159;&#32531;&#35299;&#24615;&#33021;&#19979;&#38477;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#39044;&#35757;&#32451;&#21487;&#20197;&#36890;&#36807;&#23558;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#30456;&#36830;&#25509;&#30340;&#36890;&#29992;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65288;&#20363;&#22914;&#36974;&#34109;&#25110;&#21098;&#35009;&#65289;&#26469;&#25552;&#39640;&#20998;&#24067;&#19981;&#21516;&#30340;&#38169;&#35823;&#29575;&#65292;&#21363;&#20351;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#20004;&#20010;&#39046;&#22495;&#30456;&#24046;&#24456;&#36828;&#12290;&#26412;&#25991;&#36890;&#36807;&#30495;&#23454;&#20219;&#21153;&#23637;&#31034;&#20102;&#22312;&#39044;&#35757;&#32451;&#21518;&#36827;&#34892;&#26631;&#20934;&#24494;&#35843;&#24182;&#19981;&#33021;&#25345;&#32493;&#25913;&#21892;&#20998;&#24067;&#19981;&#21516;&#30340;&#38169;&#35823;&#29575;&#65292;&#30456;&#27604;&#22312;&#26631;&#35760;&#30340;&#28304;&#25968;&#25454;&#19978;&#20174;&#22836;&#35757;&#32451;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451;&#26469;&#24212;&#23545;&#20998;&#24067;&#36716;&#21464;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36830;&#25509;&#24310;&#36831;&#65288;Connect Later&#65289;&#65306;&#22312;&#20351;&#29992;&#36890;&#29992;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#39044;&#35757;&#32451;&#21518;&#65292;&#29992;&#22522;&#20110;&#23545;&#30446;&#26631;&#39046;&#22495;&#20102;&#35299;&#30340;&#23450;&#21521;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models trained on a labeled source domain (e.g., labeled images from wildlife camera traps) often generalize poorly when deployed on an out-of-distribution (OOD) target domain (e.g., images from new camera trap locations). In the domain adaptation setting where unlabeled target data is available, self-supervised pretraining (e.g., masked autoencoding or contrastive learning) is a promising method to mitigate this performance drop. Pretraining improves OOD error when the generic data augmentations used (e.g., masking or cropping) connect the source and target domains, which may be far apart in the input space. In this paper, we show on real-world tasks that standard fine-tuning after pretraining does not consistently improve OOD error over simply training from scratch on labeled source data. To better leverage pretraining for distribution shifts, we propose Connect Later: after pretraining with generic augmentations, fine-tune with targeted augmentations designed with knowledge of the d
&lt;/p&gt;</description></item><item><title>&#22312;&#24369;&#30417;&#30563;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#19981;&#21516;&#20110;&#24120;&#35268;&#35748;&#30693;&#30340;&#35266;&#24565;&#65292;&#30740;&#31350;&#21457;&#29616;&#30465;&#30053;&#26579;&#33394;&#26631;&#20934;&#21270;&#21644;&#22270;&#20687;&#22686;&#24378;&#24182;&#19981;&#20250;&#24433;&#21709;&#19979;&#28216;&#20999;&#29255;&#32423;&#21035;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#33021;&#33410;&#30465;&#22823;&#37327;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2311.11772</link><description>&lt;p&gt;
&#19968;&#20010;&#33391;&#22909;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#23601;&#26159;&#20320;&#22312;&#24369;&#30417;&#30563;&#30149;&#29702;&#23398;&#20999;&#29255;&#20998;&#31867;&#20013;&#25152;&#38656;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
A Good Feature Extractor Is All You Need for Weakly Supervised Pathology Slide Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11772
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24369;&#30417;&#30563;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#19981;&#21516;&#20110;&#24120;&#35268;&#35748;&#30693;&#30340;&#35266;&#24565;&#65292;&#30740;&#31350;&#21457;&#29616;&#30465;&#30053;&#26579;&#33394;&#26631;&#20934;&#21270;&#21644;&#22270;&#20687;&#22686;&#24378;&#24182;&#19981;&#20250;&#24433;&#21709;&#19979;&#28216;&#20999;&#29255;&#32423;&#21035;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#33021;&#33410;&#30465;&#22823;&#37327;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35268;&#35748;&#20026;&#26579;&#33394;&#26631;&#20934;&#21270;&#26159;&#35745;&#31639;&#30149;&#29702;&#23398;&#27969;&#31243;&#20013;&#20851;&#38190;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;&#25105;&#20204;&#22312;&#24369;&#30417;&#30563;&#30340;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#29615;&#22659;&#20013;&#23545;&#36825;&#19968;&#20449;&#24565;&#25552;&#20986;&#36136;&#30097;&#65292;&#36825;&#19968;&#20449;&#24565;&#26159;&#30001;&#35757;&#32451;&#22312;&#22810;&#26679;&#21270;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#24378;&#22823;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#20986;&#29616;&#25152;&#28608;&#21169;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#36804;&#20170;&#20026;&#27490;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;&#30149;&#29702;&#23398;&#29305;&#24449;&#25552;&#21462;&#22120;&#36827;&#34892;&#20102;&#26368;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#28041;&#21450;&#20061;&#20010;&#20219;&#21153;&#12289;&#20116;&#20010;&#25968;&#25454;&#38598;&#12289;&#19977;&#20010;&#19979;&#28216;&#26550;&#26500;&#21644;&#21508;&#31181;&#39044;&#22788;&#29702;&#35774;&#32622;&#20013;&#30340;8000&#22810;&#20010;&#35757;&#32451;&#36816;&#34892;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24573;&#30053;&#26579;&#33394;&#26631;&#20934;&#21270;&#21644;&#22270;&#20687;&#22686;&#24378;&#24182;&#19981;&#20250;&#25439;&#23475;&#19979;&#28216;&#20999;&#29255;&#32423;&#21035;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#20250;&#22312;&#20869;&#23384;&#21644;&#35745;&#31639;&#19978;&#24102;&#26469;&#22823;&#37327;&#33410;&#30465;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#20419;&#36827;&#20102;&#30456;&#23545;&#19979;&#28216;&#24615;&#33021;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26368;&#22909;&#30340;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;&#25552;&#21462;&#22120;&#65292;&#24182;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11772v4 Announce Type: replace-cross  Abstract: Stain normalisation is thought to be a crucial preprocessing step in computational pathology pipelines. We question this belief in the context of weakly supervised whole slide image classification, motivated by the emergence of powerful feature extractors trained using self-supervised learning on diverse pathology datasets. To this end, we performed the most comprehensive evaluation of publicly available pathology feature extractors to date, involving more than 8,000 training runs across nine tasks, five datasets, three downstream architectures, and various preprocessing setups. Notably, we find that omitting stain normalisation and image augmentations does not compromise downstream slide-level classification performance, while incurring substantial savings in memory and compute. Using a new evaluation metric that facilitates relative downstream performance comparison, we identify the best publicly available extractors, and sho
&lt;/p&gt;</description></item></channel></rss>