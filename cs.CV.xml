<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;Intrinsic Vision-Language Hallucination&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65292;&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#20135;&#29983;&#30340;&#21407;&#22240;&#21644;&#21453;&#26144;&#12290;</title><link>https://arxiv.org/abs/2403.11116</link><description>&lt;p&gt;
&#21338;&#22763;&#35770;&#25991;&#65306;&#19968;&#20010;&#25552;&#31034;&#30340;&#35270;&#35273;&#24187;&#35273;&#35780;&#20272;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PhD: A Prompted Visual Hallucination Evaluation Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;Intrinsic Vision-Language Hallucination&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65292;&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#20135;&#29983;&#30340;&#21407;&#22240;&#21644;&#21453;&#26144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#22686;&#38271;&#25512;&#21160;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#21457;&#23637;&#12290;&#22312;LLMs&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#24187;&#35273;&#25361;&#25112;&#20063;&#20986;&#29616;&#22312;LVLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;LVLM&#20013;&#30340;&#23545;&#35937;&#24187;&#35273;&#19978;&#65292;&#24573;&#30053;&#20102;LVLM&#24187;&#35273;&#30340;&#22810;&#26679;&#21270;&#31867;&#22411;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22266;&#26377;&#35270;&#35273;&#35821;&#35328;&#24187;&#35273;&#65288;IVL-Hallu&#65289;&#38382;&#39064;&#65292;&#23545;&#23548;&#33268;&#24187;&#35273;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;IVL-Hallu&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#26032;&#39062;&#30340;IVL-Hallu&#20219;&#21153;&#65292;&#24182;&#23558;&#23427;&#20204;&#20998;&#20026;&#22235;&#31181;&#31867;&#22411;&#65306;&#65288;a&#65289;&#23545;&#35937;&#24187;&#35273;&#65292;&#30001;&#20110;&#23545;&#35937;&#30340;&#35823;&#35782;&#21035;&#32780;&#20135;&#29983;&#65292;&#65288;b&#65289;&#23646;&#24615;&#24187;&#35273;&#65292;&#30001;&#20110;&#23646;&#24615;&#30340;&#35823;&#35782;&#21035;&#32780;&#24341;&#36215;&#65292;&#65288;c&#65289;&#22810;&#27169;&#24577;&#20914;&#31361;&#24187;&#35273;&#65292;&#28304;&#33258;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#20043;&#38388;&#30340;&#30683;&#30462;&#65292;&#20197;&#21450;&#65288;d&#65289;&#21453;&#24120;&#35782;&#24187;&#35273;&#65292;&#30001;&#20110;&#23545;&#31435;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11116v1 Announce Type: cross  Abstract: The rapid growth of Large Language Models (LLMs) has driven the development of Large Vision-Language Models (LVLMs). The challenge of hallucination, prevalent in LLMs, also emerges in LVLMs. However, most existing efforts mainly focus on object hallucination in LVLM, ignoring diverse types of LVLM hallucinations. In this study, we delve into the Intrinsic Vision-Language Hallucination (IVL-Hallu) issue, thoroughly analyzing different types of IVL-Hallu on their causes and reflections. Specifically, we propose several novel IVL-Hallu tasks and categorize them into four types: (a) object hallucination, which arises from the misidentification of objects, (b) attribute hallucination, which is caused by the misidentification of attributes, (c) multi-modal conflicting hallucination, which derives from the contradictions between textual and visual information, and (d) counter-common-sense hallucination, which owes to the contradictions betwee
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13019</link><description>&lt;p&gt;
&#36890;&#36807;DeepFool&#31639;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#31867;&#21035;&#25805;&#32437;&#30340;&#23545;&#25239;&#25915;&#20987;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm. (arXiv:2310.13019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#24341;&#36215;&#20102;&#20005;&#37325;&#20851;&#27880;&#12290;&#20102;&#35299;&#36825;&#20123;&#26131;&#21463;&#25915;&#20987;&#24615;&#24182;&#24320;&#21457;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;DeepFool&#26159;Moosavi-Dezfooli&#31561;&#20154;&#65288;2016&#24180;&#65289;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23558;&#36755;&#20837;&#22270;&#20687;&#38169;&#35823;&#20998;&#31867;&#30340;&#26368;&#23567;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;DeepFool&#32570;&#20047;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#29305;&#23450;&#25915;&#20987;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;&#22312;&#20808;&#21069;&#30340;&#30456;&#20851;&#24037;&#20316;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#25104;&#21151;&#29575;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#22270;&#20687;&#34987;&#25197;&#26354;&#30340;&#31243;&#24230;&#12289;&#22270;&#20687;&#36136;&#37327;&#30340;&#23436;&#25972;&#24615;&#20197;&#21450;&#38169;&#35823;&#20998;&#31867;&#30340;&#32622;&#20449;&#24230;&#27700;&#24179;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Targeted DeepFool&#65292;&#36825;&#26159;DeepFool&#30340;&#22686;&#24378;&#29256;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#22686;&#24378;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have significantly advanced various domains, but their vulnerability to adversarial attacks poses serious concerns. Understanding these vulnerabilities and developing effective defense mechanisms is crucial. DeepFool, an algorithm proposed by Moosavi-Dezfooli et al. (2016), finds minimal perturbations to misclassify input images. However, DeepFool lacks a targeted approach, making it less effective in specific attack scenarios. Also, in previous related works, researchers primarily focus on success, not considering how much an image is getting distorted; the integrity of the image quality, and the confidence level to misclassifying. So, in this paper, we propose Targeted DeepFool, an augmented version of DeepFool that allows targeting specific classes for misclassification. We also introduce a minimum confidence score requirement hyperparameter to enhance flexibility. Our experiments demonstrate the effectiveness and efficiency of the proposed method across 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02694</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24230;&#37327;&#26041;&#27861;&#65306;&#19968;&#39033;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Loss Functions and Metrics in Deep Learning. A Review. (arXiv:2307.02694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#36873;&#25321;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#24230;&#37327;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#23427;&#20204;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#35780;&#35770;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#26368;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the essential components of deep learning is the choice of the loss function and performance metrics used to train and evaluate models. This paper reviews the most prevalent loss functions and performance measurements in deep learning. We examine the benefits and limits of each technique and illustrate their application to various deep-learning problems. Our review aims to give a comprehensive picture of the different loss functions and performance indicators used in the most common deep learning tasks and help practitioners choose the best method for their specific task.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24635;&#32467;&#20102;&#22312;&#20581;&#24247;&#39046;&#22495;&#20013;&#35780;&#20272;AI&#31995;&#32479;&#26102;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#22522;&#20934;&#20107;&#23454;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#24573;&#35270;&#20102;&#36825;&#19968;&#28857;&#65292;&#32780;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32479;&#35745;&#27169;&#22411;&#32858;&#21512;&#27880;&#37322;&#30340;&#26694;&#26550;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;AI&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02191</link><description>&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#30340;&#22522;&#20934;&#20107;&#23454;&#19979;&#35780;&#20272;AI&#31995;&#32479;&#65306;&#30382;&#32932;&#30149;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating AI systems under uncertain ground truth: a case study in dermatology. (arXiv:2307.02191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02191
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24635;&#32467;&#20102;&#22312;&#20581;&#24247;&#39046;&#22495;&#20013;&#35780;&#20272;AI&#31995;&#32479;&#26102;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#22522;&#20934;&#20107;&#23454;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#24573;&#35270;&#20102;&#36825;&#19968;&#28857;&#65292;&#32780;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32479;&#35745;&#27169;&#22411;&#32858;&#21512;&#27880;&#37322;&#30340;&#26694;&#26550;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;AI&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23433;&#20840;&#36215;&#35265;&#65292;&#22312;&#37096;&#32626;&#20043;&#21069;&#65292;&#21355;&#29983;&#39046;&#22495;&#30340;AI&#31995;&#32479;&#38656;&#35201;&#32463;&#36807;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#23558;&#20854;&#39044;&#27979;&#32467;&#26524;&#19982;&#20551;&#23450;&#20026;&#30830;&#23450;&#30340;&#22522;&#20934;&#20107;&#23454;&#36827;&#34892;&#39564;&#35777;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24773;&#20917;&#24182;&#38750;&#22914;&#27492;&#65292;&#22522;&#20934;&#20107;&#23454;&#21487;&#33021;&#26159;&#19981;&#30830;&#23450;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#26631;&#20934;&#30340;AI&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36825;&#19968;&#28857;&#34987;&#22823;&#37096;&#20998;&#24573;&#35270;&#20102;&#65292;&#20294;&#26159;&#23427;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#65292;&#22914;&#39640;&#20272;&#26410;&#26469;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#22522;&#20934;&#20107;&#23454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#20551;&#35774;&#23427;&#21487;&#20197;&#20998;&#35299;&#20026;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#27880;&#37322;&#19981;&#30830;&#23450;&#24615;&#26159;&#30001;&#20110;&#32570;&#20047;&#21487;&#38752;&#27880;&#37322;&#65292;&#20197;&#21450;&#30001;&#20110;&#26377;&#38480;&#30340;&#35266;&#27979;&#20449;&#24687;&#32780;&#23548;&#33268;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#30830;&#23450;&#22320;&#32858;&#21512;&#27880;&#37322;&#26102;&#65292;&#36890;&#24120;&#20250;&#24573;&#35270;&#36825;&#31181;&#22522;&#20934;&#20107;&#23454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20363;&#22914;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#25110;&#24179;&#22343;&#20540;&#26469;&#32858;&#21512;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#20351;&#29992;&#32479;&#35745;&#27169;&#22411;&#36827;&#34892;&#27880;&#37322;&#30340;&#32858;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#27880;&#37322;&#30340;&#32858;&#21512;&#26694;&#26550;&#35299;&#37322;&#20026;&#25152;&#35859;&#21487;&#33021;&#24615;&#30340;&#21518;&#39564;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
For safety, AI systems in health undergo thorough evaluations before deployment, validating their predictions against a ground truth that is assumed certain. However, this is actually not the case and the ground truth may be uncertain. Unfortunately, this is largely ignored in standard evaluation of AI models but can have severe consequences such as overestimating the future performance. To avoid this, we measure the effects of ground truth uncertainty, which we assume decomposes into two main components: annotation uncertainty which stems from the lack of reliable annotations, and inherent uncertainty due to limited observational information. This ground truth uncertainty is ignored when estimating the ground truth by deterministically aggregating annotations, e.g., by majority voting or averaging. In contrast, we propose a framework where aggregation is done using a statistical model. Specifically, we frame aggregation of annotations as posterior inference of so-called plausibilities
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;HamNoSys&#31526;&#21495;&#36716;&#25442;&#20026;&#25163;&#35821;&#23039;&#21183;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#24314;&#31435;&#25991;&#26412;&#21644;&#23039;&#21183;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65292;&#21487;&#29992;&#20110;&#19981;&#21516;&#25163;&#35821;&#20043;&#38388;&#30340;&#36890;&#29992;&#32763;&#35793;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36317;&#31163;&#27979;&#37327;&#26041;&#27861;&#21487;&#20197;&#24230;&#37327;&#25163;&#35821;&#23039;&#21183;&#24207;&#21015;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2211.13613</link><description>&lt;p&gt;
Ham2Pose&#65306;&#23558;&#25163;&#35821;&#31526;&#21495;&#36716;&#21270;&#25104;&#23039;&#21183;&#24207;&#21015;&#30340;&#21160;&#30011;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ham2Pose: Animating Sign Language Notation into Pose Sequences. (arXiv:2211.13613v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13613
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;HamNoSys&#31526;&#21495;&#36716;&#25442;&#20026;&#25163;&#35821;&#23039;&#21183;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#24314;&#31435;&#25991;&#26412;&#21644;&#23039;&#21183;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65292;&#21487;&#29992;&#20110;&#19981;&#21516;&#25163;&#35821;&#20043;&#38388;&#30340;&#36890;&#29992;&#32763;&#35793;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36317;&#31163;&#27979;&#37327;&#26041;&#27861;&#21487;&#20197;&#24230;&#37327;&#25163;&#35821;&#23039;&#21183;&#24207;&#21015;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21475;&#35821;&#32763;&#35793;&#25104;&#25163;&#35821;&#23545;&#20110;&#32843;&#21548;&#31038;&#21306;&#20043;&#38388;&#30340;&#24320;&#25918;&#24615;&#20132;&#27969;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#23558;HamNoSys&#65292;&#19968;&#31181;&#35789;&#27719;&#25163;&#35821;&#31526;&#21495;&#65292;&#36716;&#25442;&#20026;&#25163;&#35821;&#23039;&#21183;&#24207;&#21015;&#30340;&#21160;&#30011;&#26041;&#27861;&#12290;&#30001;&#20110;HamNoSys&#26159;&#36890;&#29992;&#35774;&#35745;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19981;&#21463;&#30446;&#26631;&#25163;&#35821;&#38480;&#21046;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#36880;&#28176;&#29983;&#25104;&#23039;&#21183;&#39044;&#27979;&#65292;&#21516;&#26102;&#32771;&#34385;&#23427;&#20204;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#65292;&#20026;&#35757;&#32451;&#36807;&#31243;&#25552;&#20379;&#20102;&#24369;&#30417;&#30563;&#65292;&#24182;&#19988;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20174;&#37096;&#20998;&#21644;&#19981;&#20934;&#30830;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#26102;&#25104;&#21151;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#36317;&#31163;&#27979;&#37327;&#26041;&#27861;&#65292;&#32771;&#34385;&#32570;&#22833;&#20851;&#38190;&#28857;&#65292;&#20351;&#29992;DTW-MJE&#26469;&#27979;&#37327;&#23039;&#21183;&#24207;&#21015;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#20351;&#29992;AUTSL&#36825;&#20010;&#22823;&#35268;&#27169;&#25163;&#35821;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#23427;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#19988;&#23637;&#31034;&#23427;&#21487;&#20197;&#24230;&#37327;&#25163;&#35821;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translating spoken languages into Sign languages is necessary for open communication between the hearing and hearing-impaired communities. To achieve this goal, we propose the first method for animating a text written in HamNoSys, a lexical Sign language notation, into signed pose sequences. As HamNoSys is universal by design, our proposed method offers a generic solution invariant to the target Sign language. Our method gradually generates pose predictions using transformer encoders that create meaningful representations of the text and poses while considering their spatial and temporal information. We use weak supervision for the training process and show that our method succeeds in learning from partial and inaccurate data. Additionally, we offer a new distance measurement that considers missing keypoints, to measure the distance between pose sequences using DTW-MJE. We validate its correctness using AUTSL, a large-scale Sign language dataset, show that it measures the distance betw
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#22312;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#21462;&#24471;&#20102;&#21487;&#38752;&#30340;&#32467;&#26524;&#65292;&#19982;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#30456;&#20284;&#30340;&#27979;&#37327;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#24322;&#24120;&#24515;&#33108;&#22823;&#23567;&#21644;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.04979</link><description>&lt;p&gt;
&#26080;&#26631;&#31614;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Label-free segmentation from cardiac ultrasound using self-supervised learning. (arXiv:2210.04979v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#22312;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#21462;&#24471;&#20102;&#21487;&#38752;&#30340;&#32467;&#26524;&#65292;&#19982;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#30456;&#20284;&#30340;&#27979;&#37327;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#24322;&#24120;&#24515;&#33108;&#22823;&#23567;&#21644;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#30340;&#20998;&#21106;&#21644;&#27979;&#37327;&#23545;&#20110;&#24515;&#33039;&#36229;&#22768;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#36825;&#20123;&#20219;&#21153;&#32791;&#26102;&#19988;&#38590;&#20197;&#37325;&#29616;&#12290;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#25552;&#20379;&#36741;&#21161;&#65292;&#20294;&#26159;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#20154;&#21147;&#36827;&#34892;&#25163;&#21160;&#26631;&#27880;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#32467;&#21512;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20020;&#24202;&#39046;&#22495;&#30693;&#35782;&#21644;&#28145;&#24230;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;450&#20010;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#65288;93000&#24352;&#22270;&#29255;&#65289;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#22312;8393&#20010;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#65288;4476266&#24352;&#22270;&#29255;&#65292;&#24179;&#22343;&#24180;&#40836;61&#23681;&#65292;&#22899;&#24615;&#21344;51%&#65289;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21033;&#29992;&#20998;&#21106;&#32467;&#26524;&#36827;&#34892;&#29983;&#29289;&#27979;&#37327;&#12290;&#25105;&#20204;&#36824;&#23545;&#26469;&#33258;&#39069;&#22806;10030&#21517;&#24739;&#32773;&#30340;&#22806;&#37096;&#22270;&#20687;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#36825;&#20123;&#22270;&#20687;&#20855;&#26377;&#25163;&#21160;&#25551;&#36857;&#30340;&#24038;&#23460;&#20449;&#24687;&#12290;&#22312;&#20960;&#31181;&#19981;&#21516;&#30340;&#27979;&#37327;&#25351;&#26631;&#65288;r2 0.56-0.84&#65289;&#19978;&#65292;&#20020;&#24202;&#27979;&#37327;&#21644;&#25105;&#20204;&#30340;&#27969;&#31243;&#39044;&#27979;&#20043;&#38388;&#30340;r2&#20540;&#19982;&#24050;&#25253;&#36947;&#30340;&#20020;&#24202;&#21307;&#29983;&#20043;&#38388;&#30340;&#21464;&#24322;&#31243;&#24230;&#30456;&#20284;&#65292;&#24182;&#19988;&#19982;&#30417;&#30563;&#23398;&#20064;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;&#26816;&#27979;&#24322;&#24120;&#24515;&#33108;&#22823;&#23567;&#21644;&#21151;&#33021;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#20026;0.85&#65288;&#33539;&#22260;0.71-0.97&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation and measurement of cardiac chambers is critical in cardiac ultrasound but is laborious and poorly reproducible. Neural networks can assist, but supervised approaches require the same laborious manual annotations. We built a pipeline for self-supervised (no manual labels) segmentation combining computer vision, clinical domain knowledge, and deep learning. We trained on 450 echocardiograms (93,000 images) and tested on 8,393 echocardiograms (4,476,266 images; mean 61 years, 51% female), using the resulting segmentations to calculate biometrics. We also tested against external images from an additional 10,030 patients with available manual tracings of the left ventricle. r2 between clinically measured and pipeline-predicted measurements were similar to reported inter-clinician variation and comparable to supervised learning across several different measurements (r2 0.56-0.84). Average accuracy for detecting abnormal chamber size and function was 0.85 (range 0.71-0.97) compar
&lt;/p&gt;</description></item></channel></rss>