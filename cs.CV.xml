<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25913;&#36827;&#20102;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#38750;&#23618;&#27425;Mamba&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#36830;&#32493;2D&#25195;&#25551;&#36807;&#31243;&#21644;&#26041;&#21521;&#24863;&#30693;&#26356;&#26032;&#65292;&#25552;&#39640;&#20102;&#20174;&#20108;&#32500;&#22270;&#20687;&#20013;&#23398;&#20064;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.17695</link><description>&lt;p&gt;
PlainMamba&#65306;&#25913;&#36827;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#38750;&#23618;&#27425;Mamba
&lt;/p&gt;
&lt;p&gt;
PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17695
&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#20102;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#38750;&#23618;&#27425;Mamba&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#36830;&#32493;2D&#25195;&#25551;&#36807;&#31243;&#21644;&#26041;&#21521;&#24863;&#30693;&#26356;&#26032;&#65292;&#25552;&#39640;&#20102;&#20174;&#20108;&#32500;&#22270;&#20687;&#20013;&#23398;&#20064;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;PlainMamba&#65306;&#19968;&#31181;&#31616;&#21333;&#30340;&#38750;&#23618;&#27425;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#65292;&#26088;&#22312;&#29992;&#20110;&#19968;&#33324;&#30340;&#35270;&#35273;&#35782;&#21035;&#12290;&#26368;&#36817;&#30340;Mamba&#27169;&#22411;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#39034;&#24207;&#25968;&#25454;&#19978;SSM&#21487;&#20197;&#19982;&#20854;&#20182;&#26550;&#26500;&#31454;&#20105;&#28608;&#28872;&#65292;&#24182;&#24050;&#21021;&#27493;&#23581;&#35797;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#20687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;Mamba&#30340;&#36873;&#25321;&#24615;&#25195;&#25551;&#36807;&#31243;&#20197;&#36866;&#24212;&#35270;&#35273;&#39046;&#22495;&#65292;&#36890;&#36807;&#65288;i&#65289;&#36890;&#36807;&#30830;&#20445;&#22312;&#25195;&#25551;&#24207;&#21015;&#20013;&#20196;&#29260;&#30456;&#37051;&#26469;&#25913;&#21892;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#36830;&#32493;2D&#25195;&#25551;&#36807;&#31243;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#21551;&#29992;&#27169;&#22411;&#21306;&#20998;&#20196;&#29260;&#30340;&#31354;&#38388;&#20851;&#31995;&#30340;&#26041;&#21521;&#24863;&#30693;&#26356;&#26032;&#65292;&#36890;&#36807;&#32534;&#30721;&#26041;&#21521;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#35774;&#35745;&#26131;&#20110;&#20351;&#29992;&#21644;&#26131;&#20110;&#25193;&#23637;&#65292;&#30001;&#22534;&#21472;&#30456;&#21516;&#30340;PlainMamba&#22359;&#24418;&#25104;&#65292;&#32467;&#26524;&#26159;&#22987;&#32456;&#20855;&#26377;&#24658;&#23450;&#23485;&#24230;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17695v1 Announce Type: cross  Abstract: We present PlainMamba: a simple non-hierarchical state space model (SSM) designed for general visual recognition. The recent Mamba model has shown how SSMs can be highly competitive with other architectures on sequential data and initial attempts have been made to apply it to images. In this paper, we further adapt the selective scanning process of Mamba to the visual domain, enhancing its ability to learn features from two-dimensional images by (i) a continuous 2D scanning process that improves spatial continuity by ensuring adjacency of tokens in the scanning sequence, and (ii) direction-aware updating which enables the model to discern the spatial relations of tokens by encoding directional information. Our architecture is designed to be easy to use and easy to scale, formed by stacking identical PlainMamba blocks, resulting in a model with constant width throughout all layers. The architecture is further simplified by removing the 
&lt;/p&gt;</description></item><item><title>SM4Depth&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#39044;&#22788;&#29702;&#21333;&#20803;&#21644;&#28145;&#24230;&#38388;&#38548;&#31163;&#25955;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21333;&#30446;&#24230;&#37327;&#28145;&#24230;&#20272;&#35745;&#20013;&#30340;&#30456;&#26426;&#25935;&#24863;&#24615;&#12289;&#22330;&#26223;&#31934;&#24230;&#19981;&#19968;&#33268;&#21644;&#25968;&#25454;&#20381;&#36182;&#24615;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08556</link><description>&lt;p&gt;
SM4Depth: &#19968;&#31181;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#23454;&#29616;&#36328;&#22810;&#25668;&#20687;&#22836;&#21644;&#22330;&#26223;&#30340;&#26080;&#32541;&#21333;&#30446;&#24230;&#37327;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
SM4Depth: Seamless Monocular Metric Depth Estimation across Multiple Cameras and Scenes by One Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08556
&lt;/p&gt;
&lt;p&gt;
SM4Depth&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#39044;&#22788;&#29702;&#21333;&#20803;&#21644;&#28145;&#24230;&#38388;&#38548;&#31163;&#25955;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21333;&#30446;&#24230;&#37327;&#28145;&#24230;&#20272;&#35745;&#20013;&#30340;&#30456;&#26426;&#25935;&#24863;&#24615;&#12289;&#22330;&#26223;&#31934;&#24230;&#19981;&#19968;&#33268;&#21644;&#25968;&#25454;&#20381;&#36182;&#24615;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#30446;&#24230;&#37327;&#28145;&#24230;&#20272;&#35745;&#65288;MMDE&#65289;&#30340;&#27867;&#21270;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#30456;&#23545;&#28145;&#24230;&#21644;&#24230;&#37327;&#28145;&#24230;&#25110;&#23545;&#40784;&#36755;&#20837;&#22270;&#20687;&#28966;&#36317;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#30528;&#22312;&#30456;&#26426;&#12289;&#22330;&#26223;&#21644;&#25968;&#25454;&#32423;&#21035;&#19978;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#23545;&#19981;&#21516;&#25668;&#20687;&#22836;&#30340;&#25935;&#24863;&#24615;&#65307;&#65288;2&#65289;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#31934;&#24230;&#19981;&#19968;&#33268;&#65307;&#65288;3&#65289;&#20381;&#36182;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#32541;&#30340;MMDE&#26041;&#27861;SM4Depth&#65292;&#20197;&#22312;&#21333;&#20010;&#32593;&#32476;&#20869;&#35299;&#20915;&#19978;&#36848;&#25152;&#26377;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08556v1 Announce Type: cross  Abstract: The generalization of monocular metric depth estimation (MMDE) has been a longstanding challenge. Recent methods made progress by combining relative and metric depth or aligning input image focal length. However, they are still beset by challenges in camera, scene, and data levels: (1) Sensitivity to different cameras; (2) Inconsistent accuracy across scenes; (3) Reliance on massive training data. This paper proposes SM4Depth, a seamless MMDE method, to address all the issues above within a single network. First, we reveal that a consistent field of view (FOV) is the key to resolve ``metric ambiguity'' across cameras, which guides us to propose a more straightforward preprocessing unit. Second, to achieve consistently high accuracy across scenes, we explicitly model the metric scale determination as discretizing the depth interval into bins and propose variation-based unnormalized depth bins. This method bridges the depth gap of divers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;&#65292;&#22312;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#26102;&#37319;&#29992;&#25913;&#36827;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.07818</link><description>&lt;p&gt;
&#26631;&#31614;&#20002;&#22833;&#29575;&#65306;&#21033;&#29992;&#20855;&#26377;&#22495;&#36716;&#31227;&#21644;&#37096;&#20998;&#26631;&#35760;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Label Dropout: Improved Deep Learning Echocardiography Segmentation Using Multiple Datasets With Domain Shift and Partial Labelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;&#65292;&#22312;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#26102;&#37319;&#29992;&#25913;&#36827;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;&#24515;&#21160;&#22270;&#65288;&#36229;&#22768;&#65289;&#26159;&#35780;&#20272;&#24515;&#33039;&#21151;&#33021;&#26102;&#20351;&#29992;&#30340;&#31532;&#19968;&#31181;&#25104;&#20687;&#26041;&#24335;&#12290;&#20174;&#36229;&#22768;&#20013;&#27979;&#37327;&#21151;&#33021;&#29983;&#29289;&#26631;&#24535;&#29289;&#20381;&#36182;&#20110;&#23545;&#24515;&#33039;&#32467;&#26500;&#36827;&#34892;&#20998;&#21106;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#25552;&#20986;&#26469;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23558;&#36825;&#20123;&#24037;&#20855;&#36716;&#21270;&#20026;&#24191;&#27867;&#30340;&#20020;&#24202;&#24212;&#29992;&#65292;&#37325;&#35201;&#30340;&#26159;&#20998;&#21106;&#27169;&#22411;&#23545;&#21508;&#31181;&#22270;&#20687;&#20855;&#26377;&#40065;&#26834;&#24615;&#65288;&#20363;&#22914;&#65292;&#30001;&#19981;&#21516;&#25195;&#25551;&#20202;&#33719;&#24471;&#65292;&#30001;&#19981;&#21516;&#32423;&#21035;&#30340;&#19987;&#23478;&#25805;&#20316;&#21592;&#33719;&#24471;&#31561;&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#40065;&#26834;&#24615;&#27700;&#24179;&#65292;&#26377;&#24517;&#35201;&#20351;&#29992;&#22810;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#26102;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#26631;&#31614;&#23384;&#22312;&#30340;&#21464;&#21270;&#65292;&#21363;&#21512;&#24182;&#25968;&#25454;&#36890;&#24120;&#26159;&#37096;&#20998;&#26631;&#35760;&#30340;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#30340;&#25913;&#36827;&#26469;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35757;&#32451;&#30340;naively
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07818v1 Announce Type: cross  Abstract: Echocardiography (echo) is the first imaging modality used when assessing cardiac function. The measurement of functional biomarkers from echo relies upon the segmentation of cardiac structures and deep learning models have been proposed to automate the segmentation process. However, in order to translate these tools to widespread clinical use it is important that the segmentation models are robust to a wide variety of images (e.g. acquired from different scanners, by operators with different levels of expertise etc.). To achieve this level of robustness it is necessary that the models are trained with multiple diverse datasets. A significant challenge faced when training with multiple diverse datasets is the variation in label presence, i.e. the combined data are often partially-labelled. Adaptations of the cross entropy loss function have been proposed to deal with partially labelled data. In this paper we show that training naively 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ActGen&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#35757;&#32451;&#24863;&#30693;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#22270;&#20687;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;ActGen&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#30340;&#29702;&#24565;&#65292;&#29983;&#25104;&#31867;&#20284;&#20110;&#25361;&#25112;&#24615;&#25110;&#34987;&#35823;&#20998;&#31867;&#26679;&#26412;&#30340;&#22270;&#20687;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#35757;&#32451;&#38598;&#20013;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06517</link><description>&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#30340;&#20027;&#21160;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Active Generation for Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06517
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ActGen&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#35757;&#32451;&#24863;&#30693;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#22270;&#20687;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;ActGen&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#30340;&#29702;&#24565;&#65292;&#29983;&#25104;&#31867;&#20284;&#20110;&#25361;&#25112;&#24615;&#25110;&#34987;&#35823;&#20998;&#31867;&#26679;&#26412;&#30340;&#22270;&#20687;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#35757;&#32451;&#38598;&#20013;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#19981;&#26029;&#22686;&#24378;&#30340;&#33021;&#21147;&#31361;&#26174;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#35201;&#27714;&#29983;&#25104;&#30340;&#22270;&#20687;&#25968;&#37327;&#36828;&#36828;&#36229;&#36807;&#21407;&#22987;&#25968;&#25454;&#38598;&#65292;&#32780;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#21482;&#26377;&#26497;&#23567;&#30340;&#25913;&#36827;&#12290;&#36825;&#31181;&#35745;&#31639;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#36807;&#31243;&#38459;&#30861;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#19987;&#27880;&#20110;&#27169;&#22411;&#30340;&#20855;&#20307;&#38656;&#27714;&#21644;&#29305;&#24449;&#26469;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;ActGen&#20197;&#20027;&#21160;&#23398;&#20064;&#20026;&#20013;&#24515;&#21407;&#21017;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;&#38024;&#23545;&#35757;&#32451;&#24863;&#30693;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#12290;&#23427;&#26088;&#22312;&#21019;&#24314;&#31867;&#20284;&#20110;&#24403;&#21069;&#27169;&#22411;&#36935;&#21040;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#25110;&#34987;&#35823;&#20998;&#31867;&#26679;&#26412;&#30340;&#22270;&#20687;&#65292;&#24182;&#23558;&#36825;&#20123;&#29983;&#25104;&#30340;&#22270;&#20687;&#32435;&#20837;&#35757;&#32451;&#38598;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06517v1 Announce Type: cross  Abstract: Recently, the growing capabilities of deep generative models have underscored their potential in enhancing image classification accuracy. However, existing methods often demand the generation of a disproportionately large number of images compared to the original dataset, while having only marginal improvements in accuracy. This computationally expensive and time-consuming process hampers the practicality of such approaches. In this paper, we propose to address the efficiency of image generation by focusing on the specific needs and characteristics of the model. With a central tenet of active learning, our method, named ActGen, takes a training-aware approach to image generation. It aims to create images akin to the challenging or misclassified samples encountered by the current model and incorporates these generated images into the training set to augment model performance. ActGen introduces an attentive image guidance technique, usin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#24694;&#21155;&#22825;&#27668;&#30340;&#22810;&#27169;&#22359;&#21644;&#27169;&#22359;&#21270;&#31995;&#32479;&#26550;&#26500;&#65292;&#22312;WATonoBus&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#23454;&#38469;&#27979;&#35797;&#65292;&#35777;&#26126;&#20854;&#33021;&#22815;&#35299;&#20915;&#20840;&#22825;&#20505;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38754;&#20020;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2312.00938</link><description>&lt;p&gt;
WATonoBus&#65306;&#19968;&#31181;&#20840;&#22825;&#20505;&#33258;&#21160;&#24033;&#33322;&#36710;
&lt;/p&gt;
&lt;p&gt;
WATonoBus: An All Weather Autonomous Shuttle
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00938
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#24694;&#21155;&#22825;&#27668;&#30340;&#22810;&#27169;&#22359;&#21644;&#27169;&#22359;&#21270;&#31995;&#32479;&#26550;&#26500;&#65292;&#22312;WATonoBus&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#23454;&#38469;&#27979;&#35797;&#65292;&#35777;&#26126;&#20854;&#33021;&#22815;&#35299;&#20915;&#20840;&#22825;&#20505;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38754;&#20020;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#20840;&#22825;&#20505;&#36816;&#34892;&#20013;&#38754;&#20020;&#26174;&#33879;&#25361;&#25112;&#65292;&#28085;&#30422;&#20102;&#20174;&#24863;&#30693;&#21644;&#20915;&#31574;&#21040;&#36335;&#24452;&#35268;&#21010;&#21644;&#25511;&#21046;&#30340;&#21508;&#20010;&#27169;&#22359;&#12290;&#22797;&#26434;&#24615;&#28304;&#20110;&#38656;&#35201;&#35299;&#20915;&#20687;&#38632;&#12289;&#38634;&#21644;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#22312;&#33258;&#20027;&#24615;&#22534;&#26632;&#20013;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#21644;&#21333;&#27169;&#22359;&#26041;&#27861;&#36890;&#24120;&#32570;&#20047;&#19982;&#19978;&#28216;&#25110;&#19979;&#28216;&#20219;&#21153;&#30340;&#25972;&#20307;&#38598;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#32771;&#34385;&#24694;&#21155;&#22825;&#27668;&#30340;&#22810;&#27169;&#22359;&#21644;&#27169;&#22359;&#21270;&#31995;&#32479;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#24863;&#30693;&#27700;&#24179;&#21040;&#20915;&#31574;&#21644;&#23433;&#20840;&#30417;&#27979;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#20363;&#22914;&#35206;&#30422;&#38634;&#30340;&#36335;&#32536;&#26816;&#27979;&#12290;&#36890;&#36807;&#22312;WATonoBus&#24179;&#21488;&#19978;&#27599;&#21608;&#26085;&#24120;&#26381;&#21153;&#36817;&#19968;&#24180;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#65292;&#24182;&#20174;&#36816;&#33829;&#20013;&#35266;&#23519;&#21040;&#30340;&#26497;&#31471;&#24773;&#20917;&#20013;&#33719;&#24471;&#23453;&#36149;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00938v1 Announce Type: cross  Abstract: Autonomous vehicle all-weather operation poses significant challenges, encompassing modules from perception and decision-making to path planning and control. The complexity arises from the need to address adverse weather conditions like rain, snow, and fog across the autonomy stack. Conventional model-based and single-module approaches often lack holistic integration with upstream or downstream tasks. We tackle this problem by proposing a multi-module and modular system architecture with considerations for adverse weather across the perception level, through features such as snow covered curb detection, to decision-making and safety monitoring. Through daily weekday service on the WATonoBus platform for almost a year, we demonstrate that our proposed approach is capable of addressing adverse weather conditions and provide valuable learning from edge cases observed during operation.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25361;&#25112;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#33539;&#24335;&#65292;&#36890;&#36807;&#30740;&#31350;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#22120;&#36873;&#25321;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#29702;&#35770;&#25512;&#23548;&#20986;&#20102;&#26799;&#24230;&#20914;&#31361;&#30340;&#35282;&#33394;&#12290;</title><link>https://arxiv.org/abs/2311.04698</link><description>&lt;p&gt;
&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#25361;&#25112;&#24120;&#35265;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Challenging Common Paradigms in Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04698
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25361;&#25112;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#33539;&#24335;&#65292;&#36890;&#36807;&#30740;&#31350;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#22120;&#36873;&#25321;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#29702;&#35770;&#25512;&#23548;&#20986;&#20102;&#26799;&#24230;&#20914;&#31361;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#20294;&#20854;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#24182;&#26410;&#24102;&#26469;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#30456;&#27604;&#21333;&#20219;&#21153;&#23398;&#20064;&#65288;STL&#65289;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#26356;&#28145;&#20837;&#20102;&#35299;MTL&#29305;&#23450;&#25361;&#25112;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;MTL&#20013;&#30340;&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#20960;&#28857;&#20851;&#20110;STL&#30340;&#37325;&#35201;&#24433;&#21709;&#65306;&#39318;&#20808;&#65292;&#20248;&#21270;&#22120;&#30340;&#36873;&#25321;&#23545;MTL&#30340;&#24433;&#21709;&#21482;&#21463;&#21040;&#20102;&#36731;&#24494;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#30340;&#23454;&#35777;&#26041;&#27861;&#23637;&#31034;&#20102;&#24120;&#35265;STL&#24037;&#20855;&#65288;&#20363;&#22914;Adam&#20248;&#21270;&#22120;&#65289;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;Adam&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#19968;&#23450;&#30340;&#20551;&#35774;&#19979;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#37096;&#20998;&#25439;&#22833;&#23610;&#24230;&#19981;&#21464;&#24615;&#12290;&#20854;&#27425;&#65292;&#26799;&#24230;&#20914;&#31361;&#30340;&#27010;&#24565;&#32463;&#24120;&#34987;&#25551;&#36848;&#20026;MTL&#20013;&#30340;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#26799;&#24230;&#20914;&#31361;&#22312;MTL&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#23558;&#20854;&#19982;STL&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#35282;&#24230;&#26799;&#24230;&#23545;&#40784;&#26041;&#38754;&#65292;&#25105;&#20204;&#27809;&#26377;&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04698v3 Announce Type: replace-cross  Abstract: While multi-task learning (MTL) has gained significant attention in recent years, its underlying mechanisms remain poorly understood. Recent methods did not yield consistent performance improvements over single task learning (STL) baselines, underscoring the importance of gaining more profound insights about challenges specific to MTL. In our study, we challenge paradigms in MTL in the context of STL: First, the impact of the choice of optimizer has only been mildly investigated in MTL. We show the pivotal role of common STL tools such as the Adam optimizer in MTL empirically in various experiments. To further investigate Adam's effectiveness, we theoretical derive a partial loss-scale invariance under mild assumptions. Second, the notion of gradient conflicts has often been phrased as a specific problem in MTL. We delve into the role of gradient conflicts in MTL and compare it to STL. For angular gradient alignment we find no 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31890;&#24230;&#24341;&#23548;&#26862;&#26519;&#21644;&#22810;&#20013;&#24515;&#25439;&#22833;&#30340;&#38271;&#23614;&#20998;&#31867;&#26694;&#26550;&#65292;&#21517;&#20026;Cognisance&#12290;&#35813;&#26694;&#26550;&#33268;&#21147;&#20110;&#35299;&#20915;&#38271;&#23614;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#31867;&#38388;&#21644;&#31867;&#20869;&#19981;&#24179;&#34913;&#65292;&#24182;&#36890;&#36807;&#19981;&#21464;&#29305;&#24449;&#23398;&#20064;&#26500;&#24314;&#22810;&#31890;&#24230;&#32852;&#21512;&#35299;&#20915;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.08206</link><description>&lt;p&gt;
&#22522;&#20110;&#31895;&#31890;&#24230;&#24341;&#23548;&#26862;&#26519;&#21644;&#22810;&#20013;&#24515;&#25439;&#22833;&#30340;&#38271;&#23614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Long-Tailed Classification Based on Coarse-Grained Leading Forest and Multi-Center Loss. (arXiv:2310.08206v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31890;&#24230;&#24341;&#23548;&#26862;&#26519;&#21644;&#22810;&#20013;&#24515;&#25439;&#22833;&#30340;&#38271;&#23614;&#20998;&#31867;&#26694;&#26550;&#65292;&#21517;&#20026;Cognisance&#12290;&#35813;&#26694;&#26550;&#33268;&#21147;&#20110;&#35299;&#20915;&#38271;&#23614;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#31867;&#38388;&#21644;&#31867;&#20869;&#19981;&#24179;&#34913;&#65292;&#24182;&#36890;&#36807;&#19981;&#21464;&#29305;&#24449;&#23398;&#20064;&#26500;&#24314;&#22810;&#31890;&#24230;&#32852;&#21512;&#35299;&#20915;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#20998;&#31867;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#19981;&#21487;&#36991;&#20813;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#38271;&#23614;&#20998;&#31867;&#26041;&#27861;&#20165;&#20851;&#27880;&#35299;&#20915;&#31867;&#38388;&#19981;&#24179;&#34913;&#65292;&#21363;&#22836;&#37096;&#31867;&#21035;&#30340;&#26679;&#26412;&#27604;&#23614;&#37096;&#31867;&#21035;&#30340;&#26679;&#26412;&#22810;&#65292;&#32780;&#24573;&#30053;&#20102;&#31867;&#20869;&#19981;&#24179;&#34913;&#65292;&#21363;&#21516;&#19968;&#31867;&#21035;&#20013;&#22836;&#37096;&#23646;&#24615;&#26679;&#26412;&#25968;&#37327;&#36828;&#22823;&#20110;&#23614;&#37096;&#23646;&#24615;&#26679;&#26412;&#25968;&#37327;&#12290;&#27169;&#22411;&#30340;&#20559;&#24046;&#26159;&#30001;&#36825;&#20004;&#20010;&#22240;&#32032;&#24341;&#36215;&#30340;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#20013;&#30340;&#23646;&#24615;&#26159;&#38544;&#21547;&#30340;&#19988;&#23646;&#24615;&#32452;&#21512;&#38750;&#24120;&#22797;&#26434;&#65292;&#22788;&#29702;&#31867;&#20869;&#19981;&#24179;&#34913;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31890;&#24230;&#24341;&#23548;&#26862;&#26519;&#65288;CLF&#65289;&#21644;&#22810;&#20013;&#24515;&#25439;&#22833;&#65288;MCL&#65289;&#30340;&#38271;&#23614;&#20998;&#31867;&#26694;&#26550;&#65292;&#21517;&#20026;Cognisance&#65292;&#26088;&#22312;&#36890;&#36807;&#19981;&#21464;&#29305;&#24449;&#23398;&#20064;&#26500;&#24314;&#22810;&#31890;&#24230;&#32852;&#21512;&#35299;&#20915;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#24179;&#34913;&#19981;&#21516;&#31867;&#21035;&#21644;&#23646;&#24615;&#20043;&#38388;&#30340;&#26679;&#26412;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-tailed(LT) classification is an unavoidable and challenging problem in the real world. Most of the existing long-tailed classification methods focus only on solving the inter-class imbalance in which there are more samples in the head class than in the tail class, while ignoring the intra-lass imbalance in which the number of samples of the head attribute within the same class is much larger than the number of samples of the tail attribute. The deviation in the model is caused by both of these factors, and due to the fact that attributes are implicit in most datasets and the combination of attributes is very complex, the intra-class imbalance is more difficult to handle. For this purpose, we proposed a long-tailed classification framework, known as \textbf{\textsc{Cognisance}}, which is founded on Coarse-Grained Leading Forest (CLF) and Multi-Center Loss (MCL), aiming to build a multi-granularity joint solution model by means of invariant feature learning. In this method, we desig
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#12290;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#19978;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16927</link><description>&lt;p&gt;
&#32447;&#26463;&#33258;&#21160;&#39550;&#39542;&#65306;&#25361;&#25112;&#19982;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
End-to-end Autonomous Driving: Challenges and Frontiers. (arXiv:2306.16927v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16927
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#12290;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#19978;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26041;&#27861;&#37319;&#29992;&#31471;&#21040;&#31471;&#31639;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22987;&#20256;&#24863;&#22120;&#36755;&#20837;&#29983;&#25104;&#36710;&#36742;&#36816;&#21160;&#35745;&#21010;&#65292;&#32780;&#19981;&#26159;&#19987;&#27880;&#20110;&#35832;&#22914;&#26816;&#27979;&#21644;&#36816;&#21160;&#39044;&#27979;&#31561;&#21333;&#20010;&#20219;&#21153;&#12290;&#19982;&#27169;&#22359;&#21270;&#27969;&#27700;&#32447;&#30456;&#27604;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#26469;&#33719;&#30410;&#12290;&#36825;&#19968;&#39046;&#22495;&#22240;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12289;&#38381;&#29615;&#35780;&#20272;&#20197;&#21450;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#22312;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#25191;&#34892;&#25152;&#38656;&#30340;&#38656;&#27714;&#32780;&#34028;&#21187;&#21457;&#23637;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;250&#22810;&#31687;&#35770;&#25991;&#65292;&#28085;&#30422;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#21160;&#26426;&#12289;&#36335;&#32447;&#22270;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22522;&#30784;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 250 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#38271;&#26102;&#38388;&#20869;&#30340;&#36816;&#21160;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.00416</link><description>&lt;p&gt;
&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Controllable Motion Diffusion Model. (arXiv:2306.00416v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00416
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#38271;&#26102;&#38388;&#20869;&#30340;&#36816;&#21160;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#21160;&#30011;&#20013;&#65292;&#20026;&#34394;&#25311;&#35282;&#33394;&#29983;&#25104;&#36924;&#30495;&#19988;&#21487;&#25511;&#30340;&#36816;&#21160;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20174;&#22270;&#20687;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22823;&#22810;&#38480;&#20110;&#31163;&#32447;&#24212;&#29992;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#21516;&#26102;&#29983;&#25104;&#25152;&#26377;&#27493;&#39588;&#30340;&#24207;&#21015;&#32423;&#29983;&#25104;&#12290;&#20026;&#20102;&#33021;&#22815;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20197;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#20026;&#22522;&#30784;&#65292;&#36880;&#27493;&#29983;&#25104;&#36816;&#21160;&#24207;&#21015;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#20351;&#29992;&#26631;&#20934;DDPM&#31639;&#27861;&#32780;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#20135;&#29983;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#36816;&#21160;&#25511;&#21046;&#19979;&#38271;&#26102;&#38388;&#20869;&#30340;&#39640;&#20445;&#30495;&#24230;&#36816;&#21160;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating realistic and controllable motions for virtual characters is a challenging task in computer animation, and its implications extend to games, simulations, and virtual reality. Recent studies have drawn inspiration from the success of diffusion models in image generation, demonstrating the potential for addressing this task. However, the majority of these studies have been limited to offline applications that target at sequence-level generation that generates all steps simultaneously. To enable real-time motion synthesis with diffusion models in response to time-varying control signals, we propose the framework of the Controllable Motion Diffusion Model (COMODO). Our framework begins with an auto-regressive motion diffusion model (A-MDM), which generates motion sequences step by step. In this way, simply using the standard DDPM algorithm without any additional complexity, our framework is able to generate high-fidelity motion sequences over extended periods with different type
&lt;/p&gt;</description></item></channel></rss>