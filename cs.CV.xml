<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;ObjectDR&#65292;&#21033;&#29992;&#23545;&#35937;-centric&#30340;&#22495;&#38543;&#26426;&#21270;&#21512;&#25104;&#21333;&#35270;&#22270;3D&#24418;&#29366;&#37325;&#24314;&#20013;&#32570;&#20047;&#30340;&#37197;&#23545;&#25968;&#25454;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21644;&#35299;&#32806;&#26694;&#26550;&#26469;&#29983;&#25104;&#21644;&#20445;&#30041;&#23545;&#35937;&#36718;&#24275;&#20197;&#21450;&#24191;&#27867;&#21464;&#21270;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#20026;&#22521;&#35757;&#27169;&#22411;&#25429;&#25417;&#22495;&#19981;&#21464;&#24615;&#20960;&#20309;&#24418;&#29366;&#12290;</title><link>https://arxiv.org/abs/2403.14539</link><description>&lt;p&gt;
Object-Centric Domain Randomization&#29992;&#20110;&#37326;&#22806;3D&#24418;&#29366;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Domain Randomization for 3D Shape Reconstruction in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14539
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ObjectDR&#65292;&#21033;&#29992;&#23545;&#35937;-centric&#30340;&#22495;&#38543;&#26426;&#21270;&#21512;&#25104;&#21333;&#35270;&#22270;3D&#24418;&#29366;&#37325;&#24314;&#20013;&#32570;&#20047;&#30340;&#37197;&#23545;&#25968;&#25454;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21644;&#35299;&#32806;&#26694;&#26550;&#26469;&#29983;&#25104;&#21644;&#20445;&#30041;&#23545;&#35937;&#36718;&#24275;&#20197;&#21450;&#24191;&#27867;&#21464;&#21270;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#20026;&#22521;&#35757;&#27169;&#22411;&#25429;&#25417;&#22495;&#19981;&#21464;&#24615;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35270;&#22270;3D&#24418;&#29366;&#22312;&#37326;&#22806;&#30340;&#37325;&#24314;&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#26159;&#26469;&#33258;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&lt;3D&#24418;&#29366;&#65292;2D&#22270;&#20687;&gt;-&#37197;&#23545;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#21463;&#22495;&#38543;&#26426;&#21270;&#24341;&#20154;&#27880;&#30446;&#30340;&#25104;&#23601;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ObjectDR&#65292;&#36890;&#36807;&#23545;&#23545;&#35937;&#22806;&#35266;&#21644;&#32972;&#26223;&#30340;&#35270;&#35273;&#21464;&#21270;&#36827;&#34892;&#38543;&#26426;&#20223;&#30495;&#65292;&#21512;&#25104;&#36825;&#31181;&#37197;&#23545;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#21033;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;ControlNet&#65289;&#29983;&#25104;&#31526;&#21512;&#31354;&#38388;&#26465;&#20214;&#65288;&#20363;&#22914;2.5D&#33609;&#22270;&#65289;&#30340;&#22270;&#20687;&#65292;&#36825;&#20123;&#26465;&#20214;&#21487;&#20197;&#36890;&#36807;&#20174;&#23545;&#35937;&#38598;&#21512;&#65288;&#20363;&#22914;Objaverse-XL&#65289;&#30340;&#28210;&#26579;&#36807;&#31243;&#33719;&#24471;3D&#24418;&#29366;&#12290;&#20026;&#20102;&#27169;&#25311;&#22810;&#26679;&#21270;&#30340;&#21464;&#21270;&#21516;&#26102;&#20445;&#30041;&#23884;&#20837;&#31354;&#38388;&#26465;&#20214;&#20013;&#30340;&#23545;&#35937;&#36718;&#24275;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#21033;&#29992;&#21021;&#22987;&#23545;&#35937;&#25351;&#23548;&#30340;&#35299;&#32806;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14539v1 Announce Type: cross  Abstract: One of the biggest challenges in single-view 3D shape reconstruction in the wild is the scarcity of &lt;3D shape, 2D image&gt;-paired data from real-world environments. Inspired by remarkable achievements via domain randomization, we propose ObjectDR which synthesizes such paired data via a random simulation of visual variations in object appearances and backgrounds. Our data synthesis framework exploits a conditional generative model (e.g., ControlNet) to generate images conforming to spatial conditions such as 2.5D sketches, which are obtainable through a rendering process of 3D shapes from object collections (e.g., Objaverse-XL). To simulate diverse variations while preserving object silhouettes embedded in spatial conditions, we also introduce a disentangled framework which leverages an initial object guidance. After synthesizing a wide range of data, we pre-train a model on them so that it learns to capture a domain-invariant geometry p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#26410;&#35265;&#33337;&#33334;&#31867;&#21035;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08271</link><description>&lt;p&gt;
&#29992;&#20110;&#32454;&#31890;&#24230;&#33337;&#33334;&#20998;&#31867;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained Ship Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#26410;&#35265;&#33337;&#33334;&#31867;&#21035;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#20013;&#30340;&#32454;&#31890;&#24230;&#33337;&#33334;&#20998;&#31867; (RS-FGSC) &#30001;&#20110;&#31867;&#21035;&#20043;&#38388;&#30340;&#39640;&#30456;&#20284;&#24615;&#20197;&#21450;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#24615;&#32780;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#20256;&#32479;&#30417;&#30563;&#20998;&#31867;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#36817;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#22312;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#29702;&#35299;&#22270;&#20687;&#20869;&#23481;&#26041;&#38754;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25366;&#25496;&#20102;VLMs&#30340;&#28508;&#21147;&#65292;&#20197;&#25552;&#39640;&#26410;&#35265;&#33337;&#33334;&#31867;&#21035;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#22312;&#30001;&#20110;&#25104;&#26412;&#25110;&#38544;&#31169;&#38480;&#21046;&#32780;&#25968;&#25454;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#30452;&#25509;&#20026;RS-FGSC&#24494;&#35843;VLMs&#36890;&#24120;&#20250;&#36935;&#21040;&#36807;&#25311;&#21512;&#21487;&#35265;&#31867;&#30340;&#25361;&#25112;&#65292;&#23548;&#33268;&#23545;&#26410;&#35265;&#31867;&#30340;&#27867;&#21270;&#19981;&#20339;&#65292;&#31361;&#20986;&#20102;&#21306;&#20998;&#22797;&#26434;&#32972;&#26223;&#21644;&#25429;&#25417;&#29420;&#29305;&#33337;&#33334;&#29305;&#24449;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08271v1 Announce Type: cross  Abstract: Fine-grained ship classification in remote sensing (RS-FGSC) poses a significant challenge due to the high similarity between classes and the limited availability of labeled data, limiting the effectiveness of traditional supervised classification methods. Recent advancements in large pre-trained Vision-Language Models (VLMs) have demonstrated impressive capabilities in few-shot or zero-shot learning, particularly in understanding image content. This study delves into harnessing the potential of VLMs to enhance classification accuracy for unseen ship categories, which holds considerable significance in scenarios with restricted data due to cost or privacy constraints. Directly fine-tuning VLMs for RS-FGSC often encounters the challenge of overfitting the seen classes, resulting in suboptimal generalization to unseen classes, which highlights the difficulty in differentiating complex backgrounds and capturing distinct ship features. To 
&lt;/p&gt;</description></item><item><title>Asclepius&#26159;&#19968;&#20010;&#26032;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#65292;&#26088;&#22312;&#20026;&#21487;&#20449;&#30340;Med-MLLMs&#35780;&#20272;&#25552;&#20379;&#21333;&#29420;&#19988;&#20020;&#24202;&#20195;&#34920;&#24615;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.11217</link><description>&lt;p&gt;
Asclepius&#65306;&#29992;&#20110;&#21307;&#23398;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#39057;&#35889;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11217
&lt;/p&gt;
&lt;p&gt;
Asclepius&#26159;&#19968;&#20010;&#26032;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#65292;&#26088;&#22312;&#20026;&#21487;&#20449;&#30340;Med-MLLMs&#35780;&#20272;&#25552;&#20379;&#21333;&#29420;&#19988;&#20020;&#24202;&#20195;&#34920;&#24615;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11217v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#21307;&#23398;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;Med-MLLMs&#65289;&#30340;&#37325;&#22823;&#31361;&#30772;&#36890;&#36807;&#24378;&#22823;&#30340;&#20449;&#24687;&#32508;&#21512;&#21644;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#25913;&#36896;&#20102;&#29616;&#20195;&#21307;&#30103;&#20445;&#20581;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#35786;&#26029;&#26694;&#26550;&#30340;&#22797;&#26434;&#24615;&#28085;&#30422;&#20102;&#21508;&#31181;&#21307;&#23398;&#19987;&#19994;&#65292;&#24182;&#28041;&#21450;&#22797;&#26434;&#30340;&#20020;&#24202;&#20915;&#31574;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#19981;&#36866;&#21512;Med-MLLMs&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;Med-MLLMs&#26159;&#22312;&#22823;&#37327;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#36825;&#20123;&#22522;&#20934;&#23481;&#26131;&#20986;&#29616;&#25968;&#25454;&#27844;&#38706;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20010;&#29420;&#31435;&#19988;&#20020;&#24202;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#29992;&#20110;&#21487;&#20449;&#30340;Med-MLLMs&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Asclepius&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;Med-MLLM&#22522;&#20934;&#65292;&#20005;&#26684;&#21644;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#22312;&#19981;&#21516;&#21307;&#23398;&#19987;&#19994;&#65288;&#24515;&#34880;&#31649;&#12289;&#32963;&#32928;&#31561;&#65289;&#21644;&#19981;&#21516;&#35786;&#26029;&#33021;&#21147;&#65288;&#30693;&#35273;&#12289;&#30142;&#30149;&#20998;&#26512;&#31561;&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11217v1 Announce Type: new  Abstract: The significant breakthroughs of Medical Multi-Modal Large Language Models (Med-MLLMs) renovate modern healthcare with robust information synthesis and medical decision support. However, these models are often evaluated on benchmarks that are unsuitable for the Med-MLLMs due to the intricate nature of the real-world diagnostic frameworks, which encompass diverse medical specialties and involve complex clinical decisions. Moreover, these benchmarks are susceptible to data leakage, since Med-MLLMs are trained on large assemblies of publicly available data. Thus, an isolated and clinically representative benchmark is highly desirable for credible Med-MLLMs evaluation. To this end, we introduce Asclepius, a novel Med-MLLM benchmark that rigorously and comprehensively assesses model capability in terms of: distinct medical specialties (cardiovascular, gastroenterology, etc.) and different diagnostic capacities (perception, disease analysis, e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#24494;&#32467;&#26500;&#26144;&#23556;&#12289;&#32420;&#32500;&#26463;&#25551;&#35760;&#12289;&#30333;&#36136;&#32420;&#32500;&#26463;&#20998;&#26512;&#20197;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#24635;&#32467;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00019</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion MRI with Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#24494;&#32467;&#26500;&#26144;&#23556;&#12289;&#32420;&#32500;&#26463;&#25551;&#35760;&#12289;&#30333;&#36136;&#32420;&#32500;&#26463;&#20998;&#26512;&#20197;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#24635;&#32467;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24357;&#25955;&#21152;&#26435;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#20855;&#26377;&#38750;&#20405;&#20837;&#24615;&#35780;&#20272;&#22823;&#33041;&#24494;&#32467;&#26500;&#21644;&#32467;&#26500;&#36830;&#25509;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;dMRI&#25968;&#25454;&#20197;&#25552;&#21462;&#20020;&#24202;&#21644;&#31185;&#23398;&#30446;&#30340;&#30340;&#26377;&#29992;&#20449;&#24687;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290; dMRI&#27979;&#37327;&#36890;&#24120;&#21463;&#21040;&#24378;&#22122;&#22768;&#21644;&#20266;&#24433;&#30340;&#24178;&#25200;&#65292;&#25968;&#25454;&#20013;&#36890;&#24120;&#23384;&#22312;&#39640;&#30340;&#20250;&#35805;&#38388;&#21644;&#25195;&#25551;&#32773;&#38388;&#24322;&#36136;&#24615;&#65292;&#20197;&#21450;&#22823;&#33041;&#32467;&#26500;&#30340;&#30456;&#24403;&#22823;&#30340;&#20010;&#20307;&#38388;&#21464;&#24322;&#65292;&#24182;&#19988;&#27979;&#37327;&#21644;&#24863;&#20852;&#36259;&#29616;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#21487;&#33021;&#38750;&#24120;&#22797;&#26434;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;dMRI&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#36825;&#20123;&#23581;&#35797;&#65292;&#37325;&#28857;&#20851;&#27880;&#24050;&#32463;&#35299;&#20915;&#20102;&#24494;&#32467;&#26500;&#26144;&#23556;&#12289;&#32420;&#32500;&#26463;&#25551;&#35760;&#12289;&#30333;&#36136;&#32420;&#32500;&#26463;&#20998;&#26512;&#20197;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20027;&#35201;&#21457;&#29616;&#12289;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-weighted magnetic resonance imaging (dMRI) offers unique capabilities such as noninvasive assessment of brain's micro-structure and structural connectivity. However, analyzing the dMRI data to extract useful information for clinical and scientific purposes is challenging. The dMRI measurements often suffer from strong noise and artifacts, there is usually high inter-session and inter-scanner heterogeneity in the data and considerable inter-subject variability in brain structure, and the relationship between measurements and the phenomena of interest can be highly complex. Recent years have witnessed increasing use of machine learning methods for dMRI analysis. This manuscript aims to assess these efforts, with a focus on methods that have addressed micro-structure mapping, tractography, white matter tract analysis, as well as data preprocessing and harmonization. We summarize the main findings, strengths, and weaknesses of the existing methods and suggest topics for future re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;AI&#30340;&#19977;&#32500;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#20013;&#35782;&#21035;&#22823;&#33041;&#27807;&#29305;&#24449;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2309.00903</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#19977;&#32500;&#26694;&#26550;&#25581;&#31034;&#23398;&#20064;&#27169;&#24335;&#65306;&#21464;&#37327;&#33041;&#27807;&#35782;&#21035;&#30340;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
An explainable three dimension framework to uncover learning patterns: A unified look in variable sulci recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.00903
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;AI&#30340;&#19977;&#32500;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#20013;&#35782;&#21035;&#22823;&#33041;&#27807;&#29305;&#24449;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#25361;&#25112;&#24615;&#30340;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#37324;&#65292;&#35270;&#35273;&#20027;&#39064;&#22312;&#19977;&#32500;&#31354;&#38388;&#20869;&#34920;&#29616;&#20986;&#39640;&#24230;&#22797;&#26434;&#24615;&#12290;&#31070;&#32463;&#31185;&#23398;&#30340;&#24212;&#29992;&#28041;&#21450;&#20174;MRI&#20013;&#35782;&#21035;&#22823;&#33041;&#27807;&#29305;&#24449;&#65292;&#30001;&#20110;&#19987;&#23478;&#20043;&#38388;&#30340;&#26631;&#27880;&#35268;&#31243;&#23384;&#22312;&#24046;&#24322;&#21644;&#22823;&#33041;&#22797;&#26434;&#30340;&#19977;&#32500;&#21151;&#33021;&#65292;&#25105;&#20204;&#38754;&#20020;&#30528;&#37325;&#22823;&#38556;&#30861;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#22312;&#26377;&#25928;&#39564;&#35777;&#21644;&#35780;&#20272;&#36825;&#20123;&#32593;&#32476;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#25968;&#23398;&#20844;&#24335;&#65292;&#32454;&#21270;&#20102;&#19981;&#21516;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#35299;&#37322;&#38656;&#27714;&#30340;&#21508;&#31181;&#31867;&#21035;&#65292;&#20998;&#20026;&#33258;&#35299;&#37322;&#12289;&#21322;&#35299;&#37322;&#12289;&#38750;&#35299;&#37322;&#21644;&#22522;&#20110;&#39564;&#35777;&#21327;&#35758;&#21487;&#38752;&#24615;&#30340;&#26032;&#27169;&#24335;&#23398;&#20064;&#24212;&#29992;&#12290;&#26681;&#25454;&#36825;&#20010;&#25968;&#23398;&#20844;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#37322;&#19977;&#32500;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.00903v2 Announce Type: replace-cross  Abstract: Explainable AI is crucial in medical imaging. In the challenging field of neuroscience, visual topics present a high level of complexity, particularly within three-dimensional space. The application of neuroscience, which involves identifying brain sulcal features from MRI, faces significant hurdles due to varying annotation protocols among experts and the intricate three-dimension functionality of the brain. Consequently, traditional explainability approaches fall short in effectively validating and evaluating these networks. To address this, we first present a mathematical formulation delineating various categories of explanation needs across diverse computer vision tasks, categorized into self-explanatory, semi-explanatory, non-explanatory, and new-pattern learning applications based on the reliability of the validation protocol. With respect to this mathematical formulation, we propose a 3D explainability framework aimed at
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#32479;&#19968;&#33258;&#30417;&#30563;&#32858;&#31867;&#21644;&#33021;&#37327;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#20934;&#21270;&#30340;&#25512;&#23548;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#38752;&#22320;&#24809;&#32602;&#22833;&#36133;&#27169;&#24335;&#30340;&#19979;&#30028;&#12290;&#36825;&#20010;&#19979;&#30028;&#20351;&#24471;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#26631;&#20934;&#30340;&#39592;&#26550;&#26550;&#26500;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#38750;&#23545;&#31216;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2401.00873</link><description>&lt;p&gt;
&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#32479;&#19968;&#33258;&#30417;&#30563;&#32858;&#31867;&#21644;&#33021;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Unification of Self-Supervised Clustering and Energy-Based Models. (arXiv:2401.00873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00873
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#32479;&#19968;&#33258;&#30417;&#30563;&#32858;&#31867;&#21644;&#33021;&#37327;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#20934;&#21270;&#30340;&#25512;&#23548;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#38752;&#22320;&#24809;&#32602;&#22833;&#36133;&#27169;&#24335;&#30340;&#19979;&#30028;&#12290;&#36825;&#20010;&#19979;&#30028;&#20351;&#24471;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#26631;&#20934;&#30340;&#39592;&#26550;&#26550;&#26500;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#38750;&#23545;&#31216;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#27969;&#34892;&#19988;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#30446;&#26631;&#12290;&#26412;&#30740;&#31350;&#23545;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#36827;&#34892;&#36125;&#21494;&#26031;&#20998;&#26512;&#65292;&#38416;&#26126;&#20102;&#27599;&#20010;&#31867;&#21035;&#20013;&#28508;&#22312;&#30340;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22522;&#26412;&#21407;&#29702;&#20986;&#21457;&#25512;&#23548;&#36825;&#20123;&#27169;&#22411;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#20998;&#26512;&#36824;&#34920;&#26126;&#20102;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#19982;&#22522;&#20110;&#20284;&#28982;&#30340;&#29983;&#25104;&#27169;&#22411;&#33258;&#28982;&#25972;&#21512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#32858;&#31867;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#33021;&#37327;&#27169;&#22411;&#39046;&#22495;&#20013;&#23454;&#29616;&#20102;&#36825;&#20010;&#27010;&#24565;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#19979;&#30028;&#65292;&#32463;&#35777;&#26126;&#33021;&#21487;&#38752;&#22320;&#24809;&#32602;&#26368;&#37325;&#35201;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#26032;&#25552;&#20986;&#30340;&#19979;&#30028;&#20351;&#24471;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#26631;&#20934;&#30340;&#39592;&#24178;&#26550;&#26500;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#35832;&#22914;&#20572;&#27490;&#26799;&#24230;&#12289;&#21160;&#37327;&#32534;&#30721;&#22120;&#25110;&#19987;&#38376;&#30340;&#32858;&#31867;&#31561;&#38750;&#23545;&#31216;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning is a popular and powerful method for utilizing large amounts of unlabeled data, for which a wide variety of training objectives have been proposed in the literature. In this study, we perform a Bayesian analysis of state-of-the-art self-supervised learning objectives, elucidating the underlying probabilistic graphical models in each class and presenting a standardized methodology for their derivation from first principles. The analysis also indicates a natural means of integrating self-supervised learning with likelihood-based generative models. We instantiate this concept within the realm of cluster-based self-supervised learning and energy models, introducing a novel lower bound which is proven to reliably penalize the most important failure modes. Furthermore, this newly proposed lower bound enables the training of a standard backbone architecture without the necessity for asymmetric elements such as stop gradients, momentum encoders, or specialized clusteri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.13549</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Multimodal Large Language Models. (arXiv:2306.13549v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22823;&#33041;&#25191;&#34892;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;MLLM &#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#22914;&#22522;&#20110;&#22270;&#20687;&#32534;&#20889;&#25925;&#20107;&#21644;&#26080;OCR&#25968;&#23398;&#25512;&#29702;&#31561;&#65292;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#24456;&#23569;&#35265;&#65292;&#34920;&#26126;&#20102;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;&#26412;&#25991;&#26088;&#22312;&#36861;&#36394;&#21644;&#24635;&#32467; MLLM &#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MLLM &#30340;&#26500;&#25104;&#65292;&#27010;&#36848;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#20851;&#38190;&#25216;&#26415;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#65288;M-IT&#65289;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;M-ICL&#65289;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#65288;M-CoT&#65289;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#65288;LAVR&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#37492;&#20110; MLLM &#26102;&#20195;&#25165;&#21018;&#21018;&#24320;&#22987;&#65292;&#25105;&#20204;&#20250;&#19981;&#26029;&#26356;&#26032;&#36825;&#20010;&#32508;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#26356;&#22810;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057; HMR &#26694;&#26550;&#65288;DDT&#65289;&#65292;&#23427;&#26088;&#22312;&#20174;&#36755;&#20837;&#24207;&#21015;&#20013;&#35299;&#30721;&#29305;&#23450;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#22686;&#24378;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#24182;&#36755;&#20986;&#25152;&#26377;&#24103;&#30340;&#20154;&#20307;&#32593;&#26684;&#65292;&#20351;&#24471; DDT &#26356;&#36866;&#29992;&#20110;&#26102;&#38388;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.13397</link><description>&lt;p&gt;
DDT&#65306;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#20174;&#35270;&#39057;&#20013;&#24674;&#22797;&#20154;&#20307;&#32593;&#26684;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh Recovery from a Video. (arXiv:2303.13397v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13397
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057; HMR &#26694;&#26550;&#65288;DDT&#65289;&#65292;&#23427;&#26088;&#22312;&#20174;&#36755;&#20837;&#24207;&#21015;&#20013;&#35299;&#30721;&#29305;&#23450;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#22686;&#24378;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#24182;&#36755;&#20986;&#25152;&#26377;&#24103;&#30340;&#20154;&#20307;&#32593;&#26684;&#65292;&#20351;&#24471; DDT &#26356;&#36866;&#29992;&#20110;&#26102;&#38388;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#32593;&#26684;&#24674;&#22797;&#65288;HMR&#65289;&#20026;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20154;&#20307;&#20449;&#24687;&#65292;&#20363;&#22914;&#28216;&#25103;&#12289;&#20154;&#26426;&#20132;&#20114;&#21644;&#34394;&#25311;&#29616;&#23454;&#12290;&#19982;&#21333;&#19968;&#22270;&#20687;&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;&#35270;&#39057;&#30340;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#36890;&#36807;&#34701;&#21512;&#20154;&#20307;&#36816;&#21160;&#20808;&#39564;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20687; VIBE &#36825;&#26679;&#30340;&#22810;&#23545;&#22810;&#26041;&#27861;&#23384;&#22312;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#25361;&#25112;&#12290;&#32780;&#20687; TCMR &#21644; MPS-Net &#36825;&#26679;&#30340;&#22810;&#23545;&#19968;&#26041;&#27861;&#21017;&#20381;&#36182;&#20110;&#26410;&#26469;&#24103;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26159;&#38750;&#22240;&#26524;&#21644;&#26102;&#38388;&#25928;&#29575;&#20302;&#19979;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057; HMR &#26694;&#26550;&#65288;DDT&#65289;&#12290;DDT &#26088;&#22312;&#20174;&#36755;&#20837;&#24207;&#21015;&#20013;&#35299;&#30721;&#29305;&#23450;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#22686;&#24378;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#20316;&#20026;&#19968;&#31181;&#22810;&#23545;&#22810;&#26041;&#27861;&#65292;DDT &#30340;&#35299;&#30721;&#22120;&#36755;&#20986;&#25152;&#26377;&#24103;&#30340;&#20154;&#20307;&#32593;&#26684;&#65292;&#20351; DDT &#26356;&#36866;&#29992;&#20110;&#26102;&#38388;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human mesh recovery (HMR) provides rich human body information for various real-world applications such as gaming, human-computer interaction, and virtual reality. Compared to single image-based methods, video-based methods can utilize temporal information to further improve performance by incorporating human body motion priors. However, many-to-many approaches such as VIBE suffer from motion smoothness and temporal inconsistency. While many-to-one approaches such as TCMR and MPS-Net rely on the future frames, which is non-causal and time inefficient during inference. To address these challenges, a novel Diffusion-Driven Transformer-based framework (DDT) for video-based HMR is presented. DDT is designed to decode specific motion patterns from the input sequence, enhancing motion smoothness and temporal consistency. As a many-to-many approach, the decoder of our DDT outputs the human mesh of all the frames, making DDT more viable for real-world applications where time efficiency is cruc
&lt;/p&gt;</description></item></channel></rss>