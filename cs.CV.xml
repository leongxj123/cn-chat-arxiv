<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#24341;&#20837;&#20102;&#27979;&#35797;&#26102;&#38388;&#21407;&#22411;&#36716;&#31227;&#65288;TPS&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#27599;&#20010;&#21407;&#22411;&#30340;&#36716;&#31227;&#21521;&#37327;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#39046;&#22495;&#24046;&#36317;&#24182;&#22686;&#24378;&#20102;&#31867;</title><link>https://arxiv.org/abs/2403.12952</link><description>&lt;p&gt;
&#21482;&#38656;&#36716;&#31227;&#23427;&#65306;&#27979;&#35797;&#26102;&#38388;&#21407;&#22411;&#36716;&#31227;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization with Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12952
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#27979;&#35797;&#26102;&#38388;&#21407;&#22411;&#36716;&#31227;&#65288;TPS&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#27599;&#20010;&#21407;&#22411;&#30340;&#36716;&#31227;&#21521;&#37327;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#39046;&#22495;&#24046;&#36317;&#24182;&#22686;&#24378;&#20102;&#31867;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#36827;&#23637;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#20013;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#26377;&#21069;&#26223;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#24448;&#24448;&#20250;&#22240;&#20026;&#39046;&#22495;&#36716;&#31227;&#32780;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27979;&#35797;&#26102;&#38388;&#21407;&#22411;&#36716;&#31227;&#65288;TPS&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20351;&#29992;&#26631;&#35760;&#27979;&#35797;&#36755;&#20837;&#26469;&#20351;VLM&#36866;&#24212;&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#24320;&#21019;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#22312;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#20013;&#35843;&#33410;&#27599;&#20010;&#31867;&#21035;&#30340;&#21407;&#22411;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#29983;&#25104;&#24182;&#32531;&#23384;&#21407;&#22411;&#65292;TPS&#19981;&#20165;&#20419;&#36827;&#20102;&#26080;&#38656;&#20248;&#21270;&#30340;&#21407;&#22411;&#37325;&#29992;&#36827;&#34892;&#21518;&#32493;&#39044;&#27979;&#65292;&#36824;&#35753;&#20854;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#24403;&#21069;&#36827;&#23637;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#12290;&#22312;&#27979;&#35797;&#26102;&#38388;&#65292;TPS&#20165;&#22522;&#20110;&#32473;&#23450;&#30340;&#27979;&#35797;&#26679;&#26412;&#21160;&#24577;&#23398;&#20064;&#27599;&#20010;&#21407;&#22411;&#30340;&#36716;&#31227;&#21521;&#37327;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#39046;&#22495;&#24046;&#36317;&#24182;&#22686;&#24378;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12952v1 Announce Type: cross  Abstract: Advancements in vision-language models (VLMs) have propelled the field of computer vision, particularly in the zero-shot learning setting. Despite their promise, the effectiveness of these models often diminishes due to domain shifts in test environments. To address this, we introduce the Test-Time Prototype Shifting (TPS) framework, a pioneering approach designed to adapt VLMs to test datasets using unlabeled test inputs. Our method is based on the notion of modulating per-class prototypes in the shared embedding space. By pre-computing and caching prototypes generated with the pre-trained text encoder, TPS not only facilitates optimization-free prototype reuse for subsequent predictions but also enables seamless integration with current advancements in prompt engineering. At test-time, TPS dynamically learns shift vectors for each prototype based solely on the given test sample, effectively bridging the domain gap and enhancing class
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;&#32467;&#21512;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.12151</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20869;&#23481;&#34701;&#20837;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#22686;&#24378;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12151
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;&#32467;&#21512;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#26377;&#21161;&#20110;&#35299;&#20915;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#65292;&#20294;&#29983;&#25104;&#36825;&#31181;&#30693;&#35782;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36890;&#36807;&#35821;&#20041;&#23884;&#20837;&#29983;&#25104;&#21644;&#25552;&#20379;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#23558;LLM&#38598;&#25104;&#21040;&#19968;&#20010;&#27969;&#31243;&#20013;&#65292;&#35813;&#27969;&#31243;&#22312;&#35270;&#35273;&#22522;&#30784;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#21521;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#24443;&#24213;&#30740;&#31350;&#20102;LLM&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#22522;&#20110;LLM&#30340;&#23884;&#20837;&#19982;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20511;&#37492;&#36825;&#19968;&#28040;&#34701;&#30740;&#31350;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#23545;&#31454;&#20105;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20174;&#32780;&#31361;&#20986;&#20102;&#26368;&#26032;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12151v1 Announce Type: new  Abstract: Domain-specific knowledge can significantly contribute to addressing a wide variety of vision tasks. However, the generation of such knowledge entails considerable human labor and time costs. This study investigates the potential of Large Language Models (LLMs) in generating and providing domain-specific information through semantic embeddings. To achieve this, an LLM is integrated into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors in the context of the Vision-based Zero-shot Object State Classification task. We thoroughly examine the behavior of the LLM through an extensive ablation study. Our findings reveal that the integration of LLM-based embeddings, in combination with general-purpose pre-trained embeddings, leads to substantial performance improvements. Drawing insights from this ablation study, we conduct a comparative analysis against competing models, thereby highlighting the state-of-the-art perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#38754;&#37096;&#35782;&#21035;&#12289;&#24615;&#21035;&#26816;&#27979;&#21644;&#24180;&#40836;&#20272;&#35745;&#31561;&#29983;&#29289;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;ChatGPT&#22312;&#38754;&#37096;&#35782;&#21035;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#24615;&#21035;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#65292;&#22312;&#24180;&#40836;&#20272;&#35745;&#20219;&#21153;&#20013;&#20063;&#20855;&#26377;&#30456;&#24403;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02965</link><description>&lt;p&gt;
ChatGPT&#19982;&#29983;&#29289;&#35782;&#21035;&#25216;&#26415;&#65306;&#23545;&#38754;&#37096;&#35782;&#21035;&#12289;&#24615;&#21035;&#26816;&#27979;&#21644;&#24180;&#40836;&#20272;&#35745;&#33021;&#21147;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ChatGPT and biometrics: an assessment of face recognition, gender detection, and age estimation capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#38754;&#37096;&#35782;&#21035;&#12289;&#24615;&#21035;&#26816;&#27979;&#21644;&#24180;&#40836;&#20272;&#35745;&#31561;&#29983;&#29289;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;ChatGPT&#22312;&#38754;&#37096;&#35782;&#21035;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#24615;&#21035;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#65292;&#22312;&#24180;&#40836;&#20272;&#35745;&#20219;&#21153;&#20013;&#20063;&#20855;&#26377;&#30456;&#24403;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#22312;&#29983;&#29289;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#29305;&#21035;&#26816;&#39564;&#20102;ChatGPT&#22312;&#25191;&#34892;&#29983;&#29289;&#35782;&#21035;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#38754;&#37096;&#35782;&#21035;&#12289;&#24615;&#21035;&#26816;&#27979;&#21644;&#24180;&#40836;&#20272;&#35745;&#12290;&#30001;&#20110;&#29983;&#29289;&#35782;&#21035;&#34987;&#35270;&#20026;&#25935;&#24863;&#20449;&#24687;&#65292;ChatGPT&#36991;&#20813;&#22238;&#31572;&#30452;&#25509;&#25552;&#31034;&#65292;&#22240;&#27492;&#25105;&#20204;&#35774;&#35745;&#20102;&#25552;&#31034;&#31574;&#30053;&#26469;&#32469;&#36807;&#20854;&#20445;&#25252;&#25514;&#26045;&#65292;&#24182;&#35780;&#20272;&#29983;&#29289;&#35782;&#21035;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;ChatGPT&#33021;&#22815;&#20197;&#30456;&#24403;&#39640;&#30340;&#20934;&#30830;&#24615;&#35782;&#21035;&#38754;&#37096;&#36523;&#20221;&#24182;&#22312;&#20004;&#20010;&#38754;&#37096;&#22270;&#20687;&#20043;&#38388;&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#24615;&#21035;&#26816;&#27979;&#26041;&#38754;&#24615;&#33021;&#26174;&#33879;&#65292;&#24182;&#23545;&#24180;&#40836;&#20272;&#35745;&#20219;&#21153;&#26377;&#30456;&#24403;&#20934;&#30830;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#22312;&#29983;&#29289;&#35782;&#21035;&#20013;&#24212;&#29992;LLMs&#21644;&#22522;&#30784;&#27169;&#22411;&#20855;&#26377;&#24191;&#38420;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02965v1 Announce Type: cross  Abstract: This paper explores the application of large language models (LLMs), like ChatGPT, for biometric tasks. We specifically examine the capabilities of ChatGPT in performing biometric-related tasks, with an emphasis on face recognition, gender detection, and age estimation. Since biometrics are considered as sensitive information, ChatGPT avoids answering direct prompts, and thus we crafted a prompting strategy to bypass its safeguard and evaluate the capabilities for biometrics tasks. Our study reveals that ChatGPT recognizes facial identities and differentiates between two facial images with considerable accuracy. Additionally, experimental results demonstrate remarkable performance in gender detection and reasonable accuracy for the age estimation tasks. Our findings shed light on the promising potentials in the application of LLMs and foundation models for biometrics.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20860;&#39038;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.08977</link><description>&lt;p&gt;
&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#25345;&#32493;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Weighted Ensemble Models Are Strong Continual Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08977
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20860;&#39038;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#20174;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#24471;&#20197;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#22312;&#23398;&#20064;&#24403;&#21069;&#20219;&#21153;&#25968;&#25454;&#26102;&#19981;&#21487;&#29992;&#12290;CL&#26412;&#36136;&#19978;&#26159;&#22312;&#33021;&#22815;&#23398;&#20064;&#26032;&#20219;&#21153;&#65288;&#21363;&#21487;&#22609;&#24615;&#65289;&#21644;&#20445;&#25345;&#20808;&#21069;&#23398;&#20064;&#27010;&#24565;&#30340;&#24615;&#33021;&#65288;&#21363;&#31283;&#23450;&#24615;&#65289;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23545;&#20808;&#21069;&#21644;&#24403;&#21069;&#20219;&#21153;&#30340;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21152;&#26435;&#38598;&#25104;&#12290;&#36825;&#31181;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25345;&#32493;&#27169;&#22411;&#24179;&#22343;&#65288;&#25110;CoMA&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#22609;&#24615;&#22312;&#24403;&#21069;&#20219;&#21153;&#19978;&#33719;&#24471;&#39640;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#20559;&#31163;&#22826;&#36828;&#30340;&#20808;&#21069;&#26435;&#37325;&#37197;&#32622;&#65292;&#20174;&#32780;&#30830;&#20445;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;CoMA&#30340;&#25913;&#36827;&#22411;&#21464;&#20307;&#65292;&#21517;&#20026;&#25345;&#32493;&#36153;&#33293;&#23572;&#21152;&#26435;&#27169;&#22411;&#24179;&#22343;&#65288;&#25110;CoFiMA&#65289;&#65292;&#35813;&#27169;&#22411;&#23545;&#27599;&#19968;&#20010;&#21442;&#25968;&#36827;&#34892;&#36873;&#25321;&#24615;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08977v2 Announce Type: replace-cross  Abstract: In this work, we study the problem of continual learning (CL) where the goal is to learn a model on a sequence of tasks, such that the data from the previous tasks becomes unavailable while learning on the current task data. CL is essentially a balancing act between being able to learn on the new task (i.e., plasticity) and maintaining the performance on the previously learned concepts (i.e., stability). Intending to address the stability-plasticity trade-off, we propose to perform weight-ensembling of the model parameters of the previous and current tasks. This weighted-ensembled model, which we call Continual Model Averaging (or CoMA), attains high accuracy on the current task by leveraging plasticity, while not deviating too far from the previous weight configuration, ensuring stability. We also propose an improved variant of CoMA, named Continual Fisher-weighted Model Averaging (or CoFiMA), that selectively weighs each para
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#35299;&#27010;&#29575;&#27169;&#22411;&#31934;&#30830;&#35745;&#31639;&#32422;&#26463;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#19968;&#20449;&#21495;&#26469;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#36827;&#22270;&#20687;&#20462;&#22797;&#30340;&#36136;&#37327;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03349</link><description>&lt;p&gt;
&#22270;&#20687;&#20462;&#22797;&#36890;&#36807;&#21487;&#25511;&#25193;&#25955;&#27169;&#22411;&#30340;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Image Inpainting via Tractable Steering of Diffusion Models. (arXiv:2401.03349v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#35299;&#27010;&#29575;&#27169;&#22411;&#31934;&#30830;&#35745;&#31639;&#32422;&#26463;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#19968;&#20449;&#21495;&#26469;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#36827;&#22270;&#20687;&#20462;&#22797;&#30340;&#36136;&#37327;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#29983;&#25104;&#36924;&#30495;&#22270;&#20687;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26377;&#32422;&#26463;&#30340;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#22914;&#20462;&#22797;&#65292;&#25511;&#21046;&#25277;&#26679;&#36807;&#31243;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23545;&#36825;&#20123;&#32422;&#26463;&#30340;&#31934;&#30830;&#26465;&#20214;&#35774;&#23450;&#26159;&#19981;&#21487;&#35299;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#21487;&#35299;&#30340;&#27010;&#29575;&#27169;&#22411;(TPMs)&#30340;&#33021;&#21147;&#26469;&#31934;&#30830;&#19988;&#26377;&#25928;&#22320;&#35745;&#31639;&#21463;&#32422;&#26463;&#30340;&#21518;&#39564;&#65292;&#24182;&#21033;&#29992;&#35813;&#20449;&#21495;&#26469;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#31867;&#34920;&#36798;&#21147;&#36739;&#24378;&#30340;TPMs&#65292;&#31216;&#20026;&#27010;&#29575;&#30005;&#36335;(PCs)&#12290;&#22522;&#20110;&#20808;&#21069;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#22823;&#20102;PCs&#30340;&#35268;&#27169;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19977;&#20010;&#33258;&#28982;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;&#21363;CelebA-H&#65289;&#20013;&#25345;&#32493;&#25913;&#36827;&#20462;&#22797;&#22270;&#20687;&#30340;&#25972;&#20307;&#36136;&#37327;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are the current state of the art for generating photorealistic images. Controlling the sampling process for constrained image generation tasks such as inpainting, however, remains challenging since exact conditioning on such constraints is intractable. While existing methods use various techniques to approximate the constrained posterior, this paper proposes to exploit the ability of Tractable Probabilistic Models (TPMs) to exactly and efficiently compute the constrained posterior, and to leverage this signal to steer the denoising process of diffusion models. Specifically, this paper adopts a class of expressive TPMs termed Probabilistic Circuits (PCs). Building upon prior advances, we further scale up PCs and make them capable of guiding the image generation process of diffusion models. Empirical results suggest that our approach can consistently improve the overall quality and semantic coherence of inpainted images across three natural image datasets (i.e., CelebA-H
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#27931;&#20234;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#28608;&#27963;&#20248;&#21270;&#20026;&#27169;&#22411;&#21019;&#24314;&#31614;&#21517;&#65292;&#28982;&#21518;&#35757;&#32451;&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;&#29305;&#27931;&#20234;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04877</link><description>&lt;p&gt;
&#20351;&#29992;&#28608;&#27963;&#20248;&#21270;&#36827;&#34892;&#29305;&#27931;&#20234;&#27169;&#22411;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Trojan Model Detection Using Activation Optimization. (arXiv:2306.04877v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#27931;&#20234;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#28608;&#27963;&#20248;&#21270;&#20026;&#27169;&#22411;&#21019;&#24314;&#31614;&#21517;&#65292;&#28982;&#21518;&#35757;&#32451;&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;&#29305;&#27931;&#20234;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#30340;&#19981;&#21487;&#29992;&#24615;&#25110;&#22823;&#35268;&#27169;&#65292;&#20197;&#21450;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39640;&#35745;&#31639;&#21644;&#20154;&#21147;&#25104;&#26412;&#65292;&#36890;&#24120;&#20250;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#20381;&#36182;&#20110;&#24320;&#28304;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#20174;&#23433;&#20840;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#20570;&#27861;&#38750;&#24120;&#20196;&#20154;&#25285;&#24551;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#20250;&#34987;&#24863;&#26579;&#29305;&#27931;&#20234;&#25915;&#20987;&#65292;&#22312;&#36825;&#31181;&#25915;&#20987;&#20013;&#65292;&#25915;&#20987;&#32773;&#23884;&#20837;&#19968;&#20010;&#35302;&#21457;&#22120;&#22312;&#27169;&#22411;&#20013;&#65292;&#20351;&#24471;&#24403;&#35302;&#21457;&#22120;&#23384;&#22312;&#20110;&#36755;&#20837;&#20013;&#26102;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25511;&#21046;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#27931;&#20234;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#30340;&#21021;&#27493;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26681;&#25454;&#28608;&#27963;&#20248;&#21270;&#20026;&#27169;&#22411;&#21019;&#24314;&#31614;&#21517;&#12290;&#28982;&#21518;&#35757;&#32451;&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;&#29305;&#27931;&#20234;&#27169;&#22411;&#24182;&#32473;&#20986;&#20854;&#31614;&#21517;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to data's unavailability or large size, and the high computational and human labor costs of training machine learning models, it is a common practice to rely on open source pre-trained models whenever possible. However, this practice is worry some from the security perspective. Pre-trained models can be infected with Trojan attacks, in which the attacker embeds a trigger in the model such that the model's behavior can be controlled by the attacker when the trigger is present in the input. In this paper, we present our preliminary work on a novel method for Trojan model detection. Our method creates a signature for a model based on activation optimization. A classifier is then trained to detect a Trojan model given its signature. Our method achieves state of the art performance on two public datasets.
&lt;/p&gt;</description></item></channel></rss>