<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23545;&#20559;&#35265;&#25805;&#32437;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23450;&#37327;&#25511;&#21046;&#27169;&#22411;&#20559;&#35265;&#26469;&#25805;&#32437;&#36755;&#20986;&#20005;&#37325;&#24615;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#23454;&#29616;&#31934;&#30830;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#26032;&#39062;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02530</link><description>&lt;p&gt;
&#20005;&#37325;&#25511;&#21046;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20559;&#35265;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Severity Controlled Text-to-Image Generative Model Bias Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23545;&#20559;&#35265;&#25805;&#32437;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23450;&#37327;&#25511;&#21046;&#27169;&#22411;&#20559;&#35265;&#26469;&#25805;&#32437;&#36755;&#20986;&#20005;&#37325;&#24615;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#23454;&#29616;&#31934;&#30830;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#26032;&#39062;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#27491;&#22312;&#24191;&#27867;&#27969;&#34892;&#65292;&#23588;&#20854;&#26159;&#22312;&#20844;&#20849;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22266;&#26377;&#30340;&#20559;&#35265;&#21644;&#28508;&#22312;&#30340;&#24694;&#24847;&#25805;&#32437;&#36824;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;T2I&#27169;&#22411;&#23545;&#27492;&#31867;&#25805;&#32437;&#30340;&#26131;&#24863;&#24615;&#65292;&#24182;&#39318;&#27425;&#25552;&#20986;&#20102;&#36890;&#36807;&#38024;&#23545;&#23884;&#20837;&#24335;&#35821;&#35328;&#27169;&#22411;&#21160;&#24577;&#19988;&#39640;&#25928;&#22320;&#21033;&#29992;&#27169;&#22411;&#20559;&#35265;&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#21521;&#37327;&#20195;&#25968;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#20559;&#35265;&#36890;&#36807;&#20005;&#37325;&#24615;&#30340;&#36755;&#20986;&#25805;&#32437;&#30340;&#21487;&#25193;&#23637;&#21644;&#26041;&#20415;&#25511;&#21046;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#35813;&#25511;&#21046;&#36824;&#20801;&#35768;&#19968;&#31181;&#31934;&#30830;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#20197;&#29983;&#25104;&#36890;&#24120;&#19981;&#22826;&#21487;&#33021;&#36890;&#36807;&#24120;&#35268;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25805;&#32437;&#25216;&#26415;&#22312;&#24179;&#34913;&#29983;&#25104;&#31867;&#21035;&#39057;&#29575;&#26041;&#38754;&#30340;&#24314;&#35774;&#24212;&#29992; - &#22914;&#22312;&#27169;&#22411;&#21435;&#20559;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#19981;&#38656;&#35201;&#35757;&#32451;&#65292;&#24182;&#19988;&#20063;&#20197;&#21518;&#38376;&#30340;&#24418;&#24335;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02530v1 Announce Type: cross  Abstract: Text-to-image (T2I) generative models are gaining wide popularity, especially in public domains. However, their intrinsic bias and potential malicious manipulations remain under-explored. Charting the susceptibility of T2I models to such manipulation, we first expose the new possibility of a dynamic and computationally efficient exploitation of model bias by targeting the embedded language models. By leveraging mathematical foundations of vector algebra, our technique enables a scalable and convenient control over the severity of output manipulation through model bias. As a by-product, this control also allows a form of precise prompt engineering to generate images which are generally implausible with regular text prompts. We also demonstrate a constructive application of our manipulation for balancing the frequency of generated classes - as in model debiasing. Our technique does not require training and is also framed as a backdoor at
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CF-SimCLR&#65292;&#19968;&#31181;&#21453;&#20107;&#23454;&#23545;&#29031;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36817;&#20284;&#21453;&#20107;&#23454;&#25512;&#26029;&#21019;&#36896;&#27491;&#26679;&#26412;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#37319;&#38598;&#20559;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09605</link><description>&lt;p&gt;
&#21453;&#20107;&#23454;&#23545;&#29031;&#23398;&#20064;&#65306;&#36890;&#36807;&#22240;&#26524;&#22270;&#20687;&#21512;&#25104;&#33719;&#24471;&#31283;&#20581;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Counterfactual contrastive learning: robust representations via causal image synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CF-SimCLR&#65292;&#19968;&#31181;&#21453;&#20107;&#23454;&#23545;&#29031;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36817;&#20284;&#21453;&#20107;&#23454;&#25512;&#26029;&#21019;&#36896;&#27491;&#26679;&#26412;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#37319;&#38598;&#20559;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#39044;&#35757;&#32451;&#24050;&#34987;&#24191;&#27867;&#35748;&#20026;&#33021;&#22815;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#38480;&#26631;&#31614;&#35774;&#32622;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#23545;&#22686;&#24378;&#31649;&#36947;&#30340;&#36873;&#25321;&#25935;&#24863;&#12290;&#27491;&#26679;&#26412;&#24212;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#21516;&#26102;&#30772;&#22351;&#22495;&#29305;&#23450;&#20449;&#24687;&#12290;&#26631;&#20934;&#22686;&#24378;&#31649;&#36947;&#36890;&#36807;&#39044;&#23450;&#20041;&#30340;&#20809;&#24230;&#21464;&#25442;&#27169;&#25311;&#22495;&#29305;&#23450;&#21464;&#21270;&#65292;&#20294;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#27169;&#25311;&#30495;&#23454;&#30340;&#39046;&#22495;&#21464;&#21270;&#21602;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26368;&#36817;&#22312;&#21453;&#20107;&#23454;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CF-SimCLR&#65292;&#19968;&#31181;&#21453;&#20107;&#23454;&#23545;&#29031;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#36817;&#20284;&#21453;&#20107;&#23454;&#25512;&#26029;&#36827;&#34892;&#27491;&#26679;&#26412;&#21019;&#24314;&#12290;&#23545;&#33016;&#37096;X&#20809;&#21644;&#20083;&#33146;X&#20809;&#31561;&#20116;&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;CF-SimCLR&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#33719;&#21462;&#20559;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#22312;&#20004;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#19979;&#28216;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09605v1 Announce Type: cross  Abstract: Contrastive pretraining is well-known to improve downstream task performance and model generalisation, especially in limited label settings. However, it is sensitive to the choice of augmentation pipeline. Positive pairs should preserve semantic information while destroying domain-specific information. Standard augmentation pipelines emulate domain-specific changes with pre-defined photometric transformations, but what if we could simulate realistic domain changes instead? In this work, we show how to utilise recent progress in counterfactual image generation to this effect. We propose CF-SimCLR, a counterfactual contrastive learning approach which leverages approximate counterfactual inference for positive pair creation. Comprehensive evaluation across five datasets, on chest radiography and mammography, demonstrates that CF-SimCLR substantially improves robustness to acquisition shift with higher downstream performance on both in- an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#29420;&#31435;&#30340;&#19987;&#38376;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2310.03986</link><description>&lt;p&gt;
&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#65292;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation. (arXiv:2310.03986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03986
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#29420;&#31435;&#30340;&#19987;&#38376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#26469;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#19968;&#20123;&#30456;&#20851;&#30340;&#27169;&#24577;&#20013;&#35266;&#23519;&#21040;&#65292;&#22914;&#26524;&#22312;&#27979;&#35797;&#26102;&#38388;&#32570;&#23569;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#31616;&#21333;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#31243;&#24207;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#26469;&#34917;&#20607;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#31181;&#36866;&#24212;&#21487;&#20197;&#37096;&#20998;&#24357;&#34917;&#30001;&#20110;&#32570;&#22833;&#27169;&#24577;&#32780;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#38024;&#23545;&#21487;&#29992;&#27169;&#24577;&#32452;&#21512;&#36827;&#34892;&#35757;&#32451;&#30340;&#29420;&#31435;&#30340;&#12289;&#19987;&#38376;&#30340;&#32593;&#32476;&#12290;&#25152;&#25552;&#20986;&#30340;&#36866;&#24212;&#25152;&#38656;&#30340;&#21442;&#25968;&#38750;&#24120;&#23569;&#65288;&#20363;&#22914;&#65292;&#23569;&#20110;&#65289;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning seeks to utilize data from multiple sources to improve the overall performance of downstream tasks. It is desirable for redundancies in the data to make multimodal systems robust to missing or corrupted observations in some correlated modalities. However, we observe that the performance of several existing multimodal networks significantly deteriorates if one or multiple modalities are absent at test time. To enable robustness to missing modalities, we propose simple and parameter-efficient adaptation procedures for pretrained multimodal networks. In particular, we exploit low-rank adaptation and modulation of intermediate features to compensate for the missing modalities. We demonstrate that such adaptation can partially bridge performance drop due to missing modalities and outperform independent, dedicated networks trained for the available modality combinations in some cases. The proposed adaptation requires extremely small number of parameters (e.g., fewer than 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;YOLOv8&#27169;&#22411;&#21644;&#21019;&#26032;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#30340;&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20004;&#38454;&#27573;&#39044;&#27979;&#31574;&#30053;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20803;&#32032;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#20248;&#20110;&#21333;&#20010;&#22522;&#30784;&#26550;&#26500;&#65292;&#24182;&#35299;&#20915;&#20102;BaDLAD&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;OCR&#21644;&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.00848</link><description>&lt;p&gt;
&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;-&#19968;&#31181;&#22522;&#20110;YOLOv8&#30340;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bengali Document Layout Analysis -- A YOLOV8 Based Ensembling Approach. (arXiv:2309.00848v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;YOLOv8&#27169;&#22411;&#21644;&#21019;&#26032;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#30340;&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20004;&#38454;&#27573;&#39044;&#27979;&#31574;&#30053;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20803;&#32032;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#20248;&#20110;&#21333;&#20010;&#22522;&#30784;&#26550;&#26500;&#65292;&#24182;&#35299;&#20915;&#20102;BaDLAD&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;OCR&#21644;&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20391;&#37325;&#20110;&#21033;&#29992;YOLOv8&#27169;&#22411;&#21644;&#21019;&#26032;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#25552;&#21319;&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#65288;DLA&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#20197;&#24212;&#23545;&#23391;&#21152;&#25289;&#22797;&#26434;&#25991;&#23383;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#32463;&#36807;&#20005;&#26684;&#30340;&#39564;&#35777;&#38598;&#35780;&#20272;&#65292;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20803;&#32032;&#20998;&#21106;&#30340;&#20004;&#38454;&#27573;&#39044;&#27979;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#38598;&#25104;&#27169;&#22411;&#32467;&#21512;&#21518;&#22788;&#29702;&#24615;&#33021;&#20248;&#20110;&#21333;&#20010;&#22522;&#30784;&#26550;&#26500;&#65292;&#35299;&#20915;&#20102;BaDLAD&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#26088;&#22312;&#25512;&#21160;&#23391;&#21152;&#25289;&#25991;&#26723;&#20998;&#26512;&#30340;&#21457;&#23637;&#65292;&#25552;&#39640;OCR&#21644;&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#65292;&#21516;&#26102;BaDLAD&#20316;&#20026;&#22522;&#30784;&#36164;&#28304;&#26377;&#21161;&#20110;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#20026;&#23558;&#26032;&#31574;&#30053;&#32435;&#20837;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on enhancing Bengali Document Layout Analysis (DLA) using the YOLOv8 model and innovative post-processing techniques. We tackle challenges unique to the complex Bengali script by employing data augmentation for model robustness. After meticulous validation set evaluation, we fine-tune our approach on the complete dataset, leading to a two-stage prediction strategy for accurate element segmentation. Our ensemble model, combined with post-processing, outperforms individual base architectures, addressing issues identified in the BaDLAD dataset. By leveraging this approach, we aim to advance Bengali document analysis, contributing to improved OCR and document comprehension and BaDLAD serves as a foundational resource for this endeavor, aiding future research in the field. Furthermore, our experiments provided key insights to incorporate new strategies into the established solution.
&lt;/p&gt;</description></item><item><title>Sparkles&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#23454;&#29616;&#22810;&#22270;&#23545;&#35805;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SparklesDialogue&#25968;&#25454;&#38598;&#21644;SparklesEval&#22522;&#20934;&#26469;&#25903;&#25345;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;SparklesChat&#22312;&#29702;&#35299;&#22810;&#22270;&#23545;&#35805;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16463</link><description>&lt;p&gt;
Sparkles: &#35299;&#38145;&#22810;&#22270;&#32842;&#22825;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models. (arXiv:2308.16463v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16463
&lt;/p&gt;
&lt;p&gt;
Sparkles&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#23454;&#29616;&#22810;&#22270;&#23545;&#35805;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SparklesDialogue&#25968;&#25454;&#38598;&#21644;SparklesEval&#22522;&#20934;&#26469;&#25903;&#25345;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;SparklesChat&#22312;&#29702;&#35299;&#22810;&#22270;&#23545;&#35805;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#25351;&#20196;&#36319;&#36394;&#25968;&#25454;&#26469;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#36825;&#20123;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#65288;&#22914;MiniGPT-4&#65289;&#22312;&#28041;&#21450;&#22810;&#20010;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#23545;&#35805;&#36830;&#36143;&#24615;&#38754;&#20020;&#25361;&#25112;&#12290;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#32570;&#20047;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#36825;&#19968;&#20851;&#38190;&#24212;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SparklesChat&#65292;&#19968;&#20010;&#29992;&#20110;&#22810;&#22270;&#23545;&#35805;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;&#12290;&#20026;&#20102;&#25903;&#25345;&#35757;&#32451;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SparklesDialogue&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#21333;&#35789;&#32423;&#20132;&#38169;&#22810;&#22270;&#20687;&#21644;&#25991;&#26412;&#20132;&#20114;&#32780;&#23450;&#21046;&#30340;&#26426;&#22120;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;SparklesEval&#65292;&#19968;&#20010;&#20511;&#21161;GPT&#36741;&#21161;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#23450;&#37327;&#35780;&#20272;&#27169;&#22411;&#22312;&#22810;&#20010;&#22270;&#20687;&#21644;&#23545;&#35805;&#36718;&#27425;&#20013;&#30340;&#23545;&#35805;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;SparklesChat&#22312;&#29702;&#35299;&#22810;&#22270;&#23545;&#35805;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models exhibit enhanced zero-shot performance on various tasks when fine-tuned with instruction-following data. Multimodal instruction-following models extend these capabilities by integrating both text and images. However, existing models such as MiniGPT-4 face challenges in maintaining dialogue coherence in scenarios involving multiple images. A primary reason is the lack of a specialized dataset for this critical application. To bridge these gaps, we present SparklesChat, a multimodal instruction-following model for open-ended dialogues across multiple images. To support the training, we introduce SparklesDialogue, the first machine-generated dialogue dataset tailored for word-level interleaved multi-image and text interactions. Furthermore, we construct SparklesEval, a GPT-assisted benchmark for quantitatively assessing a model's conversational competence across multiple images and dialogue turns. Our experiments validate the effectiveness of SparklesChat in understa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20811;&#26381;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#24615;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#21457;&#29616;&#19968;&#31181;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#23454;&#39564;&#20013;&#22823;&#24133;&#20943;&#23569;&#20102;&#32593;&#32476;&#26356;&#26032;&#30340;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.01904</link><description>&lt;p&gt;
&#20811;&#26381;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#24615;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Overcoming the Stability Gap in Continual Learning. (arXiv:2306.01904v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20811;&#26381;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#24615;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#21457;&#29616;&#19968;&#31181;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#23454;&#39564;&#20013;&#22823;&#24133;&#20943;&#23569;&#20102;&#32593;&#32476;&#26356;&#26032;&#30340;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#38543;&#30528;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#22686;&#38271;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24448;&#24448;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#12290;&#32771;&#34385;&#21040;&#37325;&#26032;&#35757;&#32451;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20154;&#20204;&#35748;&#20026;&#36830;&#32493;&#23398;&#20064;&#21487;&#20197;&#20351;&#32593;&#32476;&#26356;&#26032;&#26356;&#21152;&#39640;&#25928;&#12290;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#38556;&#30861;&#26159;&#31283;&#23450;&#24615;&#24046;&#36317;&#65292;&#21363;&#22312;&#26356;&#26032;&#26032;&#25968;&#25454;&#26102;&#65292;&#20808;&#21069;&#23398;&#20064;&#30340;&#25968;&#25454;&#24615;&#33021;&#20250;&#19979;&#38477;&#65292;&#28982;&#21518;&#25165;&#24471;&#20197;&#24674;&#22797;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#20943;&#23569;&#32593;&#32476;&#26356;&#26032;&#30340;&#27425;&#25968;&#65292;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#32531;&#35299;&#31283;&#23450;&#24615;&#24046;&#36317;&#65292;&#24182;&#27979;&#35797;&#20102;&#22810;&#31181;&#20551;&#35774;&#20197;&#20102;&#35299;&#20854;&#20135;&#29983;&#21407;&#22240;&#12290;&#36825;&#20351;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26174;&#33879;&#20943;&#23569;&#31283;&#23450;&#24615;&#24046;&#36317;&#30340;&#26041;&#27861;&#12290;&#22312;&#22823;&#35268;&#27169;&#30340;&#22686;&#37327;&#31867;&#21035;&#23398;&#20064;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#36830;&#32493;&#23398;&#20064;&#25152;&#38656;&#30340;&#32593;&#32476;&#26356;&#26032;&#27425;&#25968;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26377;&#21487;&#33021;&#25512;&#21160;&#36830;&#32493;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world applications, deep neural networks are retrained from scratch as a dataset grows in size. Given the computational expense for retraining networks, it has been argued that continual learning could make updating networks more efficient. An obstacle to achieving this goal is the stability gap, which refers to an observation that when updating on new data, performance on previously learned data degrades before recovering. Addressing this problem would enable learning new data with fewer network updates, resulting in increased computational efficiency. We study how to mitigate the stability gap. We test a variety of hypotheses to understand why the stability gap occurs. This leads us to discover a method that vastly reduces this gap. In large-scale class incremental learning experiments, we are able to significantly reduce the number of network updates needed for continual learning. Our work has the potential to advance the state-of-the-art in continual learning for real-
&lt;/p&gt;</description></item></channel></rss>