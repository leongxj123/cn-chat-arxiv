<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#20197;&#20943;&#23569;&#23545;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13890</link><description>&lt;p&gt;
&#20197;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Towards Learning Contrast Kinetics with Multi-Condition Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13890
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#20197;&#20943;&#23569;&#23545;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#23545;&#27604;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#23545;&#27604;&#21058;&#21487;&#20197;&#23450;&#20301;&#32959;&#30244;&#24182;&#35266;&#23519;&#20854;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#36825;&#23545;&#20110;&#30284;&#30151;&#34920;&#24449;&#21644;&#27835;&#30103;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#27604;&#21058;&#30340;&#20351;&#29992;&#19981;&#20165;&#19982;&#19981;&#33391;&#20581;&#24247;&#39118;&#38505;&#30456;&#20851;&#65292;&#32780;&#19988;&#23545;&#20110;&#24576;&#23381;&#24739;&#32773;&#12289;&#32958;&#21151;&#33021;&#38556;&#30861;&#24739;&#32773;&#25110;&#20854;&#20182;&#19981;&#33391;&#21453;&#24212;&#24739;&#32773;&#23384;&#22312;&#38480;&#21046;&#12290;&#30001;&#20110;&#23545;&#27604;&#21058;&#25668;&#21462;&#26159;&#30149;&#28790;&#24694;&#24615;&#12289;&#30284;&#30151;&#22797;&#21457;&#39118;&#38505;&#21644;&#27835;&#30103;&#21453;&#24212;&#30340;&#20851;&#38190;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#22240;&#27492;&#20943;&#23569;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#36827;&#34892;DCE-MRI&#26102;&#38388;&#24207;&#21015;&#30340;&#33719;&#21462;&#26102;&#38388;&#26465;&#20214;&#22270;&#20687;&#21512;&#25104;&#30340;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#22522;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#21464;&#24322;&#24615;&#30340;Fr\'echet&#25918;&#23556;&#32452;&#23398;&#36317;&#31163;&#20316;&#20026;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13890v1 Announce Type: cross  Abstract: Contrast agents in dynamic contrast enhanced magnetic resonance imaging allow to localize tumors and observe their contrast kinetics, which is essential for cancer characterization and respective treatment decision-making. However, contrast agent administration is not only associated with adverse health risks, but also restricted for patients during pregnancy, and for those with kidney malfunction, or other adverse reactions. With contrast uptake as key biomarker for lesion malignancy, cancer recurrence risk, and treatment response, it becomes pivotal to reduce the dependency on intravenous contrast agent administration. To this end, we propose a multi-conditional latent diffusion model capable of acquisition time-conditioned image synthesis of DCE-MRI temporal sequences. To evaluate medical image synthesis, we additionally propose and validate the Fr\'echet radiomics distance as an image quality measure based on biomarker variability 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASP&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.09857</link><description>&lt;p&gt;
&#24102;&#26377;&#27880;&#24847;&#21147;&#24863;&#30693;&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09857
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASP&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#27169;&#22411;&#26088;&#22312;&#22312;&#20445;&#30041;&#26087;&#31867;&#30693;&#35782;&#30340;&#21516;&#26102;&#65292;&#36880;&#27493;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#31232;&#32570;&#26679;&#26412;&#12290;&#29616;&#26377;&#30340;FSCIL&#26041;&#27861;&#36890;&#24120;&#23545;&#25972;&#20010;&#39592;&#24178;&#36827;&#34892;&#24494;&#35843;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#24182;&#38459;&#30861;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#28508;&#21147;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26368;&#36817;&#22522;&#20110;&#25552;&#31034;&#30340;CIL&#26041;&#27861;&#36890;&#36807;&#22312;&#27599;&#20010;&#20219;&#21153;&#20013;&#29992;&#36275;&#22815;&#30340;&#25968;&#25454;&#35757;&#32451;&#25552;&#31034;&#26469;&#20943;&#36731;&#36951;&#24536;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#27880;&#24847;&#21147;&#24863;&#30693;&#33258;&#36866;&#24212;&#25552;&#31034;&#65288;ASP&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;ASP&#36890;&#36807;&#20174;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;ASP&#20013;&#30340;&#33258;&#36866;&#24212;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#25552;&#20379;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;&#24635;&#20043;&#65292;ASP&#38450;&#27490;&#20102;&#22312;&#22522;&#30784;&#20219;&#21153;&#19978;&#30340;&#36807;&#25311;&#21512;&#65292;&#24182;&#19981;&#38656;&#35201;&#22312;&#23569;&#26679;&#26412;&#22686;&#37327;&#20219;&#21153;&#20013;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09857v1 Announce Type: cross  Abstract: Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn new classes with scarce samples while preserving knowledge of old ones. Existing FSCIL methods usually fine-tune the entire backbone, leading to overfitting and hindering the potential to learn new classes. On the other hand, recent prompt-based CIL approaches alleviate forgetting by training prompts with sufficient data in each task. In this work, we propose a novel framework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages task-invariant prompts to capture shared knowledge by reducing specific information from the attention aspect. Additionally, self-adaptive task-specific prompts in ASP provide specific information and transfer knowledge from old classes to new classes with an Information Bottleneck learning objective. In summary, ASP prevents overfitting on base task and does not require enormous data in few-shot incremental tasks. Extensi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#25506;&#27979;&#25581;&#31034;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#30693;&#35782;&#65292;&#21457;&#29616;&#39318;&#20010;&#20196;&#29260;&#30340;logit&#20998;&#24067;&#21253;&#21547;&#36275;&#22815;&#20449;&#24687;&#65292;&#21487;&#20197;&#35782;&#21035;&#26080;&#27861;&#22238;&#31572;&#30340;&#35270;&#35273;&#38382;&#39064;&#12289;&#38450;&#33539;&#22810;&#27169;&#24577;&#36234;&#29425;&#25915;&#20987;&#20197;&#21450;&#35782;&#21035;&#27450;&#39575;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#30721;&#31574;&#30053;&#20197;&#26377;&#25928;&#25913;&#21892;&#29983;&#25104;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2403.09037</link><description>&lt;p&gt;
&#31532;&#19968;&#20010;&#30693;&#36947;&#65306;&#20196;&#29260;&#20998;&#24067;&#22914;&#20309;&#25581;&#31034;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#30693;&#35782;&#65311;
&lt;/p&gt;
&lt;p&gt;
The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#25506;&#27979;&#25581;&#31034;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#30693;&#35782;&#65292;&#21457;&#29616;&#39318;&#20010;&#20196;&#29260;&#30340;logit&#20998;&#24067;&#21253;&#21547;&#36275;&#22815;&#20449;&#24687;&#65292;&#21487;&#20197;&#35782;&#21035;&#26080;&#27861;&#22238;&#31572;&#30340;&#35270;&#35273;&#38382;&#39064;&#12289;&#38450;&#33539;&#22810;&#27169;&#24577;&#36234;&#29425;&#25915;&#20987;&#20197;&#21450;&#35782;&#21035;&#27450;&#39575;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#30721;&#31574;&#30053;&#20197;&#26377;&#25928;&#25913;&#21892;&#29983;&#25104;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#26088;&#22312;&#35299;&#37322;&#21644;&#21709;&#24212;&#20154;&#31867;&#25351;&#20196;&#65292;&#20294;&#30001;&#20110;&#19981;&#24403;&#25351;&#20196;&#32780;&#20598;&#23572;&#29983;&#25104;&#24187;&#35273;&#25110;&#26377;&#23475;&#20869;&#23481;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#32447;&#24615;&#25506;&#27979;&#26469;&#25581;&#31034;LVLMs&#36755;&#20986;&#23618;&#30340;&#38544;&#34255;&#30693;&#35782;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#39318;&#20010;&#20196;&#29260;&#30340;logit&#20998;&#24067;&#21253;&#21547;&#36275;&#22815;&#20449;&#24687;&#65292;&#21487;&#20197;&#30830;&#23450;&#26159;&#21542;&#24212;&#23545;&#25351;&#20196;&#20316;&#20986;&#21709;&#24212;&#65292;&#21253;&#25324;&#35782;&#21035;&#26080;&#27861;&#22238;&#31572;&#30340;&#35270;&#35273;&#38382;&#39064;&#12289;&#38450;&#33539;&#22810;&#27169;&#24577;&#36234;&#29425;&#25915;&#20987;&#20197;&#21450;&#35782;&#21035;&#27450;&#39575;&#24615;&#38382;&#39064;&#12290;&#36825;&#31181;&#38544;&#34255;&#30693;&#35782;&#22312;&#21709;&#24212;&#29983;&#25104;&#36807;&#31243;&#20013;&#38543;&#21518;&#20196;&#29260;&#30340;logit&#36880;&#28176;&#20002;&#22833;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35299;&#30721;&#31574;&#30053;&#22312;&#29983;&#25104;&#31532;&#19968;&#20010;&#20196;&#29260;&#26102;&#65292;&#26377;&#25928;&#25913;&#21892;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#35265;&#35299;&#65306;&#39318;&#20808;&#65292;CLIP&#27169;&#22411;&#24050;&#32463;&#21253;&#21547;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#30340;&#24378;&#28872;&#20449;&#21495;&#65292;&#34920;&#26126;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09037v1 Announce Type: cross  Abstract: Large vision-language models (LVLMs), designed to interpret and respond to human instructions, occasionally generate hallucinated or harmful content due to inappropriate instructions. This study uses linear probing to shed light on the hidden knowledge at the output layer of LVLMs. We demonstrate that the logit distributions of the first tokens contain sufficient information to determine whether to respond to the instructions, including recognizing unanswerable visual questions, defending against multi-modal jailbreaking attack, and identifying deceptive questions. Such hidden knowledge is gradually lost in logits of subsequent tokens during response generation. Then, we illustrate a simple decoding strategy at the generation of the first token, effectively improving the generated content. In experiments, we find a few interesting insights: First, the CLIP model already contains a strong signal for solving these tasks, indicating poten
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#37197;&#20934;&#26469;&#22686;&#24378;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#25104;&#21151;&#35757;&#32451;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#21367;&#31215;&#31639;&#27861;&#21644;transformers&#25216;&#26415;&#22635;&#34917;&#20102;&#30693;&#35782;&#24046;&#36317;&#65292;&#21462;&#24471;&#20102;0.9005&#30340;dice&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.17317</link><description>&lt;p&gt;
&#22914;&#20309;&#36194;&#24471;BraTS 2023&#25104;&#24180;&#33014;&#36136;&#30244;&#25361;&#25112;&#65311;&#20551;&#35013;&#32780;&#24050;&#65281;&#22686;&#24378;&#30340;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#21644;&#27169;&#22411;&#38598;&#25104;&#29992;&#20110;&#33041;&#32959;&#30244;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17317
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#37197;&#20934;&#26469;&#22686;&#24378;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#25104;&#21151;&#35757;&#32451;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#21367;&#31215;&#31639;&#27861;&#21644;transformers&#25216;&#26415;&#22635;&#34917;&#20102;&#30693;&#35782;&#24046;&#36317;&#65292;&#21462;&#24471;&#20102;0.9005&#30340;dice&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#39045;&#20869;&#32959;&#30244;&#20998;&#21106;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#20294;&#36825;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#23588;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#38590;&#20197;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#20351;&#29992;&#38750;&#20256;&#32479;&#30340;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#37197;&#20934;&#34987;&#29992;&#26469;&#22823;&#37327;&#22686;&#21152;&#21487;&#29992;&#26679;&#26412;&#25968;&#65292;&#29992;&#20110;&#35757;&#32451;&#19977;&#20010;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20998;&#21035;&#29992;&#20110;&#39045;&#20869;&#32959;&#30244;&#20998;&#21106;&#30340;BraTS2023&#25361;&#25112;&#30340;&#31532;&#19968;&#20010;&#20219;&#21153;&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#26159;&#26631;&#20934;nnU-Net&#65292;&#31532;&#20108;&#20010;&#26159;Swin UNETR&#65292;&#31532;&#19977;&#20010;&#26159;BraTS 2021&#25361;&#25112;&#30340;&#33719;&#32988;&#26041;&#26696;&#12290;&#25972;&#20010;&#27969;&#31243;&#22522;&#20110;nnU-Net&#23454;&#29616;&#65292;&#38500;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#29983;&#25104;&#12290;&#21367;&#31215;&#31639;&#27861;&#21644;transformers&#30340;&#20351;&#29992;&#33021;&#22815;&#22635;&#34917;&#24444;&#27492;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;&#20351;&#29992;&#26032;&#25351;&#26631;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#36798;&#21040;&#20102;0.9005&#30340;dice&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17317v1 Announce Type: cross  Abstract: Deep Learning is the state-of-the-art technology for segmenting brain tumours. However, this requires a lot of high-quality data, which is difficult to obtain, especially in the medical field. Therefore, our solutions address this problem by using unconventional mechanisms for data augmentation. Generative adversarial networks and registration are used to massively increase the amount of available samples for training three different deep learning models for brain tumour segmentation, the first task of the BraTS2023 challenge. The first model is the standard nnU-Net, the second is the Swin UNETR and the third is the winning solution of the BraTS 2021 Challenge. The entire pipeline is built on the nnU-Net implementation, except for the generation of the synthetic data. The use of convolutional algorithms and transformers is able to fill each other's knowledge gaps. Using the new metric, our best solution achieves the dice results 0.9005
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#36817;&#37051;&#35780;&#20998;&#20989;&#25968;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22810;&#20010;&#26679;&#26412;&#22823;&#22823;&#38477;&#20302;&#20102;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12289;&#26679;&#26412;&#36136;&#37327;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08018</link><description>&lt;p&gt;
&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#36817;&#37051;&#35780;&#20998;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Nearest Neighbour Score Estimators for Diffusion Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#36817;&#37051;&#35780;&#20998;&#20989;&#25968;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22810;&#20010;&#26679;&#26412;&#22823;&#22823;&#38477;&#20302;&#20102;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12289;&#26679;&#26412;&#36136;&#37327;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20998;&#20989;&#25968;&#20272;&#35745;&#26159;&#35757;&#32451;&#21644;&#37319;&#26679;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#26368;&#24120;&#29992;&#30340;&#20272;&#35745;&#22120;&#35201;&#20040;&#26159;&#26377;&#20559;&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#65292;&#35201;&#20040;&#26159;&#22522;&#20110;&#26465;&#20214;&#35780;&#20998;&#30340;&#39640;&#26041;&#24046;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26368;&#36817;&#37051;&#35780;&#20998;&#20989;&#25968;&#20272;&#35745;&#22120;&#65292;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22810;&#20010;&#26679;&#26412;&#22823;&#22823;&#38477;&#20302;&#20102;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#24212;&#29992;&#20013;&#21033;&#29992;&#20102;&#20302;&#26041;&#24046;&#20272;&#35745;&#22120;&#12290;&#22312;&#20351;&#29992;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#36827;&#34892;&#35757;&#32451;&#19968;&#33268;&#24615;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#25910;&#25947;&#36895;&#24230;&#21644;&#26679;&#26412;&#36136;&#37327;&#26174;&#33879;&#25552;&#39640;&#12290;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#21487;&#20197;&#26367;&#20195;&#23398;&#20064;&#32593;&#32476;&#36827;&#34892;&#27010;&#29575;&#27969;ODE&#31215;&#20998;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#24320;&#36767;&#20102;&#26377;&#21069;&#26223;&#30340;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score function estimation is the cornerstone of both training and sampling from diffusion generative models. Despite this fact, the most commonly used estimators are either biased neural network approximations or high variance Monte Carlo estimators based on the conditional score. We introduce a novel nearest neighbour score function estimator which utilizes multiple samples from the training set to dramatically decrease estimator variance. We leverage our low variance estimator in two compelling applications. Training consistency models with our estimator, we report a significant increase in both convergence speed and sample quality. In diffusion models, we show that our estimator can replace a learned network for probability-flow ODE integration, opening promising new avenues of future research.
&lt;/p&gt;</description></item><item><title>Scissorhands &#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#25935;&#24863;&#24615;&#35782;&#21035;&#19982;&#36951;&#24536;&#25968;&#25454;&#30456;&#20851;&#30340;&#26368;&#30456;&#20851;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#20462;&#21098;&#30340;&#27169;&#22411;&#26469;&#25830;&#38500;&#25968;&#25454;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.06187</link><description>&lt;p&gt;
Scissorhands: &#36890;&#36807;&#32593;&#32476;&#36830;&#25509;&#25935;&#24863;&#24615;&#22312;&#25968;&#25454;&#24433;&#21709;&#20013;&#36827;&#34892;&#25968;&#25454;&#25830;&#38500;
&lt;/p&gt;
&lt;p&gt;
Scissorhands: Scrub Data Influence via Connection Sensitivity in Networks. (arXiv:2401.06187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06187
&lt;/p&gt;
&lt;p&gt;
Scissorhands &#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#25935;&#24863;&#24615;&#35782;&#21035;&#19982;&#36951;&#24536;&#25968;&#25454;&#30456;&#20851;&#30340;&#26368;&#30456;&#20851;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#20462;&#21098;&#30340;&#27169;&#22411;&#26469;&#25830;&#38500;&#25968;&#25454;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#26088;&#22312;&#25830;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#24433;&#21709;&#12290;&#23427;&#31526;&#21512;&#26368;&#26032;&#30340;&#25968;&#25454;&#30417;&#31649;&#26631;&#20934;&#65292;&#22686;&#24378;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#35775;&#38382;&#20854;&#20313;&#25968;&#25454;&#30340;&#20840;&#37096;&#20869;&#23481;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#8220;Scissorhands&#8221;&#65292;&#23427;&#21482;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26469;&#26377;&#25928;&#36816;&#34892;&#12290;&#21021;&#22987;&#38454;&#27573;&#65292;Scissorhands&#36890;&#36807;&#36830;&#25509;&#25935;&#24863;&#24615;&#22312;&#32473;&#23450;&#27169;&#22411;&#20013;&#35782;&#21035;&#19982;&#36951;&#24536;&#25968;&#25454;&#30456;&#20851;&#30340;&#26368;&#30456;&#20851;&#21442;&#25968;&#12290;&#35813;&#36807;&#31243;&#36890;&#36807;&#37325;&#26032;&#21021;&#22987;&#21270;&#36825;&#20123;&#21442;&#25968;&#20013;&#20855;&#26377;&#26368;&#22823;&#24433;&#21709;&#21147;&#30340;&#21069;k%&#30340;&#26368;&#30456;&#20851;&#21442;&#25968;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#29992;&#20110;&#25830;&#38500;&#36951;&#24536;&#25968;&#25454;&#24433;&#21709;&#30340;&#20462;&#21098;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;Scissorhands&#36890;&#36807;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#36807;&#31243;&#23545;&#20462;&#21098;&#30340;&#27169;&#22411;&#36827;&#34892;&#20877;&#35757;&#32451;&#65292;&#23547;&#25214;&#20445;&#30041;&#20449;&#24687;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning has become a pivotal task to erase the influence of data from a trained model. It adheres to recent data regulation standards and enhances the privacy and security of machine learning applications. Most existing machine unlearning methods perform well, however, they typically necessitate access to the entirety of the remaining data, which might not be feasible in certain scenarios. In this work, we present a new machine unlearning approach Scissorhands, which operates effectively with only a subset of the training data. Initially, Scissorhands identifies the most pertinent parameters in the given model relative to the forgetting data via connection sensitivity. This process involves reinitializing the most influential top-$k$ percent of these parameters, resulting in a trimmed model for erasing the influence of the forgetting data. Subsequently, Scissorhands retrains the trimmed model through a min-max optimization process, seeking parameters that preserve informatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24230;&#37327;&#29983;&#25104;&#22270;&#20687;&#38598;&#19982;&#35757;&#32451;&#38598;&#20851;&#20110;&#23646;&#24615;&#24378;&#24230;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#30456;&#20284;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.17261</link><description>&lt;p&gt;
&#22522;&#20110;&#23646;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Attribute Based Interpretable Evaluation Metrics for Generative Models. (arXiv:2310.17261v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23646;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24230;&#37327;&#29983;&#25104;&#22270;&#20687;&#38598;&#19982;&#35757;&#32451;&#38598;&#20851;&#20110;&#23646;&#24615;&#24378;&#24230;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#30456;&#20284;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#29399;&#21644;&#29483;&#30340;&#27604;&#20363;&#20026;1:1&#26102;&#65292;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#29399;&#21644;&#29483;&#20063;&#24212;&#26356;&#22909;&#22320;&#31526;&#21512;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#21482;&#25552;&#20379;&#20102;&#8220;&#22810;&#26679;&#24615;&#8221;&#36825;&#20010;&#35299;&#37322;&#24615;&#20043;&#22806;&#30340;&#32500;&#24230;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#24230;&#37327;&#29983;&#25104;&#22270;&#20687;&#38598;&#19982;&#35757;&#32451;&#38598;&#20851;&#20110;&#23646;&#24615;&#24378;&#24230;&#20998;&#24067;&#30340;&#24046;&#24322;&#26469;&#25429;&#25417;&#36825;&#31181;&#29616;&#35937;&#12290;&#21333;&#23646;&#24615;&#24046;&#24322;&#65288;SaD&#65289;&#34913;&#37327;&#20102;&#20851;&#20110;&#21333;&#20010;&#23646;&#24615;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#24046;&#24322;&#12290;&#21452;&#23646;&#24615;&#24046;&#24322;&#65288;PaD&#65289;&#34913;&#37327;&#20102;&#20851;&#20110;&#19968;&#23545;&#23646;&#24615;&#30340;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#24046;&#24322;&#12290;&#23427;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#23646;&#24615;&#12290;&#20026;&#20102;&#34913;&#37327;&#22270;&#20687;&#30340;&#23646;&#24615;&#24378;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24322;&#26500;CLIP&#35780;&#20998;&#65288;HCS&#65289;&#65292;&#23427;&#36890;&#36807;&#27979;&#37327;&#22270;&#20687;&#21644;&#25991;&#26412;&#21521;&#37327;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
When the training dataset comprises a 1:1 proportion of dogs to cats, a generative model that produces 1:1 dogs and cats better resembles the training species distribution than another model with 3:1 dogs and cats. Can we capture this phenomenon using existing metrics? Unfortunately, we cannot, because these metrics do not provide any interpretability beyond "diversity". In this context, we propose a new evaluation protocol that measures the divergence of a set of generated images from the training set regarding the distribution of attribute strengths as follows. Single-attribute Divergence (SaD) measures the divergence regarding PDFs of a single attribute. Paired-attribute Divergence (PaD) measures the divergence regarding joint PDFs of a pair of attributes. They provide which attributes the models struggle. For measuring the attribute strengths of an image, we propose Heterogeneous CLIPScore (HCS) which measures the cosine similarity between image and text vectors with heterogeneous 
&lt;/p&gt;</description></item><item><title>EGIC&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;&#20302;&#20301;&#36895;&#29575;&#29983;&#25104;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#20998;&#21106;&#25552;&#20379;&#25351;&#23548;&#12290;&#23427;&#22312;&#22833;&#30495;&#24863;&#30693;&#21644;&#22833;&#30495;&#26041;&#21521;&#22522;&#32447;&#26041;&#27861;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#24182;&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#20248;&#31168;&#30340;&#25554;&#20540;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03244</link><description>&lt;p&gt;
EGIC:&#22686;&#24378;&#30340;&#20302;&#20301;&#36895;&#29575;&#29983;&#25104;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#22312;&#35821;&#20041;&#20998;&#21106;&#30340;&#25351;&#23548;&#19979;
&lt;/p&gt;
&lt;p&gt;
EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by Semantic Segmentation. (arXiv:2309.03244v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03244
&lt;/p&gt;
&lt;p&gt;
EGIC&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;&#20302;&#20301;&#36895;&#29575;&#29983;&#25104;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#20998;&#21106;&#25552;&#20379;&#25351;&#23548;&#12290;&#23427;&#22312;&#22833;&#30495;&#24863;&#30693;&#21644;&#22833;&#30495;&#26041;&#21521;&#22522;&#32447;&#26041;&#27861;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#24182;&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#20248;&#31168;&#30340;&#25554;&#20540;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;EGIC&#65292;&#23427;&#20801;&#35768;&#20174;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#26377;&#25928;&#22320;&#36941;&#21382;&#22833;&#30495;&#24863;&#30693;&#26354;&#32447;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#32534;&#30721;&#30340;&#22270;&#20687;&#25554;&#20540;&#21464;&#20307;&#65292;&#29992;&#20110;&#39044;&#27979;&#22312;MSE&#20248;&#21270;&#21644;GAN&#20248;&#21270;&#35299;&#30721;&#22120;&#36755;&#20986;&#20043;&#38388;&#30340;&#27531;&#24046;&#12290;&#22312;&#25509;&#25910;&#31471;&#65292;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#27531;&#24046;&#23545;&#22522;&#20110;GAN&#30340;&#37325;&#24314;&#30340;&#24433;&#21709;&#12290;&#32467;&#21512;&#25913;&#36827;&#30340;&#22522;&#20110;GAN&#30340;&#26500;&#24314;&#22359;&#65292;EGIC&#22312;&#24863;&#30693;&#23548;&#21521;&#21644;&#22833;&#30495;&#23548;&#21521;&#30340;&#22522;&#32447;&#26041;&#27861;&#65288;&#21253;&#25324;HiFiC&#65292;MRIC&#21644;DIRAC&#65289;&#19978;&#34920;&#29616;&#20248;&#20110;&#22823;&#22810;&#25968;&#26041;&#27861;&#65292;&#22312;&#22833;&#30495;&#31471;&#19982;VTM-20.0&#20960;&#20046;&#30456;&#24403;&#12290;EGIC&#23454;&#29616;&#31616;&#21333;&#65292;&#38750;&#24120;&#36731;&#37327;&#32423;&#65288;&#19982;HiFiC&#30456;&#27604;&#65292;&#27169;&#22411;&#21442;&#25968;&#21482;&#26377;0.18&#20493;&#65289;&#65292;&#24182;&#25552;&#20379;&#20248;&#24322;&#30340;&#25554;&#20540;&#29305;&#24615;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#38024;&#23545;&#20302;&#20301;&#33539;&#22260;&#30340;&#23454;&#38469;&#24212;&#29992;&#30340;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce EGIC, a novel generative image compression method that allows traversing the distortion-perception curve efficiently from a single model. Specifically, we propose an implicitly encoded variant of image interpolation that predicts the residual between a MSE-optimized and GAN-optimized decoder output. On the receiver side, the user can then control the impact of the residual on the GAN-based reconstruction. Together with improved GAN-based building blocks, EGIC outperforms a wide-variety of perception-oriented and distortion-oriented baselines, including HiFiC, MRIC and DIRAC, while performing almost on par with VTM-20.0 on the distortion end. EGIC is simple to implement, very lightweight (e.g. 0.18x model parameters compared to HiFiC) and provides excellent interpolation characteristics, which makes it a promising candidate for practical applications targeting the low bit range.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#20854;&#20316;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26469;&#25552;&#39640;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#33298;&#36866;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05731</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#30340;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review. (arXiv:2308.05731v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05731
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#20854;&#20316;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26469;&#25552;&#39640;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#33298;&#36866;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#20010;&#20154;&#12289;&#20844;&#20849;&#21644;&#36135;&#36816;&#20132;&#36890;&#30340;&#26041;&#24335;&#12290;&#38500;&#20102;&#24863;&#30693;&#29615;&#22659;&#30340;&#24040;&#22823;&#25361;&#25112;&#22806;&#65292;&#21363;&#20934;&#30830;&#22320;&#20351;&#29992;&#21487;&#29992;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#24863;&#30693;&#29615;&#22659;&#65292;&#33258;&#21160;&#39550;&#39542;&#36824;&#21253;&#25324;&#35268;&#21010;&#19968;&#20010;&#23433;&#20840;&#12289;&#33298;&#36866;&#21644;&#39640;&#25928;&#30340;&#36816;&#21160;&#36712;&#36857;&#12290;&#20026;&#20102;&#20419;&#36827;&#23433;&#20840;&#21644;&#36827;&#27493;&#65292;&#35768;&#22810;&#24037;&#20316;&#20381;&#36182;&#20110;&#27169;&#22359;&#21270;&#30340;&#20132;&#36890;&#26410;&#26469;&#36816;&#21160;&#30340;&#39044;&#27979;&#12290;&#27169;&#22359;&#21270;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#36890;&#24120;&#23558;&#39044;&#27979;&#21644;&#35268;&#21010;&#20316;&#20026;&#39034;&#24207;&#30340;&#29420;&#31435;&#20219;&#21153;&#22788;&#29702;&#12290;&#34429;&#28982;&#36825;&#32771;&#34385;&#20102;&#21608;&#22260;&#20132;&#36890;&#23545;&#33258;&#36710;&#30340;&#24433;&#21709;&#65292;&#20294;&#23427;&#26410;&#33021;&#39044;&#27979;&#20132;&#36890;&#21442;&#19982;&#32773;&#23545;&#33258;&#36710;&#34892;&#20026;&#30340;&#21453;&#24212;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#39044;&#27979;&#21644;&#35268;&#21010;&#25972;&#21512;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26159;&#23454;&#29616;&#23433;&#20840;&#12289;&#39640;&#25928;&#21644;&#33298;&#36866;&#39550;&#39542;&#25152;&#24517;&#38656;&#30340;&#12290;&#34429;&#28982;&#26377;&#21508;&#31181;&#27169;&#22411;&#23454;&#29616;&#20102;&#36825;&#31181;&#38598;&#25104;&#31995;&#32479;&#65292;&#20294;&#23545;&#19981;&#21516;&#21407;&#29702;&#30340;&#20840;&#38754;&#27010;&#36848;&#21644;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated driving has the potential to revolutionize personal, public, and freight mobility. Besides the enormous challenge of perception, i.e. accurately perceiving the environment using available sensor data, automated driving comprises planning a safe, comfortable, and efficient motion trajectory. To promote safety and progress, many works rely on modules that predict the future motion of surrounding traffic. Modular automated driving systems commonly handle prediction and planning as sequential separate tasks. While this accounts for the influence of surrounding traffic on the ego-vehicle, it fails to anticipate the reactions of traffic participants to the ego-vehicle's behavior. Recent works suggest that integrating prediction and planning in an interdependent joint step is necessary to achieve safe, efficient, and comfortable driving. While various models implement such integrated systems, a comprehensive overview and theoretical understanding of different principles are lacking.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#24179;&#34913;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20943;&#23569;&#20102;&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#23398;&#20064;&#21040;&#19982;&#25968;&#25454;&#38598;&#20559;&#24046;&#30456;&#20851;&#30340;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04553</link><description>&lt;p&gt;
&#20174;&#20551;&#21040;&#30495;&#65288;FFR&#65289;&#65306;&#19968;&#31181;&#29992;&#20110;&#20943;&#23569;&#19982;&#21512;&#25104;&#25968;&#25454;&#30456;&#20851;&#24615;&#38169;&#35823;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
From Fake to Real (FFR): A two-stage training pipeline for mitigating spurious correlations with synthetic data. (arXiv:2308.04553v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#24179;&#34913;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20943;&#23569;&#20102;&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#23398;&#20064;&#21040;&#19982;&#25968;&#25454;&#38598;&#20559;&#24046;&#30456;&#20851;&#30340;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#23481;&#26131;&#23398;&#20064;&#21040;&#30001;&#20110;&#35757;&#32451;&#38598;&#30340;&#19981;&#24179;&#34913;&#23548;&#33268;&#30340;&#30456;&#20851;&#24615;&#38169;&#35823;&#65292;&#20854;&#20013;&#26576;&#20123;&#32676;&#20307;&#65288;&#22914;&#22899;&#24615;&#65289;&#22312;&#26576;&#20123;&#31867;&#21035;&#65288;&#22914;&#31243;&#24207;&#21592;&#65289;&#20013;&#20195;&#34920;&#24615;&#19981;&#36275;&#12290;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#20026;&#23569;&#25968;&#26679;&#26412;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#20943;&#23569;&#36825;&#31181;&#20559;&#24046;&#65292;&#20174;&#32780;&#24179;&#34913;&#35757;&#32451;&#38598;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#24037;&#20316;&#24573;&#35270;&#20102;&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#24448;&#24448;&#33021;&#22815;&#23398;&#20064;&#21306;&#20998;&#30495;&#23454;&#22270;&#20687;&#21644;&#21512;&#25104;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#26080;&#27861;&#28040;&#38500;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#30340;&#20559;&#24046;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#27969;&#31243;&#26469;&#20943;&#23569;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;1&#65289;&#25105;&#20204;&#22312;&#24179;&#34913;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;2&#65289;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#20351;&#29992;&#36825;&#20010;&#27969;&#31243;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#22312;&#31532;&#19968;&#27493;&#20013;&#25105;&#20204;&#23398;&#20064;&#21040;&#20102;&#25269;&#25239;&#20559;&#24046;&#30340;&#31283;&#20581;&#29305;&#24449;&#65292;&#22312;&#31532;&#20108;&#27493;&#20013;&#20943;&#36731;&#20102;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual recognition models are prone to learning spurious correlations induced by an imbalanced training set where certain groups (\eg Females) are under-represented in certain classes (\eg Programmers). Generative models offer a promising direction in mitigating this bias by generating synthetic data for the minority samples and thus balancing the training set. However, prior work that uses these approaches overlooks that visual recognition models could often learn to differentiate between real and synthetic images and thus fail to unlearn the bias in the original dataset. In our work, we propose a novel two-stage pipeline to mitigate this issue where 1) we pre-train a model on a balanced synthetic dataset and then 2) fine-tune on the real data. Using this pipeline, we avoid training on both real and synthetic data, thus avoiding the bias between real and synthetic data. Moreover, we learn robust features against the bias in the first step that mitigate the bias in the second step. Mor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36816;&#21160;&#22686;&#37327;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#35299;&#32806;&#26102;&#22495;&#21644;&#31354;&#22495;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#25552;&#21462;&#26356;&#22810;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.01097</link><description>&lt;p&gt;
&#21033;&#29992;&#36816;&#21160;&#22686;&#37327;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Branching for Motion Prediction using Motion Increments. (arXiv:2308.01097v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36816;&#21160;&#22686;&#37327;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#35299;&#32806;&#26102;&#22495;&#21644;&#31354;&#22495;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#25552;&#21462;&#26356;&#22810;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#24050;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#20294;&#30001;&#20110;&#26410;&#26469;&#23039;&#21183;&#30340;&#38543;&#26426;&#21644;&#19981;&#35268;&#21017;&#24615;&#36136;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#24037;&#29305;&#24449;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24448;&#24448;&#38590;&#20197;&#24314;&#27169;&#20154;&#20307;&#36816;&#21160;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#12290;&#26368;&#36817;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#36816;&#21160;&#30340;&#26102;&#31354;&#34920;&#31034;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#24120;&#24120;&#24573;&#35270;&#36816;&#21160;&#25968;&#25454;&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#39592;&#26550;&#33410;&#28857;&#30340;&#26102;&#22495;&#21644;&#31354;&#22495;&#20381;&#36182;&#24615;&#26159;&#19981;&#21516;&#30340;&#12290;&#26102;&#22495;&#20851;&#31995;&#25429;&#25417;&#21040;&#38543;&#26102;&#38388;&#30340;&#36816;&#21160;&#20449;&#24687;&#65292;&#32780;&#31354;&#22495;&#20851;&#31995;&#25551;&#36848;&#20102;&#36523;&#20307;&#32467;&#26500;&#21644;&#19981;&#21516;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#29992;&#22686;&#37327;&#20449;&#24687;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;&#32593;&#32476;&#65292;&#23427;&#35299;&#32806;&#20102;&#26102;&#22495;&#21644;&#31354;&#22495;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#25552;&#21462;&#20102;&#26356;&#22810;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human motion prediction (HMP) has emerged as a popular research topic due to its diverse applications, but it remains a challenging task due to the stochastic and aperiodic nature of future poses. Traditional methods rely on hand-crafted features and machine learning techniques, which often struggle to model the complex dynamics of human motion. Recent deep learning-based methods have achieved success by learning spatio-temporal representations of motion, but these models often overlook the reliability of motion data. Additionally, the temporal and spatial dependencies of skeleton nodes are distinct. The temporal relationship captures motion information over time, while the spatial relationship describes body structure and the relationships between different nodes. In this paper, we propose a novel spatio-temporal branching network using incremental information for HMP, which decouples the learning of temporal-domain and spatial-domain features, extracts more motion information, and ac
&lt;/p&gt;</description></item><item><title>DIFFender&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20301;&#21644;&#24674;&#22797;&#20004;&#20010;&#38454;&#27573;&#30340;&#25805;&#20316;&#65292;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25972;&#20307;&#38450;&#24481;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09124</link><description>&lt;p&gt;
DIFFender&#65306;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#29992;&#20110;&#25269;&#24481;Patch&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks. (arXiv:2306.09124v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09124
&lt;/p&gt;
&lt;p&gt;
DIFFender&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20301;&#21644;&#24674;&#22797;&#20004;&#20010;&#38454;&#27573;&#30340;&#25805;&#20316;&#65292;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25972;&#20307;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;Patch&#25915;&#20987;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#24320;&#21457;&#21487;&#38752;&#30340;&#38450;&#24481;&#26041;&#27861;&#20197;&#25269;&#24481;Patch&#25915;&#20987;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#24403;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#36824;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIFFender&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#12290;DIFFender&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#65306;Patch&#23450;&#20301;&#21644;Patch&#24674;&#22797;&#12290;&#22312;&#23450;&#20301;&#38454;&#27573;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#21033;&#29992;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#20010;&#26377;&#36259;&#29305;&#24615;&#65292;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#23545;&#25239;&#24615;Patch&#30340;&#20301;&#32622;&#12290;&#22312;&#24674;&#22797;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#37325;&#24314;&#22270;&#20687;&#20013;&#30340;&#23545;&#25239;&#24615;&#21306;&#22495;&#21516;&#26102;&#20445;&#25345;&#35270;&#35273;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20004;&#20010;&#38454;&#27573;&#37117;&#21463;&#21040;&#32479;&#19968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#31934;&#24515;&#24341;&#23548;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#32039;&#23494;&#30456;&#20114;&#20316;&#29992;&#26469;&#25552;&#39640;&#25972;&#20010;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks, particularly patch attacks, pose significant threats to the robustness and reliability of deep learning models. Developing reliable defenses against patch attacks is crucial for real-world applications, yet current research in this area is not satisfactory. In this paper, we propose DIFFender, a novel defense method that leverages a text-guided diffusion model to defend against adversarial patches. DIFFender includes two main stages: patch localization and patch restoration. In the localization stage, we find and exploit an intriguing property of the diffusion model to effectively identify the locations of adversarial patches. In the restoration stage, we employ the diffusion model to reconstruct the adversarial regions in the images while preserving the integrity of the visual content. Importantly, these two stages are carefully guided by a unified diffusion model, thus we can utilize the close interaction between them to improve the whole defense performance. Mor
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#25915;&#20987;&#26816;&#27979;&#21644;&#32531;&#35299;&#38382;&#39064;&#12290;&#30001;&#20110;&#36825;&#31181;&#25915;&#20987;&#21361;&#38505;&#38544;&#21311;&#65292;&#19988;&#22312;&#19979;&#28216;&#20998;&#31867;&#22120;&#20013;&#24456;&#38590;&#26816;&#27979;&#20986;&#26469;&#12290;&#30446;&#21069;&#22312;&#36229;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#28508;&#22312;&#22320;&#20445;&#25252;SSL&#19979;&#28216;&#20998;&#31867;&#22120;&#65292;&#20294;&#22312;&#20854;&#24191;&#27867;&#20256;&#25773;&#20043;&#21069;&#35782;&#21035;&#21644;&#22788;&#29702;SSL&#32534;&#30721;&#22120;&#20013;&#30340;&#35302;&#21457;&#22120;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.09079</link><description>&lt;p&gt;
SSL&#28165;&#29702;&#65306;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#26816;&#27979;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
SSL-Cleanse: Trojan Detection and Mitigation in Self-Supervised Learning. (arXiv:2303.09079v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#25915;&#20987;&#26816;&#27979;&#21644;&#32531;&#35299;&#38382;&#39064;&#12290;&#30001;&#20110;&#36825;&#31181;&#25915;&#20987;&#21361;&#38505;&#38544;&#21311;&#65292;&#19988;&#22312;&#19979;&#28216;&#20998;&#31867;&#22120;&#20013;&#24456;&#38590;&#26816;&#27979;&#20986;&#26469;&#12290;&#30446;&#21069;&#22312;&#36229;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#28508;&#22312;&#22320;&#20445;&#25252;SSL&#19979;&#28216;&#20998;&#31867;&#22120;&#65292;&#20294;&#22312;&#20854;&#24191;&#27867;&#20256;&#25773;&#20043;&#21069;&#35782;&#21035;&#21644;&#22788;&#29702;SSL&#32534;&#30721;&#22120;&#20013;&#30340;&#35302;&#21457;&#22120;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#23398;&#20064;&#21644;&#32534;&#30721;&#25968;&#25454;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;SSL&#22270;&#20687;&#32534;&#30721;&#22120;&#24182;&#22312;&#20854;&#39030;&#37096;&#35757;&#32451;&#19979;&#28216;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#32780;&#21482;&#38656;&#24456;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;SSL&#30340;&#22686;&#21152;&#20351;&#29992;&#23548;&#33268;&#20102;&#19982;SSL&#32534;&#30721;&#22120;&#30456;&#20851;&#30340;&#23433;&#20840;&#30740;&#31350;&#21644;&#21508;&#31181;&#26408;&#39532;&#25915;&#20987;&#30340;&#21457;&#23637;&#12290;&#22312;SSL&#32534;&#30721;&#22120;&#20013;&#25554;&#20837;&#26408;&#39532;&#25915;&#20987;&#30340;&#21361;&#38505;&#22312;&#20110;&#23427;&#20204;&#33021;&#22815;&#38544;&#34109;&#22320;&#25805;&#20316;&#24182;&#22312;&#21508;&#31181;&#29992;&#25143;&#21644;&#35774;&#22791;&#20043;&#38388;&#24191;&#27867;&#20256;&#25773;&#12290;Trojaned&#32534;&#30721;&#22120;&#20013;&#30340;&#21518;&#38376;&#34892;&#20026;&#30340;&#23384;&#22312;&#21487;&#33021;&#20250;&#34987;&#19979;&#28216;&#20998;&#31867;&#22120;&#24847;&#22806;&#32487;&#25215;&#65292;&#20351;&#26816;&#27979;&#21644;&#32531;&#35299;&#23041;&#32961;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#34429;&#28982;&#36229;&#30417;&#30563;&#23398;&#20064;&#20013;&#24403;&#21069;&#30340;&#26408;&#39532;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#28508;&#22312;&#22320;&#20445;&#25252;SSL&#19979;&#28216;&#20998;&#31867;&#22120;&#65292;&#20294;&#22312;&#20854;&#24191;&#27867;&#20256;&#25773;&#20043;&#21069;&#35782;&#21035;&#21644;&#22788;&#29702;SSL&#32534;&#30721;&#22120;&#20013;&#30340;&#35302;&#21457;&#22120;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) is a commonly used approach to learning and encoding data representations. By using a pre-trained SSL image encoder and training a downstream classifier on top of it, impressive performance can be achieved on various tasks with very little labeled data. The increasing usage of SSL has led to an uptick in security research related to SSL encoders and the development of various Trojan attacks. The danger posed by Trojan attacks inserted in SSL encoders lies in their ability to operate covertly and spread widely among various users and devices. The presence of backdoor behavior in Trojaned encoders can inadvertently be inherited by downstream classifiers, making it even more difficult to detect and mitigate the threat. Although current Trojan detection methods in supervised learning can potentially safeguard SSL downstream classifiers, identifying and addressing triggers in the SSL encoder before its widespread dissemination is a challenging task. This is be
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#23545;&#25239;&#25915;&#20987;&#20256;&#36882;&#24230;&#37327;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#37327;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#26550;&#26500;&#30456;&#20284;&#24230;&#20989;&#25968;&#65292;&#20998;&#26512;&#20102;69&#20010;&#26368;&#20808;&#36827;&#30340;ImageNet&#20998;&#31867;&#22120;&#65292;&#21457;&#29616;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#26550;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#38598;&#21512;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.11407</link><description>&lt;p&gt;
&#22522;&#20110;&#36755;&#20837;&#26799;&#24230;&#20256;&#36882;&#30340;&#31070;&#32463;&#26550;&#26500;&#30456;&#20284;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Similarity of Neural Architectures Based on Input Gradient Transferability. (arXiv:2210.11407v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#23545;&#25239;&#25915;&#20987;&#20256;&#36882;&#24230;&#37327;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#37327;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#26550;&#26500;&#30456;&#20284;&#24230;&#20989;&#25968;&#65292;&#20998;&#26512;&#20102;69&#20010;&#26368;&#20808;&#36827;&#30340;ImageNet&#20998;&#31867;&#22120;&#65292;&#21457;&#29616;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#26550;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#38598;&#21512;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20026;&#22270;&#20687;&#20998;&#31867;&#32780;&#24320;&#21457;&#20102;&#22823;&#37327;&#30340;&#28145;&#24230;&#31070;&#32463;&#26550;&#26500;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#30456;&#20284;&#25110;&#19981;&#21516;&#65292;&#20197;&#21450;&#20160;&#20040;&#22240;&#32032;&#24433;&#21709;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#25110;&#19981;&#21516;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#37327;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#26550;&#26500;&#30456;&#20284;&#24230;&#20989;&#25968;&#20197;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#25239;&#25915;&#20987;&#20256;&#36882;&#24230;&#37327;&#65292;&#35813;&#24230;&#37327;&#20855;&#26377;&#19982;&#36755;&#20837;&#26799;&#24230;&#21644;&#20915;&#31574;&#36793;&#30028;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#29702;&#35299;&#27169;&#22411;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#30456;&#20284;&#24230;&#20989;&#25968;&#23545;69&#20010;&#26368;&#20808;&#36827;&#30340;ImageNet&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20998;&#26512;&#65292;&#20174;&#32780;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;&#31070;&#32463;&#26550;&#26500;&#30456;&#20851;&#30340;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#22810;&#26679;&#24615;&#21487;&#20197;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#23545;&#27169;&#22411;&#38598;&#21512;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#20026;&#20160;&#20040;&#24320;&#21457;&#20855;&#26377;&#19981;&#21516;&#32452;&#20214;&#30340;&#22810;&#26679;&#21270;&#31070;&#32463;&#26550;&#26500;&#26159;&#24517;&#35201;&#30340;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, a huge amount of deep neural architectures have been developed for image classification. It remains curious whether these models are similar or different and what factors contribute to their similarities or differences. To address this question, we aim to design a quantitative and scalable similarity function between neural architectures. We utilize adversarial attack transferability, which has information related to input gradients and decision boundaries that are widely used to understand model behaviors. We conduct a large-scale analysis on 69 state-of-the-art ImageNet classifiers using our proposed similarity function to answer the question. Moreover, we observe neural architecture-related phenomena using model similarity that model diversity can lead to better performance on model ensembles and knowledge distillation under specific conditions. Our results provide insights into why the development of diverse neural architectures with distinct components is necessar
&lt;/p&gt;</description></item></channel></rss>