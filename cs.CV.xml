<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMP-Attack&#30340;&#26377;&#38024;&#23545;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25915;&#20987;&#21830;&#19994;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01369</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24335;&#20808;&#39564;&#30340;&#26377;&#38024;&#23545;&#24615;&#25915;&#20987;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cheating Suffix: Targeted Attack to Text-To-Image Diffusion Models with Multi-Modal Priors
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMP-Attack&#30340;&#26377;&#38024;&#23545;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25915;&#20987;&#21830;&#19994;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#23637;&#29616;&#20102;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#21331;&#36234;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#34987;&#24694;&#24847;&#21033;&#29992;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#22312;&#21407;&#22987;&#25552;&#31034;&#21518;&#38468;&#21152;&#29305;&#23450;&#21518;&#32512;&#26469;&#29983;&#25104;&#26377;&#23475;&#25110;&#25935;&#24863;&#22270;&#20687;&#12290;&#29616;&#26377;&#20316;&#21697;&#20027;&#35201;&#20851;&#27880;&#20351;&#29992;&#21333;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#25915;&#20987;&#65292;&#26410;&#33021;&#21033;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMP-Attack&#30340;&#26377;&#38024;&#23545;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#23427;&#23558;&#22810;&#27169;&#24577;&#20808;&#39564;&#65288;MMP&#65289;&#21363;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#36827;&#34892;&#25972;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MMP-Attack&#30340;&#30446;&#26631;&#26159;&#22312;&#22270;&#20687;&#20869;&#23481;&#20013;&#28155;&#21152;&#30446;&#26631;&#23545;&#35937;&#30340;&#21516;&#26102;&#65292;&#21516;&#26102;&#31227;&#38500;&#21407;&#22987;&#23545;&#35937;&#12290;&#19982;&#29616;&#26377;&#20316;&#21697;&#30456;&#27604;&#65292;MMP-Attack&#20855;&#26377;&#26356;&#39640;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#65292;&#22312;&#25915;&#20987;&#21830;&#19994;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#65288;&#22914;DALL-E 3&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26631;&#24535;&#30528;&#24403;&#21069;&#26368;&#20339;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have been widely deployed in various image generation tasks, demonstrating an extraordinary connection between image and text modalities. However, they face challenges of being maliciously exploited to generate harmful or sensitive images by appending a specific suffix to the original prompt. Existing works mainly focus on using single-modal information to conduct attacks, which fails to utilize multi-modal features and results in less than satisfactory performance. Integrating multi-modal priors (MMP), i.e. both text and image features, we propose a targeted attack method named MMP-Attack in this work. Specifically, the goal of MMP-Attack is to add a target object into the image content while simultaneously removing the original object. The MMP-Attack shows a notable advantage over existing works with superior universality and transferability, which can effectively attack commercial text-to-image (T2I) models such as DALL-E 3. To the best of our knowledge, this marks 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24320;&#25918;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#65292;&#22312;&#22810;&#31181;&#22320;&#24418;&#21644;&#20256;&#24863;&#22120;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#28155;&#21152;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#65292;&#24182;&#25253;&#21578;&#20912;&#24029;&#33539;&#22260;&#30340;&#26657;&#20934;&#32622;&#20449;&#24230;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15113</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24320;&#25918;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#23454;&#29616;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;
&lt;/p&gt;
&lt;p&gt;
Towards Global Glacier Mapping with Deep Learning and Open Earth Observation Data. (arXiv:2401.15113v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24320;&#25918;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#65292;&#22312;&#22810;&#31181;&#22320;&#24418;&#21644;&#20256;&#24863;&#22120;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#28155;&#21152;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#65292;&#24182;&#25253;&#21578;&#20912;&#24029;&#33539;&#22260;&#30340;&#26657;&#20934;&#32622;&#20449;&#24230;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#23545;&#20110;&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20010;&#36807;&#31243;&#21463;&#21040;&#20912;&#24029;&#22810;&#26679;&#24615;&#12289;&#38590;&#20197;&#20998;&#31867;&#30340;&#30862;&#30707;&#21644;&#22823;&#25968;&#25454;&#22788;&#29702;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Glacier-VisionTransformer-U-Net (GlaViTU)&#65292;&#19968;&#20010;&#21367;&#31215;-Transformer&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20116;&#31181;&#21033;&#29992;&#24320;&#25918;&#21355;&#26143;&#24433;&#20687;&#36827;&#34892;&#22810;&#26102;&#30456;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#30340;&#31574;&#30053;&#12290;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#36328;&#20256;&#24863;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31574;&#30053;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;IoU&#65288;&#20132;&#24182;&#27604;&#65289;&gt; 0.85&#65292;&#24182;&#19988;&#22312;&#20197;&#20912;&#38634;&#20026;&#20027;&#30340;&#22320;&#21306;&#22686;&#21152;&#21040;&#20102;&gt; 0.90&#65292;&#32780;&#22312;&#39640;&#23665;&#20122;&#27954;&#31561;&#30862;&#30707;&#20016;&#23500;&#30340;&#21306;&#22495;&#21017;&#38477;&#33267;&gt; 0.75&#12290;&#27492;&#22806;&#65292;&#28155;&#21152;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#65292;&#21363;&#22238;&#27874;&#21644;&#24178;&#28041;&#30456;&#24178;&#24230;&#65292;&#21487;&#20197;&#25552;&#39640;&#25152;&#26377;&#21487;&#29992;&#22320;&#21306;&#30340;&#20934;&#30830;&#24615;&#12290;&#25253;&#21578;&#20912;&#24029;&#33539;&#22260;&#30340;&#26657;&#20934;&#32622;&#20449;&#24230;&#20351;&#39044;&#27979;&#26356;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate global glacier mapping is critical for understanding climate change impacts. It is challenged by glacier diversity, difficult-to-classify debris and big data processing. Here we propose Glacier-VisionTransformer-U-Net (GlaViTU), a convolutional-transformer deep learning model, and five strategies for multitemporal global-scale glacier mapping using open satellite imagery. Assessing the spatial, temporal and cross-sensor generalisation shows that our best strategy achieves intersection over union &gt;0.85 on previously unobserved images in most cases, which drops to &gt;0.75 for debris-rich areas such as High-Mountain Asia and increases to &gt;0.90 for regions dominated by clean ice. Additionally, adding synthetic aperture radar data, namely, backscatter and interferometric coherence, increases the accuracy in all regions where available. The calibrated confidence for glacier extents is reported making the predictions more reliable and interpretable. We also release a benchmark dataset 
&lt;/p&gt;</description></item><item><title>HPCR&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32508;&#21512;&#20102;&#22522;&#20110;&#20195;&#29702;&#21644;&#23545;&#27604;&#25439;&#22833;&#30340;&#37325;&#25918;&#26041;&#24335;&#12290;&#36890;&#36807;&#22312;&#23545;&#27604;&#25439;&#22833;&#20013;&#20351;&#29992;&#38170;&#28857;-&#20195;&#29702;&#23545;&#26367;&#25442;&#38170;&#28857;-&#26679;&#26412;&#23545;&#65292;HPCR&#33021;&#22815;&#20943;&#36731;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#26377;&#25928;&#23398;&#20064;&#26356;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HPCR&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15038</link><description>&lt;p&gt;
HPCR: &#22522;&#20110;&#20195;&#29702;&#30340;&#32508;&#21512;&#23545;&#27604;&#37325;&#25918;&#29992;&#20110;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HPCR: Holistic Proxy-based Contrastive Replay for Online Continual Learning. (arXiv:2309.15038v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15038
&lt;/p&gt;
&lt;p&gt;
HPCR&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32508;&#21512;&#20102;&#22522;&#20110;&#20195;&#29702;&#21644;&#23545;&#27604;&#25439;&#22833;&#30340;&#37325;&#25918;&#26041;&#24335;&#12290;&#36890;&#36807;&#22312;&#23545;&#27604;&#25439;&#22833;&#20013;&#20351;&#29992;&#38170;&#28857;-&#20195;&#29702;&#23545;&#26367;&#25442;&#38170;&#28857;-&#26679;&#26412;&#23545;&#65292;HPCR&#33021;&#22815;&#20943;&#36731;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#26377;&#25928;&#23398;&#20064;&#26356;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HPCR&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#65288;OCL&#65289;&#26088;&#22312;&#36890;&#36807;&#19968;&#27425;&#22312;&#32447;&#25968;&#25454;&#27969;&#20256;&#36882;&#25345;&#32493;&#23398;&#20064;&#26032;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23427;&#36890;&#24120;&#20250;&#38754;&#20020;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#37325;&#25918;&#30340;&#26041;&#27861;&#36890;&#36807;&#20197;&#20195;&#29702;&#20026;&#22522;&#30784;&#25110;&#23545;&#27604;&#20026;&#22522;&#30784;&#30340;&#37325;&#25918;&#26041;&#24335;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20004;&#31181;&#37325;&#25918;&#26041;&#24335;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#30456;&#20114;&#34917;&#20805;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#37325;&#25918;&#30340;&#26041;&#27861;&#31216;&#20026;&#20195;&#29702;&#23545;&#27604;&#37325;&#25918;&#65288;PCR&#65289;&#65292;&#23427;&#23558;&#23545;&#27604;&#25439;&#22833;&#20013;&#30340;&#38170;&#28857;-&#26679;&#26412;&#23545;&#26367;&#25442;&#20026;&#38170;&#28857;-&#20195;&#29702;&#23545;&#65292;&#20197;&#20943;&#36731;&#36951;&#24536;&#29616;&#35937;&#12290;&#22522;&#20110;PCR&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#32508;&#21512;&#20195;&#29702;&#23545;&#27604;&#37325;&#25918;&#65288;HPCR&#65289;&#65292;&#23427;&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#12290;&#23545;&#27604;&#32452;&#20214;&#22312;PCR&#30340;&#22522;&#30784;&#19978;&#26465;&#20214;&#24615;&#22320;&#23558;&#38170;&#28857;-&#26679;&#26412;&#23545;&#32435;&#20837;&#20854;&#20013;&#65292;&#36890;&#36807;&#22823;&#22411;&#35757;&#32451;&#25209;&#27425;&#23398;&#20064;&#26356;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#31532;&#20108;&#20010;&#32452;&#20214;&#26159;&#37325;&#25918;&#32452;&#20214;&#65292;&#23427;&#22312;&#26679;&#26412;&#36873;&#25321;&#19978;&#37319;&#29992;&#20102;&#22810;&#26679;&#24615;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#20195;&#29702;&#25968;&#25454;&#19982;&#24403;&#21069;&#20219;&#21153;&#20855;&#26377;&#26356;&#39640;&#30340;&#20851;&#32852;&#24615;&#12290;&#31532;&#19977;&#20010;&#32452;&#20214;&#26159;&#27491;&#21017;&#21270;&#32452;&#20214;&#65292;&#36890;&#36807;&#32553;&#23567;&#26679;&#26412;&#31354;&#38388;&#65292;&#20419;&#36827;&#23398;&#20064;&#27169;&#22411;&#23545;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#30340;&#26356;&#22909;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HPCR&#26041;&#27861;&#22312;&#22810;&#20010;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online continual learning (OCL) aims to continuously learn new data from a single pass over the online data stream. It generally suffers from the catastrophic forgetting issue. Existing replay-based methods effectively alleviate this issue by replaying part of old data in a proxy-based or contrastive-based replay manner. In this paper, we conduct a comprehensive analysis of these two replay manners and find they can be complementary. Inspired by this finding, we propose a novel replay-based method called proxy-based contrastive replay (PCR), which replaces anchor-to-sample pairs with anchor-to-proxy pairs in the contrastive-based loss to alleviate the phenomenon of forgetting. Based on PCR, we further develop a more advanced method named holistic proxy-based contrastive replay (HPCR), which consists of three components. The contrastive component conditionally incorporates anchor-to-sample pairs to PCR, learning more fine-grained semantic information with a large training batch. The sec
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#31995;&#32479;&#22312;&#38544;&#34109;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#31574;&#30053;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#22270;&#20687;&#25200;&#21160;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08939</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#22312;&#19978;&#19979;&#25991;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#23454;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks. (arXiv:2307.08939v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08939
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#31995;&#32479;&#22312;&#38544;&#34109;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#31574;&#30053;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#22270;&#20687;&#25200;&#21160;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#39550;&#39542;&#21592;&#36741;&#21161;&#21151;&#33021;&#65292;&#29992;&#20110;&#20445;&#25345;&#26399;&#26395;&#36895;&#24230;&#21644;&#19982;&#21069;&#26041;&#36710;&#36742;&#30340;&#23433;&#20840;&#36317;&#31163;&#12290;&#26412;&#25991;&#35780;&#20272;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;ACC&#31995;&#32479;&#22312;&#38544;&#34109;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#65292;&#35813;&#25915;&#20987;&#20250;&#23545;&#25668;&#20687;&#26426;&#25968;&#25454;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#25200;&#21160;&#65292;&#20197;&#23548;&#33268;&#21069;&#26041;&#30896;&#25758;&#20107;&#25925;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#31574;&#30053;&#65292;&#29992;&#20110;&#36873;&#25321;&#35302;&#21457;&#25915;&#20987;&#26368;&#20851;&#38190;&#30340;&#26102;&#38388;&#28857;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#36816;&#34892;&#26102;&#29983;&#25104;&#36866;&#24212;&#24615;&#22270;&#20687;&#25200;&#21160;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#39550;&#39542;&#25968;&#25454;&#38598;&#21644;&#36924;&#30495;&#30340;&#20223;&#30495;&#24179;&#21488;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#20223;&#30495;&#24179;&#21488;&#20351;&#29992;&#20102;&#26469;&#33258;&#29983;&#20135;ACC&#31995;&#32479;&#30340;&#25511;&#21046;&#36719;&#20214;&#21644;&#29289;&#29702;&#19990;&#30028;&#39550;&#39542;&#27169;&#25311;&#22120;&#65292;&#24182;&#32771;&#34385;&#20102;&#39550;&#39542;&#21592;&#30340;&#24178;&#39044;&#20197;&#21450;&#33258;&#21160;&#32039;&#24613;&#21046;&#21160;&#65288;AEB&#65289;&#21644;&#21069;&#21521;&#30896;&#25758;&#35686;&#31034;&#65288;FCW&#65289;&#31561;&#23433;&#20840;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive Cruise Control (ACC) is a widely used driver assistance feature for maintaining desired speed and safe distance to the leading vehicles. This paper evaluates the security of the deep neural network (DNN) based ACC systems under stealthy perception attacks that strategically inject perturbations into camera data to cause forward collisions. We present a combined knowledge-and-data-driven approach to design a context-aware strategy for the selection of the most critical times for triggering the attacks and a novel optimization-based method for the adaptive generation of image perturbations at run-time. We evaluate the effectiveness of the proposed attack using an actual driving dataset and a realistic simulation platform with the control software from a production ACC system and a physical-world driving simulator while considering interventions by the driver and safety features such as Automatic Emergency Braking (AEB) and Forward Collision Warning (FCW). Experimental results sh
&lt;/p&gt;</description></item></channel></rss>