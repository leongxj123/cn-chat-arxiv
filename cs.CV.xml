<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>IISAN&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#25554;&#25300;&#26550;&#26500;&#65292;&#37319;&#29992;&#35299;&#32806;PEFT&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#36866;&#24212;&#65292;&#19982;&#20840;&#24494;&#35843;&#21644;&#26368;&#20808;&#36827;&#30340;PEFT&#24615;&#33021;&#21305;&#37197;&#65292;&#26174;&#33879;&#20943;&#23569;GPU&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#21152;&#36895;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2404.02059</link><description>&lt;p&gt;
IISAN&#65306;&#20351;&#29992;&#35299;&#32806;PEFT&#26377;&#25928;&#22320;&#35843;&#25972;&#22810;&#27169;&#24577;&#34920;&#31034;&#20197;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
IISAN: Efficiently Adapting Multimodal Representation for Sequential Recommendation with Decoupled PEFT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02059
&lt;/p&gt;
&lt;p&gt;
IISAN&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#25554;&#25300;&#26550;&#26500;&#65292;&#37319;&#29992;&#35299;&#32806;PEFT&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#36866;&#24212;&#65292;&#19982;&#20840;&#24494;&#35843;&#21644;&#26368;&#20808;&#36827;&#30340;PEFT&#24615;&#33021;&#21305;&#37197;&#65292;&#26174;&#33879;&#20943;&#23569;GPU&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#21152;&#36895;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#22312;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#20013;&#20855;&#26377;&#36716;&#21464;&#24615;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#34429;&#28982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#36890;&#24120;&#29992;&#20110;&#35843;&#25972;&#22522;&#30784;&#27169;&#22411;&#20197;&#36827;&#34892;&#25512;&#33616;&#20219;&#21153;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#20248;&#20808;&#32771;&#34385;&#21442;&#25968;&#25928;&#29575;&#65292;&#36890;&#24120;&#24573;&#30053;GPU&#20869;&#23384;&#25928;&#29575;&#21644;&#35757;&#32451;&#36895;&#24230;&#31561;&#20851;&#38190;&#22240;&#32032;&#12290;&#38024;&#23545;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;IISAN&#65288;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#20391;&#38754;&#36866;&#24212;&#32593;&#32476;&#65289;&#65292;&#19968;&#20010;&#20351;&#29992;&#35299;&#32806;PEFT&#32467;&#26500;&#24182;&#21033;&#29992;&#20869;&#37096;&#21644;&#36328;&#27169;&#24577;&#36866;&#24212;&#30340;&#31616;&#21333;&#21363;&#25554;&#21363;&#29992;&#26550;&#26500;&#12290;IISAN&#19982;&#20840;&#24494;&#35843;&#65288;FFT&#65289;&#21644;&#26368;&#20808;&#36827;&#30340;PEFT&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#26174;&#33879;&#20943;&#23569;&#20102;GPU&#20869;&#23384;&#20351;&#29992;&#37327; - &#23545;&#20110;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#20219;&#21153;&#65292;&#20174;47GB&#38477;&#20302;&#21040;&#20165;3GB&#12290;&#27492;&#22806;&#65292;&#19982;FFT&#30456;&#27604;&#65292;&#23427;&#23558;&#27599;&#20010;&#26102;&#20195;&#30340;&#35757;&#32451;&#26102;&#38388;&#20174;443&#31186;&#21152;&#36895;&#21040;22&#31186;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02059v1 Announce Type: new  Abstract: Multimodal foundation models are transformative in sequential recommender systems, leveraging powerful representation learning capabilities. While Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt foundation models for recommendation tasks, most research prioritizes parameter efficiency, often overlooking critical factors like GPU memory efficiency and training speed. Addressing this gap, our paper introduces IISAN (Intra- and Inter-modal Side Adapted Network for Multimodal Representation), a simple plug-and-play architecture using a Decoupled PEFT structure and exploiting both intra- and inter-modal adaptation.   IISAN matches the performance of full fine-tuning (FFT) and state-of-the-art PEFT. More importantly, it significantly reduces GPU memory usage - from 47GB to just 3GB for multimodal sequential recommendation tasks. Additionally, it accelerates training time per epoch from 443s to 22s compared to FFT. This is also
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32972;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;CLIP&#24341;&#23548;&#26041;&#27861;RAVE&#65292;&#36890;&#36807;&#27531;&#24046;&#21521;&#37327;&#23884;&#20837;&#21644;&#25552;&#31034;&#35843;&#25972;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21152;&#24555;&#20102;&#35757;&#32451;&#24182;&#25552;&#39640;&#20102;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.01889</link><description>&lt;p&gt;
RAVE: CLIP&#24341;&#23548;&#30340;&#27531;&#24046;&#21521;&#37327;&#23884;&#20837;&#29992;&#20110;&#32972;&#20809;&#22270;&#20687;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
RAVE: Residual Vector Embedding for CLIP-Guided Backlit Image Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32972;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;CLIP&#24341;&#23548;&#26041;&#27861;RAVE&#65292;&#36890;&#36807;&#27531;&#24046;&#21521;&#37327;&#23884;&#20837;&#21644;&#25552;&#31034;&#35843;&#25972;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21152;&#24555;&#20102;&#35757;&#32451;&#24182;&#25552;&#39640;&#20102;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#21453;&#24046;&#24322;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#25351;&#23548;&#36827;&#34892;&#20102;&#26032;&#39062;&#20462;&#25913;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#32972;&#20809;&#22270;&#20687;&#22686;&#24378;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24314;&#31435;&#22312;&#26368;&#20808;&#36827;&#30340;CLIP-LIT&#26041;&#27861;&#22522;&#30784;&#20043;&#19978;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32422;&#26463;&#22312;CLIP&#23884;&#20837;&#31354;&#38388;&#20013;&#19968;&#20010;&#25552;&#31034;&#23545;&#20043;&#38388;&#30340;&#25991;&#26412;-&#22270;&#20687;&#30456;&#20284;&#24615;&#26469;&#23398;&#20064;&#19968;&#20010;&#25552;&#31034;&#23545;&#65288;&#36127;/&#27491;&#26679;&#26412;&#65289;&#21644;&#30456;&#24212;&#22270;&#20687;&#65288;&#32972;&#20809;&#22270;&#20687;/&#20809;&#29031;&#33391;&#22909;&#30340;&#22270;&#20687;&#65289;&#12290;&#23398;&#20064;&#30340;&#25552;&#31034;&#28982;&#21518;&#25351;&#23548;&#22270;&#20687;&#22686;&#24378;&#32593;&#32476;&#12290;&#22522;&#20110;CLIP-LIT&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;CLIP&#24341;&#23548;&#30340;&#26032;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#35843;&#25972;&#25552;&#31034;&#32780;&#19981;&#25439;&#22833;&#36136;&#37327;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#21487;&#20197;&#30452;&#25509;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35843;&#25972;&#23427;&#20204;&#30340;&#23884;&#20837;&#65292;&#21152;&#24555;&#35757;&#32451;&#24182;&#28508;&#22312;&#22320;&#23454;&#29616;&#20351;&#29992;&#27809;&#26377;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#20854;&#20182;&#32534;&#30721;&#22120;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#25552;&#31034;&#35843;&#25972;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01889v1 Announce Type: cross  Abstract: In this paper we propose a novel modification of Contrastive Language-Image Pre-Training (CLIP) guidance for the task of unsupervised backlit image enhancement. Our work builds on the state-of-the-art CLIP-LIT approach, which learns a prompt pair by constraining the text-image similarity between a prompt (negative/positive sample) and a corresponding image (backlit image/well-lit image) in the CLIP embedding space. Learned prompts then guide an image enhancement network. Based on the CLIP-LIT framework, we propose two novel methods for CLIP guidance. First, we show that instead of tuning prompts in the space of text embeddings, it is possible to directly tune their embeddings in the latent space without any loss in quality. This accelerates training and potentially enables the use of additional encoders that do not have a text encoder. Second, we propose a novel approach that does not require any prompt tuning. Instead, based on CLIP e
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#22312;&#25552;&#39640;&#26893;&#20837;&#24335;&#21548;&#35273;&#35774;&#22791;&#30340;&#35821;&#38899;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#21069;&#30651;&#24615;&#65292;&#24182;&#36890;&#36807;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#20197;&#21450;&#24212;&#23545;&#22810;&#28304;&#35821;&#38899;&#21644;&#29615;&#22659;&#22122;&#38899;&#25361;&#25112;&#31561;&#26041;&#27861;&#26469;&#20811;&#26381;&#35821;&#38899;&#22833;&#30495;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.15442</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#32819;&#34583;&#26893;&#20837;&#35013;&#32622;&#20013;&#30340;&#20808;&#36827;&#31639;&#27861;&#65306;&#21307;&#30103;&#31574;&#30053;&#12289;&#25361;&#25112;&#21644;&#23637;&#26395;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15442
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25552;&#39640;&#26893;&#20837;&#24335;&#21548;&#35273;&#35774;&#22791;&#30340;&#35821;&#38899;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#21069;&#30651;&#24615;&#65292;&#24182;&#36890;&#36807;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#20197;&#21450;&#24212;&#23545;&#22810;&#28304;&#35821;&#38899;&#21644;&#29615;&#22659;&#22122;&#38899;&#25361;&#25112;&#31561;&#26041;&#27861;&#26469;&#20811;&#26381;&#35821;&#38899;&#22833;&#30495;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15442v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#19981;&#20165;&#20026;&#19982;&#26426;&#22120;&#20132;&#20114;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#36824;&#20026;&#37096;&#20998;&#25110;&#23436;&#20840;&#21548;&#21147;&#21463;&#25439;&#30340;&#20010;&#20307;&#25552;&#20379;&#20102;&#27807;&#36890;&#30340;&#26426;&#20250;&#12290;&#36825;&#19968;&#36807;&#31243;&#28041;&#21450;&#20197;&#27169;&#25311;&#24418;&#24335;&#25509;&#25910;&#35821;&#38899;&#20449;&#21495;&#65292;&#28982;&#21518;&#36890;&#36807;&#21508;&#31181;&#20449;&#21495;&#22788;&#29702;&#31639;&#27861;&#20351;&#20854;&#19982;&#23481;&#37327;&#26377;&#38480;&#30340;&#35774;&#22791;&#65288;&#22914;CI&#65289;&#20860;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#37197;&#22791;&#26377;&#26377;&#38480;&#25968;&#37327;&#30005;&#26497;&#30340;&#26893;&#20837;&#35013;&#32622;&#22312;&#21512;&#25104;&#36807;&#31243;&#20013;&#24448;&#24448;&#23548;&#33268;&#35821;&#38899;&#22833;&#30495;&#12290;&#23613;&#31649;&#30740;&#31350;&#20154;&#21592;&#22312;&#20351;&#29992;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#25913;&#21892;&#25509;&#25910;&#21040;&#30340;&#35821;&#38899;&#36136;&#37327;&#26041;&#38754;&#20570;&#20986;&#20102;&#21162;&#21147;&#65292;&#20294;&#22312;&#28041;&#21450;&#22810;&#20010;&#35821;&#38899;&#28304;&#12289;&#29615;&#22659;&#22122;&#22768;&#21644;&#20854;&#20182;&#24773;&#20917;&#30340;&#22330;&#26223;&#20013;&#65292;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;&#26032;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#30340;&#20986;&#29616;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15442v1 Announce Type: cross  Abstract: Automatic speech recognition (ASR) plays a pivotal role in our daily lives, offering utility not only for interacting with machines but also for facilitating communication for individuals with either partial or profound hearing impairments. The process involves receiving the speech signal in analogue form, followed by various signal processing algorithms to make it compatible with devices of limited capacity, such as cochlear implants (CIs). Unfortunately, these implants, equipped with a finite number of electrodes, often result in speech distortion during synthesis. Despite efforts by researchers to enhance received speech quality using various state-of-the-art signal processing techniques, challenges persist, especially in scenarios involving multiple sources of speech, environmental noise, and other circumstances. The advent of new artificial intelligence (AI) methods has ushered in cutting-edge strategies to address the limitations
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#22270;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#21033;&#29992;GNN&#26469;&#36827;&#34892;&#39640;&#25928;&#30340;&#22270;&#32534;&#36753;&#36317;&#31163;&#35745;&#31639;&#65292;&#36890;&#36807;&#22330;&#26223;&#22270;&#24418;&#24335;&#65292;&#32469;&#36807;NP&#22256;&#38590;&#30340;&#22270;&#30456;&#20284;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#26356;&#20855;&#25551;&#36848;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.06514</link><description>&lt;p&gt;
&#26500;&#24314;&#25968;&#25454;&#32467;&#26500;&#65306;&#36208;&#21521;&#35821;&#20041;&#22270;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Structure Your Data: Towards Semantic Graph Counterfactuals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06514
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#22270;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#21033;&#29992;GNN&#26469;&#36827;&#34892;&#39640;&#25928;&#30340;&#22270;&#32534;&#36753;&#36317;&#31163;&#35745;&#31639;&#65292;&#36890;&#36807;&#22330;&#26223;&#22270;&#24418;&#24335;&#65292;&#32469;&#36807;NP&#22256;&#38590;&#30340;&#22270;&#30456;&#20284;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#26356;&#20855;&#25551;&#36848;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CEs&#65289;&#26159;&#32771;&#34385;&#26367;&#20195;&#24773;&#26223;&#20197;&#20102;&#35299;&#21738;&#20123;&#39640;&#32423;&#35821;&#20041;&#29305;&#24449;&#23545;&#29305;&#23450;&#27169;&#22411;&#39044;&#27979;&#20570;&#20986;&#20102;&#36129;&#29486;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20276;&#38543;&#36755;&#20837;&#25968;&#25454;&#30340;&#35821;&#20041;&#22270;&#30340;CEs&#65292;&#20197;&#23454;&#29616;&#26356;&#20855;&#25551;&#36848;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#35299;&#37322;&#12290;&#20511;&#37492;&#26368;&#20808;&#36827;&#30340;&#27010;&#24565;&#23581;&#35797;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#21033;&#29992;GNN&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#22270;&#32534;&#36753;&#36317;&#31163;&#65288;GED&#65289;&#35745;&#31639;&#12290;&#25105;&#20204;&#23558;&#22270;&#24418;&#32467;&#26500;&#29992;&#20110;&#35270;&#35273;&#39046;&#22495;&#65292;&#23558;&#22270;&#20687;&#34920;&#31034;&#20026;&#22330;&#26223;&#22270;&#65292;&#24182;&#33719;&#24471;&#23427;&#20204;&#30340;GNN&#23884;&#20837;&#20197;&#32469;&#36807;&#35299;&#20915;&#25152;&#26377;&#36755;&#20837;&#23545;&#30340;NP&#22256;&#38590;&#22270;&#30456;&#20284;&#24615;&#38382;&#39064;&#65292;&#36825;&#26159;CE&#35745;&#31639;&#36807;&#31243;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#38590;&#24230;&#21644;&#35821;&#20041;&#27880;&#37322;&#21487;&#29992;&#24615;&#30340;&#22522;&#20934;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#12290;&#22312;&#21508;&#31181;&#20998;&#31867;&#22120;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;CEs&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06514v1 Announce Type: cross  Abstract: Counterfactual explanations (CEs) based on concepts are explanations that consider alternative scenarios to understand which high-level semantic features contributed to particular model predictions. In this work, we propose CEs based on the semantic graphs accompanying input data to achieve more descriptive, accurate, and human-aligned explanations. Building upon state-of-the-art (SoTA) conceptual attempts, we adopt a model-agnostic edit-based approach and introduce leveraging GNNs for efficient Graph Edit Distance (GED) computation. With a focus on the visual domain, we represent images as scene graphs and obtain their GNN embeddings to bypass solving the NP-hard graph similarity problem for all input pairs, an integral part of the CE computation process. We apply our method to benchmark and real-world datasets with varying difficulty and availability of semantic annotations. Testing on diverse classifiers, we find that our CEs outper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#25945;&#24072;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25932;&#23545;&#31034;&#20363;&#30340;&#31232;&#30095;&#36755;&#20986;&#65292;&#24182;&#19982;&#26631;&#20934;&#35757;&#32451;&#25968;&#25454;&#32467;&#21512;&#20351;&#29992;&#65292;&#26469;&#21152;&#24378;&#25945;&#24072;&#27169;&#22411;&#23545;&#23398;&#29983;&#33976;&#39311;&#30340;&#38450;&#24481;&#12290;</title><link>https://arxiv.org/abs/2403.05181</link><description>&lt;p&gt;
Adversarial Sparse Teacher: &#23545;&#25239;&#25932;&#23545;&#31034;&#20363;&#65292;&#38450;&#24481;&#29992;&#23545;&#25239;&#31034;&#20363;&#36827;&#34892;&#30340;&#22522;&#20110;&#33976;&#39311;&#30340;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Sparse Teacher: Defense Against Distillation-Based Model Stealing Attacks Using Adversarial Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#25945;&#24072;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25932;&#23545;&#31034;&#20363;&#30340;&#31232;&#30095;&#36755;&#20986;&#65292;&#24182;&#19982;&#26631;&#20934;&#35757;&#32451;&#25968;&#25454;&#32467;&#21512;&#20351;&#29992;&#65292;&#26469;&#21152;&#24378;&#25945;&#24072;&#27169;&#22411;&#23545;&#23398;&#29983;&#33976;&#39311;&#30340;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#20419;&#36827;&#20102;&#23558;&#39640;&#32423;&#25945;&#24072;&#27169;&#22411;&#30340;&#21306;&#20998;&#33021;&#21147;&#36716;&#31227;&#21040;&#26356;&#31616;&#21333;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#30830;&#20445;&#25552;&#39640;&#24615;&#33021;&#32780;&#19981;&#24433;&#21709;&#20934;&#30830;&#24615;&#12290;&#23427;&#20063;&#34987;&#29992;&#20110;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#65292;&#20854;&#20013;&#23545;&#25163;&#20351;&#29992;KD&#26469;&#27169;&#20223;&#25945;&#24072;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;&#26368;&#36817;&#22312;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#21463;&#21040;&#20102;&#21533;&#21868;&#25945;&#24072;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#31232;&#30095;&#36755;&#20986;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#30693;&#35782;&#20135;&#26435;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#35757;&#32451;&#25945;&#24072;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#26681;&#26412;&#19978;&#20445;&#25252;&#20854;logits&#65292;&#21463;&#8220;&#24694;&#27602;&#25945;&#24072;&#8221;&#29702;&#24565;&#30340;&#24433;&#21709;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#23545;&#25239;&#31034;&#20363;&#30340;&#31232;&#30095;&#36755;&#20986;&#19982;&#26631;&#20934;&#35757;&#32451;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#21152;&#24378;&#25945;&#24072;&#23545;&#23398;&#29983;&#33976;&#39311;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24039;&#22937;&#22320;&#20943;&#23569;&#20102;&#30456;&#23545;&#30340;e
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05181v1 Announce Type: new  Abstract: Knowledge Distillation (KD) facilitates the transfer of discriminative capabilities from an advanced teacher model to a simpler student model, ensuring performance enhancement without compromising accuracy. It is also exploited for model stealing attacks, where adversaries use KD to mimic the functionality of a teacher model. Recent developments in this domain have been influenced by the Stingy Teacher model, which provided empirical analysis showing that sparse outputs can significantly degrade the performance of student models. Addressing the risk of intellectual property leakage, our work introduces an approach to train a teacher model that inherently protects its logits, influenced by the Nasty Teacher concept. Differing from existing methods, we incorporate sparse outputs of adversarial examples with standard training data to strengthen the teacher's defense against student distillation. Our approach carefully reduces the relative e
&lt;/p&gt;</description></item><item><title>xT&#20026;&#35270;&#35273;Transformer&#24341;&#20837;&#20102;&#23884;&#22871;&#26631;&#35760;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#32858;&#21512;&#20102;&#20840;&#23616;&#32972;&#26223;&#21644;&#23616;&#37096;&#32454;&#33410;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#29616;&#20195;GPU&#19978;&#31471;&#21040;&#31471;&#22320;&#24314;&#27169;&#22823;&#22270;&#20687;&#65292;&#24182;&#22312;&#32463;&#20856;&#35270;&#35273;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.01915</link><description>&lt;p&gt;
xT&#65306;&#29992;&#20110;&#22823;&#22270;&#20687;&#20013;&#26356;&#22823;&#19978;&#19979;&#25991;&#30340;&#23884;&#22871;&#26631;&#35760;&#21270;
&lt;/p&gt;
&lt;p&gt;
xT: Nested Tokenization for Larger Context in Large Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01915
&lt;/p&gt;
&lt;p&gt;
xT&#20026;&#35270;&#35273;Transformer&#24341;&#20837;&#20102;&#23884;&#22871;&#26631;&#35760;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#32858;&#21512;&#20102;&#20840;&#23616;&#32972;&#26223;&#21644;&#23616;&#37096;&#32454;&#33410;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#29616;&#20195;GPU&#19978;&#31471;&#21040;&#31471;&#22320;&#24314;&#27169;&#22823;&#22270;&#20687;&#65292;&#24182;&#22312;&#32463;&#20856;&#35270;&#35273;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#27969;&#27700;&#32447;&#20197;&#20004;&#31181;&#27425;&#20248;&#26041;&#24335;&#22788;&#29702;&#22823;&#22270;&#20687;&#65306;&#19979;&#37319;&#26679;&#25110;&#35009;&#21098;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#23548;&#33268;&#22270;&#20687;&#20013;&#20449;&#24687;&#21644;&#32972;&#26223;&#30340;&#20002;&#22833;&#12290;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#20840;&#23616;&#32972;&#26223;&#30340;&#37325;&#35201;&#24615;&#19982;&#39640;&#39057;&#32454;&#33410;&#19968;&#26679;&#65292;&#20363;&#22914;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#21355;&#26143;&#22270;&#20687;&#20013;&#65307;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20154;&#21592;&#24517;&#39035;&#20570;&#20986;&#33293;&#24323;&#21738;&#20123;&#20449;&#24687;&#30340;&#22256;&#25200;&#36873;&#25321;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;xT&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#35270;&#35273;Transformer&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#32858;&#21512;&#20840;&#23616;&#32972;&#26223;&#21644;&#23616;&#37096;&#32454;&#33410;&#65292;&#24182;&#21487;&#20197;&#22312;&#24403;&#20195;GPU&#19978;&#31471;&#23545;&#31471;&#22320;&#23545;&#22823;&#22270;&#20687;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#32452;&#36328;&#32463;&#20856;&#35270;&#35273;&#20219;&#21153;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#20219;&#21153;&#20934;&#30830;&#22320;&#21453;&#26144;&#20102;&#35270;&#35273;&#27169;&#22411;&#29702;&#35299;&#30495;&#27491;&#22823;&#22411;&#22270;&#20687;&#24182;&#22312;&#22823;&#33539;&#22260;&#20869;&#34701;&#21512;&#32454;&#33410;&#30340;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20854;&#19978;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24341;&#20837;&#38024;&#23545;&#22823;&#22270;&#20687;&#30340;&#23884;&#22871;&#26631;&#35760;&#21270;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01915v1 Announce Type: cross  Abstract: Modern computer vision pipelines handle large images in one of two sub-optimal ways: down-sampling or cropping. These two methods incur significant losses in the amount of information and context present in an image. There are many downstream applications in which global context matters as much as high frequency details, such as in real-world satellite imagery; in such cases researchers have to make the uncomfortable choice of which information to discard. We introduce xT, a simple framework for vision transformers which effectively aggregates global context with local details and can model large images end-to-end on contemporary GPUs. We select a set of benchmark datasets across classic vision tasks which accurately reflect a vision model's ability to understand truly large images and incorporate fine details over large scales and assess our method's improvement on them. By introducing a nested tokenization scheme for large images in 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32452;&#21512;&#26041;&#27861;&#25913;&#21892;&#27169;&#22411;&#23545;&#30456;&#20851;&#24615;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#35299;&#20915;&#20102;&#22270;&#20687;&#20998;&#31867;&#20013;&#20266;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18919</link><description>&lt;p&gt;
Decompose-and-Compose: &#19968;&#31181;&#32452;&#21512;&#26041;&#27861;&#26469;&#20943;&#36731;&#20266;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Decompose-and-Compose: A Compositional Approach to Mitigating Spurious Correlation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18919
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32452;&#21512;&#26041;&#27861;&#25913;&#21892;&#27169;&#22411;&#23545;&#30456;&#20851;&#24615;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#65292;&#35299;&#20915;&#20102;&#22270;&#20687;&#20998;&#31867;&#20013;&#20266;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26631;&#20934;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20869;&#20998;&#24067;&#25968;&#25454;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#22806;&#20998;&#24067;&#26679;&#26412;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#20998;&#24067;&#36716;&#31227;&#26469;&#28304;&#26159;&#22270;&#20687;&#30340;&#32452;&#25104;&#24615;&#36136;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#38500;&#20102;&#30830;&#23450;&#26631;&#31614;&#30340;&#20027;&#35201;&#23545;&#35937;&#25110;&#32452;&#20214;&#22806;&#65292;&#36890;&#24120;&#36824;&#23384;&#22312;&#19968;&#20123;&#20854;&#20182;&#22270;&#20687;&#32452;&#20214;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#30340;&#36755;&#20837;&#20998;&#24067;&#36716;&#31227;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#32452;&#20214;&#21487;&#33021;&#19982;&#26631;&#31614;&#20855;&#26377;&#20266;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Decompose-and-Compose&#65288;DaC&#65289;&#65292;&#36890;&#36807;&#22522;&#20110;&#32452;&#21512;&#22270;&#20687;&#20803;&#32032;&#30340;&#32452;&#21512;&#26041;&#27861;&#25913;&#21892;&#20102;&#23545;&#30456;&#20851;&#24615;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#20351;&#29992;ERM&#35757;&#32451;&#30340;&#27169;&#22411;&#36890;&#24120;&#39640;&#24230;&#20851;&#27880;&#35201;&#20040;&#26159;&#22240;&#26524;&#32452;&#20214;&#65292;&#35201;&#20040;&#26159;&#19982;&#26631;&#31614;&#20855;&#26377;&#39640;&#20266;&#30456;&#20851;&#24615;&#30340;&#32452;&#20214;&#65288;&#23588;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18919v1 Announce Type: cross  Abstract: While standard Empirical Risk Minimization (ERM) training is proven effective for image classification on in-distribution data, it fails to perform well on out-of-distribution samples. One of the main sources of distribution shift for image classification is the compositional nature of images. Specifically, in addition to the main object or component(s) determining the label, some other image components usually exist, which may lead to the shift of input distribution between train and test environments. More importantly, these components may have spurious correlations with the label. To address this issue, we propose Decompose-and-Compose (DaC), which improves robustness to correlation shift by a compositional approach based on combining elements of images. Based on our observations, models trained with ERM usually highly attend to either the causal components or the components having a high spurious correlation with the label (especia
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.18292</link><description>&lt;p&gt;
FSL&#27169;&#22411;&#21487;&#20197;&#22240;&#20026;&#20854;&#20248;&#36234;&#24615;&#24471;&#20998;&#26356;&#39640;
&lt;/p&gt;
&lt;p&gt;
FSL Model can Score Higher as It Is
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18292
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#34987;&#27491;&#30830;&#35782;&#21035;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20542;&#21521;&#20110;&#38754;&#23545;&#38754;&#22320;&#30452;&#35270;&#38754;&#37096;&#35782;&#21035;&#26426;&#65292;&#32780;&#19981;&#26159;&#20391;&#30528;&#38754;&#23545;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#20998;&#31867;&#26412;&#36523;&#23601;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;&#23646;&#20110;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#22312;&#27979;&#35797;&#26399;&#38388;&#23545;&#25197;&#26354;&#21644;&#38750;&#20856;&#22411;&#30340;&#26597;&#35810;&#25110;&#25903;&#25345;&#22270;&#20687;&#20250;&#35753;&#27169;&#22411;&#26356;&#38590;&#27491;&#30830;&#39044;&#27979;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;&#35757;&#32451;&#36807;&#30340;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;FSL&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#20855;&#26377;&#36275;&#22815;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#20855;&#26377;&#23569;&#26679;&#26412;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#25429;&#25417;&#27979;&#35797;&#22270;&#20687;&#30340;&#39118;&#26684;&#25110;&#24418;&#29366;&#65292;&#28982;&#21518;&#35782;&#21035;&#19968;&#20010;&#36866;&#24403;&#30340;&#35757;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18292v1 Announce Type: cross  Abstract: In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable traine
&lt;/p&gt;</description></item><item><title>OmniACT&#26159;&#19968;&#20010;&#38024;&#23545;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17553</link><description>&lt;p&gt;
OmniACT&#65306;&#29992;&#20110;&#21551;&#29992;&#26700;&#38754;&#21644;Web&#22810;&#27169;&#24335;&#36890;&#29992;&#20027;&#21160;&#26234;&#33021;&#20307;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17553
&lt;/p&gt;
&lt;p&gt;
OmniACT&#26159;&#19968;&#20010;&#38024;&#23545;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#21313;&#24180;&#26469;&#65292;&#20154;&#26426;&#20132;&#20114;&#20174;&#26681;&#26412;&#19978;&#19968;&#30452;&#26159;&#25163;&#21160;&#30340;&#12290;&#21363;&#20351;&#22312;&#20170;&#22825;&#65292;&#20960;&#20046;&#25152;&#26377;&#22312;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#39640;&#25928;&#24037;&#20316;&#37117;&#38656;&#35201;&#20154;&#31867;&#22312;&#27599;&#19968;&#27493;&#37117;&#25552;&#20379;&#36755;&#20837;&#12290;&#34394;&#25311;&#20027;&#21160;&#26234;&#33021;&#20195;&#34920;&#20102;&#33258;&#21160;&#21270;&#35768;&#22810;&#36825;&#20123;&#29712;&#30862;&#20219;&#21153;&#30340;&#19968;&#20010;&#28608;&#21160;&#20154;&#24515;&#30340;&#27493;&#39588;&#12290;&#34394;&#25311;&#20195;&#29702;&#23558;&#20351;&#25216;&#26415;&#33021;&#21147;&#26377;&#38480;&#30340;&#29992;&#25143;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#21508;&#31181;&#21487;&#33021;&#24615;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#22320;&#31616;&#21270;&#35768;&#22810;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#20174;&#26085;&#21382;&#31649;&#29702;&#21040;&#22797;&#26434;&#30340;&#26053;&#34892;&#39044;&#35746;&#65292;&#20943;&#23569;&#20154;&#31867;&#24178;&#39044;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; OmniACT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#26469;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#33539;&#22260;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#35832;&#22914;"&#25773;&#25918;&#19979;&#19968;&#39318;&#27468;"&#20043;&#31867;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20197;&#21450;&#26356;&#20026;&#38271;&#26399;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17553v1 Announce Type: new  Abstract: For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as "Play the next song", as well as longer horizon tasks such
&lt;/p&gt;</description></item><item><title>MLLMs&#36890;&#36807;&#24494;&#35843;&#33719;&#24471;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#35270;&#35273;&#33021;&#21147;&#65292;&#20294;&#25237;&#24433;&#24182;&#26410;&#25552;&#21462;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#35270;&#35273;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16832</link><description>&lt;p&gt;
&#31070;&#31192;&#30340;&#25237;&#24433;&#65306;&#22810;&#27169;&#24577;LLMs&#22312;&#27809;&#26377;&#26356;&#20016;&#23500;&#30340;&#36328;&#27169;&#24577;&#25237;&#24433;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#29305;&#23450;&#39046;&#22495;&#30340;&#35270;&#35273;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16832
&lt;/p&gt;
&lt;p&gt;
MLLMs&#36890;&#36807;&#24494;&#35843;&#33719;&#24471;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#35270;&#35273;&#33021;&#21147;&#65292;&#20294;&#25237;&#24433;&#24182;&#26410;&#25552;&#21462;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#35270;&#35273;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22914;LLaVA&#21644;GPT-4(V)&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#20851;&#20110;&#22270;&#20687;&#30340;&#36890;&#29992;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#29616;&#25104;&#30340;MLLMs&#21487;&#33021;&#22312;&#35832;&#22914;&#30382;&#32932;&#30149;&#23398;&#21644;&#20892;&#19994;&#31561;&#39046;&#22495;&#30340;&#22270;&#20687;&#19978;&#20855;&#26377;&#26377;&#38480;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#24517;&#39035;&#36827;&#34892;&#24494;&#35843;&#20197;&#35299;&#38145;&#29305;&#23450;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;4&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#22312;&#20004;&#31181;&#24494;&#35843;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#38543;&#30528;MLLM&#30340;&#24494;&#35843;&#65292;&#23427;&#30830;&#23454;&#33719;&#24471;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#35270;&#35273;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#26356;&#26032;&#24182;&#27809;&#26377;&#23548;&#33268;&#25237;&#24433;&#25552;&#21462;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#35270;&#35273;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16832v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable general-purpose conversations about images with the language modality. As off-the-shelf MLLMs may have limited capabilities on images from domains like dermatology and agriculture, they must be fine-tuned to unlock domain-specific applications. The prevalent architecture of current open-source MLLMs comprises two major modules: an image-language (cross-modal) projection network and a large language model. It is desirable to understand the roles of these two modules in modeling domain-specific visual attributes to inform the design of future models and streamline the interpretability efforts on the current models. To this end, via experiments on 4 datasets and under 2 fine-tuning settings, we find that as the MLLM is fine-tuned, it indeed gains domain-specific visual capabilities, but the updates do not lead to the projection extracting relevant domain-specific visual
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SelfSwapper&#65292;&#19968;&#31181;&#36890;&#36807; Shape Agnostic Masked AutoEncoder (SAMAE) &#33258;&#30417;&#30563;&#26041;&#26696;&#26469;&#25552;&#21319;&#20154;&#33080;&#20132;&#25442;&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32469;&#36807;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#24341;&#20837;&#28165;&#26224;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#20197;&#21450;&#21033;&#29992;&#36974;&#32617;&#21644;&#23398;&#21040;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#25104;&#21151;&#35299;&#20915;&#20102;&#36523;&#20221;&#27844;&#28431;&#21644;&#24418;&#29366;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07370</link><description>&lt;p&gt;
SelfSwapper: &#36890;&#36807;&#24418;&#29366;&#26080;&#20851;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#33258;&#30417;&#30563;&#20154;&#33080;&#20132;&#25442;
&lt;/p&gt;
&lt;p&gt;
SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SelfSwapper&#65292;&#19968;&#31181;&#36890;&#36807; Shape Agnostic Masked AutoEncoder (SAMAE) &#33258;&#30417;&#30563;&#26041;&#26696;&#26469;&#25552;&#21319;&#20154;&#33080;&#20132;&#25442;&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32469;&#36807;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#24341;&#20837;&#28165;&#26224;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#20197;&#21450;&#21033;&#29992;&#36974;&#32617;&#21644;&#23398;&#21040;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#25104;&#21151;&#35299;&#20915;&#20102;&#36523;&#20221;&#27844;&#28431;&#21644;&#24418;&#29366;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#20132;&#25442;&#22240;&#20854;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#32780;&#21463;&#21040;&#26497;&#22823;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#20154;&#33080;&#20132;&#25442;&#26041;&#27861;&#20381;&#36182;&#20110;&#36343;&#36343;&#26495;&#24335;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#24120;&#23548;&#33268;&#27169;&#22411;&#35757;&#32451;&#30340;&#19981;&#31283;&#23450;&#24615;&#24182;&#20135;&#29983;&#28151;&#21512;&#36523;&#20221;&#30340;&#19981;&#26399;&#26395;&#26679;&#26412;&#65292;&#21407;&#22240;&#26159;&#30446;&#26631;&#36523;&#20221;&#27844;&#28431;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Shape Agnostic Masked AutoEncoder (SAMAE) &#35757;&#32451;&#26041;&#26696;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#33080;&#20132;&#25442;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#26696;&#36890;&#36807;&#32469;&#36807;&#20256;&#32479;&#30340;&#36343;&#36343;&#26495;&#28216;&#25103;&#24182;&#36890;&#36807;&#33258;&#37325;&#24314;&#35757;&#32451;&#26426;&#21046;&#24341;&#20837;&#28165;&#26224;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23427;&#36890;&#36807;&#36974;&#32617;&#36755;&#20837;&#22270;&#20687;&#30340;&#38754;&#37096;&#21306;&#22495;&#21644;&#21033;&#29992;&#23398;&#21040;&#30340;&#36523;&#20221;&#21644;&#38750;&#36523;&#20221;&#29305;&#24449;&#26469;&#26377;&#25928;&#20943;&#36731;&#36523;&#20221;&#27844;&#28431;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807; perforation confusion &#21644;&#38543;&#26426;&#32593;&#26684;&#32553;&#25918;&#31561;&#26032;&#25216;&#26415;&#26469;&#35299;&#20915;&#24418;&#29366;&#19981;&#23545;&#40784;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face swapping has gained significant attention for its varied applications. The majority of previous face swapping approaches have relied on the seesaw game training scheme, which often leads to the instability of the model training and results in undesired samples with blended identities due to the target identity leakage problem. This paper introduces the Shape Agnostic Masked AutoEncoder (SAMAE) training scheme, a novel self-supervised approach designed to enhance face swapping model training. Our training scheme addresses the limitations of traditional training methods by circumventing the conventional seesaw game and introducing clear ground truth through its self-reconstruction training regime. It effectively mitigates identity leakage by masking facial regions of the input images and utilizing learned disentangled identity and non-identity features. Additionally, we tackle the shape misalignment problem with new techniques including perforation confusion and random mesh scaling,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20248;&#21270;&#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;&#65288;e$^2$KD&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#35753;&#23398;&#29983;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#23398;&#29983;-&#25945;&#24072;&#19968;&#33268;&#24615;&#26041;&#38754;&#37117;&#24471;&#21040;&#22823;&#24133;&#24230;&#25552;&#21319;&#65292;&#30830;&#20445;&#23398;&#29983;&#27169;&#22411;&#20174;&#25945;&#24072;&#37027;&#37324;&#27491;&#30830;&#23398;&#21040;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2402.03119</link><description>&lt;p&gt;
&#22909;&#30340;&#25945;&#24072;&#35299;&#37322;: &#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Good Teachers Explain: Explanation-Enhanced Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03119
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;&#65288;e$^2$KD&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#35753;&#23398;&#29983;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#23398;&#29983;-&#25945;&#24072;&#19968;&#33268;&#24615;&#26041;&#38754;&#37117;&#24471;&#21040;&#22823;&#24133;&#24230;&#25552;&#21319;&#65292;&#30830;&#20445;&#23398;&#29983;&#27169;&#22411;&#20174;&#25945;&#24072;&#37027;&#37324;&#27491;&#30830;&#23398;&#21040;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#23558;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#21387;&#32553;&#25104;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#34429;&#28982;&#24050;&#32463;&#30693;&#36947;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#25945;&#24072;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#24050;&#32463;&#21457;&#29616;&#23398;&#29983;&#27169;&#22411;&#36890;&#24120;&#19981;&#20250;&#23398;&#21040;&#30456;&#21516;&#30340;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#23398;&#29983;&#27169;&#22411;&#21644;&#25945;&#24072;&#27169;&#22411;&#20043;&#38388;&#20849;&#20139;&#30456;&#20284;&#23646;&#24615;&#65292;&#22914;&#22522;&#20110;&#30456;&#21516;&#30340;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#65292;&#36890;&#24120;&#26159;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#65292;&#22240;&#20026;&#36825;&#30830;&#20445;&#23398;&#29983;&#20174;&#25945;&#24072;&#37027;&#37324;&#23398;&#21040;&#20102;&#8220;&#27491;&#30830;&#30340;&#29305;&#24449;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#32463;&#20856;&#30340;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#20197;&#21450;&#25945;&#24072;&#21644;&#23398;&#29983;&#25152;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#30456;&#20284;&#24615;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#23613;&#31649;&#36825;&#20010;&#24819;&#27861;&#31616;&#21333;&#19988;&#30452;&#35266;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#25552;&#20986;&#30340;&#8220;&#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;&#8221;&#65288;e$^2$KD&#65289;&#65288;1&#65289;&#22312;&#20934;&#30830;&#24615;&#21644;&#23398;&#29983;-&#25945;&#24072;&#19968;&#33268;&#24615;&#26041;&#38754;&#22987;&#32456;&#25552;&#20379;&#20102;&#22823;&#24133;&#24230;&#30340;&#22686;&#30410;&#65292;&#65288;2&#65289;&#30830;&#20445;&#23398;&#29983;&#20174;&#25945;&#24072;&#37027;&#37324;&#23398;&#21040;&#20102;&#27491;&#30830;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation (KD) has proven effective for compressing large teacher models into smaller student models. While it is well known that student models can achieve similar accuracies as the teachers, it has also been shown that they nonetheless often do not learn the same function. It is, however, often highly desirable that the student's and teacher's functions share similar properties such as basing the prediction on the same input features, as this ensures that students learn the 'right features' from the teachers. In this work, we explore whether this can be achieved by not only optimizing the classic KD loss but also the similarity of the explanations generated by the teacher and the student. Despite the idea being simple and intuitive, we find that our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides large gains in terms of accuracy and student-teacher agreement, (2) ensures that the student learns from the teacher to be right for the right reasons and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2311.15964</link><description>&lt;p&gt;
&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Pre-training for Localized Instruction Generation of Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#35270;&#39057;&#23637;&#31034;&#20102;&#35832;&#22914;&#39135;&#35889;&#20934;&#22791;&#31561;&#20219;&#21153;&#30340;&#36880;&#27493;&#28436;&#31034;&#12290;&#29702;&#35299;&#27492;&#31867;&#35270;&#39057;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#23545;&#27493;&#39588;&#36827;&#34892;&#31934;&#30830;&#23450;&#20301;&#24182;&#29983;&#25104;&#25991;&#23383;&#35828;&#26126;&#12290;&#25163;&#21160;&#27880;&#37322;&#27493;&#39588;&#24182;&#32534;&#20889;&#35828;&#26126;&#25104;&#26412;&#39640;&#26114;&#65292;&#36825;&#38480;&#21046;&#20102;&#24403;&#21069;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#24182;&#38459;&#30861;&#20102;&#26377;&#25928;&#23398;&#20064;&#12290;&#21033;&#29992;&#22823;&#35268;&#27169;&#20294;&#22024;&#26434;&#30340;&#35270;&#39057;-&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#21319;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25991;&#26412;&#36716;&#24405;&#21253;&#21547;&#26080;&#20851;&#20869;&#23481;&#65292;&#19982;&#20154;&#31867;&#27880;&#37322;&#21592;&#32534;&#20889;&#30340;&#35828;&#26126;&#30456;&#27604;&#23384;&#22312;&#39118;&#26684;&#21464;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;Sieve-&amp;-Swap&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#21644;&#20351;&#29992;&#25991;&#26412;&#39135;&#35889;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#33258;&#21160;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#20197;&#22686;&#24378;&#25991;&#23383;&#25351;&#20196;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15964v2 Announce Type: replace-cross  Abstract: Procedural videos show step-by-step demonstrations of tasks like recipe preparation. Understanding such videos is challenging, involving the precise localization of steps and the generation of textual instructions. Manually annotating steps and writing instructions is costly, which limits the size of current datasets and hinders effective learning. Leveraging large but noisy video-transcript datasets for pre-training can boost performance, but demands significant computational resources. Furthermore, transcripts contain irrelevant content and exhibit style variation compared to instructions written by human annotators. To mitigate both issues, we propose a technique, Sieve-&amp;-Swap, to automatically curate a smaller dataset: (i) Sieve filters irrelevant transcripts and (ii) Swap enhances the quality of the text instruction by automatically replacing the transcripts with human-written instructions from a text-only recipe dataset. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25216;&#24039;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#36716;&#31227;&#33021;&#21147;&#65292;&#24182;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08734</link><description>&lt;p&gt;
&#25552;&#39640;&#23545;&#25239;&#36716;&#31227;&#33021;&#21147;&#30340;&#19968;&#31995;&#21015;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
Bag of Tricks to Boost Adversarial Transferability. (arXiv:2401.08734v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25216;&#24039;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#36716;&#31227;&#33021;&#21147;&#65292;&#24182;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24191;&#20026;&#20154;&#30693;&#30340;&#26159;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#33030;&#24369;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#30333;&#30418;&#35774;&#32622;&#19979;&#29983;&#25104;&#30340;&#32431;&#31929;&#23545;&#25239;&#24615;&#26679;&#26412;&#22312;&#19981;&#21516;&#27169;&#22411;&#38388;&#30340;&#20256;&#36882;&#33021;&#21147;&#36890;&#24120;&#36739;&#20302;&#12290;&#30001;&#20110;&#23545;&#25239;&#24615;&#36716;&#31227;&#23545;&#23454;&#38469;&#24212;&#29992;&#36896;&#25104;&#26356;&#20005;&#37325;&#30340;&#23041;&#32961;&#65292;&#22240;&#27492;&#24050;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#25913;&#21892;&#36716;&#31227;&#33021;&#21147;&#65292;&#21253;&#25324;&#22522;&#20110;&#26799;&#24230;&#12289;&#22522;&#20110;&#36755;&#20837;&#36716;&#25442;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#25915;&#20987;&#31561;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#23545;&#25239;&#24615;&#25915;&#20987;&#20013;&#30340;&#20960;&#20010;&#24494;&#23567;&#25913;&#21464;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#25915;&#20987;&#24615;&#33021;&#65292;&#20363;&#22914;&#36845;&#20195;&#27425;&#25968;&#21644;&#27493;&#38271;&#12290;&#22522;&#20110;&#23545;&#29616;&#26377;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36827;&#34892;&#20180;&#32454;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25216;&#24039;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#36716;&#31227;&#33021;&#21147;&#65292;&#21253;&#25324;&#21160;&#37327;&#21021;&#22987;&#21270;&#12289;&#23450;&#26399;&#35843;&#25972;&#27493;&#38271;&#12289;&#23545;&#25239;&#31034;&#20363;&#12289;&#22522;&#20110;&#35889;&#30340;&#36755;&#20837;&#36716;&#25442;&#20197;&#21450;&#20960;&#31181;&#38598;&#25104;&#31574;&#30053;&#12290;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#24039;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are widely known to be vulnerable to adversarial examples. However, vanilla adversarial examples generated under the white-box setting often exhibit low transferability across different models. Since adversarial transferability poses more severe threats to practical applications, various approaches have been proposed for better transferability, including gradient-based, input transformation-based, and model-related attacks, \etc. In this work, we find that several tiny changes in the existing adversarial attacks can significantly affect the attack performance, \eg, the number of iterations and step size. Based on careful studies of existing adversarial attacks, we propose a bag of tricks to enhance adversarial transferability, including momentum initialization, scheduled step size, dual example, spectral-based input transformation, and several ensemble strategies. Extensive experiments on the ImageNet dataset validate the high effectiveness of our proposed tricks a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Vid-LLMs&#65289;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;Vid-LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#21253;&#25324;&#24320;&#25918;&#24335;&#26102;&#31354;&#25512;&#29702;&#21644;&#24120;&#35782;&#30693;&#35782;&#65292;&#20026;&#26410;&#26469;&#30340;&#35270;&#39057;&#29702;&#35299;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2312.17432</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Video Understanding with Large Language Models: A Survey. (arXiv:2312.17432v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17432
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Vid-LLMs&#65289;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;Vid-LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#21253;&#25324;&#24320;&#25918;&#24335;&#26102;&#31354;&#25512;&#29702;&#21644;&#24120;&#35782;&#30693;&#35782;&#65292;&#20026;&#26410;&#26469;&#30340;&#35270;&#39057;&#29702;&#35299;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#32447;&#35270;&#39057;&#24179;&#21488;&#30340;&#19981;&#26029;&#22686;&#38271;&#21644;&#35270;&#39057;&#20869;&#23481;&#30340;&#19981;&#26029;&#22686;&#22810;&#65292;&#23545;&#29087;&#32451;&#30340;&#35270;&#39057;&#29702;&#35299;&#24037;&#20855;&#30340;&#38656;&#27714;&#26174;&#33879;&#22686;&#21152;&#12290;&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Vid-LLMs&#65289;&#25216;&#26415;&#36827;&#34892;&#35270;&#39057;&#29702;&#35299;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;Vid-LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#20196;&#20154;&#24778;&#35766;&#65292;&#23588;&#20854;&#26159;&#23427;&#20204;&#22312;&#24320;&#25918;&#24335;&#26102;&#31354;&#25512;&#29702;&#21644;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20026;&#26410;&#26469;&#30340;&#35270;&#39057;&#29702;&#35299;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#26412;&#35843;&#26597;&#23545;Vid-LLMs&#30340;&#29420;&#29305;&#29305;&#28857;&#21644;&#33021;&#21147;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#20998;&#20026;&#22235;&#31181;&#20027;&#35201;&#31867;&#22411;&#65306;&#22522;&#20110;LLM&#30340;&#35270;&#39057;&#20195;&#29702;&#12289;Vid-LLMs&#30340;&#39044;&#35757;&#32451;&#12289;Vid-LLMs&#30340;&#25351;&#20196;&#35843;&#25972;&#21644;&#28151;&#21512;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#35843;&#26597;&#23545;Vid-LLMs&#30340;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#21478;&#22806;&#65292;&#23427;&#36824;&#25506;&#35752;&#20102;Vid-LLMs&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly. Given the remarkable capabilities of Large Language Models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of the recent advancements in video understanding harnessing the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended spatial-temporal reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into four main types: LLM-based Video Agents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods. Furthermore, this survey presents a comprehensive study of the tasks, datasets, and evaluation methodologies for Vid-LLMs. Additionally, it explores 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;&#65292;&#36890;&#36807;&#32858;&#31867;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#65292;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#65292;&#24182;&#25366;&#25496;&#20102;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.13447</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Multiscale Superpixel Structured Difference Graph Convolutional Network for VL Representation. (arXiv:2310.13447v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;&#65292;&#36890;&#36807;&#32858;&#31867;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#65292;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#65292;&#24182;&#25366;&#25496;&#20102;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#39046;&#22495;&#20013;&#65292;&#25972;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#20851;&#38190;&#22312;&#20110;&#24314;&#31435;&#19968;&#20010;&#33391;&#22909;&#30340;&#23545;&#40784;&#31574;&#30053;&#12290;&#26368;&#36817;&#65292;&#21463;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#34920;&#24449;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#35821;&#20041;&#34920;&#24449;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#24403;&#21069;&#22522;&#20110;&#20687;&#32032;&#25110;&#22359;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#25552;&#21462;&#22797;&#26434;&#22330;&#26223;&#36793;&#30028;&#26041;&#38754;&#23384;&#22312;&#31354;&#38388;&#35821;&#20041;&#36830;&#36143;&#24615;&#19981;&#36275;&#21644;&#23545;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#23558;&#36229;&#20687;&#32032;&#20316;&#20026;&#21487;&#23398;&#20064;&#22270;&#20687;&#25968;&#25454;&#30340;&#32508;&#21512;&#32039;&#20945;&#34920;&#24449;&#65292;&#36890;&#36807;&#23545;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#36827;&#34892;&#32858;&#31867;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#12290;&#20026;&#20102;&#25366;&#25496;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#12290;&#23427;&#23558;&#25972;&#20010;&#22270;&#20687;&#35299;&#26512;&#20026;&#32454;&#21040;&#31895;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25972;&#20010;&#22270;&#20687;&#30340;&#35299;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the multimodal field, the key to integrating vision and language lies in establishing a good alignment strategy. Recently, benefiting from the success of self-supervised learning, significant progress has been made in multimodal semantic representation based on pre-trained models for vision and language. However, there is still room for improvement in visual semantic representation. The lack of spatial semantic coherence and vulnerability to noise makes it challenging for current pixel or patch-based methods to accurately extract complex scene boundaries. To this end, this paper develops superpixel as a comprehensive compact representation of learnable image data, which effectively reduces the number of visual primitives for subsequent processing by clustering perceptually similar pixels. To mine more precise topological relations, we propose a Multiscale Difference Graph Convolutional Network (MDGCN). It parses the entire image as a fine-to-coarse hierarchical structure of cons
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22122;&#22768;&#23481;&#24525;&#30340;&#26080;&#30417;&#30563;&#36866;&#37197;&#22120;(NtUA)&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#26469;&#23398;&#20064;&#20248;&#31168;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;NtUA&#36890;&#36807;&#33258;&#36866;&#24212;&#32531;&#23384;&#24418;&#25104;&#21644;&#20266;&#26631;&#31614;&#20462;&#27491;&#26469;&#23545;&#25239;&#20266;&#26631;&#31614;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2309.14928</link><description>&lt;p&gt;
&#22122;&#22768;&#23481;&#24525;&#30340;&#26080;&#30417;&#30563;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Noise-Tolerant Unsupervised Adapter for Vision-Language Models. (arXiv:2309.14928v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14928
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22122;&#22768;&#23481;&#24525;&#30340;&#26080;&#30417;&#30563;&#36866;&#37197;&#22120;(NtUA)&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#26469;&#23398;&#20064;&#20248;&#31168;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;NtUA&#36890;&#36807;&#33258;&#36866;&#24212;&#32531;&#23384;&#24418;&#25104;&#21644;&#20266;&#26631;&#31614;&#20462;&#27491;&#26469;&#23545;&#25239;&#20266;&#26631;&#31614;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#26174;&#33879;&#30340;&#34920;&#29616;&#65292;&#22312;&#21508;&#31181;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23569;&#26679;&#26412;&#26377;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#20173;&#38656;&#35201;&#30446;&#26631;&#26679;&#26412;&#30340;&#26631;&#27880;&#65292;&#36825;&#22312;&#22788;&#29702;&#21508;&#31181;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22122;&#22768;&#23481;&#24525;&#30340;&#26080;&#30417;&#30563;&#36866;&#37197;&#22120;(NtUA)&#65292;&#23427;&#20801;&#35768;&#20351;&#29992;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#26469;&#23398;&#20064;&#20248;&#31168;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;NtUA&#20316;&#20026;&#19968;&#20010;&#38190;&#20540;&#32531;&#23384;&#65292;&#23558;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#30340;&#35270;&#35273;&#29305;&#24449;&#21644;&#39044;&#27979;&#30340;&#20266;&#26631;&#31614;&#20316;&#20026;&#38190;&#20540;&#23545;&#36827;&#34892;&#24314;&#27169;&#12290;&#23427;&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#35774;&#35745;&#32452;&#25104;&#12290;&#31532;&#19968;&#20010;&#26159;&#33258;&#36866;&#24212;&#32531;&#23384;&#24418;&#25104;&#65292;&#36890;&#36807;&#26681;&#25454;&#20854;&#39044;&#27979;&#32622;&#20449;&#24230;&#23545;&#38190;&#20540;&#23545;&#36827;&#34892;&#21152;&#26435;&#65292;&#20197;&#23545;&#25239;&#20266;&#26631;&#31614;&#30340;&#22122;&#22768;&#12290;&#31532;&#20108;&#20010;&#26159;&#20266;&#26631;&#31614;&#20462;&#27491;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#38190;&#20540;&#23545;&#30340;&#26435;&#37325;&#26469;&#20462;&#27491;&#20266;&#26631;&#31614;&#20197;&#21450;&#32531;&#23384;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large-scale vision-language models have achieved very impressive performance in various zero-shot image classification tasks. While prior studies have demonstrated significant improvements by introducing few-shot labelled target samples, they still require labelling of target samples, which greatly degrades their scalability while handling various visual recognition tasks. We design NtUA, a Noise-tolerant Unsupervised Adapter that allows learning superior target models with few-shot unlabelled target samples. NtUA works as a key-value cache that formulates visual features and predicted pseudo-labels of the few-shot unlabelled target samples as key-value pairs. It consists of two complementary designs. The first is adaptive cache formation that combats pseudo-label noises by weighting the key-value pairs according to their prediction confidence. The second is pseudo-label rectification, which corrects both pair values (i.e., pseudo-labels) and cache weights by leverag
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#31867;&#28608;&#27963;&#22270;&#25216;&#26415;&#65292;USL-Net&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20998;&#21106;&#21508;&#31181;&#30382;&#32932;&#30149;&#21464;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.13289</link><description>&lt;p&gt;
USL-Net&#65306;&#29992;&#20110;&#26080;&#30417;&#30563;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#30340;&#19981;&#30830;&#23450;&#24615;&#33258;&#23398;&#20064;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
USL-Net: Uncertainty Self-Learning Network for Unsupervised Skin Lesion Segmentation. (arXiv:2309.13289v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13289
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#31867;&#28608;&#27963;&#22270;&#25216;&#26415;&#65292;USL-Net&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20998;&#21106;&#21508;&#31181;&#30382;&#32932;&#30149;&#21464;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#20855;&#26377;&#22810;&#31181;&#22909;&#22788;&#65292;&#21253;&#25324;&#33410;&#32422;&#19987;&#23478;&#20154;&#21147;&#36164;&#28304;&#12289;&#20943;&#23569;&#20027;&#35266;&#20154;&#24037;&#26631;&#27880;&#24341;&#36215;&#30340;&#24046;&#24322;&#20197;&#21450;&#36866;&#24212;&#26032;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#22312;&#27809;&#26377;&#25163;&#21160;&#26631;&#27880;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#20998;&#21106;&#30382;&#32932;&#38236;&#22270;&#20687;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#65292;&#22914;&#27611;&#21457;&#22122;&#22768;&#12289;&#27700;&#30129;&#22122;&#22768;&#21644;&#32454;&#24494;&#36793;&#32536;&#24046;&#24322;&#31561;&#30382;&#32932;&#38236;&#22270;&#20687;&#20266;&#24433;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#33258;&#23398;&#20064;&#32593;&#32476;&#65288;USL-Net&#65289;&#29992;&#20110;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#12290;USL-Net&#33021;&#22815;&#26377;&#25928;&#22320;&#20998;&#21106;&#21508;&#31181;&#30149;&#21464;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#25351;&#23548;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#21462;&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#29983;&#25104;&#31867;&#28608;&#27963;&#22270;&#65288;CAMs&#65289;&#20316;&#20026;&#26174;&#33879;&#22270;&#12290;&#19981;&#21516;&#30340;CAM&#20301;&#32622;&#23545;&#24212;&#20110;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#30149;&#21464;&#21306;&#22495;&#30340;&#37325;&#35201;&#24615;&#12290;&#22320;&#22270;&#20013;&#30340;&#39640;&#26174;&#33879;&#21306;&#22495;&#29992;&#20316;&#30149;&#21464;&#21306;&#22495;&#30340;&#20266;&#26631;&#31614;&#65292;&#32780;&#20302;&#26174;&#33879;&#21306;&#22495;&#29992;&#20316;&#38750;&#30149;&#21464;&#21306;&#22495;&#30340;&#20266;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised skin lesion segmentation offers several benefits, including conserving expert human resources, reducing discrepancies due to subjective human labeling, and adapting to novel environments. However, segmenting dermoscopic images without manual labeling guidance presents significant challenges due to dermoscopic image artifacts such as hair noise, blister noise, and subtle edge differences. To address these challenges, we introduce an innovative Uncertainty Self-Learning Network (USL-Net) designed for skin lesion segmentation. The USL-Net can effectively segment a range of lesions, eliminating the need for manual labeling guidance. Initially, features are extracted using contrastive learning, followed by the generation of Class Activation Maps (CAMs) as saliency maps using these features. The different CAM locations correspond to the importance of the lesion region based on their saliency. High-saliency regions in the map serve as pseudo-labels for lesion regions while low-sa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#23545;&#20110;&#24066;&#22330;&#33829;&#38144;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#19968;&#30452;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#21442;&#19982;&#32773;&#21644;&#24191;&#21578;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#20160;&#20040;&#20351;&#24191;&#21578;&#35760;&#24518;&#28145;&#21051;&#30340;&#26377;&#36259;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.00378</link><description>&lt;p&gt;
&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Long-Term Memorability On Advertisements. (arXiv:2309.00378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#39318;&#20010;&#22823;&#35268;&#27169;&#30340;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#24191;&#21578;&#30340;&#38271;&#26399;&#35760;&#24518;&#24615;&#23545;&#20110;&#24066;&#22330;&#33829;&#38144;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#19968;&#30452;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#21442;&#19982;&#32773;&#21644;&#24191;&#21578;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#20160;&#20040;&#20351;&#24191;&#21578;&#35760;&#24518;&#28145;&#21051;&#30340;&#26377;&#36259;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24066;&#22330;&#33829;&#38144;&#20154;&#21592;&#33457;&#36153;&#25968;&#21313;&#20159;&#32654;&#20803;&#22312;&#24191;&#21578;&#19978;&#65292;&#20294;&#26159;&#25237;&#20837;&#21040;&#24191;&#21578;&#19978;&#30340;&#37329;&#38065;&#33021;&#36215;&#22810;&#22823;&#20316;&#29992;&#21602;&#65311;&#24403;&#39038;&#23458;&#22312;&#36141;&#20080;&#26102;&#26080;&#27861;&#36776;&#35748;&#20986;&#20182;&#20204;&#30475;&#36807;&#30340;&#21697;&#29260;&#30340;&#35805;&#65292;&#33457;&#22312;&#24191;&#21578;&#19978;&#30340;&#38065;&#22522;&#26412;&#19978;&#23601;&#34987;&#28010;&#36153;&#20102;&#12290;&#23613;&#31649;&#22312;&#33829;&#38144;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#25991;&#29486;&#20013;&#36824;&#27809;&#26377;&#20851;&#20110;&#24191;&#21578;&#35760;&#24518;&#21147;&#30340;&#30740;&#31350;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#26159;&#23545;&#29305;&#23450;&#20869;&#23481;&#31867;&#22411;&#65288;&#22914;&#29289;&#20307;&#21644;&#21160;&#20316;&#35270;&#39057;&#65289;&#36827;&#34892;&#30701;&#26399;&#22238;&#24518;&#65288;&lt;5&#20998;&#38047;&#65289;&#30340;&#30740;&#31350;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24191;&#21578;&#34892;&#19994;&#21482;&#20851;&#24515;&#38271;&#26399;&#35760;&#24518;&#65288;&#20960;&#20010;&#23567;&#26102;&#25110;&#26356;&#38271;&#26102;&#38388;&#65289;&#65292;&#32780;&#19988;&#24191;&#21578;&#20960;&#20046;&#24635;&#26159;&#39640;&#24230;&#22810;&#27169;&#24335;&#21270;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#24418;&#24335;&#65288;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#26469;&#35762;&#25925;&#20107;&#12290;&#22522;&#20110;&#36825;&#19968;&#21160;&#26426;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#20010;&#22823;&#35268;&#27169;&#35760;&#24518;&#24615;&#30740;&#31350;&#65292;&#20849;&#26377;1203&#21517;&#21442;&#19982;&#32773;&#21644;2205&#20010;&#24191;&#21578;&#28085;&#30422;&#20102;276&#20010;&#21697;&#29260;&#12290;&#22312;&#19981;&#21516;&#21442;&#19982;&#32773;&#23376;&#32676;&#20307;&#21644;&#24191;&#21578;&#31867;&#22411;&#19978;&#36827;&#34892;&#32479;&#35745;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#35768;&#22810;&#26377;&#20851;&#20160;&#20040;&#20351;&#24191;&#21578;&#38590;&#24536;&#30340;&#26377;&#36259;&#35265;&#35299;-&#26080;&#35770;&#26159;&#20869;&#23481;&#36824;&#26159;
&lt;/p&gt;
&lt;p&gt;
Marketers spend billions of dollars on advertisements but to what end? At the purchase time, if customers cannot recognize a brand for which they saw an ad, the money spent on the ad is essentially wasted. Despite its importance in marketing, until now, there has been no study on the memorability of ads in the ML literature. Most studies have been conducted on short-term recall (&lt;5 mins) on specific content types like object and action videos. On the other hand, the advertising industry only cares about long-term memorability (a few hours or longer), and advertisements are almost always highly multimodal, depicting a story through its different modalities (text, images, and videos). With this motivation, we conduct the first large scale memorability study consisting of 1203 participants and 2205 ads covering 276 brands. Running statistical tests over different participant subpopulations and ad-types, we find many interesting insights into what makes an ad memorable - both content and h
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#30475;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;SurgVLP&#65292;&#36890;&#36807;&#21033;&#29992;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#30340;&#35821;&#38899;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.15220</link><description>&lt;p&gt;
&#36890;&#36807;&#35266;&#30475;&#25968;&#30334;&#20010;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures. (arXiv:2307.15220v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15220
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#30475;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;SurgVLP&#65292;&#36890;&#36807;&#21033;&#29992;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#30340;&#35821;&#38899;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22806;&#31185;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#20381;&#38752;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#65292;&#20027;&#35201;&#20351;&#29992;&#35270;&#35273;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#25163;&#26415;&#35270;&#39057;&#26469;&#39044;&#27979;&#19968;&#32452;&#22266;&#23450;&#30340;&#23545;&#35937;&#31867;&#21035;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26410;&#35265;&#25163;&#26415;&#31243;&#24207;&#21644;&#21518;&#32493;&#20219;&#21153;&#19978;&#30340;&#36890;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;&#36890;&#36807;&#24320;&#25918;&#30340;&#25163;&#26415;&#30005;&#23376;&#23398;&#20064;&#24179;&#21488;&#25552;&#20379;&#30340;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#21487;&#20197;&#20026;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#26377;&#25928;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#25163;&#21160;&#27880;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#20114;&#34917;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#29983;&#25104;&#25991;&#26412;&#36716;&#24405;&#26469;&#35299;&#20915;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#23384;&#22312;&#30340;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;SurgVLP - &#25163;&#26415;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#12290;SurgVLP&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#23558;&#35270;&#39057;&#21098;&#36753;&#23884;&#20837;&#19982;&#30456;&#24212;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in surgical computer vision applications have been driven by fully-supervised methods, primarily using only visual data. These methods rely on manually annotated surgical videos to predict a fixed set of object categories, limiting their generalizability to unseen surgical procedures and downstream tasks. In this work, we put forward the idea that the surgical video lectures available through open surgical e-learning platforms can provide effective supervisory signals for multi-modal representation learning without relying on manual annotations. We address the surgery-specific linguistic challenges present in surgical video lectures by employing multiple complementary automatic speech recognition systems to generate text transcriptions. We then present a novel method, SurgVLP - Surgical Vision Language Pre-training, for multi-modal representation learning. SurgVLP constructs a new contrastive learning objective to align video clip embeddings with the corresponding m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06671</link><description>&lt;p&gt;
&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;&#21644;&#36845;&#20195;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation. (arXiv:2304.06671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#25511;&#21046;&#26159;&#21487;&#25511;&#22270;&#20687;&#29983;&#25104;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#22312;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20855;&#26377;&#31867;&#20284;&#31354;&#38388;&#37197;&#32622;&#30340;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#38598;&#19978;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#20219;&#24847;&#19981;&#30830;&#23450;&#30340;&#24067;&#23616;&#30340;&#31163;&#32447;&#20998;&#24067;&#26679;&#26412;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#34920;&#29616;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LayoutBench&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35786;&#26029;&#30340;&#22522;&#20934;&#65292;&#23427;&#26816;&#26597;&#20102;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#65306;&#25968;&#37327;&#65292;&#20301;&#32622;&#65292;&#22823;&#23567;&#21644;&#24418;&#29366;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#26368;&#36817;&#20195;&#34920;&#24615;&#30340;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35266;&#23519;&#21040;&#33391;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#65288;&#20363;&#22914;&#65292;&#36793;&#30028;&#19978;&#30340;&#23545;&#35937;&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#65292;&#23427;&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#23637;&#31034;&#20986;&#22312;LayoutBench&#30340;OOD&#24067;&#23616;&#19978;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#34920;&#26126;IterInpaint&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#29983;&#25104;&#22810;&#26679;&#21644;&#35270;&#35273;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#22270;&#20687;&#21644;&#21487;&#25511;&#30340;&#31354;&#38388;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial control is a core capability in controllable image generation. Advancements in layout-guided image generation have shown promising results on in-distribution (ID) datasets with similar spatial configurations. However, it is unclear how these models perform when facing out-of-distribution (OOD) samples with arbitrary, unseen layouts. In this paper, we propose LayoutBench, a diagnostic benchmark for layout-guided image generation that examines four categories of spatial control skills: number, position, size, and shape. We benchmark two recent representative layout-guided image generation methods and observe that the good ID layout control may not generalize well to arbitrary layouts in the wild (e.g., objects at the boundary). Next, we propose IterInpaint, a new baseline that generates foreground and background regions in a step-by-step manner via inpainting, demonstrating stronger generalizability than existing models on OOD layouts in LayoutBench. We perform quantitative and q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#36890;&#36807;&#36825;&#20123;&#26041;&#26696;&#65292;&#21487;&#20197;&#26356;&#21487;&#38752;&#22320;&#27979;&#37327;&#24402;&#22240;&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.11884</link><description>&lt;p&gt;
&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;&#26356;&#22909;&#22320;&#29702;&#35299;&#24402;&#22240;&#26041;&#27861;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Better Understanding Differences in Attribution Methods via Systematic Evaluations. (arXiv:2303.11884v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#36890;&#36807;&#36825;&#20123;&#26041;&#26696;&#65292;&#21487;&#20197;&#26356;&#21487;&#38752;&#22320;&#27979;&#37327;&#24402;&#22240;&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#20854;&#40657;&#30418;&#24615;&#36136;&#20351;&#20854;&#38590;&#20197;&#35299;&#37322;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#21518;&#32493;&#24402;&#22240;&#26041;&#27861;&#26469;&#30830;&#23450;&#23545;&#27169;&#22411;&#20915;&#31574;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;&#30001;&#20110;&#19981;&#23384;&#22312;&#22522;&#20934;&#24402;&#22240;&#65292;&#22240;&#27492;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#20197;&#26356;&#21487;&#38752;&#22320;&#27979;&#37327;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#65292;&#20351;&#23427;&#20204;&#20043;&#38388;&#30340;&#27604;&#36739;&#26356;&#20844;&#24179;&#65292;&#24182;&#20351;&#35270;&#35273;&#26816;&#26597;&#26356;&#31995;&#32479;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are very successful on many vision tasks, but hard to interpret due to their black box nature. To overcome this, various post-hoc attribution methods have been proposed to identify image regions most influential to the models' decisions. Evaluating such methods is challenging since no ground truth attributions exist. We thus propose three novel evaluation schemes to more reliably measure the faithfulness of those methods, to make comparisons between them more fair, and to make visual inspection more systematic. To address faithfulness, we propose a novel evaluation setting (DiFull) in which we carefully control which parts of the input can influence the output in order to distinguish possible from impossible attributions. To address fairness, we note that different methods are applied at different layers, which skews any comparison, and so evaluate all methods on the same layers (ML-Att) and discuss how this impacts their performance on quantitative metrics. For mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#26631;&#20934;&#20998;&#31867;&#22120;&#21644;&#40065;&#26834;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#26469;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#24230;&#24809;&#32602;&#12290;</title><link>http://arxiv.org/abs/2301.12554</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#24179;&#28369;&#25913;&#21892;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing. (arXiv:2301.12554v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#26631;&#20934;&#20998;&#31867;&#22120;&#21644;&#40065;&#26834;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#26469;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#24230;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22823;&#37327;&#22686;&#24378;&#31070;&#32463;&#20998;&#31867;&#22120;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#22312;&#28165;&#26224;&#24230;&#26041;&#38754;&#23384;&#22312;&#19981;&#21487;&#25509;&#21463;&#30340;&#20005;&#37325;&#24809;&#32602;&#65292;&#23454;&#36341;&#32773;&#20173;&#28982;&#19981;&#24895;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36890;&#36807;&#28151;&#21512;&#26631;&#20934;&#20998;&#31867;&#22120;&#21644;&#24378;&#40065;&#26834;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#20854;&#20013;&#26631;&#20934;&#32593;&#32476;&#20248;&#21270;&#28165;&#26224;&#24230;&#32780;&#19981;&#26159;&#19968;&#33324;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#26174;&#30528;&#20943;&#36731;&#36825;&#31181;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#26435;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#26174;&#31034;&#20986;&#22522;&#20110;&#40065;&#26834;&#24615;&#30340;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#31034;&#20363;&#30340;&#32622;&#20449;&#24230;&#24046;&#24322;&#26159;&#36825;&#31181;&#25913;&#21892;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#38500;&#25552;&#20379;&#30452;&#35266;&#21644;&#32463;&#39564;&#35777;&#25454;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#29616;&#23454;&#20551;&#35774;&#19979;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#19968;&#20010;&#23545;&#25239;&#24615;&#36755;&#20837;&#26816;&#27979;&#22120;&#36866;&#24212;&#20026;&#28151;&#21512;&#32593;&#32476;&#65292;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#20004;&#20010;&#22522;&#26412;&#27169;&#22411;&#30340;&#28151;&#21512;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#20943;&#23569;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#24615;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;
While prior research has proposed a plethora of methods that enhance the adversarial robustness of neural classifiers, practitioners are still reluctant to adopt these techniques due to their unacceptably severe penalties in clean accuracy. This paper shows that by mixing the output probabilities of a standard classifier and a robust model, where the standard network is optimized for clean accuracy and is not robust in general, this accuracy-robustness trade-off can be significantly alleviated. We show that the robust base classifier's confidence difference for correct and incorrect examples is the key ingredient of this improvement. In addition to providing intuitive and empirical evidence, we also theoretically certify the robustness of the mixed classifier under realistic assumptions. Furthermore, we adapt an adversarial input detector into a mixing network that adaptively adjusts the mixture of the two base models, further reducing the accuracy penalty of achieving robustness. The 
&lt;/p&gt;</description></item></channel></rss>