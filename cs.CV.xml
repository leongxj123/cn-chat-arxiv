<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#35760;&#24405;&#65292;&#25105;&#20204;&#30340;ReAlnet&#27169;&#22411;&#19982;&#20154;&#33041;&#27963;&#21160;&#30456;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#30456;&#20284;&#24615;&#65292;&#25552;&#20379;&#20102;&#26356;&#31867;&#20284;&#20154;&#31867;&#22823;&#33041;&#35270;&#35273;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2401.17231</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#31070;&#32463;&#34920;&#31034;&#23545;&#40784;&#23454;&#29616;&#26356;&#31867;&#20284;&#20154;&#31867;&#22823;&#33041;&#35270;&#35273;&#30340;ReAlnet&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ReAlnet: Achieving More Human Brain-Like Vision via Human Neural Representational Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17231
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#35760;&#24405;&#65292;&#25105;&#20204;&#30340;ReAlnet&#27169;&#22411;&#19982;&#20154;&#33041;&#27963;&#21160;&#30456;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#30456;&#20284;&#24615;&#65292;&#25552;&#20379;&#20102;&#26356;&#31867;&#20284;&#20154;&#31867;&#22823;&#33041;&#35270;&#35273;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#29289;&#20307;&#35782;&#21035;&#27169;&#22411;&#22312;&#27169;&#25311;&#20154;&#33041;&#35270;&#35273;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#21033;&#29992;&#31070;&#32463;&#25968;&#25454;&#26469;&#27169;&#20223;&#22823;&#33041;&#22788;&#29702;&#30340;&#28508;&#21147;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#38750;&#20154;&#31867;&#23454;&#39564;&#23545;&#35937;&#30340;&#20405;&#20837;&#24615;&#31070;&#32463;&#35760;&#24405;&#65292;&#36825;&#22312;&#25105;&#20204;&#23545;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#21644;&#24320;&#21457;&#26356;&#31867;&#20284;&#20154;&#31867;&#22823;&#33041;&#35270;&#35273;&#27169;&#22411;&#30340;&#29702;&#35299;&#19978;&#23384;&#22312;&#30528;&#37325;&#35201;&#30340;&#32570;&#21475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#8220;Re(presentational)Al(ignment)net&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#35760;&#24405;&#20026;&#22522;&#30784;&#30340;&#19982;&#20154;&#33041;&#27963;&#21160;&#30456;&#23545;&#40784;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#19982;&#20154;&#33041;&#34920;&#31034;&#26356;&#39640;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#22270;&#20687;&#21040;&#33041;&#22810;&#23618;&#32534;&#30721;&#23545;&#40784;&#26694;&#26550;&#19981;&#20165;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#22810;&#20010;&#23618;&#27425;&#65292;&#26631;&#24535;&#30528;&#31070;&#32463;&#23545;&#40784;&#26041;&#38754;&#30340;&#37325;&#22823;&#31361;&#30772;&#65292;&#32780;&#19988;&#36824;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#21644;&#27169;&#20223;&#20154;&#33041;&#30340;&#35270;&#35273;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable strides made in artificial intelligence, current object recognition models still lag behind in emulating the mechanism of visual information processing in human brains. Recent studies have highlighted the potential of using neural data to mimic brain processing; however, these often reply on invasive neural recordings from non-human subjects, leaving a critical gap in our understanding of human visual perception and the development of more human brain-like vision models. Addressing this gap, we present, for the first time, "Re(presentational)Al(ignment)net", a vision model aligned with human brain activity based on non-invasive EEG recordings, demonstrating a significantly higher similarity to human brain representations. Our innovative image-to-brain multi-layer encoding alignment framework not only optimizes multiple layers of the model, marking a substantial leap in neural alignment, but also enables the model to efficiently learn and mimic human brain's visua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20998;&#25968;&#21305;&#37197;&#35268;&#21017;&#65288;SMaRt&#65289;&#26469;&#25913;&#36827;GANs&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25345;&#32493;&#23558;&#29983;&#25104;&#30340;&#25968;&#25454;&#28857;&#25512;&#21521;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#65292;&#25552;&#39640;&#20102;&#21512;&#25104;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18208</link><description>&lt;p&gt;
SMaRt: &#20351;&#29992;&#20998;&#25968;&#21305;&#37197;&#35268;&#21017;&#25913;&#36827;GANs
&lt;/p&gt;
&lt;p&gt;
SMaRt: Improving GANs with Score Matching Regularity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20998;&#25968;&#21305;&#37197;&#35268;&#21017;&#65288;SMaRt&#65289;&#26469;&#25913;&#36827;GANs&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25345;&#32493;&#23558;&#29983;&#25104;&#30340;&#25968;&#25454;&#28857;&#25512;&#21521;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#65292;&#25552;&#39640;&#20102;&#21512;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#36890;&#24120;&#22312;&#23398;&#20064;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#22797;&#26434;&#25968;&#25454;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;GANs&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#24182;&#20174;&#29702;&#35770;&#19978;&#25581;&#31034;&#20102;GAN&#35757;&#32451;&#30340;&#21407;&#22987;&#23545;&#25239;&#25439;&#22833;&#19981;&#33021;&#35299;&#20915;&#29983;&#25104;&#25968;&#25454;&#27969;&#24418;&#30340;&#27491;&#27979;&#24230;&#23376;&#38598;&#33853;&#22312;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#20043;&#22806;&#30340;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#20998;&#25968;&#21305;&#37197;&#21487;&#20197;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25345;&#32493;&#23558;&#29983;&#25104;&#30340;&#25968;&#25454;&#28857;&#25512;&#21521;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#35268;&#21017;&#65288;SMaRt&#65289;&#26469;&#25913;&#36827;GANs&#30340;&#20248;&#21270;&#12290;&#23545;&#20110;&#32463;&#39564;&#35777;&#25454;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#29609;&#20855;&#31034;&#20363;&#26469;&#23637;&#31034;&#36890;&#36807;&#36741;&#21161;&#19968;&#20010;&#30495;&#23454;&#24471;&#20998;&#20989;&#25968;&#26469;&#35757;&#32451;GANs&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#20877;&#29616;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#65292;&#28982;&#21518;&#30830;&#35748;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25345;&#32493;&#25552;&#21319;&#21508;&#31181;&#29366;&#24577;&#30340;&#21512;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs) usually struggle in learning from highly diverse data, whose underlying manifold is complex. In this work, we revisit the mathematical foundations of GANs, and theoretically reveal that the native adversarial loss for GAN training is insufficient to fix the problem of subsets with positive Lebesgue measure of the generated data manifold lying out of the real data manifold. Instead, we find that score matching serves as a promising solution to this issue thanks to its capability of persistently pushing the generated data points towards the real data manifold. We thereby propose to improve the optimization of GANs with score matching regularity (SMaRt). Regarding the empirical evidences, we first design a toy example to show that training GANs by the aid of a ground-truth score function can help reproduce the real data distribution more accurately, and then confirm that our approach can consistently boost the synthesis performance of various state-o
&lt;/p&gt;</description></item></channel></rss>