<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36890;&#36807;&#31232;&#30095;&#36328;&#27169;&#24577;&#36866;&#24212;&#20462;&#22797;&#31232;&#30095;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;VLM&#20462;&#21098;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#25552;&#20986;&#31232;&#30095;&#27604;&#29575;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#20462;&#22797;&#31232;&#30095;VLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#19987;&#38376;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2404.02424</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#36328;&#27169;&#24577;&#36866;&#24212;&#20462;&#22797;&#31232;&#30095;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RESSA: Repair Sparse Vision-Language Models via Sparse Cross-Modality Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02424
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#36328;&#27169;&#24577;&#36866;&#24212;&#20462;&#22797;&#31232;&#30095;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;VLM&#20462;&#21098;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#25552;&#20986;&#31232;&#30095;&#27604;&#29575;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#20462;&#22797;&#31232;&#30095;VLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#19987;&#38376;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#25972;&#21512;&#20102;&#26469;&#33258;&#22810;&#20010;&#27169;&#24577;&#30340;&#19981;&#21516;&#20449;&#24687;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#22330;&#26223;&#20013;&#37096;&#32626;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#30340;VLMs&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#23613;&#31649;&#20462;&#21098;&#21518;&#24494;&#35843;&#25552;&#20379;&#20102;&#19968;&#31181;&#20445;&#25345;&#26356;&#23567;&#27169;&#22411;&#22823;&#23567;&#24615;&#33021;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20854;&#22312;VLMs&#20013;&#30340;&#24212;&#29992;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#65292;&#36825;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#22914;&#20309;&#22312;&#19981;&#21516;&#27169;&#24577;&#29305;&#23450;&#27169;&#22411;&#20043;&#38388;&#20998;&#37197;&#31232;&#30095;&#24615;&#65292;&#20197;&#21450;&#22914;&#20309;&#20462;&#22797;&#34987;&#20462;&#21098;&#31232;&#30095;&#30340;VLMs&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#22238;&#31572;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20851;&#20110;VLM&#20462;&#21098;&#30340;&#21021;&#27493;&#30740;&#31350;&#65292;&#21457;&#29616;&#20351;&#29992;&#30456;&#21516;&#31232;&#30095;&#27604;&#29575;&#20462;&#21098;&#35270;&#35273;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#23454;&#29616;&#25509;&#36817;&#26368;&#20339;&#24615;&#33021;&#12290;&#23545;&#20110;&#31532;&#20108;&#20010;&#38382;&#39064;&#65292;&#19982;&#24494;&#35843;&#21333;&#27169;&#31232;&#30095;&#27169;&#22411;&#19981;&#21516;&#65292;&#31232;&#30095;VLMs&#28041;&#21450;&#36328;&#27169;&#24577;&#20132;&#20114;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02424v1 Announce Type: new  Abstract: Vision-Language Models (VLMs), integrating diverse information from multiple modalities, have shown remarkable success across various tasks. However, deploying VLMs, comprising large-scale vision and language models poses challenges in resource-constrained scenarios. While pruning followed by finetuning offers a potential solution to maintain performance with smaller model sizes, its application to VLMs remains relatively unexplored, presenting two main questions: how to distribute sparsity across different modality-specific models, and how to repair the performance of pruned sparse VLMs. To answer the first question, we conducted preliminary studies on VLM pruning and found that pruning vision models and language models with the same sparsity ratios contribute to nearly optimal performance. For the second question, unlike finetuning unimodal sparse models, sparse VLMs involve cross-modality interactions, requiring specialized techniques
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MagicLens&#65292;&#19968;&#31995;&#21015;&#25903;&#25345;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#26680;&#24515;&#21019;&#26032;&#22312;&#20110;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20351;&#24471;&#22270;&#20687;&#26816;&#32034;&#21487;&#20197;&#26816;&#32034;&#21040;&#27604;&#35270;&#35273;&#30456;&#20284;&#24615;&#26356;&#20016;&#23500;&#20851;&#31995;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.19651</link><description>&lt;p&gt;
MagicLens&#65306;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#19982;&#24320;&#25918;&#24335;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MagicLens&#65292;&#19968;&#31995;&#21015;&#25903;&#25345;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#26680;&#24515;&#21019;&#26032;&#22312;&#20110;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20351;&#24471;&#22270;&#20687;&#26816;&#32034;&#21487;&#20197;&#26816;&#32034;&#21040;&#27604;&#35270;&#35273;&#30456;&#20284;&#24615;&#26356;&#20016;&#23500;&#20851;&#31995;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#26816;&#32034;&#65292;&#21363;&#26681;&#25454;&#21442;&#32771;&#22270;&#20687;&#26597;&#25214;&#25152;&#38656;&#22270;&#20687;&#65292;&#22266;&#26377;&#22320;&#21253;&#21547;&#38590;&#20197;&#20165;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#24230;&#37327;&#25429;&#25417;&#21040;&#30340;&#20016;&#23500;&#12289;&#22810;&#26041;&#38754;&#30340;&#25628;&#32034;&#24847;&#22270;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20801;&#35768;&#29992;&#25143;&#26356;&#33258;&#30001;&#22320;&#34920;&#36798;&#20182;&#20204;&#30340;&#25628;&#32034;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#37027;&#20123;&#35270;&#35273;&#19978;&#30456;&#20284;&#21644;/&#25110;&#21487;&#20197;&#29992;&#19968;&#23567;&#32452;&#39044;&#23450;&#20041;&#20851;&#31995;&#26469;&#34920;&#24449;&#30340;&#22270;&#20687;&#23545;&#19978;&#12290;&#26412;&#25991;&#30340;&#26680;&#24515;&#35770;&#28857;&#26159;&#25991;&#26412;&#25351;&#20196;&#21487;&#20197;&#20351;&#22270;&#20687;&#26816;&#32034;&#33021;&#22815;&#26816;&#32034;&#21040;&#27604;&#35270;&#35273;&#30456;&#20284;&#24615;&#26356;&#20016;&#23500;&#20851;&#31995;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MagicLens&#65292;&#19968;&#31995;&#21015;&#25903;&#25345;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#12290;MagicLens&#24314;&#31435;&#22312;&#19968;&#20010;&#37325;&#35201;&#30340;&#26032;&#39062;&#35265;&#35299;&#19978;&#65306;&#33258;&#28982;&#21457;&#29983;&#22312;&#21516;&#19968;&#32593;&#39029;&#19978;&#30340;&#22270;&#20687;&#23545;&#21253;&#21547;&#30528;&#22823;&#37327;&#38544;&#24335;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#20869;&#37096;&#35270;&#22270;&#65289;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#32508;&#21512;&#25351;&#20196;&#23558;&#36825;&#20123;&#38544;&#24335;&#20851;&#31995;&#21464;&#20026;&#26174;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19651v1 Announce Type: cross  Abstract: Image retrieval, i.e., finding desired images given a reference image, inherently encompasses rich, multi-faceted search intents that are difficult to capture solely using image-based measures. Recent work leverages text instructions to allow users to more freely express their search intents. However, existing work primarily focuses on image pairs that are visually similar and/or can be characterized by a small set of pre-defined relations. The core thesis of this paper is that text instructions can enable retrieving images with richer relations beyond visual similarity. To show this, we introduce MagicLens, a series of self-supervised image retrieval models that support open-ended instructions. MagicLens is built on a key novel insight: image pairs that naturally occur on the same web pages contain a wide range of implicit relations (e.g., inside view of), and we can bring those implicit relations explicit by synthesizing instructions
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#31383;&#21475;&#30340;Mamba UNet&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;&#23616;&#37096;&#31354;&#38388;&#24314;&#27169;&#26041;&#38754;&#26377;&#20248;&#21183;&#65292;&#21516;&#26102;&#22312;&#20840;&#23616;&#24314;&#27169;&#26041;&#38754;&#20445;&#25345;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.07332</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#31383;&#21475;&#30340;Mamba UNet&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65306;&#36229;&#36234;&#21367;&#31215;&#21644;&#33258;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large Window-based Mamba UNet for Medical Image Segmentation: Beyond Convolution and Self-attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07332
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#31383;&#21475;&#30340;Mamba UNet&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;&#23616;&#37096;&#31354;&#38388;&#24314;&#27169;&#26041;&#38754;&#26377;&#20248;&#21183;&#65292;&#21516;&#26102;&#22312;&#20840;&#23616;&#24314;&#27169;&#26041;&#38754;&#20445;&#25345;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#25552;&#20379;&#20102;&#26377;&#20851;&#30446;&#26631;&#22120;&#23448;&#25110;&#32452;&#32455;&#36718;&#24275;&#21644;&#23610;&#23544;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#26377;&#21161;&#20110;&#25913;&#36827;&#35786;&#26029;&#12289;&#20998;&#26512;&#21644;&#27835;&#30103;&#12290;&#26368;&#36817;&#20960;&#24180;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;Transformer&#22312;&#36825;&#19968;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#19968;&#23450;&#38382;&#39064;&#65292;&#22914;&#26377;&#38480;&#30340;&#24863;&#30693;&#33539;&#22260;&#25110;&#26114;&#36149;&#30340;&#36828;&#31243;&#24314;&#27169;&#12290;Mamba&#65292;&#20316;&#20026;&#19968;&#31181;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#38271;&#31243;&#20381;&#36182;&#24615;&#24314;&#27169;&#30340;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;SSM&#65289;&#65292;&#26368;&#36817;&#20986;&#29616;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;2D&#21644;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#20110;&#22823;&#31383;&#21475;&#30340;Mamba U-&#24418;&#32593;&#32476;&#65292;&#21363;LMa-UNet&#12290;&#25105;&#20204;LMa-UNet&#30340;&#19968;&#20010;&#31361;&#20986;&#29305;&#28857;&#26159;&#21033;&#29992;&#22823;&#31383;&#21475;&#65292;&#22312;&#23616;&#37096;&#31354;&#38388;&#24314;&#27169;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#23567;&#26680;&#30340;CNN&#21644;&#22522;&#20110;&#23567;&#31383;&#21475;&#30340;Transformer&#65292;&#21516;&#26102;&#19982;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#30340;&#33258;&#27880;&#24847;&#21147;&#30456;&#27604;&#65292;&#22312;&#20840;&#23616;&#24314;&#27169;&#26041;&#38754;&#20445;&#25345;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07332v1 Announce Type: cross  Abstract: In clinical practice, medical image segmentation provides useful information on the contours and dimensions of target organs or tissues, facilitating improved diagnosis, analysis, and treatment. In the past few years, convolutional neural networks (CNNs) and Transformers have dominated this area, but they still suffer from either limited receptive fields or costly long-range modeling. Mamba, a State Space Sequence Model (SSM), recently emerged as a promising paradigm for long-range dependency modeling with linear complexity. In this paper, we introduce a Large Window-based Mamba U}-shape Network, or LMa-UNet, for 2D and 3D medical image segmentation. A distinguishing feature of our LMa-UNet is its utilization of large windows, excelling in locally spatial modeling compared to small kernel-based CNNs and small window-based Transformers, while maintaining superior efficiency in global modeling compared to self-attention with quadratic co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#36890;&#36807;&#26500;&#24314;&#27602;&#32032;-&#28165;&#27905;&#25552;&#31034;&#23545;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807; Proximal Policy Optimization &#35757;&#32451;&#20248;&#21270;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#21508;&#31181; T2I &#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10882</link><description>&lt;p&gt;
&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#29992;&#20110;&#23433;&#20840;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Universal Prompt Optimizer for Safe Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10882
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#65292;&#36890;&#36807;&#26500;&#24314;&#27602;&#32032;-&#28165;&#27905;&#25552;&#31034;&#23545;&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807; Proximal Policy Optimization &#35757;&#32451;&#20248;&#21270;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#21508;&#31181; T2I &#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#22312;&#26681;&#25454;&#25991;&#23383;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#19981;&#23433;&#20840;&#36755;&#20837;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#65292;&#22914;&#33394;&#24773;&#12289;&#39578;&#25200;&#21644;&#38750;&#27861;&#27963;&#21160;&#22270;&#20687;&#12290;&#22522;&#20110;&#22270;&#20687;&#26816;&#26597;&#22120;&#12289;&#27169;&#22411;&#24494;&#35843;&#21644;&#23884;&#20837;&#24335;&#38459;&#27490;&#30340;&#29616;&#26377;&#30740;&#31350;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#40657;&#30418;&#22330;&#26223;&#20013;&#23433;&#20840; T2I &#29983;&#25104;&#30340;&#36890;&#29992;&#25552;&#31034;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10882v1 Announce Type: cross  Abstract: Text-to-Image (T2I) models have shown great performance in generating images based on textual prompts. However, these models are vulnerable to unsafe input to generate unsafe content like sexual, harassment and illegal-activity images. Existing studies based on image checker, model fine-tuning and embedding blocking are impractical in real-world applications. Hence, \textit{we propose the first universal prompt optimizer for safe T2I generation in black-box scenario}. We first construct a dataset consisting of toxic-clean prompt pairs by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting toxic prompt to clean prompt while preserving semantic information, we design a novel reward function measuring toxicity and text alignment of generated images and train the optimizer through Proximal Policy Optimization. Experiments show that our approach can effectively reduce the likelihood of various T2I models in generating in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#30784;&#35270;&#35273;&#27169;&#22411;&#23545;&#26377;&#25928;&#20027;&#21160;&#23398;&#20064;&#30340;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31616;&#21333;&#20248;&#38597;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#24179;&#34913;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.14555</link><description>&lt;p&gt;
&#22312;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#37325;&#26032;&#23457;&#35270;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Revisiting Active Learning in the Era of Vision Foundation Models. (arXiv:2401.14555v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#30784;&#35270;&#35273;&#27169;&#22411;&#23545;&#26377;&#25928;&#20027;&#21160;&#23398;&#20064;&#30340;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31616;&#21333;&#20248;&#38597;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#24179;&#34913;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#35270;&#35273;&#25110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26159;&#22312;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25110;&#22122;&#22768;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#24182;&#23398;&#20064;&#21040;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26631;&#27880;&#25110;&#23569;&#26631;&#27880;&#24615;&#33021;&#30340;&#40065;&#26834;&#34920;&#31034;&#12290;&#37492;&#20110;&#36825;&#20123;&#29305;&#24615;&#65292;&#23427;&#20204;&#26159;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#30340;&#33258;&#28982;&#36873;&#25321;&#65292;&#26088;&#22312;&#23454;&#29616;&#26631;&#35760;&#25928;&#29575;&#30340;&#26368;&#22823;&#21270;&#65292;&#20294;&#22312;&#20302;&#39044;&#31639;&#26465;&#20214;&#19979;&#65292;&#22522;&#30784;&#27169;&#22411;&#30340;&#20840;&#37096;&#28508;&#21147;&#22312;AL&#29615;&#22659;&#20013;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#30784;&#27169;&#22411;&#23545;&#26377;&#25928;AL&#30340;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#30340;&#24433;&#21709;&#65292;&#21363;1&#65289;&#21021;&#22987;&#26631;&#35760;&#26679;&#26412;&#27744;&#30340;&#36873;&#25321;&#65292;2&#65289;&#30830;&#20445;&#22810;&#26679;&#24615;&#25277;&#26679;&#65292;&#20197;&#21450;3&#65289;&#20195;&#34920;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#25277;&#26679;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22522;&#30784;&#27169;&#22411;&#65288;DINOv2&#12289;OpenCLIP&#65289;&#30340;&#40065;&#26834;&#34920;&#31034;&#22914;&#20309;&#25361;&#25112;&#24050;&#26377;&#30340;&#20027;&#21160;&#23398;&#20064;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#20026;&#19968;&#20010;&#26032;&#30340;&#31616;&#21333;&#20248;&#38597;&#30340;AL&#31574;&#30053;&#30340;&#26377;&#21407;&#21017;&#26500;&#24314;&#25552;&#20379;&#20102;&#25351;&#23548;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#20351;&#29992;dropout&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation vision or vision-language models are trained on large unlabeled or noisy data and learn robust representations that can achieve impressive zeroor few-shot performance on diverse tasks. Given these properties, they are a natural fit for active learning (AL), which aims to maximize labeling efficiency, but the full potential of foundation models has not been explored in the context of AL, specifically in the low-budget regime. In this work, we evaluate how foundation models influence three critical components of effective AL, namely, 1) initial labeled pool selection, 2) ensuring diverse sampling, and 3) the trade-off between representative and uncertainty sampling. We systematically study how the robust representations of foundation models (DINOv2, OpenCLIP) challenge existing findings in active learning. Our observations inform the principled construction of a new simple and elegant AL strategy that balances uncertainty estimated via dropout with sample diversity. We exten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26368;&#26032;&#30340;&#38754;&#37096;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#24433;&#21709;&#22240;&#32032;&#30340;&#28145;&#20837;&#35265;&#35299;&#65292;&#24182;&#22312;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.04364</link><description>&lt;p&gt;
SoK&#65306;&#38754;&#37096;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
SoK: Facial Deepfake Detectors. (arXiv:2401.04364v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26368;&#26032;&#30340;&#38754;&#37096;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#24433;&#21709;&#22240;&#32032;&#30340;&#28145;&#20837;&#35265;&#35299;&#65292;&#24182;&#22312;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#36805;&#36895;&#25104;&#20026;&#23545;&#31038;&#20250;&#26500;&#25104;&#28145;&#36828;&#21644;&#20005;&#37325;&#23041;&#32961;&#30340;&#21407;&#22240;&#20043;&#19968;&#65292;&#20027;&#35201;&#30001;&#20110;&#20854;&#26131;&#20110;&#21046;&#20316;&#21644;&#20256;&#25773;&#12290;&#36825;&#31181;&#24773;&#20917;&#21152;&#36895;&#20102;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#22312;&#39564;&#35777;&#26102; heavily &#20381;&#36182;&#23454;&#39564;&#23460;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#22320;&#35753;&#23427;&#20204;&#24212;&#23545;&#26032;&#39062;&#12289;&#26032;&#20852;&#21644;&#23454;&#38469;&#30340;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#12290;&#26412;&#25991;&#23545;&#26368;&#26032;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#36827;&#34892;&#24191;&#27867;&#20840;&#38754;&#30340;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#26681;&#25454;&#20960;&#20010;&#20851;&#38190;&#26631;&#20934;&#23545;&#23427;&#20204;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#20123;&#26631;&#20934;&#23558;&#36825;&#20123;&#26816;&#27979;&#22120;&#20998;&#20026; 4 &#20010;&#39640;&#32423;&#32452;&#21035;&#21644; 13 &#20010;&#32454;&#31890;&#24230;&#23376;&#32452;&#21035;&#65292;&#37117;&#36981;&#24490;&#19968;&#20010;&#32479;&#19968;&#30340;&#26631;&#20934;&#27010;&#24565;&#26694;&#26550;&#12290;&#36825;&#31181;&#20998;&#31867;&#21644;&#26694;&#26550;&#25552;&#20379;&#20102;&#23545;&#24433;&#21709;&#26816;&#27979;&#22120;&#21151;&#25928;&#30340;&#22240;&#32032;&#30340;&#28145;&#20837;&#21644;&#23454;&#29992;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#23545; 16 &#20010;&#20027;&#35201;&#30340;&#26816;&#27979;&#22120;&#22312;&#21508;&#31181;&#26631;&#20934;&#30340;&#25915;&#20987;&#22330;&#26223;&#20013;&#30340;&#26222;&#36866;&#24615;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#40657;&#30418;&#25915;&#20987;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deepfakes have rapidly emerged as a profound and serious threat to society, primarily due to their ease of creation and dissemination. This situation has triggered an accelerated development of deepfake detection technologies. However, many existing detectors rely heavily on lab-generated datasets for validation, which may not effectively prepare them for novel, emerging, and real-world deepfake techniques. In this paper, we conduct an extensive and comprehensive review and analysis of the latest state-of-the-art deepfake detectors, evaluating them against several critical criteria. These criteria facilitate the categorization of these detectors into 4 high-level groups and 13 fine-grained sub-groups, all aligned with a unified standard conceptual framework. This classification and framework offer deep and practical insights into the factors that affect detector efficacy. We assess the generalizability of 16 leading detectors across various standard attack scenarios, including black-bo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27874;&#21160;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#27874;&#21160;&#24615;&#26469;&#25512;&#26029;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35757;&#32451;&#35760;&#24405;&#30340;&#25104;&#21592;&#36523;&#20221;&#12290;</title><link>http://arxiv.org/abs/2308.12143</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27874;&#21160;&#30340;&#29983;&#25104;&#27169;&#22411;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Fluctuation based Membership Inference Attack for Generative Models. (arXiv:2308.12143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27874;&#21160;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#27874;&#21160;&#24615;&#26469;&#25512;&#26029;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35757;&#32451;&#35760;&#24405;&#30340;&#25104;&#21592;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;(MIA)&#36890;&#36807;&#26597;&#35810;&#27169;&#22411;&#26469;&#35782;&#21035;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35760;&#24405;&#12290;&#23545;&#32463;&#20856;&#20998;&#31867;&#27169;&#22411;&#30340;MIA&#24050;&#26377;&#24456;&#22810;&#30740;&#31350;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#24320;&#22987;&#25506;&#32034;&#22914;&#20309;&#23558;MIA&#24212;&#29992;&#21040;&#29983;&#25104;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#38754;&#21521;&#29983;&#25104;&#27169;&#22411;&#30340;MIA&#20027;&#35201;&#20381;&#36182;&#20110;&#30446;&#26631;&#27169;&#22411;&#30340;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#36807;&#25311;&#21512;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#36991;&#20813;&#65292;&#32780;&#29616;&#26377;&#30340;MIA&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#19982;&#36807;&#25311;&#21512;&#19981;&#21516;&#65292;&#35760;&#24518;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#26356;&#20026;&#26222;&#36941;&#30340;&#29616;&#35937;&#12290;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#23548;&#33268;&#29983;&#25104;&#35760;&#24405;&#30340;&#27010;&#29575;&#20998;&#24067;&#21576;&#29616;&#20986;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27874;&#21160;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#23427;&#26159;&#19968;&#31181;&#40657;&#30418;MIA&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#27874;&#21160;&#26469;&#25512;&#26029;&#25104;&#21592;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting t
&lt;/p&gt;</description></item></channel></rss>