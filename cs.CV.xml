<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09871</link><description>&lt;p&gt;
ThermoHands&#65306;&#19968;&#31181;&#29992;&#20110;&#20174;&#20027;&#35266;&#35270;&#35282;&#28909;&#22270;&#20013;&#20272;&#35745;3D&#25163;&#37096;&#23039;&#21183;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09871
&lt;/p&gt;
&lt;p&gt;
ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ThermoHands&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#22522;&#20110;&#28909;&#22270;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#20811;&#26381;&#35832;&#22914;&#20809;&#29031;&#21464;&#21270;&#21644;&#36974;&#25377;&#65288;&#20363;&#22914;&#25163;&#37096;&#31359;&#25140;&#29289;&#65289;&#31561;&#25361;&#25112;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;28&#21517;&#20027;&#20307;&#36827;&#34892;&#25163;-&#29289;&#20307;&#21644;&#25163;-&#34394;&#25311;&#20132;&#20114;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#65292;&#32463;&#36807;&#33258;&#21160;&#21270;&#36807;&#31243;&#20934;&#30830;&#26631;&#27880;&#20102;3D&#25163;&#37096;&#23039;&#21183;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#21033;&#29992;&#21452;transformer&#27169;&#22359;&#22312;&#28909;&#22270;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20102;TheFormer&#30340;&#39046;&#20808;&#24615;&#33021;&#65292;&#24182;&#30830;&#35748;&#20102;&#28909;&#25104;&#20687;&#22312;&#23454;&#29616;&#24694;&#21155;&#26465;&#20214;&#19979;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09871v1 Announce Type: cross  Abstract: In this work, we present ThermoHands, a new benchmark for thermal image-based egocentric 3D hand pose estimation, aimed at overcoming challenges like varying lighting and obstructions (e.g., handwear). The benchmark includes a diverse dataset from 28 subjects performing hand-object and hand-virtual interactions, accurately annotated with 3D hand poses through an automated process. We introduce a bespoken baseline method, TheFormer, utilizing dual transformer modules for effective egocentric 3D hand pose estimation in thermal imagery. Our experimental results highlight TheFormer's leading performance and affirm thermal imaging's effectiveness in enabling robust 3D hand pose estimation in adverse conditions.
&lt;/p&gt;</description></item><item><title>CLCE&#26041;&#27861;&#32467;&#21512;&#20102;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#36890;&#36807;&#21327;&#21516;&#21033;&#29992;&#38590;&#20363;&#25366;&#25496;&#25552;&#39640;&#20102;&#24615;&#33021;&#34920;&#29616;</title><link>https://arxiv.org/abs/2402.14551</link><description>&lt;p&gt;
CLCE&#65306;&#19968;&#31181;&#20248;&#21270;&#23398;&#20064;&#34701;&#21512;&#30340;&#25913;&#36827;&#20132;&#21449;&#29109;&#21644;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for Optimized Learning Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14551
&lt;/p&gt;
&lt;p&gt;
CLCE&#26041;&#27861;&#32467;&#21512;&#20102;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#36890;&#36807;&#21327;&#21516;&#21033;&#29992;&#38590;&#20363;&#25366;&#25496;&#25552;&#39640;&#20102;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;&#27169;&#22411;&#20027;&#35201;&#37319;&#29992;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#21021;&#22987;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#65288;CE&#65289;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#35777;&#26126;CE&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CLCE&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#19982;CE&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#20445;&#25345;&#20102;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#21183;&#65292;&#32780;&#19988;&#20197;&#21327;&#21516;&#26041;&#24335;&#21033;&#29992;&#38590;&#20363;&#25366;&#25496;&#26469;&#22686;&#24378;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14551v1 Announce Type: cross  Abstract: State-of-the-art pre-trained image models predominantly adopt a two-stage approach: initial unsupervised pre-training on large-scale datasets followed by task-specific fine-tuning using Cross-Entropy loss~(CE). However, it has been demonstrated that CE can compromise model generalization and stability. While recent works employing contrastive learning address some of these limitations by enhancing the quality of embeddings and producing better decision boundaries, they often overlook the importance of hard negative mining and rely on resource intensive and slow training using large sample batches. To counter these issues, we introduce a novel approach named CLCE, which integrates Label-Aware Contrastive Learning with CE. Our approach not only maintains the strengths of both loss functions but also leverages hard negative mining in a synergistic way to enhance performance. Experimental results demonstrate that CLCE significantly outperf
&lt;/p&gt;</description></item></channel></rss>