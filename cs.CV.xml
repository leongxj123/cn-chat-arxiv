<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#24418;&#24577;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#23384;&#22312;&#25361;&#25112;&#65292;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25506;&#35752;&#20102;&#22522;&#20110;&#24494;&#20998;&#26041;&#27861;&#21644;&#21453;&#21521;&#20256;&#25773;&#23545;&#24418;&#24577;&#32593;&#32476;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#21021;&#22987;&#21270;&#21644;&#23398;&#20064;&#29575;&#30340;&#29702;&#35770;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2403.12975</link><description>&lt;p&gt;
&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#24418;&#24577;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#20123;&#29702;&#35770;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Training morphological neural networks with gradient descent: some theoretical insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12975
&lt;/p&gt;
&lt;p&gt;
&#24418;&#24577;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#23384;&#22312;&#25361;&#25112;&#65292;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25506;&#35752;&#20102;&#22522;&#20110;&#24494;&#20998;&#26041;&#27861;&#21644;&#21453;&#21521;&#20256;&#25773;&#23545;&#24418;&#24577;&#32593;&#32476;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#21021;&#22987;&#21270;&#21644;&#23398;&#20064;&#29575;&#30340;&#29702;&#35770;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#24577;&#31070;&#32463;&#32593;&#32476;&#25110;&#23618;&#21487;&#20197;&#25104;&#20026;&#25552;&#21319;&#25968;&#23398;&#24418;&#24577;&#23398;&#36827;&#23637;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26080;&#35770;&#26159;&#22312;&#29702;&#35770;&#26041;&#38754;&#65292;&#22914;&#23436;&#25972;&#26684;&#31639;&#23376;&#30340;&#34920;&#31034;&#65292;&#36824;&#26159;&#22312;&#22270;&#20687;&#22788;&#29702;&#27969;&#31243;&#30340;&#24320;&#21457;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#26550;&#26500;&#21253;&#21547;&#22810;&#23618;&#24418;&#24577;&#23398;&#26102;&#65292;&#33267;&#23569;&#22312;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#31639;&#27861;&#30340;&#27969;&#34892;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20869;&#65292;&#36825;&#20123;&#32593;&#32476;&#24456;&#38590;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#24494;&#20998;&#26041;&#27861;&#21644;&#21453;&#21521;&#20256;&#25773;&#24212;&#29992;&#20110;&#24418;&#24577;&#32593;&#32476;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#32771;&#34385;&#21040;Bouligand&#23548;&#25968;&#30340;&#38750;&#20809;&#28369;&#20248;&#21270;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35265;&#35299;&#21644;&#39318;&#20010;&#29702;&#35770;&#25351;&#21335;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#21021;&#22987;&#21270;&#21644;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12975v1 Announce Type: cross  Abstract: Morphological neural networks, or layers, can be a powerful tool to boost the progress in mathematical morphology, either on theoretical aspects such as the representation of complete lattice operators, or in the development of image processing pipelines. However, these architectures turn out to be difficult to train when they count more than a few morphological layers, at least within popular machine learning frameworks which use gradient descent based optimization algorithms. In this paper we investigate the potential and limitations of differentiation based approaches and back-propagation applied to morphological networks, in light of the non-smooth optimization concept of Bouligand derivative. We provide insights and first theoretical guidelines, in particular regarding initialization and learning rates.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sparse MetA-Tuning&#65288;SMAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28789;&#24863;&#26469;&#33258;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#26041;&#27861;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#22495;&#22806;&#20219;&#21153;&#25935;&#24863;&#24615;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#36716;&#31227;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.08477</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#25554;&#20540;&#19987;&#23478;&#37322;&#25918;&#20803;&#35843;&#25972;&#30340;&#21147;&#37327;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sparse MetA-Tuning&#65288;SMAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28789;&#24863;&#26469;&#33258;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#26041;&#27861;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#22495;&#22806;&#20219;&#21153;&#25935;&#24863;&#24615;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#36716;&#31227;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26234;&#24935;&#24314;&#35758;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#65292;&#26159;&#35270;&#35273;&#36801;&#31227;&#23398;&#20064;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#21462;&#20195;&#20102;&#35832;&#22914;&#20803;&#23398;&#20064;&#20043;&#31867;&#30340;&#20016;&#23500;&#25991;&#29486;&#12290;&#20026;&#20102;&#20860;&#39038;&#20004;&#32773;&#30340;&#21033;&#30410;&#65292;&#20803;&#35843;&#25972;&#24341;&#20837;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#38543;&#21518;&#20248;&#21270;&#38454;&#27573;&#65292;&#20294;&#36804;&#20170;&#21482;&#23637;&#29616;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#65292;&#20851;&#38190;&#22320;&#22312;&#22495;&#22806;&#65288;OOD&#65289;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28789;&#24863;&#26469;&#33258;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#26041;&#27861;&#30340; Sparse MetA-Tuning&#65288;SMAT&#65289;&#26041;&#27861;&#65292;&#23427;&#32463;&#36807;&#35757;&#32451;&#20197;&#33258;&#21160;&#22320;&#20026;&#27599;&#20010;&#20219;&#21153;&#38548;&#31163;&#39044;&#35757;&#32451;&#21442;&#25968;&#23376;&#38598;&#20197;&#36827;&#34892;&#20803;&#35843;&#25972;&#12290;SMAT&#25104;&#21151;&#20811;&#26381;&#20102;OOD&#25935;&#24863;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#22686;&#24378;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#36716;&#31227;&#33021;&#21147;&#30340;&#25215;&#35834;&#12290;&#25105;&#20204;&#22312;Meta-Dataset&#19982;&#39069;&#22806;&#30340;OO&#25361;&#25112;&#32452;&#21512;&#19978;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08477v1 Announce Type: cross  Abstract: Conventional wisdom suggests parameter-efficient fine-tuning of foundation models as the state-of-the-art method for transfer learning in vision, replacing the rich literature of alternatives such as meta-learning. In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-domain (OOD) tasks. In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task. SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient finetuning. We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OO
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#65292;&#26469;&#22686;&#24378;&#23545;&#24433;&#21709;&#25830;&#38500;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07362</link><description>&lt;p&gt;
&#25361;&#25112;&#36951;&#24536;&#65306;&#25581;&#31034;&#26426;&#22120;&#36951;&#24536;&#20013;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;
&lt;/p&gt;
&lt;p&gt;
Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#65292;&#26469;&#22686;&#24378;&#23545;&#24433;&#21709;&#25830;&#38500;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38752;&#35889;&#30340;&#26426;&#22120;&#23398;&#20064;(Machine Learning, ML)&#31038;&#21306;&#36234;&#26469;&#36234;&#35748;&#35782;&#21040;&#27169;&#22411;&#22312;&#35757;&#32451;&#21518;&#26377;&#36873;&#25321;&#24615;&#22320;&#8220;&#36951;&#24536;&#8221;&#25968;&#25454;&#28857;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#24341;&#20986;&#20102;&#26426;&#22120;&#36951;&#24536;(Machine Unlearning, MU)&#38382;&#39064;&#65292;&#26088;&#22312;&#28040;&#38500;&#36873;&#23450;&#25968;&#25454;&#28857;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#27169;&#22411;&#22312;&#36951;&#24536;&#21518;&#30340;&#23454;&#29992;&#24615;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;MU&#26041;&#27861;&#26469;&#25830;&#38500;&#25968;&#25454;&#24433;&#21709;&#65292;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#22312;&#38543;&#26426;&#25968;&#25454;&#36951;&#24536;&#19978;&#65292;&#24573;&#35270;&#20102;&#23545;&#20110;&#30495;&#23454;&#34913;&#37327;&#36951;&#24536;&#24615;&#33021;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#30340;&#37325;&#35201;&#25506;&#31350;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;MU&#35780;&#20272;&#35270;&#35282;&#12290;&#25105;&#20204;&#25552;&#20986;&#30830;&#23450;&#37027;&#20123;&#23545;&#24433;&#21709;&#25830;&#38500;&#26500;&#25104;&#26368;&#22823;&#25361;&#25112;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#25214;&#20986;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#12290;&#21033;&#29992;&#21452;&#23618;&#20248;&#21270;&#21407;&#21017;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#22312;&#19978;&#23618;&#20248;&#21270;&#20013;&#30340;&#36951;&#24536;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07362v1 Announce Type: cross  Abstract: The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively 'unlearning' data points after training. This leads to the problem of machine unlearning (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning. Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#39044;&#24494;&#35843;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#20302;&#31209;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#24674;&#22797;&#20934;&#30830;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;&#65292;&#21033;&#29992;&#36825;&#20010;&#26032;&#28431;&#27934;&#25915;&#20987;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.10208</link><description>&lt;p&gt;
&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Recovering the Pre-Fine-Tuning Weights of Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10208
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#39044;&#24494;&#35843;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#20302;&#31209;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#24674;&#22797;&#20934;&#30830;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;&#65292;&#21033;&#29992;&#36825;&#20010;&#26032;&#28431;&#27934;&#25915;&#20987;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#65292;&#20027;&#27969;&#27169;&#24335;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;i) &#22312;&#22823;&#35268;&#27169;&#20294;&#19981;&#23433;&#20840;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;ii) &#36890;&#36807;&#24494;&#35843;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12290;&#36825;&#31181;&#20570;&#27861;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#65292;&#22240;&#20026;&#30446;&#21069;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#19981;&#23433;&#20840;&#30340;&#39044;&#24494;&#35843;&#27169;&#22411;&#26435;&#37325;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#36825;&#31181;&#20551;&#35774;&#36890;&#24120;&#26159;&#38169;&#35823;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35889;&#21453;&#35843;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#20302;&#31209;&#65288;LoRA&#65289;&#24494;&#35843;&#27169;&#22411;&#24674;&#22797;&#39044;&#24494;&#35843;&#27169;&#22411;&#30340;&#26435;&#37325;&#12290;&#19982;&#20808;&#21069;&#35797;&#22270;&#24674;&#22797;&#39044;&#24494;&#35843;&#33021;&#21147;&#30340;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#24674;&#22797;&#31934;&#30830;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36825;&#20010;&#26032;&#30340;&#23545;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#20363;&#22914;&#20010;&#24615;&#21270;&#30340;&#31283;&#23450;&#25193;&#25955;&#21644;&#23545;&#40784;&#30340;Mistral&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10208v1 Announce Type: cross  Abstract: The dominant paradigm in generative modeling consists of two steps: i) pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model with human values via fine-tuning. This practice is considered safe, as no current method can recover the unsafe, pre-fine-tuning model weights. In this paper, we demonstrate that this assumption is often false. Concretely, we present Spectral DeTuning, a method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, our method aims to recover the exact pre-fine-tuning weights. Our approach exploits this new vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned Mistral.
&lt;/p&gt;</description></item><item><title>AdaTreeFormer&#26159;&#19968;&#31181;&#20174;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.02956</link><description>&lt;p&gt;
AdaTreeFormer: &#20174;&#19968;&#24352;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#36827;&#34892;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02956
&lt;/p&gt;
&lt;p&gt;
AdaTreeFormer&#26159;&#19968;&#31181;&#20174;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#19968;&#24352;&#33322;&#31354;&#25110;&#21355;&#26143;&#22270;&#20687;&#26469;&#20272;&#35745;&#21644;&#35745;&#25968;&#26641;&#26408;&#23494;&#24230;&#26159;&#25668;&#24433;&#27979;&#37327;&#21644;&#36965;&#24863;&#39046;&#22495;&#20013;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26862;&#26519;&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19981;&#21516;&#22320;&#24418;&#19978;&#21508;&#31181;&#21508;&#26679;&#30340;&#26641;&#26408;&#31181;&#31867;&#20005;&#37325;&#38459;&#30861;&#20102;&#26641;&#26408;&#35745;&#25968;&#27169;&#22411;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#20010;&#20174;&#20855;&#26377;&#36275;&#22815;&#26631;&#27880;&#26641;&#26408;&#30340;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;AdaTreeFormer&#65292;&#21253;&#21547;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#29992;&#20110;&#20174;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20013;&#25552;&#21462;&#31283;&#20581;&#30340;&#29305;&#24449;&#12290;&#23427;&#36824;&#21253;&#25324;&#19977;&#20010;&#23376;&#32593;&#32476;&#65306;&#20004;&#20010;&#29992;&#20110;&#20998;&#21035;&#20174;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#25552;&#21462;&#33258;&#27880;&#24847;&#21147;&#22270;&#65292;&#24182;&#19968;&#20010;&#29992;&#20110;&#25552;&#21462;&#36328;&#39046;&#22495;&#27880;&#24847;&#21147;&#22270;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#36866;&#24212;&#26426;&#21046;&#65292;&#29992;&#20110;&#20174;&#19981;&#21516;&#39046;&#22495;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of estimating and counting tree density using only a single aerial or satellite image is a difficult task in the fields of photogrammetry and remote sensing. However, it plays a crucial role in the management of forests. The huge variety of trees in varied topography severely hinders tree counting models to perform well. The purpose of this paper is to propose a framework that is learnt from the source domain with sufficient labeled trees and is adapted to the target domain with only a limited number of labeled trees. Our method, termed as AdaTreeFormer, contains one shared encoder with a hierarchical feature extraction scheme to extract robust features from the source and target domains. It also consists of three subnets: two for extracting self-domain attention maps from source and target domains respectively and one for extracting cross-domain attention maps. For the latter, an attention-to-adapt mechanism is introduced to distill relevant information from different doma
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#38754;&#21521;&#20154;&#33080;&#21464;&#24418;&#30340;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#30740;&#31350;&#20102;&#37319;&#26679;&#31639;&#27861;&#12289;&#36870;&#21521;DDIM&#27714;&#35299;&#22120;&#21644;&#37096;&#20998;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09484</link><description>&lt;p&gt;
&#25506;&#32034;&#38754;&#21521;&#20154;&#33080;&#21464;&#24418;&#30340;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#35774;&#35745;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Exploring the Design Space of Diffusion Autoencoders for Face Morphing. (arXiv:2310.09484v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09484
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#38754;&#21521;&#20154;&#33080;&#21464;&#24418;&#30340;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#30740;&#31350;&#20102;&#37319;&#26679;&#31639;&#27861;&#12289;&#36870;&#21521;DDIM&#27714;&#35299;&#22120;&#21644;&#37096;&#20998;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#21019;&#24314;&#30340;&#20154;&#33080;&#21464;&#24418;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#21019;&#26032;&#65292;&#32780;&#36825;&#31181;&#26041;&#27861;&#30340;&#35774;&#35745;&#31354;&#38388;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#35774;&#35745;&#31354;&#38388;&#30340;&#19977;&#20010;&#26041;&#38754;&#65292;&#21363;1&#65289;&#37319;&#26679;&#31639;&#27861;&#65292;2&#65289;&#36870;&#21521;DDIM&#27714;&#35299;&#22120;&#65292;&#20197;&#21450;3&#65289;&#36890;&#36807;&#28155;&#21152;&#23569;&#37327;&#22122;&#22768;&#36827;&#34892;&#37096;&#20998;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face morphs created by Diffusion Autoencoders are a recent innovation and the design space of such an approach has not been well explored. We explore three axes of the design space, i.e., 1) sampling algorithms, 2) the reverse DDIM solver, and 3) partial sampling through small amounts of added noise.
&lt;/p&gt;</description></item><item><title>&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#20250;&#36951;&#24536;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LDIFS&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#35777;&#26126;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2308.13320</link><description>&lt;p&gt;
&#24494;&#35843;&#21487;&#33021;&#21066;&#24369;&#22522;&#30784;&#27169;&#22411;&#65307;&#20445;&#30041;&#29305;&#24449;&#21487;&#33021;&#26159;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning can cripple your foundation model; preserving features may be the solution. (arXiv:2308.13320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13320
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#20250;&#36951;&#24536;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LDIFS&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#35777;&#26126;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#20027;&#35201;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#23481;&#37327;&#21644;&#23545;&#20174;&#20114;&#32852;&#32593;&#19978;&#29228;&#21462;&#30340;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#26292;&#38706;&#65292;&#20139;&#26377;&#23384;&#20648;&#20851;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#27010;&#24565;&#30340;&#30693;&#35782;&#30340;&#20248;&#21183;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20135;&#29983;&#20986;&#33394;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;&#24494;&#35843;&#27169;&#22411;&#22312;&#19982;&#19979;&#28216;&#20219;&#21153;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#35782;&#21035;&#27010;&#24565;&#30340;&#33021;&#21147;&#26174;&#33879;&#38477;&#20302;&#12290;&#36825;&#26174;&#28982;&#26159;&#19981;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#22312;&#39318;&#27425;&#23398;&#20064;&#36825;&#20123;&#27010;&#24565;&#26102;&#65292;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#37329;&#38065;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#19981;&#21487;&#21462;&#30340;&#29616;&#35937;&#31216;&#20026;&#8220;&#27010;&#24565;&#36951;&#24536;&#8221;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#22823;&#22810;&#25968;&#31471;&#21040;&#31471;&#24494;&#35843;&#26041;&#27861;&#37117;&#20005;&#37325;&#21463;&#21040;&#36825;&#31181;&#21103;&#20316;&#29992;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#30456;&#24403;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#21363;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;LDIFS&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained foundation models, owing primarily to their enormous capacity and exposure to vast amount of training data scraped from the internet, enjoy the advantage of storing knowledge about plenty of real-world concepts. Such models are typically fine-tuned on downstream datasets to produce remarkable state-of-the-art performances. While various fine-tuning methods have been devised and are shown to be highly effective, we observe that a fine-tuned model's ability to recognize concepts on tasks $\textit{different}$ from the downstream one is reduced significantly compared to its pre-trained counterpart. This is clearly undesirable as a huge amount of time and money went into learning those very concepts in the first place. We call this undesirable phenomenon "concept forgetting" and via experiments show that most end-to-end fine-tuning approaches suffer heavily from this side effect. To this end, we also propose a rather simple fix to this problem by designing a method called LDIFS 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#26681;&#25454;&#20219;&#21153;&#22797;&#26434;&#24230;&#21160;&#24577;&#35843;&#25972;&#27491;&#21017;&#21270;&#24378;&#24230;&#65292;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#21516;&#26102;&#38450;&#27490;&#36951;&#24536;&#20043;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#12290;&#23454;&#39564;&#34920;&#26126;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#20934;&#30830;&#21644;&#19981;&#26131;&#36951;&#24536;&#30340;&#35270;&#35273;&#22686;&#37327;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.13113</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Regularization for Class-Incremental Learning. (arXiv:2303.13113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#26681;&#25454;&#20219;&#21153;&#22797;&#26434;&#24230;&#21160;&#24577;&#35843;&#25972;&#27491;&#21017;&#21270;&#24378;&#24230;&#65292;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#21516;&#26102;&#38450;&#27490;&#36951;&#24536;&#20043;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#12290;&#23454;&#39564;&#34920;&#26126;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#20934;&#30830;&#21644;&#19981;&#26131;&#36951;&#24536;&#30340;&#35270;&#35273;&#22686;&#37327;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#26159;&#25351;&#22312;&#32500;&#25345;&#20808;&#21069;&#23398;&#20064;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#65292;&#26356;&#26032;&#20855;&#26377;&#26032;&#31867;&#21035;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#12290;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#26469;&#38450;&#27490;&#36951;&#24536;&#20043;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#26159;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#25972;&#20010;&#22686;&#37327;&#23398;&#20064;&#36807;&#31243;&#20013;&#20351;&#29992;&#24658;&#23450;&#30340;&#24378;&#24230;&#65292;&#21487;&#33021;&#26080;&#27861;&#21453;&#26144;&#25152;&#36935;&#21040;&#30340;&#20219;&#21153;&#38590;&#24230;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24517;&#35201;&#24615;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#25163;&#22836;&#20219;&#21153;&#30340;&#22797;&#26434;&#24230;&#21160;&#24577;&#35843;&#25972;&#27491;&#21017;&#21270;&#24378;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#30830;&#23450;&#27599;&#20010;&#23398;&#20064;&#20219;&#21153;&#30340;&#26368;&#20339;&#27491;&#21017;&#21270;&#24378;&#24230;&#12290;&#36890;&#36807;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#20004;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#23545;&#20110;&#23454;&#29616;&#26356;&#21152;&#20934;&#30830;&#21644;&#19981;&#26131;&#36951;&#24536;&#30340;&#35270;&#35273;&#22686;&#37327;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-Incremental Learning updates a deep classifier with new categories while maintaining the previously observed class accuracy. Regularizing the neural network weights is a common method to prevent forgetting previously learned classes while learning novel ones. However, existing regularizers use a constant magnitude throughout the learning sessions, which may not reflect the varying levels of difficulty of the tasks encountered during incremental learning. This study investigates the necessity of adaptive regularization in Class-Incremental Learning, which dynamically adjusts the regularization strength according to the complexity of the task at hand. We propose a Bayesian Optimization-based approach to automatically determine the optimal regularization magnitude for each learning task. Our experiments on two datasets via two regularizers demonstrate the importance of adaptive regularization for achieving accurate and less forgetful visual incremental learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#27010;&#29575;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25552;&#31034;&#26631;&#35760;&#25512;&#21521;&#24544;&#23454;&#25429;&#25417;&#26631;&#31614;&#29305;&#23450;&#30340;&#35270;&#35273;&#27010;&#24565;&#65292;&#32780;&#19981;&#26159;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#31867;&#21035;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25552;&#31034;&#24037;&#31243;&#30340;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.09100</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#34917;&#19969;-&#20196;&#29260;&#23545;&#40784;&#30340;&#36125;&#21494;&#26031;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models. (arXiv:2303.09100v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#27010;&#29575;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25552;&#31034;&#26631;&#35760;&#25512;&#21521;&#24544;&#23454;&#25429;&#25417;&#26631;&#31614;&#29305;&#23450;&#30340;&#35270;&#35273;&#27010;&#24565;&#65292;&#32780;&#19981;&#26159;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#31867;&#21035;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25552;&#31034;&#24037;&#31243;&#30340;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#26500;&#24314;&#26377;&#25928;&#25552;&#31034;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#36153;&#26102;&#36153;&#21147;&#30340;&#25163;&#21160;&#35774;&#35745;&#65292;&#35201;&#20040;&#23558;&#25552;&#31034;&#35843;&#20248;&#20316;&#20026;&#28857;&#20272;&#35745;&#38382;&#39064;&#36827;&#34892;&#20248;&#21270;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25551;&#36848;&#31867;&#21035;&#30340;&#22810;&#26679;&#29305;&#24449;&#24182;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#27010;&#29575;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#20174;&#28508;&#22312;&#20998;&#24067;&#20013;&#39318;&#20808;&#37319;&#26679;&#38544;&#21521;&#37327;&#65292;&#28982;&#21518;&#37319;&#29992;&#36731;&#37327;&#32423;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#26631;&#31614;&#29305;&#23450;&#30340;&#38543;&#26426;&#25552;&#31034;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#35270;&#35273;&#30693;&#35782;&#19982;&#22270;&#20687;&#30340;&#35821;&#20041;&#35268;&#21017;&#21270;&#65292;&#24182;&#23558;&#22270;&#20687;&#21644;&#30456;&#24212;&#30340;&#25552;&#31034;&#35270;&#20026;&#34917;&#19969;&#21644;&#20196;&#29260;&#38598;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23558;&#25552;&#31034;&#26631;&#35760;&#25512;&#21521;&#24544;&#23454;&#25429;&#25417;&#26631;&#31614;&#29305;&#23450;&#30340;&#35270;&#35273;&#27010;&#24565;&#65292;&#32780;&#19981;&#26159;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20449;&#24687;&#26469;&#29983;&#25104;&#26356;&#20855;&#20449;&#24687;&#37327;&#21644;&#20934;&#30830;&#24615;&#30340;&#25552;&#31034;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#34917;&#19969;-&#20196;&#29260;&#23545;&#40784;&#30340;&#36125;&#21494;&#26031;&#25552;&#31034;&#23398;&#20064;&#65288;PTBPL&#65289;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
For downstream applications of vision-language pre-trained models, there has been significant interest in constructing effective prompts. Existing works on prompt engineering, which either require laborious manual designs or optimize the prompt tuning as a point estimation problem, may fail to describe diverse characteristics of categories and limit their applications. We introduce a Bayesian probabilistic resolution to prompt learning, where the label-specific stochastic prompts are generated hierarchically by first sampling a latent vector from an underlying distribution and then employing a lightweight generative model. Importantly, we semantically regularize prompt learning with the visual knowledge and view images and the corresponding prompts as patch and token sets under optimal transport, which pushes the prompt tokens to faithfully capture the label-specific visual concepts, instead of overfitting the training categories. Moreover, the proposed model can also be straightforwar
&lt;/p&gt;</description></item></channel></rss>