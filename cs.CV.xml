<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>HARMamba&#21033;&#29992;&#26356;&#36731;&#37327;&#32423;&#30340;&#36873;&#25321;&#24615;SSM&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#26550;&#26500;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#36164;&#28304;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.20183</link><description>&lt;p&gt;
HARMamba: &#22522;&#20110;&#21452;&#21521;&#36873;&#25321;&#24615;SSM&#30340;&#39640;&#25928;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
HARMamba: Efficient Wearable Sensor Human Activity Recognition Based on Bidirectional Selective SSM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20183
&lt;/p&gt;
&lt;p&gt;
HARMamba&#21033;&#29992;&#26356;&#36731;&#37327;&#32423;&#30340;&#36873;&#25321;&#24615;SSM&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#26550;&#26500;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#36164;&#28304;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#27963;&#21160;&#24863;&#30693;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#30828;&#20214;&#24863;&#30693;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;Mamba&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#20986;&#29616;&#12290;HARMamba&#24341;&#20837;&#20102;&#26356;&#36731;&#37327;&#32423;&#30340;&#36873;&#25321;&#24615;SSM&#20316;&#20026;&#27963;&#21160;&#35782;&#21035;&#30340;&#22522;&#26412;&#27169;&#22411;&#26550;&#26500;&#65292;&#20197;&#35299;&#20915;&#31995;&#32479;&#35745;&#31639;&#36127;&#36733;&#21644;&#20869;&#23384;&#20351;&#29992;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20183v1 Announce Type: cross  Abstract: Wearable sensor human activity recognition (HAR) is a crucial area of research in activity sensing. While transformer-based temporal deep learning models have been extensively studied and implemented, their large number of parameters present significant challenges in terms of system computing load and memory usage, rendering them unsuitable for real-time mobile activity recognition applications. Recently, an efficient hardware-aware state space model (SSM) called Mamba has emerged as a promising alternative. Mamba demonstrates strong potential in long sequence modeling, boasts a simpler network architecture, and offers an efficient hardware-aware design. Leveraging SSM for activity recognition represents an appealing avenue for exploration. In this study, we introduce HARMamba, which employs a more lightweight selective SSM as the foundational model architecture for activity recognition. The goal is to address the computational resourc
&lt;/p&gt;</description></item><item><title>FOOL&#26159;&#19968;&#31181;OEC&#26412;&#22320;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#29305;&#24449;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21534;&#21520;&#37327;&#12289;&#23884;&#20837;&#19978;&#19979;&#25991;&#21644;&#21033;&#29992;&#29943;&#30742;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#38477;&#20302;&#20256;&#36755;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16677</link><description>&lt;p&gt;
FOOL: &#29992;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#35299;&#20915;&#21355;&#26143;&#35745;&#31639;&#20013;&#30340;&#19979;&#34892;&#29942;&#39048;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FOOL: Addressing the Downlink Bottleneck in Satellite Computing with Neural Feature Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16677
&lt;/p&gt;
&lt;p&gt;
FOOL&#26159;&#19968;&#31181;OEC&#26412;&#22320;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#29305;&#24449;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21534;&#21520;&#37327;&#12289;&#23884;&#20837;&#19978;&#19979;&#25991;&#21644;&#21033;&#29992;&#29943;&#30742;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#38477;&#20302;&#20256;&#36755;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#20256;&#24863;&#22120;&#30340;&#32435;&#21355;&#26143;&#26143;&#24231;&#25429;&#33719;&#22823;&#33539;&#22260;&#22320;&#29702;&#21306;&#22495;&#65292;&#20026;&#22320;&#29699;&#35266;&#27979;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#38543;&#30528;&#26143;&#24231;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#32593;&#32476;&#20105;&#29992;&#24418;&#25104;&#20102;&#19979;&#34892;&#29942;&#39048;&#12290;&#36712;&#36947;&#36793;&#32536;&#35745;&#31639;&#65288;OEC&#65289;&#21033;&#29992;&#26377;&#38480;&#30340;&#26426;&#36733;&#35745;&#31639;&#36164;&#28304;&#36890;&#36807;&#22312;&#28304;&#22836;&#22788;&#29702;&#21407;&#22987;&#25429;&#33719;&#26469;&#20943;&#23569;&#20256;&#36755;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20381;&#36182;&#31895;&#31961;&#30340;&#36807;&#28388;&#26041;&#27861;&#25110;&#36807;&#20998;&#20248;&#20808;&#32771;&#34385;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#65292;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26377;&#38480;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FOOL&#65292;&#19968;&#31181;OEC&#26412;&#22320;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#29305;&#24449;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20445;&#30041;&#39044;&#27979;&#24615;&#33021;&#12290;FOOL&#23558;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#20998;&#21306;&#65292;&#20197;&#26368;&#22823;&#21270;&#21534;&#21520;&#37327;&#12290;&#27492;&#22806;&#65292;&#23427;&#23884;&#20837;&#19978;&#19979;&#25991;&#24182;&#21033;&#29992;&#29943;&#30742;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#36739;&#20302;&#30340;&#24320;&#38144;&#38477;&#20302;&#20256;&#36755;&#25104;&#26412;&#12290;&#34429;&#28982;FOOL&#26159;&#19968;&#31181;&#29305;&#24449;&#21387;&#32553;&#22120;&#65292;&#20294;&#23427;&#21487;&#20197;&#22312;&#20302;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16677v1 Announce Type: new  Abstract: Nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for Earth observation. As constellation sizes increase, network contention poses a downlink bottleneck. Orbital Edge Computing (OEC) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source. However, current solutions have limited practicability due to reliance on crude filtering methods or over-prioritizing particular downstream tasks.   This work presents FOOL, an OEC-native and task-agnostic feature compression method that preserves prediction performance. FOOL partitions high-resolution satellite imagery to maximize throughput. Further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead. While FOOL is a feature compressor, it can recover images with competitive scores on perceptual quality measures at low
&lt;/p&gt;</description></item><item><title>P2LHAP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Patch-to-Label Seq2Seq&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#19968;&#20219;&#21153;&#27169;&#22411;&#20013;&#21516;&#26102;&#23454;&#29616;&#20154;&#31867;&#27963;&#21160;&#30340;&#20998;&#21106;&#12289;&#35782;&#21035;&#21644;&#39044;&#27979;</title><link>https://arxiv.org/abs/2403.08214</link><description>&lt;p&gt;
P2LHAP&#65306;&#22522;&#20110;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#12289;&#20998;&#21106;&#21644;&#39044;&#27979;&#30340;Patch-to-Label Seq2Seq Transformer
&lt;/p&gt;
&lt;p&gt;
P2LHAP:Wearable sensor-based human activity recognition, segmentation and forecast through Patch-to-Label Seq2Seq Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08214
&lt;/p&gt;
&lt;p&gt;
P2LHAP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Patch-to-Label Seq2Seq&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#19968;&#20219;&#21153;&#27169;&#22411;&#20013;&#21516;&#26102;&#23454;&#29616;&#20154;&#31867;&#27963;&#21160;&#30340;&#20998;&#21106;&#12289;&#35782;&#21035;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24456;&#38590;&#21516;&#26102;&#20174;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#20998;&#21106;&#12289;&#35782;&#21035;&#21644;&#39044;&#27979;&#20154;&#31867;&#27963;&#21160;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#36741;&#21161;&#29983;&#27963;&#31561;&#39046;&#22495;&#30340;&#23454;&#29992;&#24615;&#65292;&#32780;&#36825;&#20123;&#39046;&#22495;&#23545;&#20110;&#23454;&#26102;&#29702;&#35299;&#27491;&#22312;&#36827;&#34892;&#21644;&#21363;&#23558;&#21457;&#29983;&#30340;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;P2LHAP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;Patch-to-Label Seq2Seq&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#19968;&#20219;&#21153;&#27169;&#22411;&#20013;&#35299;&#20915;&#36825;&#19977;&#20010;&#20219;&#21153;&#12290;P2LHAP&#23558;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#21010;&#20998;&#20026;&#19968;&#31995;&#21015;&#8220;&#34917;&#19969;&#8221;&#65292;&#20316;&#20026;&#36755;&#20837;&#26631;&#35760;&#65292;&#24182;&#36755;&#20986;&#19968;&#31995;&#21015;&#21253;&#25324;&#39044;&#27979;&#30340;&#26410;&#26469;&#27963;&#21160;&#22312;&#20869;&#30340;&#34917;&#19969;&#32423;&#27963;&#21160;&#26631;&#31614;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21608;&#22260;&#34917;&#19969;&#26631;&#31614;&#30340;&#29420;&#29305;&#24179;&#28369;&#25216;&#26415;&#65292;&#21487;&#20934;&#30830;&#35782;&#21035;&#27963;&#21160;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;P2LHAP&#36890;&#36807;&#20256;&#24863;&#22120;&#20449;&#21495;&#36890;&#36947;&#29420;&#31435;&#30340;Transformer&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23398;&#20064;&#34917;&#19969;&#32423;&#34920;&#31034;&#12290;&#25152;&#26377;&#36890;&#36947;&#22312;&#25152;&#26377;&#24207;&#21015;&#19978;&#20849;&#20139;&#23884;&#20837;&#21644;Transformer&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08214v1 Announce Type: cross  Abstract: Traditional deep learning methods struggle to simultaneously segment, recognize, and forecast human activities from sensor data. This limits their usefulness in many fields such as healthcare and assisted living, where real-time understanding of ongoing and upcoming activities is crucial. This paper introduces P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles all three tasks in a efficient single-task model. P2LHAP divides sensor data streams into a sequence of "patches", served as input tokens, and outputs a sequence of patch-level activity labels including the predicted future activities. A unique smoothing technique based on surrounding patch labels, is proposed to identify activity boundaries accurately. Additionally, P2LHAP learns patch-level representation by sensor signal channel-independent Transformer encoders and decoders. All channels share embedding and Transformer weights across all sequences. Evaluated on thre
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CFRet-DVQA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#20013;&#23450;&#20301;&#20449;&#24687;&#21644;&#38480;&#21046;&#27169;&#22411;&#36755;&#20837;&#30340;&#38271;&#24230;&#31561;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#31572;&#26696;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00816</link><description>&lt;p&gt;
CFRet-DVQA&#65306;&#31895;&#21040;&#31934;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#29992;&#20110;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CFRet-DVQA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#20013;&#23450;&#20301;&#20449;&#24687;&#21644;&#38480;&#21046;&#27169;&#22411;&#36755;&#20837;&#30340;&#38271;&#24230;&#31561;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#31572;&#26696;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#65288;DVQA&#65289;&#26159;&#19968;&#20010;&#28041;&#21450;&#26681;&#25454;&#22270;&#20687;&#20869;&#23481;&#22238;&#31572;&#26597;&#35810;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#24037;&#20316;&#20165;&#38480;&#20110;&#23450;&#20301;&#21333;&#39029;&#20869;&#30340;&#20449;&#24687;&#65292;&#19981;&#25903;&#25345;&#36328;&#39029;&#38754;&#38382;&#31572;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#23545;&#27169;&#22411;&#36755;&#20837;&#30340;&#26631;&#35760;&#38271;&#24230;&#38480;&#21046;&#21487;&#33021;&#23548;&#33268;&#19982;&#31572;&#26696;&#30456;&#20851;&#30340;&#37096;&#20998;&#34987;&#25130;&#26029;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#23398;&#65292;&#31216;&#20026;CFRet-DVQA&#65292;&#37325;&#28857;&#25918;&#22312;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#19978;&#65292;&#20197;&#26377;&#25928;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#25991;&#26723;&#20013;&#26816;&#32034;&#19982;&#25152;&#25552;&#38382;&#39064;&#30456;&#20851;&#30340;&#22810;&#20010;&#29255;&#27573;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20808;&#36827;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;&#25991;&#26723;&#26631;&#31614;&#30340;&#39118;&#26684;&#30456;&#31526;&#12290;&#23454;&#39564;&#28436;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00816v1 Announce Type: cross  Abstract: Document Visual Question Answering (DVQA) is a task that involves responding to queries based on the content of images. Existing work is limited to locating information within a single page and does not facilitate cross-page question-and-answer interaction. Furthermore, the token length limitation imposed on inputs to the model may lead to truncation of segments pertinent to the answer. In this study, we introduce a simple but effective methodology called CFRet-DVQA, which focuses on retrieval and efficient tuning to address this critical issue effectively. For that, we initially retrieve multiple segments from the document that correlate with the question at hand. Subsequently, we leverage the advanced reasoning abilities of the large language model (LLM), further augmenting its performance through instruction tuning. This approach enables the generation of answers that align with the style of the document labels. The experiments demo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#35270;&#39057;&#39044;&#35757;&#32451;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#39118;&#38505;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25163;&#35821;&#32763;&#35793;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09611</link><description>&lt;p&gt;
&#23454;&#29616;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Towards Privacy-Aware Sign Language Translation at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#35270;&#39057;&#39044;&#35757;&#32451;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#39118;&#38505;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25163;&#35821;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#32763;&#35793;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#25968;&#25454;&#31232;&#32570;&#12290;&#30446;&#21069;&#22312;&#32593;&#32476;&#19978;&#21487;&#29992;&#30340;&#22823;&#37096;&#20998;&#25163;&#35821;&#25968;&#25454;&#30001;&#20110;&#32570;&#20047;&#23545;&#40784;&#30340;&#23383;&#24149;&#32780;&#26080;&#27861;&#29992;&#20110;&#35757;&#32451;&#30417;&#30563;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#32593;&#32476;&#25235;&#21462;&#30340;&#25968;&#25454;&#38598;&#26469;&#25193;&#23637;&#25163;&#35821;&#32763;&#35793;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;&#20854;&#20013;&#21253;&#21547;&#29983;&#29289;&#29305;&#24449;&#20449;&#24687;&#65292;&#36127;&#36131;&#20219;&#22320;&#24320;&#21457;&#25163;&#35821;&#32763;&#35793;&#25216;&#26415;&#24212;&#35813;&#32771;&#34385;&#36825;&#19968;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SSVP-SLT&#65292;&#23427;&#21033;&#29992;&#21311;&#21517;&#21644;&#26410;&#27880;&#37322;&#30340;&#35270;&#39057;&#36827;&#34892;&#33258;&#30417;&#30563;&#35270;&#39057;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#21033;&#29992;&#32463;&#36807;&#31579;&#36873;&#30340;&#24179;&#34892;&#25968;&#25454;&#38598;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#25163;&#35821;&#32763;&#35793;&#24494;&#35843;&#12290; SSVP-SLT&#22312;How2Sign&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24494;&#35843;&#21644;&#38646;&#27425;gloss-free&#25163;&#35821;&#32763;&#35793;&#24615;&#33021;&#65292;&#27604;&#26368;&#24378;&#30340;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;3&#20010;BLEU-4&#12290;&#36890;&#36807;&#21463;&#25511;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#35821;&#35328;&#21644;&#25163;&#35821;&#35789;&#27719;&#19978;&#37117;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09611v1 Announce Type: new  Abstract: A major impediment to the advancement of sign language translation (SLT) is data scarcity. Much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for. In this work, we propose a two-stage framework for privacy-aware SLT at scale that addresses both of these issues. We introduce SSVP-SLT, which leverages self-supervised video pretraining on anonymized and unannotated videos, followed by supervised SLT finetuning on a curated parallel dataset. SSVP-SLT achieves state-of-the-art finetuned and zero-shot gloss-free SLT performance on the How2Sign dataset, outperforming the strongest respective baselines by over 3 BLEU-4. Based on controlled experiments, we fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#19988;&#20855;&#26222;&#36866;&#24615;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#35843;&#31354;&#38388;&#21644;&#20809;&#35889;&#34920;&#31034;&#65292;&#24341;&#20837;&#20809;&#35889;&#30456;&#20851;&#31995;&#25968;&#30446;&#26631;&#26469;&#25552;&#39640;&#23545;&#20013;&#38454;&#29305;&#24449;&#21644;&#19978;&#19979;&#25991;&#38271;&#31243;&#20381;&#36182;&#30340;&#25429;&#25417;&#33021;&#21147;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10373</link><description>&lt;p&gt;
&#40065;&#26834;&#19988;&#20855;&#26222;&#36866;&#24615;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#31354;&#38388;&#21644;&#20809;&#35889;&#23398;&#20064;&#30340;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
Harmonized Spatial and Spectral Learning for Robust and Generalized Medical Image Segmentation. (arXiv:2401.10373v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#19988;&#20855;&#26222;&#36866;&#24615;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#35843;&#31354;&#38388;&#21644;&#20809;&#35889;&#34920;&#31034;&#65292;&#24341;&#20837;&#20809;&#35889;&#30456;&#20851;&#31995;&#25968;&#30446;&#26631;&#26469;&#25552;&#39640;&#23545;&#20013;&#38454;&#29305;&#24449;&#21644;&#19978;&#19979;&#25991;&#38271;&#31243;&#20381;&#36182;&#30340;&#25429;&#25417;&#33021;&#21147;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31867;&#20869;&#21464;&#24322;&#24615;&#21644;&#31867;&#38388;&#29420;&#31435;&#24615;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#21516;&#19968;&#31867;&#22312;&#19981;&#21516;&#26679;&#26412;&#20013;&#34920;&#29616;&#19981;&#21516;&#65292;&#38590;&#20197;&#25429;&#25417;&#19981;&#21516;&#23545;&#35937;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#39640;&#30340;&#38169;&#35823;&#36127;&#20363;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#35843;&#31354;&#38388;&#21644;&#20809;&#35889;&#34920;&#31034;&#26469;&#22686;&#24378;&#39046;&#22495;&#36890;&#29992;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21019;&#26032;&#30340;&#20809;&#35889;&#30456;&#20851;&#31995;&#25968;&#30446;&#26631;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#25429;&#25417;&#20013;&#38454;&#29305;&#24449;&#21644;&#19978;&#19979;&#25991;&#38271;&#31243;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#30446;&#26631;&#36890;&#36807;&#34701;&#20837;&#26377;&#20215;&#20540;&#30340;&#20809;&#35889;&#20449;&#24687;&#26469;&#34917;&#20805;&#20256;&#32479;&#30340;&#31354;&#38388;&#30446;&#26631;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#20248;&#21270;&#36825;&#20010;&#30446;&#26631;&#19982;&#29616;&#26377;&#30340;UNet&#21644;TransUNet&#26550;&#26500;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has demonstrated remarkable achievements in medical image segmentation. However, prevailing deep learning models struggle with poor generalization due to (i) intra-class variations, where the same class appears differently in different samples, and (ii) inter-class independence, resulting in difficulties capturing intricate relationships between distinct objects, leading to higher false negative cases. This paper presents a novel approach that synergies spatial and spectral representations to enhance domain-generalized medical image segmentation. We introduce the innovative Spectral Correlation Coefficient objective to improve the model's capacity to capture middle-order features and contextual long-range dependencies. This objective complements traditional spatial objectives by incorporating valuable spectral information. Extensive experiments reveal that optimizing this objective with existing architectures like UNet and TransUNet significantly enhances generalization, 
&lt;/p&gt;</description></item><item><title>Safe DreamerV3&#26159;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#32500;&#24230;&#21644;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#20960;&#20046;&#38646;&#25104;&#26412;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.07176</link><description>&lt;p&gt;
Safe DreamerV3&#65306;&#24102;&#26377;&#19990;&#30028;&#27169;&#22411;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe DreamerV3: Safe Reinforcement Learning with World Models. (arXiv:2307.07176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07176
&lt;/p&gt;
&lt;p&gt;
Safe DreamerV3&#26159;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#32500;&#24230;&#21644;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#20960;&#20046;&#38646;&#25104;&#26412;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#36824;&#27809;&#26377;&#23454;&#29616;, &#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#20854;&#26410;&#33021;&#28385;&#36275;&#36825;&#20123;&#31995;&#32479;&#30340;&#22522;&#26412;&#23433;&#20840;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#25104;&#26412;&#20989;&#25968;&#26469;&#22686;&#24378;&#23433;&#20840;&#24615;&#65292;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#65292;&#21253;&#25324;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#65292;&#21363;&#20351;&#36827;&#34892;&#20840;&#38754;&#30340;&#25968;&#25454;&#37319;&#26679;&#21644;&#35757;&#32451;&#65292;&#20063;&#26080;&#27861;&#23454;&#29616;&#38646;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Safe DreamerV3&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#22312;SafeRL&#20013;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#36827;&#27493;&#65292;&#26159;&#31532;&#19968;&#20010;&#22312;Safety-Gymnasium&#22522;&#20934;&#20013;&#23454;&#29616;&#36817;&#20046;&#38646;&#25104;&#26412;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#32593;&#31449;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#65306;https://sites.google.com/view/safedreamerv3&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread application of Reinforcement Learning (RL) in real-world situations is yet to come to fruition, largely as a result of its failure to satisfy the essential safety demands of such systems. Existing safe reinforcement learning (SafeRL) methods, employing cost functions to enhance safety, fail to achieve zero-cost in complex scenarios, including vision-only tasks, even with comprehensive data sampling and training. To address this, we introduce Safe DreamerV3, a novel algorithm that integrates both Lagrangian-based and planning-based methods within a world model. Our methodology represents a significant advancement in SafeRL as the first algorithm to achieve nearly zero-cost in both low-dimensional and vision-only tasks within the Safety-Gymnasium benchmark. Our project website can be found in: https://sites.google.com/view/safedreamerv3.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02694</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24230;&#37327;&#26041;&#27861;&#65306;&#19968;&#39033;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Loss Functions and Metrics in Deep Learning. A Review. (arXiv:2307.02694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#36873;&#25321;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#24230;&#37327;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#23427;&#20204;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#35780;&#35770;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#26368;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the essential components of deep learning is the choice of the loss function and performance metrics used to train and evaluate models. This paper reviews the most prevalent loss functions and performance measurements in deep learning. We examine the benefits and limits of each technique and illustrate their application to various deep-learning problems. Our review aims to give a comprehensive picture of the different loss functions and performance indicators used in the most common deep learning tasks and help practitioners choose the best method for their specific task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20266;&#24433;&#38477;&#22122;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#31232;&#30095;&#35270;&#22270;&#19979;&#33258;&#21160;&#20986;&#34880;&#26816;&#27979;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#19982;&#23436;&#20840;&#37319;&#26679;&#30340;&#22270;&#20687;&#36827;&#34892;&#21516;&#31561;&#31934;&#30830;&#24230;&#30340;&#20998;&#31867;&#21644;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.09340</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20266;&#24433;&#38477;&#22122;&#30340;&#31232;&#30095;&#35270;&#22270;CT&#22270;&#20687;&#33258;&#21160;&#20986;&#34880;&#26816;&#27979;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Automated Hemorrhage Detection in Sparse-view Computed Tomography via Deep Convolutional Neural Network based Artifact Reduction. (arXiv:2303.09340v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20266;&#24433;&#38477;&#22122;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#31232;&#30095;&#35270;&#22270;&#19979;&#33258;&#21160;&#20986;&#34880;&#26816;&#27979;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#19982;&#23436;&#20840;&#37319;&#26679;&#30340;&#22270;&#20687;&#36827;&#34892;&#21516;&#31561;&#31934;&#30830;&#24230;&#30340;&#20998;&#31867;&#21644;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39045;&#20869;&#20986;&#34880;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#38656;&#35201;&#24555;&#36895;&#19988;&#24120;&#24120;&#38750;&#24120;&#23494;&#38598;&#30340;&#21307;&#30103;&#27835;&#30103;&#12290;&#20026;&#20102;&#35786;&#26029;&#65292;&#36890;&#24120;&#35201;&#36827;&#34892;&#39045;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CCT&#65289;&#25195;&#25551;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36752;&#23556;&#24341;&#36215;&#30340;&#22686;&#21152;&#30340;&#20581;&#24247;&#39118;&#38505;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#38477;&#20302;&#36825;&#31181;&#28508;&#22312;&#39118;&#38505;&#30340;&#26368;&#37325;&#35201;&#31574;&#30053;&#26159;&#23613;&#21487;&#33021;&#20445;&#25345;&#36752;&#23556;&#21058;&#37327;&#20302;&#65292;&#24182;&#19982;&#35786;&#26029;&#20219;&#21153;&#19968;&#33268;&#12290; &#31232;&#30095;&#35270;&#22270;CT&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#25152;&#37319;&#38598;&#30340;&#35270;&#22270;&#24635;&#25968;&#65292;&#20174;&#32780;&#38477;&#20302;&#21058;&#37327;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#20294;&#20195;&#20215;&#26159;&#38477;&#20302;&#22270;&#20687;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;U-Net&#26550;&#26500;&#26469;&#20943;&#23569;&#31232;&#30095;&#35270;&#22270;CCT&#30340;&#20266;&#24433;&#65292;&#20174;&#31232;&#30095;&#35270;&#22270;&#20013;&#39044;&#27979;&#23436;&#20840;&#37319;&#26679;&#30340;&#37325;&#24314;&#22270;&#20687;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#20986;&#34880;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#23436;&#20840;&#37319;&#26679;&#30340;CCT&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20266;&#24433;&#38477;&#22122;&#21518;&#30340;CCT&#22270;&#20687;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#21644;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#19982;&#23436;&#20840;&#37319;&#26679;&#30340;CCT&#22270;&#20687;&#27809;&#26377;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intracranial hemorrhage poses a serious health problem requiring rapid and often intensive medical treatment. For diagnosis, a Cranial Computed Tomography (CCT) scan is usually performed. However, the increased health risk caused by radiation is a concern. The most important strategy to reduce this potential risk is to keep the radiation dose as low as possible and consistent with the diagnostic task. Sparse-view CT can be an effective strategy to reduce dose by reducing the total number of views acquired, albeit at the expense of image quality. In this work, we use a U-Net architecture to reduce artifacts from sparse-view CCTs, predicting fully sampled reconstructions from sparse-view ones. We evaluate the hemorrhage detectability in the predicted CCTs with a hemorrhage classification convolutional neural network, trained on fully sampled CCTs to detect and classify different sub-types of hemorrhages. Our results suggest that the automated classification and detection accuracy of hemo
&lt;/p&gt;</description></item></channel></rss>