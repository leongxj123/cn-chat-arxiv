<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VSTAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#26102;&#24207;&#25252;&#29702;&#65288;GTN&#65289;&#30340;&#27010;&#24565;&#65292;&#33258;&#21160;&#29983;&#25104;&#35270;&#39057;&#26775;&#27010;&#24182;&#25913;&#21892;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#29983;&#25104;&#26356;&#38271;&#12289;&#26356;&#21160;&#24577;&#30340;&#35270;&#39057;</title><link>https://arxiv.org/abs/2403.13501</link><description>&lt;p&gt;
VSTAR&#65306;&#29992;&#20110;&#29983;&#25104;&#38271;&#21160;&#24577;&#35270;&#39057;&#21512;&#25104;&#30340;&#26102;&#38388;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13501
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VSTAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#26102;&#24207;&#25252;&#29702;&#65288;GTN&#65289;&#30340;&#27010;&#24565;&#65292;&#33258;&#21160;&#29983;&#25104;&#35270;&#39057;&#26775;&#27010;&#24182;&#25913;&#21892;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#29983;&#25104;&#26356;&#38271;&#12289;&#26356;&#21160;&#24577;&#30340;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;T2V&#65289;&#21512;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20294;&#24320;&#28304;&#30340;T2V&#25193;&#25955;&#27169;&#22411;&#38590;&#20197;&#29983;&#25104;&#20855;&#26377;&#21160;&#24577;&#21464;&#21270;&#21644;&#19981;&#26029;&#36827;&#21270;&#20869;&#23481;&#30340;&#36739;&#38271;&#35270;&#39057;&#12290;&#23427;&#20204;&#24448;&#24448;&#21512;&#25104;&#20934;&#38745;&#24577;&#35270;&#39057;&#65292;&#24573;&#30053;&#20102;&#25991;&#26412;&#25552;&#31034;&#20013;&#28041;&#21450;&#30340;&#24517;&#35201;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#35270;&#35273;&#21464;&#21270;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#25193;&#23637;&#21040;&#23454;&#29616;&#26356;&#38271;&#12289;&#26356;&#21160;&#24577;&#30340;&#35270;&#39057;&#21512;&#25104;&#24448;&#24448;&#22312;&#35745;&#31639;&#19978;&#38590;&#20197;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29983;&#25104;&#26102;&#24207;&#25252;&#29702;&#65288;GTN&#65289;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21363;&#26102;&#25913;&#21464;&#29983;&#25104;&#36807;&#31243;&#65292;&#20197;&#25913;&#21892;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#25511;&#21046;&#65292;&#24182;&#23454;&#29616;&#29983;&#25104;&#26356;&#38271;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;GTN&#26041;&#27861;&#65292;&#21517;&#20026;VSTAR&#65292;&#23427;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#35201;&#32032;&#65306;1&#65289;&#35270;&#39057;&#26775;&#27010;&#25552;&#31034;&#65288;VSP&#65289;-&#22522;&#20110;&#21407;&#22987;&#21333;&#20010;&#25552;&#31034;&#33258;&#21160;&#29983;&#25104;&#35270;&#39057;&#26775;&#27010;&#65292;&#21033;&#29992;LLMs&#25552;&#20379;&#20934;&#30830;&#30340;&#25991;&#26412;&#25351;&#23548;&#65292;&#20197;&#23454;&#29616;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#31934;&#30830;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13501v1 Announce Type: cross  Abstract: Despite tremendous progress in the field of text-to-video (T2V) synthesis, open-sourced T2V diffusion models struggle to generate longer videos with dynamically varying and evolving content. They tend to synthesize quasi-static videos, ignoring the necessary visual change-over-time implied in the text prompt. At the same time, scaling these models to enable longer, more dynamic video synthesis often remains computationally intractable. To address this challenge, we introduce the concept of Generative Temporal Nursing (GTN), where we aim to alter the generative process on the fly during inference to improve control over the temporal dynamics and enable generation of longer videos. We propose a method for GTN, dubbed VSTAR, which consists of two key ingredients: 1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis based on the original single prompt leveraging LLMs, which gives accurate textual guidance to differe
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.12029</link><description>&lt;p&gt;
&#23545;&#40784;&#19982;&#25552;&#28860;&#65306;&#32479;&#19968;&#21644;&#25913;&#36827;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Align and Distill: Unifying and Improving Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12029
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#22120;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#20110;&#19982;&#20854;&#35757;&#32451;&#38598;&#19981;&#21516;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65288;DAOD&#65289;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#19978;&#30340;&#24378;&#22823;&#32467;&#26524;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#38519;&#38449;&#65292;&#36825;&#20123;&#38519;&#38449;&#23545;&#36807;&#21435;&#30340;&#32467;&#26524;&#25552;&#20986;&#36136;&#30097;&#24182;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#65306;&#65288;a&#65289;&#30001;&#20110;&#22522;&#32447;&#19981;&#36275;&#23548;&#33268;&#24615;&#33021;&#39640;&#20272;&#65292;&#65288;b&#65289;&#19981;&#19968;&#33268;&#30340;&#23454;&#29616;&#23454;&#36341;&#38459;&#27490;&#20102;&#26041;&#27861;&#30340;&#36879;&#26126;&#27604;&#36739;&#65292;&#65288;c&#65289;&#30001;&#20110;&#36807;&#26102;&#30340;&#39592;&#24178;&#21644;&#22522;&#20934;&#27979;&#35797;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#23548;&#33268;&#32570;&#20047;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20197;&#19979;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;&#65292;Align and Distill&#65288;ALDI&#65289;&#65292;&#25903;&#25345;DAOD&#26041;&#27861;&#30340;&#27604;&#36739;&#24182;&#25903;&#25345;&#26410;&#26469;&#21457;&#23637;&#65292;&#65288;2&#65289;&#19968;&#20010;&#20844;&#24179;&#19988;&#29616;&#20195;&#30340;DAOD&#35757;&#32451;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#35299;&#20915;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#38519;&#38449;&#65292;&#65288;3&#65289;&#19968;&#20010;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;CFC-DAOD&#65292;&#33021;&#22815;&#22312;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12029v1 Announce Type: cross  Abstract: Object detectors often perform poorly on data that differs from their training set. Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge. Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks. We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#20110;&#20266;&#26631;&#31614;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#30340;&#20840;&#38754;&#19988;&#26377;&#32452;&#32455;&#30340;&#27010;&#36848;&#65292;&#25506;&#35752;&#20102;&#20266;&#26631;&#31614;&#25216;&#26415;&#22312;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;&#20855;&#20307;&#26041;&#27861;&#65292;&#36824;&#30740;&#31350;&#20102;&#20854;&#22312;&#21307;&#23398;&#21644;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.01909</link><description>&lt;p&gt;
&#22522;&#20110;&#20266;&#26631;&#31614;&#30340;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01909
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#20110;&#20266;&#26631;&#31614;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#30340;&#20840;&#38754;&#19988;&#26377;&#32452;&#32455;&#30340;&#27010;&#36848;&#65292;&#25506;&#35752;&#20102;&#20266;&#26631;&#31614;&#25216;&#26415;&#22312;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;&#20855;&#20307;&#26041;&#27861;&#65292;&#36824;&#30740;&#31350;&#20102;&#20854;&#22312;&#21307;&#23398;&#21644;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#28909;&#38376;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20391;&#37325;&#20110;&#22522;&#20110;&#35821;&#20041;&#23545;&#22270;&#20687;&#20013;&#30340;&#20687;&#32032;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#36880;&#20687;&#32032;&#26631;&#35760;&#22270;&#20687;&#30340;&#36807;&#31243;&#32791;&#26102;&#19988;&#32321;&#29712;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#20013;&#20266;&#26631;&#31614;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#30340;&#39318;&#27425;&#32508;&#21512;&#21644;&#26377;&#32452;&#32455;&#30340;&#27010;&#36848;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#35282;&#24230;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#39046;&#22495;&#30340;&#20855;&#20307;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20266;&#26631;&#31614;&#25216;&#26415;&#22312;&#21307;&#23398;&#21644;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#34892;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01909v1 Announce Type: cross  Abstract: Semantic segmentation is an important and popular research area in computer vision that focuses on classifying pixels in an image based on their semantics. However, supervised deep learning requires large amounts of data to train models and the process of labeling images pixel by pixel is time-consuming and laborious. This review aims to provide a first comprehensive and organized overview of the state-of-the-art research results on pseudo-label methods in the field of semi-supervised semantic segmentation, which we categorize from different perspectives and present specific methods for specific application areas. In addition, we explore the application of pseudo-label technology in medical and remote-sensing image segmentation. Finally, we also propose some feasible future research directions to address the existing challenges.
&lt;/p&gt;</description></item><item><title>StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.17042</link><description>&lt;p&gt;
StochGradAdam: &#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling. (arXiv:2310.17042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17042
&lt;/p&gt;
&lt;p&gt;
StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#39046;&#22495;&#20013;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;StochGradAdam&#20248;&#21270;&#22120;&#65292;&#36825;&#26159;&#23545;&#24191;&#21463;&#36190;&#35465;&#30340;Adam&#31639;&#27861;&#30340;&#26032;&#39062;&#25913;&#36827;&#12290;StochGradAdam&#30340;&#26680;&#24515;&#26159;&#20854;&#26799;&#24230;&#25277;&#26679;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#30830;&#20445;&#31283;&#23450;&#25910;&#25947;&#65292;&#32780;&#19988;&#21033;&#29992;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#20943;&#36731;&#22122;&#22768;&#25110;&#24322;&#24120;&#25968;&#25454;&#30340;&#24433;&#21709;&#21644;&#22686;&#24378;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#25552;&#21319;&#20102;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;StochGradAdam&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;Adam&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#31934;&#24515;&#36873;&#25321;&#19968;&#37096;&#20998;&#26799;&#24230;&#36827;&#34892;&#25277;&#26679;&#65292;&#35813;&#20248;&#21270;&#22120;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22797;&#26434;&#27169;&#22411;&#30340;&#31649;&#29702;&#12290;&#26412;&#25991;&#20174;&#25968;&#23398;&#22522;&#30784;&#21040;&#20559;&#24046;&#26657;&#27491;&#31574;&#30053;&#20840;&#38754;&#25506;&#35752;&#20102;StochGradAdam&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#25216;&#26415;&#30340;&#21487;&#26399;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing domain of deep learning optimization, this paper unveils the StochGradAdam optimizer, a novel adaptation of the well-regarded Adam algorithm. Central to StochGradAdam is its gradient sampling technique. This method not only ensures stable convergence but also leverages the advantages of selective gradient consideration, fostering robust training by potentially mitigating the effects of noisy or outlier data and enhancing the exploration of the loss landscape for more dependable convergence. In both image classification and segmentation tasks, StochGradAdam has demonstrated superior performance compared to the traditional Adam optimizer. By judiciously sampling a subset of gradients at each iteration, the optimizer is optimized for managing intricate models. The paper provides a comprehensive exploration of StochGradAdam's methodology, from its mathematical foundations to bias correction strategies, heralding a promising advancement in deep learning training tec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;-&#31526;&#21495;&#36807;&#28193;&#23383;&#20856;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#31526;&#21495;&#24605;&#32500;&#36827;&#34892;&#32467;&#21512;&#12290;&#36890;&#36807;&#23398;&#20064;&#36807;&#28193;&#34920;&#31034;&#65292;&#24182;&#33258;&#30417;&#30563;&#22320;&#21457;&#29616;&#38544;&#21547;&#30340;&#35859;&#35789;&#32467;&#26500;&#65292;&#20197;&#21450;&#36890;&#36807;&#21338;&#24328;&#21644;&#24378;&#21270;&#23398;&#20064;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#23545;&#39640;&#32500;&#20449;&#24687;&#30340;&#21387;&#32553;&#21644;&#31526;&#21495;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2308.02000</link><description>&lt;p&gt;
&#20174;&#31070;&#32463;&#34920;&#31034;&#21040;&#31526;&#21495;&#30693;&#35782;&#30340;&#36807;&#28193;
&lt;/p&gt;
&lt;p&gt;
On the Transition from Neural Representation to Symbolic Knowledge. (arXiv:2308.02000v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02000
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;-&#31526;&#21495;&#36807;&#28193;&#23383;&#20856;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#31526;&#21495;&#24605;&#32500;&#36827;&#34892;&#32467;&#21512;&#12290;&#36890;&#36807;&#23398;&#20064;&#36807;&#28193;&#34920;&#31034;&#65292;&#24182;&#33258;&#30417;&#30563;&#22320;&#21457;&#29616;&#38544;&#21547;&#30340;&#35859;&#35789;&#32467;&#26500;&#65292;&#20197;&#21450;&#36890;&#36807;&#21338;&#24328;&#21644;&#24378;&#21270;&#23398;&#20064;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#23545;&#39640;&#32500;&#20449;&#24687;&#30340;&#21387;&#32553;&#21644;&#31526;&#21495;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24357;&#21512;&#31070;&#32463;&#34920;&#31034;&#19982;&#31526;&#21495;&#34920;&#31034;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#36317;&#21487;&#33021;&#20351;&#31526;&#21495;&#24605;&#32500;&#20174;&#26412;&#36136;&#19978;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#12290;&#21463;&#20154;&#31867;&#22914;&#20309;&#36880;&#28176;&#20174;&#36890;&#36807;&#30693;&#35273;&#21644;&#29615;&#22659;&#20132;&#20114;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#31526;&#21495;&#26500;&#24314;&#22797;&#26434;&#30340;&#31526;&#21495;&#34920;&#31034;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;-&#31526;&#21495;&#36807;&#28193;&#23383;&#20856;&#23398;&#20064;&#65288;TDL&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;EM&#31639;&#27861;&#23398;&#20064;&#25968;&#25454;&#30340;&#36807;&#28193;&#34920;&#31034;&#65292;&#23558;&#36755;&#20837;&#30340;&#39640;&#32500;&#35270;&#35273;&#37096;&#20998;&#20449;&#24687;&#21387;&#32553;&#21040;&#19968;&#32452;&#24352;&#37327;&#20316;&#20026;&#31070;&#32463;&#21464;&#37327;&#65292;&#24182;&#33258;&#30417;&#30563;&#22320;&#21457;&#29616;&#38544;&#21547;&#30340;&#35859;&#35789;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#36755;&#20837;&#20998;&#35299;&#35270;&#20026;&#21512;&#20316;&#21338;&#24328;&#26469;&#23454;&#29616;&#26694;&#26550;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#35859;&#35789;&#65292;&#24182;&#36890;&#36807;RL&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#36827;&#19968;&#27493;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#65292;&#20197;&#34701;&#20837;&#20027;&#35266;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bridging the huge disparity between neural and symbolic representation can potentially enable the incorporation of symbolic thinking into neural networks from essence. Motivated by how human gradually builds complex symbolic representation from the prototype symbols that are learned through perception and environmental interactions. We propose a Neural-Symbolic Transitional Dictionary Learning (TDL) framework that employs an EM algorithm to learn a transitional representation of data that compresses high-dimension information of visual parts of an input into a set of tensors as neural variables and discover the implicit predicate structure in a self-supervised way. We implement the framework with a diffusion model by regarding the decomposition of input as a cooperative game, then learn predicates by prototype clustering. We additionally use RL enabled by the Markovian of diffusion models to further tune the learned prototypes by incorporating subjective factors. Extensive experiments 
&lt;/p&gt;</description></item></channel></rss>