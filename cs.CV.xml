<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;CLIP&#29992;&#20110;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21453;&#21367;&#31215;&#35299;&#30721;&#22120;&#21644;&#21487;&#23398;&#20064;&#23884;&#20837;&#30697;&#38453;&#65292;&#20351;&#24471;CLIP&#33021;&#22815;&#29702;&#35299;&#28145;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#28145;&#24230;&#20272;&#35745;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#24182;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03251</link><description>&lt;p&gt;
CLIP&#21487;&#20197;&#29702;&#35299;&#28145;&#24230;
&lt;/p&gt;
&lt;p&gt;
CLIP Can Understand Depth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;CLIP&#29992;&#20110;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21453;&#21367;&#31215;&#35299;&#30721;&#22120;&#21644;&#21487;&#23398;&#20064;&#23884;&#20837;&#30697;&#38453;&#65292;&#20351;&#24471;CLIP&#33021;&#22815;&#29702;&#35299;&#28145;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#28145;&#24230;&#20272;&#35745;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#24182;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#23558;CLIP&#25512;&#24191;&#21040;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#32593;&#32476;&#29228;&#21462;&#30340;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;CLIP&#22312;&#22270;&#20687;&#22359;&#21644;&#19982;&#28145;&#24230;&#30456;&#20851;&#30340;&#25552;&#31034;&#20043;&#38388;&#24471;&#21040;&#36866;&#24403;&#30456;&#20284;&#24615;&#26159;&#20302;&#25928;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36866;&#24212;CLIP&#29992;&#20110;&#26377;&#24847;&#20041;&#30340;&#23494;&#38598;&#39044;&#27979;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#20854;&#21407;&#22987;&#30340;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#12290;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#19968;&#20010;&#32039;&#20945;&#30340;&#21453;&#21367;&#31215;&#35299;&#30721;&#22120;&#21644;&#19968;&#20010;&#21517;&#20026;mirror&#30340;&#23567;&#22411;&#21487;&#23398;&#20064;&#23884;&#20837;&#30697;&#38453;&#20316;&#20026;&#20854;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#38745;&#24577;&#25552;&#31034;&#65292;CLIP&#33021;&#22815;&#29702;&#35299;&#28145;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;NYU Depth v2&#21644;KITTI&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#19982;&#20960;&#20010;&#20808;&#21069;&#30340;&#20165;&#35270;&#35273;&#27169;&#22411;&#30456;&#21305;&#37197;&#65292;&#32780;&#19988;&#32988;&#36807;&#20102;&#27599;&#20010;&#22522;&#20110;CLIP&#30340;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;&#20851;&#20110;&#26102;&#38388;&#28145;&#24230;&#19968;&#33268;&#24615;&#21644;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;CLIP&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#26102;&#28382;&#30740;&#31350;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on generalizing CLIP for monocular depth estimation reveal that CLIP pre-trained on web-crawled data is inefficient for deriving proper similarities between image patches and depth-related prompts. In this paper, we adapt CLIP for meaningful quality of monocular depth estimation with dense prediction, without fine-tuning its original vision-language alignment. By jointly training a compact deconvolutional decoder with a tiny learnable embedding matrix named mirror, as a static prompt for its text encoder, CLIP is enabled to understand depth. With this approach, our model exhibits impressive performance matching several previous state-of-the-art vision-only models on the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth estimation model with a large margin. Experiments on temporal depth consistency and spatial continuity demonstrate that the prior knowledge of CLIP can be effectively refined by our proposed framework. Furthermore, an ablation study on 
&lt;/p&gt;</description></item></channel></rss>