<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>LaRE^2 &#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#65288;LaRE&#65289;&#21644;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#27169;&#22359;&#65288;EGRE&#65289;&#23454;&#29616;&#20102;&#23545;&#29305;&#24449;&#30340;&#26377;&#25928;&#25552;&#21462;&#21644;&#22686;&#24378;&#65292;&#20174;&#32780;&#21306;&#20998;&#30495;&#23454;&#21644;&#29983;&#25104;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.17465</link><description>&lt;p&gt;
LaRE^2: &#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#30340;&#25193;&#25955;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17465
&lt;/p&gt;
&lt;p&gt;
LaRE^2 &#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#65288;LaRE&#65289;&#21644;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#27169;&#22359;&#65288;EGRE&#65289;&#23454;&#29616;&#20102;&#23545;&#29305;&#24449;&#30340;&#26377;&#25928;&#25552;&#21462;&#21644;&#22686;&#24378;&#65292;&#20174;&#32780;&#21306;&#20998;&#30495;&#23454;&#21644;&#29983;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17465v1 &#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#20351;&#30495;&#23454;&#22270;&#20687;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30340;&#21306;&#20998;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#23613;&#31649;&#36825;&#19968;&#36827;&#23637;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#26041;&#27861;&#65288;LaRE^2&#65289;&#26469;&#26816;&#27979;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#65288;LaRE&#65289;&#65292;&#20316;&#20026;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#30340;&#31532;&#19968;&#20010;&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#30340;&#29305;&#24449;&#12290;LaRE&#22312;&#29305;&#24449;&#25552;&#21462;&#25928;&#29575;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21306;&#20998;&#30495;&#20551;&#25152;&#38656;&#30340;&#20851;&#38190;&#32447;&#32034;&#12290;&#20026;&#20102;&#21033;&#29992;LaRE&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#27169;&#22359;&#65288;EGRE&#65289;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;LaRE&#24341;&#23548;&#30340;&#26041;&#24335;&#32454;&#21270;&#22270;&#20687;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17465v1 Announce Type: cross  Abstract: The evolution of Diffusion Models has dramatically improved image generation quality, making it increasingly difficult to differentiate between real and generated images. This development, while impressive, also raises significant privacy and security concerns. In response to this, we propose a novel Latent REconstruction error guided feature REfinement method (LaRE^2) for detecting the diffusion-generated images. We come up with the Latent Reconstruction Error (LaRE), the first reconstruction-error based feature in the latent space for generated image detection. LaRE surpasses existing methods in terms of feature extraction efficiency while preserving crucial cues required to differentiate between the real and the fake. To exploit LaRE, we propose an Error-Guided feature REfinement module (EGRE), which can refine the image feature guided by LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an align-then-refine m
&lt;/p&gt;</description></item><item><title>MIM-Refiner&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;MIM&#27169;&#22411;&#20013;&#30340;&#20013;&#38388;&#23618;&#34920;&#31034;&#21644;&#22810;&#20010;&#23545;&#27604;&#22836;&#65292;&#33021;&#22815;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#65292;&#24182;&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10093</link><description>&lt;p&gt;
MIM-Refiner&#65306;&#19968;&#31181;&#20174;&#20013;&#38388;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#33719;&#24471;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10093
&lt;/p&gt;
&lt;p&gt;
MIM-Refiner&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;MIM&#27169;&#22411;&#20013;&#30340;&#20013;&#38388;&#23618;&#34920;&#31034;&#21644;&#22810;&#20010;&#23545;&#27604;&#22836;&#65292;&#33021;&#22815;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#65292;&#24182;&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;MIM-Refiner&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#35757;&#32451;MIM&#27169;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#12290;MIM-Refiner&#30340;&#21160;&#26426;&#22312;&#20110;MIM&#27169;&#22411;&#20013;&#30340;&#26368;&#20339;&#34920;&#31034;&#36890;&#24120;&#20301;&#20110;&#20013;&#38388;&#23618;&#12290;&#22240;&#27492;&#65292;MIM-Refiner&#21033;&#29992;&#36830;&#25509;&#21040;&#19981;&#21516;&#20013;&#38388;&#23618;&#30340;&#22810;&#20010;&#23545;&#27604;&#22836;&#12290;&#22312;&#27599;&#20010;&#22836;&#20013;&#65292;&#20462;&#25913;&#21518;&#30340;&#26368;&#36817;&#37051;&#30446;&#26631;&#24110;&#21161;&#26500;&#24314;&#30456;&#24212;&#30340;&#35821;&#20041;&#32858;&#31867;&#12290;&#27492;&#36807;&#31243;&#30701;&#32780;&#26377;&#25928;&#65292;&#22312;&#20960;&#20010;epochs&#20869;&#65292;&#25105;&#20204;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#12290;&#20351;&#29992;data2vec 2.0&#22312;ImageNet-1K&#19978;&#39044;&#35757;&#32451;&#30340;ViT-H&#32463;&#36807;&#25913;&#36827;&#21518;&#65292;&#22312;&#32447;&#24615;&#25506;&#27979;&#21644;&#20302;&#26679;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65288;&#20998;&#21035;&#20026;84.7%&#21644;64.2%&#65289;&#65292;&#36229;&#36807;&#20102;&#22312;ImageNet-1K&#19978;&#39044;&#35757;&#32451;&#30340;&#20854;&#20182;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10093v1 Announce Type: cross  Abstract: We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted in the insight that optimal representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are connected to diverse intermediate layers. In each head, a modified nearest neighbor objective helps to construct respective semantic clusters.   The refinement process is short but effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, achieves new state-of-the-art results in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K 1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%, outperforming larger mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26059;&#36716;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#30340;SO(3)&#31283;&#20581;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#65292;&#23545;&#24322;&#24120;&#20540;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#20197;&#23481;&#24525;&#19981;&#23436;&#32654;&#30340;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.10465</link><description>&lt;p&gt;
&#22522;&#20110;&#26059;&#36716;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#30340;SO(3)&#31283;&#20581;&#27010;&#29575;&#24314;&#27169;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Probabilistic Modeling on SO(3) via Rotation Laplace Distribution. (arXiv:2305.10465v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26059;&#36716;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#30340;SO(3)&#31283;&#20581;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#65292;&#23545;&#24322;&#24120;&#20540;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#20197;&#23481;&#24525;&#19981;&#23436;&#32654;&#30340;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21333;&#24352;RGB&#22270;&#20687;&#20272;&#35745;&#19977;&#32500;&#33258;&#30001;&#26059;&#36716;&#26159;&#19968;&#39033;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#27010;&#29575;&#26059;&#36716;&#24314;&#27169;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#21333;&#39044;&#27979;&#26059;&#36716;&#22238;&#24402;&#21487;&#20197;&#39069;&#22806;&#25552;&#20379;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#12290;&#23545;&#20110;SO(3)&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#24314;&#27169;&#65292;&#20351;&#29992;&#31867;&#20284;&#20110;&#39640;&#26031;&#30340;Bingham&#20998;&#24067;&#21644;&#30697;&#38453;Fisher&#20998;&#24067;&#26159;&#33258;&#28982;&#30340;&#65292;&#20294;&#26159;&#23427;&#20204;&#23545;&#24322;&#24120;&#39044;&#27979;&#24456;&#25935;&#24863;&#65292;&#20363;&#22914;180&#24230;&#35823;&#24046;&#65292;&#22240;&#27492;&#19981;&#22826;&#21487;&#33021;&#20197;&#26368;&#20339;&#24615;&#33021;&#25910;&#25947;&#12290;&#26412;&#25991;&#20174;&#22810;&#20803;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SO(3)&#26059;&#36716;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26059;&#36716;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#23545;&#24322;&#24120;&#20540;&#30340;&#24178;&#25200;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#24378;&#21046;&#26045;&#21152;&#26799;&#24230;&#21040;&#20302;&#35823;&#24046;&#21306;&#22495;&#65292;&#20197;&#25913;&#36827;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#23567;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22240;&#27492;&#21487;&#20197;&#23481;&#24525;&#19981;&#23436;&#32654;&#30340;&#27880;&#37322;&#12290;&#21033;&#29992;&#36825;&#20010;&#20248;&#21183;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21322;&#30417;&#30563;&#22238;&#24402;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the 3DoF rotation from a single RGB image is an important yet challenging problem. As a popular approach, probabilistic rotation modeling additionally carries prediction uncertainty information, compared to single-prediction rotation regression. For modeling probabilistic distribution over SO(3), it is natural to use Gaussian-like Bingham distribution and matrix Fisher, however they are shown to be sensitive to outlier predictions, e.g. $180^\circ$ error and thus are unlikely to converge with optimal performance. In this paper, we draw inspiration from multivariate Laplace distribution and propose a novel rotation Laplace distribution on SO(3). Our rotation Laplace distribution is robust to the disturbance of outliers and enforces much gradient to the low-error region that it can improve. In addition, we show that our method also exhibits robustness to small noises and thus tolerates imperfect annotations. With this benefit, we demonstrate its advantages in semi-supervised r
&lt;/p&gt;</description></item></channel></rss>