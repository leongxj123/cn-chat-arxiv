<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>D-STGCNT&#26159;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;STGCN&#21644;transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#24739;&#32773;&#36523;&#20307;&#24247;&#22797;&#38203;&#28860;&#12290;&#23427;&#36890;&#36807;&#23558;&#39592;&#26550;&#25968;&#25454;&#35270;&#20026;&#22270;&#24418;&#65292;&#24182;&#26816;&#27979;&#20851;&#38190;&#20851;&#33410;&#65292;&#22312;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23494;&#38598;&#36830;&#25509;&#21644;GRU&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#22411;3D&#39592;&#26550;&#36755;&#20837;&#65292;&#26377;&#25928;&#24314;&#31435;&#26102;&#31354;&#21160;&#24577;&#27169;&#22411;&#12290;transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#20110;&#35780;&#20272;&#24247;&#22797;&#38203;&#28860;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.06150</link><description>&lt;p&gt;
D-STGCNT:&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#23494;&#38598;&#26102;&#31354;&#22270;&#21367;&#31215;GRU&#32593;&#32476;&#29992;&#20110;&#35780;&#20272;&#24739;&#32773;&#36523;&#20307;&#24247;&#22797;
&lt;/p&gt;
&lt;p&gt;
D-STGCNT: A Dense Spatio-Temporal Graph Conv-GRU Network based on transformer for assessment of patient physical rehabilitation. (arXiv:2401.06150v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06150
&lt;/p&gt;
&lt;p&gt;
D-STGCNT&#26159;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;STGCN&#21644;transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#24739;&#32773;&#36523;&#20307;&#24247;&#22797;&#38203;&#28860;&#12290;&#23427;&#36890;&#36807;&#23558;&#39592;&#26550;&#25968;&#25454;&#35270;&#20026;&#22270;&#24418;&#65292;&#24182;&#26816;&#27979;&#20851;&#38190;&#20851;&#33410;&#65292;&#22312;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23494;&#38598;&#36830;&#25509;&#21644;GRU&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#22411;3D&#39592;&#26550;&#36755;&#20837;&#65292;&#26377;&#25928;&#24314;&#31435;&#26102;&#31354;&#21160;&#24577;&#27169;&#22411;&#12290;transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#20110;&#35780;&#20272;&#24247;&#22797;&#38203;&#28860;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#33258;&#21160;&#35780;&#20272;&#26080;&#20020;&#24202;&#30417;&#30563;&#24773;&#20917;&#19979;&#24739;&#32773;&#36827;&#34892;&#36523;&#20307;&#24247;&#22797;&#38203;&#28860;&#30340;&#25361;&#25112;&#12290;&#20854;&#30446;&#26631;&#26159;&#25552;&#20379;&#36136;&#37327;&#35780;&#20998;&#20197;&#30830;&#20445;&#27491;&#30830;&#25191;&#34892;&#21644;&#33719;&#24471;&#26399;&#26395;&#32467;&#26524;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;Dense Spatio-Temporal Graph Conv-GRU Network with Transformer&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#25913;&#36827;&#30340;STGCN&#21644;transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#25928;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#39592;&#26550;&#25968;&#25454;&#35270;&#20026;&#22270;&#24418;&#65292;&#24182;&#26816;&#27979;&#27599;&#20010;&#24247;&#22797;&#38203;&#28860;&#20013;&#36215;&#20027;&#35201;&#20316;&#29992;&#30340;&#20851;&#33410;&#12290;&#23494;&#38598;&#36830;&#25509;&#21644;GRU&#26426;&#21046;&#29992;&#20110;&#24555;&#36895;&#22788;&#29702;&#22823;&#22411;3D&#39592;&#26550;&#36755;&#20837;&#24182;&#26377;&#25928;&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#12290;transformer&#32534;&#30721;&#22120;&#30340;&#27880;&#24847;&#26426;&#21046;&#20391;&#37325;&#20110;&#36755;&#20837;&#24207;&#21015;&#30340;&#30456;&#20851;&#37096;&#20998;&#65292;&#20351;&#20854;&#22312;&#35780;&#20272;&#24247;&#22797;&#38203;&#28860;&#26041;&#38754;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper tackles the challenge of automatically assessing physical rehabilitation exercises for patients who perform the exercises without clinician supervision. The objective is to provide a quality score to ensure correct performance and achieve desired results. To achieve this goal, a new graph-based model, the Dense Spatio-Temporal Graph Conv-GRU Network with Transformer, is introduced. This model combines a modified version of STGCN and transformer architectures for efficient handling of spatio-temporal data. The key idea is to consider skeleton data respecting its non-linear structure as a graph and detecting joints playing the main role in each rehabilitation exercise. Dense connections and GRU mechanisms are used to rapidly process large 3D skeleton inputs and effectively model temporal dynamics. The transformer encoder's attention mechanism focuses on relevant parts of the input sequence, making it useful for evaluating rehabilitation exercises. The evaluation of our propose
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#24443;&#24213;&#30340;&#39044;&#22788;&#29702;&#21644;&#29305;&#24449;&#36873;&#25321;&#65292;&#25104;&#21151;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#20914;&#20987;&#22368;&#33853;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.05407</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#29305;&#24449;&#25490;&#24207;&#22312;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#20914;&#20987;&#22368;&#33853;&#26816;&#27979;&#20107;&#20214;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Feature Ranking for Impact Fall Detection Event Using Multisensor Data. (arXiv:2401.05407v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#24443;&#24213;&#30340;&#39044;&#22788;&#29702;&#21644;&#29305;&#24449;&#36873;&#25321;&#65292;&#25104;&#21151;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#20914;&#20987;&#22368;&#33853;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20154;&#30340;&#36300;&#20498;&#65292;&#29305;&#21035;&#26159;&#32769;&#24180;&#20154;&#65292;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#20260;&#23475;&#21644;&#24182;&#21457;&#30151;&#12290;&#22312;&#36300;&#20498;&#20107;&#20214;&#20013;&#26816;&#27979;&#20914;&#20987;&#30636;&#38388;&#23545;&#20110;&#21450;&#26102;&#25552;&#20379;&#24110;&#21161;&#21644;&#20943;&#23569;&#36127;&#38754;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#38598;&#24212;&#29992;&#24443;&#24213;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#30446;&#30340;&#26159;&#28040;&#38500;&#22122;&#38899;&#24182;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#26469;&#35782;&#21035;&#22810;&#20256;&#24863;&#22120;UP-FALL&#25968;&#25454;&#38598;&#20013;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#32467;&#26524;&#25968;&#25454;&#20449;&#24687;&#35780;&#20272;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26816;&#27979;&#20914;&#20987;&#30636;&#38388;&#26041;&#38754;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#20914;&#20987;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#23637;&#31034;&#20102;&#21033;&#29992;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Falls among individuals, especially the elderly population, can lead to serious injuries and complications. Detecting impact moments within a fall event is crucial for providing timely assistance and minimizing the negative consequences. In this work, we aim to address this challenge by applying thorough preprocessing techniques to the multisensor dataset, the goal is to eliminate noise and improve data quality. Furthermore, we employ a feature selection process to identify the most relevant features derived from the multisensor UP-FALL dataset, which in turn will enhance the performance and efficiency of machine learning models. We then evaluate the efficiency of various machine learning models in detecting the impact moment using the resulting data information from multiple sensors. Through extensive experimentation, we assess the accuracy of our approach using various evaluation metrics. Our results achieve high accuracy rates in impact detection, showcasing the power of leveraging 
&lt;/p&gt;</description></item></channel></rss>