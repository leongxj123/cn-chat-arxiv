<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#21435;&#22122;&#20316;&#20026;&#38024;&#23545;&#24178;&#20928;&#26631;&#31614;&#20013;&#27602;&#30340;&#35748;&#35777;&#38450;&#24481;&#65292;&#33021;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#21040;0-16%&#65292;&#21516;&#26102;&#20960;&#20046;&#19981;&#24433;&#21709;&#27979;&#35797;&#20934;&#30830;&#24615;&#65292;&#20026;&#26410;&#26469;&#24320;&#21457;&#26356;&#24378;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#21644;&#21033;&#29992;&#35813;&#38450;&#24481;&#25514;&#26045;&#20316;&#20026;&#24378;&#26377;&#21147;&#22522;&#30784;&#25552;&#20379;&#20102;&#37325;&#35201;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.11981</link><description>&lt;p&gt;
&#25193;&#25955;&#21435;&#22122;&#20316;&#20026;&#19968;&#31181;&#38024;&#23545;&#24178;&#20928;&#26631;&#31614;&#20013;&#27602;&#30340;&#35748;&#35777;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Diffusion Denoising as a Certified Defense against Clean-label Poisoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11981
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#21435;&#22122;&#20316;&#20026;&#38024;&#23545;&#24178;&#20928;&#26631;&#31614;&#20013;&#27602;&#30340;&#35748;&#35777;&#38450;&#24481;&#65292;&#33021;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#21040;0-16%&#65292;&#21516;&#26102;&#20960;&#20046;&#19981;&#24433;&#21709;&#27979;&#35797;&#20934;&#30830;&#24615;&#65292;&#20026;&#26410;&#26469;&#24320;&#21457;&#26356;&#24378;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#21644;&#21033;&#29992;&#35813;&#38450;&#24481;&#25514;&#26045;&#20316;&#20026;&#24378;&#26377;&#21147;&#22522;&#30784;&#25552;&#20379;&#20102;&#37325;&#35201;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24178;&#20928;&#26631;&#31614;&#20013;&#27602;&#25915;&#20987;&#30340;&#35748;&#35777;&#38450;&#24481;&#26041;&#27861;&#12290;&#36825;&#20123;&#25915;&#20987;&#36890;&#36807;&#21521;&#35757;&#32451;&#25968;&#25454;&#20013;&#27880;&#20837;&#23569;&#37327;&#30340;&#27602;&#23475;&#26679;&#26412;&#65288;&#20363;&#22914;1%&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;$p$-&#33539;&#25968;&#21463;&#38480;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#20174;&#32780;&#35825;&#23548;&#23545;&#27979;&#35797;&#36755;&#20837;&#30340;&#30446;&#26631;&#35823;&#20998;&#31867;&#12290;&#21463;&#21040;$&#21435;&#22122;&#24179;&#28369;$&#23454;&#29616;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#19968;&#20010;&#29616;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#23545;&#31713;&#25913;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#28040;&#27602;&#12290;&#25105;&#20204;&#24191;&#27867;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#38450;&#24481;&#25514;&#26045;&#23545;&#19971;&#31181;&#24178;&#20928;&#26631;&#31614;&#20013;&#27602;&#25915;&#20987;&#30340;&#38450;&#25252;&#25928;&#26524;&#65292;&#24182;&#19988;&#23558;&#23427;&#20204;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#21040;0-16%&#65292;&#21516;&#26102;&#27979;&#35797;&#20934;&#30830;&#24615;&#20960;&#20046;&#27809;&#26377;&#19979;&#38477;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#38450;&#24481;&#19982;&#29616;&#26377;&#30340;&#38024;&#23545;&#24178;&#20928;&#26631;&#31614;&#20013;&#27602;&#30340;&#23545;&#31574;&#36827;&#34892;&#27604;&#36739;&#65292;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#38450;&#24481;&#25928;&#26524;&#26368;&#20339;&#65292;&#24182;&#25552;&#20379;&#26368;&#20339;&#30340;&#27169;&#22411;&#25928;&#29992;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#26410;&#26469;&#38656;&#35201;&#24320;&#23637;&#26356;&#24378;&#22823;&#30340;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#35748;&#35777;&#20294;&#23454;&#29992;&#30340;&#38450;&#24481;&#20316;&#20026;&#31283;&#22266;&#22522;&#30784;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11981v1 Announce Type: cross  Abstract: We present a certified defense to clean-label poisoning attacks. These attacks work by injecting a small number of poisoning samples (e.g., 1%) that contain $p$-norm bounded adversarial perturbations into the training data to induce a targeted misclassification of a test-time input. Inspired by the adversarial robustness achieved by $denoised$ $smoothing$, we show how an off-the-shelf diffusion model can sanitize the tampered training data. We extensively test our defense against seven clean-label poisoning attacks and reduce their attack success to 0-16% with only a negligible drop in the test time accuracy. We compare our defense with existing countermeasures against clean-label poisoning, showing that the defense reduces the attack success the most and offers the best model utility. Our results highlight the need for future work on developing stronger clean-label attacks and using our certified yet practical defense as a strong base
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#21644;&#20998;&#23618;&#23545;&#40784;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#22810;&#27169;&#24577;&#32479;&#19968;&#34920;&#31034;&#30340;&#32454;&#31890;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36328;&#27169;&#24577;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.05168</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#21644;&#20998;&#23618;&#23545;&#40784;&#35299;&#38145;&#22810;&#27169;&#24577;&#32479;&#19968;&#31163;&#25955;&#34920;&#31034;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of Multimodal Unified Discrete Representation through Training-Free Codebook Optimization and Hierarchical Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#21644;&#20998;&#23618;&#23545;&#40784;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#22810;&#27169;&#24577;&#32479;&#19968;&#34920;&#31034;&#30340;&#32454;&#31890;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36328;&#27169;&#24577;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#34920;&#26126;&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#32479;&#19968;&#30721;&#26412;&#30340;&#21452;&#20132;&#21449;&#27169;&#24577;&#20449;&#24687;&#35299;&#32544;&#65288;DCID&#65289;&#27169;&#22411;&#22312;&#23454;&#29616;&#32454;&#31890;&#24230;&#34920;&#31034;&#21644;&#36328;&#27169;&#24577;&#27867;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20173;&#21463;&#21040;&#23545;&#25152;&#26377;&#36890;&#36947;&#30340;&#22343;&#31561;&#23545;&#24453;&#20197;&#21450;&#24573;&#35270;&#27425;&#35201;&#20107;&#20214;&#20449;&#24687;&#30340;&#38459;&#30861;&#65292;&#23548;&#33268;&#26469;&#33258;&#26080;&#20851;&#36890;&#36947;&#30340;&#24178;&#25200;&#24182;&#22312;&#32454;&#31890;&#24230;&#20219;&#21153;&#20013;&#34920;&#29616;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#30721;&#26412;&#20248;&#21270;&#65288;TOC&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32479;&#19968;&#31354;&#38388;&#20013;&#36873;&#25321;&#37325;&#35201;&#36890;&#36947;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#21452;&#20132;&#21449;&#27169;&#24577;&#20449;&#24687;&#35299;&#32544;&#65288;H-DCID&#65289;&#26041;&#27861;&#23558;&#20449;&#24687;&#20998;&#31163;&#21644;&#23545;&#40784;&#25193;&#23637;&#21040;&#20004;&#20010;&#32423;&#21035;&#65292;&#25429;&#25417;&#26356;&#22810;&#36328;&#27169;&#24577;&#32454;&#33410;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05168v1 Announce Type: cross  Abstract: Recent advances in representation learning have demonstrated the significance of multimodal alignment. The Dual Cross-modal Information Disentanglement (DCID) model, utilizing a unified codebook, shows promising results in achieving fine-grained representation and cross-modal generalization. However, it is still hindered by equal treatment of all channels and neglect of minor event information, resulting in interference from irrelevant channels and limited performance in fine-grained tasks. Thus, in this work, We propose a Training-free Optimization of Codebook (TOC) method to enhance model performance by selecting important channels in the unified space without retraining. Additionally, we introduce the Hierarchical Dual Cross-modal Information Disentanglement (H-DCID) approach to extend information separation and alignment to two levels, capturing more cross-modal details. The experiment results demonstrate significant improvements a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;AbdGen&#65292;&#29992;&#20110;&#23558;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#19982;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#12290;&#23427;&#35299;&#20915;&#20102;&#31526;&#21495;&#36171;&#20540;&#21644;&#35268;&#21017;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;&#31526;&#21495;&#36171;&#20540;&#65292;&#36890;&#36807;&#23545;&#27604;&#20803;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#31934;&#30830;&#30340;&#35268;&#21017;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.17451</link><description>&lt;p&gt;
&#36890;&#36807;&#29702;&#35299;&#29983;&#25104;&#65306;&#20855;&#26377;&#36923;&#36753;&#31526;&#21495;&#22522;&#30784;&#30340;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Generating by Understanding: Neural Visual Generation with Logical Symbol Groundings. (arXiv:2310.17451v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17451
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;AbdGen&#65292;&#29992;&#20110;&#23558;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#19982;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#12290;&#23427;&#35299;&#20915;&#20102;&#31526;&#21495;&#36171;&#20540;&#21644;&#35268;&#21017;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;&#31526;&#21495;&#36171;&#20540;&#65292;&#36890;&#36807;&#23545;&#27604;&#20803;&#35825;&#23548;&#26041;&#27861;&#23454;&#29616;&#31934;&#30830;&#30340;&#35268;&#21017;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23558;&#20854;&#19982;&#24378;&#22823;&#30340;&#31526;&#21495;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#38598;&#25104;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20027;&#35201;&#25361;&#25112;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#19968;&#20010;&#26159;&#31526;&#21495;&#36171;&#20540;&#65292;&#21363;&#23558;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#22120;&#30340;&#28508;&#22312;&#22240;&#32032;&#19982;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#20013;&#30340;&#26377;&#24847;&#20041;&#30340;&#31526;&#21495;&#36827;&#34892;&#32465;&#23450;&#12290;&#21478;&#19968;&#20010;&#26159;&#35268;&#21017;&#23398;&#20064;&#65292;&#21363;&#23398;&#20064;&#26032;&#30340;&#35268;&#21017;&#65292;&#36825;&#20123;&#35268;&#21017;&#25511;&#21046;&#25968;&#25454;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#25512;&#29702;&#31995;&#32479;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#31526;&#21495;&#22522;&#30784;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;Abductive Visual Generation (AbdGen)&#65292;&#29992;&#20110;&#22522;&#20110;&#35825;&#23548;&#23398;&#20064;&#26694;&#26550;&#23558;&#36923;&#36753;&#32534;&#31243;&#31995;&#32479;&#19982;&#31070;&#32463;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#36215;&#26469;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#38752;&#39640;&#25928;&#30340;&#31526;&#21495;&#36171;&#20540;&#65292;&#24341;&#20837;&#20102;&#37327;&#21270;&#35825;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#32534;&#30721;&#26412;&#20013;&#30340;&#26368;&#36817;&#37051;&#26597;&#25214;&#29983;&#25104;&#35825;&#23548;&#25552;&#26696;&#12290;&#20026;&#20102;&#23454;&#29616;&#31934;&#30830;&#30340;&#35268;&#21017;&#23398;&#20064;&#65292;&#24341;&#20837;&#20102;&#23545;&#27604;&#20803;&#35825;&#23548;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great success of neural visual generative models in recent years, integrating them with strong symbolic knowledge reasoning systems remains a challenging task. The main challenges are two-fold: one is symbol assignment, i.e. bonding latent factors of neural visual generators with meaningful symbols from knowledge reasoning systems. Another is rule learning, i.e. learning new rules, which govern the generative process of the data, to augment the knowledge reasoning systems. To deal with these symbol grounding problems, we propose a neural-symbolic learning approach, Abductive Visual Generation (AbdGen), for integrating logic programming systems with neural visual generative models based on the abductive learning framework. To achieve reliable and efficient symbol assignment, the quantized abduction method is introduced for generating abduction proposals by the nearest-neighbor lookups within semantic codebooks. To achieve precise rule learning, the contrastive meta-abduction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20851;&#32852;&#30340;&#22270;&#20687;&#23545;AI&#29983;&#25104;&#25253;&#21578;&#36827;&#34892;&#20107;&#23454;&#26680;&#26597;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21306;&#20998;&#25253;&#21578;&#20013;&#30340;&#30495;&#20551;&#21477;&#23376;&#12290;&#36825;&#23545;&#21152;&#24555;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#38477;&#20302;&#24635;&#20307;&#25104;&#26412;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.14634</link><description>&lt;p&gt;
AI&#29983;&#25104;&#25253;&#21578;&#30340;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
Fact-Checking of AI-Generated Reports. (arXiv:2307.14634v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20851;&#32852;&#30340;&#22270;&#20687;&#23545;AI&#29983;&#25104;&#25253;&#21578;&#36827;&#34892;&#20107;&#23454;&#26680;&#26597;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21306;&#20998;&#25253;&#21578;&#20013;&#30340;&#30495;&#20551;&#21477;&#23376;&#12290;&#36825;&#23545;&#21152;&#24555;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#38477;&#20302;&#24635;&#20307;&#25104;&#26412;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36827;&#27493;&#65292;&#29616;&#22312;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#33258;&#21160;&#25253;&#21578;&#26469;&#23545;&#25918;&#23556;&#23398;&#22270;&#20687;&#36827;&#34892;&#21021;&#27493;&#38405;&#35835;&#12290;&#36825;&#21487;&#20197;&#21152;&#24555;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#38477;&#20302;&#24635;&#20307;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#36825;&#31181;&#27169;&#22411;&#24448;&#24448;&#20250;&#20135;&#29983;&#24187;&#35273;&#65292;&#23548;&#33268;&#29983;&#25104;&#25253;&#21578;&#20013;&#20986;&#29616;&#38169;&#35823;&#30340;&#21457;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30456;&#20851;&#32852;&#30340;&#22270;&#20687;&#23545;AI&#29983;&#25104;&#25253;&#21578;&#36827;&#34892;&#20107;&#23454;&#26680;&#26597;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#19982;&#25551;&#36848;&#30495;&#23454;&#25110;&#28508;&#22312;&#34394;&#20551;&#21457;&#29616;&#30340;&#21477;&#23376;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24320;&#21457;&#30340;&#26680;&#26597;&#32773;&#21306;&#20998;&#25253;&#21578;&#20013;&#30340;&#30495;&#20551;&#21477;&#23376;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#26679;&#30340;&#26680;&#26597;&#32773;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25200;&#21160;&#21407;&#22987;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#21457;&#29616;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#20266;&#36896;&#25253;&#21578;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#23558;&#26469;&#33258;&#36825;&#20123;&#25253;&#21578;&#30340;&#30495;&#20551;&#21477;&#23376;&#30340;&#25991;&#26412;&#32534;&#30721;&#19982;&#22270;&#20687;&#32534;&#30721;&#37197;&#23545;&#65292;&#23398;&#20064;&#26144;&#23556;&#21040;&#30495;/&#20551;&#26631;&#31614;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
With advances in generative artificial intelligence (AI), it is now possible to produce realistic-looking automated reports for preliminary reads of radiology images. This can expedite clinical workflows, improve accuracy and reduce overall costs. However, it is also well-known that such models often hallucinate, leading to false findings in the generated reports. In this paper, we propose a new method of fact-checking of AI-generated reports using their associated images. Specifically, the developed examiner differentiates real and fake sentences in reports by learning the association between an image and sentences describing real or potentially fake findings. To train such an examiner, we first created a new dataset of fake reports by perturbing the findings in the original ground truth radiology reports associated with images. Text encodings of real and fake sentences drawn from these reports are then paired with image encodings to learn the mapping to real/fake labels. The utility 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TraCoCo&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#25968;&#25454;&#35270;&#22270;&#30340;&#19981;&#21516;&#31354;&#38388;&#19978;&#19979;&#25991;&#26469;&#25200;&#21160;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#20174;&#21487;&#35270;&#21270;&#23545;&#35937;&#20013;&#23398;&#20064;&#20998;&#21106;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#30340;&#32763;&#35793;&#19968;&#33268;&#21322;&#30417;&#30563;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2203.14523</link><description>&lt;p&gt;
&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#30340;&#32763;&#35793;&#19968;&#33268;&#21322;&#30417;&#30563;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Translation Consistent Semi-supervised Segmentation for 3D Medical Images. (arXiv:2203.14523v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.14523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TraCoCo&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#25968;&#25454;&#35270;&#22270;&#30340;&#19981;&#21516;&#31354;&#38388;&#19978;&#19979;&#25991;&#26469;&#25200;&#21160;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#20174;&#21487;&#35270;&#21270;&#23545;&#35937;&#20013;&#23398;&#20064;&#20998;&#21106;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#30340;&#32763;&#35793;&#19968;&#33268;&#21322;&#30417;&#30563;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#65292;&#20294;&#20854;&#20381;&#36182;&#20110;&#28085;&#30422;&#22823;&#37327;&#20307;&#32032;&#30340;&#27880;&#37322;&#25968;&#25454;&#65292;&#36825;&#26159;&#19968;&#20010;&#38656;&#35201;&#35299;&#20915;&#30340;&#21155;&#21183;&#65292;&#22240;&#20026;&#33719;&#21462;&#36825;&#31181;&#27880;&#37322;&#30340;&#25104;&#26412;&#24456;&#39640;&#12290;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#21644;&#23569;&#37327;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26368;&#25104;&#21151;&#30340;SSL&#26041;&#27861;&#22522;&#20110;&#19968;&#33268;&#24615;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#25200;&#21160;&#35270;&#22270;&#33719;&#24471;&#30340;&#27169;&#22411;&#21709;&#24212;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#25200;&#21160;&#36890;&#24120;&#20250;&#20445;&#25345;&#35270;&#22270;&#20043;&#38388;&#30340;&#31354;&#38388;&#36755;&#20837;&#19978;&#19979;&#25991;&#30456;&#24403;&#19968;&#33268;&#65292;&#36825;&#21487;&#33021;&#20250;&#20351;&#27169;&#22411;&#20174;&#31354;&#38388;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#20998;&#21106;&#27169;&#24335;&#65292;&#32780;&#19981;&#26159;&#20174;&#20998;&#21106;&#23545;&#35937;&#26412;&#36523;&#20013;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#32763;&#35793;&#19968;&#33268;&#21327;&#21516;&#35757;&#32451;&#65288;TraCoCo&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19968;&#33268;&#24615;&#23398;&#20064;SSL&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#25913;&#21464;&#19981;&#21516;&#30340;&#31354;&#38388;&#36755;&#20837;&#19978;&#19979;&#25991;&#26469;&#25200;&#21160;&#36755;&#20837;&#25968;&#25454;&#35270;&#22270;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20174;&#21487;&#35270;&#21270;&#23545;&#35937;&#20013;&#23398;&#20064;&#20998;&#21106;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D medical image segmentation methods have been successful, but their dependence on large amounts of voxel-level annotated data is a disadvantage that needs to be addressed given the high cost to obtain such annotation. Semi-supervised learning (SSL) solve this issue by training models with a large unlabelled and a small labelled dataset. The most successful SSL approaches are based on consistency learning that minimises the distance between model responses obtained from perturbed views of the unlabelled data. These perturbations usually keep the spatial input context between views fairly consistent, which may cause the model to learn segmentation patterns from the spatial input contexts instead of the segmented objects. In this paper, we introduce the Translation Consistent Co-training (TraCoCo) which is a consistency learning SSL method that perturbs the input data views by varying their spatial input context, allowing the model to learn segmentation patterns from visual objects. Fur
&lt;/p&gt;</description></item></channel></rss>