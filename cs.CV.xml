<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NavCoT&#30340;&#26032;&#31574;&#30053;&#65292;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20013;&#36890;&#36807;&#23398;&#20064;&#35299;&#32806;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#33258;&#20027;&#23548;&#33322;&#20915;&#31574;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;&#39046;&#22495;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.07376</link><description>&lt;p&gt;
NavCoT: &#36890;&#36807;&#23398;&#20064;&#35299;&#32806;&#25512;&#29702;&#25552;&#21319;&#22522;&#20110;LLM&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NavCoT&#30340;&#26032;&#31574;&#30053;&#65292;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20013;&#36890;&#36807;&#23398;&#20064;&#35299;&#32806;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#33258;&#20027;&#23548;&#33322;&#20915;&#31574;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;&#39046;&#22495;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;(VLN)&#20316;&#20026;&#20855;&#26377;&#37325;&#35201;&#30740;&#31350;&#20215;&#20540;&#30340;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#65292;&#38656;&#35201;&#19968;&#20010;&#20855;&#36523;&#20195;&#29702;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#31034;&#31359;&#36234;&#22797;&#26434;&#30340;3D&#29615;&#22659;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;VLN&#20013;&#25552;&#39640;&#23548;&#33322;&#25512;&#29702;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20027;&#35201;&#22312;&#31163;&#32447;&#26041;&#24335;&#19979;&#30340;&#20351;&#29992;&#36890;&#24120;&#22312;VLN&#20219;&#21153;&#21644;LLM&#35757;&#32451;&#35821;&#26009;&#24211;&#20043;&#38388;&#36973;&#21463;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#23548;&#33322;&#24605;&#32500;&#38142;(NavCoT)&#30340;&#26032;&#22411;&#31574;&#30053;&#65292;&#25105;&#20204;&#36890;&#36807;&#23436;&#25104;&#39046;&#22495;&#20869;&#39640;&#25928;&#21442;&#25968;&#35757;&#32451;&#65292;&#23454;&#29616;&#33258;&#20027;&#23548;&#33322;&#20915;&#31574;&#65292;&#26377;&#25928;&#20943;&#36731;&#39046;&#22495;&#24046;&#36317;&#30340;&#25104;&#26412;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#65292;LLM&#34987;&#25552;&#31034;&#36890;&#36807;&#20316;&#20026;&#19990;&#30028;&#27169;&#22411;&#26469;&#39044;&#27979;&#23548;&#33322;&#24605;&#32500;&#38142;&#65306;1)&#26681;&#25454;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07376v1 Announce Type: cross  Abstract: Vision-and-Language Navigation (VLN), as a crucial research problem of Embodied AI, requires an embodied agent to navigate through complex 3D environments following natural language instructions. Recent research has highlighted the promising capacity of large language models (LLMs) in VLN by improving navigational reasoning accuracy and interpretability. However, their predominant use in an offline manner usually suffers from substantial domain gap between the VLN task and the LLM training corpus. This paper introduces a novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill parameter-efficient in-domain training to enable self-guided navigational decision, leading to a significant mitigation of the domain gap in a cost-effective manner. Specifically, at each timestep, the LLM is prompted to forecast the navigational chain-of-thought by: 1) acting as a world model to imagine the next observation according to the
&lt;/p&gt;</description></item><item><title>I3SB&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#32467;&#21512;&#25439;&#22351;&#30340;&#22270;&#20687;&#25913;&#21892;&#32441;&#29702;&#24674;&#22797;&#65292;&#22312;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.06069</link><description>&lt;p&gt;
&#38544;&#24335;&#22270;&#20687;&#23545;&#22270;&#20687;Schrodinger&#26725;&#29992;&#20110;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Implicit Image-to-Image Schrodinger Bridge for CT Super-Resolution and Denoising
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06069
&lt;/p&gt;
&lt;p&gt;
I3SB&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#32467;&#21512;&#25439;&#22351;&#30340;&#22270;&#20687;&#25913;&#21892;&#32441;&#29702;&#24674;&#22797;&#65292;&#22312;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#22312;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#32780;&#24471;&#21040;&#35748;&#21487;&#65292;&#28982;&#32780;&#65292;&#20854;&#20174;&#39640;&#26031;&#22122;&#22768;&#24320;&#22987;&#30340;&#36845;&#20195;&#21435;&#22122;&#36807;&#31243;&#24448;&#24448;&#23548;&#33268;&#25512;&#26029;&#36895;&#24230;&#24930;&#12290;&#20316;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#22270;&#20687;&#23545;&#22270;&#20687;Schr&#246;dinger&#26725;&#65288;I2SB&#65289;&#20174;&#25439;&#22351;&#30340;&#22270;&#20687;&#24320;&#22987;&#21021;&#22987;&#21270;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#38598;&#25104;&#20102;&#26377;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25216;&#26415;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#38544;&#24335;&#22270;&#20687;&#23545;&#22270;&#20687;Schr&#246;dinger&#26725;&#65288;I3SB&#65289;&#25193;&#23637;&#20102;I2SB&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#19968;&#29983;&#25104;&#27493;&#39588;&#20013;&#32435;&#20837;&#25439;&#22351;&#30340;&#22270;&#20687;&#65292;&#23558;&#20854;&#29983;&#25104;&#36807;&#31243;&#36716;&#25442;&#20026;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#12290;&#36825;&#31181;&#22686;&#24378;&#20351;&#24471;I3SB&#33021;&#22815;&#22312;&#23569;&#37327;&#29983;&#25104;&#27493;&#39588;&#20013;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#32441;&#29702;&#24674;&#22797;&#30340;&#22270;&#20687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;&#20219;&#21153;&#19978;&#24471;&#21040;&#39564;&#35777;&#65292;&#24182;&#36229;&#36234;&#20102;&#21253;&#25324;&#26377;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#20869;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06069v1 Announce Type: cross  Abstract: Conditional diffusion models have gained recognition for their effectiveness in image restoration tasks, yet their iterative denoising process, starting from Gaussian noise, often leads to slow inference speeds. As a promising alternative, the Image-to-Image Schr\"odinger Bridge (I2SB) initializes the generative process from corrupted images and integrates training techniques from conditional diffusion models. In this study, we extended the I2SB method by introducing the Implicit Image-to-Image Schrodinger Bridge (I3SB), transitioning its generative process to a non-Markovian process by incorporating corrupted images in each generative step. This enhancement empowers I3SB to generate images with better texture restoration using a small number of generative steps. The proposed method was validated on CT super-resolution and denoising tasks and outperformed existing methods, including the conditional denoising diffusion probabilistic mod
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550; - &#22810;&#32423;&#20248;&#21270;&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MLO-MAE&#65289;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#21453;&#39304;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#26368;&#20339;&#30340;&#36974;&#32617;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.18128</link><description>&lt;p&gt;
&#22312;&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#20351;&#29992;&#22810;&#32423;&#20248;&#21270;&#30340;&#19979;&#28216;&#20219;&#21153;&#24341;&#23548;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18128
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550; - &#22810;&#32423;&#20248;&#21270;&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MLO-MAE&#65289;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#21453;&#39304;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#26368;&#20339;&#30340;&#36974;&#32617;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#26159;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#20013;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#19968;&#20010;&#26174;&#33879;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#38543;&#26426;&#36974;&#32617;&#22270;&#20687;&#34917;&#19969;&#65292;&#24182;&#20351;&#29992;&#26410;&#36974;&#32617;&#30340;&#34917;&#19969;&#37325;&#24314;&#36825;&#20123;&#36974;&#32617;&#34917;&#19969;&#12290; MAE&#30340;&#19968;&#20010;&#20851;&#38190;&#23616;&#38480;&#24615;&#22312;&#20110;&#20854;&#24573;&#35270;&#19981;&#21516;&#34917;&#19969;&#30340;&#20449;&#24687;&#37327;&#19981;&#21516;&#65292;&#22240;&#20026;&#23427;&#20250;&#32479;&#19968;&#36873;&#25321;&#35201;&#36974;&#32617;&#30340;&#34917;&#19969;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#19968;&#20123;&#26041;&#27861;&#25552;&#20986;&#22522;&#20110;&#34917;&#19969;&#20449;&#24687;&#37327;&#36827;&#34892;&#36974;&#32617;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#19981;&#32771;&#34385;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#23450;&#38656;&#27714;&#65292;&#21487;&#33021;&#23548;&#33268;&#36825;&#20123;&#20219;&#21153;&#30340;&#34920;&#31034;&#27425;&#20248;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#32423;&#20248;&#21270;&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MLO-MAE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#21453;&#39304;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#26368;&#20339;&#36974;&#32617;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20102;MLO-MAE&#22312;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18128v1 Announce Type: cross  Abstract: Masked Autoencoder (MAE) is a notable method for self-supervised pretraining in visual representation learning. It operates by randomly masking image patches and reconstructing these masked patches using the unmasked ones. A key limitation of MAE lies in its disregard for the varying informativeness of different patches, as it uniformly selects patches to mask. To overcome this, some approaches propose masking based on patch informativeness. However, these methods often do not consider the specific requirements of downstream tasks, potentially leading to suboptimal representations for these tasks. In response, we introduce the Multi-level Optimized Mask Autoencoder (MLO-MAE), a novel framework that leverages end-to-end feedback from downstream tasks to learn an optimal masking strategy during pretraining. Our experimental findings highlight MLO-MAE's significant advancements in visual representation learning. Compared to existing metho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38646;-shot&#20998;&#31867;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#22914;&#24694;&#24847;&#27169;&#22240;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2402.12198</link><description>&lt;p&gt;
&#38646;-shot &#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#65306;&#25105;&#20204;&#24050;&#32463;&#21040;&#36798;&#30446;&#26631;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Zero shot VLMs for hate meme detection: Are we there yet?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38646;-shot&#20998;&#31867;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#22914;&#24694;&#24847;&#27169;&#22240;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#20854;&#20013;&#27169;&#22240;&#20316;&#20026;&#19968;&#31181;&#29420;&#29305;&#24418;&#24335;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#19968;&#20123;&#24694;&#24847;&#29992;&#25143;&#21033;&#29992;&#27169;&#22240;&#38024;&#23545;&#20010;&#20154;&#25110;&#26131;&#21463;&#25915;&#20987;&#30340;&#31038;&#21306;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#35782;&#21035;&#21644;&#35299;&#20915;&#27492;&#31867;&#24694;&#24847;&#27169;&#22240;&#12290;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#24320;&#21457;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#20010;&#26174;&#33879;&#23616;&#38480;&#24615;&#26159;&#38656;&#35201;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#25165;&#33021;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#30028;&#35265;&#35777;&#20102;&#20960;&#31181;&#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#36825;&#20123;&#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#35832;&#22914;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#31561;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#35774;&#32622;&#26469;&#19987;&#27880;&#20110;&#23545;&#24694;&#24847;/&#26377;&#23475;&#27169;&#22240;&#30340;&#38646;-shot &#20998;&#31867;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12198v1 Announce Type: new  Abstract: Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification. Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection. We use various prompt settings to focus on zero-shot classification of hateful/harmful memes. Through our analysis, we o
&lt;/p&gt;</description></item><item><title>&#23558;&#27573;&#20998;&#31163;&#20219;&#24847;&#27169;&#22411;&#25512;&#36827;&#33267;&#39640;&#24230;&#20934;&#30830;&#30340;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;&#65292;&#36890;&#36807;&#25552;&#20986;DIS-SAM&#26694;&#26550;&#65292;&#25104;&#21151;&#25913;&#36827;SAM&#27169;&#22411;&#22312;&#32454;&#33410;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#22686;&#24378;&#30340;&#20998;&#21106;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2401.00248</link><description>&lt;p&gt;
&#23558;&#8220;&#27573;&#20998;&#31163;&#20219;&#24847;&#27169;&#22411;&#8221;&#25512;&#36827;&#33267;&#39640;&#24230;&#20934;&#30830;&#30340;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Promoting Segment Anything Model towards Highly Accurate Dichotomous Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00248
&lt;/p&gt;
&lt;p&gt;
&#23558;&#27573;&#20998;&#31163;&#20219;&#24847;&#27169;&#22411;&#25512;&#36827;&#33267;&#39640;&#24230;&#20934;&#30830;&#30340;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;&#65292;&#36890;&#36807;&#25552;&#20986;DIS-SAM&#26694;&#26550;&#65292;&#25104;&#21151;&#25913;&#36827;SAM&#27169;&#22411;&#22312;&#32454;&#33410;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#22686;&#24378;&#30340;&#20998;&#21106;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM)&#20195;&#34920;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;SAM&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#20854;&#20998;&#21106;&#33945;&#29256;&#32570;&#20047;&#32454;&#31890;&#24230;&#32454;&#33410;&#65292;&#29305;&#21035;&#26159;&#22312;&#20934;&#30830;&#25551;&#32472;&#23545;&#35937;&#36793;&#30028;&#26041;&#38754;&#12290;&#25105;&#20204;&#23545;SAM&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#36827;&#19968;&#27493;&#25913;&#36827;&#20197;&#23454;&#29616;&#39640;&#24230;&#31934;&#30830;&#30340;&#23545;&#35937;&#20998;&#21106;&#65288;&#21363;&#31216;&#20026;&#20108;&#20803;&#22270;&#20687;&#20998;&#21106;DIS&#65289;&#25265;&#26377;&#24456;&#39640;&#26399;&#26395;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIS-SAM&#65292;&#23558;SAM&#25512;&#36827;&#33267;DIS&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#31934;&#30830;&#32454;&#33410;&#12290;DIS-SAM&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#39640;&#24230;&#20934;&#30830;&#20998;&#21106;&#32780;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#20445;&#25345;&#20102;SAM&#30340;&#21487;&#20419;&#36827;&#35774;&#35745;&#12290;DIS-SAM&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#23558;SAM&#19982;&#19987;&#38376;&#29992;&#20110;DIS&#30340;&#20462;&#25913;&#21518;&#30340;IS-Net&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;DIS-SAM&#30456;&#27604;SAM&#21644;HQ-SA&#34920;&#29616;&#20986;&#26174;&#30528;&#22686;&#24378;&#30340;&#20998;&#21106;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00248v2 Announce Type: replace-cross  Abstract: The Segment Anything Model (SAM) represents a significant breakthrough into foundation models for computer vision, providing a large-scale image segmentation model. However, despite SAM's zero-shot performance, its segmentation masks lack fine-grained details, particularly in accurately delineating object boundaries. We have high expectations regarding whether SAM, as a foundation model, can be improved towards highly accurate object segmentation, which is known as dichotomous image segmentation (DIS). To address this issue, we propose DIS-SAM, which advances SAM towards DIS with extremely accurate details. DIS-SAM is a framework specifically tailored for highly accurate segmentation, maintaining SAM's promptable design. DIS-SAM employs a two-stage approach, integrating SAM with a modified IS-Net dedicated to DIS. Despite its simplicity, DIS-SAM demonstrates significantly enhanced segmentation accuracy compared to SAM and HQ-SA
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21305;&#37197;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;(DataDAM)&#26041;&#27861;&#12290;&#36890;&#36807;&#21305;&#37197;&#31354;&#38388;&#27880;&#24847;&#21147;&#26469;&#23398;&#20064;&#21512;&#25104;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.00093</link><description>&lt;p&gt;
DataDAM: &#22522;&#20110;&#27880;&#24847;&#21147;&#21305;&#37197;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
DataDAM: Efficient Dataset Distillation with Attention Matching. (arXiv:2310.00093v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21305;&#37197;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;(DataDAM)&#26041;&#27861;&#12290;&#36890;&#36807;&#21305;&#37197;&#31354;&#38388;&#27880;&#24847;&#21147;&#26469;&#23398;&#20064;&#21512;&#25104;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#22312;&#23613;&#37327;&#20943;&#23569;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#26356;&#22823;&#30495;&#23454;&#25968;&#25454;&#38598;&#20449;&#24687;&#30340;&#23567;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#26368;&#32456;&#23454;&#29616;&#19982;&#25972;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#26041;&#27861;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#24182;&#19981;&#33021;&#20687;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#37027;&#26679;&#20998;&#24067;&#21644;&#21306;&#20998;&#65292;&#32780;&#19988;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#20294;&#31934;&#28860;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#19982;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#21305;&#37197;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;(DataDAM)&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#21305;&#37197;&#31354;&#38388;&#27880;&#24847;&#21147;&#26469;&#23398;&#20064;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have long tried to minimize training costs in deep learning while maintaining strong generalization across diverse datasets. Emerging research on dataset distillation aims to reduce training costs by creating a small synthetic set that contains the information of a larger real dataset and ultimately achieves test accuracy equivalent to a model trained on the whole dataset. Unfortunately, the synthetic data generated by previous methods are not guaranteed to distribute and discriminate as well as the original training data, and they incur significant computational costs. Despite promising results, there still exists a significant performance gap between models trained on condensed synthetic sets and those trained on the whole dataset. In this paper, we address these challenges using efficient Dataset Distillation with Attention Matching (DataDAM), achieving state-of-the-art performance while reducing training costs. Specifically, we learn synthetic images by matching the spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SupReMix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#26679;&#26412;&#65292;&#29305;&#21035;&#26159;&#28151;&#21512;&#36127;&#26679;&#26412;&#21644;&#28151;&#21512;&#27491;&#26679;&#26412;&#65292;&#26469;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#20934;&#30830;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16633</link><description>&lt;p&gt;
&#28151;&#21512;&#20320;&#33258;&#24049;&#30340;&#23545;&#27604;&#23545;
&lt;/p&gt;
&lt;p&gt;
Mixup Your Own Pairs. (arXiv:2309.16633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SupReMix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#26679;&#26412;&#65292;&#29305;&#21035;&#26159;&#28151;&#21512;&#36127;&#26679;&#26412;&#21644;&#28151;&#21512;&#27491;&#26679;&#26412;&#65292;&#26469;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#20934;&#30830;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#22238;&#24402;&#38382;&#39064;&#20256;&#32479;&#19978;&#27604;&#20998;&#31867;&#38382;&#39064;&#21463;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#30452;&#25509;&#24212;&#29992;&#20026;&#20998;&#31867;&#35774;&#35745;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#21040;&#22238;&#24402;&#38382;&#39064;&#24448;&#24448;&#20250;&#23548;&#33268;&#28508;&#31354;&#38388;&#20013;&#30862;&#29255;&#21270;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#20135;&#29983;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#30001;&#20110;&#24573;&#35270;&#20102;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#24207;&#24207;&#24863;&#30693;&#21644;&#38590;&#24230;&#65292;&#23545;&#20110;&#22238;&#24402;&#38382;&#39064;&#32780;&#35328;&#65292;&#23545;&#27604;&#23398;&#20064;&#30340;&#28508;&#33021;&#34987;&#24573;&#35270;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20513;&#8220;&#28151;&#21512;&#33258;&#24049;&#30340;&#23545;&#27604;&#23545;&#36827;&#34892;&#30417;&#30563;&#24615;&#23545;&#27604;&#22238;&#24402;&#8221;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#38752;&#30495;&#23454;/&#22686;&#24378;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#24335;&#30417;&#30563;&#23545;&#27604;&#22238;&#24402;&#23398;&#20064;&#65288;SupReMix&#65289;&#12290;&#23427;&#22312;&#23884;&#20837;&#32423;&#21035;&#19978;&#20197;&#38170;&#28857;&#21253;&#21547;&#30340;&#28151;&#21512;&#65288;&#38170;&#28857;&#21644;&#19968;&#20010;&#19981;&#21516;&#30340;&#36127;&#26679;&#26412;&#30340;&#28151;&#21512;&#65289;&#20316;&#20026;&#22256;&#38590;&#36127;&#23545;&#65292;&#20197;&#38170;&#28857;&#25490;&#38500;&#30340;&#28151;&#21512;&#65288;&#20004;&#20010;&#19981;&#21516;&#30340;&#36127;&#26679;&#26412;&#30340;&#28151;&#21512;&#65289;&#20316;&#20026;&#22256;&#38590;&#27491;&#23545;&#12290;&#36825;&#19968;&#31574;&#30053;&#24418;&#25104;&#20102;&#22256;&#38590;&#26679;&#26412;&#23545;&#23398;&#20064;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In representation learning, regression has traditionally received less attention than classification. Directly applying representation learning techniques designed for classification to regression often results in fragmented representations in the latent space, yielding sub-optimal performance. In this paper, we argue that the potential of contrastive learning for regression has been overshadowed due to the neglect of two crucial aspects: ordinality-awareness and hardness. To address these challenges, we advocate "mixup your own contrastive pairs for supervised contrastive regression", instead of relying solely on real/augmented samples. Specifically, we propose Supervised Contrastive Learning for Regression with Mixup (SupReMix). It takes anchor-inclusive mixtures (mixup of the anchor and a distinct negative sample) as hard negative pairs and anchor-exclusive mixtures (mixup of two distinct negative samples) as hard positive pairs at the embedding level. This strategy formulates harde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24615;&#33021;&#20272;&#35745;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#21442;&#25968;&#32622;&#20449;&#21306;&#38388;&#30340;&#23485;&#24230;&#19982;&#20998;&#21106;&#38382;&#39064;&#30340;&#29305;&#28857;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2307.10926</link><description>&lt;p&gt;
&#23545;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24615;&#33021;&#20272;&#35745;&#30340;&#32622;&#20449;&#21306;&#38388;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Confidence intervals for performance estimates in 3D medical image segmentation. (arXiv:2307.10926v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24615;&#33021;&#20272;&#35745;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#21442;&#25968;&#32622;&#20449;&#21306;&#38388;&#30340;&#23485;&#24230;&#19982;&#20998;&#21106;&#38382;&#39064;&#30340;&#29305;&#28857;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#20998;&#21106;&#27169;&#22411;&#30340;&#35780;&#20272;&#26159;&#22522;&#20110;&#26377;&#38480;&#30340;&#20363;&#22270;&#20687;&#65292;&#22240;&#27492;&#35780;&#20272;&#32467;&#26524;&#23384;&#22312;&#22122;&#22768;&#12290;&#38500;&#20102;&#25253;&#21578;&#24179;&#22343;&#24615;&#33021;&#25351;&#26631;&#22806;&#65292;&#25253;&#21578;&#32622;&#20449;&#21306;&#38388;&#20063;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#65292;&#24456;&#23569;&#26377;&#20154;&#36825;&#26679;&#20570;&#12290;&#32622;&#20449;&#21306;&#38388;&#30340;&#23485;&#24230;&#21462;&#20915;&#20110;&#27979;&#35797;&#38598;&#22823;&#23567;&#21644;&#24615;&#33021;&#25351;&#26631;&#30340;&#25955;&#24067;&#31243;&#24230;&#65288;&#21363;&#27979;&#35797;&#38598;&#19978;&#30340;&#26631;&#20934;&#24046;&#65289;&#12290;&#23545;&#20110;&#20998;&#31867;&#38382;&#39064;&#65292;&#38656;&#35201;&#35768;&#22810;&#27979;&#35797;&#22270;&#20687;&#20197;&#36991;&#20813;&#23485;&#27867;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20998;&#21106;&#38382;&#39064;&#65292;&#36825;&#20010;&#24773;&#20917;&#23578;&#26410;&#30740;&#31350;&#65292;&#22240;&#20026;&#32473;&#23450;&#30340;&#27979;&#35797;&#22270;&#20687;&#25152;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#19981;&#21516;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#20856;&#22411;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#30340;nnU-net&#26694;&#26550;&#22312;&#20004;&#20010;&#26469;&#33258;Medical Decathlon&#25361;&#25112;&#36187;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;3D&#22270;&#20687;&#20998;&#21106;&#30340;&#23454;&#39564;&#65292;&#24182;&#20351;&#29992;Dice&#20934;&#30830;&#24230;&#21644;Hausdorff&#36317;&#31163;&#20004;&#20010;&#24615;&#33021;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;&#21442;&#25968;&#32622;&#20449;&#21306;&#38388;&#30340;&#23485;&#24230;&#19982;&#20998;&#21106;&#38382;&#39064;&#30340;&#29305;&#28857;&#26377;&#20851;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#30740;&#31350;&#25165;&#33021;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical segmentation models are evaluated empirically. As such an evaluation is based on a limited set of example images, it is unavoidably noisy. Beyond a mean performance measure, reporting confidence intervals is thus crucial. However, this is rarely done in medical image segmentation. The width of the confidence interval depends on the test set size and on the spread of the performance measure (its standard-deviation across of the test set). For classification, many test images are needed to avoid wide confidence intervals. Segmentation, however, has not been studied, and it differs by the amount of information brought by a given test image. In this paper, we study the typical confidence intervals in medical image segmentation. We carry experiments on 3D image segmentation using the standard nnU-net framework, two datasets from the Medical Decathlon challenge and two performance measures: the Dice accuracy and the Hausdorff distance. We show that the parametric confidence intervals
&lt;/p&gt;</description></item></channel></rss>