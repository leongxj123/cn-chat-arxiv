<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35757;&#32451;&#35774;&#35745;&#65292;&#20174;&#32780;&#21033;&#29992;&#36741;&#21161;&#30340;&#20813;&#30123;&#33639;&#20809;&#22270;&#20687;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#21452;&#21521;&#21040;&#21333;&#21521;IHC&#22270;&#20687;&#30340;&#20219;&#21153;&#24863;&#30693;&#22495;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19979;&#28216;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07389</link><description>&lt;p&gt;
&#36741;&#21161;CycleGAN&#24341;&#23548;&#19979;&#30340;&#20174;&#21452;&#21521;&#21040;&#21333;&#21521;IHC&#22270;&#20687;&#30340;&#20219;&#21153;&#24863;&#30693;&#22495;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from Duplex to Monoplex IHC Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07389
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35757;&#32451;&#35774;&#35745;&#65292;&#20174;&#32780;&#21033;&#29992;&#36741;&#21161;&#30340;&#20813;&#30123;&#33639;&#20809;&#22270;&#20687;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#21452;&#21521;&#21040;&#21333;&#21521;IHC&#22270;&#20687;&#30340;&#20219;&#21153;&#24863;&#30693;&#22495;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19979;&#28216;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#20351;&#24471;&#20174;&#19968;&#20010;&#28304;&#22270;&#20687;&#22495;&#21040;&#19968;&#20010;&#22312;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#22495;&#30340;&#36716;&#25442;&#25104;&#20026;&#21487;&#33021;&#12290;&#34429;&#28982;Cycle&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20854;&#20013;&#30340;&#24490;&#29615;&#19968;&#33268;&#24615;&#32422;&#26463;&#20381;&#36182;&#20110;&#20004;&#20010;&#22495;&#20043;&#38388;&#23384;&#22312;&#21487;&#36870;&#26144;&#23556;&#30340;&#24773;&#20917;&#65292;&#32780;&#22312;&#26579;&#33394;&#21333;&#21521;&#21644;&#21452;&#21521;&#20813;&#30123;&#32452;&#21270;&#65288;IHC&#65289;&#26816;&#27979;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#36716;&#25442;&#19981;&#26159;&#36825;&#26679;&#30340;&#12290;&#38024;&#23545;&#20174;&#21518;&#32773;&#21040;&#21069;&#32773;&#30340;&#36716;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#35774;&#35745;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32422;&#26463;&#65292;&#21033;&#29992;&#19968;&#32452;&#20813;&#30123;&#33639;&#20809;&#65288;IF&#65289;&#22270;&#20687;&#20316;&#20026;&#36741;&#21161;&#30340;&#19981;&#37197;&#23545;&#22270;&#20687;&#22495;&#12290;&#22312;&#19979;&#28216;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07389v1 Announce Type: cross  Abstract: Generative models enable the translation from a source image domain where readily trained models are available to a target domain unseen during training. While Cycle Generative Adversarial Networks (GANs) are well established, the associated cycle consistency constrain relies on that an invertible mapping exists between the two domains. This is, however, not the case for the translation between images stained with chromogenic monoplex and duplex immunohistochemistry (IHC) assays. Focusing on the translation from the latter to the first, we propose - through the introduction of a novel training design, an alternative constrain leveraging a set of immunofluorescence (IF) images as an auxiliary unpaired image domain. Quantitative and qualitative results on a downstream segmentation task show the benefit of the proposed method in comparison to baseline approaches.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#31181;&#39044;&#35757;&#32451;&#21644;&#27867;&#21270;&#20219;&#21153;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#27169;&#24577;&#23545;&#20110;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.02500</link><description>&lt;p&gt;
&#28857;&#20113;&#38382;&#39064;:&#37325;&#26032;&#24605;&#32771;&#19981;&#21516;&#35266;&#27979;&#31354;&#38388;&#23545;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02500
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#31181;&#39044;&#35757;&#32451;&#21644;&#27867;&#21270;&#20219;&#21153;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#27169;&#24577;&#23545;&#20110;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#35266;&#27979;&#31354;&#38388;&#23545;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#19977;&#31181;&#20027;&#35201;&#27169;&#24577;&#65306;RGB&#65292;RGB-D&#21644;&#28857;&#20113;&#12290;&#36890;&#36807;&#22312;&#36229;&#36807;17&#20010;&#19981;&#21516;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#28041;&#21450;&#20004;&#20010;&#22522;&#20934;&#21644;&#20223;&#30495;&#22120;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#36235;&#21183;&#65306;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#35774;&#35745;&#65292;&#36890;&#24120;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;RGB&#21644;RGB-D&#30340;&#23545;&#24212;&#29289;&#12290;&#36825;&#22312;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20004;&#31181;&#24773;&#20917;&#19979;&#37117;&#26159;&#19968;&#33268;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#22312;&#30456;&#26426;&#35270;&#35282;&#12289;&#29031;&#26126;&#26465;&#20214;&#12289;&#22122;&#22768;&#27700;&#24179;&#21644;&#32972;&#26223;&#22806;&#35266;&#31561;&#21508;&#31181;&#20960;&#20309;&#21644;&#35270;&#35273;&#32447;&#32034;&#26041;&#38754;&#65292;&#37117;&#33021;&#25552;&#39640;&#31574;&#30053;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19977;&#32500;&#28857;&#20113;&#26159;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#26377;&#20215;&#20540;&#30340;&#35266;&#27979;&#27169;&#24577;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#25152;&#26377;&#30340;&#20195;&#30721;&#21644;&#26816;&#26597;&#28857;&#65292;&#24076;&#26395;&#25105;&#20204;&#30340;&#35266;&#28857;&#33021;&#24110;&#21161;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the influence of different observation spaces on robot learning, focusing on three predominant modalities: RGB, RGB-D, and point cloud. Through extensive experimentation on over 17 varied contact-rich manipulation tasks, conducted across two benchmarks and simulators, we have observed a notable trend: point cloud-based methods, even those with the simplest designs, frequently surpass their RGB and RGB-D counterparts in performance. This remains consistent in both scenarios: training from scratch and utilizing pretraining. Furthermore, our findings indicate that point cloud observations lead to improved policy zero-shot generalization in relation to various geometry and visual clues, including camera viewpoints, lighting conditions, noise levels and background appearance. The outcomes suggest that 3D point cloud is a valuable observation modality for intricate robotic tasks. We will open-source all our codes and checkpoints, hoping that our insights can help de
&lt;/p&gt;</description></item><item><title>StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.17042</link><description>&lt;p&gt;
StochGradAdam: &#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling. (arXiv:2310.17042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17042
&lt;/p&gt;
&lt;p&gt;
StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#39046;&#22495;&#20013;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;StochGradAdam&#20248;&#21270;&#22120;&#65292;&#36825;&#26159;&#23545;&#24191;&#21463;&#36190;&#35465;&#30340;Adam&#31639;&#27861;&#30340;&#26032;&#39062;&#25913;&#36827;&#12290;StochGradAdam&#30340;&#26680;&#24515;&#26159;&#20854;&#26799;&#24230;&#25277;&#26679;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#30830;&#20445;&#31283;&#23450;&#25910;&#25947;&#65292;&#32780;&#19988;&#21033;&#29992;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#20943;&#36731;&#22122;&#22768;&#25110;&#24322;&#24120;&#25968;&#25454;&#30340;&#24433;&#21709;&#21644;&#22686;&#24378;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#25552;&#21319;&#20102;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;StochGradAdam&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;Adam&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#31934;&#24515;&#36873;&#25321;&#19968;&#37096;&#20998;&#26799;&#24230;&#36827;&#34892;&#25277;&#26679;&#65292;&#35813;&#20248;&#21270;&#22120;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22797;&#26434;&#27169;&#22411;&#30340;&#31649;&#29702;&#12290;&#26412;&#25991;&#20174;&#25968;&#23398;&#22522;&#30784;&#21040;&#20559;&#24046;&#26657;&#27491;&#31574;&#30053;&#20840;&#38754;&#25506;&#35752;&#20102;StochGradAdam&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#25216;&#26415;&#30340;&#21487;&#26399;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing domain of deep learning optimization, this paper unveils the StochGradAdam optimizer, a novel adaptation of the well-regarded Adam algorithm. Central to StochGradAdam is its gradient sampling technique. This method not only ensures stable convergence but also leverages the advantages of selective gradient consideration, fostering robust training by potentially mitigating the effects of noisy or outlier data and enhancing the exploration of the loss landscape for more dependable convergence. In both image classification and segmentation tasks, StochGradAdam has demonstrated superior performance compared to the traditional Adam optimizer. By judiciously sampling a subset of gradients at each iteration, the optimizer is optimized for managing intricate models. The paper provides a comprehensive exploration of StochGradAdam's methodology, from its mathematical foundations to bias correction strategies, heralding a promising advancement in deep learning training tec
&lt;/p&gt;</description></item></channel></rss>