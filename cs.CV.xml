<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;nnU-Net&#30340;&#30417;&#30563;&#26041;&#27861;&#26469;&#20248;&#21270;&#24515;&#23460;&#21521;&#37327;&#27969;&#21160;&#26144;&#23556;&#65292;&#25928;&#26524;&#19982;&#20256;&#32479;&#31639;&#27861;&#30456;&#24403;&#65292;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13040</link><description>&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24515;&#23460;&#21521;&#37327;&#27969;&#21160;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13040
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;nnU-Net&#30340;&#30417;&#30563;&#26041;&#27861;&#26469;&#20248;&#21270;&#24515;&#23460;&#21521;&#37327;&#27969;&#21160;&#26144;&#23556;&#65292;&#25928;&#26524;&#19982;&#20256;&#32479;&#31639;&#27861;&#30456;&#24403;&#65292;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Intraventricular vector flow mapping (iVFM)&#26088;&#22312;&#22686;&#24378;&#21644;&#37327;&#21270;&#24515;&#33039;&#25104;&#20687;&#20013;&#30340;&#24425;&#33394;&#22810;&#26222;&#21202;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21644;&#22522;&#20110;&#29289;&#29702;&#24341;&#23548;&#30340;nnU-Net&#30417;&#30563;&#26041;&#27861;&#26469;&#20248;&#21270;&#20256;&#32479;&#30340;iVFM&#20248;&#21270;&#26041;&#26696;&#12290;&#36890;&#36807;&#23545;&#22522;&#20110;&#24739;&#32773;&#29305;&#23450;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#22411;&#20135;&#29983;&#30340;&#27169;&#25311;&#24425;&#33394;&#22810;&#26222;&#21202;&#22270;&#20687;&#21644;&#20307;&#20869;&#22810;&#26222;&#21202;&#37319;&#38598;&#30340;&#20005;&#26684;&#35780;&#20272;&#65292;&#20004;&#31181;&#26041;&#27861;&#22343;&#23637;&#29616;&#20986;&#19982;&#21407;&#22987;iVFM&#31639;&#27861;&#30456;&#24403;&#30340;&#37325;&#24314;&#24615;&#33021;&#12290; PINNs&#30340;&#25928;&#29575;&#36890;&#36807;&#21452;&#38454;&#27573;&#20248;&#21270;&#21644;&#39044;&#20248;&#21270;&#26435;&#37325;&#24471;&#21040;&#25552;&#21319;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;nnU-Net&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#21644;&#23454;&#26102;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;nnU-Net&#22312;&#31232;&#30095;&#21644;&#25130;&#26029;&#22810;&#26222;&#21202;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#29420;&#31435;&#20110;&#26126;&#30830;&#30340;&#36793;&#30028;&#26465;&#20214;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13040v1 Announce Type: cross  Abstract: Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify color Doppler in cardiac imaging. In this study, we propose novel alternatives to the traditional iVFM optimization scheme by utilizing physics-informed neural networks (PINNs) and a physics-guided nnU-Net-based supervised approach. Through rigorous evaluation on simulated color Doppler images derived from a patient-specific computational fluid dynamics model and in vivo Doppler acquisitions, both approaches demonstrate comparable reconstruction performance to the original iVFM algorithm. The efficiency of PINNs is boosted through dual-stage optimization and pre-optimized weights. On the other hand, the nnU-Net method excels in generalizability and real time capabilities. Notably, nnU-Net shows superior robustness on sparse and truncated Doppler data while maintaining independence from explicit boundary conditions. Overall, our results highlight the effectiveness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#21069;&#27839;&#27169;&#22411;&#23578;&#23384;&#22312;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#24046;&#36317;&#65292;&#25506;&#35752;&#20102;&#35757;&#32451;&#24320;&#28304;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20197;&#24357;&#34917;&#20020;&#24202;&#38656;&#27714;&#30340;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.08002</link><description>&lt;p&gt;
&#35757;&#32451;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20197;&#22635;&#34917;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#65306;&#20197;&#25918;&#23556;&#23398;&#25104;&#20687;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#21069;&#27839;&#27169;&#22411;&#23578;&#23384;&#22312;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#24046;&#36317;&#65292;&#25506;&#35752;&#20102;&#35757;&#32451;&#24320;&#28304;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20197;&#24357;&#34917;&#20020;&#24202;&#38656;&#27714;&#30340;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#22823;&#22522;&#30784;&#27169;&#22411;&#30340;&#23610;&#24230;&#35268;&#24459;&#21644;&#38750;&#20961;&#34920;&#29616;&#28608;&#21169;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#24320;&#21457;&#21644;&#21033;&#29992;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#19968;&#20123;&#29983;&#29289;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26089;&#26399;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20043;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#37325;&#22823;&#25361;&#25112;&#12290;&#20687;GPT-4V&#36825;&#26679;&#30340;&#21069;&#27839;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#20173;&#23384;&#22312;&#37325;&#22823;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#35775;&#38382;&#12289;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#21512;&#35268;&#31561;&#23454;&#38469;&#38382;&#39064;&#20351;&#20020;&#24202;&#21307;&#29983;&#38590;&#20197;&#30452;&#25509;&#22312;&#31169;&#20154;&#24739;&#32773;&#25968;&#25454;&#19978;&#20351;&#29992;&#31169;&#20154;&#25176;&#31649;&#30340;&#26368;&#20808;&#36827;&#22823;&#22411;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#35757;&#32451;&#24320;&#28304;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;SMMs&#65289;&#26469;&#22635;&#34917;&#26410;&#28385;&#36275;&#30340;&#20020;&#24202;&#38656;&#27714;&#30340;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#37319;&#29992;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#23558;&#29992;&#20110;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#26368;&#20808;&#36827;&#39044;&#35757;&#32451;&#27169;&#22411;&#32435;&#20837;&#65292;&#24182;&#20391;&#37325;&#20110;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08002v1 Announce Type: new  Abstract: The scaling laws and extraordinary performance of large foundation models motivate the development and utilization of such large models in biomedicine. However, despite early promising results on some biomedical benchmarks, there are still major challenges that need to be addressed before these models can be used in real-world applications. Frontier models such as GPT-4V still have major competency gaps in multimodal capabilities for biomedical applications. Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data. In this paper, we explore training open-source small multimodal models (SMMs) to bridge biomedical competency gaps for unmet clinical needs. To maximize data efficiency, we adopt a modular approach by incorporating state-of-the-art pre-trained models for image and text modalities, and focusing on t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#25463;&#24452;&#23398;&#20064;&#29616;&#35937;&#25193;&#23637;&#21040;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#65292;&#21457;&#29616;&#20020;&#24202;&#27880;&#37322;&#21644;&#29305;&#23450;&#25968;&#25454;&#22788;&#29702;&#26041;&#24335;&#21487;&#33021;&#35823;&#23548;&#27169;&#22411;&#24182;&#24433;&#21709;&#20998;&#21106;&#20934;&#30830;&#24615;&#65292;&#25552;&#20986;&#20102;&#32531;&#35299;&#25463;&#24452;&#23398;&#20064;&#24433;&#21709;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.06748</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#25463;&#24452;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Shortcut Learning in Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#25463;&#24452;&#23398;&#20064;&#29616;&#35937;&#25193;&#23637;&#21040;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#65292;&#21457;&#29616;&#20020;&#24202;&#27880;&#37322;&#21644;&#29305;&#23450;&#25968;&#25454;&#22788;&#29702;&#26041;&#24335;&#21487;&#33021;&#35823;&#23548;&#27169;&#22411;&#24182;&#24433;&#21709;&#20998;&#21106;&#20934;&#30830;&#24615;&#65292;&#25552;&#20986;&#20102;&#32531;&#35299;&#25463;&#24452;&#23398;&#20064;&#24433;&#21709;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25463;&#24452;&#23398;&#20064;&#26159;&#19968;&#31181;&#29616;&#35937;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20248;&#20808;&#23398;&#20064;&#31616;&#21333;&#12289;&#28508;&#22312;&#35823;&#23548;&#30340;&#25968;&#25454;&#25552;&#31034;&#65292;&#36825;&#20123;&#25552;&#31034;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#24456;&#38590;&#27867;&#21270;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#25506;&#35752;&#36825;&#19968;&#29616;&#35937;&#22312;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#65292;&#32780;&#36825;&#39033;&#30740;&#31350;&#23558;&#25463;&#24452;&#23398;&#20064;&#25506;&#32034;&#24310;&#20280;&#21040;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20020;&#24202;&#27880;&#37322;&#22914;&#21345;&#23610;&#65292;&#20197;&#21450;&#25968;&#25454;&#38598;&#20013;&#38646;&#22635;&#20805;&#21367;&#31215;&#21644;&#20013;&#24515;&#35009;&#21098;&#30340;&#32452;&#21512;&#21487;&#20197;&#26080;&#24847;&#20013;&#20316;&#20026;&#25463;&#24452;&#65292;&#24433;&#21709;&#20998;&#21106;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#20294;&#24120;&#35265;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#35782;&#21035;&#21644;&#35780;&#20272;&#20102;&#25463;&#24452;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32531;&#35299;&#25463;&#24452;&#23398;&#20064;&#24433;&#21709;&#12289;&#25552;&#39640;&#20998;&#21106;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#36890;&#36807;&#25581;&#31034;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#25463;&#24452;&#30340;&#23384;&#22312;&#21644;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06748v1 Announce Type: cross  Abstract: Shortcut learning is a phenomenon where machine learning models prioritize learning simple, potentially misleading cues from data that do not generalize well beyond the training set. While existing research primarily investigates this in the realm of image classification, this study extends the exploration of shortcut learning into medical image segmentation. We demonstrate that clinical annotations such as calipers, and the combination of zero-padded convolutions and center-cropped training sets in the dataset can inadvertently serve as shortcuts, impacting segmentation accuracy. We identify and evaluate the shortcut learning on two different but common medical image segmentation tasks. In addition, we suggest strategies to mitigate the influence of shortcut learning and improve the generalizability of the segmentation models. By uncovering the presence and implications of shortcuts in medical image segmentation, we provide insights a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35748;&#20026;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#31649;&#29702;&#24335;&#30417;&#31649;&#26041;&#27861;&#20013;&#65292;&#21152;&#24378;&#20154;&#31867;&#24341;&#23548;&#21644;&#22521;&#35757;&#25216;&#26415;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#23545;&#20110;&#25552;&#39640;AI&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#25216;&#26415;&#21644;&#20262;&#29702;&#38382;&#39064;&#31561;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.08466</link><description>&lt;p&gt;
&#35748;&#30495;&#23545;&#24453;&#22521;&#35757;&#65306;&#20154;&#24037;&#26234;&#33021;&#30340;&#20154;&#31867;&#24341;&#23548;&#19982;&#22522;&#20110;&#31649;&#29702;&#30340;&#30417;&#31649;
&lt;/p&gt;
&lt;p&gt;
Taking Training Seriously: Human Guidance and Management-Based Regulation of Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08466
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35748;&#20026;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#31649;&#29702;&#24335;&#30417;&#31649;&#26041;&#27861;&#20013;&#65292;&#21152;&#24378;&#20154;&#31867;&#24341;&#23548;&#21644;&#22521;&#35757;&#25216;&#26415;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#23545;&#20110;&#25552;&#39640;AI&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#25216;&#26415;&#21644;&#20262;&#29702;&#38382;&#39064;&#31561;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30456;&#20851;&#21361;&#23475;&#26356;&#24378;&#22823;&#30340;&#27835;&#29702;&#30340;&#28909;&#24773;&#21628;&#22768;&#27491;&#22312;&#19990;&#30028;&#33539;&#22260;&#20869;&#24341;&#36215;&#31649;&#29702;&#23398;&#32773;&#25152;&#31216;&#30340;&#22522;&#20110;&#31649;&#29702;&#30340;&#30417;&#31649;&#26041;&#27861;&#30340;&#37319;&#29992;&#12290;&#32654;&#22269;&#21644;&#27431;&#27954;&#30340;&#26368;&#26032;&#20513;&#35758;&#20197;&#21450;&#22269;&#38469;&#26631;&#20934;&#21270;&#32452;&#32455;&#37319;&#32435;&#30340;&#37325;&#35201;&#33258;&#25105;&#30417;&#31649;&#26631;&#20934;&#37117;&#20849;&#21516;&#20855;&#26377;&#19968;&#20010;&#26680;&#24515;&#30340;&#22522;&#20110;&#31649;&#29702;&#30340;&#33539;&#24335;&#12290;&#36825;&#20123;&#22522;&#20110;&#31649;&#29702;&#30340;&#20513;&#35758;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#20154;&#31867;&#23545;AI&#24037;&#20855;&#30340;&#22521;&#35757;&#21644;&#24320;&#21457;&#30340;&#30417;&#30563;&#26469;&#28608;&#21169;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#20010;&#26032;&#20852;&#30340;&#22522;&#20110;&#31649;&#29702;&#30340;&#30417;&#31649;&#33539;&#24335;&#26102;&#20195;&#20013;&#65292;&#38656;&#35201;&#23545;&#20154;&#31867;&#24341;&#23548;&#22521;&#35757;&#25216;&#26415;&#36827;&#34892;&#23436;&#21892;&#21644;&#31995;&#32479;&#21270;&#12290;&#22914;&#26524;&#35748;&#30495;&#23545;&#24453;&#65292;&#20154;&#31867;&#24341;&#23548;&#22521;&#35757;&#21487;&#20197;&#20943;&#36731;&#19968;&#20123;&#23545;AI&#30340;&#25216;&#26415;&#21644;&#20262;&#29702;&#21387;&#21147;&#65292;&#20197;&#20154;&#31867;&#30452;&#35273;&#25552;&#39640;AI&#30340;&#24615;&#33021;&#65292;&#24182;&#26356;&#22909;&#22320;&#28385;&#36275;&#23545;&#20844;&#24179;&#24615;&#21644;&#26377;&#25928;&#35299;&#37322;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fervent calls for more robust governance of the harms associated with artificial intelligence (AI) are leading to the adoption around the world of what regulatory scholars have called a management-based approach to regulation. Recent initiatives in the United States and Europe, as well as the adoption of major self-regulatory standards by the International Organization for Standardization, share in common a core management-based paradigm. These management-based initiatives seek to motivate an increase in human oversight of how AI tools are trained and developed. Refinements and systematization of human-guided training techniques will thus be needed to fit within this emerging era of management-based regulatory paradigm. If taken seriously, human-guided training can alleviate some of the technical and ethical pressures on AI, boosting AI performance with human intuition as well as better addressing the needs for fairness and effective explainability. In this paper, we discuss the connec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#22270;&#20687;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.04929</link><description>&lt;p&gt;
&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#25193;&#25955;&#24341;&#23548;&#28304;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#22270;&#20687;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;DM-SFDA&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;DM-SFDA&#26041;&#27861;&#21253;&#25324;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#30446;&#26631;&#22270;&#20687;&#30340;&#29305;&#24449;&#26469;&#25351;&#23548;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#28304;&#22495;&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#34987;&#24494;&#35843;&#20197;&#29983;&#25104;&#26368;&#23567;&#21270;&#29109;&#24182;&#26368;&#22823;&#21270;&#39044;&#35757;&#32451;&#28304;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#28304;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#24050;&#24314;&#31435;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#29983;&#25104;&#30340;&#28304;&#22270;&#20687;&#19982;&#30446;&#26631;&#22495;&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;Office-31&#12289;Office-Home&#21644;VisDA&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#12289;&#39046;&#22495;&#29305;&#23450;&#30340;&#22270;&#20687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process. Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model. We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data. We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA. The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images.
&lt;/p&gt;</description></item><item><title>AdaTreeFormer&#26159;&#19968;&#31181;&#20174;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.02956</link><description>&lt;p&gt;
AdaTreeFormer: &#20174;&#19968;&#24352;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#36827;&#34892;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02956
&lt;/p&gt;
&lt;p&gt;
AdaTreeFormer&#26159;&#19968;&#31181;&#20174;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#19968;&#24352;&#33322;&#31354;&#25110;&#21355;&#26143;&#22270;&#20687;&#26469;&#20272;&#35745;&#21644;&#35745;&#25968;&#26641;&#26408;&#23494;&#24230;&#26159;&#25668;&#24433;&#27979;&#37327;&#21644;&#36965;&#24863;&#39046;&#22495;&#20013;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26862;&#26519;&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19981;&#21516;&#22320;&#24418;&#19978;&#21508;&#31181;&#21508;&#26679;&#30340;&#26641;&#26408;&#31181;&#31867;&#20005;&#37325;&#38459;&#30861;&#20102;&#26641;&#26408;&#35745;&#25968;&#27169;&#22411;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#20010;&#20174;&#20855;&#26377;&#36275;&#22815;&#26631;&#27880;&#26641;&#26408;&#30340;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;AdaTreeFormer&#65292;&#21253;&#21547;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#29992;&#20110;&#20174;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20013;&#25552;&#21462;&#31283;&#20581;&#30340;&#29305;&#24449;&#12290;&#23427;&#36824;&#21253;&#25324;&#19977;&#20010;&#23376;&#32593;&#32476;&#65306;&#20004;&#20010;&#29992;&#20110;&#20998;&#21035;&#20174;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#25552;&#21462;&#33258;&#27880;&#24847;&#21147;&#22270;&#65292;&#24182;&#19968;&#20010;&#29992;&#20110;&#25552;&#21462;&#36328;&#39046;&#22495;&#27880;&#24847;&#21147;&#22270;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#36866;&#24212;&#26426;&#21046;&#65292;&#29992;&#20110;&#20174;&#19981;&#21516;&#39046;&#22495;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of estimating and counting tree density using only a single aerial or satellite image is a difficult task in the fields of photogrammetry and remote sensing. However, it plays a crucial role in the management of forests. The huge variety of trees in varied topography severely hinders tree counting models to perform well. The purpose of this paper is to propose a framework that is learnt from the source domain with sufficient labeled trees and is adapted to the target domain with only a limited number of labeled trees. Our method, termed as AdaTreeFormer, contains one shared encoder with a hierarchical feature extraction scheme to extract robust features from the source and target domains. It also consists of three subnets: two for extracting self-domain attention maps from source and target domains respectively and one for extracting cross-domain attention maps. For the latter, an attention-to-adapt mechanism is introduced to distill relevant information from different doma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23545;&#24515;&#30005;&#22270;&#35786;&#26029;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#24494;&#35843;&#23545;&#20110;&#23567;&#22411;&#25968;&#25454;&#38598;&#26159;&#36739;&#22909;&#30340;&#36873;&#25321;&#65292;&#24403;&#25968;&#25454;&#38598;&#36275;&#22815;&#22823;&#26102;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#21487;&#27604;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#21516;&#26102;&#65292;&#36801;&#31227;&#23398;&#20064;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02021</link><description>&lt;p&gt;
ECG&#35786;&#26029;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#26377;&#25928;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning in ECG Diagnosis: Is It Effective?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23545;&#24515;&#30005;&#22270;&#35786;&#26029;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#24494;&#35843;&#23545;&#20110;&#23567;&#22411;&#25968;&#25454;&#38598;&#26159;&#36739;&#22909;&#30340;&#36873;&#25321;&#65292;&#24403;&#25968;&#25454;&#38598;&#36275;&#22815;&#22823;&#26102;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#21487;&#27604;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#21516;&#26102;&#65292;&#36801;&#31227;&#23398;&#20064;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#24515;&#30005;&#22270;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#24448;&#24448;&#21463;&#21040;&#22823;&#35268;&#27169;&#12289;&#26631;&#35760;&#33391;&#22909;&#30340;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#30340;&#38480;&#21046;&#65292;&#22240;&#27492;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#21033;&#29992;&#20174;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#21040;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36801;&#31227;&#23398;&#20064;&#22987;&#32456;&#20248;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#26222;&#36941;&#20551;&#35774;&#20174;&#26410;&#34987;&#31995;&#32479;&#39564;&#35777;&#36807;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#22810;&#26631;&#31614;&#24515;&#30005;&#22270;&#20998;&#31867;&#20013;&#36827;&#34892;&#24494;&#35843;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24515;&#30005;&#22270;&#25968;&#25454;&#38598;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#24191;&#27867;&#30340;&#32463;&#39564;&#24615;&#30740;&#31350;&#26469;&#39564;&#35777;&#36801;&#31227;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35777;&#23454;&#65292;&#23545;&#20110;&#23567;&#22411;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#26469;&#35828;&#65292;&#24494;&#35843;&#26159;&#26356;&#22909;&#30340;&#36873;&#25321;&#65307;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#38598;&#36275;&#22815;&#22823;&#26102;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#21487;&#27604;&#24615;&#33021;&#65292;&#23613;&#31649;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#26469;&#36814;&#22836;&#36214;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#36801;&#31227;&#23398;&#20064;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26356;&#22909;&#30340;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of deep learning in ECG diagnosis is often hindered by the scarcity of large, well-labeled datasets in real-world scenarios, leading to the use of transfer learning to leverage features learned from larger datasets. Yet the prevailing assumption that transfer learning consistently outperforms training from scratch has never been systematically validated. In this study, we conduct the first extensive empirical study on the effectiveness of transfer learning in multi-label ECG classification, by investigating comparing the fine-tuning performance with that of training from scratch, covering a variety of ECG datasets and deep neural networks. We confirm that fine-tuning is the preferable choice for small downstream datasets; however, when the dataset is sufficiently large, training from scratch can achieve comparable performance, albeit requiring a longer training time to catch up. Furthermore, we find that transfer learning exhibits better compatibility with convolutional ne
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22810;&#38754;&#26495;&#35270;&#35273;&#38382;&#31572;&#65288;MultipanelVQA&#65289;&#22522;&#20934;&#25361;&#25112;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#23545;&#29702;&#35299;&#22810;&#38754;&#26495;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;LVLMs&#22312;&#36825;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.15847</link><description>&lt;p&gt;
&#26494;&#39292;&#36824;&#26159;&#21513;&#23043;&#23043;&#65311;&#29992;&#22810;&#38754;&#26495;VQA&#25361;&#25112;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15847
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22810;&#38754;&#26495;&#35270;&#35273;&#38382;&#31572;&#65288;MultipanelVQA&#65289;&#22522;&#20934;&#25361;&#25112;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#23545;&#29702;&#35299;&#22810;&#38754;&#26495;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;LVLMs&#22312;&#36825;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#38754;&#26495;&#22270;&#20687;&#65292;&#36890;&#24120;&#22312;&#32593;&#39029;&#25130;&#22270;&#12289;&#28023;&#25253;&#31561;&#20013;&#30475;&#21040;&#65292;&#20805;&#26021;&#30528;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#12290;&#36825;&#20123;&#22270;&#20687;&#20197;&#22810;&#20010;&#23376;&#22270;&#20197;&#19981;&#21516;&#24067;&#23616;&#32452;&#25104;&#65292;&#26377;&#25928;&#22320;&#21521;&#20154;&#20204;&#20256;&#36798;&#20449;&#24687;&#12290;&#20026;&#20102;&#26500;&#24314;&#39640;&#32423;&#30340;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#22914;&#33021;&#29702;&#35299;&#22797;&#26434;&#22330;&#26223;&#24182;&#22312;&#32593;&#39029;&#20013;&#23548;&#33322;&#30340;&#20195;&#29702;&#31243;&#24207;&#65292;&#22810;&#38754;&#26495;&#35270;&#35273;&#25512;&#29702;&#30340;&#25216;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#23545;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#38754;&#26495;&#35270;&#35273;&#38382;&#31572;&#65288;MultipanelVQA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6,600&#20010;&#38382;&#39064;&#12289;&#31572;&#26696;&#21644;&#22810;&#38754;&#26495;&#22270;&#20687;&#19977;&#20803;&#32452;&#65292;&#19987;&#38376;&#25361;&#25112;&#27169;&#22411;&#29702;&#35299;&#22810;&#38754;&#26495;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;MultipanelVQA&#22522;&#20934;&#20013;&#30340;&#38382;&#39064;&#23545;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#21363;&#20351;&#20154;&#31867;&#21487;&#20197;&#33719;&#24471;&#32422;99%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15847v2 Announce Type: replace-cross  Abstract: Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Large Vision Language Models (LVLMs) tested, even though humans can attain approximately 99\% accurac
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;</title><link>https://arxiv.org/abs/2311.16480</link><description>&lt;p&gt;
&#30149;&#29702;&#25253;&#21578;&#30340;&#22810;&#23454;&#20363;&#29983;&#25104;&#29992;&#20110;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
WsiCaption: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16480
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#20999;&#29255;&#22270;&#20687;&#26159;&#29992;&#20110;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#25968;&#23383;&#30149;&#29702;&#23398;&#30340;&#22522;&#30784;&#12290;&#25776;&#20889;&#30149;&#29702;&#25253;&#21578;&#23545;&#32463;&#39564;&#19981;&#36275;&#30340;&#30149;&#29702;&#23398;&#23478;&#26469;&#35828;&#26159;&#36153;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#12290;&#20026;&#20102;&#20943;&#23569;&#24037;&#20316;&#37327;&#24182;&#25913;&#21892;&#20020;&#24202;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#29983;&#25104;&#32473;&#23450;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;&#22312;&#25968;&#25454;&#31471;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#26368;&#22823;&#30340;WSI-&#25991;&#26412;&#25968;&#25454;&#38598;&#65288;TCGA-PathoText&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#21644;&#28165;&#29702;TCGA&#20013;&#21465;&#36848;&#35786;&#26029;&#24187;&#28783;&#29255;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#25910;&#38598;&#20102;&#36817;1&#19975;&#23545;&#39640;&#36136;&#37327;&#30340;WSI-&#25991;&#26412;&#37197;&#23545;&#65292;&#20379;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#12290;&#22312;&#27169;&#22411;&#31471;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20197;&#20026;&#21315;&#20159;&#20687;&#32032;WSI&#29983;&#25104;&#30149;&#29702;&#25253;&#21578;&#30340;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#65288;MI-Gen&#65289;&#12290;&#25105;&#20204;&#22312;TCGA-PathoText&#30340;&#26368;&#22823;&#23376;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;&#27492;&#22806;&#65292;WSI-&#25991;&#26412;&#39044;&#27979;&#21487;&#34987;&#35270;&#20026;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16480v2 Announce Type: replace-cross  Abstract: Whole slide images are the foundation of digital pathology for the diagnosis and treatment of carcinomas. Writing pathology reports is laborious and error-prone for inexperienced pathologists. To reduce the workload and improve clinical automation, we investigate how to generate pathology reports given whole slide images. On the data end, we curated the largest WSI-text dataset (TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text pairs for visual-language models by recognizing and cleaning pathology reports which narrate diagnostic slides in TCGA. On the model end, we propose the multiple instance generative model (MI-Gen) which can produce pathology reports for gigapixel WSIs. We benchmark our model on the largest subset of TCGA-PathoText. Experimental results show our model can generate pathology reports which contain multiple clinical clues. Furthermore, WSI-text prediction can be seen as an approac
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25361;&#25112;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#33539;&#24335;&#65292;&#36890;&#36807;&#30740;&#31350;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#22120;&#36873;&#25321;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#29702;&#35770;&#25512;&#23548;&#20986;&#20102;&#26799;&#24230;&#20914;&#31361;&#30340;&#35282;&#33394;&#12290;</title><link>https://arxiv.org/abs/2311.04698</link><description>&lt;p&gt;
&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#25361;&#25112;&#24120;&#35265;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Challenging Common Paradigms in Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04698
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25361;&#25112;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#33539;&#24335;&#65292;&#36890;&#36807;&#30740;&#31350;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#22120;&#36873;&#25321;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#29702;&#35770;&#25512;&#23548;&#20986;&#20102;&#26799;&#24230;&#20914;&#31361;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#20294;&#20854;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#24182;&#26410;&#24102;&#26469;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#30456;&#27604;&#21333;&#20219;&#21153;&#23398;&#20064;&#65288;STL&#65289;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#26356;&#28145;&#20837;&#20102;&#35299;MTL&#29305;&#23450;&#25361;&#25112;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;MTL&#20013;&#30340;&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#20960;&#28857;&#20851;&#20110;STL&#30340;&#37325;&#35201;&#24433;&#21709;&#65306;&#39318;&#20808;&#65292;&#20248;&#21270;&#22120;&#30340;&#36873;&#25321;&#23545;MTL&#30340;&#24433;&#21709;&#21482;&#21463;&#21040;&#20102;&#36731;&#24494;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#30340;&#23454;&#35777;&#26041;&#27861;&#23637;&#31034;&#20102;&#24120;&#35265;STL&#24037;&#20855;&#65288;&#20363;&#22914;Adam&#20248;&#21270;&#22120;&#65289;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;Adam&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#19968;&#23450;&#30340;&#20551;&#35774;&#19979;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#37096;&#20998;&#25439;&#22833;&#23610;&#24230;&#19981;&#21464;&#24615;&#12290;&#20854;&#27425;&#65292;&#26799;&#24230;&#20914;&#31361;&#30340;&#27010;&#24565;&#32463;&#24120;&#34987;&#25551;&#36848;&#20026;MTL&#20013;&#30340;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#26799;&#24230;&#20914;&#31361;&#22312;MTL&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#23558;&#20854;&#19982;STL&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#35282;&#24230;&#26799;&#24230;&#23545;&#40784;&#26041;&#38754;&#65292;&#25105;&#20204;&#27809;&#26377;&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04698v3 Announce Type: replace-cross  Abstract: While multi-task learning (MTL) has gained significant attention in recent years, its underlying mechanisms remain poorly understood. Recent methods did not yield consistent performance improvements over single task learning (STL) baselines, underscoring the importance of gaining more profound insights about challenges specific to MTL. In our study, we challenge paradigms in MTL in the context of STL: First, the impact of the choice of optimizer has only been mildly investigated in MTL. We show the pivotal role of common STL tools such as the Adam optimizer in MTL empirically in various experiments. To further investigate Adam's effectiveness, we theoretical derive a partial loss-scale invariance under mild assumptions. Second, the notion of gradient conflicts has often been phrased as a specific problem in MTL. We delve into the role of gradient conflicts in MTL and compare it to STL. For angular gradient alignment we find no 
&lt;/p&gt;</description></item><item><title>MixerFlow&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;MLP-Mixer&#26550;&#26500;&#30340;&#27491;&#21017;&#21270;&#27969;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#20379;&#26377;&#25928;&#30340;&#26435;&#37325;&#20849;&#20139;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#22270;&#20687;&#23494;&#24230;&#20272;&#35745;&#24615;&#33021;&#21644;&#26356;&#20016;&#23500;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.16777</link><description>&lt;p&gt;
&#22270;&#20687;&#24314;&#27169;&#30340;MixerFlow
&lt;/p&gt;
&lt;p&gt;
MixerFlow for Image Modelling. (arXiv:2310.16777v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16777
&lt;/p&gt;
&lt;p&gt;
MixerFlow&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;MLP-Mixer&#26550;&#26500;&#30340;&#27491;&#21017;&#21270;&#27969;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#20379;&#26377;&#25928;&#30340;&#26435;&#37325;&#20849;&#20139;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#22270;&#20687;&#23494;&#24230;&#20272;&#35745;&#24615;&#33021;&#21644;&#26356;&#20016;&#23500;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#27969;&#26159;&#19968;&#31181;&#32479;&#35745;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#23556;&#21464;&#25442;&#23558;&#22797;&#26434;&#23494;&#24230;&#36716;&#25442;&#20026;&#31616;&#21333;&#23494;&#24230;&#65292;&#23454;&#29616;&#20102;&#23494;&#24230;&#20272;&#35745;&#21644;&#20174;&#21333;&#20010;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#30340;&#21151;&#33021;&#12290;&#22312;&#22270;&#20687;&#24314;&#27169;&#30340;&#32972;&#26223;&#19979;&#65292;&#20027;&#35201;&#36873;&#25321;&#30340;&#26159;&#22522;&#20110;Glow&#30340;&#26550;&#26500;&#65292;&#32780;&#20854;&#20182;&#26550;&#26500;&#22312;&#30740;&#31350;&#30028;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MLP-Mixer&#26550;&#26500;&#30340;&#26032;&#22411;&#26550;&#26500;MixerFlow&#65292;&#36827;&#19968;&#27493;&#32479;&#19968;&#20102;&#29983;&#25104;&#24615;&#21644;&#21028;&#21035;&#24615;&#24314;&#27169;&#26550;&#26500;&#12290;MixerFlow&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26435;&#37325;&#20849;&#20139;&#26426;&#21046;&#65292;&#36866;&#29992;&#20110;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22266;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#65292;MixerFlow&#22312;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#23494;&#24230;&#20272;&#35745;&#24615;&#33021;&#65292;&#24182;&#19988;&#38543;&#30528;&#22270;&#20687;&#20998;&#36776;&#29575;&#30340;&#22686;&#21152;&#65292;&#20854;&#24615;&#33021;&#20063;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#25193;&#23637;&#65292;&#20351;&#24471;MixerFlow&#25104;&#20026;Glow-based&#26550;&#26500;&#30340;&#19968;&#20010;&#24378;&#22823;&#32780;&#31616;&#21333;&#30340;&#26367;&#20195;&#21697;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;MixerFlow&#25552;&#20379;&#20102;&#27604;Glow-based&#26550;&#26500;&#26356;&#20016;&#23500;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalising flows are statistical models that transform a complex density into a simpler density through the use of bijective transformations enabling both density estimation and data generation from a single model. In the context of image modelling, the predominant choice has been the Glow-based architecture, whereas alternative architectures remain largely unexplored in the research community. In this work, we propose a novel architecture called MixerFlow, based on the MLP-Mixer architecture, further unifying the generative and discriminative modelling architectures. MixerFlow offers an effective mechanism for weight sharing for flow-based models. Our results demonstrate better density estimation on image datasets under a fixed computational budget and scales well as the image resolution increases, making MixeFlow a powerful yet simple alternative to the Glow-based architectures. We also show that MixerFlow provides more informative embeddings than Glow-based architectures.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoSAM Adapter&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;SAM&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#28040;&#38500;&#20102;&#23545;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.14936</link><description>&lt;p&gt;
&#20026;&#31227;&#21160;&#21451;&#22909;&#30340;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#33258;&#21160;&#25552;&#31034;SAM
&lt;/p&gt;
&lt;p&gt;
Auto-Prompting SAM for Mobile Friendly 3D Medical Image Segmentation. (arXiv:2308.14936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14936
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoSAM Adapter&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;SAM&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#28040;&#38500;&#20102;&#23545;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM)&#24050;&#32463;&#34987;&#36805;&#36895;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;SAM&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#38500;&#20102;&#33258;&#28982;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#22806;&#65292;2D&#21644;3D&#22270;&#20687;&#20043;&#38388;&#30340;&#31354;&#38388;&#24067;&#23616;&#24046;&#24322;&#65292;&#24378;&#22823;&#30340;GPU&#26381;&#21153;&#22120;&#25152;&#24102;&#26469;&#30340;&#22823;&#37327;&#35745;&#31639;&#36127;&#25285;&#65292;&#20197;&#21450;&#32791;&#26102;&#30340;&#25163;&#21160;&#25552;&#31034;&#29983;&#25104;&#20351;&#24471;SAM&#26080;&#27861;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;AutoSAM Adapter&#65292;&#19987;&#20026;3D&#22810;&#22120;&#23448;CT&#20998;&#21106;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#20197;&#20419;&#36827;&#23558;SAM&#27169;&#22411;&#30340;&#33021;&#21147;&#36716;&#21270;&#20026;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) has rapidly been adopted for segmenting a wide range of natural images. However, recent studies have indicated that SAM exhibits subpar performance on 3D medical image segmentation tasks. In addition to the domain gaps between natural and medical images, disparities in the spatial arrangement between 2D and 3D images, the substantial computational burden imposed by powerful GPU servers, and the time-consuming manual prompt generation impede the extension of SAM to a broader spectrum of medical image segmentation applications. To address these challenges, in this work, we introduce a novel method, AutoSAM Adapter, designed specifically for 3D multi-organ CT-based segmentation. We employ parameter-efficient adaptation techniques in developing an automatic prompt learning paradigm to facilitate the transformation of the SAM model's capabilities to 3D medical image segmentation, eliminating the need for manually generated prompts. Furthermore, we effectivel
&lt;/p&gt;</description></item></channel></rss>