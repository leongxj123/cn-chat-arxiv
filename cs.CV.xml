<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21517;&#20026;SidAE&#65292;&#23427;&#32467;&#21512;&#20102;&#23402;&#29983;&#26550;&#26500;&#21644;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25552;&#21462;&#36755;&#20837;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#20197;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02549</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#23402;&#29983;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Siamese Autoencoders. (arXiv:2304.02549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21517;&#20026;SidAE&#65292;&#23427;&#32467;&#21512;&#20102;&#23402;&#29983;&#26550;&#26500;&#21644;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25552;&#21462;&#36755;&#20837;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#20197;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23436;&#20840;&#30417;&#30563;&#30340;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#24448;&#24448;&#26159;&#26114;&#36149;&#19988;&#38590;&#20197;&#33719;&#24471;&#30340;&#12290;&#30456;&#21453;&#65292;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20943;&#23569;&#20102;&#23454;&#29616;&#30456;&#21516;&#25110;&#26356;&#39640;&#19979;&#28216;&#24615;&#33021;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#12290;&#30446;&#26631;&#26159;&#22312;&#33258;&#30417;&#30563;&#20219;&#21153;&#19978;&#39044;&#20808;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20415;&#32593;&#32476;&#33021;&#22815;&#20174;&#21407;&#22987;&#36755;&#20837;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#29305;&#24449;&#29992;&#20316;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#22270;&#20687;&#20998;&#31867;&#65289;&#20013;&#30340;&#36755;&#20837;&#12290;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#33258;&#32534;&#30721;&#22120;&#21644;&#23402;&#29983;&#32593;&#32476;&#65288;&#22914;SimSiam&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#36825;&#20123;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#23558;&#29305;&#24449;&#30340;&#29305;&#24615;&#65288;&#20363;&#22914;&#65292;&#32454;&#33410;&#32423;&#21035;&#65289;&#19982;&#32473;&#23450;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#23402;&#29983;&#26550;&#26500;&#21644;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#20248;&#21183;&#30340;&#26032;&#33258;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#21517;&#20026;SidAE&#65288;&#23402;&#29983;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#65289;&#65292;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#32988;&#36807;&#20102;&#20004;&#20010;&#33258;&#30417;&#30563;&#26368;&#26032;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully supervised models often require large amounts of labeled training data, which tends to be costly and hard to acquire. In contrast, self-supervised representation learning reduces the amount of labeled data needed for achieving the same or even higher downstream performance. The goal is to pre-train deep neural networks on a self-supervised task such that afterwards the networks are able to extract meaningful features from raw input data. These features are then used as inputs in downstream tasks, such as image classification. Previously, autoencoders and Siamese networks such as SimSiam have been successfully employed in those tasks. Yet, challenges remain, such as matching characteristics of the features (e.g., level of detail) to the given task and data set. In this paper, we present a new self-supervised method that combines the benefits of Siamese architectures and denoising autoencoders. We show that our model, called SidAE (Siamese denoising autoencoder), outperforms two se
&lt;/p&gt;</description></item></channel></rss>