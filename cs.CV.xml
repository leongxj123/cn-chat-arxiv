<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.19103</link><description>&lt;p&gt;
&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#40657;&#30418;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19103
&lt;/p&gt;
&lt;p&gt;
PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#25163;&#21160;&#21046;&#20316;&#25552;&#31034;&#32780;&#23548;&#33268;&#24037;&#20316;&#32321;&#37325;&#12290;&#36825;&#19968;&#25361;&#25112;&#20419;&#20351;&#20102;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;T2I&#27169;&#22411;&#20043;&#38388;&#30340;&#21487;&#20256;&#36882;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#38656;&#35201;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#30333;&#30418;&#35775;&#38382;&#65292;&#24182;&#20135;&#29983;&#38750;&#30452;&#35266;&#30340;&#25552;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PRISM&#65292;&#36825;&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#23601;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#29425;&#30340;&#21551;&#21457;&#65292;PRISM&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;&#32473;&#23450;&#21442;&#32771;&#22270;&#20687;&#30340;&#20505;&#36873;&#25552;&#31034;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;PRISM&#22312;&#20026;&#23545;&#35937;&#12289;&#26679;&#24335;&#31561;&#29983;&#25104;&#20934;&#30830;&#25552;&#31034;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19103v1 Announce Type: cross  Abstract: Prompt engineering is effective for controlling the output of text-to-image (T2I) generative models, but it is also laborious due to the need for manually crafted prompts. This challenge has spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, and produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, sty
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CLIP&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;QualiCLIP&#65292;&#36890;&#36807;&#36136;&#37327;&#24863;&#30693;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#19981;&#38656;&#35201;&#26631;&#35760;MOS&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.11176</link><description>&lt;p&gt;
&#38754;&#21521;&#29616;&#23454;&#19990;&#30028;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#30340;&#36136;&#37327;&#24863;&#30693;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Quality-Aware Image-Text Alignment for Real-World Image Quality Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11176
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CLIP&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;QualiCLIP&#65292;&#36890;&#36807;&#36136;&#37327;&#24863;&#30693;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#19981;&#38656;&#35201;&#26631;&#35760;MOS&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#21442;&#32771;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#65288;NR-IQA&#65289;&#33268;&#21147;&#20110;&#35774;&#35745;&#19968;&#31181;&#22312;&#27809;&#26377;&#39640;&#36136;&#37327;&#21442;&#32771;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#27979;&#37327;&#22270;&#20687;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#31526;&#21512;&#20154;&#31867;&#24863;&#30693;&#65292;&#22823;&#37096;&#20998;&#26368;&#20808;&#36827;&#30340;NR-IQA&#26041;&#27861;&#20013;&#20381;&#36182;&#26631;&#27880;&#30340;&#20027;&#35266;&#35780;&#20998;&#65288;MOS&#65289;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;QualiCLIP&#65288;Quality-aware CLIP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;CLIP&#30340;&#33258;&#30417;&#30563;&#19981;&#38656;&#35201;&#26631;&#35760;MOS&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36136;&#37327;&#24863;&#30693;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#31574;&#30053;&#65292;&#20351;&#24471;CLIP&#29983;&#25104;&#30340;&#34920;&#31034;&#19982;&#22270;&#20687;&#22266;&#26377;&#36136;&#37327;&#30456;&#20851;&#12290;&#20174;&#21407;&#22987;&#22270;&#20687;&#24320;&#22987;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#26029;&#22686;&#21152;&#30340;&#24378;&#24230;&#21512;&#25104;&#22320;&#21155;&#21270;&#23427;&#20204;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;CLIP&#26681;&#25454;&#20854;&#19982;&#36136;&#37327;&#30456;&#20851;&#30340;&#21453;&#20041;&#25991;&#26412;&#25552;&#31034;&#30340;&#30456;&#20284;&#24615;&#23545;&#36825;&#20123;&#38477;&#35299;&#22270;&#20687;&#36827;&#34892;&#25490;&#21517;&#65292;&#21516;&#26102;&#20445;&#35777;&#19968;&#33268;&#30340;&#34920;&#36798;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11176v1 Announce Type: cross  Abstract: No-Reference Image Quality Assessment (NR-IQA) focuses on designing methods to measure image quality in alignment with human perception when a high-quality reference image is unavailable. The reliance on annotated Mean Opinion Scores (MOS) in the majority of state-of-the-art NR-IQA approaches limits their scalability and broader applicability to real-world scenarios. To overcome this limitation, we propose QualiCLIP (Quality-aware CLIP), a CLIP-based self-supervised opinion-unaware method that does not require labeled MOS. In particular, we introduce a quality-aware image-text alignment strategy to make CLIP generate representations that correlate with the inherent quality of the images. Starting from pristine images, we synthetically degrade them with increasing levels of intensity. Then, we train CLIP to rank these degraded images based on their similarity to quality-related antonym text prompts, while guaranteeing consistent represe
&lt;/p&gt;</description></item><item><title>RealDex&#25968;&#25454;&#38598;&#25429;&#25417;&#20102;&#30495;&#23454;&#30340;&#28789;&#24039;&#25163;&#25235;&#21462;&#21160;&#20316;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#20351;&#24471;&#35757;&#32451;&#28789;&#24039;&#25163;&#26356;&#21152;&#33258;&#28982;&#21644;&#31934;&#30830;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#28789;&#24039;&#25235;&#21462;&#21160;&#20316;&#29983;&#25104;&#26694;&#26550;&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#31867;&#20154;&#26426;&#22120;&#20154;&#30340;&#33258;&#21160;&#24863;&#30693;&#12289;&#35748;&#30693;&#21644;&#25805;&#32437;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13853</link><description>&lt;p&gt;
RealDex: &#23454;&#29616;&#26426;&#22120;&#20154;&#28789;&#24039;&#25163;&#31867;&#20154;&#24335;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
RealDex: Towards Human-like Grasping for Robotic Dexterous Hand
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13853
&lt;/p&gt;
&lt;p&gt;
RealDex&#25968;&#25454;&#38598;&#25429;&#25417;&#20102;&#30495;&#23454;&#30340;&#28789;&#24039;&#25163;&#25235;&#21462;&#21160;&#20316;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#20351;&#24471;&#35757;&#32451;&#28789;&#24039;&#25163;&#26356;&#21152;&#33258;&#28982;&#21644;&#31934;&#30830;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#28789;&#24039;&#25235;&#21462;&#21160;&#20316;&#29983;&#25104;&#26694;&#26550;&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#31867;&#20154;&#26426;&#22120;&#20154;&#30340;&#33258;&#21160;&#24863;&#30693;&#12289;&#35748;&#30693;&#21644;&#25805;&#32437;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RealDex&#65292;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#25429;&#25417;&#20102;&#34701;&#20837;&#20102;&#20154;&#31867;&#34892;&#20026;&#27169;&#24335;&#30340;&#30495;&#23454;&#28789;&#24039;&#25163;&#25235;&#21462;&#21160;&#20316;&#65292;&#21516;&#26102;&#36890;&#36807;&#22810;&#35270;&#35282;&#21644;&#22810;&#27169;&#24577;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#20102;&#20016;&#23500;&#12290;&#21033;&#29992;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#26102;&#26080;&#32541;&#21516;&#27493;&#20154;-&#26426;&#22120;&#20154;&#25163;&#23039;&#21183;&#12290;&#36825;&#20123;&#31867;&#20154;&#21160;&#20316;&#30340;&#38598;&#21512;&#23545;&#20110;&#35757;&#32451;&#28789;&#24039;&#25163;&#26356;&#33258;&#28982;&#12289;&#26356;&#31934;&#30830;&#22320;&#27169;&#20223;&#20154;&#31867;&#21160;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;RealDex&#22312;&#25512;&#21160;&#31867;&#20154;&#26426;&#22120;&#20154;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#33258;&#21160;&#24863;&#30693;&#12289;&#35748;&#30693;&#21644;&#25805;&#32437;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21069;&#27839;&#30340;&#28789;&#24039;&#25235;&#21462;&#21160;&#20316;&#29983;&#25104;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#31526;&#21512;&#20154;&#31867;&#32463;&#39564;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;RealDex&#21644;&#20854;&#20182;&#24320;&#25918;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;&#23436;&#25972;&#30340;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#23558;&#20250;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13853v1 Announce Type: cross  Abstract: In this paper, we introduce RealDex, a pioneering dataset capturing authentic dexterous hand grasping motions infused with human behavioral patterns, enriched by multi-view and multimodal visual data. Utilizing a teleoperation system, we seamlessly synchronize human-robot hand poses in real time. This collection of human-like motions is crucial for training dexterous hands to mimic human movements more naturally and precisely. RealDex holds immense promise in advancing humanoid robot for automated perception, cognition, and manipulation in real-world scenarios. Moreover, we introduce a cutting-edge dexterous grasping motion generation framework, which aligns with human experience and enhances real-world applicability through effectively utilizing Multimodal Large Language Models. Extensive experiments have demonstrated the superior performance of our method on RealDex and other open datasets. The complete dataset and code will be made 
&lt;/p&gt;</description></item><item><title>InkSight&#26159;&#19968;&#20010;&#21487;&#20197;&#23558;&#31163;&#32447;&#25163;&#20889;&#36716;&#25442;&#20026;&#22312;&#32447;&#25163;&#20889;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#38405;&#35835;&#21644;&#20070;&#20889;&#20808;&#39564;&#30693;&#35782;&#65292;&#22312;&#22810;&#26679;&#21270;&#30340;&#29031;&#29255;&#20013;&#26377;&#25928;&#22320;Derendering&#25163;&#20889;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.05804</link><description>&lt;p&gt;
InkSight&#65306;&#36890;&#36807;&#23398;&#20064;&#38405;&#35835;&#21644;&#20070;&#20889;&#23454;&#29616;&#31163;&#32447;&#21040;&#22312;&#32447;&#25163;&#20889;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
InkSight: Offline-to-Online Handwriting Conversion by Learning to Read and Write
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05804
&lt;/p&gt;
&lt;p&gt;
InkSight&#26159;&#19968;&#20010;&#21487;&#20197;&#23558;&#31163;&#32447;&#25163;&#20889;&#36716;&#25442;&#20026;&#22312;&#32447;&#25163;&#20889;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#38405;&#35835;&#21644;&#20070;&#20889;&#20808;&#39564;&#30693;&#35782;&#65292;&#22312;&#22810;&#26679;&#21270;&#30340;&#29031;&#29255;&#20013;&#26377;&#25928;&#22320;Derendering&#25163;&#20889;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#31508;&#35760;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32784;&#29992;&#12289;&#21487;&#32534;&#36753;&#21644;&#26131;&#20110;&#32034;&#24341;&#30340;&#23384;&#20648;&#31508;&#35760;&#30340;&#26041;&#24335;&#65292;&#21363;&#30690;&#37327;&#21270;&#24418;&#24335;&#30340;&#25968;&#23383;&#22696;&#27700;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31508;&#35760;&#26041;&#24335;&#19982;&#20256;&#32479;&#30340;&#32440;&#31508;&#35760;&#26041;&#24335;&#20043;&#38388;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#32780;&#20256;&#32479;&#32440;&#31508;&#35760;&#26041;&#24335;&#20173;&#21463;&#21040;&#32477;&#22823;&#22810;&#25968;&#20154;&#30340;&#38738;&#30544;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;InkSight&#26088;&#22312;&#24357;&#21512;&#36825;&#31181;&#24046;&#36317;&#65292;&#20351;&#23454;&#20307;&#31508;&#35760;&#32773;&#33021;&#22815;&#36731;&#26494;&#22320;&#23558;&#20182;&#20204;&#30340;&#20316;&#21697;&#65288;&#31163;&#32447;&#25163;&#20889;&#65289;&#36716;&#25442;&#20026;&#25968;&#23383;&#22696;&#27700;&#65288;&#22312;&#32447;&#25163;&#20889;&#65289;&#65292;&#36825;&#20010;&#36807;&#31243;&#25105;&#20204;&#31216;&#20043;&#20026;Derendering&#12290;&#20043;&#21069;&#20851;&#20110;&#27492;&#20027;&#39064;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#22270;&#20687;&#30340;&#20960;&#20309;&#23646;&#24615;&#19978;&#65292;&#23548;&#33268;&#20102;&#22312;&#35757;&#32451;&#39046;&#22495;&#20043;&#22806;&#30340;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#38405;&#35835;&#21644;&#20070;&#20889;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20801;&#35768;&#22312;&#32570;&#20047;&#22823;&#37327;&#37197;&#23545;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#37197;&#23545;&#26679;&#26412;&#24456;&#38590;&#33719;&#21462;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26377;&#25928;&#22320;&#23545;&#20855;&#26377;&#22810;&#26679;&#21270;&#35270;&#35273;&#29305;&#24449;&#21644;&#32972;&#26223;&#30340;&#20219;&#24847;&#29031;&#29255;&#20013;&#30340;&#25163;&#20889;&#25991;&#26412;&#36827;&#34892;Derendering&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital note-taking is gaining popularity, offering a durable, editable, and easily indexable way of storing notes in the vectorized form, known as digital ink. However, a substantial gap remains between this way of note-taking and traditional pen-and-paper note-taking, a practice still favored by a vast majority. Our work, InkSight, aims to bridge the gap by empowering physical note-takers to effortlessly convert their work (offline handwriting) to digital ink (online handwriting), a process we refer to as Derendering. Prior research on the topic has focused on the geometric properties of images, resulting in limited generalization beyond their training domains. Our approach combines reading and writing priors, allowing training a model in the absence of large amounts of paired samples, which are difficult to obtain. To our knowledge, this is the first work that effectively derenders handwritten text in arbitrary photos with diverse visual characteristics and backgrounds. Furthermore,
&lt;/p&gt;</description></item><item><title>CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;</title><link>https://arxiv.org/abs/2402.05374</link><description>&lt;p&gt;
CIC&#65306;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CIC: A framework for Culturally-aware Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05374
&lt;/p&gt;
&lt;p&gt;
CIC&#26159;&#19968;&#31181;&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#38382;&#31572;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#33021;&#25551;&#36848;&#22270;&#20687;&#20013;&#25991;&#21270;&#20803;&#32032;&#30340;&#35814;&#32454;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;VLPs&#65289;&#22914;BLIP&#20174;&#22270;&#20687;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#65292;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#25152;&#25551;&#32472;&#30340;&#25991;&#21270;&#20803;&#32032;&#65288;&#20363;&#22914;&#20122;&#27954;&#25991;&#21270;&#32676;&#20307;&#30340;&#20256;&#32479;&#26381;&#35013;&#65289;&#29983;&#25104;&#35814;&#32454;&#25551;&#36848;&#24615;&#23383;&#24149;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;\textbf{&#38754;&#21521;&#25991;&#21270;&#24863;&#30693;&#22270;&#20687;&#23383;&#24149;&#65288;CIC&#65289;}&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20174;&#20195;&#34920;&#19981;&#21516;&#25991;&#21270;&#30340;&#22270;&#20687;&#20013;&#29983;&#25104;&#23383;&#24149;&#24182;&#25551;&#36848;&#25991;&#21270;&#20803;&#32032;&#12290;&#21463;&#21040;&#23558;&#35270;&#35273;&#27169;&#24577;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#36827;&#34892;&#32452;&#21512;&#30340;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#65288;1&#65289;&#26681;&#25454;&#22270;&#20687;&#20013;&#30340;&#25991;&#21270;&#31867;&#21035;&#29983;&#25104;&#38382;&#39064;&#65292;&#65288;2&#65289;&#21033;&#29992;&#29983;&#25104;&#30340;&#38382;&#39064;&#20174;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20013;&#25552;&#21462;&#25991;&#21270;&#35270;&#35273;&#20803;&#32032;&#65292;&#65288;3&#65289;&#20351;&#29992;&#24102;&#26377;&#25552;&#31034;&#30340;LLMs&#29983;&#25104;&#25991;&#21270;&#24863;&#30693;&#23383;&#24149;&#12290;&#25105;&#20204;&#22312;4&#20010;&#19981;&#21516;&#22823;&#23398;&#30340;45&#21517;&#21442;&#19982;&#32773;&#19978;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Captioning generates descriptive sentences from images using Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved greatly. However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups. In this paper, we propose a new framework, \textbf{Culturally-aware Image Captioning (CIC)}, that generates captions and describes cultural elements extracted from cultural visual elements in images representing cultures. Inspired by methods combining visual modality and Large Language Models (LLMs) through appropriate prompts, our framework (1) generates questions based on cultural categories from images, (2) extracts cultural visual elements from Visual Question Answering (VQA) using generated questions, and (3) generates culturally-aware captions using LLMs with the prompts. Our human evaluation conducted on 45 participants from 4 dif
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;ActiveAnno3D&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#20351;&#29992;&#19968;&#21322;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#23454;&#29616;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#36817;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03235</link><description>&lt;p&gt;
ActiveAnno3D - &#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ActiveAnno3D - An Active Learning Framework for Multi-Modal 3D Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03235
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;ActiveAnno3D&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#20351;&#29992;&#19968;&#21322;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#23454;&#29616;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#36817;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#31579;&#36873;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#65292;&#25968;&#25454;&#36890;&#24120;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#65292;&#21019;&#24314;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#38590;&#39064;&#20381;&#28982;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ActiveAnno3D&#65292;&#19968;&#20010;&#29992;&#20110;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#36830;&#32493;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#38598;&#25104;&#20102;&#22312;&#35745;&#31639;&#38656;&#27714;&#21644;&#26816;&#27979;&#24615;&#33021;&#26041;&#38754;&#26368;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;nuScenes&#21644;TUM Traffic Intersection&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#20351;&#29992;BEVFusion&#21644;PV-RCNN&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20165;&#20351;&#29992;TUM Traffic Intersection&#25968;&#25454;&#38598;&#30340;&#19968;&#21322;&#35757;&#32451;&#25968;&#25454;&#65288;77.25 mAP&#30456;&#27604;&#20110;83.50 mAP&#65289;&#26102;&#65292;&#20351;&#29992;PV-RCNN&#21644;&#22522;&#20110;&#29109;&#30340;&#26597;&#35810;&#31574;&#30053;&#20960;&#20046;&#21487;&#20197;&#36798;&#21040;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#32780;BEVFusion&#21017;&#22312;&#20351;&#29992;&#19968;&#21322;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#33719;&#24471;&#20102;64.31&#30340;mAP&#12290;
&lt;/p&gt;
&lt;p&gt;
The curation of large-scale datasets is still costly and requires much time and resources. Data is often manually labeled, and the challenge of creating high-quality datasets remains. In this work, we fill the research gap using active learning for multi-modal 3D object detection. We propose ActiveAnno3D, an active learning framework to select data samples for labeling that are of maximum informativeness for training. We explore various continuous training methods and integrate the most efficient method regarding computational demand and detection performance. Furthermore, we perform extensive experiments and ablation studies with BEVFusion and PV-RCNN on the nuScenes and TUM Traffic Intersection dataset. We show that we can achieve almost the same performance with PV-RCNN and the entropy-based query strategy when using only half of the training data (77.25 mAP compared to 83.50 mAP) of the TUM Traffic Intersection dataset. BEVFusion achieved an mAP of 64.31 when using half of the trai
&lt;/p&gt;</description></item><item><title>GD-CAF&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#38477;&#27700;&#39044;&#25253;&#20316;&#20026;&#19968;&#20010;&#26102;&#31354;&#22270;&#24207;&#21015;&#39044;&#25253;&#38382;&#39064;&#65292;&#21033;&#29992;&#22270;&#24418;&#21452;&#27969;&#21367;&#31215;&#27880;&#24847;&#21147;&#34701;&#21512;&#26469;&#23398;&#20064;&#21382;&#21490;&#38477;&#27700;&#22270;&#24182;&#22312;&#19981;&#21516;&#31354;&#38388;&#20301;&#32622;&#19978;&#39044;&#27979;&#26410;&#26469;&#30340;&#38477;&#27700;&#12290;</title><link>https://arxiv.org/abs/2401.07958</link><description>&lt;p&gt;
GD-CAF&#65306;&#29992;&#20110;&#38477;&#27700;&#39044;&#25253;&#30340;&#22270;&#24418;&#21452;&#27969;&#21367;&#31215;&#27880;&#24847;&#21147;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
GD-CAF: Graph Dual-stream Convolutional Attention Fusion for Precipitation Nowcasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07958
&lt;/p&gt;
&lt;p&gt;
GD-CAF&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#38477;&#27700;&#39044;&#25253;&#20316;&#20026;&#19968;&#20010;&#26102;&#31354;&#22270;&#24207;&#21015;&#39044;&#25253;&#38382;&#39064;&#65292;&#21033;&#29992;&#22270;&#24418;&#21452;&#27969;&#21367;&#31215;&#27880;&#24847;&#21147;&#34701;&#21512;&#26469;&#23398;&#20064;&#21382;&#21490;&#38477;&#27700;&#22270;&#24182;&#22312;&#19981;&#21516;&#31354;&#38388;&#20301;&#32622;&#19978;&#39044;&#27979;&#26410;&#26469;&#30340;&#38477;&#27700;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#38477;&#27700;&#39044;&#25253;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#27946;&#27700;&#39044;&#27979;&#12289;&#28798;&#23475;&#31649;&#29702;&#12289;&#20248;&#21270;&#20892;&#19994;&#27963;&#21160;&#12289;&#31649;&#29702;&#20132;&#36890;&#36335;&#32447;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#12290;&#26412;&#25991;&#23558;&#38477;&#27700;&#39044;&#25253;&#24418;&#24335;&#21270;&#20026;&#26102;&#31354;&#22270;&#24207;&#21015;&#39044;&#25253;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24418;&#21452;&#27969;&#21367;&#31215;&#27880;&#24847;&#21147;&#34701;&#21512;&#65288;GD-CAF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#21382;&#21490;&#38477;&#27700;&#22270;&#30340;&#26102;&#31354;&#22270;&#20013;&#23398;&#20064;&#65292;&#24182;&#39044;&#27979;&#26410;&#26469;&#19981;&#21516;&#31354;&#38388;&#20301;&#32622;&#30340;&#38477;&#27700;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07958v2 Announce Type: replace  Abstract: Accurate precipitation nowcasting is essential for various applications, including flood prediction, disaster management, optimizing agricultural activities, managing transportation routes and renewable energy. While several studies have addressed this challenging task from a sequence-to-sequence perspective, most of them have focused on a single area without considering the existing correlation between multiple disjoint regions. In this paper, we formulate precipitation nowcasting as a spatiotemporal graph sequence nowcasting problem. In particular, we introduce Graph Dual-stream Convolutional Attention Fusion (GD-CAF), a novel approach designed to learn from historical spatiotemporal graph of precipitation maps and nowcast future time step ahead precipitation at different spatial locations. GD-CAF consists of spatio-temporal convolutional attention as well as gated fusion modules which are equipped with depthwise-separable convolut
&lt;/p&gt;</description></item><item><title>GeoSAM&#26159;&#19968;&#20010;&#22522;&#20110;SAM&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;CNN&#20998;&#21106;&#27169;&#22411;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#22320;&#29702;&#22270;&#20687;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.11319</link><description>&lt;p&gt;
GeoSAM: &#20351;&#29992;&#31232;&#30095;&#21644;&#23494;&#38598;&#30340;&#35270;&#35273;&#25552;&#31034;&#23545;SAM&#36827;&#34892;&#25913;&#36827;&#65292;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#31227;&#21160;&#22522;&#30784;&#35774;&#26045;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
GeoSAM: Fine-tuning SAM with Sparse and Dense Visual Prompting for Automated Segmentation of Mobility Infrastructure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11319
&lt;/p&gt;
&lt;p&gt;
GeoSAM&#26159;&#19968;&#20010;&#22522;&#20110;SAM&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;CNN&#20998;&#21106;&#27169;&#22411;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#22320;&#29702;&#22270;&#20687;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24212;&#29992;&#20110;&#33258;&#28982;&#22270;&#20687;&#20998;&#21106;&#26102;&#65292;Segment Anything Model (SAM)&#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#22320;&#29702;&#22270;&#20687;&#65288;&#22914;&#33322;&#25293;&#21644;&#21355;&#26143;&#22270;&#20687;&#65289;&#20013;&#38754;&#20020;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#21106;&#36947;&#36335;&#12289;&#20154;&#34892;&#36947;&#21644;&#20154;&#34892;&#27178;&#36947;&#31561;&#31227;&#21160;&#22522;&#30784;&#35774;&#26045;&#26102;&#12290;&#36825;&#31181;&#36739;&#24046;&#30340;&#24615;&#33021;&#28304;&#20110;&#36825;&#20123;&#23545;&#35937;&#30340;&#31364;&#23567;&#29305;&#24449;&#65292;&#23427;&#20204;&#30340;&#32441;&#29702;&#34701;&#20837;&#29615;&#22659;&#20013;&#65292;&#20197;&#21450;&#26641;&#26408;&#12289;&#24314;&#31569;&#29289;&#12289;&#36710;&#36742;&#21644;&#34892;&#20154;&#31561;&#29289;&#20307;&#30340;&#24178;&#25200;&#65292;&#36825;&#20123;&#37117;&#21487;&#33021;&#20351;&#27169;&#22411;&#22833;&#21435;&#23450;&#21521;&#20135;&#29983;&#19981;&#20934;&#30830;&#30340;&#20998;&#21106;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22320;&#29702;SAM&#65288;GeoSAM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;SAM&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#26469;&#33258;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#23494;&#38598;&#35270;&#35273;&#25552;&#31034;&#21644;&#39044;&#35757;&#32451;CNN&#20998;&#21106;&#27169;&#22411;&#30340;&#31232;&#30095;&#35270;&#35273;&#25552;&#31034;&#23454;&#26045;&#20102;&#32454;&#35843;&#31574;&#30053;&#12290;&#25152;&#25552;&#20986;&#30340;GeoSAM&#22312;&#22320;&#29702;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#12289;&#34892;&#20154;&#22522;&#30784;&#35774;&#26045;&#30340;&#20998;&#21106;&#24615;&#33021;&#25552;&#21319;&#20102;26&#65285;&#12289;7&#65285;&#21644;17&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) has shown impressive performance when applied to natural image segmentation. However, it struggles with geographical images like aerial and satellite imagery, especially when segmenting mobility infrastructure including roads, sidewalks, and crosswalks. This inferior performance stems from the narrow features of these objects, their textures blending into the surroundings, and interference from objects like trees, buildings, vehicles, and pedestrians - all of which can disorient the model to produce inaccurate segmentation maps. To address these challenges, we propose Geographical SAM (GeoSAM), a novel SAM-based framework that implements a fine-tuning strategy using the dense visual prompt from zero-shot learning, and the sparse visual prompt from a pre-trained CNN segmentation model. The proposed GeoSAM outperforms existing approaches for geographical image segmentation, specifically by 26%, 7%, and 17% for road infrastructure, pedestrian infrastructur
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35814;&#32454;&#22238;&#39038;&#20102;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#39046;&#22495;&#27867;&#21270;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22312;DL&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#20998;&#24067;&#28418;&#31227;&#38382;&#39064;&#21644;&#23454;&#29616;&#31283;&#20581;&#24615;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#20102;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#23545;&#25972;&#20010;MedIA&#24037;&#20316;&#27969;&#31243;&#30340;&#25805;&#20316;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.08598</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#39046;&#22495;&#27867;&#21270;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization for Medical Image Analysis: A Survey. (arXiv:2310.08598v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35814;&#32454;&#22238;&#39038;&#20102;&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#39046;&#22495;&#27867;&#21270;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22312;DL&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#20998;&#24067;&#28418;&#31227;&#38382;&#39064;&#21644;&#23454;&#29616;&#31283;&#20581;&#24615;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#20102;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#23545;&#25972;&#20010;MedIA&#24037;&#20316;&#27969;&#31243;&#30340;&#25805;&#20316;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#65288;MedIA&#65289;&#24050;&#25104;&#20026;&#21307;&#23398;&#21644;&#20445;&#20581;&#39046;&#22495;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22312;&#30142;&#30149;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#27835;&#30103;&#35268;&#21010;&#26041;&#38754;&#36215;&#21040;&#20102;&#24456;&#22823;&#30340;&#20316;&#29992;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26368;&#26032;&#25104;&#21151;&#20026;&#20854;&#36827;&#23637;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;MedIA&#30340;DL&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26679;&#26412;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#19979;&#24456;&#38590;&#27867;&#21270;&#65292;&#36825;&#34987;&#31216;&#20026;&#20998;&#24067;&#28418;&#31227;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#33268;&#21147;&#20110;&#24320;&#21457;&#21508;&#31181;DL&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#24182;&#22312;&#26410;&#30693;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#31283;&#20581;&#22320;&#36816;&#34892;&#12290;&#26412;&#25991;&#32508;&#21512;&#35780;&#36848;&#20102;&#19987;&#38376;&#38024;&#23545;MedIA&#30340;&#39046;&#22495;&#27867;&#21270;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#22312;&#26356;&#22823;&#33539;&#22260;MedIA&#31995;&#32479;&#20869;&#30340;&#20132;&#20114;&#26041;&#24335;&#30340;&#25972;&#20307;&#35270;&#22270;&#65292;&#19981;&#20165;&#20165;&#32771;&#34385;&#26041;&#27861;&#23398;&#65292;&#36824;&#32771;&#34385;&#20102;&#23545;&#25972;&#20010;MedIA&#24037;&#20316;&#27969;&#31243;&#30340;&#25805;&#20316;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#20998;&#20026;&#25968;&#25454;&#23618;&#27425;&#30340;&#26041;&#27861;&#8230;
&lt;/p&gt;
&lt;p&gt;
Medical Image Analysis (MedIA) has become an essential tool in medicine and healthcare, aiding in disease diagnosis, prognosis, and treatment planning, and recent successes in deep learning (DL) have made significant contributions to its advances. However, DL models for MedIA remain challenging to deploy in real-world situations, failing for generalization under the distributional gap between training and testing samples, known as a distribution shift problem. Researchers have dedicated their efforts to developing various DL methods to adapt and perform robustly on unknown and out-of-distribution data distributions. This paper comprehensively reviews domain generalization studies specifically tailored for MedIA. We provide a holistic view of how domain generalization techniques interact within the broader MedIA system, going beyond methodologies to consider the operational implications on the entire MedIA workflow. Specifically, we categorize domain generalization methods into data-lev
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#32570;&#20047;&#31283;&#23450;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#24314;&#35758;&#37319;&#29992;&#24191;&#27867;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#32479;&#19968;&#30340;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2308.04137</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#32508;&#21512;&#35780;&#20272;&#25581;&#31034;&#20986;&#24778;&#20154;&#30340;&#32570;&#20047;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Assessment of the Performance of Deep Learning Classifiers Reveals a Surprising Lack of Robustness. (arXiv:2308.04137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04137
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#32570;&#20047;&#31283;&#23450;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#24314;&#35758;&#37319;&#29992;&#24191;&#27867;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#32479;&#19968;&#30340;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#32780;&#31283;&#20581;&#30340;&#35780;&#20272;&#26041;&#27861;&#26159;&#24320;&#21457;&#26412;&#36523;&#31283;&#20581;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24517;&#35201;&#31532;&#19968;&#27493;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#35780;&#20272;&#20998;&#31867;&#22120;&#30340;&#24120;&#35268;&#35780;&#20272;&#21327;&#35758;&#22312;&#32508;&#21512;&#35780;&#20272;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#20381;&#36182;&#20110;&#26377;&#38480;&#31867;&#22411;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#24573;&#35270;&#20854;&#20182;&#31867;&#22411;&#30340;&#25968;&#25454;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;&#26631;&#20934;&#27979;&#35797;&#25968;&#25454;&#26080;&#27861;&#35780;&#20272;&#20998;&#31867;&#22120;&#23545;&#20110;&#26410;&#32463;&#35757;&#32451;&#30340;&#31867;&#21035;&#26679;&#26412;&#30340;&#39044;&#27979;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20351;&#29992;&#21253;&#21547;&#26410;&#30693;&#31867;&#21035;&#26679;&#26412;&#30340;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#26080;&#27861;&#35780;&#20272;&#20998;&#31867;&#22120;&#23545;&#20110;&#24050;&#30693;&#31867;&#21035;&#26631;&#31614;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20513;&#20351;&#29992;&#21508;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#36827;&#34892;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21487;&#24212;&#29992;&#20110;&#25152;&#26377;&#36825;&#20123;&#25968;&#25454;&#31867;&#22411;&#30340;&#21333;&#19968;&#25351;&#26631;&#65292;&#20197;&#20135;&#29983;&#19968;&#33268;&#30340;&#24615;&#33021;&#35780;&#20272;&#32467;&#26524;&#12290;&#36890;&#36807;&#36825;&#26679;&#30340;&#22522;&#20934;&#27979;&#35797;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;&#20351;&#29992;&#35748;&#20026;&#26159;&#20840;&#38754;&#30340;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#20063;&#23384;&#22312;&#32570;&#20047;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable and robust evaluation methods are a necessary first step towards developing machine learning models that are themselves robust and reliable. Unfortunately, current evaluation protocols typically used to assess classifiers fail to comprehensively evaluate performance as they tend to rely on limited types of test data, and ignore others. For example, using the standard test data fails to evaluate the predictions made by the classifier to samples from classes it was not trained on. On the other hand, testing with data containing samples from unknown classes fails to evaluate how well the classifier can predict the labels for known classes. This article advocates bench-marking performance using a wide range of different types of data and using a single metric that can be applied to all such data types to produce a consistent evaluation of performance. Using such a benchmark it is found that current deep neural networks, including those trained with methods that are believed to pro
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20540;&#20989;&#25968;&#31639;&#27861;&#21644;&#26799;&#24230;&#31639;&#27861;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#21453;&#36716;&#37325;&#24314;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#30417;&#30563;&#20449;&#21495;&#65292;&#20197;&#35299;&#20915;&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.09273</link><description>&lt;p&gt;
&#20320;&#30340;&#25151;&#38388;&#19981;&#26159;&#31169;&#23494;&#30340;&#65306;&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Your Room is not Private: Gradient Inversion Attack on Reinforcement Learning. (arXiv:2306.09273v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09273
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20540;&#20989;&#25968;&#31639;&#27861;&#21644;&#26799;&#24230;&#31639;&#27861;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#21453;&#36716;&#37325;&#24314;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#30417;&#30563;&#20449;&#21495;&#65292;&#20197;&#35299;&#20915;&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26174;&#33879;&#21457;&#23637;&#21560;&#24341;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#35813;&#25216;&#26415;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#23548;&#33322;&#12289;&#24863;&#30693;&#21644;&#20114;&#21160;&#12290;&#30001;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#38544;&#31169;&#38382;&#39064;&#22312;&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#26426;&#22120;&#20154;&#21487;&#20197;&#35775;&#38382;&#22823;&#37327;&#20010;&#20154;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#20540;&#20989;&#25968;&#31639;&#27861;&#21644;&#26799;&#24230;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#22312;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#32771;&#34385;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#25915;&#20987;&#20540;&#20989;&#25968;&#31639;&#27861;&#21644;&#26799;&#24230;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#21453;&#36716;&#37325;&#24314;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#30417;&#30563;&#20449;&#21495;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36873;&#25321;&#20351;&#29992;&#26799;&#24230;&#36827;&#34892;&#25915;&#20987;&#26159;&#22240;&#20026;&#24120;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#20165;&#21033;&#29992;&#22522;&#20110;&#31169;&#20154;&#29992;&#25143;&#25968;&#25454;&#35745;&#31639;&#30340;&#26799;&#24230;&#26469;&#20248;&#21270;&#27169;&#22411;&#65292;&#32780;&#19981;&#23384;&#20648;&#25110;&#20256;&#36755;&#29992;&#25143;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prominence of embodied Artificial Intelligence (AI), which empowers robots to navigate, perceive, and engage within virtual environments, has attracted significant attention, owing to the remarkable advancements in computer vision and large language models. Privacy emerges as a pivotal concern within the realm of embodied AI, as the robot accesses substantial personal information. However, the issue of privacy leakage in embodied AI tasks, particularly in relation to reinforcement learning algorithms, has not received adequate consideration in research. This paper aims to address this gap by proposing an attack on the value-based algorithm and the gradient-based algorithm, utilizing gradient inversion to reconstruct states, actions, and supervision signals. The choice of using gradients for the attack is motivated by the fact that commonly employed federated learning techniques solely utilize gradients computed based on private user data to optimize models, without storing or trans
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#20248;&#21270;&#22120;FAME&#65292;&#20351;&#29992;&#19977;&#37325;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#65288;TEMA&#65289;&#26469;&#20272;&#35745;&#26799;&#24230;&#30697;&#65292;&#25552;&#20379;&#26356;&#20016;&#23500;&#21644;&#20934;&#30830;&#30340;&#25968;&#25454;&#21464;&#21270;&#21644;&#36235;&#21183;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#20013;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.01423</link><description>&lt;p&gt;
&#21033;&#29992;&#19977;&#37325;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#23454;&#29616;&#24555;&#36895;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Leveraging the Triple Exponential Moving Average for Fast-Adaptive Moment Estimation. (arXiv:2306.01423v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#20248;&#21270;&#22120;FAME&#65292;&#20351;&#29992;&#19977;&#37325;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#65288;TEMA&#65289;&#26469;&#20272;&#35745;&#26799;&#24230;&#30697;&#65292;&#25552;&#20379;&#26356;&#20016;&#23500;&#21644;&#20934;&#30830;&#30340;&#25968;&#25454;&#21464;&#21270;&#21644;&#36235;&#21183;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#20013;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20248;&#21270;&#26159;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#30452;&#25509;&#24433;&#21709;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#22810;&#31181;&#39046;&#22495;&#20013;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#22810;&#31181;&#20248;&#21270;&#22120;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24555;&#36895;&#22320;&#35782;&#21035;&#26799;&#24230;&#36235;&#21183;&#26041;&#38754;&#20173;&#28982;&#26377;&#38480;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#32593;&#32476;&#24615;&#33021;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#20248;&#21270;&#22120;&#65292;&#31216;&#20026;&#24555;&#36895;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#65288;FAME&#65289;&#65292;&#23427;&#39318;&#27425;&#20351;&#29992;&#19977;&#37325;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#65288;TEMA&#65289;&#26469;&#20272;&#35745;&#26799;&#24230;&#30697;&#12290;&#23558;TEMA&#32435;&#20837;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#20016;&#23500;&#21644;&#20934;&#30830;&#30340;&#25968;&#25454;&#21464;&#21270;&#21644;&#36235;&#21183;&#20449;&#24687;&#65292;&#19982;&#30446;&#21069;&#25152;&#26377;&#20027;&#35201;&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#30456;&#27604;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;FAME&#20248;&#21270;&#22120;&#24050;&#32463;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;CIFAR-10&#65292;CIFAR-100&#65292;PASCAL-VOC&#65292;MS-COCO&#21644;Cityscapes&#12290;
&lt;/p&gt;
&lt;p&gt;
Network optimization is a crucial step in the field of deep learning, as it directly affects the performance of models in various domains such as computer vision. Despite the numerous optimizers that have been developed over the years, the current methods are still limited in their ability to accurately and quickly identify gradient trends, which can lead to sub-optimal network performance. In this paper, we propose a novel deep optimizer called Fast-Adaptive Moment Estimation (FAME), which for the first time estimates gradient moments using a Triple Exponential Moving Average (TEMA). Incorporating TEMA into the optimization process provides richer and more accurate information on data changes and trends, as compared to the standard Exponential Moving Average used in essentially all current leading adaptive optimization methods. Our proposed FAME optimizer has been extensively validated through a wide range of benchmarks, including CIFAR-10, CIFAR-100, PASCAL-VOC, MS-COCO, and Cityscap
&lt;/p&gt;</description></item></channel></rss>