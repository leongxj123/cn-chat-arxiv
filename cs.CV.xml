<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>ContourDiff&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#30340;&#39046;&#22495;&#19981;&#21464;&#35299;&#21078;&#36718;&#24275;&#34920;&#31034;&#65292;&#26088;&#22312;&#24110;&#21161;&#20934;&#30830;&#32763;&#35793;&#21307;&#23398;&#22270;&#20687;&#24182;&#20445;&#25345;&#20854;&#35299;&#21078;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10786</link><description>&lt;p&gt;
ContourDiff&#65306;&#24102;&#36718;&#24275;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#26080;&#37197;&#23545;&#22270;&#20687;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
ContourDiff: Unpaired Image Translation with Contour-Guided Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10786
&lt;/p&gt;
&lt;p&gt;
ContourDiff&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#30340;&#39046;&#22495;&#19981;&#21464;&#35299;&#21078;&#36718;&#24275;&#34920;&#31034;&#65292;&#26088;&#22312;&#24110;&#21161;&#20934;&#30830;&#32763;&#35793;&#21307;&#23398;&#22270;&#20687;&#24182;&#20445;&#25345;&#20854;&#35299;&#21078;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#32763;&#35793;&#21307;&#23398;&#22270;&#20687;&#65288;&#20363;&#22914;&#20174;CT&#21040;MRI&#65289;&#23545;&#20110;&#35768;&#22810;&#20020;&#24202;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ContourDiff&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22270;&#20687;&#30340;&#39046;&#22495;&#19981;&#21464;&#35299;&#21078;&#36718;&#24275;&#34920;&#31034;&#12290;&#36825;&#20123;&#34920;&#31034;&#26131;&#20110;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#65292;&#20294;&#23545;&#20854;&#35299;&#21078;&#20869;&#23481;&#24418;&#25104;&#31934;&#30830;&#30340;&#31354;&#38388;&#32422;&#26463;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#26469;&#33258;&#20219;&#24847;&#36755;&#20837;&#39046;&#22495;&#30340;&#22270;&#20687;&#30340;&#36718;&#24275;&#34920;&#31034;&#36716;&#25442;&#20026;&#36755;&#20986;&#39046;&#22495;&#20013;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10786v1 Announce Type: cross  Abstract: Accurately translating medical images across different modalities (e.g., CT to MRI) has numerous downstream clinical and machine learning applications. While several methods have been proposed to achieve this, they often prioritize perceptual quality with respect to output domain features over preserving anatomical fidelity. However, maintaining anatomy during translation is essential for many tasks, e.g., when leveraging masks from the input domain to develop a segmentation model with images translated to the output domain. To address these challenges, we propose ContourDiff, a novel framework that leverages domain-invariant anatomical contour representations of images. These representations are simple to extract from images, yet form precise spatial constraints on their anatomical content. We introduce a diffusion model that converts contour representations of images from arbitrary input domains into images in the output domain of in
&lt;/p&gt;</description></item></channel></rss>