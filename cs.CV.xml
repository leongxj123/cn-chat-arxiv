<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#35268;&#21010;&#12289;&#20223;&#30495;&#21644;&#20851;&#38190;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#35201;&#36129;&#29486;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#32763;&#35793;&#33021;&#21147;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#39550;&#39542;&#22330;&#26223;&#21019;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01105</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey for Foundation Models in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#35268;&#21010;&#12289;&#20223;&#30495;&#21644;&#20851;&#38190;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#35201;&#36129;&#29486;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#32763;&#35793;&#33021;&#21147;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#39550;&#39542;&#22330;&#26223;&#21019;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21457;&#29983;&#20102;&#38761;&#21629;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#23545;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#23637;&#31034;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#20316;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#35268;&#21010;&#21644;&#20223;&#30495;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#22312;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#20851;&#38190;&#20219;&#21153;&#20013;&#24471;&#21040;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#20197;&#21450;&#20026;&#20223;&#30495;&#21644;&#27979;&#35797;&#21019;&#24314;&#36924;&#30495;&#30340;&#39550;&#39542;&#22330;&#26223;&#12290;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#25972;&#21512;&#22810;&#26679;&#30340;&#36755;&#20837;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#65292;&#23545;&#20110;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20998;&#31867;&#65292;&#26681;&#25454;&#27169;&#24577;&#21644;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#21151;&#33021;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of foundation models has revolutionized the fields of natural language processing and computer vision, paving the way for their application in autonomous driving (AD). This survey presents a comprehensive review of more than 40 research papers, demonstrating the role of foundation models in enhancing AD. Large language models contribute to planning and simulation in AD, particularly through their proficiency in reasoning, code generation and translation. In parallel, vision foundation models are increasingly adapted for critical tasks such as 3D object detection and tracking, as well as creating realistic driving scenarios for simulation and testing. Multi-modal foundation models, integrating diverse inputs, exhibit exceptional visual understanding and spatial reasoning, crucial for end-to-end AD. This survey not only provides a structured taxonomy, categorizing foundation models based on their modalities and functionalities within the AD domain but also delves into the meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20363;&#32423;&#26816;&#32034;&#20219;&#21153;&#65306;PointCloud-Text&#21305;&#37197;&#65288;PTM&#65289;&#65292;&#24182;&#26500;&#24314;&#20102;&#19977;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#12289;&#25991;&#26412;&#27169;&#31946;&#31561;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;RoMa&#26041;&#27861;&#20316;&#20026;PTM&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.19386</link><description>&lt;p&gt;
PointCloud-Text&#21305;&#37197;&#65306;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
PointCloud-Text Matching: Benchmark Datasets and a Baseline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20363;&#32423;&#26816;&#32034;&#20219;&#21153;&#65306;PointCloud-Text&#21305;&#37197;&#65288;PTM&#65289;&#65292;&#24182;&#26500;&#24314;&#20102;&#19977;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#12289;&#25991;&#26412;&#27169;&#31946;&#31561;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;RoMa&#26041;&#27861;&#20316;&#20026;PTM&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#21644;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20363;&#32423;&#26816;&#32034;&#20219;&#21153;&#65306;PointCloud-Text Matching&#65288;PTM&#65289;&#65292;&#26088;&#22312;&#25214;&#21040;&#19982;&#32473;&#23450;&#30340;&#28857;&#20113;&#26597;&#35810;&#25110;&#25991;&#26412;&#26597;&#35810;&#21305;&#37197;&#30340;&#30830;&#20999;&#36328;&#27169;&#24577;&#23454;&#20363;&#12290;PTM&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#65292;&#22914;&#23460;&#20869;/&#22478;&#24066;&#23777;&#35895;&#23450;&#20301;&#21644;&#22330;&#26223;&#26816;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#23578;&#26080;&#36866;&#29992;&#30340;&#12289;&#26377;&#38024;&#23545;&#24615;&#30340;PTM&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19977;&#20010;&#26032;&#30340;PTM&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#20026;3D2T-SR&#12289;3D2T-NR&#21644;3D2T-QA&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25968;&#25454;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#30001;&#20110;&#28857;&#20113;&#30340;&#31232;&#30095;&#12289;&#22122;&#22768;&#25110;&#26080;&#24207;&#65292;&#20197;&#21450;&#25991;&#26412;&#30340;&#27169;&#31946;&#12289;&#21547;&#31946;&#25110;&#19981;&#23436;&#25972;&#65292;&#23548;&#33268;&#23384;&#22312;&#22024;&#26434;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#20351;&#24471;&#29616;&#26377;&#30340;&#36328;&#27169;&#24577;&#21305;&#37197;&#26041;&#27861;&#23545;PTM&#26080;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;PTM&#22522;&#32447;&#65292;&#21629;&#21517;&#20026;Robust PointCloud-Text Matching&#26041;&#27861;&#65288;RoMa&#65289;&#12290;RoMa&#21253;&#21547;&#20004;&#20010;&#27169;&#22359;&#65306;&#21452;&#37325;&#27880;&#24847;&#24863;&#30693;&#27169;&#22359;&#65288;DAP&#65289;&#21644;&#40065;&#26834;&#36127;&#23545;&#27604;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19386v1 Announce Type: cross  Abstract: In this paper, we present and study a new instance-level retrieval task: PointCloud-Text Matching~(PTM), which aims to find the exact cross-modal instance that matches a given point-cloud query or text query. PTM could be applied to various scenarios, such as indoor/urban-canyon localization and scene retrieval. However, there exists no suitable and targeted dataset for PTM in practice. Therefore, we construct three new PTM benchmark datasets, namely 3D2T-SR, 3D2T-NR, and 3D2T-QA. We observe that the data is challenging and with noisy correspondence due to the sparsity, noise, or disorder of point clouds and the ambiguity, vagueness, or incompleteness of texts, which make existing cross-modal matching methods ineffective for PTM. To tackle these challenges, we propose a PTM baseline, named Robust PointCloud-Text Matching method (RoMa). RoMa consists of two modules: a Dual Attention Perception module (DAP) and a Robust Negative Contrast
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;AICAttack&#65292;&#26088;&#22312;&#36890;&#36807;&#24494;&#23567;&#30340;&#22270;&#20687;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#65292;&#22312;&#40657;&#30418;&#25915;&#20987;&#24773;&#26223;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.11940</link><description>&lt;p&gt;
AICAttack&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#20248;&#21270;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#23383;&#24149;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11940
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;AICAttack&#65292;&#26088;&#22312;&#36890;&#36807;&#24494;&#23567;&#30340;&#22270;&#20687;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#65292;&#22312;&#40657;&#30418;&#25915;&#20987;&#24773;&#26223;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21462;&#24471;&#20102;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31561;&#35768;&#22810;&#20219;&#21153;&#19978;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;CV&#21644;NLP&#20132;&#21449;&#28857;&#19978;&#30340;&#22270;&#20687;&#23383;&#24149;&#38382;&#39064;&#20013;&#65292;&#30456;&#20851;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;&#65292;&#31216;&#20026;AICAttack&#65288;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#20687;&#23383;&#24149;&#25915;&#20987;&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#22270;&#20687;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#12290;&#22312;&#40657;&#30418;&#25915;&#20987;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#21442;&#25968;&#25110;&#26799;&#24230;&#20449;&#24687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20505;&#36873;&#36873;&#25321;&#26426;&#21046;&#65292;&#21487;&#35782;&#21035;&#26368;&#20339;&#20687;&#32032;&#36827;&#34892;&#25915;&#20987;&#65292;&#28982;&#21518;&#37319;&#29992;&#24046;&#20998;&#36827;&#21270;&#65288;DE&#65289;&#26469;&#25200;&#20081;&#20687;&#32032;&#30340;RGB&#20540;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;AICAttack&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11940v1 Announce Type: cross  Abstract: Recent advances in deep learning research have shown remarkable achievements across many tasks in computer vision (CV) and natural language processing (NLP). At the intersection of CV and NLP is the problem of image captioning, where the related models' robustness against adversarial attacks has not been well studied. In this paper, we present a novel adversarial attack strategy, which we call AICAttack (Attention-based Image Captioning Attack), designed to attack image captioning models through subtle perturbations on images. Operating within a black-box attack scenario, our algorithm requires no access to the target model's architecture, parameters, or gradient information. We introduce an attention-based candidate selection mechanism that identifies the optimal pixels to attack, followed by Differential Evolution (DE) for perturbing pixels' RGB values. We demonstrate AICAttack's effectiveness through extensive experiments on benchma
&lt;/p&gt;</description></item></channel></rss>