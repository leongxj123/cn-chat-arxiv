<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#24341;&#20837;FedPLVM&#36890;&#36807;&#24314;&#31435;&#26041;&#24046;&#24863;&#30693;&#30340;&#21452;&#23618;&#21407;&#22411;&#32858;&#31867;&#21644;&#20351;&#29992;&#26032;&#22411;$\alpha$-&#31232;&#30095;&#21407;&#22411;&#25439;&#22833;&#65292;&#20197;&#20943;&#23569;&#36328;&#39046;&#22495;&#29305;&#24449;&#34920;&#31034;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.09048</link><description>&lt;p&gt;
&#39535;&#26381;&#24322;&#26500;&#25968;&#25454;&#22495;&#20013;&#32852;&#37030;&#21407;&#22411;&#23398;&#20064;&#20013;&#30340;&#36328;&#39046;&#22495;&#34920;&#31034;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09048
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;FedPLVM&#36890;&#36807;&#24314;&#31435;&#26041;&#24046;&#24863;&#30693;&#30340;&#21452;&#23618;&#21407;&#22411;&#32858;&#31867;&#21644;&#20351;&#29992;&#26032;&#22411;$\alpha$-&#31232;&#30095;&#21407;&#22411;&#25439;&#22833;&#65292;&#20197;&#20943;&#23569;&#36328;&#39046;&#22495;&#29305;&#24449;&#34920;&#31034;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#22312;&#19981;&#20849;&#20139;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;FL&#26041;&#27861;&#20551;&#35774;&#23458;&#25143;&#31471;&#20043;&#38388;&#20855;&#26377;&#30456;&#21516;&#30340;&#25968;&#25454;&#39046;&#22495;&#65292;&#20294;&#29616;&#23454;&#22330;&#26223;&#20013;&#36890;&#24120;&#28041;&#21450;&#24322;&#26500;&#25968;&#25454;&#39046;&#22495;&#12290;&#32852;&#37030;&#21407;&#22411;&#23398;&#20064;&#65288;FedPL&#65289;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;&#24179;&#22343;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#21407;&#22411;&#26469;&#22686;&#24378;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;FedPL&#26041;&#27861;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#21019;&#24314;&#30456;&#21516;&#25968;&#37327;&#30340;&#21407;&#22411;&#65292;&#23548;&#33268;&#36328;&#39046;&#22495;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#20351;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#30340;&#23458;&#25143;&#31471;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#20943;&#36731;&#36328;&#39046;&#22495;&#29305;&#24449;&#34920;&#31034;&#24046;&#24322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FedPLVM&#65292;&#23427;&#24314;&#31435;&#20102;&#26041;&#24046;&#24863;&#30693;&#30340;&#21452;&#23618;&#21407;&#22411;&#32858;&#31867;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$\alpha$-&#31232;&#30095;&#21407;&#22411;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09048v1 Announce Type: new  Abstract: Federated learning (FL) allows collaborative machine learning training without sharing private data. While most FL methods assume identical data domains across clients, real-world scenarios often involve heterogeneous data domains. Federated Prototype Learning (FedPL) addresses this issue, using mean feature vectors as prototypes to enhance model generalization. However, existing FedPL methods create the same number of prototypes for each client, leading to cross-domain performance gaps and disparities for clients with varied data distributions. To mitigate cross-domain feature representation variance, we introduce FedPLVM, which establishes variance-aware dual-level prototypes clustering and employs a novel $\alpha$-sparsity prototype loss. The dual-level prototypes clustering strategy creates local clustered prototypes based on private data features, then performs global prototypes clustering to reduce communication complexity and pres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#22522;&#30784;&#19978;&#34701;&#21512;&#29420;&#29305;&#30340;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08773</link><description>&lt;p&gt;
Veagle: &#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Veagle: Advancements in Multimodal Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#22522;&#30784;&#19978;&#34701;&#21512;&#29420;&#29305;&#30340;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#23545;&#35821;&#35328;&#21644;&#35270;&#35273;&#22914;&#20309;&#32467;&#21512;&#20135;&#29983;&#20102;&#27987;&#21402;&#20852;&#36259;&#65292;&#20174;&#32780;&#20652;&#29983;&#20102;&#26088;&#22312;&#26080;&#32541;&#25972;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24310;&#20280;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#33539;&#22260;&#20174;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#21040;&#35270;&#35273;&#23450;&#20301;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#20934;&#30830;&#35299;&#37322;&#22270;&#20687;&#24182;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#29616;&#26377;&#27169;&#22411;&#22810;&#27169;&#24577;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#38024;&#23545;&#24403;&#21069;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20013;&#35266;&#23519;&#21040;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;Veagle&#65292;&#34701;&#21512;&#20102;&#21463;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08773v1 Announce Type: cross  Abstract: Lately, researchers in artificial intelligence have been really interested in how language and vision come together, giving rise to the development of multimodal models that aim to seamlessly integrate textual and visual information. Multimodal models, an extension of Large Language Models (LLMs), have exhibited remarkable capabilities in addressing a diverse array of tasks, ranging from image captioning and visual question answering (VQA) to visual grounding. While these models have showcased significant advancements, challenges persist in accurately interpreting images and answering the question, a common occurrence in real-world scenarios. This paper introduces a novel approach to enhance the multimodal capabilities of existing models. In response to the limitations observed in current Vision Language Models (VLMs) and Multimodal Large Language Models (MLLMs), our proposed model Veagle, incorporates a unique mechanism inspired by th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#38024;&#23545;&#20154;&#31867;&#22270;&#20687;&#21512;&#25104;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#32654;&#23398;&#20998;&#25968;&#39044;&#27979;&#27169;&#22411;&#65292;&#35780;&#20272;&#29983;&#25104;&#22270;&#20687;&#30340;&#35270;&#35273;&#21560;&#24341;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#26377;&#29983;&#25104;&#30340;&#20154;&#31867;&#22270;&#20687;&#20013;&#20302;&#36136;&#37327;&#21306;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#33258;&#21160;&#32570;&#38519;&#26816;&#27979;&#65292;&#21516;&#26102;&#20063;&#30740;&#31350;&#20102;&#27169;&#22411;&#23545;&#27010;&#24565;&#35206;&#30422;&#24230;&#21644;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.05125</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65306;&#20851;&#20110;&#20154;&#31867;&#22270;&#20687;&#21512;&#25104;&#30340;&#32463;&#39564;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating Text-to-Image Generative Models: An Empirical Study on Human Image Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#38024;&#23545;&#20154;&#31867;&#22270;&#20687;&#21512;&#25104;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#32654;&#23398;&#20998;&#25968;&#39044;&#27979;&#27169;&#22411;&#65292;&#35780;&#20272;&#29983;&#25104;&#22270;&#20687;&#30340;&#35270;&#35273;&#21560;&#24341;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#26377;&#29983;&#25104;&#30340;&#20154;&#31867;&#22270;&#20687;&#20013;&#20302;&#36136;&#37327;&#21306;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#33258;&#21160;&#32570;&#38519;&#26816;&#27979;&#65292;&#21516;&#26102;&#20063;&#30740;&#31350;&#20102;&#27169;&#22411;&#23545;&#27010;&#24565;&#35206;&#30422;&#24230;&#21644;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#20154;&#31867;&#22270;&#20687;&#21512;&#25104;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#35780;&#20272;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;&#26041;&#38754;&#65306;&#31532;&#19968;&#65292;&#19987;&#27880;&#20110;&#22270;&#20687;&#36136;&#37327;&#65292;&#22914;&#32654;&#23398;&#21644;&#36924;&#30495;&#24230;&#65307;&#31532;&#20108;&#65292;&#36890;&#36807;&#27010;&#24565;&#35206;&#30422;&#24230;&#21644;&#20844;&#24179;&#24615;&#26469;&#26816;&#26597;&#25991;&#26412;&#26465;&#20214;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#32654;&#23398;&#20998;&#25968;&#39044;&#27979;&#27169;&#22411;&#65292;&#35780;&#20272;&#29983;&#25104;&#22270;&#20687;&#30340;&#35270;&#35273;&#21560;&#24341;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#26377;&#29983;&#25104;&#30340;&#20154;&#31867;&#22270;&#20687;&#20013;&#20302;&#36136;&#37327;&#21306;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#33258;&#21160;&#32570;&#38519;&#26816;&#27979;&#12290;&#25105;&#20204;&#23545;&#27010;&#24565;&#35206;&#30422;&#33539;&#22260;&#30340;&#25506;&#32034;&#35843;&#26597;&#20102;&#27169;&#22411;&#22312;&#20934;&#30830;&#35299;&#37322;&#21644;&#21576;&#29616;&#22522;&#20110;&#25991;&#26412;&#30340;&#27010;&#24565;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#25105;&#20204;&#23545;&#20844;&#24179;&#24615;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#20559;&#35265;&#65292;&#37325;&#28857;&#20851;&#27880;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#20154;&#31867;&#22270;&#20687;&#65292;&#20294;&#36825;&#31181;&#21452;&#37325;&#26041;&#38754;&#30340;&#26041;&#27861;&#26159;&#20026;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05125v1 Announce Type: cross  Abstract: In this paper, we present an empirical study introducing a nuanced evaluation framework for text-to-image (T2I) generative models, applied to human image synthesis. Our framework categorizes evaluations into two distinct groups: first, focusing on image qualities such as aesthetics and realism, and second, examining text conditions through concept coverage and fairness. We introduce an innovative aesthetic score prediction model that assesses the visual appeal of generated images and unveils the first dataset marked with low-quality regions in generated human images to facilitate automatic defect detection. Our exploration into concept coverage probes the model's effectiveness in interpreting and rendering text-based concepts accurately, while our analysis of fairness reveals biases in model outputs, with an emphasis on gender, race, and age. While our study is grounded in human imagery, this dual-faceted approach is designed with the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04306</link><description>&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Effectiveness Assessment of Recent Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#20986;&#29616;&#20195;&#34920;&#30528;&#36808;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#31243;&#24230;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#27969;&#34892;&#30340;LVLMs&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#22312;&#19987;&#19994;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#37327;&#36523;&#23450;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#33258;&#28982;&#12289;&#21307;&#30103;&#21644;&#24037;&#19994;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#30340;&#20840;&#38754;&#27979;&#35797;&#24179;&#21488;&#65292;&#28085;&#30422;&#20845;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#26174;&#33879;&#12289;&#20266;&#35013;&#21644;&#36879;&#26126;&#29289;&#20307;&#26816;&#27979;&#65292;&#20197;&#21450;&#24687;&#32905;&#21644;&#30382;&#32932;&#30149;&#21464;&#26816;&#27979;&#65292;&#20197;&#21450;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#26368;&#36817;&#19977;&#31181;&#24320;&#28304;LVLMs--MiniGPT-v2&#12289;LLaVA-1.5&#21644;Shikra--&#22312;&#35270;&#35273;&#35782;&#21035;&#21644;&#23450;&#20301;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04306v1 Announce Type: cross  Abstract: The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection. We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localiza
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23433;&#20840;&#39640;&#25928;&#22788;&#29702;&#20799;&#31461;&#24615;&#34384;&#24453;&#22270;&#20687;&#25968;&#25454;&#30340;&#22330;&#26223;&#35782;&#21035;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01183</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20799;&#31461;&#24615;&#34384;&#24453;&#22270;&#20687;&#22330;&#26223;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Leveraging Self-Supervised Learning for Scene Recognition in Child Sexual Abuse Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01183
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23433;&#20840;&#39640;&#25928;&#22788;&#29702;&#20799;&#31461;&#24615;&#34384;&#24453;&#22270;&#20687;&#25968;&#25454;&#30340;&#22330;&#26223;&#35782;&#21035;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
21&#19990;&#32426;&#30340;&#29359;&#32618;&#20998;&#20026;&#34394;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#24050;&#32463;&#25104;&#20026;&#23545;&#21518;&#32773;&#20154;&#20204;&#31119;&#31049;&#21644;&#23433;&#20840;&#26500;&#25104;&#20840;&#29699;&#23041;&#32961;&#12290;&#23427;&#25552;&#20986;&#30340;&#25361;&#25112;&#24517;&#39035;&#36890;&#36807;&#32479;&#19968;&#30340;&#20840;&#29699;&#21512;&#20316;&#26469;&#38754;&#23545;&#65292;&#25105;&#20204;&#24517;&#39035;&#27604;&#20197;&#24448;&#26356;&#21152;&#20381;&#36182;&#33258;&#21160;&#21270;&#20294;&#20540;&#24471;&#20449;&#36182;&#30340;&#24037;&#20855;&#26469;&#24212;&#23545;&#32593;&#32476;&#29359;&#32618;&#26085;&#30410;&#22686;&#38271;&#30340;&#26412;&#36136;&#12290;&#27599;&#24180;&#26377;&#36229;&#36807;1000&#19975;&#36215;&#20799;&#31461;&#24615;&#34384;&#24453;&#25253;&#21578;&#25552;&#20132;&#32473;&#32654;&#22269;&#22269;&#23478;&#22833;&#36394;&#21644;&#34987;&#21093;&#21066;&#20799;&#31461;&#20013;&#24515;&#65292;&#36229;&#36807;80%&#26469;&#33258;&#32593;&#32476;&#26469;&#28304;&#12290;&#22240;&#27492;&#65292;&#35843;&#26597;&#20013;&#24515;&#21644;&#28165;&#38500;&#20013;&#24515;&#26080;&#27861;&#25163;&#21160;&#22788;&#29702;&#21644;&#27491;&#30830;&#35843;&#26597;&#25152;&#26377;&#22270;&#20687;&#12290;&#22522;&#20110;&#27492;&#65292;&#33021;&#22815;&#23433;&#20840;&#39640;&#25928;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#30340;&#21487;&#38752;&#33258;&#21160;&#21270;&#24037;&#20855;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#22330;&#26223;&#35782;&#21035;&#20219;&#21153;&#23547;&#25214;&#29615;&#22659;&#20013;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#65292;&#33021;&#22815;&#32452;&#32455;&#21644;&#20998;&#31867;&#20799;&#31461;&#24615;&#34384;&#24453;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#22312;&#25935;&#24863;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01183v1 Announce Type: cross  Abstract: Crime in the 21st century is split into a virtual and real world. However, the former has become a global menace to people's well-being and security in the latter. The challenges it presents must be faced with unified global cooperation, and we must rely more than ever on automated yet trustworthy tools to combat the ever-growing nature of online offenses. Over 10 million child sexual abuse reports are submitted to the US National Center for Missing &amp; Exploited Children every year, and over 80% originated from online sources. Therefore, investigation centers and clearinghouses cannot manually process and correctly investigate all imagery. In light of that, reliable automated tools that can securely and efficiently deal with this data are paramount. In this sense, the scene recognition task looks for contextual cues in the environment, being able to group and classify child sexual abuse data without requiring to be trained on sensitive 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33402;&#26415;&#35774;&#35745;&#23454;&#29616;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#23567;&#26356;&#25913;&#29616;&#26377;&#35268;&#33539;&#26469;&#25269;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.04660</link><description>&lt;p&gt;
&#36890;&#36807;&#33402;&#26415;&#35774;&#35745;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness Through Artifact Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04660
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33402;&#26415;&#35774;&#35745;&#23454;&#29616;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#23567;&#26356;&#25913;&#29616;&#26377;&#35268;&#33539;&#26469;&#25269;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#20986;&#29616;&#32473;&#26426;&#22120;&#23398;&#20064;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#38459;&#30861;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#22823;&#22810;&#25968;&#38450;&#24481;&#26041;&#27861;&#37117;&#25913;&#21464;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#24335;&#65288;&#22914;&#23545;&#25239;&#24615;&#35757;&#32451;&#65289;&#25110;&#25512;&#29702;&#36807;&#31243;&#65288;&#22914;&#38543;&#26426;&#24179;&#28369;&#65289;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#27169;&#22411;&#20173;&#28982;&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;&#22312;&#26576;&#20123;&#39046;&#22495;&#22914;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#35937;&#26159;&#25353;&#29031;&#35268;&#33539;&#26469;&#35774;&#35745;&#65288;&#22914;&#26631;&#24535;&#35268;&#33539;&#65289;&#12290;&#20026;&#20102;&#25913;&#21892;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37325;&#26032;&#23450;&#20041;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#23545;&#29616;&#26377;&#35268;&#33539;&#36827;&#34892;&#24494;&#23567;&#30340;&#26356;&#25913;&#65292;&#20197;&#38450;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#25105;&#20204;&#23558;&#33402;&#26415;&#35774;&#35745;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#21644;&#36138;&#23146;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#22312;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#39046;&#22495;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20351;&#20854;&#33021;&#22815;&#25913;&#21464;&#20132;&#36890;&#26631;&#24535;&#20013;&#30340;&#35937;&#24418;&#22270;&#26631;&#65288;&#21363;&#26631;&#24535;&#20869;&#30340;&#31526;&#21495;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples arose as a challenge for machine learning. To hinder them, most defenses alter how models are trained (e.g., adversarial training) or inference is made (e.g., randomized smoothing). Still, while these approaches markedly improve models' adversarial robustness, models remain highly susceptible to adversarial examples. Identifying that, in certain domains such as traffic-sign recognition, objects are implemented per standards specifying how artifacts (e.g., signs) should be designed, we propose a novel approach for improving adversarial robustness. Specifically, we offer a method to redefine standards, making minor changes to existing ones, to defend against adversarial examples. We formulate the problem of artifact design as a robust optimization problem, and propose gradient-based and greedy search methods to solve it. We evaluated our approach in the domain of traffic-sign recognition, allowing it to alter traffic-sign pictograms (i.e., symbols within the signs) a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;&#22522;&#20110;f-&#25955;&#24230;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;f-&#39046;&#22495;&#24046;&#24322;&#24230;&#37327;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#21435;&#38500;&#32477;&#23545;&#20540;&#20989;&#25968;&#21644;&#24341;&#20837;&#32553;&#25918;&#21442;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#30446;&#26631;&#35823;&#24046;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20174;&#32780;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#24674;&#22797;&#20197;&#21069;&#30340;KL&#32467;&#26524;&#65292;&#23558;&#31639;&#27861;&#21644;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#65292;&#24182;&#36890;&#36807;&#23450;&#20301;&#25216;&#26415;&#24320;&#21457;&#20102;&#24555;&#36895;&#29575;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#22522;&#20110;f-DD&#30340;&#39046;&#22495;&#23398;&#20064;&#31639;&#27861;&#22312;&#27969;&#34892;&#30340;UDA&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01887</link><description>&lt;p&gt;
&#22522;&#20110;f-&#25955;&#24230;&#21407;&#29702;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#65306;&#19968;&#20010;&#25913;&#36827;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
On f-Divergence Principled Domain Adaptation: An Improved Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;&#22522;&#20110;f-&#25955;&#24230;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;f-&#39046;&#22495;&#24046;&#24322;&#24230;&#37327;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#21435;&#38500;&#32477;&#23545;&#20540;&#20989;&#25968;&#21644;&#24341;&#20837;&#32553;&#25918;&#21442;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#30446;&#26631;&#35823;&#24046;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20174;&#32780;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#24674;&#22797;&#20197;&#21069;&#30340;KL&#32467;&#26524;&#65292;&#23558;&#31639;&#27861;&#21644;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#65292;&#24182;&#36890;&#36807;&#23450;&#20301;&#25216;&#26415;&#24320;&#21457;&#20102;&#24555;&#36895;&#29575;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#22522;&#20110;f-DD&#30340;&#39046;&#22495;&#23398;&#20064;&#31639;&#27861;&#22312;&#27969;&#34892;&#30340;UDA&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#22312;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25913;&#36827;Acuna&#31561;&#20154;&#65288;2021&#24180;&#65289;&#25552;&#20986;&#30340;UDA&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#23545;&#20854;&#22522;&#20110;f-&#25955;&#24230;&#30340;&#24046;&#24322;&#24230;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#21363;f-&#39046;&#22495;&#24046;&#24322;&#65288;f-DD&#65289;&#12290;&#36890;&#36807;&#21435;&#38500;&#32477;&#23545;&#20540;&#20989;&#25968;&#24182;&#24341;&#20837;&#19968;&#20010;&#32553;&#25918;&#21442;&#25968;&#65292;f-DD&#20135;&#29983;&#20102;&#26032;&#30340;&#30446;&#26631;&#35823;&#24046;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#24674;&#22797;&#20197;&#21069;&#22522;&#20110;KL&#30340;&#32467;&#26524;&#65292;&#24182;&#24357;&#21512;&#20102;Acuna&#31561;&#20154;&#65288;2021&#24180;&#65289;&#20013;&#25552;&#20986;&#30340;&#31639;&#27861;&#21644;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#21033;&#29992;&#23450;&#20301;&#25216;&#26415;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#24555;&#36895;&#29575;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27969;&#34892;&#30340;UDA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22522;&#20110;f-DD&#30340;&#39046;&#22495;&#23398;&#20064;&#31639;&#27861;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (UDA) plays a crucial role in addressing distribution shifts in machine learning. In this work, we improve the theoretical foundations of UDA proposed by Acuna et al. (2021) by refining their f-divergence-based discrepancy and additionally introducing a new measure, f-domain discrepancy (f-DD). By removing the absolute value function and incorporating a scaling parameter, f-DD yields novel target error and sample complexity bounds, allowing us to recover previous KL-based results and bridging the gap between algorithms and theory presented in Acuna et al. (2021). Leveraging a localization technique, we also develop a fast-rate generalization bound. Empirical results demonstrate the superior performance of f-DD-based domain learning algorithms over previous works in popular UDA benchmarks.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#30284;&#20999;&#29255;&#20013;&#19978;&#30382;&#32454;&#32990;&#20998;&#21106;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#24341;&#23548;&#20998;&#21106;&#20986;&#33391;&#24615;&#19978;&#30382;&#32454;&#32990;&#12289;&#21407;&#20301;&#30149;&#21464;&#21644;&#28024;&#28070;&#24615;&#19978;&#30382;&#32454;&#32990;&#12290;</title><link>https://arxiv.org/abs/2311.13261</link><description>&lt;p&gt;
&#20083;&#33146;&#30284;&#20999;&#29255;&#20013;&#33391;&#24615;&#19978;&#30382;&#32454;&#32990;&#12289;&#21407;&#20301;&#30149;&#21464;&#21644;&#28024;&#28070;&#24615;&#19978;&#30382;&#32454;&#32990;&#30340;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#24341;&#23548;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Immunohistochemistry guided segmentation of benign epithelial cells, in situ lesions, and invasive epithelial cells in breast cancer slides
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13261
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#30284;&#20999;&#29255;&#20013;&#19978;&#30382;&#32454;&#32990;&#20998;&#21106;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#24341;&#23548;&#20998;&#21106;&#20986;&#33391;&#24615;&#19978;&#30382;&#32454;&#32990;&#12289;&#21407;&#20301;&#30149;&#21464;&#21644;&#28024;&#28070;&#24615;&#19978;&#30382;&#32454;&#32990;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#33258;&#21160;&#20998;&#26512;&#32452;&#32455;&#30149;&#29702;&#20999;&#29255;&#12290;&#33258;&#21160;&#35780;&#20272;&#21487;&#20197;&#25552;&#39640;&#35786;&#26029;&#25928;&#29575;&#65292;&#24182;&#24110;&#21161;&#25214;&#21040;&#24418;&#24577;&#29305;&#24449;&#19982;&#20020;&#24202;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#24320;&#21457;&#36825;&#26679;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#36776;&#35748;&#28024;&#28070;&#24615;&#19978;&#30382;&#32454;&#32990;&#65292;&#24182;&#23558;&#20854;&#19982;&#33391;&#24615;&#19978;&#30382;&#32454;&#32990;&#21644;&#21407;&#20301;&#30149;&#21464;&#20998;&#24320;&#23558;&#26159;&#31532;&#19968;&#27493;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#30284;&#20999;&#29255;&#20013;&#19978;&#30382;&#32454;&#32990;&#20998;&#21106;&#30340;AI&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#37325;&#26032;&#26579;&#33394;&#34880;&#32418;&#34507;&#30333;&#21644;&#21980;&#37240;&#24615;&#26579;&#33394;&#32454;&#32990;&#35282;&#34507;&#30333;(CK) AE1/AE3 HE&#20999;&#29255;&#65292;&#20197;&#21450;&#30149;&#29702;&#23398;&#23478;&#30340;&#27880;&#37322;&#29983;&#25104;&#20102;&#19978;&#30382;&#22522;&#26412;&#30495;&#20540;&#25513;&#27169;&#12290;HE/CK&#22270;&#20687;&#23545;&#34987;&#29992;&#20110;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#25968;&#25454;&#22686;&#24378;&#34987;&#29992;&#26469;&#20351;&#27169;&#22411;&#26356;&#31283;&#20581;&#12290;839&#21517;&#24739;&#32773;&#30340;&#32452;&#32455;&#24494;&#38453;&#21015;&#65288;TMAs&#65289;&#21644;&#20004;&#21517;&#24739;&#32773;&#30340;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#29992;&#20110;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13261v2 Announce Type: replace-cross  Abstract: Digital pathology enables automatic analysis of histopathological sections using artificial intelligence (AI). Automatic evaluation could improve diagnostic efficiency and help find associations between morphological features and clinical outcome. For development of such prediction models, identifying invasive epithelial cells, and separating these from benign epithelial cells and in situ lesions would be the first step. In this study, we aimed to develop an AI model for segmentation of epithelial cells in sections from breast cancer. We generated epithelial ground truth masks by restaining hematoxylin and eosin (HE) sections with cytokeratin (CK) AE1/AE3, and by pathologists' annotations. HE/CK image pairs were used to train a convolutional neural network, and data augmentation was used to make the model more robust. Tissue microarrays (TMAs) from 839 patients, and whole slide images from two patients were used for training an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZoDiac&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#22312;&#21487;&#35757;&#32451;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#27880;&#20837;&#27700;&#21360;&#65292;&#20174;&#32780;&#20351;&#27700;&#21360;&#33021;&#22815;&#22312;&#21463;&#25915;&#20987;&#26102;&#21487;&#38752;&#26816;&#27979;&#21040;&#65292;&#23545;&#26368;&#20808;&#36827;&#30340;&#27700;&#21360;&#25915;&#20987;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#27700;&#21360;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04247</link><description>&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#25216;&#26415;&#30340;&#40065;&#26834;&#22270;&#20687;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Robust Image Watermarking using Stable Diffusion. (arXiv:2401.04247v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZoDiac&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#22312;&#21487;&#35757;&#32451;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#27880;&#20837;&#27700;&#21360;&#65292;&#20174;&#32780;&#20351;&#27700;&#21360;&#33021;&#22815;&#22312;&#21463;&#25915;&#20987;&#26102;&#21487;&#38752;&#26816;&#27979;&#21040;&#65292;&#23545;&#26368;&#20808;&#36827;&#30340;&#27700;&#21360;&#25915;&#20987;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#27700;&#21360;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#27700;&#21360;&#23545;&#20110;&#36861;&#36394;&#22270;&#20687;&#26469;&#28304;&#21644;&#22768;&#26126;&#25152;&#26377;&#26435;&#38750;&#24120;&#37325;&#35201;&#12290;&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#31283;&#23450;&#25193;&#25955;&#65289;&#30340;&#20986;&#29616;&#65292;&#33021;&#22815;&#21019;&#24314;&#34394;&#20551;&#20294;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#27700;&#21360;&#25104;&#20026;&#20102;&#23588;&#20026;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#20351;&#29983;&#25104;&#30340;&#22270;&#20687;&#21487;&#38752;&#22320;&#36776;&#35748;&#20986;&#26469;&#12290;&#28982;&#32780;&#65292;&#27491;&#26159;&#36825;&#31181;&#31283;&#23450;&#25193;&#25955;&#25216;&#26415;&#21487;&#20197;&#31227;&#38500;&#20351;&#29992;&#29616;&#26377;&#26041;&#27861;&#27880;&#20837;&#30340;&#27700;&#21360;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZoDiac&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#23558;&#27700;&#21360;&#27880;&#20837;&#21040;&#21487;&#35757;&#32451;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#22312;&#21463;&#25915;&#20987;&#26102;&#20173;&#28982;&#21487;&#20197;&#21487;&#38752;&#22320;&#22312;&#28508;&#22312;&#21521;&#37327;&#20013;&#26816;&#27979;&#21040;&#27700;&#21360;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598; MS-COCO&#12289;DiffusionDB &#21644; WikiArt &#19978;&#35780;&#20272;&#20102; ZoDiac&#65292;&#24182;&#21457;&#29616; ZoDiac &#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#27700;&#21360;&#25915;&#20987;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#27700;&#21360;&#26816;&#27979;&#29575;&#36229;&#36807;98%&#65292;&#35823;&#25253;&#29575;&#20302;&#20110;6.4%&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#27700;&#21360;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31283;&#23450;&#25193;&#25955;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Watermarking images is critical for tracking image provenance and claiming ownership. With the advent of generative models, such as stable diffusion, able to create fake but realistic images, watermarking has become particularly important, e.g., to make generated images reliably identifiable. Unfortunately, the very same stable diffusion technology can remove watermarks injected using existing methods. To address this problem, we present a ZoDiac, which uses a pre-trained stable diffusion model to inject a watermark into the trainable latent space, resulting in watermarks that can be reliably detected in the latent vector, even when attacked. We evaluate ZoDiac on three benchmarks, MS-COCO, DiffusionDB, and WikiArt, and find that ZoDiac is robust against state-of-the-art watermark attacks, with a watermark detection rate over 98% and a false positive rate below 6.4%, outperforming state-of-the-art watermarking methods. Our research demonstrates that stable diffusion is a promising appr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#35299;&#20915;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#30340;&#23545;&#31216;&#24615;&#38480;&#21046;&#21644;&#24341;&#20837;&#20840;&#23616;&#20449;&#24687;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#22312;CIFAR-10/100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#23545;&#25239;&#35757;&#32451;&#21644;&#30693;&#35782;&#33976;&#39311;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13948</link><description>&lt;p&gt;
&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Decoupled Kullback-Leibler Divergence Loss. (arXiv:2305.13948v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#35299;&#20915;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#30340;&#23545;&#31216;&#24615;&#38480;&#21046;&#21644;&#24341;&#20837;&#20840;&#23616;&#20449;&#24687;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#22312;CIFAR-10/100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#23545;&#25239;&#35757;&#32451;&#21644;&#30693;&#35782;&#33976;&#39311;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26356;&#28145;&#20837;&#22320;&#25506;&#31350;&#20102;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#21457;&#29616;&#23427;&#19982;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#31561;&#20215;&#65292;&#21518;&#32773;&#30001;&#21152;&#26435;&#22343;&#26041;&#24046;&#25439;&#22833;&#21644;&#21253;&#21547;&#36719;&#26631;&#31614;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#32452;&#25104;&#12290;&#36890;&#36807;&#23545;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#30830;&#23450;&#20102;&#20004;&#20010;&#25913;&#36827;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#30693;&#35782;&#33976;&#39311;&#31561;&#22330;&#26223;&#19979;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#30340;&#23545;&#31216;&#24615;&#38480;&#21046;&#38382;&#39064;&#12290;&#36825;&#20010;&#25913;&#36827;&#20445;&#35777;&#20102;&#22312;&#35757;&#32451;&#26399;&#38388;wMSE&#32452;&#20214;&#22987;&#32456;&#26377;&#25928;&#65292;&#25552;&#20379;&#39069;&#22806;&#30340;&#26500;&#36896;&#24615;&#26263;&#31034;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#20840;&#23616;&#20449;&#24687;&#24341;&#20837;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#29992;&#20110;&#31867;&#20869;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#12290;&#36890;&#36807;&#36825;&#20004;&#20010;&#25913;&#36827;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25913;&#36827;&#30340;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;CIFAR-10/100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#65292;&#37325;&#28857;&#26159;&#23545;&#25239;&#35757;&#32451;&#21644;&#30693;&#35782;&#33976;&#39311;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we delve deeper into the Kullback-Leibler (KL) Divergence loss and observe that it is equivalent to the Doupled Kullback-Leibler (DKL) Divergence loss that consists of 1) a weighted Mean Square Error (wMSE) loss and 2) a Cross-Entropy loss incorporating soft labels. From our analysis of the DKL loss, we have identified two areas for improvement. Firstly, we address the limitation of DKL in scenarios like knowledge distillation by breaking its asymmetry property in training optimization. This modification ensures that the wMSE component is always effective during training, providing extra constructive cues. Secondly, we introduce global information into DKL for intra-class consistency regularization. With these two enhancements, we derive the Improved Kullback-Leibler (IKL) Divergence loss and evaluate its effectiveness by conducting experiments on CIFAR-10/100 and ImageNet datasets, focusing on adversarial training and knowledge distillation tasks. The proposed approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25163;&#25915;&#20987;VAEs&#30340;&#26368;&#20339;&#26041;&#27861;&#26159;&#21033;&#29992;&#30001;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32593;&#32476;&#24341;&#36215;&#30340;&#38543;&#26426;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#30340;&#26041;&#21521;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2208.03923</link><description>&lt;p&gt;
&#36890;&#36807;&#26412;&#22320;&#20960;&#20309;&#35282;&#24230;&#29702;&#35299;VAEs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness of VAEs through the lens of local geometry. (arXiv:2208.03923v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25163;&#25915;&#20987;VAEs&#30340;&#26368;&#20339;&#26041;&#27861;&#26159;&#21033;&#29992;&#30001;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32593;&#32476;&#24341;&#36215;&#30340;&#38543;&#26426;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#30340;&#26041;&#21521;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#36827;&#34892;&#26080;&#30417;&#30563;&#25915;&#20987;&#26102;&#65292;&#23545;&#25163;&#20250;&#25214;&#21040;&#19968;&#20010;&#36755;&#20837;&#26679;&#26412;&#20013;&#30340;&#23567;&#25200;&#21160;&#65292;&#20174;&#32780;&#26174;&#30528;&#25913;&#21464;&#20854;&#28508;&#22312;&#31354;&#38388;&#32534;&#30721;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#19968;&#20010;&#22266;&#23450;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#12290;&#36825;&#31181;&#33030;&#24369;&#24615;&#24050;&#30693;&#30340;&#21407;&#22240;&#26159;&#28508;&#22312;&#21518;&#39564;&#20998;&#24067;&#30340;&#36817;&#20284;&#19982;&#20808;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#23548;&#33268;&#30340;&#28508;&#22312;&#31354;&#38388;&#25197;&#26354;&#12290;&#22240;&#27492;&#65292;&#36755;&#20837;&#26679;&#26412;&#20013;&#30340;&#24494;&#23567;&#21464;&#21270;&#21487;&#33021;&#20250;&#23558;&#20854;&#32534;&#30721;&#31227;&#21160;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20302;/&#38646;&#23494;&#24230;&#21306;&#22495;&#65292;&#20174;&#32780;&#20135;&#29983;&#26080;&#38480;&#21046;&#30340;&#29983;&#25104;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25163;&#25915;&#20987;VAEs&#30340;&#26368;&#20339;&#26041;&#27861;&#26159;&#21033;&#29992;&#30001;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32593;&#32476;&#24341;&#36215;&#30340;&#38543;&#26426;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#30340;&#26041;&#21521;&#20559;&#24046;&#12290;&#32534;&#30721;&#22120;&#30340;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#27979;&#37327;&#23427;&#20174;&#36755;&#20837;&#21040;&#28508;&#22312;&#31354;&#38388;&#30340;&#24494;&#23567;&#28508;&#22312;&#20307;&#31215;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#34987;&#35270;&#20026;&#20998;&#26512;&#36755;&#20837;&#25200;&#21160;&#23548;&#33268;&#28508;&#22312;&#31354;&#38388;&#25197;&#26354;&#25928;&#26524;&#30340;&#38236;&#22836;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an unsupervised attack on variational autoencoders (VAEs), an adversary finds a small perturbation in an input sample that significantly changes its latent space encoding, thereby compromising the reconstruction for a fixed decoder. A known reason for such vulnerability is the distortions in the latent space resulting from a mismatch between approximated latent posterior and a prior distribution. Consequently, a slight change in an input sample can move its encoding to a low/zero density region in the latent space resulting in an unconstrained generation. This paper demonstrates that an optimal way for an adversary to attack VAEs is to exploit a directional bias of a stochastic pullback metric tensor induced by the encoder and decoder networks. The pullback metric tensor of an encoder measures the change in infinitesimal latent volume from an input to a latent space. Thus, it can be viewed as a lens to analyse the effect of input perturbations leading to latent space distortions. We
&lt;/p&gt;</description></item></channel></rss>