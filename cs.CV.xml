<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.19103</link><description>&lt;p&gt;
&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#40657;&#30418;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19103
&lt;/p&gt;
&lt;p&gt;
PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#25163;&#21160;&#21046;&#20316;&#25552;&#31034;&#32780;&#23548;&#33268;&#24037;&#20316;&#32321;&#37325;&#12290;&#36825;&#19968;&#25361;&#25112;&#20419;&#20351;&#20102;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;T2I&#27169;&#22411;&#20043;&#38388;&#30340;&#21487;&#20256;&#36882;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#38656;&#35201;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#30333;&#30418;&#35775;&#38382;&#65292;&#24182;&#20135;&#29983;&#38750;&#30452;&#35266;&#30340;&#25552;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PRISM&#65292;&#36825;&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#23601;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#29425;&#30340;&#21551;&#21457;&#65292;PRISM&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;&#32473;&#23450;&#21442;&#32771;&#22270;&#20687;&#30340;&#20505;&#36873;&#25552;&#31034;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;PRISM&#22312;&#20026;&#23545;&#35937;&#12289;&#26679;&#24335;&#31561;&#29983;&#25104;&#20934;&#30830;&#25552;&#31034;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19103v1 Announce Type: cross  Abstract: Prompt engineering is effective for controlling the output of text-to-image (T2I) generative models, but it is also laborious due to the need for manually crafted prompts. This challenge has spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, and produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, sty
&lt;/p&gt;</description></item><item><title>GeoSAM&#26159;&#19968;&#20010;&#22522;&#20110;SAM&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;CNN&#20998;&#21106;&#27169;&#22411;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#22320;&#29702;&#22270;&#20687;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.11319</link><description>&lt;p&gt;
GeoSAM: &#20351;&#29992;&#31232;&#30095;&#21644;&#23494;&#38598;&#30340;&#35270;&#35273;&#25552;&#31034;&#23545;SAM&#36827;&#34892;&#25913;&#36827;&#65292;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#31227;&#21160;&#22522;&#30784;&#35774;&#26045;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
GeoSAM: Fine-tuning SAM with Sparse and Dense Visual Prompting for Automated Segmentation of Mobility Infrastructure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11319
&lt;/p&gt;
&lt;p&gt;
GeoSAM&#26159;&#19968;&#20010;&#22522;&#20110;SAM&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;CNN&#20998;&#21106;&#27169;&#22411;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#22320;&#29702;&#22270;&#20687;&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24212;&#29992;&#20110;&#33258;&#28982;&#22270;&#20687;&#20998;&#21106;&#26102;&#65292;Segment Anything Model (SAM)&#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#22320;&#29702;&#22270;&#20687;&#65288;&#22914;&#33322;&#25293;&#21644;&#21355;&#26143;&#22270;&#20687;&#65289;&#20013;&#38754;&#20020;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#21106;&#36947;&#36335;&#12289;&#20154;&#34892;&#36947;&#21644;&#20154;&#34892;&#27178;&#36947;&#31561;&#31227;&#21160;&#22522;&#30784;&#35774;&#26045;&#26102;&#12290;&#36825;&#31181;&#36739;&#24046;&#30340;&#24615;&#33021;&#28304;&#20110;&#36825;&#20123;&#23545;&#35937;&#30340;&#31364;&#23567;&#29305;&#24449;&#65292;&#23427;&#20204;&#30340;&#32441;&#29702;&#34701;&#20837;&#29615;&#22659;&#20013;&#65292;&#20197;&#21450;&#26641;&#26408;&#12289;&#24314;&#31569;&#29289;&#12289;&#36710;&#36742;&#21644;&#34892;&#20154;&#31561;&#29289;&#20307;&#30340;&#24178;&#25200;&#65292;&#36825;&#20123;&#37117;&#21487;&#33021;&#20351;&#27169;&#22411;&#22833;&#21435;&#23450;&#21521;&#20135;&#29983;&#19981;&#20934;&#30830;&#30340;&#20998;&#21106;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22320;&#29702;SAM&#65288;GeoSAM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;SAM&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#26469;&#33258;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#23494;&#38598;&#35270;&#35273;&#25552;&#31034;&#21644;&#39044;&#35757;&#32451;CNN&#20998;&#21106;&#27169;&#22411;&#30340;&#31232;&#30095;&#35270;&#35273;&#25552;&#31034;&#23454;&#26045;&#20102;&#32454;&#35843;&#31574;&#30053;&#12290;&#25152;&#25552;&#20986;&#30340;GeoSAM&#22312;&#22320;&#29702;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#12289;&#34892;&#20154;&#22522;&#30784;&#35774;&#26045;&#30340;&#20998;&#21106;&#24615;&#33021;&#25552;&#21319;&#20102;26&#65285;&#12289;7&#65285;&#21644;17&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) has shown impressive performance when applied to natural image segmentation. However, it struggles with geographical images like aerial and satellite imagery, especially when segmenting mobility infrastructure including roads, sidewalks, and crosswalks. This inferior performance stems from the narrow features of these objects, their textures blending into the surroundings, and interference from objects like trees, buildings, vehicles, and pedestrians - all of which can disorient the model to produce inaccurate segmentation maps. To address these challenges, we propose Geographical SAM (GeoSAM), a novel SAM-based framework that implements a fine-tuning strategy using the dense visual prompt from zero-shot learning, and the sparse visual prompt from a pre-trained CNN segmentation model. The proposed GeoSAM outperforms existing approaches for geographical image segmentation, specifically by 26%, 7%, and 17% for road infrastructure, pedestrian infrastructur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;&#24050;&#26377;&#27169;&#22411;&#24182;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#21644;&#39640;&#24615;&#33021;&#30340;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#12290;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29273;&#40831;&#23454;&#20363;&#20998;&#21106;&#21644;&#29273;&#40831;&#23450;&#20301;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17176</link><description>&lt;p&gt;
&#20174;&#20840;&#26223;X&#23556;&#32447;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach to Teeth Segmentation and Orientation from Panoramic X-rays. (arXiv:2310.17176v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;&#24050;&#26377;&#27169;&#22411;&#24182;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#21644;&#39640;&#24615;&#33021;&#30340;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#12290;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29273;&#40831;&#23454;&#20363;&#20998;&#21106;&#21644;&#29273;&#40831;&#23450;&#20301;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#22312;&#29616;&#20195;&#21475;&#33108;&#20445;&#20581;&#20013;&#26159;&#22522;&#30784;&#65292;&#21487;&#23454;&#29616;&#31934;&#30830;&#35786;&#26029;&#12289;&#27835;&#30103;&#35745;&#21010;&#21644;&#29273;&#40831;&#31181;&#26893;&#35774;&#35745;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#12290;&#25105;&#20204;&#26681;&#25454;FUSegNet&#26500;&#24314;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#21021;&#29992;&#20110;&#21019;&#38754;&#20998;&#21106;&#30340;&#27969;&#34892;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23558;&#22522;&#20110;&#32593;&#26684;&#30340;&#27880;&#24847;&#21147;&#38376;&#24341;&#20837;&#36339;&#36291;&#36830;&#25509;&#36827;&#34892;&#20102;&#20462;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#24341;&#20837;&#23450;&#21521;&#36793;&#30028;&#26694;&#65288;OBB&#65289;&#29983;&#25104;&#65292;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#29273;&#40831;&#23450;&#20301;&#20272;&#35745;&#12290;&#22312;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;DNS&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;543&#20010;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#65292;&#25105;&#20204;&#22312;&#29273;&#40831;&#23454;&#20363;&#20998;&#21106;&#20013;&#24471;&#21040;&#20102;&#26368;&#39640;&#30340;&#20132;&#24182;&#27604;&#65288;IoU&#65289;&#24471;&#20998;82.43%&#65292;Dice&#30456;&#20284;&#31995;&#25968;&#65288;DSC&#65289;&#24471;&#20998;90.37%&#65292;&#22312;OBB&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26059;&#36716;&#30340;&#20132;&#24182;&#27604;&#65288;RIoU&#65289;&#24471;&#20998;82.82%&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate teeth segmentation and orientation are fundamental in modern oral healthcare, enabling precise diagnosis, treatment planning, and dental implant design. In this study, we present a comprehensive approach to teeth segmentation and orientation from panoramic X-ray images, leveraging deep learning techniques. We build our model based on FUSegNet, a popular model originally developed for wound segmentation, and introduce modifications by incorporating grid-based attention gates into the skip connections. We introduce oriented bounding box (OBB) generation through principal component analysis (PCA) for precise tooth orientation estimation. Evaluating our approach on the publicly available DNS dataset, comprising 543 panoramic X-ray images, we achieve the highest Intersection-over-Union (IoU) score of 82.43% and Dice Similarity Coefficient (DSC) score of 90.37% among compared models in teeth instance segmentation. In OBB analysis, we obtain the Rotated IoU (RIoU) score of 82.82%. We
&lt;/p&gt;</description></item></channel></rss>