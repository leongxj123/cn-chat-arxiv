<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#36827;&#34892;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#65292;&#19987;&#27880;&#20110;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#25152;&#38656;&#30149;&#29702;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#19987;&#23478;&#27880;&#37322;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#35774;&#35745;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#20934;&#25991;&#26412;&#29305;&#24449;&#36716;&#25442;&#22120;&#27169;&#22359;&#12290;</title><link>https://arxiv.org/abs/2404.00226</link><description>&lt;p&gt;
&#24819;&#35201;&#30340;&#35774;&#35745;&#65306;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#36827;&#34892;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Design as Desired: Utilizing Visual Question Answering for Multimodal Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#36827;&#34892;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#65292;&#19987;&#27880;&#20110;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#25152;&#38656;&#30149;&#29702;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#19987;&#23478;&#27880;&#37322;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#35774;&#35745;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#20934;&#25991;&#26412;&#29305;&#24449;&#36716;&#25442;&#22120;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#22312;&#21307;&#30103;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#65292;&#20174;&#25104;&#23545;&#30340;&#21307;&#30103;&#25253;&#21578;&#20013;&#23398;&#20064;&#21307;&#23398;&#35270;&#35273;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#39044;&#35757;&#32451;&#20219;&#21153;&#38656;&#35201;&#20020;&#24202;&#21307;&#29983;&#39069;&#22806;&#30340;&#27880;&#37322;&#65292;&#22823;&#22810;&#25968;&#20219;&#21153;&#26410;&#33021;&#26126;&#30830;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#19981;&#21516;&#30149;&#29702;&#29305;&#24449;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#36827;&#34892;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#22242;&#38431;&#65292;&#20197;&#24341;&#23548;&#26694;&#26550;&#19987;&#27880;&#20110;&#30446;&#26631;&#30149;&#29702;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21307;&#30103;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#35774;&#35745;&#20102;&#19982;&#19981;&#21516;&#30142;&#30149;&#30456;&#20851;&#30340;&#22810;&#31890;&#24230;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#36825;&#26377;&#21161;&#20110;&#26694;&#26550;&#22312;&#39044;&#35757;&#32451;&#20013;&#26080;&#38656;&#19987;&#23478;&#39069;&#22806;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#20934;&#25991;&#26412;&#29305;&#24449;&#36716;&#25442;&#22120;&#27169;&#22359;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#35270;&#35273;&#29305;&#24449;&#36716;&#25442;&#21040;&#25509;&#36817;&#25991;&#26412;&#39046;&#22495;&#30340;&#20934;&#25991;&#26412;&#31354;&#38388;&#26469;&#36741;&#21161;&#39044;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00226v1 Announce Type: cross  Abstract: Multimodal pre-training demonstrates its potential in the medical domain, which learns medical visual representations from paired medical reports. However, many pre-training tasks require extra annotations from clinicians, and most of them fail to explicitly guide the model to learn the desired features of different pathologies. To the best of our knowledge, we are the first to utilize Visual Question Answering (VQA) for multimodal pre-training to guide the framework focusing on targeted pathological features. In this work, we leverage descriptions in medical reports to design multi-granular question-answer pairs associated with different diseases, which assist the framework in pre-training without requiring extra annotations from experts. We also propose a novel pre-training framework with a quasi-textual feature transformer, a module designed to transform visual features into a quasi-textual space closer to the textual domain via a c
&lt;/p&gt;</description></item><item><title>ParFormer&#25552;&#20986;&#20102;&#24182;&#34892;&#23616;&#37096;&#20840;&#23616;&#26631;&#35760;&#28151;&#21512;&#22120;&#21644;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;&#65292;&#20248;&#21270;&#20102;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;CNN&#21644;&#26368;&#20808;&#36827;&#30340;Transformer&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.15004</link><description>&lt;p&gt;
ParFormer&#65306;&#20855;&#26377;&#24182;&#34892;&#23616;&#37096;&#20840;&#23616;&#26631;&#35760;&#28151;&#21512;&#22120;&#21644;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;&#30340;&#35270;&#35273;Transformer&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
ParFormer: Vision Transformer Baseline with Parallel Local Global Token Mixer and Convolution Attention Patch Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15004
&lt;/p&gt;
&lt;p&gt;
ParFormer&#25552;&#20986;&#20102;&#24182;&#34892;&#23616;&#37096;&#20840;&#23616;&#26631;&#35760;&#28151;&#21512;&#22120;&#21644;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;&#65292;&#20248;&#21270;&#20102;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;CNN&#21644;&#26368;&#20808;&#36827;&#30340;Transformer&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ParFormer&#20316;&#20026;&#19968;&#31181;&#22686;&#24378;&#22411;Transformer&#26550;&#26500;&#65292;&#20801;&#35768;&#23558;&#19981;&#21516;&#30340;&#26631;&#35760;&#28151;&#21512;&#22120;&#25972;&#21512;&#21040;&#21333;&#20010;&#38454;&#27573;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#12290;&#21516;&#26102;&#25972;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#25968;&#25454;&#65292;&#23454;&#29616;&#23545;&#30701;&#31243;&#21644;&#38271;&#31243;&#31354;&#38388;&#20851;&#31995;&#30340;&#31934;&#30830;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#20687;&#24179;&#31227;&#31383;&#21475;&#36825;&#26679;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#38500;&#20102;&#24182;&#34892;&#26631;&#35760;&#28151;&#21512;&#22120;&#32534;&#30721;&#22120;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;(CAPE)&#65292;&#20316;&#20026;&#26631;&#20934;&#34917;&#19969;&#23884;&#20837;&#30340;&#22686;&#24378;&#65292;&#36890;&#36807;&#21367;&#31215;&#27880;&#24847;&#21147;&#27169;&#22359;&#25913;&#36827;&#26631;&#35760;&#28151;&#21512;&#22120;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;ParFormer&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29289;&#20307;&#35782;&#21035;&#31561;&#22810;&#20010;&#22797;&#26434;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20110;CNN&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;CAPE&#24050;&#34987;&#35777;&#26126;&#26377;&#30410;&#20110;&#25972;&#20307;MetaFormer&#26550;&#26500;&#65292;&#21363;&#20351;&#20351;&#29992;Id&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15004v1 Announce Type: cross  Abstract: This work presents ParFormer as an enhanced transformer architecture that allows the incorporation of different token mixers into a single stage, hence improving feature extraction capabilities. Integrating both local and global data allows for precise representation of short- and long-range spatial relationships without the need for computationally intensive methods such as shifting windows. Along with the parallel token mixer encoder, We offer the Convolutional Attention Patch Embedding (CAPE) as an enhancement of standard patch embedding to improve token mixer extraction with a convolutional attention module. Our comprehensive evaluation demonstrates that our ParFormer outperforms CNN-based and state-of-the-art transformer-based architectures in image classification and several complex tasks such as object recognition. The proposed CAPE has been demonstrated to benefit the overall MetaFormer architecture, even while utilizing the Id
&lt;/p&gt;</description></item><item><title>&#30446;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#21644;&#30450;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.04732</link><description>&lt;p&gt;
&#25105;&#20204;&#36317;&#31163;&#26234;&#33021;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#36824;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Are We from Intelligent Visual Deductive Reasoning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04732
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#21644;&#30450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;GPT-4V&#20043;&#31867;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#35270;&#35273;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#22797;&#26434;&#20294;&#19981;&#22826;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#24182;&#21457;&#29616;&#20102;&#24403;&#21069;&#39046;&#20808;&#30340;VLM&#20013;&#20197;&#21069;&#26410;&#26292;&#38706;&#30340;&#30450;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#29790;&#25991;&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#26469;&#35780;&#20272;VLM&#22312;&#20165;&#20381;&#38752;&#35270;&#35273;&#32447;&#32034;&#36827;&#34892;&#22810;&#36339;&#20851;&#31995;&#21644;&#28436;&#32462;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#20960;&#31181;&#27969;&#34892;&#30340;VLM&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#37319;&#29992;&#20102;&#26631;&#20934;&#31574;&#30053;&#65292;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#33258;&#25105;&#19968;&#33268;&#24615;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;Mensa&#26234;&#21830;&#27979;&#35797;&#12289;&#26234;&#21830;&#27979;&#35797;&#21644;RAVEN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;LLM&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#26377;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04732v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated incredible strides on diverse vision language tasks. We dig into vision-based deductive reasoning, a more sophisticated but less explored realm, and find previously unexposed blindspots in the current SOTA VLMs. Specifically, we leverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop relational and deductive reasoning relying solely on visual clues. We perform comprehensive evaluations of several popular VLMs employing standard strategies such as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test, IntelligenceTest, and RAVEN. The results reveal that despite the impressive capabilities of LLMs in text-based reasoning, we are still far from achieving comparable proficiency in visual deductive reasoning. We found that certain standard strategies that are effective
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#20010;&#21035;&#25551;&#32472;&#20301;&#20110;&#35199;&#29677;&#29273;&#20869;&#21326;&#36798;&#23665;&#33033;&#26519;&#32447;&#19978;&#30340;&#26460;&#26494;&#28748;&#26408;&#12290;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#36965;&#24863;&#24433;&#20687;&#21644;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26816;&#27979;&#21644;&#25551;&#32472;&#33258;&#28982;&#23545;&#35937;&#22797;&#26434;&#29983;&#38271;&#27169;&#24335;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.17985</link><description>&lt;p&gt;
&#21315;&#38754;&#28748;&#26408;&#65306;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#36827;&#34892;&#20010;&#20307;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Shrub of a thousand faces: an individual segmentation from satellite images using deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17985
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#20010;&#21035;&#25551;&#32472;&#20301;&#20110;&#35199;&#29677;&#29273;&#20869;&#21326;&#36798;&#23665;&#33033;&#26519;&#32447;&#19978;&#30340;&#26460;&#26494;&#28748;&#26408;&#12290;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#36965;&#24863;&#24433;&#20687;&#21644;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26816;&#27979;&#21644;&#25551;&#32472;&#33258;&#28982;&#23545;&#35937;&#22797;&#26434;&#29983;&#38271;&#27169;&#24335;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#27979;&#38271;&#23551;&#28748;&#26408;&#65288;&#22914;&#24120;&#35265;&#30340;&#26460;&#26494;&#65289;&#30340;&#20998;&#24067;&#21644;&#22823;&#23567;&#32467;&#26500;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#27668;&#20505;&#21464;&#21270;&#23545;&#39640;&#23665;&#21644;&#39640;&#32428;&#24230;&#29983;&#24577;&#31995;&#32479;&#30340;&#38271;&#26399;&#24433;&#21709;&#12290;&#21382;&#21490;&#33322;&#31354;&#36229;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#25552;&#20379;&#20102;&#19968;&#31181;&#22238;&#39038;&#24615;&#24037;&#20855;&#65292;&#21487;&#20197;&#39640;&#31934;&#24230;&#30417;&#27979;&#28748;&#26408;&#30340;&#29983;&#38271;&#21644;&#20998;&#24067;&#12290;&#30446;&#21069;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26816;&#27979;&#21644;&#25551;&#32472;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#24418;&#29366;&#30340;&#23545;&#35937;&#30340;&#36718;&#24275;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#36866;&#24212;&#20110;&#26816;&#27979;&#34920;&#36798;&#22797;&#26434;&#29983;&#38271;&#27169;&#24335;&#30340;&#33258;&#28982;&#23545;&#35937;&#65288;&#20363;&#22914;&#26460;&#26494;&#65289;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#36965;&#24863;RGB&#24433;&#20687;&#19982;&#22522;&#20110;Mask R-CNN&#30340;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20010;&#21035;&#25551;&#32472;&#35199;&#29677;&#29273;&#20869;&#21326;&#36798;&#23665;&#33033;&#26519;&#32447;&#19978;&#30340;&#26460;&#26494;&#28748;&#26408;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#26500;&#36896;&#35774;&#35745;&#65292;&#20854;&#20013;&#20351;&#29992;&#30340;&#26159;&#20809;&#35299;&#35793;&#25968;&#25454;&#65288;PI&#65289;&#21644;&#23454;&#22320;&#35843;&#26597;&#25968;&#25454;&#65288;FW&#65289;&#20998;&#21035;&#36827;&#34892;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring the distribution and size structure of long-living shrubs, such as Juniperus communis, can be used to estimate the long-term effects of climate change on high-mountain and high latitude ecosystems. Historical aerial very-high resolution imagery offers a retrospective tool to monitor shrub growth and distribution at high precision. Currently, deep learning models provide impressive results for detecting and delineating the contour of objects with defined shapes. However, adapting these models to detect natural objects that express complex growth patterns, such as junipers, is still a challenging task.   This research presents a novel approach that leverages remotely sensed RGB imagery in conjunction with Mask R-CNN-based instance segmentation models to individually delineate Juniperus shrubs above the treeline in Sierra Nevada (Spain). In this study, we propose a new data construction design that consists in using photo interpreted (PI) and field work (FW) data to respectivel
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#29983;&#25104;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#22686;&#21152;&#27169;&#22411;&#22810;&#26679;&#24615;&#65292;&#24182;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>https://arxiv.org/abs/2311.16176</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#21270;&#21512;&#25104;&#21644;&#25193;&#25955;&#27169;&#22411;&#20943;&#36731;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Biases with Diverse Ensembles and Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16176
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#29983;&#25104;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#22686;&#21152;&#27169;&#22411;&#22810;&#26679;&#24615;&#65292;&#24182;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#21363;&#22810;&#20010;&#32447;&#32034;&#21487;&#20197;&#39044;&#27979;&#30446;&#26631;&#26631;&#31614;&#65292;&#24120;&#24120;&#23548;&#33268;&#19968;&#31181;&#31216;&#20026;&#25463;&#24452;&#20559;&#35265;&#30340;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#20381;&#36182;&#20110;&#38169;&#35823;&#30340;&#12289;&#26131;&#23398;&#30340;&#32447;&#32034;&#65292;&#32780;&#24573;&#30053;&#21487;&#38752;&#30340;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#30340;&#38598;&#25104;&#22810;&#26679;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#29305;&#23450;&#30340;&#35757;&#32451;&#38388;&#38548;&#20013;&#65292;DPMs&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21363;&#20351;&#22312;&#26174;&#31034;&#30456;&#20851;&#36755;&#20837;&#29305;&#24449;&#30340;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#23646;&#24615;&#36890;&#36807;&#38598;&#25104;&#19981;&#19968;&#33268;&#24615;&#29983;&#25104;&#21512;&#25104;&#21453;&#20107;&#23454;&#26469;&#22686;&#21152;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DPM&#24341;&#23548;&#30340;&#22810;&#26679;&#21270;&#36275;&#20197;&#28040;&#38500;&#23545;&#20027;&#35201;&#25463;&#24452;&#32447;&#32034;&#30340;&#20381;&#36182;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20960;&#20010;&#22810;&#26679;&#21270;&#30446;&#26631;&#19978;&#22312;&#23454;&#35777;&#19978;&#37327;&#21270;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#26368;&#32456;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16176v2 Announce Type: replace-cross  Abstract: Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to a phenomenon known as shortcut bias, where a model relies on erroneous, easy-to-learn cues while ignoring reliable ones. In this work, we propose an ensemble diversification framework exploiting Diffusion Probabilistic Models (DPMs) for shortcut bias mitigation. We show that at particular training intervals, DPMs can generate images with novel feature combinations, even when trained on samples displaying correlated input features. We leverage this crucial property to generate synthetic counterfactuals to increase model diversity via ensemble disagreement. We show that DPM-guided diversification is sufficient to remove dependence on primary shortcut cues, without a need for additional supervised signals. We further empirically quantify its efficacy on several diversification objectives, and finally show improved generalizati
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LadaGAN&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#65292;&#36890;&#36807;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09596</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#21147;Transformer&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Efficient generative adversarial networks using linear additive-attention Transformers. (arXiv:2401.09596v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LadaGAN&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#65292;&#36890;&#36807;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#31561;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#35745;&#31639;&#22797;&#26434;&#30340;&#26550;&#26500;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30740;&#31350;&#23454;&#39564;&#23460;&#21644;&#36164;&#28304;&#20805;&#36275;&#30340;&#20844;&#21496;&#20013;&#30340;&#37319;&#29992;&#21644;&#20351;&#29992;&#65292;&#21516;&#26102;&#20063;&#26497;&#22823;&#22320;&#22686;&#21152;&#20102;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#30340;&#30899;&#36275;&#36857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LadaGAN&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#24314;&#31435;&#22312;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#19978;&#12290;&#35813;&#22359;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#19968;&#20010;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#65292;&#23427;&#27599;&#20010;&#22836;&#37096;&#35745;&#31639;&#19968;&#20010;&#27880;&#24847;&#21521;&#37327;&#65292;&#32780;&#19981;&#26159;&#20108;&#27425;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#22312;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#20013;&#37117;&#37319;&#29992;&#20102;Ladaformer&#65292;&#36825;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#20811;&#26381;&#20102;Transformer GAN&#32463;&#24120;&#20986;&#29616;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#12290;LadaGAN&#19968;&#30452;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;GANs&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the capacity of deep generative models for image generation, such as Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has dramatically improved in recent years, much of their success can be attributed to computationally expensive architectures. This has limited their adoption and use to research laboratories and companies with large resources, while significantly raising the carbon footprint for training, fine-tuning, and inference. In this work, we present LadaGAN, an efficient generative adversarial network that is built upon a novel Transformer block named Ladaformer. The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention. We employ Ladaformer in both the generator and discriminator, which reduces the computational complexity and overcomes the training instabilities often associated with Transformer GANs. LadaGAN consistently outperforms exist
&lt;/p&gt;</description></item></channel></rss>