<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#21033;&#29992;&#36319;&#36394;&#31574;&#30053;&#21644;&#33258;&#21160;&#26631;&#35760;&#31639;&#27861;&#65292;&#38024;&#23545;&#20107;&#20214;&#30456;&#26426;&#20013;&#30340;&#38590;&#20197;&#36776;&#35782;&#30340;&#23545;&#35937;&#65292;&#25581;&#31034;&#20854;&#29305;&#24449;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.18330</link><description>&lt;p&gt;
&#20351;&#29992;&#36319;&#36394;&#36741;&#21161;&#30340;&#20107;&#20214;&#30456;&#26426;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Tracking-Assisted Object Detection with Event Cameras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#36319;&#36394;&#31574;&#30053;&#21644;&#33258;&#21160;&#26631;&#35760;&#31639;&#27861;&#65292;&#38024;&#23545;&#20107;&#20214;&#30456;&#26426;&#20013;&#30340;&#38590;&#20197;&#36776;&#35782;&#30340;&#23545;&#35937;&#65292;&#25581;&#31034;&#20854;&#29305;&#24449;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20107;&#20214;&#30456;&#26426;&#20855;&#26377;&#39640;&#21160;&#24577;&#33539;&#22260;&#21644;&#26080;&#21160;&#24577;&#27169;&#31946;&#31561;&#29305;&#27530;&#23646;&#24615;&#65292;&#20107;&#20214;&#39537;&#21160;&#30446;&#26631;&#26816;&#27979;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29305;&#24449;&#30340;&#19981;&#21516;&#27493;&#24615;&#21644;&#31232;&#30095;&#24615;&#23548;&#33268;&#20102;&#30001;&#20110;&#30456;&#26426;&#19982;&#20043;&#27809;&#26377;&#30456;&#23545;&#36816;&#21160;&#32780;&#23548;&#33268;&#30340;&#30475;&#19981;&#35265;&#30340;&#23545;&#35937;&#65292;&#36825;&#23545;&#20219;&#21153;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#30475;&#19981;&#35265;&#30340;&#23545;&#35937;&#35270;&#20026;&#20266;&#36974;&#25377;&#23545;&#35937;&#65292;&#24182;&#26088;&#22312;&#25581;&#31034;&#20854;&#29305;&#24449;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#35937;&#30340;&#21487;&#35265;&#24615;&#23646;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#26631;&#35760;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#29616;&#26377;&#30340;&#20107;&#20214;&#30456;&#26426;&#25968;&#25454;&#38598;&#19978;&#38468;&#21152;&#39069;&#22806;&#30340;&#21487;&#35265;&#24615;&#26631;&#31614;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#36319;&#36394;&#31574;&#30053;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18330v1 Announce Type: cross  Abstract: Event-based object detection has recently garnered attention in the computer vision community due to the exceptional properties of event cameras, such as high dynamic range and no motion blur. However, feature asynchronism and sparsity cause invisible objects due to no relative motion to the camera, posing a significant challenge in the task. Prior works have studied various memory mechanisms to preserve as many features as possible at the current time, guided by temporal clues. While these implicit-learned memories retain some short-term information, they still struggle to preserve long-term features effectively. In this paper, we consider those invisible objects as pseudo-occluded objects and aim to reveal their features. Firstly, we introduce visibility attribute of objects and contribute an auto-labeling algorithm to append additional visibility labels on an existing event camera dataset. Secondly, we exploit tracking strategies fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26631;&#31614;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#20449;&#24515;&#33258;&#26657;&#20934;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#26368;&#22823;&#29109;&#27491;&#21017;&#21270;&#65292;&#25913;&#21892;&#20102;&#22810;&#26631;&#31614;&#20449;&#24515;&#26657;&#20934;&#65292;&#20943;&#23569;&#20102;&#36807;&#24230;&#33258;&#20449;&#30340;&#35823;&#25253;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2403.12559</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#20449;&#24515;&#33258;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Confidence Self-Calibration for Multi-Label Class-Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26631;&#31614;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#20449;&#24515;&#33258;&#26657;&#20934;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#26368;&#22823;&#29109;&#27491;&#21017;&#21270;&#65292;&#25913;&#21892;&#20102;&#22810;&#26631;&#31614;&#20449;&#24515;&#26657;&#20934;&#65292;&#20943;&#23569;&#20102;&#36807;&#24230;&#33258;&#20449;&#30340;&#35823;&#25253;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;MLCIL&#65289;&#20013;&#30340;&#37096;&#20998;&#26631;&#31614;&#25361;&#25112;&#26159;&#22312;&#35757;&#32451;&#26399;&#38388;&#21482;&#26377;&#26032;&#31867;&#21035;&#34987;&#26631;&#35760;&#65292;&#32780;&#36807;&#21435;&#21644;&#26410;&#26469;&#26631;&#31614;&#20173;&#28982;&#19981;&#21487;&#29992;&#12290;&#36825;&#20010;&#38382;&#39064;&#20250;&#23548;&#33268;&#30001;&#20110;&#38169;&#35823;&#22320;&#39640;&#32622;&#20449;&#24230;&#22810;&#26631;&#31614;&#39044;&#27979;&#32780;&#20986;&#29616;&#22823;&#37327;&#35823;&#25253;&#38169;&#35823;&#65292;&#21152;&#21095;&#20102;&#22312;&#19981;&#21516;&#26631;&#31614;&#31354;&#38388;&#20869;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;MLCIL&#20013;&#25913;&#36827;&#22810;&#26631;&#31614;&#20449;&#24515;&#26657;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24515;&#33258;&#26657;&#20934;&#65288;CSC&#65289;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#26631;&#31614;&#20851;&#31995;&#26657;&#20934;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#31867;&#22686;&#37327;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#26500;&#24314;&#21487;&#23398;&#20064;&#30340;&#12289;&#21160;&#24577;&#25193;&#23637;&#30340;&#26631;&#31614;&#20851;&#31995;&#22270;&#26469;&#36830;&#25509;&#23396;&#31435;&#30340;&#26631;&#31614;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20449;&#24515;&#26657;&#20934;&#65292;&#25105;&#20204;&#38024;&#23545;&#27599;&#20010;&#22810;&#26631;&#31614;&#22686;&#37327;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#22823;&#29109;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#23545;&#36807;&#20110;&#33258;&#20449;&#30340;&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#24809;&#32602;&#65292;&#20419;&#36827;&#20102;&#20449;&#24515;&#30340;&#33258;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12559v1 Announce Type: cross  Abstract: The partial label challenge in Multi-Label Class-Incremental Learning (MLCIL) arises when only the new classes are labeled during training, while past and future labels remain unavailable. This issue leads to a proliferation of false-positive errors due to erroneously high confidence multi-label predictions, exacerbating catastrophic forgetting within the disjoint label space. In this paper, we aim to refine multi-label confidence calibration in MLCIL and propose a Confidence Self-Calibration (CSC) approach. Firstly, for label relationship calibration, we introduce a class-incremental graph convolutional network that bridges the isolated label spaces by constructing learnable, dynamically extended label relationship graph. Then, for confidence calibration, we present a max-entropy regularization for each multi-label increment, facilitating confidence self-calibration through the penalization of over-confident output distributions. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;2D&#21644;3D&#26684;&#24335;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#27668;&#36947;&#30149;&#21464;&#20307;&#31215;&#20998;&#21106;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;3D&#27169;&#22411;&#22312;&#25429;&#25417;&#22797;&#26434;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#24322;&#65292;&#24182;&#36890;&#36807;&#23545;2D&#27169;&#22411;&#23454;&#26045;&#32454;&#24494;&#32467;&#26500;&#20998;&#21106;&#25439;&#22833;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#22806;&#37096;&#39564;&#35777;&#35777;&#23454;&#20102;&#30740;&#31350;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2403.08042</link><description>&lt;p&gt;
CT&#35780;&#20272;2D&#21644;3D&#25972;&#20307;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#27668;&#36947;&#30149;&#21464;&#30340;&#20307;&#31215;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
CT evaluation of 2D and 3D holistic deep learning methods for the volumetric segmentation of airway lesions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;2D&#21644;3D&#26684;&#24335;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#27668;&#36947;&#30149;&#21464;&#20307;&#31215;&#20998;&#21106;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;3D&#27169;&#22411;&#22312;&#25429;&#25417;&#22797;&#26434;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#24322;&#65292;&#24182;&#36890;&#36807;&#23545;2D&#27169;&#22411;&#23454;&#26045;&#32454;&#24494;&#32467;&#26500;&#20998;&#21106;&#25439;&#22833;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#22806;&#37096;&#39564;&#35777;&#35777;&#23454;&#20102;&#30740;&#31350;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#22312;2D&#21644;3D&#26684;&#24335;&#20013;&#30340;&#25972;&#20307;&#20998;&#21106;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#25506;&#35752;&#65292;&#37325;&#28857;&#20851;&#27880;&#22218;&#24615;&#32420;&#32500;&#21270;&#65288;CF&#65289;&#30149;&#21464;&#12290;&#30740;&#31350;&#21033;&#29992;&#20102;&#26469;&#33258;&#20004;&#20010;CF&#21442;&#32771;&#20013;&#24515;&#30340;&#25968;&#25454;&#65292;&#28085;&#30422;&#20102;&#20116;&#20010;&#20027;&#35201;&#30340;CF&#32467;&#26500;&#21464;&#21270;&#12290;&#39318;&#20808;&#27604;&#36739;&#20102;2D&#21644;3D&#27169;&#22411;&#65292;&#31361;&#20986;&#20102;3D&#27169;&#22411;&#22312;&#25429;&#25417;&#31896;&#28082;&#26643;&#21644;&#23454;&#21464;&#31561;&#22797;&#26434;&#29305;&#24449;&#26041;&#38754;&#30340;&#20248;&#36234;&#33021;&#21147;&#12290;&#20026;&#20102;&#25552;&#39640;2D&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#26045;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32454;&#24494;&#32467;&#26500;&#20998;&#21106;&#30340;&#25439;&#22833;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#20934;&#30830;&#24615;&#65292;&#23613;&#31649;&#27809;&#26377;&#36229;&#36234;3D&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27169;&#22411;&#32463;&#36807;&#36827;&#19968;&#27493;&#36890;&#36807;&#23545;&#32954;&#21151;&#33021;&#27979;&#35797;&#65288;PFTs&#65289;&#30340;&#22806;&#37096;&#35780;&#20272;&#36827;&#34892;&#39564;&#35777;&#65292;&#30830;&#35748;&#20102;&#30740;&#31350;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#39033;&#30740;&#31350;&#19981;&#20165;&#38480;&#20110;&#27604;&#36739;&#25351;&#26631;&#65307;&#36824;&#21253;&#25324;&#23545;&#27169;&#22411;&#35299;&#37322;&#24615;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08042v1 Announce Type: cross  Abstract: This research embarked on a comparative exploration of the holistic segmentation capabilities of Convolutional Neural Networks (CNNs) in both 2D and 3D formats, focusing on cystic fibrosis (CF) lesions. The study utilized data from two CF reference centers, covering five major CF structural changes. Initially, it compared the 2D and 3D models, highlighting the 3D model's superior capability in capturing complex features like mucus plugs and consolidations. To improve the 2D model's performance, a loss adapted to fine structures segmentation was implemented and evaluated, significantly enhancing its accuracy, though not surpassing the 3D model's performance. The models underwent further validation through external evaluation against pulmonary function tests (PFTs), confirming the robustness of the findings. Moreover, this study went beyond comparing metrics; it also included comprehensive assessments of the models' interpretability and 
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.18292</link><description>&lt;p&gt;
FSL&#27169;&#22411;&#21487;&#20197;&#22240;&#20026;&#20854;&#20248;&#36234;&#24615;&#24471;&#20998;&#26356;&#39640;
&lt;/p&gt;
&lt;p&gt;
FSL Model can Score Higher as It Is
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18292
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#34987;&#27491;&#30830;&#35782;&#21035;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20542;&#21521;&#20110;&#38754;&#23545;&#38754;&#22320;&#30452;&#35270;&#38754;&#37096;&#35782;&#21035;&#26426;&#65292;&#32780;&#19981;&#26159;&#20391;&#30528;&#38754;&#23545;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#20998;&#31867;&#26412;&#36523;&#23601;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;&#23646;&#20110;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#22312;&#27979;&#35797;&#26399;&#38388;&#23545;&#25197;&#26354;&#21644;&#38750;&#20856;&#22411;&#30340;&#26597;&#35810;&#25110;&#25903;&#25345;&#22270;&#20687;&#20250;&#35753;&#27169;&#22411;&#26356;&#38590;&#27491;&#30830;&#39044;&#27979;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;&#35757;&#32451;&#36807;&#30340;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;FSL&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#20855;&#26377;&#36275;&#22815;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#20855;&#26377;&#23569;&#26679;&#26412;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#25429;&#25417;&#27979;&#35797;&#22270;&#20687;&#30340;&#39118;&#26684;&#25110;&#24418;&#29366;&#65292;&#28982;&#21518;&#35782;&#21035;&#19968;&#20010;&#36866;&#24403;&#30340;&#35757;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18292v1 Announce Type: cross  Abstract: In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable traine
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#27169;&#25311;&#22270;&#20687;&#24341;&#23548;&#30340;&#20197;&#22806;&#31185;&#21307;&#29983;&#20026;&#20013;&#24515;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#23398;&#24466;&#31995;&#32479;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#22312;&#30524;&#31185;&#30333;&#20869;&#38556;&#25163;&#26415;&#20013;&#36866;&#24212;&#22806;&#31185;&#21307;&#29983;&#30340;&#25216;&#33021;&#27700;&#24179;&#21644;&#22806;&#31185;&#25163;&#26415;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2311.17693</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#22806;&#31185;&#21307;&#29983;&#21442;&#19982;&#30524;&#31185;&#26426;&#22120;&#20154;&#23398;&#24466;&#31995;&#32479;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Toward a Surgeon-in-the-Loop Ophthalmic Robotic Apprentice using Reinforcement and Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17693
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#27169;&#25311;&#22270;&#20687;&#24341;&#23548;&#30340;&#20197;&#22806;&#31185;&#21307;&#29983;&#20026;&#20013;&#24515;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#23398;&#24466;&#31995;&#32479;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#22312;&#30524;&#31185;&#30333;&#20869;&#38556;&#25163;&#26415;&#20013;&#36866;&#24212;&#22806;&#31185;&#21307;&#29983;&#30340;&#25216;&#33021;&#27700;&#24179;&#21644;&#22806;&#31185;&#25163;&#26415;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#36741;&#21161;&#25163;&#26415;&#31995;&#32479;&#22312;&#25552;&#39640;&#25163;&#26415;&#31934;&#30830;&#24230;&#21644;&#20943;&#23569;&#20154;&#20026;&#38169;&#35823;&#26041;&#38754;&#23637;&#31034;&#20102;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#32570;&#20047;&#36866;&#24212;&#20010;&#21035;&#22806;&#31185;&#21307;&#29983;&#30340;&#29420;&#29305;&#20559;&#22909;&#21644;&#35201;&#27714;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20027;&#35201;&#38598;&#20013;&#22312;&#26222;&#36890;&#25163;&#26415;&#65288;&#22914;&#33145;&#33108;&#38236;&#25163;&#26415;&#65289;&#65292;&#19981;&#36866;&#29992;&#20110;&#38750;&#24120;&#31934;&#23494;&#30340;&#24494;&#21019;&#25163;&#26415;&#65292;&#22914;&#30524;&#31185;&#25163;&#26415;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#22270;&#20687;&#24341;&#23548;&#30340;&#20197;&#22806;&#31185;&#21307;&#29983;&#20026;&#20013;&#24515;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#23398;&#24466;&#31995;&#32479;&#65292;&#21487;&#22312;&#30524;&#31185;&#30333;&#20869;&#38556;&#25163;&#26415;&#36807;&#31243;&#20013;&#36866;&#24212;&#20010;&#21035;&#22806;&#31185;&#21307;&#29983;&#30340;&#25216;&#33021;&#27700;&#24179;&#21644;&#39318;&#36873;&#22806;&#31185;&#25163;&#26415;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#27169;&#25311;&#29615;&#22659;&#26469;&#35757;&#32451;&#20197;&#22270;&#20687;&#25968;&#25454;&#20026;&#25351;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#65292;&#20197;&#25191;&#34892;&#30333;&#20869;&#38556;&#25163;&#26415;&#30340;&#20999;&#21475;&#38454;&#27573;&#25152;&#26377;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#22806;&#31185;&#21307;&#29983;&#30340;&#21160;&#20316;&#21644;&#20559;&#22909;&#25972;&#21512;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35753;&#22806;&#31185;&#21307;&#29983;&#21442;&#19982;&#20854;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17693v2 Announce Type: replace-cross  Abstract: Robotic-assisted surgical systems have demonstrated significant potential in enhancing surgical precision and minimizing human errors. However, existing systems lack the ability to accommodate the unique preferences and requirements of individual surgeons. Additionally, they primarily focus on general surgeries (e.g., laparoscopy) and are not suitable for highly precise microsurgeries, such as ophthalmic procedures. Thus, we propose a simulation-based image-guided approach for surgeon-centered autonomous agents that can adapt to the individual surgeon's skill level and preferred surgical techniques during ophthalmic cataract surgery. Our approach utilizes a simulated environment to train reinforcement and imitation learning agents guided by image data to perform all tasks of the incision phase of cataract surgery. By integrating the surgeon's actions and preferences into the training process with the surgeon-in-the-loop, our ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EdgeOL&#65292;&#19968;&#31181;&#36793;&#32536;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#35843;&#20248;&#26469;&#20248;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12289;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#25928;&#29575;&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.16694</link><description>&lt;p&gt;
EdgeOL: &#36793;&#32536;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#21407;&#20301;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EdgeOL: Efficient in-situ Online Learning on Edge Devices. (arXiv:2401.16694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EdgeOL&#65292;&#19968;&#31181;&#36793;&#32536;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#35843;&#20248;&#26469;&#20248;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12289;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#25928;&#29575;&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#24212;&#29992;&#65292;&#22914;&#26426;&#22120;&#20154;&#36741;&#21161;&#20859;&#32769;&#21644;&#29289;&#20307;&#35782;&#21035;&#65292;&#36890;&#24120;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#19988;&#33258;&#28982;&#38656;&#35201;&#65306;i) &#22788;&#29702;&#23454;&#26102;&#25512;&#29702;&#35831;&#27714;&#21644;ii) &#36866;&#24212;&#21487;&#33021;&#30340;&#37096;&#32626;&#22330;&#26223;&#21464;&#21270;&#12290;&#22312;&#32447;&#27169;&#22411;&#24494;&#35843;&#34987;&#24191;&#27867;&#37319;&#29992;&#20197;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#33021;&#37327;&#28040;&#32791;&#65292;&#20351;&#20854;&#38590;&#20197;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EdgeOL&#65292;&#19968;&#31181;&#36793;&#32536;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#35843;&#20248;&#26469;&#20248;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12289;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;EdgeOL&#24179;&#22343;&#20943;&#23569;&#20102;82%&#30340;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#65292;74%&#30340;&#33021;&#37327;&#28040;&#32791;&#65292;&#24182;&#25552;&#39640;&#20102;&#24179;&#22343;&#25512;&#29702;&#20934;&#30830;&#29575;1.70%&#65292;&#30456;&#23545;&#20110;&#21363;&#26102;&#22312;&#32447;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging applications, such as robot-assisted eldercare and object recognition, generally employ deep learning neural networks (DNNs) models and naturally require: i) handling streaming-in inference requests and ii) adapting to possible deployment scenario changes. Online model fine-tuning is widely adopted to satisfy these needs. However, fine-tuning involves significant energy consumption, making it challenging to deploy on edge devices. In this paper, we propose EdgeOL, an edge online learning framework that optimizes inference accuracy, fine-tuning execution time, and energy efficiency through both inter-tuning and intra-tuning optimizations. Experimental results show that, on average, EdgeOL reduces overall fine-tuning execution time by 82%, energy consumption by 74%, and improves average inference accuracy by 1.70% over the immediate online learning strategy.
&lt;/p&gt;</description></item><item><title>FedRSU&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#30417;&#30563;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#23427;&#36890;&#36807;&#25972;&#21512;RSU&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#37319;&#29992;&#24490;&#29615;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12862</link><description>&lt;p&gt;
FedRSU: &#22522;&#20110;RSU&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#22330;&#26223;&#27969;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
FedRSU: Federated Learning for Scene Flow Estimation on Roadside Units. (arXiv:2401.12862v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12862
&lt;/p&gt;
&lt;p&gt;
FedRSU&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#30417;&#30563;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#23427;&#36890;&#36807;&#25972;&#21512;RSU&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#37319;&#29992;&#24490;&#29615;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RSU&#65288;&#36335;&#36793;&#21333;&#20803;&#65289;&#36890;&#36807;&#36710;&#32852;&#32593;&#65288;V2X&#65289;&#36890;&#20449;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#30446;&#21069;&#65292;&#21333;&#20010;RSU&#30340;&#20351;&#29992;&#20027;&#35201;&#38598;&#20013;&#22312;&#23454;&#26102;&#25512;&#26029;&#21644;V2X&#21327;&#20316;&#19978;&#65292;&#32780;&#24573;&#35270;&#20102;RSU&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#28508;&#22312;&#20215;&#20540;&#12290;&#25972;&#21512;&#22823;&#37327;&#26469;&#33258;&#22810;&#20010;RSU&#30340;&#25968;&#25454;&#21487;&#20197;&#20026;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20016;&#23500;&#30340;&#25968;&#25454;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#22320;&#38754;&#30495;&#20540;&#26631;&#27880;&#21644;&#20256;&#36755;&#24222;&#22823;&#25968;&#25454;&#37327;&#30340;&#22256;&#38590;&#26159;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#38544;&#34255;&#20215;&#20540;&#30340;&#20004;&#20010;&#19981;&#21487;&#36991;&#20813;&#30340;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FedRSU&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#30417;&#30563;&#22330;&#26223;&#27969;&#20272;&#35745;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;FedRSU&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#65292;&#23545;&#20110;&#27599;&#20010;RSU&#65292;&#21487;&#20197;&#36890;&#36807;&#20854;&#21518;&#32493;&#26410;&#26469;&#30340;&#22810;&#27169;&#24577;&#35266;&#27979;&#23545;&#27599;&#20010;&#26102;&#38388;&#25139;&#30340;&#28857;&#30340;&#22330;&#26223;&#27969;&#39044;&#27979;&#36827;&#34892;&#30417;&#30563;&#12290;FedRSU&#30340;&#21478;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#32852;&#37030;
&lt;/p&gt;
&lt;p&gt;
Roadside unit (RSU) can significantly improve the safety and robustness of autonomous vehicles through Vehicle-to-Everything (V2X) communication. Currently, the usage of a single RSU mainly focuses on real-time inference and V2X collaboration, while neglecting the potential value of the high-quality data collected by RSU sensors. Integrating the vast amounts of data from numerous RSUs can provide a rich source of data for model training. However, the absence of ground truth annotations and the difficulty of transmitting enormous volumes of data are two inevitable barriers to fully exploiting this hidden value. In this paper, we introduce FedRSU, an innovative federated learning framework for self-supervised scene flow estimation. In FedRSU, we present a recurrent self-supervision training paradigm, where for each RSU, the scene flow prediction of points at every timestamp can be supervised by its subsequent future multi-modality observation. Another key component of FedRSU is federated
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#19977;&#24179;&#38754;&#32593;&#32476;&#20197;&#21450;&#32852;&#21512;&#24103;&#24314;&#27169;&#26041;&#27861;&#21644;&#22522;&#20110;&#20809;&#27969;&#30340;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#26102;&#38388;&#36830;&#36143;&#19988;&#26080;&#35270;&#35273;&#20266;&#24433;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2401.06035</link><description>&lt;p&gt;
RAVEN&#65306;&#29992;&#39640;&#25928;&#30340;&#19977;&#24179;&#38754;&#32593;&#32476;&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#24615;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
RAVEN: Rethinking Adversarial Video Generation with Efficient Tri-plane Networks. (arXiv:2401.06035v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06035
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#19977;&#24179;&#38754;&#32593;&#32476;&#20197;&#21450;&#32852;&#21512;&#24103;&#24314;&#27169;&#26041;&#27861;&#21644;&#22522;&#20110;&#20809;&#27969;&#30340;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#26102;&#38388;&#36830;&#36143;&#19988;&#26080;&#35270;&#35273;&#20266;&#24433;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#38271;&#26399;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21463;&#21040;&#19977;&#32500;&#24863;&#30693;&#29983;&#25104;&#26694;&#26550;&#21551;&#21457;&#30340;&#28151;&#21512;&#26174;&#24335;-&#38544;&#24335;&#19977;&#24179;&#38754;&#34920;&#31034;&#27861;&#24182;&#20837;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21807;&#19968;&#30340;&#28508;&#22312;&#32534;&#30721;&#26469;&#24314;&#27169;&#25972;&#20010;&#35270;&#39057;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#20174;&#20013;&#38388;&#19977;&#24179;&#38754;&#34920;&#31034;&#27861;&#20013;&#21512;&#25104;&#21333;&#20010;&#35270;&#39057;&#24103;&#65292;&#35813;&#34920;&#31034;&#27861;&#26412;&#36523;&#26159;&#20174;&#20027;&#35201;&#28508;&#22312;&#32534;&#30721;&#20013;&#27966;&#29983;&#30340;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#23558;&#35745;&#31639;&#22797;&#26434;&#24230;&#20943;&#23569;&#20102;2&#20493;&#65292;&#20197;FLOPs&#24230;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20415;&#20110;&#39640;&#25928;&#21644;&#26102;&#38388;&#36830;&#36143;&#22320;&#29983;&#25104;&#35270;&#39057;&#12290;&#27492;&#22806;&#65292;&#19982;&#33258;&#22238;&#24402;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32852;&#21512;&#24103;&#24314;&#27169;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#35270;&#35273;&#20266;&#24433;&#30340;&#20135;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#20013;&#38598;&#25104;&#22522;&#20110;&#20809;&#27969;&#30340;&#27169;&#22359;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel unconditional video generative model designed to address long-term spatial and temporal dependencies. To capture these dependencies, our approach incorporates a hybrid explicit-implicit tri-plane representation inspired by 3D-aware generative frameworks developed for three-dimensional object representation and employs a singular latent code to model an entire video sequence. Individual video frames are then synthesized from an intermediate tri-plane representation, which itself is derived from the primary latent code. This novel strategy reduces computational complexity by a factor of $2$ as measured in FLOPs. Consequently, our approach facilitates the efficient and temporally coherent generation of videos. Moreover, our joint frame modeling approach, in contrast to autoregressive methods, mitigates the generation of visual artifacts. We further enhance the model's capabilities by integrating an optical flow-based module within our Generative Adversarial Network (GAN
&lt;/p&gt;</description></item><item><title>CoFiI2P&#26159;&#19968;&#31181;&#31895;&#21040;&#31934;&#30340;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#21644;&#29305;&#24449;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.14660</link><description>&lt;p&gt;
CoFiI2P: &#31895;&#21040;&#31934;&#30340;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#30340;&#23545;&#24212;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
CoFiI2P: Coarse-to-Fine Correspondences for Image-to-Point Cloud Registration. (arXiv:2309.14660v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14660
&lt;/p&gt;
&lt;p&gt;
CoFiI2P&#26159;&#19968;&#31181;&#31895;&#21040;&#31934;&#30340;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#21644;&#29305;&#24449;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21040;&#28857;&#20113;&#65288;I2P&#65289;&#27880;&#20876;&#26159;&#26426;&#22120;&#20154;&#23548;&#33322;&#21644;&#31227;&#21160;&#24314;&#22270;&#39046;&#22495;&#20013;&#30340;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;I2P&#27880;&#20876;&#26041;&#27861;&#22312;&#28857;&#21040;&#20687;&#32032;&#32423;&#21035;&#19978;&#20272;&#35745;&#23545;&#24212;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#20840;&#23616;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#26469;&#33258;&#20840;&#23616;&#32422;&#26463;&#30340;&#39640;&#32423;&#24341;&#23548;&#30340;I2P&#21305;&#37197;&#23481;&#26131;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;I2P&#27880;&#20876;&#32593;&#32476;CoFiI2P&#65292;&#36890;&#36807;&#31895;&#21040;&#31934;&#30340;&#26041;&#24335;&#25552;&#21462;&#23545;&#24212;&#20851;&#31995;&#65292;&#20197;&#24471;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#39318;&#20808;&#65292;&#23558;&#22270;&#20687;&#21644;&#28857;&#20113;&#36755;&#20837;&#21040;&#19968;&#20010;&#20849;&#20139;&#32534;&#30721;-&#35299;&#30721;&#32593;&#32476;&#20013;&#36827;&#34892;&#23618;&#27425;&#21270;&#29305;&#24449;&#25552;&#21462;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31895;&#21040;&#31934;&#30340;&#21305;&#37197;&#27169;&#22359;&#65292;&#21033;&#29992;&#29305;&#24449;&#24314;&#31435;&#31283;&#20581;&#30340;&#29305;&#24449;&#23545;&#24212;&#20851;&#31995;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#31895;&#21305;&#37197;&#22359;&#20013;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;I2P&#21464;&#25442;&#27169;&#22359;&#65292;&#20174;&#22270;&#20687;&#21644;&#28857;&#20113;&#20013;&#25429;&#25417;&#21516;&#36136;&#21644;&#24322;&#36136;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#36890;&#36807;&#21028;&#21035;&#25551;&#36848;&#23376;&#65292;&#23436;&#25104;&#31895;-&#32454;&#29305;&#24449;&#21305;&#37197;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#32454;&#21270;&#21305;&#37197;&#27169;&#22359;&#36827;&#19968;&#27493;&#25552;&#21319;&#23545;&#24212;&#20851;&#31995;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-to-point cloud (I2P) registration is a fundamental task in the fields of robot navigation and mobile mapping. Existing I2P registration works estimate correspondences at the point-to-pixel level, neglecting the global alignment. However, I2P matching without high-level guidance from global constraints may converge to the local optimum easily. To solve the problem, this paper proposes CoFiI2P, a novel I2P registration network that extracts correspondences in a coarse-to-fine manner for the global optimal solution. First, the image and point cloud are fed into a Siamese encoder-decoder network for hierarchical feature extraction. Then, a coarse-to-fine matching module is designed to exploit features and establish resilient feature correspondences. Specifically, in the coarse matching block, a novel I2P transformer module is employed to capture the homogeneous and heterogeneous global information from image and point cloud. With the discriminate descriptors, coarse super-point-to-su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;LEGO&#65292;&#36890;&#36807;&#38598;&#25104;&#22270;&#20248;&#21270;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#20013;&#30340;&#25968;&#25454;&#20851;&#32852;&#24615;&#33021;&#12290;&#20351;&#29992;LiDAR&#21333;&#29420;&#36827;&#34892;&#36319;&#36394;&#30340;LEGO&#26041;&#27861;&#22312;KITTI&#30446;&#26631;&#36319;&#36394;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09908</link><description>&lt;p&gt;
LEGO: &#23545;&#20110;&#22522;&#20110;&#28857;&#20113;&#30340;&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;
&lt;/p&gt;
&lt;p&gt;
LEGO: Learning and Graph-Optimized Modular Tracker for Online Multi-Object Tracking with Point Clouds. (arXiv:2308.09908v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;LEGO&#65292;&#36890;&#36807;&#38598;&#25104;&#22270;&#20248;&#21270;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#20013;&#30340;&#25968;&#25454;&#20851;&#32852;&#24615;&#33021;&#12290;&#20351;&#29992;LiDAR&#21333;&#29420;&#36827;&#34892;&#36319;&#36394;&#30340;LEGO&#26041;&#27861;&#22312;KITTI&#30446;&#26631;&#36319;&#36394;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MOT&#65289;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#36319;&#36394;-&#26816;&#27979;&#26041;&#27861;&#65292;&#25968;&#25454;&#20851;&#32852;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#22270;&#20248;&#21270;&#65288;LEGO&#65289;&#30340;&#27169;&#22359;&#21270;&#36319;&#36394;&#22120;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#20851;&#32852;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;LEGO&#36319;&#36394;&#22120;&#38598;&#25104;&#20102;&#22270;&#20248;&#21270;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#21046;&#23450;&#20851;&#32852;&#35780;&#20998;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20934;&#30830;&#39640;&#25928;&#30340;&#30446;&#26631;&#21305;&#37197;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#29366;&#24577;&#26356;&#26032;&#36807;&#31243;&#65292;&#26412;&#25991;&#36824;&#28155;&#21152;&#20102;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65292;&#36890;&#36807;&#23558;&#23545;&#35937;&#29366;&#24577;&#30340;&#26102;&#38388;&#36830;&#36143;&#24615;&#32435;&#20837;&#36319;&#36394;&#20013;&#65292;&#30830;&#20445;&#19968;&#33268;&#30340;&#36319;&#36394;&#12290;&#19982;&#20854;&#20182;&#22312;&#32447;&#36319;&#36394;&#26041;&#27861;&#65288;&#21253;&#25324;&#22522;&#20110;LiDAR&#21644;&#22522;&#20110;LiDAR-&#30456;&#26426;&#34701;&#21512;&#30340;&#26041;&#27861;&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20165;&#21033;&#29992;LiDAR&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#24615;&#33021;&#12290;&#22312;&#25552;&#20132;&#32467;&#26524;&#33267;KITTI&#30446;&#26631;&#36319;&#36394;&#35780;&#20272;&#25490;&#34892;&#27036;&#26102;&#65292;LEGO&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online multi-object tracking (MOT) plays a pivotal role in autonomous systems. The state-of-the-art approaches usually employ a tracking-by-detection method, and data association plays a critical role. This paper proposes a learning and graph-optimized (LEGO) modular tracker to improve data association performance in the existing literature. The proposed LEGO tracker integrates graph optimization and self-attention mechanisms, which efficiently formulate the association score map, facilitating the accurate and efficient matching of objects across time frames. To further enhance the state update process, the Kalman filter is added to ensure consistent tracking by incorporating temporal coherence in the object states. Our proposed method utilizing LiDAR alone has shown exceptional performance compared to other online tracking approaches, including LiDAR-based and LiDAR-camera fusion-based methods. LEGO ranked 1st at the time of submitting results to KITTI object tracking evaluation ranki
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#20449;&#36182;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;TrustDD&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#20869;&#37096;&#20998;&#24067;&#65288;InD&#65289;&#20998;&#31867;&#21644;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#31934;&#28860;&#20026;&#23567;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#20449;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09165</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#20449;&#36182;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Towards Trustworthy Dataset Distillation. (arXiv:2307.09165v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21487;&#20449;&#36182;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;TrustDD&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#20869;&#37096;&#20998;&#24067;&#65288;InD&#65289;&#20998;&#31867;&#21644;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#31934;&#28860;&#20026;&#23567;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#20449;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#26102;&#65292;&#25928;&#29575;&#21644;&#21487;&#20449;&#36182;&#24615;&#26159;&#20004;&#20010;&#27704;&#24658;&#30340;&#36861;&#27714;&#12290;&#23601;&#25928;&#29575;&#32780;&#35328;&#65292;&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;DD&#65289;&#33268;&#21147;&#20110;&#36890;&#36807;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#31934;&#28860;&#20026;&#23567;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20165;&#38598;&#20013;&#20110;&#22312;&#23553;&#38381;&#19990;&#30028;&#29615;&#22659;&#19979;&#30340;&#20869;&#37096;&#20998;&#24067;&#65288;InD&#65289;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#26679;&#26412;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;OOD&#26816;&#27979;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#20449;&#36182;&#24615;&#65292;&#22312;&#23436;&#25972;&#25968;&#25454;&#35774;&#32622;&#19979;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#12290;&#25105;&#20204;&#39318;&#27425;&#21516;&#26102;&#32771;&#34385;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#31216;&#20026;&#21487;&#20449;&#36182;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;TrustDD&#65289;&#12290;&#36890;&#36807;&#31934;&#28860;InD&#26679;&#26412;&#21644;&#24322;&#24120;&#20540;&#65292;&#36825;&#20123;&#34987;&#31579;&#36873;&#30340;&#25968;&#25454;&#38598;&#33021;&#22815;&#35757;&#32451;&#20986;&#26082;&#25797;&#38271;InD&#20998;&#31867;&#21448;&#33021;&#36827;&#34892;OOD&#26816;&#27979;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#32531;&#35299;&#23545;&#30495;&#23454;&#24322;&#24120;&#20540;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#24182;&#20351;OOD&#26816;&#27979;&#26356;&#21152;&#23454;&#29992;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#23545;InD&#26679;&#26412;&#25439;&#22351;&#20197;&#29983;&#25104;&#20266;&#26679;&#26412;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiency and trustworthiness are two eternal pursuits when applying deep learning in real-world applications. With regard to efficiency, dataset distillation (DD) endeavors to reduce training costs by distilling the large dataset into a tiny synthetic dataset. However, existing methods merely concentrate on in-distribution (InD) classification in a closed-world setting, disregarding out-of-distribution (OOD) samples. On the other hand, OOD detection aims to enhance models' trustworthiness, which is always inefficiently achieved in full-data settings. For the first time, we simultaneously consider both issues and propose a novel paradigm called Trustworthy Dataset Distillation (TrustDD). By distilling both InD samples and outliers, the condensed datasets are capable to train models competent in both InD classification and OOD detection. To alleviate the requirement of real outlier data and make OOD detection more practical, we further propose to corrupt InD samples to generate pseudo-
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#27531;&#24046;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#36830;&#36143;&#30340;&#36229;&#39640;&#36136;&#37327;&#35270;&#39057;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2305.13840</link><description>&lt;p&gt;
Control-A-Video: &#25511;&#21046;&#24615;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models. (arXiv:2305.13840v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#27531;&#24046;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#36830;&#36143;&#30340;&#36229;&#39640;&#36136;&#37327;&#35270;&#39057;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#20449;&#21495;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#35270;&#39057;&#65288;T2V&#65289;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;Video-ControlNet&#12290;&#35813;&#27169;&#22411;&#26159;&#22312;&#39044;&#35757;&#32451;&#30340;&#26377;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#22522;&#30784;&#19978;&#26500;&#24314;&#30340;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#21487;&#35757;&#32451;&#30340;&#26102;&#38388;&#23618;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#36328;&#24103;&#24314;&#27169;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19968;&#24103;&#26465;&#20214;&#31574;&#30053;&#65292;&#20197;&#20419;&#36827;&#27169;&#22411;&#22312;&#33258;&#22238;&#24402;&#26041;&#24335;&#19979;&#29983;&#25104;&#36716;&#25442;&#33258;&#22270;&#20687;&#39046;&#22495;&#20197;&#21450;&#20219;&#24847;&#38271;&#24230;&#35270;&#39057;&#12290;&#27492;&#22806;&#65292;Video-ControlNet&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#30340;&#22122;&#22768;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#20174;&#36755;&#20837;&#35270;&#39057;&#20013;&#24341;&#20837;&#36816;&#21160;&#20808;&#39564;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#36830;&#36143;&#30340;&#35270;&#39057;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26550;&#26500;&#21644;&#31574;&#30053;&#65292;Video-ControlNet&#21487;&#20197;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;&#25910;&#25947;&#65292;&#29983;&#25104;&#20855;&#26377;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#20248;&#36136;&#19968;&#33268;&#35270;&#39057;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a controllable text-to-video (T2V) diffusion model, named Video-ControlNet, that generates videos conditioned on a sequence of control signals, such as edge or depth maps. Video-ControlNet is built on a pre-trained conditional text-to-image (T2I) diffusion model by incorporating a spatial-temporal self-attention mechanism and trainable temporal layers for efficient cross-frame modeling. A first-frame conditioning strategy is proposed to facilitate the model to generate videos transferred from the image domain as well as arbitrary-length videos in an auto-regressive manner. Moreover, Video-ControlNet employs a novel residual-based noise initialization strategy to introduce motion prior from an input video, producing more coherent videos. With the proposed architecture and strategies, Video-ControlNet can achieve resource-efficient convergence and generate superior quality and consistent videos with fine-grained control. Extensive experiments demonstrate its success i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#35813;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#23454;&#39564;&#26174;&#31034;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05215</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Billion-scale Foundation Model for Remote Sensing Images. (arXiv:2304.05215v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#35813;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#23454;&#39564;&#26174;&#31034;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20808;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#39044;&#35757;&#32451;&#26041;&#27861;&#12289;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#20197;&#21450;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#26368;&#36817;&#65292;&#36965;&#24863;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#65292;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#22522;&#30784;&#27169;&#22411;&#22312;&#26059;&#36716;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#26469;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#21442;&#25968;&#65288;&#21253;&#25324;86M&#12289;605.26M&#12289;1.3B&#21644;2.4B&#65289;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#30830;&#23450;&#21442;&#25968;&#22686;&#21152;&#26159;&#21542;&#20250;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10&#20159;&#20010;&#36965;&#24863;&#22270;&#20687;&#30340;&#26032;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#21521;&#30740;&#31350;&#31038;&#21306;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the potential of foundation models in visual tasks has garnered significant attention, pretraining these models before downstream tasks has become a crucial step. The three key factors in pretraining foundation models are the pretraining method, the size of the pretraining dataset, and the number of model parameters. Recently, research in the remote sensing field has focused primarily on the pretraining method and the size of the dataset, with limited emphasis on the number of model parameters. This paper addresses this gap by examining the effect of increasing the number of model parameters on the performance of foundation models in downstream tasks such as rotated object detection and semantic segmentation. We pretrained foundation models with varying numbers of parameters, including 86M, 605.26M, 1.3B, and 2.4B, to determine whether performance in downstream tasks improved with an increase in parameters. To the best of our knowledge, this is the first billion-scale foundation mod
&lt;/p&gt;</description></item></channel></rss>