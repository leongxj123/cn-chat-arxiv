<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#30340;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23450;&#21046;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21644;&#24341;&#20837;&#22810;&#27169;&#24577;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#22810;&#27169;&#24577;&#24322;&#24120;&#26816;&#27979;&#21644;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.11083</link><description>&lt;p&gt;
&#20026;&#22810;&#27169;&#24577;&#24322;&#24120;&#26816;&#27979;&#21644;&#25512;&#29702;&#23450;&#21046;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11083
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#30340;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23450;&#21046;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21644;&#24341;&#20837;&#22810;&#27169;&#24577;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#22810;&#27169;&#24577;&#24322;&#24120;&#26816;&#27979;&#21644;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#21508;&#31181;&#24037;&#19994;&#22330;&#26223;&#20013;&#21313;&#20998;&#37325;&#35201;&#65292;&#21253;&#25324;&#29983;&#20135;&#32447;&#19978;&#24322;&#24120;&#27169;&#24335;&#30340;&#35782;&#21035;&#21644;&#29992;&#20110;&#36136;&#37327;&#25511;&#21046;&#30340;&#21046;&#36896;&#32570;&#38519;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31181;&#22330;&#26223;&#30340;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#25317;&#26377;&#24191;&#27867;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#30340;&#36890;&#29992;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23450;&#21046;&#20026;&#24322;&#24120;&#26816;&#27979;&#22120;&#21644;&#25512;&#29702;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#25552;&#31034;&#31574;&#30053;&#65292;&#23558;&#39046;&#22495;&#19987;&#23478;&#30340;&#39046;&#22495;&#30693;&#35782;&#20316;&#20026;&#26465;&#20214;&#24341;&#23548;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#22810;&#27169;&#24577;&#25552;&#31034;&#31867;&#22411;&#65292;&#21253;&#25324;&#20219;&#21153;&#25551;&#36848;&#12289;&#31867;&#21035;&#19978;&#19979;&#25991;&#12289;&#27491;&#24120;&#35268;&#21017;&#21644;&#21442;&#32771;&#22270;&#20687;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#34920;&#31034;&#32479;&#19968;&#20026;2D&#22270;&#20687;&#26684;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11083v1 Announce Type: cross  Abstract: Anomaly detection is vital in various industrial scenarios, including the identification of unusual patterns in production lines and the detection of manufacturing defects for quality control. Existing techniques tend to be specialized in individual scenarios and lack generalization capacities. In this study, we aim to develop a generic anomaly detection model applicable across multiple scenarios. To achieve this, we customize generic visual-language foundation models that possess extensive knowledge and robust reasoning abilities into anomaly detectors and reasoners. Specifically, we introduce a multi-modal prompting strategy that incorporates domain knowledge from experts as conditions to guide the models. Our approach considers multi-modal prompt types, including task descriptions, class context, normality rules, and reference images. In addition, we unify the input representation of multi-modality into a 2D image format, enabling m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#35821;&#35328;&#39537;&#21160;&#30340;HOI&#26816;&#27979;&#26041;&#27861;FreeA&#65292;&#26080;&#38656;&#26631;&#35760;&#65292;&#21033;&#29992;&#20102;CLIP&#26469;&#29983;&#25104;&#28508;&#22312;&#30340;HOI&#26631;&#31614;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01840</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#30001;&#27880;&#37322;&#26631;&#31614;&#36827;&#34892;&#20154;-&#29289;&#20114;&#21160;&#26816;&#27979;&#30340;FreeA&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FreeA: Human-object Interaction Detection using Free Annotation Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01840
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#35821;&#35328;&#39537;&#21160;&#30340;HOI&#26816;&#27979;&#26041;&#27861;FreeA&#65292;&#26080;&#38656;&#26631;&#35760;&#65292;&#21033;&#29992;&#20102;CLIP&#26469;&#29983;&#25104;&#28508;&#22312;&#30340;HOI&#26631;&#31614;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20154;-&#29289;&#20114;&#21160;&#65288;HOI&#65289;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#21171;&#21160;&#21147;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#38656;&#35201;&#20840;&#38754;&#27880;&#37322;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#35821;&#35328;&#39537;&#21160;&#30340;HOI&#26816;&#27979;&#26041;&#27861;FreeA&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;CLIP&#30340;&#36866;&#24212;&#24615;&#26469;&#29983;&#25104;&#28508;&#22312;&#30340;HOI&#26631;&#31614;&#65292;&#26080;&#38656;&#26631;&#35760;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FreeA&#23558;&#20154;-&#29289;&#23545;&#30340;&#22270;&#20687;&#29305;&#24449;&#19982;HOI&#25991;&#26412;&#27169;&#26495;&#36827;&#34892;&#21305;&#37197;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#30340;&#25513;&#27169;&#26041;&#27861;&#26469;&#25233;&#21046;&#19981;&#22826;&#21487;&#33021;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;FreeA&#21033;&#29992;&#20102;&#25552;&#20986;&#30340;&#20132;&#20114;&#30456;&#20851;&#24615;&#21305;&#37197;&#26041;&#27861;&#26469;&#22686;&#24378;&#19982;&#25351;&#23450;&#21160;&#20316;&#30456;&#20851;&#30340;&#21160;&#20316;&#30340;&#21487;&#33021;&#24615;&#65292;&#36827;&#19968;&#27493;&#23436;&#21892;&#29983;&#25104;&#30340;HOI&#26631;&#31614;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;FreeA&#22312;&#24369;&#30417;&#30563;HOI&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;HICO-DET&#19978;&#30340;&#24179;&#22343;&#31934;&#24230;&#65288;mAP&#65289;&#25552;&#39640;&#20102;+8.58&#65292;&#22312;V-COCO&#19978;&#25552;&#39640;&#20102;+1.23&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01840v1 Announce Type: cross  Abstract: Recent human-object interaction (HOI) detection approaches rely on high cost of manpower and require comprehensive annotated image datasets. In this paper, we propose a novel self-adaption language-driven HOI detection method, termed as FreeA, without labeling by leveraging the adaptability of CLIP to generate latent HOI labels. To be specific, FreeA matches image features of human-object pairs with HOI text templates, and a priori knowledge-based mask method is developed to suppress improbable interactions. In addition, FreeA utilizes the proposed interaction correlation matching method to enhance the likelihood of actions related to a specified action, further refine the generated HOI labels. Experiments on two benchmark datasets show that FreeA achieves state-of-the-art performance among weakly supervised HOI models. Our approach is +8.58 mean Average Precision (mAP) on HICO-DET and +1.23 mAP on V-COCO more accurate in localizing an
&lt;/p&gt;</description></item></channel></rss>