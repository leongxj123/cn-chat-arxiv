<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#36951;&#24536;&#65288;BlockFUL&#65289;&#65292;&#20351;&#29992;Chameleon Hash&#65288;CH&#65289;&#25216;&#26415;&#37325;&#26032;&#35774;&#35745;&#21306;&#22359;&#38142;&#32467;&#26500;&#65292;&#20943;&#23569;&#27169;&#22411;&#26356;&#26032;&#30340;&#22797;&#26434;&#24615;&#21644;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.16294</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#19978;&#30340;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Decentralized Federated Unlearning on Blockchain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16294
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#36951;&#24536;&#65288;BlockFUL&#65289;&#65292;&#20351;&#29992;Chameleon Hash&#65288;CH&#65289;&#25216;&#26415;&#37325;&#26032;&#35774;&#35745;&#21306;&#22359;&#38142;&#32467;&#26500;&#65292;&#20943;&#23569;&#27169;&#22411;&#26356;&#26032;&#30340;&#22797;&#26434;&#24615;&#21644;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22312;&#30830;&#20445;FL&#36807;&#31243;&#30340;&#23436;&#25972;&#24615;&#21644;&#21487;&#36861;&#28335;&#24615;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#21306;&#22359;&#38142;FL&#28041;&#21450;&#21442;&#19982;&#32773;&#22312;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#24182;&#38543;&#21518;&#23558;&#27169;&#22411;&#21457;&#24067;&#21040;&#21306;&#22359;&#38142;&#19978;&#65292;&#24418;&#25104;&#34920;&#31034;&#27169;&#22411;&#20851;&#31995;&#30340;&#31867;&#20284;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#30340;&#32487;&#25215;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;DAG&#30340;&#32467;&#26500;&#22312;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#26356;&#26032;&#27169;&#22411;&#26102;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#28041;&#21450;&#30340;&#22797;&#26434;&#24615;&#21644;&#24320;&#38144;&#36739;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#36951;&#24536;&#65288;BlockFUL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20351;&#29992;&#21464;&#33394;&#40857;&#21704;&#24076;&#65288;CH&#65289;&#25216;&#26415;&#37325;&#26032;&#35774;&#35745;&#21306;&#22359;&#38142;&#32467;&#26500;&#65292;&#20197;&#20943;&#36731;&#27169;&#22411;&#26356;&#26032;&#30340;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#36951;&#24536;&#20219;&#21153;&#30340;&#35745;&#31639;&#21644;&#20849;&#35782;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;BlockFUL&#25903;&#25345;&#21508;&#31181;&#32852;&#37030;&#36951;&#24536;&#26041;&#27861;&#65292;&#30830;&#20445;&#27169;&#22411;&#26356;&#26032;&#30340;&#23436;&#25972;&#24615;&#21644;&#21487;&#36861;&#28335;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16294v1 Announce Type: cross  Abstract: Blockchained Federated Learning (FL) has been gaining traction for ensuring the integrity and traceability of FL processes. Blockchained FL involves participants training models locally with their data and subsequently publishing the models on the blockchain, forming a Directed Acyclic Graph (DAG)-like inheritance structure that represents the model relationship. However, this particular DAG-based structure presents challenges in updating models with sensitive data, due to the complexity and overhead involved. To address this, we propose Blockchained Federated Unlearning (BlockFUL), a generic framework that redesigns the blockchain structure using Chameleon Hash (CH) technology to mitigate the complexity of model updating, thereby reducing the computational and consensus costs of unlearning tasks.Furthermore, BlockFUL supports various federated unlearning methods, ensuring the integrity and traceability of model updates, whether conduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#36880;&#27493;&#32454;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#20248;&#21270;&#22270;&#20687;/&#25991;&#26412;&#38170;&#28857;&#19982;&#20854;&#36127;&#26679;&#26412;&#25991;&#26412;/&#22270;&#20687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#26377;&#25928;&#24212;&#23545;&#20102;&#65288;&#37096;&#20998;&#65289;&#35823;&#21453;&#26679;&#26412;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.04474</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#35843;&#33410;&#30340;&#23545;&#27604;&#23398;&#20064;&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Vision Langauge Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation. (arXiv:2305.04474v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#36880;&#27493;&#32454;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#20248;&#21270;&#22270;&#20687;/&#25991;&#26412;&#38170;&#28857;&#19982;&#20854;&#36127;&#26679;&#26412;&#25991;&#26412;/&#22270;&#20687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#26377;&#25928;&#24212;&#23545;&#20102;&#65288;&#37096;&#20998;&#65289;&#35823;&#21453;&#26679;&#26412;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#65292;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#38754;&#20020;&#30528;&#65288;&#37096;&#20998;&#65289;&#35823;&#21453;&#26679;&#26412;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20174; mutual information &#20248;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#28041;&#21450;&#21040;&#36127;&#26679;&#26412;&#30340;&#20114;&#20449;&#24687;&#20063;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#36880;&#27493;&#32454;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#20248;&#21270;&#22270;&#20687;/&#25991;&#26412;&#38170;&#28857;&#19982;&#20854;&#36127;&#26679;&#26412;&#25991;&#26412;/&#22270;&#20687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#19979;&#28216;&#36328;&#27169;&#24577;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#29702;&#35770;&#25351;&#23548;&#19979;&#31995;&#32479;&#22320;&#24179;&#34913;&#20102;&#65288;&#37096;&#20998;&#65289;&#35823;&#21453;&#26679;&#26412;&#30340;&#26377;&#30410;&#24433;&#21709;&#21644;&#26377;&#23475;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal contrastive learning in vision language pretraining (VLP) faces the challenge of (partial) false negatives. In this paper, we study this problem from the perspective of Mutual Information (MI) optimization. It is common sense that InfoNCE loss used in contrastive learning will maximize the lower bound of MI between anchors and their positives, while we theoretically prove that MI involving negatives also matters when noises commonly exist. Guided by a more general lower bound form for optimization, we propose a contrastive learning strategy regulated by progressively refined cross-modal similarity, to more accurately optimize MI between an image/text anchor and its negative texts/images instead of improperly minimizing it. Our method performs competitively on four downstream cross-modal tasks and systematically balances the beneficial and harmful effects of (partial) false negative samples under theoretical guidance.
&lt;/p&gt;</description></item></channel></rss>