<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#25968;&#25454;&#38598;&#20197;&#25913;&#36827;&#27169;&#22411;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02353</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#22312;&#22270;&#20687;&#20013;&#36827;&#34892;&#35821;&#20041;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Semantic Augmentation in Images using Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02353
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#25968;&#25454;&#38598;&#20197;&#25913;&#36827;&#27169;&#22411;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#38750;&#24120;&#24222;&#22823;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#32570;&#20047;&#36825;&#20123;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#24182;&#38480;&#21046;&#20854;&#27867;&#21270;&#21040;&#29616;&#23454;&#19990;&#30028;&#31034;&#20363;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#33021;&#22815;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#21033;&#29992;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#25193;&#25955;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#22270;&#20687;&#26469;&#22686;&#24378;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#26377;&#25928;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02353v1 Announce Type: cross  Abstract: Deep Learning models are incredibly data-hungry and require very large labeled datasets for supervised learning. As a consequence, these models often suffer from overfitting, limiting their ability to generalize to real-world examples. Recent advancements in diffusion models have enabled the generation of photorealistic images based on textual inputs. Leveraging the substantial datasets used to train these diffusion models, we propose a technique to utilize generated images to augment existing datasets. This paper explores various strategies for effective data augmentation to improve the out-of-domain generalization capabilities of deep learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#30740;&#31350;&#20102;&#20960;&#20309;&#32422;&#26463;&#21644;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#30340;&#37325;&#21512;&#37096;&#20998;&#65292;&#27604;&#36739;&#20102;&#28145;&#24230;&#20272;&#35745;&#31561;&#38382;&#39064;&#20013;&#38598;&#25104;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#20960;&#20309;&#24378;&#21046;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.12431</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#20960;&#20309;&#32422;&#26463;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Geometric Constraints in Deep Learning Frameworks: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#30740;&#31350;&#20102;&#20960;&#20309;&#32422;&#26463;&#21644;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#30340;&#37325;&#21512;&#37096;&#20998;&#65292;&#27604;&#36739;&#20102;&#28145;&#24230;&#20272;&#35745;&#31561;&#38382;&#39064;&#20013;&#38598;&#25104;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#20960;&#20309;&#24378;&#21046;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stereophotogrammetry&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#22330;&#26223;&#29702;&#35299;&#25216;&#26415;&#12290;&#20854;&#36215;&#28304;&#21487;&#20197;&#36861;&#28335;&#21040;&#33267;&#23569;19&#19990;&#32426;&#65292;&#24403;&#26102;&#20154;&#20204;&#24320;&#22987;&#30740;&#31350;&#20351;&#29992;&#29031;&#29255;&#26469;&#27979;&#37327;&#19990;&#30028;&#30340;&#29289;&#29702;&#23646;&#24615;&#12290;&#33258;&#37027;&#26102;&#20197;&#26469;&#65292;&#24050;&#32463;&#25506;&#32034;&#20102;&#25104;&#21315;&#19978;&#19975;&#31181;&#26041;&#27861;&#12290;&#32463;&#20856;&#20960;&#20309;&#25216;&#26415;&#30340;Shape from Stereo&#24314;&#31435;&#22312;&#20351;&#29992;&#20960;&#20309;&#26469;&#23450;&#20041;&#22330;&#26223;&#21644;&#25668;&#20687;&#26426;&#20960;&#20309;&#30340;&#32422;&#26463;&#65292;&#28982;&#21518;&#35299;&#20915;&#38750;&#32447;&#24615;&#26041;&#31243;&#32452;&#12290;&#26356;&#36817;&#26399;&#30340;&#24037;&#20316;&#37319;&#29992;&#20102;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#32780;&#27809;&#26377;&#26126;&#30830;&#24314;&#27169;&#20960;&#20309;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#20960;&#20309;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#37325;&#21472;&#37096;&#20998;&#12290;&#25105;&#20204;&#27604;&#36739;&#21644;&#23545;&#27604;&#20102;&#38598;&#25104;&#21040;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#29992;&#20110;&#28145;&#24230;&#20272;&#35745;&#25110;&#20854;&#20182;&#23494;&#20999;&#30456;&#20851;&#38382;&#39064;&#30340;&#20960;&#20309;&#24378;&#21046;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#26222;&#36941;&#20960;&#20309;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12431v1 Announce Type: cross  Abstract: Stereophotogrammetry is an emerging technique of scene understanding. Its origins go back to at least the 1800s when people first started to investigate using photographs to measure the physical properties of the world. Since then, thousands of approaches have been explored. The classic geometric techniques of Shape from Stereo is built on using geometry to define constraints on scene and camera geometry and then solving the non-linear systems of equations. More recent work has taken an entirely different approach, using end-to-end deep learning without any attempt to explicitly model the geometry. In this survey, we explore the overlap for geometric-based and deep learning-based frameworks. We compare and contrast geometry enforcing constraints integrated into a deep learning framework for depth estimation or other closely related problems. We present a new taxonomy for prevalent geometry enforcing constraints used in modern deep lear
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36229;&#20998;&#36776;&#29575;&#21644;&#32463;&#36807;&#35843;&#25972;&#30340;&#36731;&#37327;&#32423;YOLOv5&#26550;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#33322;&#31354;&#24433;&#20687;&#20013;&#23567;&#32780;&#23494;&#38598;&#29289;&#20307;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#36229;&#20998;&#36776;&#29575;YOLOv5&#27169;&#22411;&#37319;&#29992;Transformer&#32534;&#30721;&#22120;&#22359;&#65292;&#33021;&#22815;&#25429;&#25417;&#20840;&#23616;&#32972;&#26223;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#39640;&#23494;&#24230;&#12289;&#36974;&#25377;&#26465;&#20214;&#19979;&#25552;&#39640;&#26816;&#27979;&#32467;&#26524;&#12290;&#36825;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;&#19981;&#20165;&#20934;&#30830;&#24615;&#26356;&#39640;&#65292;&#32780;&#19988;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#39640;&#65292;&#38750;&#24120;&#36866;&#21512;&#23454;&#26102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.14661</link><description>&lt;p&gt;
&#20174;&#27169;&#31946;&#21040;&#26126;&#20142;&#30340;&#26816;&#27979;&#65306;&#22522;&#20110;YOLOv5&#30340;&#36229;&#20998;&#36776;&#29575;&#33322;&#31354;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
From Blurry to Brilliant Detection: YOLOv5-Based Aerial Object Detection with Super Resolution. (arXiv:2401.14661v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14661
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36229;&#20998;&#36776;&#29575;&#21644;&#32463;&#36807;&#35843;&#25972;&#30340;&#36731;&#37327;&#32423;YOLOv5&#26550;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#33322;&#31354;&#24433;&#20687;&#20013;&#23567;&#32780;&#23494;&#38598;&#29289;&#20307;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#36229;&#20998;&#36776;&#29575;YOLOv5&#27169;&#22411;&#37319;&#29992;Transformer&#32534;&#30721;&#22120;&#22359;&#65292;&#33021;&#22815;&#25429;&#25417;&#20840;&#23616;&#32972;&#26223;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#39640;&#23494;&#24230;&#12289;&#36974;&#25377;&#26465;&#20214;&#19979;&#25552;&#39640;&#26816;&#27979;&#32467;&#26524;&#12290;&#36825;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;&#19981;&#20165;&#20934;&#30830;&#24615;&#26356;&#39640;&#65292;&#32780;&#19988;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#39640;&#65292;&#38750;&#24120;&#36866;&#21512;&#23454;&#26102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26080;&#20154;&#26426;&#21644;&#21355;&#26143;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#23545;&#33322;&#31354;&#24433;&#20687;&#20013;&#20934;&#30830;&#29289;&#20307;&#26816;&#27979;&#30340;&#38656;&#27714;&#22823;&#22823;&#22686;&#21152;&#12290;&#20256;&#32479;&#30340;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#22312;&#20559;&#21521;&#22823;&#29289;&#20307;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#23545;&#20110;&#33322;&#31354;&#22330;&#26223;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#23567;&#32780;&#23494;&#38598;&#30340;&#29289;&#20307;&#38590;&#20197;&#21457;&#25381;&#26368;&#20339;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#36229;&#20998;&#36776;&#29575;&#21644;&#32463;&#36807;&#35843;&#25972;&#30340;&#36731;&#37327;&#32423;YOLOv5&#26550;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;VisDrone-2023&#12289;SeaDroneSee&#12289;VEDAI&#21644;NWPU VHR-10&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#36229;&#20998;&#36776;&#29575;YOLOv5&#26550;&#26500;&#37319;&#29992;Transformer&#32534;&#30721;&#22120;&#22359;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#21040;&#20840;&#23616;&#32972;&#26223;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#27979;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#23494;&#24230;&#12289;&#36974;&#25377;&#26465;&#20214;&#19979;&#12290;&#36825;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;&#19981;&#20165;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#36824;&#30830;&#20445;&#20102;&#36164;&#28304;&#30340;&#26377;&#25928;&#21033;&#29992;&#65292;&#38750;&#24120;&#36866;&#21512;&#23454;&#26102;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#33322;&#31354;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demand for accurate object detection in aerial imagery has surged with the widespread use of drones and satellite technology. Traditional object detection models, trained on datasets biased towards large objects, struggle to perform optimally in aerial scenarios where small, densely clustered objects are prevalent. To address this challenge, we present an innovative approach that combines super-resolution and an adapted lightweight YOLOv5 architecture. We employ a range of datasets, including VisDrone-2023, SeaDroneSee, VEDAI, and NWPU VHR-10, to evaluate our model's performance. Our Super Resolved YOLOv5 architecture features Transformer encoder blocks, allowing the model to capture global context and context information, leading to improved detection results, especially in high-density, occluded conditions. This lightweight model not only delivers improved accuracy but also ensures efficient resource utilization, making it well-suited for real-time applications. Our experimental 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22686;&#37327;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#23454;&#20363;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#22312;&#27599;&#20010;&#26032;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.11482</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#22686;&#37327;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking Class-incremental Learning in the Era of Large Pre-trained Models via Test-Time Adaptation. (arXiv:2310.11482v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22686;&#37327;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#23454;&#20363;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#22312;&#27599;&#20010;&#26032;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#25345;&#32493;&#23398;&#20064;&#23558;&#31867;&#21035;&#21010;&#20998;&#21040;&#26032;&#20219;&#21153;&#20013;&#65292;&#21516;&#26102;&#19981;&#20250;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#21152;&#24555;&#20102;&#22686;&#37327;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#22240;&#20026;&#39640;&#24230;&#21487;&#20256;&#36755;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#34920;&#31034;&#20351;&#24471;&#22312;&#35843;&#25972;&#19968;&#23567;&#32452;&#21442;&#25968;&#26102;&#65292;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#20256;&#32479;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#27599;&#20010;&#20219;&#21153;&#36827;&#34892;&#21453;&#22797;&#24494;&#35843;&#20250;&#30772;&#22351;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#24182;&#23548;&#33268;&#36951;&#24536;&#20043;&#21069;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#30452;&#25509;&#22312;&#27979;&#35797;&#23454;&#20363;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#36866;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22686;&#37327;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#8221;&#65288;TTACIL&#65289;&#65292;&#23427;&#39318;&#20808;&#22312;&#27599;&#20010;&#27979;&#35797;&#23454;&#20363;&#19978;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23618;&#24402;&#19968;&#21270;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental learning (CIL) is a challenging task that involves continually learning to categorize classes into new tasks without forgetting previously learned information. The advent of the large pre-trained models (PTMs) has fast-tracked the progress in CIL due to the highly transferable PTM representations, where tuning a small set of parameters results in state-of-the-art performance when compared with the traditional CIL methods that are trained from scratch. However, repeated fine-tuning on each task destroys the rich representations of the PTMs and further leads to forgetting previous tasks. To strike a balance between the stability and plasticity of PTMs for CIL, we propose a novel perspective of eliminating training on every new task and instead performing test-time adaptation (TTA) directly on the test instances. Concretely, we propose "Test-Time Adaptation for Class-Incremental Learning" (TTACIL) that first fine-tunes Layer Norm parameters of the PTM on each test instan
&lt;/p&gt;</description></item></channel></rss>