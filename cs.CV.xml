<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#39318;&#27425;&#20351;&#29992;&#26469;&#33258;&#21363;&#25554;&#21363;&#29992;&#30340;&#38388;&#25509;&#39134;&#34892;&#26102;&#38388;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#20809;&#32447;&#21453;&#23556;&#21457;&#29983;&#30340;&#34920;&#38754;&#37325;&#26032;&#26500;&#24314;&#20026;&#34394;&#25311;&#38236;&#23376;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#33719;&#21462;&#38544;&#34255;&#22330;&#26223;&#28145;&#24230;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19376</link><description>&lt;p&gt;
NIGHT -- &#38388;&#25509;&#39134;&#34892;&#26102;&#38388;&#25968;&#25454;&#30340;&#38750;&#35270;&#36317;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
NIGHT -- Non-Line-of-Sight Imaging from Indirect Time of Flight Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20351;&#29992;&#26469;&#33258;&#21363;&#25554;&#21363;&#29992;&#30340;&#38388;&#25509;&#39134;&#34892;&#26102;&#38388;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#20809;&#32447;&#21453;&#23556;&#21457;&#29983;&#30340;&#34920;&#38754;&#37325;&#26032;&#26500;&#24314;&#20026;&#34394;&#25311;&#38236;&#23376;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#33719;&#21462;&#38544;&#34255;&#22330;&#26223;&#28145;&#24230;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38750;&#35270;&#35282;&#30456;&#26426;&#22806;&#37096;&#33719;&#21462;&#29289;&#20307;&#26159;&#19968;&#20010;&#38750;&#24120;&#24341;&#20154;&#27880;&#30446;&#20294;&#20063;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#21033;&#29992;&#23450;&#21046;&#30340;&#30452;&#25509;&#39134;&#34892;&#26102;&#38388;&#20256;&#24863;&#22120;&#20135;&#29983;&#30340;&#30636;&#26102;&#25104;&#20687;&#25968;&#25454;&#65292;&#36825;&#20010;&#24819;&#27861;&#26159;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20351;&#29992;&#26469;&#33258;&#21363;&#25554;&#21363;&#29992;&#30340;&#38388;&#25509;&#39134;&#34892;&#26102;&#38388;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#30828;&#20214;&#35201;&#27714;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#20809;&#32447;&#21453;&#23556;&#21457;&#29983;&#30340;&#34920;&#38754;&#37325;&#26032;&#26500;&#24314;&#20026;&#34394;&#25311;&#38236;&#23376;&#12290;&#36825;&#31181;&#24314;&#27169;&#20351;&#24471;&#20219;&#21153;&#26356;&#23481;&#26131;&#22788;&#29702;&#65292;&#20063;&#26377;&#21161;&#20110;&#26500;&#24314;&#24102;&#26377;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20174;&#33719;&#24471;&#30340;&#25968;&#25454;&#20013;&#65292;&#21487;&#20197;&#24674;&#22797;&#38544;&#34255;&#22330;&#26223;&#30340;&#28145;&#24230;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#39318;&#21019;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#24819;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19376v1 Announce Type: cross  Abstract: The acquisition of objects outside the Line-of-Sight of cameras is a very intriguing but also extremely challenging research topic. Recent works showed the feasibility of this idea exploiting transient imaging data produced by custom direct Time of Flight sensors. In this paper, for the first time, we tackle this problem using only data from an off-the-shelf indirect Time of Flight sensor without any further hardware requirement. We introduced a Deep Learning model able to re-frame the surfaces where light bounces happen as a virtual mirror. This modeling makes the task easier to handle and also facilitates the construction of annotated training data. From the obtained data it is possible to retrieve the depth information of the hidden scene. We also provide a first-in-its-kind synthetic dataset for the task and demonstrate the feasibility of the proposed idea over it.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HandFormer&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#21644;&#31232;&#30095;&#37319;&#26679;&#30340;RGB&#24103;&#65292;&#29992;&#20110;&#26377;&#25928;&#24314;&#27169;&#25163;&#37096;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09805</link><description>&lt;p&gt;
&#20851;&#20110;3D&#25163;&#37096;&#23039;&#21183;&#22312;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Utility of 3D Hand Poses for Action Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HandFormer&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#21644;&#31232;&#30095;&#37319;&#26679;&#30340;RGB&#24103;&#65292;&#29992;&#20110;&#26377;&#25928;&#24314;&#27169;&#25163;&#37096;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#25163;&#37096;&#23039;&#21183;&#26159;&#19968;&#31181;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#21160;&#20316;&#35782;&#21035;&#27169;&#24577;&#12290;&#23039;&#21183;&#26082;&#32039;&#20945;&#21448;&#20449;&#24687;&#20016;&#23500;&#65292;&#24182;&#19988;&#21487;&#20197;&#26497;&#22823;&#22320;&#21463;&#30410;&#20110;&#35745;&#31639;&#39044;&#31639;&#26377;&#38480;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#21333;&#29420;&#30340;&#23039;&#21183;&#19981;&#33021;&#23436;&#20840;&#29702;&#35299;&#20154;&#31867;&#19982;&#20043;&#20132;&#20114;&#30340;&#29289;&#20307;&#21644;&#29615;&#22659;&#12290;&#20026;&#20102;&#26377;&#25928;&#24314;&#27169;&#25163;&#37096;&#29289;&#20307;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HandFormer&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;Transformer&#12290;HandFormer&#32467;&#21512;&#20102;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#65292;&#29992;&#20110;&#31934;&#32454;&#36816;&#21160;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#31232;&#30095;&#37319;&#26679;&#30340;RGB&#24103;&#26469;&#32534;&#30721;&#22330;&#26223;&#35821;&#20041;&#12290;&#35266;&#23519;&#25163;&#37096;&#23039;&#21183;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#25105;&#20204;&#23545;&#25163;&#37096;&#24314;&#27169;&#36827;&#34892;&#20102;&#26102;&#38388;&#20998;&#35299;&#65292;&#24182;&#36890;&#36807;&#20854;&#30701;&#26399;&#36712;&#36857;&#34920;&#31034;&#27599;&#20010;&#20851;&#33410;&#28857;&#12290;&#36825;&#31181;&#34987;&#20998;&#35299;&#30340;&#23039;&#21183;&#34920;&#31034;&#19982;&#31232;&#30095;&#30340;RGB&#37319;&#26679;&#30456;&#32467;&#21512;&#65292;&#25928;&#29575;&#38750;&#24120;&#39640;&#65292;&#24182;&#19988;&#36798;&#21040;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#20165;&#26377;&#25163;&#37096;&#23039;&#21183;&#30340;&#21333;&#27169;HandFormer&#22312;5&#20493;&#26356;&#23569;&#30340;FLO&#19979;&#32988;&#36807;&#29616;&#26377;&#22522;&#20110;&#39592;&#26550;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09805v1 Announce Type: cross  Abstract: 3D hand poses are an under-explored modality for action recognition. Poses are compact yet informative and can greatly benefit applications with limited compute budgets. However, poses alone offer an incomplete understanding of actions, as they cannot fully capture objects and environments with which humans interact. To efficiently model hand-object interactions, we propose HandFormer, a novel multimodal transformer. HandFormer combines 3D hand poses at a high temporal resolution for fine-grained motion modeling with sparsely sampled RGB frames for encoding scene semantics. Observing the unique characteristics of hand poses, we temporally factorize hand modeling and represent each joint by its short-term trajectories. This factorized pose representation combined with sparse RGB samples is remarkably efficient and achieves high accuracy. Unimodal HandFormer with only hand poses outperforms existing skeleton-based methods at 5x fewer FLO
&lt;/p&gt;</description></item><item><title>VIRUS-NeRF&#26159;&#22522;&#20110;&#35270;&#35273;&#12289;&#32418;&#22806;&#21644;&#36229;&#22768;&#27874;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#36890;&#36807;&#25972;&#21512;&#36229;&#22768;&#27874;&#21644;&#32418;&#22806;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#27979;&#37327;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22312;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#20013;&#36798;&#21040;&#19982;LiDAR&#28857;&#20113;&#30456;&#23218;&#32654;&#30340;&#26144;&#23556;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09477</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#12289;&#32418;&#22806;&#21644;&#36229;&#22768;&#27874;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#8212;&#8212;VIRUS-NeRF
&lt;/p&gt;
&lt;p&gt;
VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09477
&lt;/p&gt;
&lt;p&gt;
VIRUS-NeRF&#26159;&#22522;&#20110;&#35270;&#35273;&#12289;&#32418;&#22806;&#21644;&#36229;&#22768;&#27874;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#36890;&#36807;&#25972;&#21512;&#36229;&#22768;&#27874;&#21644;&#32418;&#22806;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#27979;&#37327;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22312;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#20013;&#36798;&#21040;&#19982;LiDAR&#28857;&#20113;&#30456;&#23218;&#32654;&#30340;&#26144;&#23556;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#22312;&#29616;&#20195;&#24037;&#21378;&#21644;&#20179;&#24211;&#25805;&#20316;&#20013;&#36215;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38556;&#30861;&#29289;&#26816;&#27979;&#12289;&#22238;&#36991;&#21644;&#36335;&#24452;&#35268;&#21010;&#26159;&#20851;&#38190;&#30340;&#23433;&#20840;&#30456;&#20851;&#20219;&#21153;&#65292;&#36890;&#24120;&#20351;&#29992;&#26114;&#36149;&#30340;LiDAR&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#25668;&#20687;&#22836;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25104;&#26412;&#25928;&#30410;&#30340;&#20302;&#20998;&#36776;&#29575;&#27979;&#36317;&#20256;&#24863;&#22120;&#65292;&#22914;&#36229;&#22768;&#27874;&#21644;&#32418;&#22806;&#26102;&#38388;&#39134;&#34892;&#20256;&#24863;&#22120;&#65292;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#35270;&#35273;&#12289;&#32418;&#22806;&#21644;&#36229;&#22768;&#27874;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;(VIRUS-NeRF)&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;VIRUS-NeRF&#26500;&#24314;&#22312;&#30636;&#26102;&#31070;&#32463;&#22270;&#24418;&#22522;&#20803;&#19982;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32534;&#30721;(Instant-NGP)&#30340;&#22522;&#30784;&#19978;&#65292;&#34701;&#21512;&#20102;&#36229;&#22768;&#27874;&#21644;&#32418;&#22806;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#27979;&#37327;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#26356;&#26032;&#29992;&#20110;&#20809;&#32447;&#36319;&#36394;&#30340;&#21344;&#25454;&#32593;&#26684;&#12290;&#22312;2D&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;VIRUS-NeRF&#23454;&#29616;&#20102;&#19982;LiDAR&#28857;&#20113;&#30456;&#23218;&#32654;&#30340;&#26144;&#23556;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#23567;&#22411;&#29615;&#22659;&#20013;&#65292;&#20854;&#20934;&#30830;&#24615;&#19982;LiDAR&#27979;&#37327;&#30456;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09477v1 Announce Type: cross  Abstract: Autonomous mobile robots are an increasingly integral part of modern factory and warehouse operations. Obstacle detection, avoidance and path planning are critical safety-relevant tasks, which are often solved using expensive LiDAR sensors and depth cameras. We propose to use cost-effective low-resolution ranging sensors, such as ultrasonic and infrared time-of-flight sensors by developing VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance Fields. Building upon Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from ultrasonic and infrared sensors and utilizes them to update the occupancy grid used for ray marching. Experimental evaluation in 2D demonstrates that VIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds regarding coverage. Notably, in small environments, its accuracy aligns with that of LiDAR measurements, whi
&lt;/p&gt;</description></item><item><title>&#22312;&#25968;&#25454;&#21098;&#26525;&#20013;&#24341;&#20837;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#39044;&#20808;&#35757;&#32451;&#30340;&#25945;&#24072;&#32593;&#32476;&#36719;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#21098;&#26525;&#26041;&#27861;&#21644;&#25152;&#26377;&#21098;&#26525;&#20998;&#25968;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.07854</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#21098;&#26525;&#20013;&#33976;&#39311;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distilling the Knowledge in Data Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07854
&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#21098;&#26525;&#20013;&#24341;&#20837;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#39044;&#20808;&#35757;&#32451;&#30340;&#25945;&#24072;&#32593;&#32476;&#36719;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#21098;&#26525;&#26041;&#27861;&#21644;&#25152;&#26377;&#21098;&#26525;&#20998;&#25968;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#19981;&#26029;&#22686;&#21152;&#65292;&#25968;&#25454;&#21098;&#26525;&#25104;&#20026;&#20102;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#25968;&#25454;&#21098;&#26525;&#31639;&#27861;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#24230;&#21098;&#26525;&#30340;&#24773;&#20917;&#19979;&#19982;&#20351;&#29992;&#23436;&#25972;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#22522;&#20110;&#21098;&#26525;&#23376;&#38598;&#30340;&#27169;&#22411;&#26102;&#65292;&#32467;&#21512;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30340;&#24212;&#29992;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#19981;&#20165;&#20381;&#36182;&#20110;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#65292;&#36824;&#20351;&#29992;&#20102;&#24050;&#22312;&#23436;&#25972;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#32769;&#24072;&#32593;&#32476;&#30340;&#36719;&#39044;&#27979;&#12290;&#36890;&#36807;&#23558;&#30693;&#35782;&#33976;&#39311;&#25972;&#21512;&#21040;&#35757;&#32451;&#20013;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#21098;&#26525;&#26041;&#27861;&#21644;&#25152;&#26377;&#21098;&#26525;&#20998;&#25968;&#19978;&#37117;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#37319;&#29992;&#33258;&#33976;&#39311;&#26469;&#25913;&#21892;&#22312;&#21098;&#26525;&#25968;&#25454;&#19978;&#30340;&#35757;&#32451;&#30340;&#29702;&#35770;&#21160;&#26426;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#36827;&#34892;&#20102;&#24341;&#20154;&#27880;&#30446;&#19988;&#39640;&#24230;&#23454;&#29992;&#30340;&#35266;&#23519;&#65306;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65292;&#31616;&#21333;&#30340;&#38543;&#26426;&#21098;&#26525;&#20063;&#20250;&#21462;&#24471;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07854v1 Announce Type: cross  Abstract: With the increasing size of datasets used for training neural networks, data pruning becomes an attractive field of research. However, most current data pruning algorithms are limited in their ability to preserve accuracy compared to models trained on the full data, especially in high pruning regimes. In this paper we explore the application of data pruning while incorporating knowledge distillation (KD) when training on a pruned subset. That is, rather than relying solely on ground-truth labels, we also use the soft predictions from a teacher network pre-trained on the complete data. By integrating KD into training, we demonstrate significant improvement across datasets, pruning methods, and on all pruning fractions. We first establish a theoretical motivation for employing self-distillation to improve training on pruned data. Then, we empirically make a compelling and highly practical observation: using KD, simple random pruning is c
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CFRet-DVQA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#20013;&#23450;&#20301;&#20449;&#24687;&#21644;&#38480;&#21046;&#27169;&#22411;&#36755;&#20837;&#30340;&#38271;&#24230;&#31561;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#31572;&#26696;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00816</link><description>&lt;p&gt;
CFRet-DVQA&#65306;&#31895;&#21040;&#31934;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#29992;&#20110;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CFRet-DVQA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#65292;&#35299;&#20915;&#20102;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#20013;&#23450;&#20301;&#20449;&#24687;&#21644;&#38480;&#21046;&#27169;&#22411;&#36755;&#20837;&#30340;&#38271;&#24230;&#31561;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#31572;&#26696;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#65288;DVQA&#65289;&#26159;&#19968;&#20010;&#28041;&#21450;&#26681;&#25454;&#22270;&#20687;&#20869;&#23481;&#22238;&#31572;&#26597;&#35810;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#24037;&#20316;&#20165;&#38480;&#20110;&#23450;&#20301;&#21333;&#39029;&#20869;&#30340;&#20449;&#24687;&#65292;&#19981;&#25903;&#25345;&#36328;&#39029;&#38754;&#38382;&#31572;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#23545;&#27169;&#22411;&#36755;&#20837;&#30340;&#26631;&#35760;&#38271;&#24230;&#38480;&#21046;&#21487;&#33021;&#23548;&#33268;&#19982;&#31572;&#26696;&#30456;&#20851;&#30340;&#37096;&#20998;&#34987;&#25130;&#26029;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#23398;&#65292;&#31216;&#20026;CFRet-DVQA&#65292;&#37325;&#28857;&#25918;&#22312;&#26816;&#32034;&#21644;&#39640;&#25928;&#35843;&#20248;&#19978;&#65292;&#20197;&#26377;&#25928;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#25991;&#26723;&#20013;&#26816;&#32034;&#19982;&#25152;&#25552;&#38382;&#39064;&#30456;&#20851;&#30340;&#22810;&#20010;&#29255;&#27573;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20808;&#36827;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;&#25991;&#26723;&#26631;&#31614;&#30340;&#39118;&#26684;&#30456;&#31526;&#12290;&#23454;&#39564;&#28436;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00816v1 Announce Type: cross  Abstract: Document Visual Question Answering (DVQA) is a task that involves responding to queries based on the content of images. Existing work is limited to locating information within a single page and does not facilitate cross-page question-and-answer interaction. Furthermore, the token length limitation imposed on inputs to the model may lead to truncation of segments pertinent to the answer. In this study, we introduce a simple but effective methodology called CFRet-DVQA, which focuses on retrieval and efficient tuning to address this critical issue effectively. For that, we initially retrieve multiple segments from the document that correlate with the question at hand. Subsequently, we leverage the advanced reasoning abilities of the large language model (LLM), further augmenting its performance through instruction tuning. This approach enables the generation of answers that align with the style of the document labels. The experiments demo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#20256;&#36755;&#20449;&#24687;&#29942;&#39048;&#26469;&#23454;&#29616;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21387;&#32553;&#34920;&#31034;&#20449;&#24687;&#21644;&#20445;&#30041;&#37325;&#35201;&#20449;&#24687;&#20043;&#38388;&#32500;&#25345;&#24179;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#21464;&#20998;&#25512;&#26029;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#21487;&#35745;&#31639;&#20272;&#35745;&#30340;DisTIB&#12290;</title><link>http://arxiv.org/abs/2311.01686</link><description>&lt;p&gt;
&#20351;&#29992;&#20256;&#36755;&#30340;&#20449;&#24687;&#29942;&#39048;&#23454;&#29616;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning with Transmitted Information Bottleneck. (arXiv:2311.01686v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#20256;&#36755;&#20449;&#24687;&#29942;&#39048;&#26469;&#23454;&#29616;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21387;&#32553;&#34920;&#31034;&#20449;&#24687;&#21644;&#20445;&#30041;&#37325;&#35201;&#20449;&#24687;&#20043;&#38388;&#32500;&#25345;&#24179;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#21464;&#20998;&#25512;&#26029;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#21487;&#35745;&#31639;&#20272;&#35745;&#30340;DisTIB&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#32534;&#30721;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#21407;&#22987;&#25968;&#25454;&#20449;&#24687;&#65292;&#21363;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#34429;&#28982;&#22312;&#34920;&#31034;&#20013;&#21033;&#29992;&#20449;&#24687;&#29702;&#35770;&#23545;&#20449;&#24687;&#36827;&#34892;&#35268;&#33539;&#21270;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#34920;&#31034;&#21387;&#32553;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65307;2&#65289;&#23545;&#34920;&#31034;&#30340;&#35299;&#32544;&#32422;&#26463;&#23384;&#22312;&#22797;&#26434;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20256;&#36755;&#20449;&#24687;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#26469;&#25551;&#36848;&#35299;&#32544;&#36807;&#31243;&#20013;&#36755;&#20837;&#21644;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"DisTIB"&#65288;&#29992;&#20110;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#20256;&#36755;&#20449;&#24687;&#29942;&#39048;&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#24179;&#34913;&#20449;&#24687;&#21387;&#32553;&#21644;&#20445;&#30041;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#37319;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#23548;&#20986;DisTIB&#30340;&#21487;&#35745;&#31639;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Encoding only the task-related information from the raw data, \ie, disentangled representation learning, can greatly contribute to the robustness and generalizability of models. Although significant advances have been made by regularizing the information in representations with information theory, two major challenges remain: 1) the representation compression inevitably leads to performance drop; 2) the disentanglement constraints on representations are in complicated optimization. To these issues, we introduce Bayesian networks with transmitted information to formulate the interaction among input and representations during disentanglement. Building upon this framework, we propose \textbf{DisTIB} (\textbf{T}ransmitted \textbf{I}nformation \textbf{B}ottleneck for \textbf{Dis}entangled representation learning), a novel objective that navigates the balance between information compression and preservation. We employ variational inference to derive a tractable estimation for DisTIB. This es
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoCLIP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;AutoCLIP&#36890;&#36807;&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20174;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#25512;&#23548;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.16414</link><description>&lt;p&gt;
AutoCLIP: &#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models. (arXiv:2309.16414v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoCLIP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;AutoCLIP&#36890;&#36807;&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20174;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#25512;&#23548;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#26500;&#24314;&#30340;&#20998;&#31867;&#22120;&#22312;&#24191;&#27867;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#26681;&#25454;&#25552;&#31034;&#27169;&#26495;&#33258;&#21160;&#21019;&#24314;&#27599;&#20010;&#31867;&#21035;&#30340;&#25551;&#36848;&#31526;&#38598;&#30340;&#19981;&#21516;&#26041;&#24335;&#65292;&#21253;&#25324;&#25163;&#24037;&#35774;&#35745;&#30340;&#27169;&#26495;&#12289;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#30340;&#27169;&#26495;&#20197;&#21450;&#20174;&#38543;&#26426;&#21333;&#35789;&#21644;&#23383;&#31526;&#26500;&#24314;&#30340;&#27169;&#26495;&#12290;&#28982;&#32780;&#65292;&#20174;&#30456;&#24212;&#30340;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#23548;&#20986;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#65306;&#23558;&#22270;&#20687;&#30340;&#24179;&#22343;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#19982;&#32534;&#30721;&#22270;&#20687;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#26368;&#22823;&#21270;&#20197;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#24403;&#26576;&#20123;&#25551;&#36848;&#31526;&#27604;&#20854;&#20182;&#25551;&#36848;&#31526;&#26356;&#22909;&#22320;&#21305;&#37197;&#32473;&#23450;&#22270;&#20687;&#19978;&#30340;&#35270;&#35273;&#32447;&#32034;&#26102;&#65292;&#23558;&#25152;&#26377;&#31867;&#21035;&#25551;&#36848;&#31526;&#31561;&#26435;&#37325;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35843;&#35856;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;AutoCLIP&#12290;AutoCLIP&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#20102;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#36825;&#20123;&#26435;&#37325;&#26159;&#20174;s
&lt;/p&gt;
&lt;p&gt;
Classifiers built upon vision-language models such as CLIP have shown remarkable zero-shot performance across a broad range of image classification tasks. Prior work has studied different ways of automatically creating descriptor sets for every class based on prompt templates, ranging from manually engineered templates over templates obtained from a large language model to templates built from random words and characters. In contrast, deriving zero-shot classifiers from the respective encoded class descriptors has remained nearly unchanged, that is: classify to the class that maximizes the cosine similarity between its averaged encoded class descriptors and the encoded image. However, weighting all class descriptors equally can be suboptimal when certain descriptors match visual clues on a given image better than others. In this work, we propose AutoCLIP, a method for auto-tuning zero-shot classifiers. AutoCLIP assigns to each prompt template per-image weights, which are derived from s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#21487;&#25193;&#23637;&#28145;&#24230;&#21387;&#32553;&#24863;&#30693;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#27979;&#37327;-&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#30340;&#37319;&#26679;&#27604;&#29575;&#21644;&#30697;&#38453;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#21452;&#22495;&#25439;&#22833;&#21644;&#22235;&#20010;&#24674;&#22797;&#38454;&#27573;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25968;&#25454;/&#20449;&#24687;&#21033;&#29992;&#29575;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13777</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#21487;&#25193;&#23637;&#28145;&#24230;&#21387;&#32553;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Scalable Deep Compressed Sensing. (arXiv:2308.13777v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#21487;&#25193;&#23637;&#28145;&#24230;&#21387;&#32553;&#24863;&#30693;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#27979;&#37327;-&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#30340;&#37319;&#26679;&#27604;&#29575;&#21644;&#30697;&#38453;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#21452;&#22495;&#25439;&#22833;&#21644;&#22235;&#20010;&#24674;&#22797;&#38454;&#27573;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25968;&#25454;/&#20449;&#24687;&#21033;&#29992;&#29575;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21387;&#32553;&#24863;&#30693;&#65288;CS&#65289;&#26159;&#38477;&#20302;&#37319;&#26679;&#25104;&#26412;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#24037;&#20855;&#12290;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;CS&#26041;&#27861;&#22312;&#25910;&#38598;&#26631;&#35760;&#30340;&#27979;&#37327;-&#22320;&#38754;&#30495;&#23454;&#65288;GT&#65289;&#25968;&#25454;&#21644;&#25512;&#24191;&#21040;&#23454;&#38469;&#24212;&#29992;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#21487;&#25193;&#23637;&#28145;&#24230;CS&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#20010;&#31216;&#20026;SCL&#30340;&#23398;&#20064;&#26041;&#26696;&#21644;&#19968;&#20010;&#21517;&#20026;SCNet&#30340;&#32593;&#32476;&#31995;&#21015;&#65292;&#23427;&#19981;&#38656;&#35201;GT&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#19968;&#26086;&#22312;&#37096;&#20998;&#27979;&#37327;&#38598;&#19978;&#35757;&#32451;&#23436;&#27605;&#23601;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#30340;&#37319;&#26679;&#27604;&#29575;&#21644;&#30697;&#38453;&#12290;&#25105;&#20204;&#30340;SCL&#21253;&#21547;&#19968;&#20010;&#21452;&#22495;&#25439;&#22833;&#21644;&#19968;&#20010;&#22235;&#38454;&#27573;&#24674;&#22797;&#31574;&#30053;&#12290;&#21069;&#32773;&#40723;&#21169;&#20004;&#20010;&#27979;&#37327;&#37096;&#20998;&#30340;&#20132;&#21449;&#19968;&#33268;&#24615;&#20197;&#21450;&#37319;&#26679;-&#37325;&#26500;&#24490;&#29615;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#25968;&#25454;/&#20449;&#24687;&#21033;&#29992;&#29575;&#12290;&#21518;&#32773;&#21487;&#20197;&#36880;&#27493;&#21033;&#29992;&#22806;&#37096;&#27979;&#37327;&#20013;&#30340;&#24120;&#35265;&#20449;&#21495;&#20808;&#39564;&#21644;&#27979;&#35797;&#26679;&#26412;&#20197;&#21450;&#23398;&#20064;&#30340;NN&#30340;&#20869;&#37096;&#29305;&#24449;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compressed sensing (CS) is a promising tool for reducing sampling costs. Current deep neural network (NN)-based CS methods face challenges in collecting labeled measurement-ground truth (GT) data and generalizing to real applications. This paper proposes a novel $\mathbf{S}$elf-supervised s$\mathbf{C}$alable deep CS method, comprising a $\mathbf{L}$earning scheme called $\mathbf{SCL}$ and a family of $\mathbf{Net}$works named $\mathbf{SCNet}$, which does not require GT and can handle arbitrary sampling ratios and matrices once trained on a partial measurement set. Our SCL contains a dual-domain loss and a four-stage recovery strategy. The former encourages a cross-consistency on two measurement parts and a sampling-reconstruction cycle-consistency regarding arbitrary ratios and matrices to maximize data/information utilization. The latter can progressively leverage common signal prior in external measurements and internal characteristics of test samples and learned NNs to improve accur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34917;&#19969;&#21270;&#33258;&#32534;&#30721;&#22120;&#65288;Patch AE&#65289;&#26694;&#26550;&#26469;&#22686;&#24378;&#33258;&#32534;&#30721;&#22120;&#23545;&#24322;&#24120;&#30340;&#37325;&#26500;&#33021;&#21147;&#65292;&#24182;&#22312;Mvtec AD&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#20855;&#26377;&#22312;&#23454;&#38469;&#24037;&#19994;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00429</link><description>&lt;p&gt;
&#22522;&#20110;&#34917;&#19969;&#21270;&#33258;&#32534;&#30721;&#22120;&#30340;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Patch-wise Auto-Encoder for Visual Anomaly Detection. (arXiv:2308.00429v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34917;&#19969;&#21270;&#33258;&#32534;&#30721;&#22120;&#65288;Patch AE&#65289;&#26694;&#26550;&#26469;&#22686;&#24378;&#33258;&#32534;&#30721;&#22120;&#23545;&#24322;&#24120;&#30340;&#37325;&#26500;&#33021;&#21147;&#65292;&#24182;&#22312;Mvtec AD&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#20855;&#26377;&#22312;&#23454;&#38469;&#24037;&#19994;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#24322;&#24120;&#20808;&#39564;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#65292;&#20256;&#32479;&#30340;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#22312;&#20165;&#36890;&#36807;&#27491;&#24120;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#26102;&#20542;&#21521;&#20110;&#22833;&#36133;&#65292;&#22240;&#20026;&#27169;&#22411;&#23558;&#26080;&#27861;&#27491;&#30830;&#37325;&#26500;&#24322;&#24120;&#22270;&#20687;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34917;&#19969;&#21270;&#33258;&#32534;&#30721;&#22120;&#65288;Patch AE&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;AE&#23545;&#24322;&#24120;&#30340;&#37325;&#26500;&#33021;&#21147;&#32780;&#19981;&#26159;&#21066;&#24369;&#23427;&#12290;&#22270;&#20687;&#30340;&#27599;&#20010;&#34917;&#19969;&#37117;&#36890;&#36807;&#30456;&#24212;&#30340;&#31354;&#38388;&#20998;&#24067;&#29305;&#24449;&#21521;&#37327;&#30340;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#36827;&#34892;&#37325;&#26500;&#65292;&#21363;&#34917;&#19969;&#21270;&#37325;&#26500;&#65292;&#36825;&#30830;&#20445;&#20102;AE&#23545;&#24322;&#24120;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21333;&#39640;&#25928;&#12290;&#23427;&#22312;Mvtec AD&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#23427;&#22312;&#23454;&#38469;&#24037;&#19994;&#24212;&#29992;&#22330;&#26223;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection without priors of the anomalies is challenging. In the field of unsupervised anomaly detection, traditional auto-encoder (AE) tends to fail based on the assumption that by training only on normal images, the model will not be able to reconstruct abnormal images correctly. On the contrary, we propose a novel patch-wise auto-encoder (Patch AE) framework, which aims at enhancing the reconstruction ability of AE to anomalies instead of weakening it. Each patch of image is reconstructed by corresponding spatially distributed feature vector of the learned feature representation, i.e., patch-wise reconstruction, which ensures anomaly-sensitivity of AE. Our method is simple and efficient. It advances the state-of-the-art performances on Mvtec AD benchmark, which proves the effectiveness of our model. It shows great potential in practical industrial application scenarios.
&lt;/p&gt;</description></item></channel></rss>