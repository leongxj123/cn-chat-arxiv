<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36890;&#36807;&#35299;&#32806;&#20132;&#20114;&#35821;&#20041;&#21644;&#21160;&#24577;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#30452;&#25509;&#35757;&#32451;&#25991;&#26412;-&#20132;&#20114;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.19652</link><description>&lt;p&gt;
InterDreamer&#65306;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#19977;&#32500;&#21160;&#24577;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19652
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35299;&#32806;&#20132;&#20114;&#35821;&#20041;&#21644;&#21160;&#24577;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#30452;&#25509;&#35757;&#32451;&#25991;&#26412;-&#20132;&#20114;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19652v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22312;&#24191;&#27867;&#30340;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#21644;&#30456;&#24212;&#30340;&#25991;&#26412;&#27880;&#37322;&#19978;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#26174;&#33879;&#25512;&#21160;&#20102;&#25991;&#26412;&#26465;&#20214;&#30340;&#20154;&#20307;&#36816;&#21160;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#25104;&#21151;&#24310;&#20280;&#21040;&#19977;&#32500;&#21160;&#24577;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#65288;HOI&#65289;&#29983;&#25104;&#38754;&#20020;&#30528;&#26174;&#33879;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#20132;&#20114;&#25968;&#25454;&#21644;&#19982;&#36825;&#20123;&#20132;&#20114;&#19968;&#33268;&#30340;&#20840;&#38754;&#25551;&#36848;&#12290;&#26412;&#25991;&#37319;&#21462;&#20102;&#34892;&#21160;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#30452;&#25509;&#35757;&#32451;&#25991;&#26412;-&#20132;&#20114;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#22312;&#23454;&#29616;&#36825;&#19968;&#28857;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#20132;&#20114;&#35821;&#20041;&#21644;&#21160;&#24577;&#21487;&#20197;&#35299;&#32806;&#12290;&#26080;&#27861;&#36890;&#36807;&#30417;&#30563;&#35757;&#32451;&#23398;&#20064;&#20132;&#20114;&#35821;&#20041;&#65292;&#25105;&#20204;&#36716;&#32780;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#23558;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#36816;&#21160;&#27169;&#22411;&#30340;&#30693;&#35782;&#30456;&#36741;&#30456;&#25104;&#12290;&#23613;&#31649;&#36825;&#26679;&#30340;&#30693;&#35782;&#25552;&#20379;&#20102;&#23545;&#20132;&#20114;&#35821;&#20041;&#30340;&#39640;&#32423;&#25511;&#21046;&#65292;&#20294;&#19981;&#33021;&#25552;&#20379;&#21040;&#19981;&#25104;&#23545;&#20132;&#20114;&#25991;&#26412;&#30340;&#30452;&#25509;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19652v1 Announce Type: cross  Abstract: Text-conditioned human motion generation has experienced significant advancements with diffusion models trained on extensive motion capture data and corresponding textual annotations. However, extending such success to 3D dynamic human-object interaction (HOI) generation faces notable challenges, primarily due to the lack of large-scale interaction data and comprehensive descriptions that align with these interactions. This paper takes the initiative and showcases the potential of generating human-object interactions without direct training on text-interaction pair data. Our key insight in achieving this is that interaction semantics and dynamics can be decoupled. Being unable to learn interaction semantics through supervised training, we instead leverage pre-trained large models, synergizing knowledge from a large language model and a text-to-motion model. While such knowledge offers high-level control over interaction semantics, it c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#35299;&#24352;&#37327;&#32593;&#32476;&#65288;FTN&#65289;&#65292;&#23427;&#21487;&#20197;&#20811;&#26381;&#22810;&#20219;&#21153;&#22810;&#39046;&#22495;&#23398;&#20064;&#20013;&#30340;&#20849;&#20139;&#20449;&#24687;&#21033;&#29992;&#25361;&#25112;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#12289;&#23384;&#20648;&#25104;&#26412;&#12289;&#35745;&#31639;&#37327;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#31561;&#26041;&#38754;&#23454;&#29616;&#39640;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FTN&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#24212;&#22823;&#37327;&#30340;&#30446;&#26631;&#39046;&#22495;&#21644;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.06124</link><description>&lt;p&gt;
&#20998;&#35299;&#24352;&#37327;&#32593;&#32476;&#29992;&#20110;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Factorized Tensor Networks for Multi-Task and Multi-Domain Learning. (arXiv:2310.06124v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#35299;&#24352;&#37327;&#32593;&#32476;&#65288;FTN&#65289;&#65292;&#23427;&#21487;&#20197;&#20811;&#26381;&#22810;&#20219;&#21153;&#22810;&#39046;&#22495;&#23398;&#20064;&#20013;&#30340;&#20849;&#20139;&#20449;&#24687;&#21033;&#29992;&#25361;&#25112;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#12289;&#23384;&#20648;&#25104;&#26412;&#12289;&#35745;&#31639;&#37327;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#31561;&#26041;&#38754;&#23454;&#29616;&#39640;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FTN&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#24212;&#22823;&#37327;&#30340;&#30446;&#26631;&#39046;&#22495;&#21644;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#20351;&#29992;&#21333;&#20010;&#32479;&#19968;&#30340;&#32593;&#32476;&#20849;&#21516;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;/&#39046;&#22495;&#65292;&#25110;&#32773;&#20808;&#21518;&#23398;&#20064;&#23427;&#20204;&#12290;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#20250;&#26159;&#21033;&#29992;&#20219;&#21153;&#21644;&#39046;&#22495;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#25552;&#39640;&#32479;&#19968;&#32593;&#32476;&#30340;&#25928;&#29575;&#65292;&#21253;&#25324;&#20934;&#30830;&#24615;&#12289;&#23384;&#20648;&#25104;&#26412;&#12289;&#35745;&#31639;&#37327;&#25110;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#35299;&#24352;&#37327;&#32593;&#32476;&#65288;FTN&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#23569;&#37327;&#38468;&#21152;&#21442;&#25968;&#23454;&#29616;&#19982;&#29420;&#31435;&#21333;&#20219;&#21153;/&#39046;&#22495;&#32593;&#32476;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;FTN&#20351;&#29992;&#28304;&#27169;&#22411;&#30340;&#20923;&#32467;&#20027;&#24178;&#32593;&#32476;&#65292;&#24182;&#36880;&#27493;&#28155;&#21152;&#20219;&#21153;/&#39046;&#22495;&#29305;&#23450;&#30340;&#20302;&#31209;&#24352;&#37327;&#22240;&#23376;&#21040;&#20849;&#20139;&#30340;&#20923;&#32467;&#32593;&#32476;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36866;&#24212;&#22823;&#37327;&#30446;&#26631;&#39046;&#22495;&#21644;&#20219;&#21153;&#65292;&#32780;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;FTN&#38656;&#35201;&#36739;&#23569;&#30340;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22810;&#39046;&#22495;&#21644;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task and multi-domain learning methods seek to learn multiple tasks/domains, jointly or one after another, using a single unified network. The key challenge and opportunity is to exploit shared information across tasks and domains to improve the efficiency of the unified network. The efficiency can be in terms of accuracy, storage cost, computation, or sample complexity. In this paper, we propose a factorized tensor network (FTN) that can achieve accuracy comparable to independent single-task/domain networks with a small number of additional parameters. FTN uses a frozen backbone network from a source model and incrementally adds task/domain-specific low-rank tensor factors to the shared frozen network. This approach can adapt to a large number of target domains and tasks without catastrophic forgetting. Furthermore, FTN requires a significantly smaller number of task-specific parameters compared to existing methods. We performed experiments on widely used multi-domain and multi-
&lt;/p&gt;</description></item></channel></rss>