<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20165;&#36816;&#34892;&#26102;&#35299;&#30721;&#22120;&#30340;&#36229;&#32593;&#32476;&#65292;&#19981;&#20381;&#36182;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36328;&#23618;&#21442;&#25968;&#20887;&#20313;&#12290;</title><link>https://arxiv.org/abs/2403.19163</link><description>&lt;p&gt;
D'OH: &#20165;&#35299;&#30721;&#22120;&#38543;&#26426;&#36229;&#32593;&#32476;&#29992;&#20110;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
D'OH: Decoder-Only random Hypernetworks for Implicit Neural Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20165;&#36816;&#34892;&#26102;&#35299;&#30721;&#22120;&#30340;&#36229;&#32593;&#32476;&#65292;&#19981;&#20381;&#36182;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36328;&#23618;&#21442;&#25968;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38544;&#24335;&#20989;&#25968;&#34987;&#21457;&#29616;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#32534;&#30721;&#21508;&#31181;&#33258;&#28982;&#20449;&#21495;&#12290;&#23427;&#20204;&#30340;&#21560;&#24341;&#21147;&#22312;&#20110;&#33021;&#22815;&#32039;&#20945;&#22320;&#34920;&#31034;&#20449;&#21495;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#31163;&#32447;&#35757;&#32451;&#25968;&#25454;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#30340;&#38544;&#24335;&#20559;&#24046;&#26469;&#35299;&#32806;&#20449;&#21495;&#20013;&#30340;&#38544;&#34255;&#20887;&#20313;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#26679;&#19968;&#20010;&#20551;&#35774;&#65306;&#36890;&#36807;&#21033;&#29992;&#23618;&#20043;&#38388;&#23384;&#22312;&#30340;&#20887;&#20313;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#21387;&#32553;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20165;&#36816;&#34892;&#26102;&#35299;&#30721;&#22120;&#30340;&#36229;&#32593;&#32476; - &#23427;&#19981;&#20351;&#29992;&#31163;&#32447;&#35757;&#32451;&#25968;&#25454; - &#26469;&#26356;&#22909;&#22320;&#24314;&#27169;&#36328;&#23618;&#21442;&#25968;&#20887;&#20313;&#12290;&#20808;&#21069;&#22312;&#28145;&#24230;&#38544;&#24335;&#20989;&#25968;&#20013;&#24212;&#29992;&#36229;&#32593;&#32476;&#30340;&#24212;&#29992;&#37117;&#37319;&#29992;&#20102;&#20381;&#36182;&#22823;&#37327;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#21069;&#39304;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26080;&#27861;&#27867;&#21270;&#21040;&#35757;&#32451;&#20449;&#21495;&#20043;&#22806;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#21021;&#22987;&#21270;&#36816;&#34892;&#26102;&#28145;&#24230;&#38544;&#24335;&#20989;&#25968;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19163v1 Announce Type: new  Abstract: Deep implicit functions have been found to be an effective tool for efficiently encoding all manner of natural signals. Their attractiveness stems from their ability to compactly represent signals with little to no off-line training data. Instead, they leverage the implicit bias of deep networks to decouple hidden redundancies within the signal. In this paper, we explore the hypothesis that additional compression can be achieved by leveraging the redundancies that exist between layers. We propose to use a novel run-time decoder-only hypernetwork - that uses no offline training data - to better model this cross-layer parameter redundancy. Previous applications of hyper-networks with deep implicit functions have applied feed-forward encoder/decoder frameworks that rely on large offline datasets that do not generalize beyond the signals they were trained on. We instead present a strategy for the initialization of run-time deep implicit func
&lt;/p&gt;</description></item><item><title>LS&#26041;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#24179;&#28369;&#25928;&#26524;&#34987;&#21457;&#29616;&#20250;&#36127;&#38754;&#24433;&#21709;&#36873;&#25321;&#24615;&#20998;&#31867;&#65292;&#36890;&#36807;&#24433;&#21709;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#27492;&#30740;&#31350;&#38416;&#26126;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.14715</link><description>&lt;p&gt;
&#29702;&#35299;&#20026;&#20309;&#26631;&#31614;&#24179;&#28369;&#20250;&#38477;&#20302;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#25928;&#26524;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14715
&lt;/p&gt;
&lt;p&gt;
LS&#26041;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#24179;&#28369;&#25928;&#26524;&#34987;&#21457;&#29616;&#20250;&#36127;&#38754;&#24433;&#21709;&#36873;&#25321;&#24615;&#20998;&#31867;&#65292;&#36890;&#36807;&#24433;&#21709;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#27492;&#30740;&#31350;&#38416;&#26126;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#65288;LS&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#22312;&#25552;&#39640;&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#65292;&#24182;&#19988;&#23454;&#29616;&#31616;&#21333;&#12290;"&#30828;"&#30340;one-hot&#26631;&#31614;&#36890;&#36807;&#23558;&#27010;&#29575;&#36136;&#37327;&#22343;&#21248;&#20998;&#37197;&#32473;&#20854;&#20182;&#31867;&#21035;&#26469;&#36827;&#34892;"&#24179;&#28369;&#21270;"&#65292;&#20174;&#32780;&#20943;&#23569;&#36807;&#24230;&#25311;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LS&#22914;&#20309;&#36127;&#38754;&#24433;&#21709;&#36873;&#25321;&#24615;&#20998;&#31867;&#65288;SC&#65289;- &#20854;&#30446;&#26631;&#26159;&#21033;&#29992;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26469;&#25298;&#32477;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#21644;&#26550;&#26500;&#20013;&#20174;&#32463;&#39564;&#19978;&#35777;&#26126;LS&#20250;&#23548;&#33268;SC&#30340;&#19968;&#33268;&#24615;&#38477;&#32423;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;logit&#32423;&#21035;&#30340;&#26799;&#24230;&#26469;&#35299;&#37322;&#36825;&#19968;&#28857;&#65292;&#34920;&#26126;LS&#36890;&#36807;&#22312;&#38169;&#35823;&#27010;&#29575;&#20302;&#26102;&#26356;&#21152;&#27491;&#21017;&#21270;&#26368;&#22823;logit&#65292;&#32780;&#22312;&#38169;&#35823;&#27010;&#29575;&#39640;&#26102;&#26356;&#23569;&#27491;&#21017;&#21270;&#65292;&#21152;&#21095;&#20102;&#36807;&#24230;&#33258;&#20449;&#21644;&#20302;&#33258;&#20449;&#12290;&#36825;&#38416;&#26126;&#20102;&#20197;&#21069;&#25253;&#36947;&#30340;&#24378;&#20998;&#31867;&#22120;&#22312;SC&#20013;&#24615;&#33021;&#19981;&#20339;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14715v1 Announce Type: cross  Abstract: Label smoothing (LS) is a popular regularisation method for training deep neural network classifiers due to its effectiveness in improving test accuracy and its simplicity in implementation. "Hard" one-hot labels are "smoothed" by uniformly distributing probability mass to other classes, reducing overfitting. In this work, we reveal that LS negatively affects selective classification (SC) - where the aim is to reject misclassifications using a model's predictive uncertainty. We first demonstrate empirically across a range of tasks and architectures that LS leads to a consistent degradation in SC. We then explain this by analysing logit-level gradients, showing that LS exacerbates overconfidence and underconfidence by regularising the max logit more when the probability of error is low, and less when the probability of error is high. This elucidates previously reported experimental results where strong classifiers underperform in SC. We
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;Segment Anything Model (SAM)&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23450;&#21046;&#21270;&#23454;&#20363;&#20998;&#21106;&#65292;&#35299;&#20915;&#20102;&#22312;&#24212;&#29992;&#20110;&#23450;&#21046;&#21270;&#23454;&#20363;&#20998;&#21106;&#26102;&#38754;&#20020;&#30340;&#36755;&#20837;&#25552;&#31034;&#27169;&#31946;&#24615;&#21644;&#39069;&#22806;&#35757;&#32451;&#38656;&#27714;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.09199</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23450;&#21046;&#21270;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Customizing Segmentation Foundation Model via Prompt Learning for Instance Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09199
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;Segment Anything Model (SAM)&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23450;&#21046;&#21270;&#23454;&#20363;&#20998;&#21106;&#65292;&#35299;&#20915;&#20102;&#22312;&#24212;&#29992;&#20110;&#23450;&#21046;&#21270;&#23454;&#20363;&#20998;&#21106;&#26102;&#38754;&#20020;&#30340;&#36755;&#20837;&#25552;&#31034;&#27169;&#31946;&#24615;&#21644;&#39069;&#22806;&#35757;&#32451;&#38656;&#27714;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#21560;&#24341;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24471;&#21040;&#31215;&#26497;&#25506;&#35752;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;Segment Anything Model (SAM)&#22240;&#20854;&#22312;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#32780;&#33073;&#39062;&#32780;&#20986;&#65292;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#23545;&#35937;&#25513;&#27169;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;SAM&#20855;&#26377;&#36825;&#20123;&#20248;&#21183;&#65292;&#22312;&#24212;&#29992;&#20110;&#23450;&#21046;&#21270;&#23454;&#20363;&#20998;&#21106;&#26102;&#65288;&#23545;&#29305;&#23450;&#23545;&#35937;&#25110;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#36890;&#24120;&#19981;&#23384;&#22312;&#30340;&#29420;&#29305;&#29615;&#22659;&#20013;&#36827;&#34892;&#20998;&#21106;&#65289;&#65292;SAM&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;1&#65289;&#36755;&#20837;&#25552;&#31034;&#20013;&#22266;&#26377;&#30340;&#27169;&#31946;&#24615;&#65292;2&#65289;&#20026;&#23454;&#29616;&#26368;&#20339;&#20998;&#21106;&#38656;&#35201;&#22823;&#37327;&#39069;&#22806;&#35757;&#32451;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23450;&#21046;&#21270;&#23454;&#20363;&#20998;&#21106;&#65292;&#38024;&#23545;SAM&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#21547;&#19968;&#20010;&#25552;&#31034;&#23398;&#20064;&#27169;&#22359;&#65288;PLM&#65289;&#65292;&#21487;&#20197;&#35843;&#25972;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09199v1 Announce Type: cross  Abstract: Recently, foundation models trained on massive datasets to adapt to a wide range of domains have attracted considerable attention and are actively being explored within the computer vision community. Among these, the Segment Anything Model (SAM) stands out for its remarkable progress in generalizability and flexibility for image segmentation tasks, achieved through prompt-based object mask generation. However, despite its strength, SAM faces two key limitations when applied to customized instance segmentation that segments specific objects or those in unique environments not typically present in the training data: 1) the ambiguity inherent in input prompts and 2) the necessity for extensive additional training to achieve optimal segmentation. To address these challenges, we propose a novel method, customized instance segmentation via prompt learning tailored to SAM. Our method involves a prompt learning module (PLM), which adjusts inpu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20020;&#24202;&#21644;RT-PCR&#25968;&#25454;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26469;&#39044;&#27979;COVID-19&#24739;&#32773;&#24247;&#22797;&#25110;&#27515;&#20129;&#39118;&#38505;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.13925</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#20915;&#31574;&#26862;&#26519;&#65306;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;COVID-19&#24739;&#32773;&#24247;&#22797;&#25110;&#27515;&#20129;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#20020;&#24202;&#21644;RT-PCR&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Decision Forest: A Novel Approach for Predicting Recovery or Decease of COVID-19 Patients with Clinical and RT-PCR. (arXiv:2311.13925v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20020;&#24202;&#21644;RT-PCR&#25968;&#25454;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26469;&#39044;&#27979;COVID-19&#24739;&#32773;&#24247;&#22797;&#25110;&#27515;&#20129;&#39118;&#38505;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#23459;&#24067;&#22823;&#27969;&#34892;&#24050;&#32463;&#32467;&#26463;&#65292;&#20294;COVID-19&#20173;&#28982;&#34987;&#35270;&#20026;&#19968;&#31181;&#22320;&#26041;&#24615;&#30142;&#30149;&#12290;&#36825;&#27425;&#22823;&#27969;&#34892;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26041;&#24335;&#25171;&#20081;&#20102;&#20154;&#20204;&#30340;&#29983;&#27963;&#24182;&#23548;&#33268;&#24191;&#27867;&#30340;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#12290;&#22240;&#27492;&#65292;&#32039;&#24613;&#21307;&#29983;&#26377;&#24517;&#35201;&#30830;&#23450;&#39640;&#39118;&#38505;&#27515;&#20129;&#24739;&#32773;&#65292;&#20197;&#20415;&#20248;&#20808;&#32771;&#34385;&#21307;&#38498;&#35774;&#22791;&#30340;&#20998;&#37197;&#65292;&#23588;&#20854;&#26159;&#22312;&#21307;&#30103;&#36164;&#28304;&#26377;&#38480;&#30340;&#22320;&#21306;&#12290;&#23613;&#31649;&#23384;&#22312;&#21738;&#31181;&#25968;&#25454;&#26368;&#20934;&#30830;&#30340;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#20294;&#24739;&#32773;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#23545;&#20110;&#39044;&#27979;COVID-19&#30149;&#20363;&#30340;&#32467;&#26524;&#26159;&#26377;&#30410;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24819;&#35201;&#26816;&#26597;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#33021;&#22815;&#39044;&#27979;&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20020;&#24202;&#21644;RT-PCR&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#20197;&#30830;&#23450;&#21738;&#20010;&#26356;&#21487;&#38752;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#22235;&#20010;&#19981;&#21516;&#29305;&#24449;&#38598;&#30340;&#38454;&#27573;&#65292;&#24182;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26500;&#24314;&#20102;&#30456;&#24212;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
COVID-19 continues to be considered an endemic disease in spite of the World Health Organization's declaration that the pandemic is over. This pandemic has disrupted people's lives in unprecedented ways and caused widespread morbidity and mortality. As a result, it is important for emergency physicians to identify patients with a higher mortality risk in order to prioritize hospital equipment, especially in areas with limited medical services. The collected data from patients is beneficial to predict the outcome of COVID-19 cases, although there is a question about which data makes the most accurate predictions. Therefore, this study aims to accomplish two main objectives. First, we want to examine whether deep learning algorithms can predict a patient's morality. Second, we investigated the impact of Clinical and RT-PCR on prediction to determine which one is more reliable. We defined four stages with different feature sets and used interpretable deep learning methods to build appropr
&lt;/p&gt;</description></item><item><title>PostRainBench&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#38477;&#27700;&#39044;&#27979;&#22522;&#20934;&#65292;&#32467;&#21512;AI&#21518;&#22788;&#29702;&#25216;&#26415;&#21644;&#20256;&#32479;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#20934;&#30830;&#24615;&#24182;&#35299;&#20915;&#22797;&#26434;&#30340;&#38477;&#27700;&#39044;&#27979;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.02676</link><description>&lt;p&gt;
PostRainBench: &#19968;&#31181;&#20840;&#38754;&#30340;&#38477;&#27700;&#39044;&#27979;&#22522;&#20934;&#21644;&#26032;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PostRainBench: A comprehensive benchmark and a new model for precipitation forecasting. (arXiv:2310.02676v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02676
&lt;/p&gt;
&lt;p&gt;
PostRainBench&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#38477;&#27700;&#39044;&#27979;&#22522;&#20934;&#65292;&#32467;&#21512;AI&#21518;&#22788;&#29702;&#25216;&#26415;&#21644;&#20256;&#32479;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#20934;&#30830;&#24615;&#24182;&#35299;&#20915;&#22797;&#26434;&#30340;&#38477;&#27700;&#39044;&#27979;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#38477;&#27700;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#31185;&#23398;&#21644;&#31038;&#20250;&#37325;&#35201;&#24615;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#24191;&#27867;&#37319;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#36182;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#27169;&#25311;&#22522;&#30784;&#29289;&#29702;&#36807;&#31243;&#26041;&#38754;&#26377;&#38480;&#65292;&#20351;&#24471;&#20934;&#30830;&#39044;&#27979;&#22256;&#38590;&#12290;&#23558;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#19982;&#20256;&#32479;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20026;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#20043;&#21069;&#36827;&#34892;&#36807;&#21518;&#22788;&#29702;&#30340;&#23581;&#35797;&#65292;&#20294;&#30001;&#20110;&#19981;&#21516;&#20301;&#32622;&#30340;&#38477;&#27700;&#25968;&#25454;&#22833;&#34913;&#21644;&#22810;&#20010;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20934;&#30830;&#39044;&#27979;&#22823;&#38632;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PostRainBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#21464;&#37327;NWP&#21518;&#22788;&#29702;&#22522;&#20934;&#65292;&#21253;&#25324;&#19977;&#20010;&#29992;&#20110;NWP&#21518;&#22788;&#29702;&#38477;&#27700;&#39044;&#27979;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#28192;&#36947;&#27880;&#24847;&#21147;&#27169;&#22411;CAMT&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate precipitation forecasting is a vital challenge of both scientific and societal importance. Data-driven approaches have emerged as a widely used solution for addressing this challenge. However, solely relying on data-driven approaches has limitations in modeling the underlying physics, making accurate predictions difficult. Coupling AI-based post-processing techniques with traditional Numerical Weather Prediction (NWP) methods offers a more effective solution for improving forecasting accuracy. Despite previous post-processing efforts, accurately predicting heavy rainfall remains challenging due to the imbalanced precipitation data across locations and complex relationships between multiple meteorological variables. To address these limitations, we introduce the PostRainBench, a comprehensive multi-variable NWP post-processing benchmark consisting of three datasets for NWP post-processing-based precipitation forecasting. We propose CAMT, a simple yet effective Channel Attention
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDAN&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65292;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#12290;&#35813;&#32593;&#32476;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#12289;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12902</link><description>&lt;p&gt;
CDAN: &#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement. (arXiv:2308.12902v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDAN&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65292;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#12290;&#35813;&#32593;&#32476;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#12289;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#22270;&#20687;&#20197;&#19981;&#36275;&#30340;&#29031;&#26126;&#20026;&#29305;&#24449;&#65292;&#38754;&#20020;&#28165;&#26224;&#24230;&#20943;&#24369;&#12289;&#39068;&#33394;&#26263;&#28129;&#21644;&#32454;&#33410;&#20943;&#23569;&#30340;&#25361;&#25112;&#12290;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#25913;&#21892;&#20142;&#24230;&#12289;&#23545;&#27604;&#24230;&#21644;&#25972;&#20307;&#24863;&#30693;&#36136;&#37327;&#26469;&#32416;&#27491;&#36825;&#20123;&#38382;&#39064;&#65292;&#20174;&#32780;&#20419;&#36827;&#20934;&#30830;&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65288;CDAN&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#20302;&#20809;&#22270;&#20687;&#12290;CDAN&#23558;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#19982;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#30456;&#32467;&#21512;&#65292;&#37197;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#12290;&#35813;&#26550;&#26500;&#30830;&#20445;&#20102;&#26377;&#25928;&#30340;&#20449;&#24687;&#20256;&#36882;&#21644;&#29305;&#24449;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#39046;&#22495;&#30340;&#26368;&#26032;&#25104;&#26524;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-light images, characterized by inadequate illumination, pose challenges of diminished clarity, muted colors, and reduced details. Low-light image enhancement, an essential task in computer vision, aims to rectify these issues by improving brightness, contrast, and overall perceptual quality, thereby facilitating accurate analysis and interpretation. This paper introduces the Convolutional Dense Attention-guided Network (CDAN), a novel solution for enhancing low-light images. CDAN integrates an autoencoder-based architecture with convolutional and dense blocks, complemented by an attention mechanism and skip connections. This architecture ensures efficient information propagation and feature learning. Furthermore, a dedicated post-processing phase refines color balance and contrast. Our approach demonstrates notable progress compared to state-of-the-art results in low-light image enhancement, showcasing its robustness across a wide range of challenging scenarios. Our model performs 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.10947</link><description>&lt;p&gt;
&#20851;&#20110;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#36777;&#25252;
&lt;/p&gt;
&lt;p&gt;
In Defense of Pure 16-bit Floating-Point Neural Networks. (arXiv:2305.10947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#25152;&#38656;&#30340;&#20301;&#25968;&#26159;&#38750;&#24120;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21152;&#24555;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20197;&#24320;&#21457;&#21033;&#29992;&#26356;&#20302;&#31934;&#24230;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#27604;&#22914;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#32431;16&#20301;&#28014;&#28857;&#35774;&#32622;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;&#36896;&#25104;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#28014;&#28857;&#35823;&#24046;&#21644;&#23481;&#24525;&#24230;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#23494;&#20999;&#36924;&#36817;&#32467;&#26524;&#30340;&#26465;&#20214;&#12290;&#36825;&#31181;&#29702;&#35770;&#25506;&#32034;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reducing the number of bits needed to encode the weights and activations of neural networks is highly desirable as it speeds up their training and inference time while reducing memory consumption. For these reasons, research in this area has attracted significant attention toward developing neural networks that leverage lower-precision computing, such as mixed-precision training. Interestingly, none of the existing approaches has investigated pure 16-bit floating-point settings. In this paper, we shed light on the overlooked efficiency of pure 16-bit floating-point neural networks. As such, we provide a comprehensive theoretical analysis to investigate the factors contributing to the differences observed between 16-bit and 32-bit models. We formalize the concepts of floating-point error and tolerance, enabling us to quantitatively explain the conditions under which a 16-bit model can closely approximate the results of its 32-bit counterpart. This theoretical exploration offers perspect
&lt;/p&gt;</description></item></channel></rss>