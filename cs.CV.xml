<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25972;&#21512;&#27491;&#24358;&#20989;&#25968;&#21040;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19243</link><description>&lt;p&gt;
&#29992;&#27491;&#24358;&#28608;&#27963;&#30340;&#20302;&#31209;&#30697;&#38453;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sine Activated Low-Rank Matrices for Parameter Efficient Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19243
&lt;/p&gt;
&lt;p&gt;
&#25972;&#21512;&#27491;&#24358;&#20989;&#25968;&#21040;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#20998;&#35299;&#24050;&#32463;&#25104;&#20026;&#22312;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#22686;&#24378;&#21442;&#25968;&#25928;&#29575;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#20123;&#25216;&#26415;&#26174;&#33879;&#38477;&#20302;&#20102;&#21442;&#25968;&#25968;&#37327;&#65292;&#21462;&#24471;&#20102;&#31616;&#27905;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#26159;&#22312;&#21442;&#25968;&#25928;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#20570;&#20986;&#22949;&#21327;&#65292;&#21442;&#25968;&#20943;&#23569;&#24448;&#24448;&#23548;&#33268;&#20934;&#30830;&#24615;&#19981;&#21450;&#23436;&#25972;&#31209;&#23545;&#24212;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#22312;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#25972;&#21512;&#20102;&#19968;&#20010;&#27491;&#24358;&#20989;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20445;&#30041;&#20102;&#20302;&#31209;&#26041;&#27861;&#30340;&#21442;&#25968;&#25928;&#29575;&#29305;&#24615;&#30340;&#22909;&#22788;&#65292;&#36824;&#22686;&#21152;&#20102;&#20998;&#35299;&#30340;&#31209;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#29616;&#26377;&#20302;&#31209;&#27169;&#22411;&#30340;&#19968;&#31181;&#36866;&#24212;&#24615;&#22686;&#24378;&#65292;&#27491;&#22914;&#20854;&#25104;&#21151;&#35777;&#23454;&#30340;&#37027;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19243v1 Announce Type: new  Abstract: Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and performance. However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts. In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process. This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition's rank, thereby enhancing model accuracy. Our method proves to be an adaptable enhancement for existing low-rank models, as evidenced by its successful 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.06869</link><description>&lt;p&gt;
&#22312;&#26377;&#22122;&#22768;&#22522;&#30784;&#27169;&#22411;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#35843;&#25972;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#24448;&#24448;&#26080;&#27861;&#33719;&#21462;&#25110;&#25104;&#26412;&#36807;&#39640;&#65292;&#21487;&#33021;&#21253;&#21547;&#26631;&#31614;&#22122;&#22768;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36896;&#25104;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#24102;&#26469;&#24847;&#24819;&#19981;&#21040;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#26159;&#39318;&#20010;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22122;&#22768;&#24615;&#36136;&#65292;&#24182;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#26377;&#22122;&#22768;&#30340;ImageNet-1K&#12289;YFCC15M&#21644;CC12M&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23436;&#20840;&#30417;&#30563;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#23613;&#31649;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#20351;&#21516;&#39046;&#22495;&#65288;ID&#65289;&#24615;&#33021;&#21463;&#30410;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20849;&#20139;&#31867;&#20284;&#20998;&#24067;&#65292;&#20294;&#23427;&#24635;&#26159;&#20250;&#30772;&#22351;&#36328;&#39046;&#22495;&#65288;OOD&#65289;&#24615;&#33021;&#65292;&#22312;&#37027;&#37324;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#26126;&#26174;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06869v1 Announce Type: cross  Abstract: Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are signific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Valley&#30340;&#35270;&#39057;&#21161;&#25163;&#65292;&#23427;&#26159;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#29702;&#35299;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2306.07207</link><description>&lt;p&gt;
Valley: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#35270;&#39057;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Valley: Video Assistant with Large Language model Enhanced abilitY. (arXiv:2306.07207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Valley&#30340;&#35270;&#39057;&#21161;&#25163;&#65292;&#23427;&#26159;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#29702;&#35299;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20197;&#20854;&#21331;&#36234;&#30340;&#20250;&#35805;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#25104;&#20026;&#24378;&#22823;&#30340;AI&#21161;&#25163;&#12290;&#37492;&#20110;&#27492;&#65292;&#19968;&#20010;&#30452;&#35266;&#30340;&#38382;&#39064;&#26159;&#65306;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#26500;&#24314;&#22810;&#27169;&#24577;&#30340;&#35270;&#35273;&#24212;&#29992;AI&#21161;&#25163;&#65311;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#12290;&#23427;&#20204;&#36890;&#24120;&#39044;&#20808;&#35757;&#32451;&#19968;&#20010;&#36866;&#24212;&#27169;&#22359;&#26469;&#23545;&#40784;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#65292;&#28982;&#21518;&#22312;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20010;&#27969;&#31243;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#22312;&#35270;&#39057;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#36824;&#27809;&#26377;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#22312;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20869;&#29702;&#35299;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#35821;&#35328;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Valley&#65292;&#19968;&#20010;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#35270;&#39057;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), with their remarkable conversational capabilities, have demonstrated impressive performance across various applications and have emerged as formidable AI assistants. In view of this, it raises an intuitive question: Can we harness the power of LLMs to build multimodal AI assistants for visual applications? Recently, several multi-modal models have been developed for this purpose. They typically pre-train an adaptation module to align the semantics of the vision encoder and language model, followed by fine-tuning on instruction-following data. However, despite the success of this pipeline in image and language understanding, its effectiveness in joint video and language understanding has not been widely explored. In this paper, we aim to develop a novel multi-modal foundation model capable of comprehending video, image, and language within a general framework. To achieve this goal, we introduce Valley, a Video Assistant with Large Language model Enhanced ab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RViDeformer&#21407;&#22987;&#35270;&#39057;&#21435;&#22122;&#21464;&#25442;&#22120;&#21450;&#20854;&#37197;&#22871;&#25968;&#25454;&#38598;ReCRVD&#65292;&#20854;&#20013;&#21033;&#29992;&#39640;&#20302;ISO&#35774;&#32622;&#37325;&#26032;&#25429;&#25417;&#29616;&#26377;&#35270;&#39057;&#20197;&#26500;&#24314;&#22122;&#22768;-&#28165;&#26224;&#23545;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#38750;&#26412;&#22320;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.00767</link><description>&lt;p&gt;
RViDeformer&#65306;&#20855;&#26377;&#26356;&#22823;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#39640;&#25928;&#21407;&#22987;&#35270;&#39057;&#21435;&#22122;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
RViDeformer: Efficient Raw Video Denoising Transformer with a Larger Benchmark Dataset. (arXiv:2305.00767v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RViDeformer&#21407;&#22987;&#35270;&#39057;&#21435;&#22122;&#21464;&#25442;&#22120;&#21450;&#20854;&#37197;&#22871;&#25968;&#25454;&#38598;ReCRVD&#65292;&#20854;&#20013;&#21033;&#29992;&#39640;&#20302;ISO&#35774;&#32622;&#37325;&#26032;&#25429;&#25417;&#29616;&#26377;&#35270;&#39057;&#20197;&#26500;&#24314;&#22122;&#22768;-&#28165;&#26224;&#23545;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#38750;&#26412;&#22320;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#19982;&#25104;&#20687;&#36807;&#31243;&#30340;&#19968;&#33268;&#24615;&#21644;&#21407;&#22987;&#39046;&#22495;&#20013;&#25104;&#29087;&#30340;&#22122;&#22768;&#24314;&#27169;&#65292;&#21407;&#22987;&#35270;&#39057;&#21435;&#22122;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20173;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#38459;&#30861;&#20102;&#21435;&#22122;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#23545;&#20110;&#21463;&#25511;&#30340;&#21407;&#22987;&#35270;&#39057;&#21435;&#22122;&#26469;&#35828;&#65292;&#27809;&#26377;&#20855;&#26377;&#30495;&#23454;&#36816;&#21160;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#22240;&#20026;&#20026;&#30495;&#23454;&#21160;&#24577;&#22330;&#26223;&#25429;&#25417;&#22122;&#22768;&#21644;&#28165;&#26224;&#24103;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#37325;&#26032;&#25429;&#25417;&#20197;&#39640;&#20302;ISO&#35774;&#32622;&#26174;&#31034;&#30340;&#29616;&#26377;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#20197;&#26500;&#24314;&#22122;&#22768;-&#28165;&#26224;&#37197;&#23545;&#24103;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#26500;&#24314;&#19968;&#20010;&#35270;&#39057;&#21435;&#22122;&#25968;&#25454;&#38598;&#65288;&#21629;&#21517;&#20026;ReCRVD&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;120&#32452;&#22122;&#22768;-&#28165;&#26224;&#35270;&#39057;&#65292;&#20854;ISO&#20540;&#20174;1600&#21040;25600&#19981;&#31561;&#12290;&#20854;&#27425;&#65292;&#34429;&#28982;&#38750;&#26412;&#22320;&#26102;&#31354;&#20851;&#27880;&#23545;&#20110;&#21435;&#22122;&#26377;&#30410;&#65292;&#20294;&#23427;&#36890;&#24120;&#23548;&#33268;&#27785;&#37325;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21407;&#22987;&#35270;&#39057;&#21435;&#22122;&#21464;&#25442;&#22120;&#32593;&#32476;&#65288;RViDeformer&#65289;&#65292;&#23427;&#25506;&#32034;&#20102;&#30701;&#36317;&#31163;&#21644;&#38271;&#36317;&#31163;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31354;&#38388;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#26412;&#22320;&#35270;&#35273;&#29305;&#24449;&#20197;&#20943;&#23569;&#31354;&#38388;&#20887;&#20313;&#24182;&#21152;&#36895;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#38271;&#31243;&#22122;&#22768;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#21464;&#25442;&#22120;&#32593;&#32476;&#65292;&#21516;&#26102;&#27169;&#22411;&#21270;&#38750;&#26412;&#22320;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, raw video denoising has garnered increased attention due to the consistency with the imaging process and well-studied noise modeling in the raw domain. However, two problems still hinder the denoising performance. Firstly, there is no large dataset with realistic motions for supervised raw video denoising, as capturing noisy and clean frames for real dynamic scenes is difficult. To address this, we propose recapturing existing high-resolution videos displayed on a 4K screen with high-low ISO settings to construct noisy-clean paired frames. In this way, we construct a video denoising dataset (named as ReCRVD) with 120 groups of noisy-clean videos, whose ISO values ranging from 1600 to 25600. Secondly, while non-local temporal-spatial attention is beneficial for denoising, it often leads to heavy computation costs. We propose an efficient raw video denoising transformer network (RViDeformer) that explores both short and long-distance correlations. Specifically, we propos
&lt;/p&gt;</description></item></channel></rss>