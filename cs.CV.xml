<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#25163;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#20102;&#26377;&#25928;&#23398;&#20064;&#65292;&#21253;&#25324;&#20219;&#21153;&#20998;&#35299;&#12289;&#32039;&#23494;&#32806;&#21512;&#30340;&#23039;&#21183;&#34920;&#31034;&#21644;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.17827</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#21512;&#25104;&#25163;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17827
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#25163;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#20102;&#26377;&#25928;&#23398;&#20064;&#65292;&#21253;&#25324;&#20219;&#21153;&#20998;&#35299;&#12289;&#32039;&#23494;&#32806;&#21512;&#30340;&#23039;&#21183;&#34920;&#31034;&#21644;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#33258;&#28982;&#30340;3D&#25163;-&#29289;&#20307;&#20132;&#20114;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#26399;&#26395;&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#29289;&#20307;&#21160;&#20316;&#22312;&#29289;&#29702;&#19978;&#26159;&#21512;&#29702;&#30340;&#65292;&#24182;&#19988;&#22312;&#35821;&#20041;&#19978;&#26159;&#26377;&#24847;&#20041;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffH2O&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#25552;&#20379;&#30340;&#25991;&#26412;&#25552;&#31034;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#21333;&#25163;&#25110;&#21452;&#25163;&#29289;&#20307;&#20132;&#20114;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#19977;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#25235;&#21462;&#38454;&#27573;&#21644;&#22522;&#20110;&#25991;&#26412;&#20132;&#20114;&#38454;&#27573;&#65292;&#24182;&#20026;&#27599;&#20010;&#38454;&#27573;&#20351;&#29992;&#21333;&#29420;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#25235;&#21462;&#38454;&#27573;&#20013;&#65292;&#27169;&#22411;&#20165;&#29983;&#25104;&#25163;&#37096;&#21160;&#20316;&#65292;&#32780;&#22312;&#20132;&#20114;&#38454;&#27573;&#20013;&#65292;&#25163;&#37096;&#21644;&#29289;&#20307;&#23039;&#21183;&#37117;&#34987;&#21512;&#25104;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#23494;&#32806;&#21512;&#25163;&#37096;&#21644;&#29289;&#20307;&#23039;&#21183;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17827v1 Announce Type: cross  Abstract: Generating natural hand-object interactions in 3D is challenging as the resulting hand and object motions are expected to be physically plausible and semantically meaningful. Furthermore, generalization to unseen objects is hindered by the limited scale of available hand-object interaction datasets. We propose DiffH2O, a novel method to synthesize realistic, one or two-handed object interactions from provided text prompts and geometry of the object. The method introduces three techniques that enable effective learning from limited data. First, we decompose the task into a grasping stage and a text-based interaction stage and use separate diffusion models for each. In the grasping stage, the model only generates hand motions, whereas in the interaction phase both hand and object poses are synthesized. Second, we propose a compact representation that tightly couples hand and object poses. Third, we propose two different guidance schemes 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#25345;&#32493;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#65288;CVLN&#65289;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#35757;&#32451;VLN&#20195;&#29702;&#26041;&#27861;&#22266;&#26377;&#30340;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#37325;&#22823;&#38480;&#21046;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2403.15049</link><description>&lt;p&gt;
Continual Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
Continual Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15049
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#25345;&#32493;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#65288;CVLN&#65289;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#35757;&#32451;VLN&#20195;&#29702;&#26041;&#27861;&#22266;&#26377;&#30340;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#37325;&#22823;&#38480;&#21046;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20195;&#29702;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#35266;&#23519;&#21040;&#30340;&#35270;&#35273;&#20449;&#24687;&#23548;&#33322;&#21040;&#30446;&#30340;&#22320;&#12290;&#29616;&#26377;&#30340;VLN&#20195;&#29702;&#35757;&#32451;&#26041;&#27861;&#39044;&#35774;&#22266;&#23450;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#19968;&#20010;&#37325;&#22823;&#38480;&#21046;&#65306;&#24341;&#20837;&#26032;&#29615;&#22659;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#20197;&#20445;&#30041;&#24050;&#32463;&#36935;&#21040;&#30340;&#29615;&#22659;&#30340;&#30693;&#35782;&#12290;&#36825;&#20351;&#24471;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#35757;&#32451;VLN&#20195;&#29702;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25345;&#32493;&#35270;&#35273;&#21644;&#35821;&#35328;&#23548;&#33322;&#65288;CVLN&#65289;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#20010;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#35780;&#20272;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15049v1 Announce Type: cross  Abstract: Vision-and-Language Navigation (VLN) agents navigate to a destination using natural language instructions and the visual information they observe. Existing methods for training VLN agents presuppose fixed datasets, leading to a significant limitation: the introduction of new environments necessitates retraining with previously encountered environments to preserve their knowledge. This makes it difficult to train VLN agents that operate in the ever-changing real world. To address this limitation, we present the Continual Vision-and-Language Navigation (CVLN) paradigm, designed to evaluate agents trained through a continual learning process. For the training and evaluation of CVLN agents, we re-arrange existing VLN datasets to propose two datasets: CVLN-I, focused on navigation via initial-instruction interpretation, and CVLN-D, aimed at navigation through dialogue with other agents. Furthermore, we propose two novel rehearsal-based meth
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#24490;&#29615;&#23545;&#40784;&#32593;&#32476;&#65288;RAN&#65289;&#26469;&#26368;&#23567;&#21270;&#39046;&#22495;&#24046;&#24322;&#65292;&#36890;&#36807;&#24490;&#29615;&#23545;&#40784;&#31574;&#30053;&#26377;&#25928;&#22320;&#22312;&#26102;&#38388;-&#29366;&#24577;&#21644;&#26102;&#38388;-&#24207;&#21015;&#32423;&#21035;&#23545;&#40784;&#36712;&#36857;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#24191;&#20041;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.05810</link><description>&lt;p&gt;
&#29992;&#20110;&#24191;&#20041;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#30340;&#24490;&#29615;&#23545;&#40784;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Recurrent Aligned Network for Generalized Pedestrian Trajectory Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05810
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#24490;&#29615;&#23545;&#40784;&#32593;&#32476;&#65288;RAN&#65289;&#26469;&#26368;&#23567;&#21270;&#39046;&#22495;&#24046;&#24322;&#65292;&#36890;&#36807;&#24490;&#29615;&#23545;&#40784;&#31574;&#30053;&#26377;&#25928;&#22320;&#22312;&#26102;&#38388;-&#29366;&#24577;&#21644;&#26102;&#38388;-&#24207;&#21015;&#32423;&#21035;&#23545;&#40784;&#36712;&#36857;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#24191;&#20041;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#26159;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#32780;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#30446;&#26631;&#39046;&#22495;&#30340;&#37096;&#20998;&#36712;&#36857;&#25968;&#25454;&#26469;&#35843;&#25972;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#36825;&#20123;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#20026;&#19981;&#22826;&#21487;&#33021;&#20174;&#25152;&#26377;&#28508;&#22312;&#30340;&#30446;&#26631;&#39046;&#22495;&#25910;&#38598;&#36712;&#36857;&#25968;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#39033;&#21517;&#20026;&#24191;&#20041;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#27169;&#22411;&#25512;&#24191;&#21040;&#30475;&#19981;&#35265;&#30340;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#23427;&#20204;&#30340;&#36712;&#36857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24490;&#29615;&#23545;&#40784;&#32593;&#32476;&#65288;RAN&#65289;&#26469;&#36890;&#36807;&#39046;&#22495;&#23545;&#40784;&#26469;&#26368;&#23567;&#21270;&#39046;&#22495;&#24046;&#24322;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24490;&#29615;&#23545;&#40784;&#27169;&#22359;&#65292;&#36890;&#36807;&#24490;&#29615;&#23545;&#40784;&#31574;&#30053;&#26377;&#25928;&#22320;&#22312;&#26102;&#38388;-&#29366;&#24577;&#21644;&#26102;&#38388;-&#24207;&#21015;&#32423;&#21035;&#23545;&#40784;&#36712;&#36857;&#29305;&#24449;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05810v1 Announce Type: cross  Abstract: Pedestrian trajectory prediction is a crucial component in computer vision and robotics, but remains challenging due to the domain shift problem. Previous studies have tried to tackle this problem by leveraging a portion of the trajectory data from the target domain to adapt the model. However, such domain adaptation methods are impractical in real-world scenarios, as it is infeasible to collect trajectory data from all potential target domains. In this paper, we study a task named generalized pedestrian trajectory prediction, with the aim of generalizing the model to unseen domains without accessing their trajectories. To tackle this task, we introduce a Recurrent Aligned Network~(RAN) to minimize the domain gap through domain alignment. Specifically, we devise a recurrent alignment module to effectively align the trajectory feature spaces at both time-state and time-sequence levels by the recurrent alignment strategy.Furthermore, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#37319;&#26679;&#29575;&#19982;&#23398;&#20064;&#31070;&#32463;&#38544;&#24335;&#22330;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#22312;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#36866;&#24403;&#30340;&#37319;&#26679;&#29575;&#65292;&#20197;&#35299;&#20915;MLP&#20013;&#22122;&#22768;&#20266;&#24433;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01391</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#30340;MLP&#23398;&#20064;SDF&#30340;&#26368;&#20248;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Optimal Sampling for Learning SDF Using MLPs Equipped with Positional Encoding. (arXiv:2401.01391v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#37319;&#26679;&#29575;&#19982;&#23398;&#20064;&#31070;&#32463;&#38544;&#24335;&#22330;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#22312;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#36866;&#24403;&#30340;&#37319;&#26679;&#29575;&#65292;&#20197;&#35299;&#20915;MLP&#20013;&#22122;&#22768;&#20266;&#24433;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#38544;&#24335;&#22330;&#65292;&#22914;&#24418;&#29366;&#30340;&#31070;&#32463;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;SDF&#65289;&#65292;&#24050;&#25104;&#20026;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#24378;&#22823;&#34920;&#31034;&#26041;&#27861;&#65292;&#20363;&#22914;&#32534;&#30721;3D&#24418;&#29366;&#21644;&#25191;&#34892;&#30896;&#25758;&#26816;&#27979;&#12290;&#36890;&#24120;&#65292;&#38544;&#24335;&#22330;&#30001;&#24102;&#26377;&#20301;&#32622;&#32534;&#30721;&#65288;PE&#65289;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#36827;&#34892;&#32534;&#30721;&#20197;&#25429;&#25417;&#39640;&#39057;&#20960;&#20309;&#32454;&#33410;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24102;&#26377;PE&#30340;MLP&#30340;&#19968;&#20010;&#26174;&#33879;&#21103;&#20316;&#29992;&#26159;&#23398;&#20064;&#21040;&#30340;&#38544;&#24335;&#22330;&#20013;&#23384;&#22312;&#22122;&#22768;&#20266;&#24433;&#12290;&#23613;&#31649;&#22686;&#21152;&#37319;&#26679;&#29575;&#36890;&#24120;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#20266;&#24433;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20613;&#31435;&#21494;&#20998;&#26512;&#30340;&#35270;&#35282;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#33391;&#29616;&#35937;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#30830;&#23450;&#23398;&#20064;&#31934;&#30830;&#31070;&#32463;&#38544;&#24335;&#22330;&#30340;&#36866;&#24403;&#37319;&#26679;&#29575;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#19981;&#33391;&#30340;&#21103;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#32593;&#32476;&#21709;&#24212;&#30340;&#20613;&#37324;&#21494;&#20998;&#26512;&#65292;&#29992;&#20110;&#20272;&#35745;&#24102;&#26377;&#38543;&#26426;&#26435;&#37325;&#30340;&#32473;&#23450;&#32593;&#32476;&#30340;&#20869;&#22312;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural implicit fields, such as the neural signed distance field (SDF) of a shape, have emerged as a powerful representation for many applications, e.g., encoding a 3D shape and performing collision detection. Typically, implicit fields are encoded by Multi-layer Perceptrons (MLP) with positional encoding (PE) to capture high-frequency geometric details. However, a notable side effect of such PE-equipped MLPs is the noisy artifacts present in the learned implicit fields. While increasing the sampling rate could in general mitigate these artifacts, in this paper we aim to explain this adverse phenomenon through the lens of Fourier analysis. We devise a tool to determine the appropriate sampling rate for learning an accurate neural implicit field without undesirable side effects. Specifically, we propose a simple yet effective method to estimate the intrinsic frequency of a given network with randomized weights based on the Fourier analysis of the network's responses. It is observed that
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;VAE&#20013;&#32534;&#30721;&#22120;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.19653</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#30340;&#26080;&#38480;&#25968;&#25454;&#35745;&#21010;&#21319;&#32423;VAE&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Upgrading VAE Training With Unlimited Data Plans Provided by Diffusion Models. (arXiv:2310.19653v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19653
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;VAE&#20013;&#32534;&#30721;&#22120;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#20854;&#32534;&#30721;&#22120;&#23481;&#26131;&#36807;&#25311;&#21512;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#30495;&#23454;&#65288;&#36830;&#32493;&#65289;&#25968;&#25454;&#20998;&#24067;$p_{\mathrm{data}}(\mathbf{x})$&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#22266;&#23450;&#32534;&#30721;&#22120;&#36991;&#20813;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#34920;&#31034;&#19981;&#22826;&#21487;&#35299;&#37322;&#65292;&#20294;&#31616;&#21270;&#20102;&#35757;&#32451;&#65292;&#21487;&#20197;&#31934;&#30830;&#21644;&#36830;&#32493;&#22320;&#36924;&#36817;$p_{\mathrm{data}}(\mathbf{x})$&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#35757;&#32451;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;VAE&#20013;&#32534;&#30721;&#22120;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#20123;&#20986;&#20154;&#24847;&#26009;&#65292;&#22240;&#20026;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;&#21478;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#65292;&#29983;&#25104;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#35757;&#32451;&#30340;VAE&#30340;&#27867;&#21270;&#24615;&#33021;&#12289;&#20998;&#25674;&#24046;&#36317;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoders (VAEs) are popular models for representation learning but their encoders are susceptible to overfitting (Cremer et al., 2018) because they are trained on a finite training set instead of the true (continuous) data distribution $p_{\mathrm{data}}(\mathbf{x})$. Diffusion models, on the other hand, avoid this issue by keeping the encoder fixed. This makes their representations less interpretable, but it simplifies training, enabling accurate and continuous approximations of $p_{\mathrm{data}}(\mathbf{x})$. In this paper, we show that overfitting encoders in VAEs can be effectively mitigated by training on samples from a pre-trained diffusion model. These results are somewhat unexpected as recent findings (Alemohammad et al., 2023; Shumailov et al., 2023) observe a decay in generative performance when models are trained on data generated by another generative model. We analyze generalization performance, amortization gap, and robustness of VAEs trained with our pro
&lt;/p&gt;</description></item></channel></rss>