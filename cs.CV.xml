<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#21512;&#20316;&#24863;&#30693;&#21644;&#36816;&#21160;&#39044;&#27979;&#27169;&#22359;&#20849;&#20139;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17916</link><description>&lt;p&gt;
CMP&#65306;&#20855;&#26377;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#30340;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CMP: Cooperative Motion Prediction with Multi-Agent Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17916
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#21512;&#20316;&#24863;&#30693;&#21644;&#36816;&#21160;&#39044;&#27979;&#27169;&#22359;&#20849;&#20139;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#30340;&#21457;&#23637;&#21644;&#36710;&#32852;&#32593;&#65288;V2X&#65289;&#36890;&#20449;&#30340;&#25104;&#29087;&#65292;&#21512;&#20316;&#36830;&#25509;&#30340;&#33258;&#21160;&#21270;&#36710;&#36742;&#65288;CAVs&#65289;&#30340;&#21151;&#33021;&#21464;&#24471;&#21487;&#33021;&#12290;&#26412;&#25991;&#22522;&#20110;&#21512;&#20316;&#24863;&#30693;&#65292;&#25506;&#35752;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;CMP&#20197;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#22686;&#24378;&#36319;&#36394;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#19982;&#36807;&#21435;&#19987;&#27880;&#20110;&#21512;&#20316;&#24863;&#30693;&#25110;&#36816;&#21160;&#39044;&#27979;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#35299;&#20915;CAVs&#22312;&#24863;&#30693;&#21644;&#39044;&#27979;&#27169;&#22359;&#20013;&#20849;&#20139;&#20449;&#24687;&#30340;&#32479;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#20013;&#36824;&#34701;&#20837;&#20102;&#33021;&#22815;&#23481;&#24525;&#29616;&#23454;V2X&#24102;&#23485;&#38480;&#21046;&#21644;&#20256;&#36755;&#24310;&#36831;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#22788;&#29702;&#24222;&#22823;&#30340;&#24863;&#30693;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#39044;&#27979;&#32858;&#21512;&#27169;&#22359;&#65292;&#32479;&#19968;&#20102;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17916v1 Announce Type: cross  Abstract: The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs). Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR signals as input to enhance tracking and prediction capabilities. Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules. Incorporated into our design is the unique capability to tolerate realistic V2X bandwidth limitations and transmission delays, while dealing with bulky perception representations. We also propose a prediction aggregation module, which unifies the predict
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#24207;&#21015;&#21040;&#24207;&#21015;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#22120;&#20013;&#36793;&#38469;&#21270;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#26631;&#35760;&#30340;&#32852;&#21512;&#20998;&#24067;&#24314;&#27169;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#20102;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.02249</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#24207;&#21015;&#21040;&#24207;&#21015;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive Sequence-to-Sequence Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02249
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#24207;&#21015;&#21040;&#24207;&#21015;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#22120;&#20013;&#36793;&#38469;&#21270;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#26631;&#35760;&#30340;&#32852;&#21512;&#20998;&#24067;&#24314;&#27169;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#20102;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#29983;&#25104;&#39044;&#27979;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#24310;&#36831;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24182;&#34892;&#35299;&#30721;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;Query-CTC&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#35299;&#30721;&#22120;&#20013;&#36793;&#38469;&#21270;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#26631;&#35760;&#30340;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#19981;&#20687;&#33258;&#22238;&#24402;&#27169;&#22411;&#37027;&#26679;&#38480;&#21046;&#22312;&#26465;&#20214;&#20998;&#24067;&#19978;&#12290;&#32467;&#26524;&#27169;&#22411;NARVL&#22312;&#25512;&#29702;&#26102;&#38388;&#19978;&#36798;&#21040;&#20102;&#19982;&#26368;&#26032;&#33258;&#22238;&#24402;&#23545;&#24212;&#29289;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#26356;&#24555;&#65292;&#20174;&#19982;&#39034;&#24207;&#29983;&#25104;&#26631;&#35760;&#30456;&#20851;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#20943;&#23569;&#21040;&#24120;&#37327;&#26102;&#38388;&#32852;&#21512;&#25512;&#29702;&#30340;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02249v1 Announce Type: cross  Abstract: Sequence-to-sequence vision-language models are showing promise, but their applicability is limited by their inference latency due to their autoregressive way of generating predictions. We propose a parallel decoding sequence-to-sequence vision-language model, trained with a Query-CTC loss, that marginalizes over multiple inference paths in the decoder. This allows us to model the joint distribution of tokens, rather than restricting to conditional distribution as in an autoregressive model. The resulting model, NARVL, achieves performance on-par with its state-of-the-art autoregressive counterpart, but is faster at inference time, reducing from the linear complexity associated with the sequential generation of tokens to a paradigm of constant time joint inference.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22312;&#23376;&#23545;&#35937;&#32423;&#21035;&#36827;&#34892;&#22270;&#20687;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#23558;&#23376;&#23545;&#35937;&#27573;&#21387;&#32553;&#20026;&#32039;&#20945;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#23545;&#35937;&#21644;&#23646;&#24615;&#25551;&#36848;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.14327</link><description>&lt;p&gt;
&#23376;&#23545;&#35937;&#32423;&#22270;&#20687;&#26631;&#35760;&#21270;
&lt;/p&gt;
&lt;p&gt;
Subobject-level Image Tokenization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14327
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22312;&#23376;&#23545;&#35937;&#32423;&#21035;&#36827;&#34892;&#22270;&#20687;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#23558;&#23376;&#23545;&#35937;&#27573;&#21387;&#32553;&#20026;&#32039;&#20945;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#23545;&#35937;&#21644;&#23646;&#24615;&#25551;&#36848;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35270;&#35273;&#27169;&#22411;&#36890;&#24120;&#23558;&#22270;&#20687;&#26631;&#35760;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#26041;&#24418;&#34917;&#19969;&#20316;&#20026;&#36755;&#20837;&#21333;&#20803;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#23545;&#22270;&#20687;&#20869;&#23481;&#30340;&#36866;&#24212;&#24615;&#65292;&#24182;&#24573;&#30053;&#20102;&#22266;&#26377;&#30340;&#20687;&#32032;&#20998;&#32452;&#32467;&#26500;&#12290;&#21463;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#37319;&#29992;&#30340;&#23376;&#35789;&#26631;&#35760;&#21270;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23376;&#23545;&#35937;&#32423;&#21035;&#36827;&#34892;&#22270;&#20687;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#23376;&#23545;&#35937;&#30001;&#36890;&#36807;&#20998;&#21106;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#20998;&#21106;&#20219;&#20309;&#27169;&#22411;&#65289;&#33719;&#24471;&#30340;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#22270;&#20687;&#27573;&#34920;&#31034;&#12290;&#20026;&#20102;&#23454;&#29616;&#22522;&#20110;&#23376;&#23545;&#35937;&#26631;&#35760;&#21270;&#30340;&#23398;&#20064;&#31995;&#32479;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#24207;&#21015;&#33258;&#32534;&#30721;&#22120;&#65288;SeqAE&#65289;&#65292;&#23558;&#19981;&#21516;&#22823;&#23567;&#21644;&#24418;&#29366;&#30340;&#23376;&#23545;&#35937;&#27573;&#21387;&#32553;&#20026;&#32039;&#20945;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#28982;&#21518;&#23558;&#23376;&#23545;&#35937;&#23884;&#20837;&#39304;&#36865;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23376;&#23545;&#35937;&#32423;&#21035;&#26631;&#35760;&#21270;&#26174;&#33879;&#20419;&#36827;&#20102;&#26377;&#25928;&#22320;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#23545;&#35937;&#21644;&#23646;&#24615;&#25551;&#36848;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14327v1 Announce Type: cross  Abstract: Transformer-based vision models typically tokenize images into fixed-size square patches as input units, which lacks the adaptability to image content and overlooks the inherent pixel grouping structure. Inspired by the subword tokenization widely adopted in language models, we propose an image tokenizer at a subobject level, where the subobjects are represented by semantically meaningful image segments obtained by segmentation models (e.g., segment anything models). To implement a learning system based on subobject tokenization, we first introduced a Sequence-to-sequence AutoEncoder (SeqAE) to compress subobject segments of varying sizes and shapes into compact embedding vectors, then fed the subobject embeddings into a large language model for vision language learning. Empirical results demonstrated that our subobject-level tokenization significantly facilitates efficient learning of translating images into object and attribute descr
&lt;/p&gt;</description></item></channel></rss>