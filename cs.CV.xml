<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>MedPromptX&#26159;&#31532;&#19968;&#20010;&#23558;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;&#35270;&#35273;&#22522;&#30784;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#34917;&#20805;&#32570;&#22833;&#30340;EHR&#20449;&#24687;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24187;&#35273;&#38382;&#39064;&#65292;&#20294;&#36873;&#25321;&#26368;&#20339;&#23569;&#26679;&#26412;&#31034;&#20363;&#21644;&#39640;&#36136;&#37327;&#20505;&#36873;&#32773;&#20173;&#26377;&#24453;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2403.15585</link><description>&lt;p&gt;
MedPromptX&#65306;&#22522;&#20110;&#29616;&#23454;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15585
&lt;/p&gt;
&lt;p&gt;
MedPromptX&#26159;&#31532;&#19968;&#20010;&#23558;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;&#35270;&#35273;&#22522;&#30784;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#34917;&#20805;&#32570;&#22833;&#30340;EHR&#20449;&#24687;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24187;&#35273;&#38382;&#39064;&#65292;&#20294;&#36873;&#25321;&#26368;&#20339;&#23569;&#26679;&#26412;&#31034;&#20363;&#21644;&#39640;&#36136;&#37327;&#20505;&#36873;&#32773;&#20173;&#26377;&#24453;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33016;&#37096;X&#32447;&#22270;&#20687;&#36890;&#24120;&#29992;&#20110;&#39044;&#27979;&#24613;&#24615;&#21644;&#24930;&#24615;&#24515;&#32954;&#30142;&#30149;&#65292;&#20294;&#26159;&#23558;&#23427;&#20204;&#19982;&#32467;&#26500;&#21270;&#20020;&#24202;&#25968;&#25454;&#25972;&#21512;&#30340;&#21162;&#21147;&#38754;&#20020;&#30528;&#22240;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#19981;&#23436;&#25972;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;MedPromptX&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12289;&#23569;&#26679;&#26412;&#25552;&#31034;&#65288;FP&#65289;&#21644;&#35270;&#35273;&#22522;&#30784;&#65288;VG&#65289;&#30456;&#32467;&#21512;&#65292;&#23558;&#22270;&#20687;&#19982;EHR&#25968;&#25454;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;&#30340;&#27169;&#22411;&#12290;&#39044;&#35757;&#32451;&#30340;MLLM&#34987;&#29992;&#26469;&#34917;&#20805;&#32570;&#22833;&#30340;EHR&#20449;&#24687;&#65292;&#25552;&#20379;&#23545;&#24739;&#32773;&#30149;&#21490;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#23569;&#26679;&#26412;&#25552;&#31034;&#20943;&#23569;&#20102;&#23545;MLLM&#30340;&#22823;&#37327;&#35757;&#32451;&#30340;&#24517;&#35201;&#24615;&#65292;&#21516;&#26102;&#26377;&#25928;&#35299;&#20915;&#20102;&#24187;&#35273;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#26368;&#20339;&#23569;&#26679;&#26412;&#31034;&#20363;&#30340;&#36807;&#31243;&#21644;&#36873;&#25321;&#39640;&#36136;&#37327;&#20505;&#36873;&#32773;&#21487;&#33021;&#36807;&#20110;&#32321;&#29712;&#65292;&#20294;&#23427;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#30528;&#28145;&#36828;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#26469;&#21160;&#24577;&#22320;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15585v1 Announce Type: cross  Abstract: Chest X-ray images are commonly used for predicting acute and chronic cardiopulmonary conditions, but efforts to integrate them with structured clinical data face challenges due to incomplete electronic health records (EHR). This paper introduces \textbf{MedPromptX}, the first model to integrate multimodal large language models (MLLMs), few-shot prompting (FP) and visual grounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A pre-trained MLLM is utilized to complement the missing EHR information, providing a comprehensive understanding of patients' medical history. Additionally, FP reduces the necessity for extensive training of MLLMs while effectively tackling the issue of hallucination. Nevertheless, the process of determining the optimal number of few-shot examples and selecting high-quality candidates can be burdensome, yet it profoundly influences model performance. Hence, we propose a new technique that dynam
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26469;&#36873;&#25321;&#38656;&#35201;&#36827;&#19968;&#27493;&#36866;&#24212;&#30340;&#23618;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30001;&#20110;&#20266;&#26631;&#31614;&#24341;&#36215;&#30340;&#19981;&#20934;&#30830;&#24615;&#22256;&#25200;&#12290;</title><link>https://arxiv.org/abs/2403.10650</link><description>&lt;p&gt;
PALM&#65306;&#25512;&#36827;&#29992;&#20110;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26469;&#36873;&#25321;&#38656;&#35201;&#36827;&#19968;&#27493;&#36866;&#24212;&#30340;&#23618;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30001;&#20110;&#20266;&#26631;&#31614;&#24341;&#36215;&#30340;&#19981;&#20934;&#30830;&#24615;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#27169;&#22411;&#38754;&#20020;&#39046;&#22495;&#20998;&#24067;&#30340;&#24555;&#36895;&#36716;&#21464;&#65292;&#23548;&#33268;&#35782;&#21035;&#24615;&#33021;&#19979;&#38477;&#12290;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;CTTA&#65289;&#30452;&#25509;&#26681;&#25454;&#27979;&#35797;&#25968;&#25454;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#28304;&#21028;&#21035;&#27169;&#22411;&#20197;&#36866;&#24212;&#36825;&#20123;&#19981;&#26029;&#21464;&#21270;&#30340;&#39046;&#22495;&#12290;&#19968;&#31181;&#39640;&#24230;&#26377;&#25928;&#30340;CTTA&#26041;&#27861;&#28041;&#21450;&#24212;&#29992;&#36880;&#23618;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#24182;&#36873;&#25321;&#24615;&#22320;&#35843;&#25972;&#39044;&#35757;&#32451;&#23618;&#12290;&#28982;&#32780;&#65292;&#23427;&#21463;&#21040;&#39046;&#22495;&#36716;&#31227;&#20272;&#35745;&#19981;&#20934;&#30830;&#21644;&#30001;&#20266;&#26631;&#31614;&#24341;&#36215;&#30340;&#19981;&#20934;&#30830;&#24615;&#25152;&#22256;&#25200;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#23618;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26469;&#36873;&#25321;&#23618;&#65292;&#32780;&#26080;&#39035;&#20381;&#36182;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#21033;&#29992;&#26799;&#24230;&#30340;&#22823;&#23567;&#20316;&#20026;&#19968;&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;softmax&#36755;&#20986;&#19982;&#22343;&#21248;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#35745;&#31639;&#65292;&#20197;&#36873;&#25321;&#38656;&#35201;&#36827;&#19968;&#27493;&#36866;&#24212;&#30340;&#23618;&#12290;&#38543;&#21518;&#65292;&#20165;&#23646;&#20110;&#36825;&#20123;&#23618;&#30340;&#21442;&#25968;&#23558;&#34987;&#36827;&#19968;&#27493;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10650v1 Announce Type: cross  Abstract: Real-world vision models in dynamic environments face rapid shifts in domain distributions, leading to decreased recognition performance. Continual test-time adaptation (CTTA) directly adjusts a pre-trained source discriminative model to these changing domains using test data. A highly effective CTTA method involves applying layer-wise adaptive learning rates, and selectively adapting pre-trained layers. However, it suffers from the poor estimation of domain shift and the inaccuracies arising from the pseudo-labels. In this work, we aim to overcome these limitations by identifying layers through the quantification of model prediction uncertainty without relying on pseudo-labels. We utilize the magnitude of gradients as a metric, calculated by backpropagating the KL divergence between the softmax output and a uniform distribution, to select layers for further adaptation. Subsequently, for the parameters exclusively belonging to these se
&lt;/p&gt;</description></item><item><title>DiFaReli&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#35299;&#30721;&#35299;&#32806;&#30340;&#20809;&#32534;&#30721;&#20197;&#21450;&#20174;&#29616;&#25104;&#30340;&#20272;&#31639;&#22120;&#25512;&#26029;&#20986;&#30340;&#19982;3D&#24418;&#29366;&#21644;&#38754;&#37096;&#36523;&#20221;&#30456;&#20851;&#30340;&#20854;&#20182;&#32534;&#30721;&#65292;&#33021;&#22815;&#22788;&#29702;&#21333;&#35270;&#35282;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#30340;&#20154;&#33080;&#37325;&#29031;&#65292;&#26080;&#38656;&#20809;&#32447;&#33310;&#21488;&#25968;&#25454;&#12289;&#22810;&#35270;&#22270;&#22270;&#20687;&#25110;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09479</link><description>&lt;p&gt;
DiFaReli: &#25193;&#25955;&#20154;&#33080;&#37325;&#29031;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
DiFaReli : Diffusion Face Relighting. (arXiv:2304.09479v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09479
&lt;/p&gt;
&lt;p&gt;
DiFaReli&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#35299;&#30721;&#35299;&#32806;&#30340;&#20809;&#32534;&#30721;&#20197;&#21450;&#20174;&#29616;&#25104;&#30340;&#20272;&#31639;&#22120;&#25512;&#26029;&#20986;&#30340;&#19982;3D&#24418;&#29366;&#21644;&#38754;&#37096;&#36523;&#20221;&#30456;&#20851;&#30340;&#20854;&#20182;&#32534;&#30721;&#65292;&#33021;&#22815;&#22788;&#29702;&#21333;&#35270;&#35282;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#30340;&#20154;&#33080;&#37325;&#29031;&#65292;&#26080;&#38656;&#20809;&#32447;&#33310;&#21488;&#25968;&#25454;&#12289;&#22810;&#35270;&#22270;&#22270;&#20687;&#25110;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#37326;&#22806;&#29615;&#22659;&#19979;&#30340;&#21333;&#35270;&#35282;&#20154;&#33080;&#37325;&#29031;&#12290;&#22788;&#29702;&#20840;&#23616;&#29031;&#26126;&#25110;&#25237;&#24433;&#38452;&#24433;&#31561;&#38750;&#28459;&#21453;&#23556;&#25928;&#24212;&#19968;&#30452;&#26159;&#20154;&#33080;&#37325;&#29031;&#39046;&#22495;&#30340;&#38590;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#20551;&#23450;&#20848;&#20271;&#29305;&#21453;&#23556;&#34920;&#38754;&#65292;&#31616;&#21270;&#20809;&#29031;&#27169;&#22411;&#65292;&#25110;&#32773;&#38656;&#35201;&#20272;&#35745;&#19977;&#32500;&#24418;&#29366;&#12289;&#21453;&#23556;&#29575;&#25110;&#38452;&#24433;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20272;&#35745;&#26159;&#23481;&#26131;&#20986;&#38169;&#30340;&#65292;&#38656;&#35201;&#35768;&#22810;&#20855;&#26377;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#30340;&#35757;&#32451;&#26679;&#26412;&#25165;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32469;&#36807;&#20102;&#20934;&#30830;&#20272;&#35745;&#22266;&#26377;&#32452;&#20214;&#30340;&#38656;&#35201;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;2D&#22270;&#20687;&#35757;&#32451;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#20809;&#32447;&#33310;&#21488;&#25968;&#25454;&#12289;&#22810;&#35270;&#22270;&#22270;&#20687;&#25110;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#65288;DDIM&#65289;&#35299;&#30721;&#35299;&#32806;&#30340;&#20809;&#32534;&#30721;&#20197;&#21450;&#20174;&#29616;&#25104;&#30340;&#20272;&#31639;&#22120;&#25512;&#26029;&#20986;&#30340;&#19982;3D&#24418;&#29366;&#21644;&#38754;&#37096;&#36523;&#20221;&#30456;&#20851;&#30340;&#20854;&#20182;&#32534;&#30721;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#33410;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#31616;&#21270;&#20809;&#19982;&#20960;&#20309;&#20043;&#38388;&#22797;&#26434;&#20114;&#21160;&#30340;&#24314;&#27169;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to single-view face relighting in the wild. Handling non-diffuse effects, such as global illumination or cast shadows, has long been a challenge in face relighting. Prior work often assumes Lambertian surfaces, simplified lighting models or involves estimating 3D shape, albedo, or a shadow map. This estimation, however, is error-prone and requires many training examples with lighting ground truth to generalize well. Our work bypasses the need for accurate estimation of intrinsic components and can be trained solely on 2D images without any light stage data, multi-view images, or lighting ground truth. Our key idea is to leverage a conditional diffusion implicit model (DDIM) for decoding a disentangled light encoding along with other encodings related to 3D shape and facial identity inferred from off-the-shelf estimators. We also propose a novel conditioning technique that eases the modeling of the complex interaction between light and geometry by using a ren
&lt;/p&gt;</description></item><item><title>R2C-GAN&#26159;&#19968;&#31181;&#29992;&#20110;&#30450;&#30446;X&#23556;&#32447;&#24674;&#22797;&#21644;COVID-19&#20998;&#31867;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#22270;&#20687;&#24674;&#22797;&#25552;&#39640;X&#23556;&#32447;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#35786;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.14770</link><description>&lt;p&gt;
R2C-GAN: &#29992;&#20110;&#30450;&#30446;X&#23556;&#32447;&#24674;&#22797;&#21644;COVID-19&#20998;&#31867;&#30340;&#24674;&#22797;&#21040;&#20998;&#31867;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
R2C-GAN: Restore-to-Classify GANs for Blind X-Ray Restoration and COVID-19 Classification. (arXiv:2209.14770v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14770
&lt;/p&gt;
&lt;p&gt;
R2C-GAN&#26159;&#19968;&#31181;&#29992;&#20110;&#30450;&#30446;X&#23556;&#32447;&#24674;&#22797;&#21644;COVID-19&#20998;&#31867;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#22270;&#20687;&#24674;&#22797;&#25552;&#39640;X&#23556;&#32447;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#35786;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#38024;&#23545;&#30450;&#30446;X&#23556;&#32447;&#24674;&#22797;&#30340;&#32852;&#21512;&#27169;&#22411;&#65306;&#24674;&#22797;&#21040;&#20998;&#31867;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(R2C-GANs)&#12290;&#35813;&#27169;&#22411;&#22312;&#20445;&#25345;&#30142;&#30149;&#23436;&#25972;&#24615;&#30340;&#21516;&#26102;&#36827;&#34892;&#22270;&#20687;&#24674;&#22797;&#65292;&#20174;&#32780;&#25552;&#39640;X&#23556;&#32447;&#22270;&#20687;&#30340;&#36136;&#37327;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#35786;&#26029;&#24615;&#33021;&#12290;&#23558;&#24674;&#22797;&#20219;&#21153;&#23450;&#20041;&#20026;&#20174;&#36136;&#37327;&#36739;&#24046;&#21253;&#21547;&#26377;&#22122;&#22768;&#12289;&#27169;&#31946;&#25110;&#36807;/&#27424;&#26333;&#22270;&#29255;&#21040;&#39640;&#36136;&#37327;&#22270;&#20687;&#39046;&#22495;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#38382;&#39064;&#12290;R2C-GAN&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restoration of poor quality images with a blended set of artifacts plays a vital role for a reliable diagnosis. Existing studies have focused on specific restoration problems such as image deblurring, denoising, and exposure correction where there is usually a strong assumption on the artifact type and severity. As a pioneer study in blind X-ray restoration, we propose a joint model for generic image restoration and classification: Restore-to-Classify Generative Adversarial Networks (R2C-GANs). Such a jointly optimized model keeps any disease intact after the restoration. Therefore, this will naturally lead to a higher diagnosis performance thanks to the improved X-ray image quality. To accomplish this crucial objective, we define the restoration task as an Image-to-Image translation problem from poor quality having noisy, blurry, or over/under-exposed images to high quality image domain. The proposed R2C-GAN model is able to learn forward and inverse transforms between the two domains
&lt;/p&gt;</description></item></channel></rss>