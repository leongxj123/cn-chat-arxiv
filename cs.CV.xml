<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#23454;&#29616;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#24555;&#36895;&#36866;&#24212;&#65292;&#20026;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#26102;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2404.02785</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#39046;&#22495;&#27867;&#21270;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization through Meta-Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02785
&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#23454;&#29616;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#24555;&#36895;&#36866;&#24212;&#65292;&#20026;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#26102;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#65292;&#20294;&#26159;&#24403;&#38754;&#23545;&#20998;&#24067;&#20043;&#22806;(out-of-distribution, OOD)&#25968;&#25454;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#22240;&#20026;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#19981;&#21487;&#36991;&#20813;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#34987;&#20551;&#23450;&#20026;&#20849;&#20139;&#30456;&#21516;&#20998;&#24067;&#30340;&#24120;&#35265;&#24773;&#20917;&#12290;&#23613;&#31649;DNNs&#22312;&#22823;&#37327;&#25968;&#25454;&#21644;&#35745;&#31639;&#33021;&#21147;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#24456;&#38590;&#24212;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#12290;&#20803;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#30340;&#31639;&#27861;&#36827;&#34892;&#24555;&#36895;&#36866;&#24212;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#38656;&#35201;&#20174;&#22836;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#20803;&#23398;&#20064;&#39046;&#22495;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02785v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) have revolutionized artificial intelligence but often lack performance when faced with out-of-distribution (OOD) data, a common scenario due to the inevitable domain shifts in real-world applications. This limitation stems from the common assumption that training and testing data share the same distribution-an assumption frequently violated in practice. Despite their effectiveness with large amounts of data and computational power, DNNs struggle with distributional shifts and limited labeled data, leading to overfitting and poor generalization across various tasks and domains. Meta-learning presents a promising approach by employing algorithms that acquire transferable knowledge across various tasks for fast adaptation, eliminating the need to learn each task from scratch. This survey paper delves into the realm of meta-learning with a focus on its contribution to domain generalization. We first clarify the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;IllusionVQA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#38169;&#35273;&#21644;&#38590;&#35299;&#22330;&#26223;&#19979;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#29702;&#35299;&#20219;&#21153;&#21644;&#23450;&#20301;&#20219;&#21153;&#19978;&#65292;&#34920;&#29616;&#26368;&#20339;&#30340;VLM&#20026;GPT4V&#65292;&#32780;&#20154;&#31867;&#34920;&#29616;&#26356;&#32988;&#19968;&#31609;&#12290;</title><link>https://arxiv.org/abs/2403.15952</link><description>&lt;p&gt;
IllusionVQA&#65306;&#19968;&#20010;&#25361;&#25112;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35273;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15952
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;IllusionVQA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#38169;&#35273;&#21644;&#38590;&#35299;&#22330;&#26223;&#19979;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#29702;&#35299;&#20219;&#21153;&#21644;&#23450;&#20301;&#20219;&#21153;&#19978;&#65292;&#34920;&#29616;&#26368;&#20339;&#30340;VLM&#20026;GPT4V&#65292;&#32780;&#20154;&#31867;&#34920;&#29616;&#26356;&#32988;&#19968;&#31609;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#20986;&#29616;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#35843;&#26597;&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#35273;&#29702;&#35299;&#12290; VLM&#19981;&#20165;&#33021;&#22815;&#36827;&#34892;&#23545;&#35937;&#20998;&#31867;&#21644;&#26816;&#27979;&#65292;&#36824;&#33021;&#22815;&#36827;&#34892;&#35270;&#35273;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#12290; &#36825;&#33258;&#28982;&#32780;&#28982;&#22320;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#22270;&#20687;&#26412;&#36523;&#26159;&#19981;&#21512;&#29702;&#30340;&#26102;&#65292;VLM&#20250;&#22914;&#20309;&#22238;&#24212;&#65311; &#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IllusionVQA&#65306;&#19968;&#20010;&#21253;&#21547;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20809;&#23398;&#38169;&#35273;&#21644;&#38590;&#20197;&#35299;&#37322;&#30340;&#22330;&#26223;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#65292;&#20197;&#27979;&#35797;VLM&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#22810;&#36873;VQA&#20219;&#21153; - &#29702;&#35299;&#21644;&#36719;&#23450;&#20301;&#30340;&#33021;&#21147;&#12290; &#34920;&#29616;&#26368;&#20339;&#30340;VLM GPT4V&#22312;&#29702;&#35299;&#20219;&#21153;&#65288;4-shot&#65289;&#19978;&#23454;&#29616;&#20102;62.99&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#23450;&#20301;&#20219;&#21153;&#65288;4-shot&#21644;Chain-of-Thought&#65289;&#19978;&#23454;&#29616;&#20102;49.7&#65285;&#30340;&#20934;&#30830;&#29575;&#12290; &#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;&#20154;&#31867;&#22312;&#29702;&#35299;&#21644;&#23450;&#20301;&#26041;&#38754;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;91.03&#65285;&#21644;100&#65285;&#12290; &#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;Chain-of-Thought&#25512;&#29702;&#26041;&#38754;&#26377;&#24456;&#22823;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15952v1 Announce Type: cross  Abstract: The advent of Vision Language Models (VLM) has allowed researchers to investigate the visual understanding of a neural network using natural language. Beyond object classification and detection, VLMs are capable of visual comprehension and common-sense reasoning. This naturally led to the question: How do VLMs respond when the image itself is inherently unreasonable? To this end, we present IllusionVQA: a diverse dataset of challenging optical illusions and hard-to-interpret scenes to test the capability of VLMs in two distinct multiple-choice VQA tasks - comprehension and soft localization. GPT4V, the best-performing VLM, achieves 62.99% accuracy (4-shot) on the comprehension task and 49.7% on the localization task (4-shot and Chain-of-Thought). Human evaluation reveals that humans achieve 91.03% and 100% accuracy in comprehension and localization. We discover that In-Context Learning (ICL) and Chain-of-Thought reasoning substantially
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#30828;&#21644;&#36719;&#36127;&#26679;&#26412;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24247;&#22797;&#38203;&#28860;&#35780;&#20272;&#20013;&#26679;&#26412;&#31232;&#32570;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.02772</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#24247;&#22797;&#38203;&#28860;&#36136;&#37327;&#35780;&#20272;&#65292;&#32467;&#21512;&#30828;&#36127;&#26679;&#26412;&#21644;&#36719;&#36127;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Rehabilitation Exercise Quality Assessment through Supervised Contrastive Learning with Hard and Soft Negatives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02772
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#30828;&#21644;&#36719;&#36127;&#26679;&#26412;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24247;&#22797;&#38203;&#28860;&#35780;&#20272;&#20013;&#26679;&#26412;&#31232;&#32570;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38203;&#28860;&#30340;&#24247;&#22797;&#35745;&#21010;&#24050;&#34987;&#35777;&#26126;&#22312;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#12289;&#38477;&#20302;&#27515;&#20129;&#29575;&#21644;&#20877;&#20303;&#38498;&#29575;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#34394;&#25311;&#24247;&#22797;&#65292;&#24739;&#32773;&#21487;&#20197;&#22312;&#23478;&#29420;&#31435;&#23436;&#25104;&#38203;&#28860;&#65292;&#21033;&#29992;AI&#31639;&#27861;&#20998;&#26512;&#38203;&#28860;&#25968;&#25454;&#65292;&#20026;&#24739;&#32773;&#25552;&#20379;&#21453;&#39304;&#65292;&#24182;&#21521;&#20020;&#24202;&#21307;&#29983;&#26356;&#26032;&#20182;&#20204;&#30340;&#36827;&#23637;&#24773;&#20917;&#12290;&#36825;&#20123;&#35745;&#21010;&#36890;&#24120;&#20250;&#25351;&#23450;&#21508;&#31181;&#38203;&#28860;&#31867;&#22411;&#65292;&#36825;&#23548;&#33268;&#24247;&#22797;&#38203;&#28860;&#35780;&#20272;&#25968;&#25454;&#38598;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#65306;&#34429;&#28982;&#22312;&#25972;&#20307;&#35757;&#32451;&#26679;&#26412;&#20013;&#20016;&#23500;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#23545;&#27599;&#31181;&#20855;&#20307;&#38203;&#32451;&#31867;&#22411;&#30340;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#12290;&#36825;&#31181;&#24046;&#24322;&#24433;&#21709;&#20102;&#29616;&#26377;&#26041;&#27861;&#35757;&#32451;&#20855;&#26377;&#23567;&#26679;&#26412;&#37327;&#30340;&#27599;&#31181;&#38203;&#32451;&#30340;&#21487;&#27867;&#21270;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#30828;&#36127;&#26679;&#26412;&#21644;&#36719;&#36127;&#26679;&#26412;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#21033;&#29992;&#20102;&#25972;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02772v1 Announce Type: cross  Abstract: Exercise-based rehabilitation programs have proven to be effective in enhancing the quality of life and reducing mortality and rehospitalization rates. AI-driven virtual rehabilitation, which allows patients to independently complete exercises at home, utilizes AI algorithms to analyze exercise data, providing feedback to patients and updating clinicians on their progress. These programs commonly prescribe a variety of exercise types, leading to a distinct challenge in rehabilitation exercise assessment datasets: while abundant in overall training samples, these datasets often have a limited number of samples for each individual exercise type. This disparity hampers the ability of existing approaches to train generalizable models with such a small sample size per exercise. Addressing this issue, our paper introduces a novel supervised contrastive learning framework with hard and soft negative samples that effectively utilizes the entir
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26465;&#20214;&#35270;&#39057;&#25193;&#25955;&#23398;&#20064;&#22870;&#21169;&#65292;&#35299;&#20915;&#22797;&#26434;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#36229;&#36234;&#20102;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.14134</link><description>&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#35270;&#39057;&#25193;&#25955;&#23398;&#20064;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Diffusion Reward: Learning Rewards via Conditional Video Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14134
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#35270;&#39057;&#25193;&#25955;&#23398;&#20064;&#22870;&#21169;&#65292;&#35299;&#20915;&#22797;&#26434;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#36229;&#36234;&#20102;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#19987;&#23478;&#35270;&#39057;&#20013;&#23398;&#20064;&#22870;&#21169;&#20026;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25351;&#23450;&#39044;&#26399;&#34892;&#20026;&#25552;&#20379;&#20102;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion Reward&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#26465;&#20214;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#20174;&#19987;&#23478;&#35270;&#39057;&#20013;&#23398;&#20064;&#22870;&#21169;&#20197;&#35299;&#20915;&#22797;&#26434;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#22312;&#19987;&#23478;&#36712;&#36857;&#30340;&#26465;&#20214;&#19979;&#35266;&#23519;&#21040;&#36739;&#20302;&#30340;&#29983;&#25104;&#22810;&#26679;&#24615;&#12290;&#22240;&#27492;&#65292;Diffusion Reward&#34987;&#24418;&#24335;&#21270;&#20026;&#36127;&#30340;&#26465;&#20214;&#29109;&#65292;&#40723;&#21169;&#19987;&#23478;&#24335;&#34892;&#20026;&#30340;&#26377;&#25928;&#25506;&#32034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MetaWorld&#21644;Adroit&#30340;10&#20010;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#20197;&#35270;&#35273;&#36755;&#20837;&#21644;&#31232;&#30095;&#22870;&#21169;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;Diffusion Reward&#29978;&#33267;&#33021;&#22815;&#25104;&#21151;&#26377;&#25928;&#22320;&#35299;&#20915;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#22823;&#22823;&#36229;&#36234;&#20102;&#22522;&#32447;&#26041;&#27861;&#12290;&#39033;&#30446;&#39029;&#38754;&#21644;&#20195;&#30721;&#65306;https://diffusion-reward.github.io/&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14134v2 Announce Type: replace  Abstract: Learning rewards from expert videos offers an affordable and effective solution to specify the intended behaviors for reinforcement learning tasks. In this work, we propose Diffusion Reward, a novel framework that learns rewards from expert videos via conditional video diffusion models for solving complex visual RL problems. Our key insight is that lower generative diversity is observed when conditioned on expert trajectories. Diffusion Reward is accordingly formalized by the negative of conditional entropy that encourages productive exploration of expert-like behaviors. We show the efficacy of our method over 10 robotic manipulation tasks from MetaWorld and Adroit with visual input and sparse reward. Moreover, Diffusion Reward could even solve unseen tasks successfully and effectively, largely surpassing baseline methods. Project page and code: https://diffusion-reward.github.io/.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8220;&#19968;&#33324;&#26446;&#26222;&#24076;&#33576;&#65288;GL&#65289;&#8221;&#65292;&#29992;&#20110;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35748;&#35777;&#65292;&#20197;&#25269;&#24481;&#21487;&#32452;&#21512;&#30340;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#24178;&#25200;&#12290;&#26041;&#27861;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#34920;&#29616;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2309.16710</link><description>&lt;p&gt;
&#36890;&#36807;&#20381;&#36182;&#20110;&#21464;&#25442;&#30340;&#38543;&#26426;&#24179;&#28369;&#65292;&#25552;&#20379;&#19968;&#33324;&#26446;&#26222;&#24076;&#33576;&#65306;&#38024;&#23545;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#21464;&#25442;&#30340;&#35748;&#35777;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;
General Lipschitz: Certified Robustness Against Resolvable Semantic Transformations via Transformation-Dependent Randomized Smoothing. (arXiv:2309.16710v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16710
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8220;&#19968;&#33324;&#26446;&#26222;&#24076;&#33576;&#65288;GL&#65289;&#8221;&#65292;&#29992;&#20110;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35748;&#35777;&#65292;&#20197;&#25269;&#24481;&#21487;&#32452;&#21512;&#30340;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#24178;&#25200;&#12290;&#26041;&#27861;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24179;&#28369;&#26159;&#30446;&#21069;&#26500;&#24314;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#21487;&#20197;&#35777;&#26126;&#20854;&#23545;&#20110;&#26377;&#30028;&#24178;&#25200;&#30340;&#25239;&#24615;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#38024;&#23545;&#35821;&#20041;&#21464;&#25442;&#65288;&#20363;&#22914;&#65292;&#22270;&#20687;&#27169;&#31946;&#12289;&#24179;&#31227;&#12289;Gamma&#30699;&#27491;&#65289;&#21450;&#20854;&#32452;&#21512;&#30340;&#21512;&#29702;&#35777;&#20070;&#26356;&#20026;&#22797;&#26434;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8220;&#19968;&#33324;&#26446;&#26222;&#24076;&#33576;&#65288;GL&#65289;&#8221;&#65292;&#29992;&#20110;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35748;&#35777;&#65292;&#20197;&#25269;&#24481;&#21487;&#32452;&#21512;&#30340;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#24178;&#25200;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24179;&#28369;&#20998;&#31867;&#22120;&#19982;&#21464;&#25442;&#21442;&#25968;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#25512;&#23548;&#20986;&#30456;&#24212;&#30340;&#20581;&#22766;&#24615;&#35777;&#20070;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized smoothing is the state-of-the-art approach to construct image classifiers that are provably robust against additive adversarial perturbations of bounded magnitude. However, it is more complicated to construct reasonable certificates against semantic transformation (e.g., image blurring, translation, gamma correction) and their compositions. In this work, we propose \emph{General Lipschitz (GL),} a new framework to certify neural networks against composable resolvable semantic perturbations. Within the framework, we analyze transformation-dependent Lipschitz-continuity of smoothed classifiers w.r.t. transformation parameters and derive corresponding robustness certificates. Our method performs comparably to state-of-the-art approaches on the ImageNet dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#25105;&#20204;&#36890;&#36807;&#24050;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23884;&#20837;&#24335;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#24182;&#20026;MVMR&#20219;&#21153;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;</title><link>http://arxiv.org/abs/2309.16701</link><description>&lt;p&gt;
MVMR: &#22312;&#22810;&#20010;&#21487;&#38752;&#35270;&#39057;&#38598;&#20013;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
MVMR: Evaluating Natural Language Video Localization Bias over Multiple Reliable Videos Pool. (arXiv:2309.16701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#25105;&#20204;&#36890;&#36807;&#24050;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23884;&#20837;&#24335;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#24182;&#20026;MVMR&#20219;&#21153;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#28608;&#22686;&#65292;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#23427;&#33268;&#21147;&#20110;&#26816;&#27979;&#19982;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#21305;&#37197;&#30340;&#35270;&#39057;&#29255;&#27573;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#37117;&#27809;&#26377;&#25506;&#32034;&#22312;&#23384;&#22312;&#22810;&#20010;&#27491;&#36127;&#35270;&#39057;&#30340;&#22823;&#37327;&#35821;&#26009;&#24211;&#20013;&#23450;&#20301;&#19968;&#20010;&#26102;&#21051;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#65288;Massive Videos Moment Retrieval&#65289;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#29616;&#26377;&#35270;&#39057;&#23450;&#20301;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#23884;&#20837;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30446;&#26631;&#26597;&#35810;&#19982;&#35270;&#39057;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#20174;&#32780;&#23450;&#20041;&#27491;&#36127;&#38598;&#12290;&#38024;&#23545;&#25552;&#20986;&#30340;MVMR&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;
&lt;/p&gt;
&lt;p&gt;
With the explosion of multimedia content in recent years, natural language video localization, which focuses on detecting video moment that matches a given natural language query, has become a critical problem. However, none of the previous research explores localizing a moment from a large corpus where multiple positive and negative videos exist. In this paper, we propose an MVMR (Massive Videos Moment Retrieval) task, which aims to localize video frames from a massive set of videos given a text query. For this task, we suggest methods for constructing datasets by employing similarity filtering on the existing video localization datasets and introduce three MVMR datasets. Specifically, we employ embedding-based text similarity matching and video-language grounding techniques to calculate the relevance score between a target query and videos to define positive and negative sets. For the proposed MVMR task, we further develop a strong model, Reliable Mutual Matching Network (RMMN), whic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pair-Net&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#37197;&#23545;&#25552;&#26696;&#32593;&#32476;&#65288;PPN&#65289;&#26469;&#23398;&#20064;&#21644;&#36807;&#28388;&#20027;&#20307;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#31232;&#30095;&#37197;&#23545;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#20013;&#24573;&#35270;&#30340;&#23545;&#35937;&#38388;&#37197;&#23545;&#22238;&#24518;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.08699</link><description>&lt;p&gt;
&#32858;&#28966;&#22330;&#26223;&#22270;&#29983;&#25104;&#30340;&#37197;&#23545;-&#20851;&#31995;&#65306;&#22522;&#20110;&#37197;&#23545;&#32593;&#32476;&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Pair then Relation: Pair-Net for Panoptic Scene Graph Generation. (arXiv:2307.08699v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pair-Net&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#37197;&#23545;&#25552;&#26696;&#32593;&#32476;&#65288;PPN&#65289;&#26469;&#23398;&#20064;&#21644;&#36807;&#28388;&#20027;&#20307;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#31232;&#30095;&#37197;&#23545;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#20013;&#24573;&#35270;&#30340;&#23545;&#35937;&#38388;&#37197;&#23545;&#22238;&#24518;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26159;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#19968;&#39033;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#20854;&#26088;&#22312;&#20351;&#29992;&#20840;&#26223;&#20998;&#21106;&#20195;&#26367;&#36793;&#30028;&#26694;&#26469;&#21019;&#24314;&#26356;&#20840;&#38754;&#30340;&#22330;&#26223;&#22270;&#34920;&#31034;&#12290;&#19982;&#22330;&#26223;&#22270;&#29983;&#25104;&#30456;&#27604;&#65292;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#20855;&#26377;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65306;&#20687;&#32032;&#32423;&#20998;&#21106;&#36755;&#20986;&#21644;&#23436;&#20840;&#20851;&#31995;&#25506;&#32034;&#65288;&#23427;&#36824;&#32771;&#34385;&#20102;&#29289;&#20307;&#21644;&#29289;&#36136;&#20043;&#38388;&#30340;&#20851;&#31995;&#65289;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#30340;&#24615;&#33021;&#26377;&#38480;&#65292;&#36825;&#38459;&#30861;&#20102;&#19979;&#28216;&#20219;&#21153;&#25110;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;&#26412;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#31181;&#26032;&#39062;&#19988;&#24378;&#22823;&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#22522;&#20934;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#24403;&#21069;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#29942;&#39048;&#65292;&#21457;&#29616;&#23545;&#35937;&#38388;&#37197;&#23545;&#30340;&#22238;&#24518;&#29575;&#26159;&#20808;&#21069;&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#25152;&#24573;&#35270;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#21644;&#26368;&#36817;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65306;&#37197;&#23545;-&#20851;&#31995;&#65288;Pair-Net&#65289;&#65292;&#23427;&#20351;&#29992;&#37197;&#23545;&#25552;&#26696;&#32593;&#32476;&#65288;PPN&#65289;&#26469;&#23398;&#20064;&#21644;&#36807;&#28388;&#20027;&#20307;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#31232;&#30095;&#37197;&#23545;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Panoptic Scene Graph (PSG) is a challenging task in Scene Graph Generation (SGG) that aims to create a more comprehensive scene graph representation using panoptic segmentation instead of boxes. Compared to SGG, PSG has several challenging problems: pixel-level segment outputs and full relationship exploration (It also considers thing and stuff relation). Thus, current PSG methods have limited performance, which hinders downstream tasks or applications. The goal of this work aims to design a novel and strong baseline for PSG. To achieve that, we first conduct an in-depth analysis to identify the bottleneck of the current PSG models, finding that inter-object pair-wise recall is a crucial factor that was ignored by previous PSG methods. Based on this and the recent query-based frameworks, we present a novel framework: Pair then Relation (Pair-Net), which uses a Pair Proposal Network (PPN) to learn and filter sparse pair-wise relationships between subjects and objects. Moreover, we also 
&lt;/p&gt;</description></item></channel></rss>