<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#20803;&#20849;&#35757;&#32451;&#36890;&#36807;&#22312;&#25968;&#25454;&#19978;&#26500;&#24314;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#24182;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20849;&#21516;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18083</link><description>&lt;p&gt;
&#20803;&#20849;&#35757;&#32451;&#65306;&#20004;&#31181;&#35270;&#35282;&#20248;&#20110;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
Meta Co-Training: Two Views are Better than One
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18083
&lt;/p&gt;
&lt;p&gt;
&#20803;&#20849;&#35757;&#32451;&#36890;&#36807;&#22312;&#25968;&#25454;&#19978;&#26500;&#24314;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#24182;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20849;&#21516;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#22330;&#26223;&#20013;&#65292;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#24456;&#22810;&#65292;&#20294;&#26631;&#31614;&#21364;&#31232;&#32570;&#19988;&#38590;&#20197;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#21322;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#25552;&#21319;&#30417;&#30563;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#24050;&#32463;&#22312;&#26368;&#36817;&#30340;&#25991;&#29486;&#20013;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#20851;&#27880;&#12290;&#20854;&#20013;&#19968;&#31181;&#20027;&#35201;&#30340;&#21322;&#30417;&#30563;&#31639;&#27861;&#26159;&#20849;&#35757;&#32451;&#12290;&#22312;&#20849;&#35757;&#32451;&#20013;&#65292;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#21033;&#29992;&#25968;&#25454;&#30340;&#19981;&#21516;&#29420;&#31435;&#21644;&#36275;&#22815;&#30340;&#8220;&#35270;&#35282;&#8221;&#26469;&#20849;&#21516;&#36827;&#34892;&#26356;&#22909;&#30340;&#39044;&#27979;&#12290;&#22312;&#20849;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27599;&#20010;&#27169;&#22411;&#22312;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#28857;&#19978;&#21019;&#24314;&#20266;&#26631;&#31614;&#65292;&#29992;&#20110;&#25913;&#36827;&#21478;&#19968;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24120;&#35265;&#24773;&#20917;&#19979;&#65292;&#24403;&#29420;&#31435;&#35270;&#35282;&#19981;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#24265;&#20215;&#22320;&#26500;&#24314;&#36825;&#20123;&#35270;&#35282;&#12290;&#22312;&#26500;&#24314;&#30340;&#35270;&#35282;&#19978;&#36827;&#34892;&#20849;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20248;&#20110;&#25105;&#20204;&#26500;&#24314;&#30340;&#20219;&#20309;&#21333;&#20010;&#35270;&#35282;&#65292;&#24182;&#19988;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#26041;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#20294;&#20855;&#26377;&#19968;&#20123;&#19981;&#21487;&#21462;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many practical computer vision scenarios unlabeled data is plentiful, but labels are scarce and difficult to obtain. As a result, semi-supervised learning which leverages unlabeled data to boost the performance of supervised classifiers have received significant attention in recent literature. One major class of semi-supervised algorithms is co-training. In co-training two different models leverage different independent and sufficient "views" of the data to jointly make better predictions. During co-training each model creates pseudo labels on unlabeled points which are used to improve the other model. We show that in the common case when independent views are not available we can construct such views inexpensively using pre-trained models. Co-training on the constructed views yields a performance improvement over any of the individual views we construct and performance comparable with recent approaches in semi-supervised learning, but has some undesirable properties. To alleviate t
&lt;/p&gt;</description></item><item><title>RO-LMM&#26159;&#19968;&#20010;&#38024;&#23545;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Consistency Embedding Fine-Tuning&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#20445;&#25345;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#21319;&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#29992;&#20110;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#21644;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2311.15876</link><description>&lt;p&gt;
LMM&#36741;&#21161;&#30340;&#19968;&#33268;&#24615;&#23884;&#20837;&#19979;&#20083;&#33146;&#30284;&#27835;&#30103;&#30446;&#26631;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
LMM-Assisted Breast Cancer Treatment Target Segmentation with Consistency Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15876
&lt;/p&gt;
&lt;p&gt;
RO-LMM&#26159;&#19968;&#20010;&#38024;&#23545;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Consistency Embedding Fine-Tuning&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#20445;&#25345;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#21319;&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#29992;&#20110;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#21644;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#28145;&#21051;&#24433;&#21709;&#20102;&#21307;&#23398;&#39046;&#22495;&#65292;&#20026;&#38477;&#20302;&#20020;&#24202;&#24037;&#20316;&#37327;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21463;&#38480;&#20110;&#25191;&#34892;&#21333;&#27169;&#24335;&#20219;&#21153;&#65292;&#19982;&#21307;&#23398;&#19987;&#19994;&#20154;&#21592;&#25152;&#20351;&#29992;&#30340;&#32508;&#21512;&#26041;&#27861;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;RO-LMM&#65292;&#19968;&#20010;&#19987;&#20026;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMM&#65289;&#12290;&#35813;&#27169;&#22411;&#28085;&#30422;&#20102;&#20020;&#24202;&#24037;&#20316;&#27969;&#20013;&#30340;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#25797;&#38271;&#20020;&#24202;&#25253;&#21578;&#25688;&#35201;&#12289;&#25918;&#30103;&#27835;&#30103;&#35745;&#21010;&#24314;&#35758;&#21644;&#35745;&#21010;&#24341;&#23548;&#30340;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;&#20026;&#20102;&#25191;&#34892;&#36830;&#32493;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#24615;&#23884;&#20837;&#24494;&#35843;&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#25552;&#21319;&#20102;LMM&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#35813;&#27010;&#24565;&#36716;&#21270;&#20026;LMM&#39537;&#21160;&#30340;&#20998;&#21106;&#26694;&#26550;&#65292;&#21363;&#19968;&#33268;&#24615;&#23884;&#20837;S&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15876v2 Announce Type: replace-cross  Abstract: Recent advancements in Artificial Intelligence (AI) have profoundly influenced medical fields, by providing tools to reduce clinical workloads. However, most AI models are constrained to execute unimodal tasks, in stark contrast to the comprehensive approaches utilized by medical professionals. To address this, here we present RO-LMM, a multi-purpose large multimodal model (LMM) tailored for the field of radiation oncology. This model covers series of tasks within clinical workflow, adept at clinical report summarization, radiation treatment plan suggestion, and plan-guided target volume segmentation. In particular, to perform consecutive clinical tasks, we further present a novel Consistency Embedding Fine-Tuning (CEFTune) technique, which boosts LMM's robustness to noisy inputs while preserving the capability of handling clean inputs, and transform this concept into LMM-driven segmentation framework as Consistency Embedding S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#27491;&#28608;&#21169;&#22122;&#22768;&#26694;&#26550;&#19979;&#30340;&#38543;&#26426;&#22122;&#22768;&#20351;&#32463;&#20856;&#27169;&#22411;&#21463;&#30410;&#65292;&#24182;&#25552;&#20986;&#20102;&#21464;&#20998;Pi-Noise&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#21407;&#22987;&#27169;&#22411;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#21644;&#31616;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.07651</link><description>&lt;p&gt;
&#21464;&#20998;&#28608;&#21169;&#22122;&#22768;&#65306;&#22122;&#22768;&#22914;&#20309;&#25913;&#36827;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Variational Positive-incentive Noise: How Noise Benefits Models. (arXiv:2306.07651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#27491;&#28608;&#21169;&#22122;&#22768;&#26694;&#26550;&#19979;&#30340;&#38543;&#26426;&#22122;&#22768;&#20351;&#32463;&#20856;&#27169;&#22411;&#21463;&#30410;&#65292;&#24182;&#25552;&#20986;&#20102;&#21464;&#20998;Pi-Noise&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#21407;&#22987;&#27169;&#22411;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#21644;&#31616;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30740;&#31350;&#26088;&#22312;&#20943;&#36731;&#30001;&#20110;&#36127;&#38754;&#22122;&#22768;&#30340;&#22522;&#26412;&#20551;&#35774;&#32780;&#23548;&#33268;&#30340;&#22122;&#22768;&#24433;&#21709;&#12290;&#20294;&#26159;&#65292;&#19968;&#20123;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#27491;&#28608;&#21169;&#22122;&#22768;&#65288;Pi-Noise&#65289;&#26694;&#26550;&#19979;&#36890;&#36807;&#38543;&#26426;&#22122;&#22768;&#20351;&#32463;&#20856;&#27169;&#22411;&#21463;&#30410;&#12290;&#30001;&#20110;Pi-Noise&#30340;&#29702;&#24819;&#30446;&#26631;&#26159;&#38590;&#20197;&#23454;&#29616;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20854;&#21464;&#20998;&#19979;&#30028;&#36827;&#34892;&#20248;&#21270;&#30340;&#21464;&#20998;Pi-Noise&#65288;VPN&#65289;&#65292;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;VPN&#29983;&#25104;&#22120;&#26469;&#22686;&#24378;&#22522;&#30784;&#27169;&#22411;&#24182;&#31616;&#21270;&#22522;&#30784;&#27169;&#22411;&#30340;&#25512;&#26029;&#65292;&#32780;&#19981;&#25913;&#21464;&#22522;&#30784;&#27169;&#22411;&#30340;&#26550;&#26500;&#12290;&#30001;&#20110;&#22522;&#30784;&#27169;&#22411;&#21644;VPN&#29983;&#25104;&#22120;&#30340;&#29420;&#31435;&#35774;&#35745;&#65292; VPN&#29983;&#25104;&#22120;&#21487;&#20197;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#19968;&#36215;&#20351;&#29992;&#12290;&#20174;&#23454;&#39564;&#32467;&#26524;&#26469;&#30475;&#65292;&#25152;&#25552;&#20986;&#30340;VPN&#29983;&#25104;&#22120;&#21487;&#20197;&#25913;&#36827;&#22522;&#26412;&#27169;&#22411;&#12290;&#20540;&#24471;&#31216;&#36190;&#30340;&#26159;&#65292;&#35757;&#32451;&#26377;&#32032;&#30340;&#21464;&#20998;VPN&#29983;&#25104;&#22120;&#26356;&#21916;&#27426;&#29420;&#31435;&#23494;&#38598;&#22411;&#22122;&#22768;&#12290;&#65288;&#32763;&#35793;&#26377;&#21024;&#20943;&#65289;
&lt;/p&gt;
&lt;p&gt;
A large number of works aim to alleviate the impact of noise due to an underlying conventional assumption of the negative role of noise. However, some existing works show that the assumption does not always hold. In this paper, we investigate how to benefit the classical models by random noise under the framework of Positive-incentive Noise (Pi-Noise). Since the ideal objective of Pi-Noise is intractable, we propose to optimize its variational bound instead, namely variational Pi-Noise (VPN). With the variational inference, a VPN generator implemented by neural networks is designed for enhancing base models and simplifying the inference of base models, without changing the architecture of base models. Benefiting from the independent design of base models and VPN generators, the VPN generator can work with most existing models. From the experiments, it is shown that the proposed VPN generator can improve the base models. It is appealing that the trained variational VPN generator prefers
&lt;/p&gt;</description></item></channel></rss>