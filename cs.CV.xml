<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SaFaRI&#30340;&#22522;&#20110;&#31354;&#38388;&#21644;&#39057;&#29575;&#24863;&#30693;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#22270;&#20687;&#24674;&#22797;&#12290;&#22312;&#21508;&#31181;&#22122;&#22768;&#36870;&#38382;&#39064;&#19978;&#65292;SaFaRI&#22312;ImageNet&#25968;&#25454;&#38598;&#21644;FFHQ&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17629</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31354;&#38388;&#21644;&#39057;&#29575;&#24863;&#30693;&#22270;&#20687;&#24674;&#22797;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spatial-and-Frequency-aware Restoration method for Images based on Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SaFaRI&#30340;&#22522;&#20110;&#31354;&#38388;&#21644;&#39057;&#29575;&#24863;&#30693;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#22270;&#20687;&#24674;&#22797;&#12290;&#22312;&#21508;&#31181;&#22122;&#22768;&#36870;&#38382;&#39064;&#19978;&#65292;SaFaRI&#22312;ImageNet&#25968;&#25454;&#38598;&#21644;FFHQ&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26368;&#36817;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#22270;&#20687;&#24674;&#22797;&#65288;IR&#65289;&#26694;&#26550;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#20860;&#23481;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;IR&#20013;&#22122;&#22768;&#36870;&#38382;&#39064;&#30340;&#26041;&#27861;&#36890;&#24120;&#20165;&#32771;&#34385;&#20687;&#32032;&#32423;&#30340;&#25968;&#25454;&#20445;&#30495;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SaFaRI&#30340;&#22522;&#20110;&#31354;&#38388;&#21644;&#39057;&#29575;&#24863;&#30693;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24102;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;IR&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#40723;&#21169;&#22270;&#20687;&#22312;&#31354;&#38388;&#21644;&#39057;&#29575;&#22495;&#20013;&#20445;&#25345;&#25968;&#25454;&#20445;&#30495;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#22122;&#22768;&#36870;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20462;&#22797;&#12289;&#38477;&#22122;&#21644;&#36229;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#30340;&#32454;&#33268;&#35780;&#20272;&#34920;&#26126;&#65292;SaFaRI&#22312;ImageNet&#25968;&#25454;&#38598;&#21644;FFHQ&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20197;LPIPS&#21644;FID&#25351;&#26631;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;IR&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have recently emerged as a promising framework for Image Restoration (IR), owing to their ability to produce high-quality reconstructions and their compatibility with established methods. Existing methods for solving noisy inverse problems in IR, considers the pixel-wise data-fidelity. In this paper, we propose SaFaRI, a spatial-and-frequency-aware diffusion model for IR with Gaussian noise. Our model encourages images to preserve data-fidelity in both the spatial and frequency domains, resulting in enhanced reconstruction quality. We comprehensively evaluate the performance of our model on a variety of noisy inverse problems, including inpainting, denoising, and super-resolution. Our thorough evaluation demonstrates that SaFaRI achieves state-of-the-art performance on both the ImageNet datasets and FFHQ datasets, outperforming existing zero-shot IR methods in terms of LPIPS and FID metrics.
&lt;/p&gt;</description></item><item><title>LSAP&#36890;&#36807;&#23545;&#28508;&#31354;&#38388;&#23454;&#29616;&#23545;&#40784;&#35299;&#20915;&#20102;&#21453;&#28436;&#21644;&#32534;&#36753;&#32467;&#26524;&#20013;&#20445;&#30495;&#24230;&#12289;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#30340;&#38382;&#39064;&#65292;&#20351;&#24471;&#22312;&#20445;&#30041;&#37325;&#24314;&#20445;&#30495;&#24230;&#30340;&#21069;&#25552;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.12746</link><description>&lt;p&gt;
LSAP: &#37325;&#26032;&#24605;&#32771;GAN&#28508;&#31354;&#38388;&#20013;&#21453;&#28436;&#30340;&#20445;&#30495;&#24230;&#12289;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;
&lt;/p&gt;
&lt;p&gt;
LSAP: Rethinking Inversion Fidelity, Perception and Editability in GAN Latent Space. (arXiv:2209.12746v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12746
&lt;/p&gt;
&lt;p&gt;
LSAP&#36890;&#36807;&#23545;&#28508;&#31354;&#38388;&#23454;&#29616;&#23545;&#40784;&#35299;&#20915;&#20102;&#21453;&#28436;&#21644;&#32534;&#36753;&#32467;&#26524;&#20013;&#20445;&#30495;&#24230;&#12289;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#30340;&#38382;&#39064;&#65292;&#20351;&#24471;&#22312;&#20445;&#30041;&#37325;&#24314;&#20445;&#30495;&#24230;&#30340;&#21069;&#25552;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#21453;&#28436;&#20027;&#35201;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#12290;&#31532;&#19968;&#27493;&#26159;&#22270;&#20687;&#23884;&#20837;&#65292;&#22312;&#36825;&#20010;&#27493;&#39588;&#20013;&#65292;&#32534;&#30721;&#22120;&#25110;&#32773;&#20248;&#21270;&#36807;&#31243;&#23884;&#20837;&#22270;&#20687;&#20197;&#33719;&#21462;&#30456;&#24212;&#30340;&#28508;&#22312;&#30721;&#12290;&#20043;&#21518;&#65292;&#31532;&#20108;&#27493;&#26088;&#22312;&#25913;&#21892;&#21453;&#28436;&#21644;&#32534;&#36753;&#32467;&#26524;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32467;&#26524;&#32454;&#21270;&#12290;&#23613;&#31649;&#31532;&#20108;&#27493;&#26174;&#33879;&#25552;&#39640;&#20102;&#20445;&#30495;&#24230;&#65292;&#20294;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#65292;&#28145;&#24230;&#20381;&#36182;&#20110;&#22312;&#31532;&#19968;&#27493;&#20013;&#33719;&#24471;&#30340;&#21453;&#21521;&#28508;&#22312;&#30721;&#12290;&#22240;&#27492;&#65292;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#22312;&#20445;&#30041;&#37325;&#24314;&#20445;&#30495;&#24230;&#30340;&#21516;&#26102;&#33719;&#24471;&#20855;&#26377;&#26356;&#22909;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#30340;&#28508;&#22312;&#30721;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25351;&#20986;&#36825;&#20004;&#20010;&#29305;&#24449;&#19982;&#21453;&#21521;&#30721;&#19982;&#21512;&#25104;&#20998;&#24067;&#30340;&#23545;&#40784;&#65288;&#25110;&#19981;&#23545;&#40784;&#65289;&#31243;&#24230;&#26377;&#20851;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#31354;&#38388;&#23545;&#40784;&#21453;&#28436;&#33539;&#20363;&#65288;LSAP&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#35780;&#20272;&#25351;&#26631;&#21644;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26631;&#20934;&#21270;&#39118;&#26684;&#31354;&#38388;&#65288;$\mathcal{S^N}$&#65289;&#21644;&#26631;&#20934;&#21270;&#20869;&#23481;&#31354;&#38388;&#65288;$\mathcal{C^N}$&#65289;&#65292;&#20998;&#21035;&#22312;&#39118;&#26684;&#21644;&#20869;&#23481;&#19978;&#23545;&#40784;&#27491;&#21521;&#21644;&#36127;&#21521;&#28508;&#22312;&#30721;&#21644;&#21512;&#25104;&#20998;&#24067;&#12290; LSAP&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20363;&#22914;&#22270;&#20687;&#32534;&#36753;&#12289;&#22270;&#20687;&#36716;&#25442;&#21644;&#22270;&#20687;&#21512;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LSAP&#20855;&#26377;&#27604;&#20197;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#29305;&#24615;&#65292;&#22914;&#25913;&#36827;&#30340;&#21487;&#32534;&#36753;&#24615;&#12289;&#35270;&#35273;&#36136;&#37327;&#21644;&#26356;&#23569;&#30340;&#27169;&#24335;&#23849;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the methods evolve, inversion is mainly divided into two steps. The first step is Image Embedding, in which an encoder or optimization process embeds images to get the corresponding latent codes. Afterward, the second step aims to refine the inversion and editing results, which we named Result Refinement. Although the second step significantly improves fidelity, perception and editability are almost unchanged, deeply dependent on inverse latent codes attained in the first step. Therefore, a crucial problem is gaining the latent codes with better perception and editability while retaining the reconstruction fidelity. In this work, we first point out that these two characteristics are related to the degree of alignment (or disalignment) of the inverse codes with the synthetic distribution. Then, we propose Latent Space Alignment Inversion Paradigm (LSAP), which consists of evaluation metric and solution for this problem. Specifically, we introduce Normalized Style Space ($\mathcal{S^N
&lt;/p&gt;</description></item></channel></rss>