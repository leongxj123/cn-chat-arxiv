<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#30340;&#28151;&#21512;SNN-ANN&#32593;&#32476;&#65292;&#21253;&#25324;&#20102;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26725;&#25509;&#27169;&#22359;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#31232;&#30095;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10173</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#30340;&#28151;&#21512;SNN-ANN&#32593;&#32476;&#65292;&#20855;&#26377;&#31354;&#38388;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Hybrid SNN-ANN Network for Event-based Object Detection with Spatial and Temporal Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10173
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#30340;&#28151;&#21512;SNN-ANN&#32593;&#32476;&#65292;&#21253;&#25324;&#20102;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26725;&#25509;&#27169;&#22359;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#31232;&#30095;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#25552;&#20379;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#21160;&#24577;&#33539;&#22260;&#65292;&#20960;&#20046;&#27809;&#26377;&#36816;&#21160;&#27169;&#31946;&#65292;&#38750;&#24120;&#36866;&#21512;&#23545;&#35937;&#26816;&#27979;&#20219;&#21153;&#12290;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19982;&#20107;&#20214;&#39537;&#21160;&#24863;&#30693;&#25968;&#25454;&#22825;&#29983;&#21305;&#37197;&#65292;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#33021;&#22815;&#23454;&#29616;&#36229;&#20302;&#21151;&#32791;&#21644;&#20302;&#24310;&#36831;&#25512;&#26029;&#65292;&#32780;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#21017;&#23637;&#31034;&#20986;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#20855;&#26377;&#26356;&#22909;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#28151;&#21512;SNN-ANN&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#21033;&#29992;SNN&#21644;ANN&#20307;&#31995;&#32467;&#26500;&#30340;&#20248;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;&#27880;&#24847;&#21147;&#30340;SNN-ANN&#39592;&#24178;&#32593;&#32476;&#65292;&#29992;&#20110;&#20351;&#29992;&#20107;&#20214;&#30456;&#26426;&#36827;&#34892;&#23545;&#35937;&#26816;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;SNN-ANN&#26725;&#25509;&#27169;&#22359;&#65292;&#20174;SNN&#23618;&#20013;&#25429;&#25417;&#31232;&#30095;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#23494;&#38598;&#29305;&#24449;&#22270;&#65292;&#20379;&#39592;&#24178;&#32593;&#32476;&#30340;ANN&#37096;&#20998;&#20351;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10173v1 Announce Type: cross  Abstract: Event cameras offer high temporal resolution and dynamic range with minimal motion blur, making them promising for object detection tasks. While Spiking Neural Networks (SNNs) are a natural match for event-based sensory data and enable ultra-energy efficient and low latency inference on neuromorphic hardware, Artificial Neural Networks (ANNs) tend to display more stable training dynamics and faster convergence resulting in greater task performance. Hybrid SNN-ANN approaches are a promising alternative, enabling to leverage the strengths of both SNN and ANN architectures. In this work, we introduce the first Hybrid Attention-based SNN-ANN backbone for object detection using event cameras. We propose a novel Attention-based SNN-ANN bridge module to capture sparse spatial and temporal relations from the SNN layer and convert them into dense feature maps for the ANN part of the backbone. Experimental results demonstrate that our proposed m
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#36879;&#26126;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#25490;&#21517;&#22270;&#20687;&#36827;&#34892;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#22823;&#21270;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2308.01196</link><description>&lt;p&gt;
&#21487;&#25345;&#32493;&#36879;&#26126;&#30340;&#25512;&#33616;&#31995;&#32479;: &#29992;&#20110;&#35299;&#37322;&#24615;&#30340;&#36125;&#21494;&#26031;&#22270;&#20687;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Sustainable Transparency in Recommender Systems: Bayesian Ranking of Images for Explainability. (arXiv:2308.01196v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#36879;&#26126;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#25490;&#21517;&#22270;&#20687;&#36827;&#34892;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#22823;&#21270;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#29616;&#20195;&#19990;&#30028;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#36890;&#24120;&#25351;&#23548;&#29992;&#25143;&#25214;&#21040;&#30456;&#20851;&#30340;&#20869;&#23481;&#25110;&#20135;&#21697;&#65292;&#24182;&#23545;&#29992;&#25143;&#21644;&#20844;&#27665;&#30340;&#20915;&#31574;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#30340;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20449;&#20219;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65307;&#20010;&#24615;&#21270;&#35299;&#37322;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#25512;&#33616;&#25552;&#20379;&#29702;&#30001;&#12290;&#22312;&#29983;&#25104;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;&#29992;&#25143;&#21019;&#24314;&#30340;&#35270;&#35273;&#20869;&#23481;&#26159;&#19968;&#20010;&#29305;&#21035;&#26377;&#28508;&#21147;&#30340;&#36873;&#39033;&#65292;&#26377;&#28508;&#21147;&#26368;&#22823;&#21270;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#35299;&#37322;&#25512;&#33616;&#26102;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#21487;&#25345;&#32493;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#32463;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23548;&#33268;&#30340;&#30899;&#25490;&#25918;&#37327;&#19982;&#23427;&#20204;&#34987;&#25972;&#21512;&#21040;&#25512;&#33616;&#31995;&#32479;&#20013;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#20351;&#29992;&#30340;&#26367;&#20195;&#23398;&#20064;&#30446;&#26631;&#19982;&#25490;&#21517;&#26368;&#26377;&#25928;&#30340;&#30446;&#26631;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender Systems have become crucial in the modern world, commonly guiding users towards relevant content or products, and having a large influence over the decisions of users and citizens. However, ensuring transparency and user trust in these systems remains a challenge; personalized explanations have emerged as a solution, offering justifications for recommendations. Among the existing approaches for generating personalized explanations, using visual content created by the users is one particularly promising option, showing a potential to maximize transparency and user trust. Existing models for explaining recommendations in this context face limitations: sustainability has been a critical concern, as they often require substantial computational resources, leading to significant carbon emissions comparable to the Recommender Systems where they would be integrated. Moreover, most models employ surrogate learning goals that do not align with the objective of ranking the most effect
&lt;/p&gt;</description></item></channel></rss>