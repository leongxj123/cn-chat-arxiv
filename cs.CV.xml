<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.02263</link><description>&lt;p&gt;
MixedNUTS: &#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#23454;&#29616;&#26080;&#38656;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02263
&lt;/p&gt;
&lt;p&gt;
MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#24448;&#24448;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#65292;&#38459;&#30861;&#20102;&#40065;&#26834;&#20998;&#31867;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#19982;&#24050;&#35757;&#32451;&#30340;&#22823;&#22411;&#39640;&#24615;&#33021;&#27169;&#22411;&#20860;&#23481;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#27492;&#38656;&#35201;&#25506;&#32034;&#26080;&#38656;&#35757;&#32451;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#40065;&#26834;&#27169;&#22411;&#22312;&#24178;&#20928;&#25968;&#25454;&#21644;&#23545;&#25239;&#25968;&#25454;&#19978;&#30340;&#27491;&#30830;&#39044;&#27979;&#27604;&#38169;&#35823;&#39044;&#27979;&#26356;&#33258;&#20449;&#65292;&#25105;&#20204;&#25512;&#27979;&#36890;&#36807;&#22686;&#24378;&#36825;&#31181;&#8220;&#33391;&#24615;&#32622;&#20449;&#24230;&#29305;&#24615;&#8221;&#21487;&#20197;&#22312;&#38598;&#25104;&#29615;&#22659;&#20013;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;MixedNUTS&#8221;&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20165;&#26377;&#19977;&#20010;&#21442;&#25968;&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#26469;&#22788;&#29702;&#40065;&#26834;&#20998;&#31867;&#22120;&#21644;&#26631;&#20934;&#38750;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;Logits&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#21518;&#65292;MixedNUTS&#23558;&#36716;&#25442;&#21518;&#30340;Logits&#36716;&#25442;&#20026;&#27010;&#29575;&#65292;&#24182;&#23558;&#23427;&#20204;&#28151;&#21512;&#20316;&#20026;&#26368;&#32456;&#30340;&#36755;&#20986;&#12290;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness often comes at the cost of degraded accuracy, impeding the real-life application of robust classification models. Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches. Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this "benign confidence property" can reconcile accuracy and robustness in an ensemble setting. To achieve so, we propose "MixedNUTS", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm. MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet da
&lt;/p&gt;</description></item><item><title>AnimateLCM&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31163;&#30340;&#19968;&#33268;&#24615;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#29983;&#25104;&#20248;&#20808;&#32423;&#21644;&#21160;&#20316;&#29983;&#25104;&#20248;&#20808;&#32423;&#30340;&#33976;&#39311;&#20998;&#31163;&#24320;&#26469;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#24182;&#22686;&#24378;&#20102;&#29983;&#25104;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.00769</link><description>&lt;p&gt;
AnimateLCM: &#20351;&#29992;&#20998;&#31163;&#30340;&#19968;&#33268;&#24615;&#23398;&#20064;&#21152;&#36895;&#20010;&#24615;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#36866;&#37197;&#22120;&#30340;&#21160;&#30011;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00769
&lt;/p&gt;
&lt;p&gt;
AnimateLCM&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31163;&#30340;&#19968;&#33268;&#24615;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#29983;&#25104;&#20248;&#20808;&#32423;&#21644;&#21160;&#20316;&#29983;&#25104;&#20248;&#20808;&#32423;&#30340;&#33976;&#39311;&#20998;&#31163;&#24320;&#26469;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#24182;&#22686;&#24378;&#20102;&#29983;&#25104;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#33021;&#22815;&#20135;&#29983;&#36830;&#36143;&#19988;&#39640;&#20445;&#30495;&#24230;&#30340;&#35270;&#39057;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36845;&#20195;&#30340;&#21435;&#22122;&#36807;&#31243;&#20351;&#20854;&#35745;&#31639;&#23494;&#38598;&#19988;&#32791;&#26102;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;&#21463;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CM&#65289;&#30340;&#21551;&#21457;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#26368;&#23567;&#30340;&#27493;&#39588;&#33976;&#39311;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20197;&#21152;&#36895;&#37319;&#26679;&#65292;&#20197;&#21450;&#20854;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#19978;&#30340;&#25104;&#21151;&#25193;&#23637;&#8212;&#8212;&#28508;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;LCM&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AnimateLCM&#65292;&#20801;&#35768;&#22312;&#26368;&#23567;&#30340;&#27493;&#39588;&#20869;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31163;&#30340;&#19968;&#33268;&#24615;&#23398;&#20064;&#31574;&#30053;&#65292;&#23558;&#22270;&#20687;&#29983;&#25104;&#20248;&#20808;&#32423;&#21644;&#21160;&#20316;&#29983;&#25104;&#20248;&#20808;&#32423;&#30340;&#33976;&#39311;&#20998;&#31163;&#24320;&#26469;&#65292;&#36825;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#24182;&#22686;&#24378;&#20102;&#29983;&#25104;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#31283;&#23450;&#30340;&#25193;&#25955;&#31038;&#21306;&#20013;&#30340;&#21363;&#25554;&#21363;&#29992;&#36866;&#37197;&#22120;&#30340;&#32452;&#21512;&#20197;&#23454;&#29616;&#21508;&#31181;&#20462;&#25913;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#36866;&#37197;&#22120;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video diffusion models has been gaining increasing attention for its ability to produce videos that are both coherent and of high fidelity. However, the iterative denoising process makes it computationally intensive and time-consuming, thus limiting its applications. Inspired by the Consistency Model (CM) that distills pretrained image diffusion models to accelerate the sampling with minimal steps and its successful extension Latent Consistency Model (LCM) on conditional image generation, we propose AnimateLCM, allowing for high-fidelity video generation within minimal steps. Instead of directly conducting consistency learning on the raw video dataset, we propose a decoupled consistency learning strategy that decouples the distillation of image generation priors and motion generation priors, which improves the training efficiency and enhance the generation visual quality. Additionally, to enable the combination of plug-and-play adapters in stable diffusion community to achieve various 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#32467;&#21512;&#20102;&#22810;&#31181;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#65292;&#24182;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01472</link><description>&lt;p&gt;
&#21453;&#21521;&#31283;&#23450;&#25193;&#25955;&#65306;&#29983;&#25104;&#35813;&#22270;&#20687;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Reverse Stable Diffusion: What prompt was used to generate this image?. (arXiv:2308.01472v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#32467;&#21512;&#20102;&#22810;&#31181;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#65292;&#24182;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65292;&#26368;&#36817;&#21560;&#24341;&#20102;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#20852;&#36259;&#65292;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;&#29983;&#25104;&#36807;&#31243;&#21644;&#22914;&#20309;&#35774;&#35745;&#25552;&#31034;&#20197;&#33719;&#24471;&#25152;&#38656;&#22270;&#20687;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#31995;&#21015;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65288;&#26377;&#21644;&#26080;&#23545;&#25193;&#25955;&#32593;&#32476;&#26435;&#37325;&#36827;&#34892;&#35775;&#38382;&#65289;&#26469;&#22788;&#29702;&#25152;&#25552;&#20986;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#32852;&#21512;&#25552;&#31034;&#22238;&#24402;&#21644;&#22810;&#26631;&#31614;&#35789;&#27719;&#20998;&#31867;&#30446;&#26631;&#65292;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#35838;&#31243;&#23398;&#20064;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#20855;&#26377;&#26356;&#20302;&#26631;&#27880;&#22122;&#22768;&#65288;&#21363;&#26356;&#22909;&#23545;&#40784;&#65289;&#30340;&#22270;&#20687;&#25552;&#31034;&#23545;&#30340;&#23398;&#20064;&#65292;&#24182;&#19988;&#20351;&#29992;&#30456;&#20284;&#24615;&#36827;&#34892;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models such as Stable Diffusion have recently attracted the interest of many researchers, and inverting the diffusion process can play an important role in better understanding the generative process and how to engineer prompts in order to obtain the desired images. To this end, we introduce the new task of predicting the text prompt given an image generated by a generative diffusion model. We combine a series of white-box and black-box models (with and without access to the weights of the diffusion network) to deal with the proposed task. We propose a novel learning framework comprising of a joint prompt regression and multi-label vocabulary classification objective that generates improved prompts. To further improve our method, we employ a curriculum learning procedure that promotes the learning of image-prompt pairs with lower labeling noise (i.e. that are better aligned), and an unsupervised domain-adaptive kernel learning method that uses the similarities b
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;DeepMSS&#27169;&#22411;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;Segmentated-to-Survival&#65288;STS&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#28176;&#36827;&#32858;&#21512;&#32593;&#32476;&#65288;MMPAN&#65289;&#26469;&#25506;&#32034;&#32959;&#30244;&#20869;&#22806;&#30340;&#39044;&#21518;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#22686;&#24378;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#36827;&#34892;&#29983;&#23384;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#20004;&#20010;&#20844;&#20849;PET/CT&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.09946</link><description>&lt;p&gt;
DeepMSS&#65306;&#22522;&#20110;PET/CT&#22270;&#20687;&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#20999;&#29255;&#21040;&#29983;&#23384;&#39044;&#27979;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DeepMSS: Deep Multi-Modality Segmentation-to-Survival Learning for Survival Outcome Prediction from PET/CT Images. (arXiv:2305.09946v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09946
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;DeepMSS&#27169;&#22411;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;Segmentated-to-Survival&#65288;STS&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#28176;&#36827;&#32858;&#21512;&#32593;&#32476;&#65288;MMPAN&#65289;&#26469;&#25506;&#32034;&#32959;&#30244;&#20869;&#22806;&#30340;&#39044;&#21518;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#22686;&#24378;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#36827;&#34892;&#29983;&#23384;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#20004;&#20010;&#20844;&#20849;PET/CT&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#39044;&#27979;&#26159;&#30284;&#30151;&#31649;&#29702;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#29992;&#20110;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#25191;&#34892;&#31471;&#21040;&#31471;&#30340;&#29983;&#23384;&#39044;&#27979;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#36890;&#36807;&#32852;&#21512;&#25191;&#34892;&#32959;&#30244;&#20998;&#21106;&#21644;&#29983;&#23384;&#39044;&#27979;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#25351;&#23548;&#27169;&#22411;&#25552;&#21462;&#19982;&#32959;&#30244;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#22312;&#25506;&#32034;&#32959;&#30244;&#22806;&#39044;&#21518;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#23616;&#37096;&#28107;&#24052;&#32467;&#36716;&#31227;&#21644;&#37051;&#36817;&#32452;&#32455;&#20405;&#34989;&#65289;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#22312;&#21033;&#29992;&#22810;&#27169;&#24577;&#22270;&#20687;&#26041;&#38754;&#27424;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepMSS&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#20999;&#29255;&#21040;&#29983;&#23384;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;Segmentated-to-Survival&#65288;STS&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#31163;&#20998;&#21106;&#21644;&#29983;&#23384;&#39044;&#27979;&#20219;&#21153;&#26469;&#36827;&#34892;&#12290;&#23545;&#20110;&#20998;&#21106;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#28176;&#36827;&#32858;&#21512;&#32593;&#32476;&#65288;MMPAN&#65289;&#26469;&#25506;&#32034;&#32959;&#30244;&#20869;&#22806;&#30340;&#39044;&#21518;&#20449;&#24687;&#12290;&#23545;&#20110;&#29983;&#23384;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#22686;&#24378;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;MMPAN&#30340;&#29305;&#24449;&#34920;&#31034;&#24182;&#25191;&#34892;&#29983;&#23384;&#39044;&#27979;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;PET/CT&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;DeepMSS&#27169;&#22411;&#22312;&#29983;&#23384;&#39044;&#27979;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival prediction is a major concern for cancer management. Deep survival models based on deep learning have been widely adopted to perform end-to-end survival prediction from medical images. Recent deep survival models achieved promising performance by jointly performing tumor segmentation with survival prediction, where the models were guided to extract tumor-related information through Multi-Task Learning (MTL). However, existing deep survival models have difficulties in exploring out-of-tumor prognostic information (e.g., local lymph node metastasis and adjacent tissue invasions). In addition, existing deep survival models are underdeveloped in utilizing multi-modality images. Empirically-designed strategies were commonly adopted to fuse multi-modality information via fixed pre-designed networks. In this study, we propose a Deep Multi-modality Segmentation-to-Survival model (DeepMSS) for survival prediction from PET/CT images. Instead of adopting MTL, we propose a novel Segmentat
&lt;/p&gt;</description></item></channel></rss>