<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#20351;&#29992;&#38750;&#36127;&#24352;&#37327;&#22240;&#23376;&#20998;&#35299;&#35299;&#20915;&#20102;&#22522;&#20110;&#38170;&#22270;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#32570;&#20047;&#32858;&#31867;&#21487;&#35299;&#37322;&#24615;&#21644;&#24573;&#35270;&#35270;&#22270;&#38388;&#20449;&#24687;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2404.00883</link><description>&lt;p&gt;
&#22522;&#20110;&#38170;&#22270;&#24352;&#37327;&#20998;&#35299;&#30340;&#21487;&#35299;&#37322;&#22810;&#35270;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Interpretable Multi-View Clustering Based on Anchor Graph Tensor Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00883
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38750;&#36127;&#24352;&#37327;&#22240;&#23376;&#20998;&#35299;&#35299;&#20915;&#20102;&#22522;&#20110;&#38170;&#22270;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#32570;&#20047;&#32858;&#31867;&#21487;&#35299;&#37322;&#24615;&#21644;&#24573;&#35270;&#35270;&#22270;&#38388;&#20449;&#24687;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38170;&#22270;&#30340;&#32858;&#31867;&#26041;&#27861;&#22240;&#20854;&#20986;&#33394;&#30340;&#32858;&#31867;&#24615;&#33021;&#21644;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23398;&#20064;&#20855;&#26377;K&#20010;&#36830;&#25509;&#32452;&#20214;&#30340;&#20108;&#37096;&#22270;&#65292;&#26377;&#21161;&#20110;&#36991;&#20813;&#21518;&#22788;&#29702;&#30340;&#38656;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22522;&#20110;&#38170;&#22270;&#22240;&#23376;&#21270;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#32570;&#20047;&#23545;&#20998;&#35299;&#30697;&#38453;&#30340;&#20805;&#20998;&#32858;&#31867;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#32463;&#24120;&#24573;&#35270;&#35270;&#22270;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#38750;&#36127;&#24352;&#37327;&#22240;&#23376;&#20998;&#35299;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#20197;&#20998;&#35299;&#32467;&#21512;&#20102;&#22810;&#35270;&#22270;&#38170;&#22270;&#30340;&#38170;&#22270;&#24352;&#37327;&#12290;&#36825;&#19968;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#21516;&#26102;&#32771;&#34385;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#33719;&#24471;&#26356;&#20855;&#35299;&#37322;&#24615;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00883v1 Announce Type: new  Abstract: The clustering method based on the anchor graph has gained significant attention due to its exceptional clustering performance and ability to process large-scale data. One common approach is to learn bipartite graphs with K-connected components, helping avoid the need for post-processing. However, this method has strict parameter requirements and may not always get K-connected components. To address this issue, an alternative approach is to directly obtain the cluster label matrix by performing non-negative matrix factorization (NMF) on the anchor graph. Nevertheless, existing multi-view clustering methods based on anchor graph factorization lack adequate cluster interpretability for the decomposed matrix and often overlook the inter-view information. We address this limitation by using non-negative tensor factorization to decompose an anchor graph tensor that combines anchor graphs from multiple views. This approach allows us to conside
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#31867;&#21644;&#35282;&#33394;&#20998;&#26512;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2404.00282</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#26597;:&#27010;&#24565;&#12289;&#20998;&#31867;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00282
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#31867;&#21644;&#35282;&#33394;&#20998;&#26512;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25317;&#26377;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#39640;&#32423;&#36890;&#29992;&#33021;&#21147;&#65292;&#23427;&#20204;&#22312;&#22686;&#24378;&#23398;&#20064;&#26041;&#38754;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#20219;&#21153;&#35268;&#21010;&#31561;&#26041;&#38754;&#23637;&#29616;&#20986;&#28508;&#21147;&#12290;&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#29616;&#26377;$\textit{LLM&#22686;&#24378;RL}$&#25991;&#29486;&#65292;&#24635;&#32467;&#20102;&#20854;&#19982;&#20256;&#32479;RL&#26041;&#27861;&#30340;&#29305;&#24449;&#65292;&#26088;&#22312;&#28548;&#28165;&#30740;&#31350;&#33539;&#22260;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#21033;&#29992;&#32463;&#20856;&#30340;Agent-&#29615;&#22659;&#20132;&#20114;&#33539;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20998;&#31867;&#27861;&#65292;&#31995;&#32479;&#22320;&#23558;LLMs&#22312;RL&#20013;&#30340;&#21151;&#33021;&#20998;&#31867;&#65292;&#21253;&#25324;&#22235;&#31181;&#35282;&#33394;&#65306;&#20449;&#24687;&#22788;&#29702;&#22120;&#12289;&#22870;&#21169;&#35774;&#35745;&#32773;&#12289;&#20915;&#31574;&#32773;&#21644;&#29983;&#25104;&#22120;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#27599;&#20010;&#35282;&#33394;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#26041;&#27861;&#35770;&#65292;&#20998;&#26512;&#20102;&#32531;&#35299;&#30340;&#29305;&#23450;RL&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#28508;&#22312;&#24212;&#29992;&#12289;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00282v1 Announce Type: cross  Abstract: With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and task planning. In this survey, we provide a comprehensive review of the existing literature in $\textit{LLM-enhanced RL}$ and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. Additionally, for each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, potential applications, prospecti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#24341;&#23548;&#30340;&#35748;&#30693;&#36827;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#23618;&#34701;&#21512;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#33258;&#28982;&#36807;&#31243;&#65292;&#26377;&#25928;&#39044;&#27979;&#21271;&#28201;&#24102;&#28246;&#27850;&#20013;&#30340;&#28342;&#35299;&#27687;&#27987;&#24230;</title><link>https://arxiv.org/abs/2403.18923</link><description>&lt;p&gt;
&#33258;&#28982;&#24341;&#23548;&#30340;&#35748;&#30693;&#36827;&#21270;&#29992;&#20110;&#39044;&#27979;&#21271;&#28201;&#24102;&#28246;&#27850;&#20013;&#30340;&#28342;&#35299;&#27687;&#27987;&#24230;
&lt;/p&gt;
&lt;p&gt;
Nature-Guided Cognitive Evolution for Predicting Dissolved Oxygen Concentrations in North Temperate Lakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18923
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#24341;&#23548;&#30340;&#35748;&#30693;&#36827;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#23618;&#34701;&#21512;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#33258;&#28982;&#36807;&#31243;&#65292;&#26377;&#25928;&#39044;&#27979;&#21271;&#28201;&#24102;&#28246;&#27850;&#20013;&#30340;&#28342;&#35299;&#27687;&#27987;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21271;&#28201;&#24102;&#28246;&#27850;&#20013;&#30340;&#28342;&#35299;&#27687;&#65288;DO&#65289;&#27987;&#24230;&#38656;&#35201;&#23545;&#19981;&#21516;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#29289;&#20505;&#27169;&#24335;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#65292;&#36825;&#20984;&#26174;&#20102;&#36873;&#25321;&#29289;&#20505;&#29305;&#24449;&#21644;&#29305;&#24449;&#20132;&#20114;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#36807;&#31243;&#30340;&#27169;&#22411;&#21463;&#37096;&#20998;&#36807;&#31243;&#30693;&#35782;&#38480;&#21046;&#25110;&#29305;&#24449;&#34920;&#31034;&#36807;&#20110;&#31616;&#21270;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26377;&#25928;&#36873;&#25321;&#19981;&#21516;&#28246;&#27850;&#31867;&#22411;&#21644;&#20219;&#21153;&#30340;&#30456;&#20851;&#29305;&#24449;&#20132;&#20114;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;DO&#25968;&#25454;&#25910;&#38598;&#19981;&#39057;&#32321;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#24341;&#23548;&#30340;&#35748;&#30693;&#36827;&#21270;&#65288;NGCE&#65289;&#31574;&#30053;&#65292;&#36825;&#20195;&#34920;&#20102;&#33258;&#36866;&#24212;&#23398;&#20064;&#19982;&#33258;&#28982;&#36807;&#31243;&#22810;&#23618;&#34701;&#21512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20195;&#35874;&#36807;&#31243;&#20026;&#22522;&#30784;&#30340;&#27169;&#22411;&#29983;&#25104;&#27169;&#25311;DO&#26631;&#31614;&#12290;&#21033;&#29992;&#36825;&#20123;&#27169;&#25311;&#26631;&#31614;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#20010;&#22810;&#31181;&#32676;&#35748;&#30693;&#36827;&#21270;&#25628;&#32034;&#65292;&#27169;&#22411;&#21453;&#26144;&#33258;&#28982;&#26377;&#26426;&#20307;&#65292;&#36866;&#24212;&#24615;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18923v1 Announce Type: cross  Abstract: Predicting dissolved oxygen (DO) concentrations in north temperate lakes requires a comprehensive study of phenological patterns across various ecosystems, which highlights the significance of selecting phenological features and feature interactions. Process-based models are limited by partial process knowledge or oversimplified feature representations, while machine learning models face challenges in efficiently selecting relevant feature interactions for different lake types and tasks, especially under the infrequent nature of DO data collection. In this paper, we propose a Nature-Guided Cognitive Evolution (NGCE) strategy, which represents a multi-level fusion of adaptive learning with natural processes. Specifically, we utilize metabolic process-based models to generate simulated DO labels. Using these simulated labels, we implement a multi-population cognitive evolutionary search, where models, mirroring natural organisms, adaptiv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#26041;&#27861;CB-Norm&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21152;&#36895;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.16798</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#23618;
&lt;/p&gt;
&lt;p&gt;
Cluster-Based Normalization Layer for Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16798
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#26041;&#27861;CB-Norm&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21152;&#36895;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#20869;&#37096;&#21327;&#21464;&#37327;&#28418;&#31227;&#12289;&#26631;&#31614;&#28418;&#31227;&#12289;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#12289;&#36807;&#25311;&#21512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#20256;&#32479;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#22914;&#25209;&#26631;&#20934;&#21270;&#65292;&#26088;&#22312;&#35299;&#20915;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#20294;&#36890;&#24120;&#20381;&#36182;&#20110;&#38480;&#21046;&#20854;&#36866;&#24212;&#24615;&#30340;&#20551;&#35774;&#12290;&#28151;&#21512;&#35268;&#33539;&#21270;&#22312;&#22788;&#29702;&#22810;&#20010;&#39640;&#26031;&#20998;&#24067;&#26102;&#38754;&#20020;&#35745;&#31639;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#32858;&#31867;&#30340;&#35268;&#33539;&#21270;&#65288;CB-Norm&#65289;&#30340;&#20004;&#20010;&#21464;&#20307;&#8212;&#8212;&#30417;&#30563;&#24335;&#22522;&#20110;&#32858;&#31867;&#30340;&#35268;&#33539;&#21270;&#65288;SCB-Norm&#65289;&#21644;&#26080;&#30417;&#30563;&#24335;&#22522;&#20110;&#32858;&#31867;&#30340;&#35268;&#33539;&#21270;&#65288;UCB-Norm&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#19968;&#27493;&#35268;&#33539;&#21270;&#26041;&#27861;&#12290;CB-Norm&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26469;&#19987;&#38376;&#35299;&#20915;&#19982;&#26799;&#24230;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21152;&#36895;&#26377;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16798v1 Announce Type: cross  Abstract: Deep learning faces significant challenges during the training of neural networks, including internal covariate shift, label shift, vanishing/exploding gradients, overfitting, and computational complexity. While conventional normalization methods, such as Batch Normalization, aim to tackle some of these issues, they often depend on assumptions that constrain their adaptability. Mixture Normalization faces computational hurdles in its pursuit of handling multiple Gaussian distributions.   This paper introduces Cluster-Based Normalization (CB-Norm) in two variants - Supervised Cluster-Based Normalization (SCB-Norm) and Unsupervised Cluster-Based Normalization (UCB-Norm) - proposing a groundbreaking one-step normalization approach. CB-Norm leverages a Gaussian mixture model to specifically address challenges related to gradient stability and learning acceleration.   For SCB-Norm, a supervised variant, the novel mechanism involves introduc
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#26377;&#38480;&#35282;&#24230;&#23618;&#26512;&#25104;&#20687;&#20013;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#33021;&#26356;&#31283;&#23450;&#22320;&#37325;&#26500;&#26356;&#22810;&#20449;&#24687;</title><link>https://arxiv.org/abs/2403.11350</link><description>&lt;p&gt;
&#26377;&#38480;&#35282;&#24230;&#23618;&#26512;&#25104;&#20687;&#20013;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustness of the data-driven approach in limited angle tomography
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11350
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#26377;&#38480;&#35282;&#24230;&#23618;&#26512;&#25104;&#20687;&#20013;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#33021;&#26356;&#31283;&#23450;&#22320;&#37325;&#26500;&#26356;&#22810;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#35282;&#24230;Radon&#21464;&#25442;&#30001;&#20110;&#20854;&#19981;&#36870;&#38382;&#39064;&#32780;&#38395;&#21517;&#20110;&#19990;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#35299;&#37322;&#65292;&#21363;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#21487;&#20197;&#20197;&#26356;&#31283;&#23450;&#30340;&#26041;&#24335;&#37325;&#26500;&#26356;&#22810;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11350v1 Announce Type: cross  Abstract: The limited angle Radon transform is notoriously difficult to invert due to the ill-posedness. In this work, we give a mathematical explanation that the data-driven approach based on deep neural networks can reconstruct more information in a stable way compared to traditional methods.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#24046;&#20998;&#38544;&#31169;&#34920;&#31034;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#22270;&#20687;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#21644;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.02506</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Representation Learning via Image Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02506
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#24046;&#20998;&#38544;&#31169;&#34920;&#31034;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#22270;&#20687;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#21644;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26426;&#22120;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#20174;&#25935;&#24863;&#25968;&#25454;&#20013;&#35757;&#32451;&#27169;&#22411;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#30340;&#40644;&#37329;&#26631;&#20934;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#19968;&#29702;&#24819;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#20854;&#27425;&#20248;&#30340;&#38544;&#31169;-&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#22312;DP&#34920;&#31034;&#23398;&#20064;&#20013;&#29305;&#21035;&#26126;&#26174;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24050;&#32463;&#35777;&#26126;&#22312;&#36866;&#24230;&#30340;&#38544;&#31169;&#39044;&#31639;&#19979;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#23398;&#20064;&#30340;&#34920;&#31034;&#24182;&#19981;&#27604;&#25163;&#24037;&#29305;&#24449;&#26174;&#33879;&#26356;&#22909;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#21644;&#25193;&#23637;&#21040;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;DP&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#24037;&#31243;&#25216;&#24039;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#20351;&#29992;&#21487;&#35266;&#30340;&#35745;&#31639;&#37327;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#20102;DP&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120;&#65288;DP-Cap&#65289;&#22312;&#26469;&#33258;LAION-2B&#30340;233M&#23376;&#38598;&#19978;&#65292;&#24182;&#33719;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#35270;&#35273;&#21644;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02506v1 Announce Type: cross  Abstract: Differentially private (DP) machine learning is considered the gold-standard solution for training a model from sensitive data while still preserving privacy. However, a major barrier to achieving this ideal is its sub-optimal privacy-accuracy trade-off, which is particularly visible in DP representation learning. Specifically, it has been shown that under modest privacy budgets, most models learn representations that are not significantly better than hand-crafted features. In this work, we show that effective DP representation learning can be done via image captioning and scaling up to internet-scale multimodal datasets. Through a series of engineering tricks, we successfully train a DP image captioner (DP-Cap) on a 233M subset of LAION-2B from scratch using a reasonable amount of computation, and obtaining unprecedented high-quality image features that can be used in a variety of downstream vision and vision-language tasks. For examp
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26597;&#35810;&#23398;&#20064;&#20855;&#26377;&#23450;&#26102;&#22120;&#30340;Mealy&#26426;&#22120;&#30340;&#31639;&#27861;&#65292;&#22312;&#23454;&#29616;&#19978;&#26126;&#26174;&#27604;&#24050;&#26377;&#31639;&#27861;&#26356;&#26377;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.02019</link><description>&lt;p&gt;
&#20855;&#26377;&#23450;&#26102;&#22120;&#30340;Mealy&#26426;&#22120;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning of Mealy Machines with Timers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02019
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26597;&#35810;&#23398;&#20064;&#20855;&#26377;&#23450;&#26102;&#22120;&#30340;Mealy&#26426;&#22120;&#30340;&#31639;&#27861;&#65292;&#22312;&#23454;&#29616;&#19978;&#26126;&#26174;&#27604;&#24050;&#26377;&#31639;&#27861;&#26356;&#26377;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#40657;&#30418;&#29615;&#22659;&#20013;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#26597;&#35810;&#23398;&#20064;&#19968;&#33324;&#31867;&#21035;&#30340;&#20855;&#26377;&#23450;&#26102;&#22120;&#30340;Mealy&#26426;&#22120;&#65288;MMTs&#65289;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;Vaandrager&#31561;&#20154;&#30340;L&#65283;&#31639;&#27861;&#23545;&#23450;&#26102;&#35774;&#32622;&#30340;&#25193;&#23637;&#12290;&#31867;&#20284;&#20110;Waga&#25552;&#20986;&#30340;&#29992;&#20110;&#23398;&#20064;&#23450;&#26102;&#33258;&#21160;&#26426;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21463;&#21040;Maler&#65286;Pnueli&#24605;&#24819;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;Waga&#30340;&#31639;&#27861;&#37117;&#20351;&#29992;&#31526;&#21495;&#26597;&#35810;&#36827;&#34892;&#22522;&#30784;&#35821;&#35328;&#23398;&#20064;&#65292;&#28982;&#21518;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#20855;&#20307;&#26597;&#35810;&#36827;&#34892;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;Waga&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#20855;&#20307;&#26597;&#35810;&#26469;&#23454;&#29616;&#21333;&#20010;&#31526;&#21495;&#26597;&#35810;&#65292;&#32780;&#25105;&#20204;&#21482;&#38656;&#35201;&#22810;&#39033;&#24335;&#25968;&#37327;&#12290;&#36825;&#26159;&#22240;&#20026;&#35201;&#23398;&#20064;&#23450;&#26102;&#33258;&#21160;&#26426;&#65292;&#23398;&#20064;&#32773;&#38656;&#35201;&#30830;&#23450;&#27599;&#20010;&#36716;&#25442;&#30340;&#30830;&#20999;&#21355;&#20853;&#21644;&#37325;&#32622;&#65288;&#26377;&#25351;&#25968;&#22810;&#31181;&#21487;&#33021;&#24615;&#65289;&#65292;&#32780;&#35201;&#23398;&#20064;MMT&#65292;&#23398;&#20064;&#32773;&#21482;&#38656;&#35201;&#24324;&#28165;&#26970;&#21738;&#20123;&#20808;&#21069;&#30340;&#36716;&#25442;&#23548;&#33268;&#36229;&#26102;&#12290;&#27491;&#22914;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#25152;&#31034;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02019v1 Announce Type: cross  Abstract: We present the first algorithm for query learning of a general class of Mealy machines with timers (MMTs) in a black-box context. Our algorithm is an extension of the L# algorithm of Vaandrager et al. to a timed setting. Like the algorithm for learning timed automata proposed by Waga, our algorithm is inspired by ideas of Maler &amp; Pnueli. Based on the elementary languages of, both Waga's and our algorithm use symbolic queries, which are then implemented using finitely many concrete queries. However, whereas Waga needs exponentially many concrete queries to implement a single symbolic query, we only need a polynomial number. This is because in order to learn a timed automaton, a learner needs to determine the exact guard and reset for each transition (out of exponentially many possibilities), whereas for learning an MMT a learner only needs to figure out which of the preceding transitions caused a timeout. As shown in our previous work, 
&lt;/p&gt;</description></item><item><title>DE-DeepONet&#36890;&#36807;&#25972;&#21512;&#23548;&#25968;&#20449;&#24687;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#30456;&#27604;&#20256;&#32479;DeepONet&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.19242</link><description>&lt;p&gt;
&#28145;&#24230;&#23548;&#25968;&#22686;&#24378;&#30340;&#25805;&#20316;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Derivative-enhanced Deep Operator Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19242
&lt;/p&gt;
&lt;p&gt;
DE-DeepONet&#36890;&#36807;&#25972;&#21512;&#23548;&#25968;&#20449;&#24687;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#30456;&#27604;&#20256;&#32479;DeepONet&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#25805;&#20316;&#32593;&#32476;&#65288;DeepONets&#65289;&#26159;&#19968;&#31867;&#23398;&#20064;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#26144;&#23556;&#30340;&#31070;&#32463;&#31639;&#23376;&#65292;&#26368;&#36817;&#34987;&#24320;&#21457;&#20026;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23548;&#25968;&#22686;&#24378;&#30340;&#28145;&#24230;&#25805;&#20316;&#32593;&#32476;&#65288;DE-DeepONet&#65289;&#65292;&#23427;&#21033;&#29992;&#23548;&#25968;&#20449;&#24687;&#22686;&#24378;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#23548;&#25968;&#36817;&#20284;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;DE-DeepONet&#23558;&#36755;&#20837;&#30340;&#32500;&#24230;&#32553;&#20943;&#21040;DeepONet&#20013;&#65292;&#24182;&#22312;&#35757;&#32451;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#21253;&#21547;&#20004;&#31181;&#31867;&#22411;&#30340;&#23548;&#25968;&#26631;&#31614;&#65292;&#21363;&#20851;&#20110;&#36755;&#20837;&#20989;&#25968;&#30340;&#36755;&#20986;&#20989;&#25968;&#30340;&#26041;&#21521;&#23548;&#25968;&#21644;&#20851;&#20110;&#29289;&#29702;&#22495;&#21464;&#37327;&#30340;&#36755;&#20986;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#26029;&#22686;&#21152;&#22797;&#26434;&#24230;&#30340;&#26041;&#31243;&#19978;&#27979;&#35797;DE-DeepONet&#65292;&#20197;&#23637;&#31034;&#20854;&#30456;&#23545;&#20110;&#26222;&#36890;DeepONet&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19242v1 Announce Type: new  Abstract: Deep operator networks (DeepONets), a class of neural operators that learn mappings between function spaces, have recently been developed as surrogate models for parametric partial differential equations (PDEs). In this work we propose a derivative-enhanced deep operator network (DE-DeepONet), which leverages the derivative information to enhance the prediction accuracy, and provide a more accurate approximation of the derivatives, especially when the training data are limited. DE-DeepONet incorporates dimension reduction of input into DeepONet and includes two types of derivative labels in the loss function for training, that is, the directional derivatives of the output function with respect to the input function and the gradient of the output function with respect to the physical domain variables. We test DE-DeepONet on three different equations with increasing complexity to demonstrate its effectiveness compared to the vanilla DeepON
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#38646;&#38454;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#20811;&#26381;&#20102;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#37319;&#26679;&#20013;&#30340;&#20122;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#37319;&#26679;&#31934;&#24230;&#20855;&#26377;&#20498;&#22810;&#39033;&#24335;&#20381;&#36182;&#12290;</title><link>https://arxiv.org/abs/2402.17886</link><description>&lt;p&gt;
&#29992;&#20110;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#30340;&#38646;&#38454;&#37319;&#26679;&#26041;&#27861;&#65306;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#32531;&#35299;&#20122;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#38646;&#38454;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#20811;&#26381;&#20102;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#37319;&#26679;&#20013;&#30340;&#20122;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#37319;&#26679;&#31934;&#24230;&#20855;&#26377;&#20498;&#22810;&#39033;&#24335;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32771;&#34385;&#20102;&#22522;&#20110;&#20854;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#26410;&#24402;&#19968;&#21270;&#23494;&#24230;&#26597;&#35810;&#30340;&#37319;&#26679;&#38382;&#39064;&#12290;&#39318;&#20808;&#25551;&#36848;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#25311;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#21363;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#65288;DMC&#65289;&#65292;&#20854;&#24471;&#20998;&#20989;&#25968;&#36890;&#36807;&#36890;&#29992;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#22120;&#36924;&#36817;&#12290;DMC&#26159;&#19968;&#20010;&#22522;&#20110;&#31070;&#35861;&#30340;&#20803;&#31639;&#27861;&#65292;&#20854;&#20013;&#31070;&#35861;&#26159;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#29983;&#25104;&#33945;&#29305;&#21345;&#27931;&#20998;&#25968;&#20272;&#35745;&#22120;&#30340;&#26679;&#26412;&#30340;&#35775;&#38382;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#25298;&#32477;&#37319;&#26679;&#30340;&#36825;&#20010;&#31070;&#35861;&#30340;&#23454;&#29616;&#65292;&#36825;&#23558;DMC&#36716;&#21270;&#20026;&#19968;&#20010;&#30495;&#27491;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#38646;&#38454;&#25193;&#25955;&#33945;&#29305;&#21345;&#27931;&#65288;ZOD-MC&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21363;DMC&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#32780;&#19981;&#20551;&#35774;&#30446;&#26631;&#20998;&#24067;&#20026;&#23545;&#25968;&#20985;&#25110;&#28385;&#36275;&#20219;&#20309;&#31561;&#21608;&#19981;&#31561;&#24335;&#65292;&#25552;&#20379;&#20102;&#25910;&#25947;&#20998;&#26512;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;ZOD-MC&#23545;&#25152;&#38656;&#37319;&#26679;&#31934;&#24230;&#20855;&#26377;&#20498;&#22810;&#39033;&#24335;&#20381;&#36182;&#65292;&#23613;&#31649;&#20173;&#28982;&#21463;&#21040;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17886v1 Announce Type: cross  Abstract: This paper considers the problem of sampling from non-logconcave distribution, based on queries of its unnormalized density. It first describes a framework, Diffusion Monte Carlo (DMC), based on the simulation of a denoising diffusion process with its score function approximated by a generic Monte Carlo estimator. DMC is an oracle-based meta-algorithm, where its oracle is the assumed access to samples that generate a Monte Carlo score estimator. Then we provide an implementation of this oracle, based on rejection sampling, and this turns DMC into a true algorithm, termed Zeroth-Order Diffusion Monte Carlo (ZOD-MC). We provide convergence analyses by first constructing a general framework, i.e. a performance guarantee for DMC, without assuming the target distribution to be log-concave or satisfying any isoperimetric inequality. Then we prove that ZOD-MC admits an inverse polynomial dependence on the desired sampling accuracy, albeit sti
&lt;/p&gt;</description></item><item><title>&#26080;&#38170;&#32858;&#31867;&#26041;&#27861;AFCAGF&#36890;&#36807;&#23398;&#20064;&#38170;&#22270;&#24182;&#20248;&#21270;&#25104;&#23545;&#26679;&#26412;&#36317;&#31163;&#65292;&#36991;&#20813;&#20102;&#38170;&#28857;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#30340;&#38656;&#35201;&#65292;&#25552;&#21319;&#20102;&#32858;&#31867;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15688</link><description>&lt;p&gt;
&#22522;&#20110;&#38170;&#22270;&#22240;&#23376;&#20998;&#35299;&#30340;&#26080;&#38170;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Anchor-free Clustering based on Anchor Graph Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15688
&lt;/p&gt;
&lt;p&gt;
&#26080;&#38170;&#32858;&#31867;&#26041;&#27861;AFCAGF&#36890;&#36807;&#23398;&#20064;&#38170;&#22270;&#24182;&#20248;&#21270;&#25104;&#23545;&#26679;&#26412;&#36317;&#31163;&#65292;&#36991;&#20813;&#20102;&#38170;&#28857;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#30340;&#38656;&#35201;&#65292;&#25552;&#21319;&#20102;&#32858;&#31867;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38170;&#28857;&#26041;&#27861;&#26159;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#32858;&#31867;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#21253;&#25324;&#20004;&#20010;&#19981;&#21516;&#38454;&#27573;&#65306;&#36873;&#25321;&#38170;&#28857;&#21644;&#26500;&#24314;&#38170;&#22270;&#12290;&#36825;&#31181;&#20108;&#20998;&#20197;&#21450;&#38170;&#28857;&#30340;&#21021;&#22987;&#21270;&#26174;&#33879;&#24433;&#21709;&#31639;&#27861;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#38170;&#22270;&#22240;&#23376;&#20998;&#35299;&#30340;&#26080;&#38170;&#32858;&#31867;&#65288;AFCAGF&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;AFCAGF&#22312;&#23398;&#20064;&#38170;&#22270;&#26041;&#38754;&#20855;&#26377;&#21019;&#26032;&#24615;&#65292;&#21482;&#38656;&#35201;&#35745;&#31639;&#26679;&#26412;&#20043;&#38388;&#30340;&#25104;&#23545;&#36317;&#31163;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#36807;&#31243;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#26126;&#30830;&#36873;&#25321;&#38170;&#28857;&#30340;&#24517;&#35201;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#27169;&#31946;k&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#65288;FKM&#65289;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#65292;&#26080;&#38656;&#21021;&#22987;&#21270;&#32858;&#31867;&#20013;&#24515;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#35760;&#24518;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15688v1 Announce Type: new  Abstract: Anchor-based methods are a pivotal approach in handling clustering of large-scale data. However, these methods typically entail two distinct stages: selecting anchor points and constructing an anchor graph. This bifurcation, along with the initialization of anchor points, significantly influences the overall performance of the algorithm. To mitigate these issues, we introduce a novel method termed Anchor-free Clustering based on Anchor Graph Factorization (AFCAGF). AFCAGF innovates in learning the anchor graph, requiring only the computation of pairwise distances between samples. This process, achievable through straightforward optimization, circumvents the necessity for explicit selection of anchor points. More concretely, our approach enhances the Fuzzy k-means clustering algorithm (FKM), introducing a new manifold learning technique that obviates the need for initializing cluster centers. Additionally, we evolve the concept of the mem
&lt;/p&gt;</description></item><item><title>NeuralThink &#26159;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15393</link><description>&lt;p&gt;
NeuralThink: &#22312;&#19968;&#33324;&#20219;&#21153;&#20013;&#36827;&#34892;&#22806;&#25512;&#30340;&#31639;&#27861;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15393
&lt;/p&gt;
&lt;p&gt;
NeuralThink &#26159;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25797;&#38271;&#27169;&#24335;&#35782;&#21035;&#65292;&#20294;&#22312;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#26041;&#24335;&#19978;&#22788;&#29702;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#26102;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#24605;&#32500;&#26041;&#27861;&#23637;&#29616;&#20102;&#23398;&#20064;&#21487;&#20197;&#22806;&#25512;&#30340;&#31639;&#27861;&#30340;&#28508;&#21147;&#65306;&#22312;&#36739;&#23567;&#30340;&#29615;&#22659;&#20013;&#23398;&#20064;&#24182;&#22312;&#36739;&#22823;&#30340;&#29615;&#22659;&#20013;&#25191;&#34892;&#23398;&#21040;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#23616;&#38480;&#20110;&#23545;&#31216;&#20219;&#21153;&#65292;&#21363;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#32500;&#24230;&#30456;&#21516;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; NeuralThink&#65292;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#20854;&#20013;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#32500;&#24230;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19981;&#23545;&#31216;&#20219;&#21153;&#22806;&#25512;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; NeuralThink &#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#19968;&#30452;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15393v1 Announce Type: cross  Abstract: While machine learning methods excel at pattern recognition, they struggle with complex reasoning tasks in a scalable, algorithmic manner. Recent Deep Thinking methods show promise in learning algorithms that extrapolate: learning in smaller environments and executing the learned algorithm in larger environments. However, these works are limited to symmetrical tasks, where the input and output dimensionalities are the same. To address this gap, we propose NeuralThink, a new recurrent architecture that can consistently extrapolate to both symmetrical and asymmetrical tasks, where the dimensionality of the input and output are different. We contribute with a novel benchmark of asymmetrical tasks for extrapolation. We show that NeuralThink consistently outperforms the prior state-of-the-art Deep Thinking architectures, in regards to stable extrapolation to large observations from smaller training sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#32531;&#23384;&#26041;&#27861;&#65292;&#36890;&#36807;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;SMDP&#65289;&#36866;&#24212;&#29616;&#23454;&#22330;&#26223;&#20013;&#38543;&#26426;&#35831;&#27714;&#21040;&#36798;&#30340;&#29305;&#24615;&#65292;&#32508;&#21512;&#32771;&#34385;&#21508;&#31181;&#25991;&#20214;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.14576</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#36793;&#32536;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Edge Caching Based on Deep Reinforcement Learning and Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#32531;&#23384;&#26041;&#27861;&#65292;&#36890;&#36807;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;SMDP&#65289;&#36866;&#24212;&#29616;&#23454;&#22330;&#26223;&#20013;&#38543;&#26426;&#35831;&#27714;&#21040;&#36798;&#30340;&#29305;&#24615;&#65292;&#32508;&#21512;&#32771;&#34385;&#21508;&#31181;&#25991;&#20214;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#32593;&#32476;&#20013;&#20887;&#20313;&#25968;&#25454;&#20256;&#36755;&#26085;&#30410;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;&#27969;&#37327;&#28608;&#22686;&#24050;&#32463;&#20351;&#20013;&#32487;&#38142;&#36335;&#21644;&#39592;&#24178;&#32593;&#32476;&#25215;&#21387;&#65292;&#20419;&#20351;&#23545;&#36793;&#32536;&#36335;&#30001;&#22120;&#30340;&#32531;&#23384;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25506;&#32034;&#12290;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#20381;&#36182;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#22788;&#29702;&#32531;&#23384;&#38382;&#39064;&#65292;&#20551;&#35774;&#22266;&#23450;&#26102;&#38388;&#38388;&#38548;&#30340;&#20915;&#31574;&#65307;&#28982;&#32780;&#65292;&#29616;&#23454;&#22330;&#26223;&#28041;&#21450;&#38543;&#26426;&#35831;&#27714;&#21040;&#36798;&#65292;&#23613;&#31649;&#21508;&#31181;&#25991;&#20214;&#29305;&#24449;&#22312;&#30830;&#23450;&#26368;&#20339;&#32531;&#23384;&#31574;&#30053;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#30456;&#20851;&#30340;&#29616;&#26377;&#24037;&#20316;&#24182;&#26410;&#32771;&#34385;&#25152;&#26377;&#36825;&#20123;&#25991;&#20214;&#29305;&#24449;&#26469;&#24418;&#25104;&#32531;&#23384;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#39318;&#20808;&#25105;&#20204;&#21033;&#29992;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;SMDP&#65289;&#26469;&#24314;&#27169;&#32531;&#23384;&#38382;&#39064;&#65292;&#20197;&#36866;&#24212;&#29616;&#23454;&#22330;&#26223;&#30340;&#36830;&#32493;&#26102;&#38388;&#29305;&#24615;&#65292;&#20801;&#35768;&#22312;&#25991;&#20214;&#35831;&#27714;&#26102;&#38543;&#26426;&#36827;&#34892;&#32531;&#23384;&#20915;&#31574;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#32531;&#23384;&#26041;&#27861;&#65292;&#20840;&#38754;&#32771;&#34385;&#20102;&#19981;&#21516;&#25991;&#20214;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14576v1 Announce Type: cross  Abstract: This paper addresses the escalating challenge of redundant data transmission in networks. The surge in traffic has strained backhaul links and backbone networks, prompting the exploration of caching solutions at the edge router. Existing work primarily relies on Markov Decision Processes (MDP) for caching issues, assuming fixed-time interval decisions; however, real-world scenarios involve random request arrivals, and despite the critical role of various file characteristics in determining an optimal caching policy, none of the related existing work considers all these file characteristics in forming a caching policy. In this paper, first, we formulate the caching problem using a semi-Markov Decision Process (SMDP) to accommodate the continuous-time nature of real-world scenarios allowing for caching decisions at random times upon file requests. Then, we propose a double deep Q-learning-based caching approach that comprehensively accou
&lt;/p&gt;</description></item><item><title>&#32447;&#24615;&#21464;&#25442;&#22120;&#23637;&#31034;&#20102;&#22312;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#21644;&#22122;&#38899;&#24178;&#25200;&#25968;&#25454;&#26102;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#36890;&#36807;&#21457;&#29616;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36229;&#36234;&#20102;&#35768;&#22810;&#21512;&#29702;&#30340;&#22522;&#32447;&#12290;</title><link>https://arxiv.org/abs/2402.14180</link><description>&lt;p&gt;
&#32447;&#24615;&#21464;&#25442;&#22120;&#26159;&#22810;&#21151;&#33021;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Linear Transformers are Versatile In-Context Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14180
&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#21464;&#25442;&#22120;&#23637;&#31034;&#20102;&#22312;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#21644;&#22122;&#38899;&#24178;&#25200;&#25968;&#25454;&#26102;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#36890;&#36807;&#21457;&#29616;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36229;&#36234;&#20102;&#35768;&#22810;&#21512;&#29702;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21464;&#25442;&#22120;&#65292;&#29305;&#21035;&#26159;&#32447;&#24615;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#22312;&#21069;&#21521;&#25512;&#29702;&#27493;&#39588;&#20013;&#23545;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#25968;&#25454;&#38544;&#21547;&#22320;&#25191;&#34892;&#31867;&#20284;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#20219;&#20309;&#32447;&#24615;&#21464;&#25442;&#22120;&#37117;&#20445;&#25345;&#38544;&#24335;&#32447;&#24615;&#27169;&#22411;&#65292;&#24182;&#21487;&#35299;&#37322;&#20026;&#25191;&#34892;&#19968;&#31181;&#21464;&#24418;&#30340;&#39044;&#26465;&#20214;&#26799;&#24230;&#19979;&#38477;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#21464;&#25442;&#22120;&#22312;&#35757;&#32451;&#25968;&#25454;&#21463;&#21040;&#19981;&#21516;&#27700;&#24179;&#22122;&#38899;&#24178;&#25200;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#32447;&#24615;&#21464;&#25442;&#22120;&#21457;&#29616;&#20102;&#19968;&#31181;&#22797;&#26434;&#19988;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36229;&#36234;&#25110;&#19982;&#35768;&#22810;&#21512;&#29702;&#22522;&#32447;&#30340;&#34920;&#29616;&#30456;&#21305;&#25932;&#12290;&#25105;&#20204;&#21453;&#21521;&#24037;&#31243;&#20102;&#36825;&#20010;&#31639;&#27861;&#65292;&#24182;&#34920;&#26126;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;&#21644;&#22122;&#38899;&#27700;&#24179;&#30340;&#33258;&#36866;&#24212;&#37325;&#32553;&#25918;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14180v1 Announce Type: new  Abstract: Recent research has demonstrated that transformers, particularly linear attention models, implicitly execute gradient-descent-like algorithms on data provided in-context during their forward inference step. However, their capability in handling more complex problems remains unexplored. In this paper, we prove that any linear transformer maintains an implicit linear model and can be interpreted as performing a variant of preconditioned gradient descent. We also investigate the use of linear transformers in a challenging scenario where the training data is corrupted with different levels of noise. Remarkably, we demonstrate that for this problem linear transformers discover an intricate and highly effective optimization algorithm, surpassing or matching in performance many reasonable baselines. We reverse-engineer this algorithm and show that it is a novel approach incorporating momentum and adaptive rescaling based on noise levels. Our fi
&lt;/p&gt;</description></item><item><title>&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#22256;&#38590;&#24615;&#20855;&#26377;&#32039;&#20945;&#30340;&#26377;&#38480;&#29305;&#24615;&#34920;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.10360</link><description>&lt;p&gt;
&#23398;&#20064;&#24615;&#26159;&#19968;&#31181;&#32039;&#20945;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Learnability is a Compact Property
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10360
&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#22256;&#38590;&#24615;&#20855;&#26377;&#32039;&#20945;&#30340;&#26377;&#38480;&#29305;&#24615;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#23398;&#20064;&#30340;&#24037;&#20316;&#21462;&#24471;&#20102;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#32467;&#26524;&#65306;&#21508;&#31181;&#38382;&#39064;&#30340;&#21487;&#23398;&#20064;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#21028;&#23450;&#30340;&#65292;&#25110;&#32773;&#19982;&#26631;&#20934;&#38598;&#21512;&#35770;ZFC&#20844;&#29702;&#26080;&#20851;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#38382;&#39064;&#30340;&#21487;&#23398;&#20064;&#24615;&#21487;&#33021;&#19981;&#26159;&#20855;&#26377;&#26377;&#38480;&#29305;&#24615;&#30340;&#23646;&#24615;&#65306;&#38750;&#27491;&#24335;&#22320;&#35828;&#65292;&#23427;&#19981;&#33021;&#36890;&#36807;&#26816;&#26597;&#38382;&#39064;&#30340;&#26377;&#38480;&#25237;&#24433;&#26469;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10360v1 Announce Type: new  Abstract: Recent work on learning has yielded a striking result: the learnability of various problems can be undecidable, or independent of the standard ZFC axioms of set theory. Furthermore, the learnability of such problems can fail to be a property of finite character: informally, it cannot be detected by examining finite projections of the problem.   On the other hand, learning theory abounds with notions of dimension that characterize learning and consider only finite restrictions of the problem, i.e., are properties of finite character. How can these results be reconciled? More precisely, which classes of learning problems are vulnerable to logical undecidability, and which are within the grasp of finite characterizations?   We demonstrate that the difficulty of supervised learning with metric losses admits a tight finite characterization. In particular, we prove that the sample complexity of learning a hypothesis class can be detected by ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#22240;&#24341;&#23548;GFlowNet (Genetic GFN) &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#36845;&#20195;&#36951;&#20256;&#25628;&#32034;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;16.213&#30340;&#26368;&#26032;&#24471;&#20998;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20339;&#24471;&#20998;15.185&#65292;&#21516;&#26102;&#22312;14&#20010;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#25152;&#26377;&#23545;&#27604;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05961</link><description>&lt;p&gt;
&#22522;&#22240;&#24341;&#23548;GFlowNets&#65306;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270;&#22522;&#20934;&#26041;&#38754;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Genetic-guided GFlowNets: Advancing in Practical Molecular Optimization Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#22240;&#24341;&#23548;GFlowNet (Genetic GFN) &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#36845;&#20195;&#36951;&#20256;&#25628;&#32034;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;16.213&#30340;&#26368;&#26032;&#24471;&#20998;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20339;&#24471;&#20998;15.185&#65292;&#21516;&#26102;&#22312;14&#20010;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#25152;&#26377;&#23545;&#27604;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GFlowNet&#21464;&#20307;&#65292;&#21363;&#22522;&#22240;&#24341;&#23548;GFlowNet (Genetic GFN)&#65292;&#23427;&#23558;&#36845;&#20195;&#36951;&#20256;&#25628;&#32034;&#38598;&#25104;&#21040;GFlowNet&#20013;&#12290;&#36951;&#20256;&#25628;&#32034;&#26377;&#25928;&#22320;&#24341;&#23548;GFlowNet&#36827;&#20837;&#39640;&#22238;&#25253;&#21306;&#22495;&#65292;&#35299;&#20915;&#20102;&#20840;&#23616;&#36807;&#24230;&#25506;&#32034;&#23548;&#33268;&#30340;&#35757;&#32451;&#25928;&#29575;&#20302;&#19979;&#21644;&#25506;&#32034;&#26377;&#38480;&#21306;&#22495;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#35757;&#32451;&#31574;&#30053;&#65292;&#22914;&#22522;&#20110;&#25490;&#21517;&#30340;&#37325;&#25918;&#35757;&#32451;&#21644;&#26080;&#30417;&#30563;&#26368;&#22823;&#20284;&#28982;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#22522;&#22240;&#24341;&#23548;GFlowNet&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270; (PMO) &#39046;&#22495;&#30340;&#23448;&#26041;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20102;16.213&#30340;&#26368;&#26032;&#24471;&#20998;&#65292;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#27979;&#35797;&#20013;&#25253;&#21578;&#30340;&#26368;&#20339;&#24471;&#20998;15.185&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;23&#20010;&#20219;&#21153;&#20013;&#30340;14&#20010;&#20219;&#21153;&#20013;&#36229;&#36807;&#20102;&#25152;&#26377;&#23545;&#27604;&#26041;&#27861;&#65292;&#21253;&#25324;&#24378;&#21270;&#23398;&#20064;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#29983;&#25104;&#27169;&#22411;&#65292;GFlowNets&#21644;&#36951;&#20256;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel variant of GFlowNet, genetic-guided GFlowNet (Genetic GFN), which integrates an iterative genetic search into GFlowNet. Genetic search effectively guides the GFlowNet to high-rewarded regions, addressing global over-exploration that results in training inefficiency and exploring limited regions. In addition, training strategies, such as rank-based replay training and unsupervised maximum likelihood pre-training, are further introduced to improve the sample efficiency of Genetic GFN. The proposed method shows a state-of-the-art score of 16.213, significantly outperforming the reported best score in the benchmark of 15.185, in practical molecular optimization (PMO), which is an official benchmark for sample-efficient molecular optimization. Remarkably, ours exceeds all baselines, including reinforcement learning, Bayesian optimization, generative models, GFlowNets, and genetic algorithms, in 14 out of 23 tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#23545;&#35282;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#20272;&#35745;&#22120;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#20998;&#26512;&#21644;&#25968;&#20540;&#30740;&#31350;&#65292;&#21457;&#29616;&#26041;&#24046;&#37327;&#21462;&#20915;&#20110;&#38750;&#32447;&#24615;&#19982;&#19981;&#21516;&#21442;&#25968;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24212;&#35813;&#22312;&#20272;&#35745;&#36153;&#33293;&#23572;&#20449;&#24687;&#26102;&#20104;&#20197;&#37325;&#35270;&#12290;</title><link>https://arxiv.org/abs/2402.05379</link><description>&lt;p&gt;
&#23545;&#35282;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#20272;&#35745;&#22120;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Tradeoffs of Diagonal Fisher Information Matrix Estimators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#23545;&#35282;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#20272;&#35745;&#22120;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#20998;&#26512;&#21644;&#25968;&#20540;&#30740;&#31350;&#65292;&#21457;&#29616;&#26041;&#24046;&#37327;&#21462;&#20915;&#20110;&#38750;&#32447;&#24615;&#19982;&#19981;&#21516;&#21442;&#25968;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24212;&#35813;&#22312;&#20272;&#35745;&#36153;&#33293;&#23572;&#20449;&#24687;&#26102;&#20104;&#20197;&#37325;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#25551;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#23616;&#37096;&#20960;&#20309;&#24615;&#36136;&#65292;&#23427;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#24037;&#20855;&#26469;&#29702;&#35299;&#21644;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#12290;&#37492;&#20110;&#20854;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#23454;&#36341;&#32773;&#36890;&#24120;&#20351;&#29992;&#38543;&#26426;&#20272;&#35745;&#22120;&#65292;&#24182;&#20165;&#35780;&#20272;&#23545;&#35282;&#32447;&#26465;&#30446;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#36825;&#26679;&#30340;&#20272;&#35745;&#22120;&#65292;&#20854;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#22797;&#26434;&#24615;&#21462;&#20915;&#20110;&#23427;&#20204;&#20851;&#32852;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#26041;&#24046;&#30340;&#30028;&#38480;&#65292;&#24182;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#32593;&#32476;&#20013;&#23454;&#20363;&#21270;&#23427;&#20204;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#21644;&#25968;&#20540;&#30740;&#31350;&#26469;&#26435;&#34913;&#36825;&#20004;&#20010;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#21457;&#29616;&#26041;&#24046;&#37327;&#21462;&#20915;&#20110;&#20851;&#20110;&#19981;&#21516;&#21442;&#25968;&#32452;&#30340;&#38750;&#32447;&#24615;&#65292;&#24403;&#20272;&#35745;&#36153;&#33293;&#23572;&#20449;&#24687;&#26102;&#19981;&#33021;&#24573;&#35270;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Fisher information matrix characterizes the local geometry in the parameter space of neural networks. It elucidates insightful theories and useful tools to understand and optimize neural networks. Given its high computational cost, practitioners often use random estimators and evaluate only the diagonal entries. We examine two such estimators, whose accuracy and sample complexity depend on their associated variances. We derive bounds of the variances and instantiate them in regression and classification networks. We navigate trade-offs of both estimators based on analytical and numerical studies. We find that the variance quantities depend on the non-linearity with respect to different parameter groups and should not be neglected when estimating the Fisher information.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#36890;&#29992;LM&#23545;&#40784;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#26126;&#30830;&#27880;&#37322;&#30340;&#22870;&#21169;&#25968;&#25454;&#65292;&#24182;&#19988;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#23545;&#40784;&#29702;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.05369</link><description>&lt;p&gt;
&#20197;&#26174;&#24335;&#22870;&#21169;&#30340;&#22122;&#22768;&#23545;&#27604;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Noise Contrastive Alignment of Language Models with Explicit Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#36890;&#29992;LM&#23545;&#40784;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#26126;&#30830;&#27880;&#37322;&#30340;&#22870;&#21169;&#25968;&#25454;&#65292;&#24182;&#19988;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#23545;&#40784;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#24847;&#22270;&#36890;&#24120;&#34987;&#24418;&#24335;&#21270;&#20026;&#38656;&#35201;&#22312;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26102;&#26368;&#22823;&#21270;&#30340;&#35780;&#20272;&#22870;&#21169;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#22914;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#65288;DPO&#65289;&#65292;&#20027;&#35201;&#36866;&#29992;&#20110;&#38544;&#21547;&#23450;&#20041;&#32780;&#38750;&#26126;&#30830;&#32473;&#23450;&#22870;&#21169;&#30340;&#20004;&#20004;&#20559;&#22909;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;LM&#23545;&#40784;&#26694;&#26550;&#65292;&#21033;&#29992;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#26469;&#35299;&#20915;&#26126;&#30830;&#27880;&#37322;&#26377;&#26631;&#37327;&#35780;&#20272;&#30340;&#22870;&#21169;&#25968;&#25454;&#22788;&#29702;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#24182;&#34892;&#31639;&#27861;&#65292;NCA&#21644;InfoNCA&#65292;&#20004;&#32773;&#37117;&#33021;&#20174;&#22870;&#21169;&#25968;&#25454;&#21644;&#20559;&#22909;&#25968;&#25454;&#20013;&#30452;&#25509;&#25552;&#21462;LM&#31574;&#30053;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DPO&#25439;&#22833;&#26159;&#25105;&#20204;&#25552;&#20986;&#30340;InfoNCA&#30446;&#26631;&#22312;&#20004;&#20004;&#20559;&#22909;&#35774;&#32622;&#19979;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20174;&#32780;&#38598;&#25104;&#21644;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#23545;&#40784;&#29702;&#35770;&#12290;&#36890;&#36807;&#23545;&#27604;NCA&#21644;InfoNCA&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;InfoNCA&#21644;DPO&#22914;&#20309;&#22312;&#19981;&#21516;&#21709;&#24212;&#23545;&#20110;&#21333;&#20010;&#25351;&#20196;&#30340;&#30456;&#23545;&#21487;&#33021;&#24615;&#19978;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
User intentions are typically formalized as evaluation rewards to be maximized when fine-tuning language models (LMs). Existing alignment methods, such as Direct Preference Optimization (DPO), are mainly tailored for pairwise preference data where rewards are implicitly defined rather than explicitly given. In this paper, we introduce a general framework for LM alignment, leveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling reward datasets explicitly annotated with scalar evaluations. Our framework comprises two parallel algorithms, NCA and InfoNCA, both enabling the direct extraction of an LM policy from reward data as well as preference data. Notably, we show that the DPO loss is a special case of our proposed InfoNCA objective under pairwise preference settings, thereby integrating and extending current alignment theories. By contrasting NCA and InfoNCA, we show that InfoNCA and DPO adjust relative likelihood across different responses to a single instruction,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20808;&#39564;&#65292;&#31216;&#20026;&#22810;&#26679;&#21270;&#22359;&#31232;&#30095;&#20808;&#39564;&#65292;&#29992;&#26469;&#25551;&#36848;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#24191;&#27867;&#22359;&#31232;&#30095;&#29616;&#35937;&#12290;&#36890;&#36807;&#20801;&#35768;&#26041;&#24046;&#21644;&#30456;&#20851;&#30697;&#38453;&#30340;&#22810;&#26679;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22359;&#31232;&#30095;&#23398;&#20064;&#26041;&#27861;&#23545;&#39044;&#23450;&#20041;&#22359;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#30340;&#22359;&#31232;&#30095;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;(DivSBL)&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#22359;&#20272;&#35745;&#65292;&#20943;&#36731;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#24182;&#24314;&#31435;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#26368;&#20248;&#24615;&#29702;&#35770;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;DivSBL&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.04646</link><description>&lt;p&gt;
&#23398;&#20064;&#26469;&#33258;&#22359;&#31232;&#30095;&#20449;&#21495;&#30340;&#22810;&#26679;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning with Diversification from Block Sparse Signal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20808;&#39564;&#65292;&#31216;&#20026;&#22810;&#26679;&#21270;&#22359;&#31232;&#30095;&#20808;&#39564;&#65292;&#29992;&#26469;&#25551;&#36848;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#24191;&#27867;&#22359;&#31232;&#30095;&#29616;&#35937;&#12290;&#36890;&#36807;&#20801;&#35768;&#26041;&#24046;&#21644;&#30456;&#20851;&#30697;&#38453;&#30340;&#22810;&#26679;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22359;&#31232;&#30095;&#23398;&#20064;&#26041;&#27861;&#23545;&#39044;&#23450;&#20041;&#22359;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#30340;&#22359;&#31232;&#30095;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;(DivSBL)&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#22359;&#20272;&#35745;&#65292;&#20943;&#36731;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#24182;&#24314;&#31435;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#26368;&#20248;&#24615;&#29702;&#35770;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;DivSBL&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20808;&#39564;Diversified Block Sparse Prior&#65292;&#26469;&#25551;&#36848;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#24191;&#27867;&#22359;&#31232;&#30095;&#29616;&#35937;&#12290;&#36890;&#36807;&#20801;&#35768;&#26041;&#24046;&#21644;&#30456;&#20851;&#30697;&#38453;&#30340;&#22810;&#26679;&#21270;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#29616;&#26377;&#22359;&#31232;&#30095;&#23398;&#20064;&#26041;&#27861;&#23545;&#39044;&#23450;&#20041;&#22359;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#22359;&#20272;&#35745;&#65292;&#21516;&#26102;&#20943;&#36731;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#30340;&#22359;&#31232;&#30095;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;(DivSBL)&#65292;&#21033;&#29992;EM&#31639;&#27861;&#21644;&#23545;&#20598;&#19978;&#21319;&#27861;&#36827;&#34892;&#36229;&#21442;&#25968;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#26368;&#20248;&#24615;&#29702;&#35770;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;DivSBL&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel prior called Diversified Block Sparse Prior to characterize the widespread block sparsity phenomenon in real-world data. By allowing diversification on variance and correlation matrix, we effectively address the sensitivity issue of existing block sparse learning methods to pre-defined block information, which enables adaptive block estimation while mitigating the risk of overfitting. Based on this, a diversified block sparse Bayesian learning method (DivSBL) is proposed, utilizing EM algorithm and dual ascent method for hyperparameter estimation. Moreover, we establish the global and local optimality theory of our model. Experiments validate the advantages of DivSBL over existing algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OLS-OFU&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#29305;&#24449;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#22312;&#32447;&#35774;&#32622;&#20013;&#30340;&#26631;&#31614;&#36716;&#31227;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;OLS-OFU&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#36716;&#31227;&#26465;&#20214;&#19979;&#37117;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03545</link><description>&lt;p&gt;
&#22312;&#32447;&#29305;&#24449;&#26356;&#26032;&#25913;&#21892;&#22312;&#32447;&#65288;&#24191;&#20041;&#65289;&#26631;&#31614;&#36716;&#31227;&#36866;&#24212;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Feature Updates Improve Online (Generalized) Label Shift Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OLS-OFU&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#29305;&#24449;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#22312;&#32447;&#35774;&#32622;&#20013;&#30340;&#26631;&#31614;&#36716;&#31227;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;OLS-OFU&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#36716;&#31227;&#26465;&#20214;&#19979;&#37117;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#35774;&#32622;&#20013;&#26631;&#31614;&#36716;&#31227;&#30340;&#26222;&#36941;&#38382;&#39064;&#65292;&#20854;&#20013;&#23384;&#22312;&#32570;&#22833;&#30340;&#26631;&#31614;&#65292;&#25968;&#25454;&#20998;&#24067;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#21450;&#26102;&#33719;&#24471;&#26631;&#31614;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#35843;&#25972;&#25110;&#26356;&#26032;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#26368;&#21518;&#19968;&#23618;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#25913;&#36827;&#29305;&#24449;&#34920;&#31034;&#30340;&#26410;&#34987;&#21457;&#25496;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#32447;&#26631;&#31614;&#36716;&#31227;&#33258;&#36866;&#24212;&#19982;&#22312;&#32447;&#29305;&#24449;&#26356;&#26032;&#65288;OLS-OFU&#65289;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#20248;&#21270;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#36827;&#39044;&#27979;&#27169;&#22411;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#23454;&#65292;OLS-OFU&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#29305;&#24449;&#20248;&#21270;&#65292;&#20943;&#23569;&#20102;&#31639;&#27861;&#36951;&#25022;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#22312;&#22312;&#32447;&#26631;&#31614;&#36716;&#31227;&#21644;&#24191;&#20041;&#26631;&#31614;&#36716;&#31227;&#26465;&#20214;&#19979;&#65292;&#24378;&#35843;&#20102;OLS-OFU&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#39046;&#22495;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the prevalent issue of label shift in an online setting with missing labels, where data distributions change over time and obtaining timely labels is challenging. While existing methods primarily focus on adjusting or updating the final layer of a pre-trained classifier, we explore the untapped potential of enhancing feature representations using unlabeled data at test-time. Our novel method, Online Label Shift adaptation with Online Feature Updates (OLS-OFU), leverages self-supervised learning to refine the feature extraction process, thereby improving the prediction model. Theoretical analyses confirm that OLS-OFU reduces algorithmic regret by capitalizing on self-supervised learning for feature refinement. Empirical studies on various datasets, under both online label shift and generalized label shift conditions, underscore the effectiveness and robustness of OLS-OFU, especially in cases of domain shifts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;Agda&#29983;&#24577;&#31995;&#32479;&#21040;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20381;&#36182;&#31867;&#22411;&#32534;&#31243;&#35821;&#35328;&#30340;&#35777;&#26126;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#32780;&#38750;&#21629;&#21517;&#21407;&#21017;&#30340;&#26032;&#39062;&#31070;&#32463;&#26550;&#26500;&#65292;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#22320;&#34920;&#31034;&#20381;&#36182;&#31867;&#22411;&#31243;&#24207;&#65292;&#24182;&#22312;&#21069;&#25552;&#36873;&#25321;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02104</link><description>&lt;p&gt;
&#23398;&#20064;&#20381;&#36182;&#31867;&#22411;&#30340;&#32467;&#26500;&#24863;&#30693;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Structure-Aware Representations of Dependent Types
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;Agda&#29983;&#24577;&#31995;&#32479;&#21040;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20381;&#36182;&#31867;&#22411;&#32534;&#31243;&#35821;&#35328;&#30340;&#35777;&#26126;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#32780;&#38750;&#21629;&#21517;&#21407;&#21017;&#30340;&#26032;&#39062;&#31070;&#32463;&#26550;&#26500;&#65292;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#22320;&#34920;&#31034;&#20381;&#36182;&#31867;&#22411;&#31243;&#24207;&#65292;&#24182;&#22312;&#21069;&#25552;&#36873;&#25321;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Agda&#26159;&#19968;&#31181;&#20381;&#36182;&#31867;&#22411;&#32534;&#31243;&#35821;&#35328;&#21644;&#35777;&#26126;&#21161;&#25163;&#65292;&#22312;&#35777;&#26126;&#24418;&#24335;&#21270;&#21644;&#32534;&#31243;&#35821;&#35328;&#29702;&#35770;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#23558;Agda&#29983;&#24577;&#31995;&#32479;&#25193;&#23637;&#21040;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#24182;&#21453;&#36807;&#26469;&#20351;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#33021;&#22815;&#20351;&#29992;Agda&#30456;&#20851;&#36164;&#28304;&#12290;&#25105;&#20204;&#20171;&#32461;&#24182;&#21457;&#24067;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Agda&#31243;&#24207;&#35777;&#26126;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26082;&#35814;&#23613;&#21448;&#24191;&#27867;&#65292;&#21487;&#20197;&#25903;&#25345;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#36825;&#26159;&#39318;&#20010;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#36229;&#39640;&#20998;&#36776;&#29575;&#65292;&#35814;&#32454;&#23637;&#31034;&#20102;&#20122;&#22411;&#32423;&#21035;&#30340;&#35777;&#26126;&#29366;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#32780;&#19981;&#26159;&#21629;&#21517;&#21407;&#21017;&#20934;&#30830;&#34920;&#31034;&#20381;&#36182;&#31867;&#22411;&#31243;&#24207;&#30340;&#26032;&#39062;&#31070;&#32463;&#26550;&#26500;&#12290;&#25105;&#20204;&#22312;&#21069;&#25552;&#36873;&#25321;&#35774;&#32622;&#20013;&#23454;&#20363;&#21270;&#21644;&#35780;&#20272;&#25105;&#20204;&#30340;&#26550;&#26500;&#65292;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agda is a dependently-typed programming language and a proof assistant, pivotal in proof formalization and programming language theory. This paper extends the Agda ecosystem into machine learning territory, and, vice versa, makes Agda-related resources available to machine learning practitioners. We introduce and release a novel dataset of Agda program-proofs that is elaborate and extensive enough to support various machine learning applications -- the first of its kind. Leveraging the dataset's ultra-high resolution, detailing proof states at the sub-type level, we propose a novel neural architecture targeted at faithfully representing dependently-typed programs on the basis of structural rather than nominal principles. We instantiate and evaluate our architecture in a premise selection setup, where it achieves strong initial results.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;MDPs&#30340;&#21442;&#25968;&#21270;&#36890;&#29992;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#22312;&#20445;&#35777;&#20302;&#36951;&#25022;&#30340;&#24773;&#20917;&#19979;&#31649;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#12290;&#31639;&#27861;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20854;&#30446;&#26631;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#21453;&#22343;&#20026; $\tilde{\mathcal{O}}({T}^{3/4})$&#12290;</title><link>https://arxiv.org/abs/2402.02042</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#21407;&#22987;-&#23545;&#20598;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#23545;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;MDP&#36827;&#34892;&#21442;&#25968;&#21270;&#36890;&#29992;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02042
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;MDPs&#30340;&#21442;&#25968;&#21270;&#36890;&#29992;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#22312;&#20445;&#35777;&#20302;&#36951;&#25022;&#30340;&#24773;&#20917;&#19979;&#31649;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#12290;&#31639;&#27861;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20854;&#30446;&#26631;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#21453;&#22343;&#20026; $\tilde{\mathcal{O}}({T}^{3/4})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22238;&#25253;&#21463;&#38480;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#30340;&#39046;&#22495;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#30740;&#31350;&#20855;&#26377;&#36890;&#29992;&#31574;&#30053;&#21442;&#25968;&#21270;&#30340;&#24179;&#22343;&#22238;&#25253;CMDP&#30340;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#35268;&#20998;&#26512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;&#23545;&#20598;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#31649;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#30830;&#20445;&#20302;&#36951;&#25022;&#20445;&#35777;&#20197;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#30446;&#26631;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#21453;&#19978;&#20855;&#26377; $\tilde{\mathcal{O}}({T}^{3/4})$ &#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the realm of infinite horizon average reward Constrained Markov Decision Processes (CMDP). To the best of our knowledge, this work is the first to delve into the regret and constraint violation analysis of average reward CMDPs with a general policy parametrization. To address this challenge, we propose a primal dual based policy gradient algorithm that adeptly manages the constraints while ensuring a low regret guarantee toward achieving a global optimal policy. In particular, we demonstrate that our proposed algorithm achieves $\tilde{\mathcal{O}}({T}^{3/4})$ objective regret and $\tilde{\mathcal{O}}({T}^{3/4})$ constraint violation bounds.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36817;&#30830;&#23450;&#24615;&#22238;&#24402;&#20013;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#21644;&#25511;&#21046;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01810</link><description>&lt;p&gt;
&#36817;&#30830;&#23450;&#24615;&#22238;&#24402;&#20013;&#30340;&#38169;&#35823;&#35268;&#33539;&#21270;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Misspecification uncertainties in near-deterministic regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01810
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36817;&#30830;&#23450;&#24615;&#22238;&#24402;&#20013;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#21644;&#25511;&#21046;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26399;&#26395;&#25439;&#22833;&#26159;&#27169;&#22411;&#27867;&#21270;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#21487;&#29992;&#20110;&#23398;&#20064;&#30340;&#40065;&#26834;PAC-Bayes&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#25439;&#22833;&#26368;&#23567;&#21270;&#34987;&#35748;&#20026;&#24573;&#30053;&#20102;&#38169;&#35823;&#35268;&#33539;&#21270;&#65292;&#21363;&#27169;&#22411;&#19981;&#33021;&#23436;&#20840;&#22797;&#21046;&#35266;&#27979;&#32467;&#26524;&#12290;&#36825;&#23548;&#33268;&#22823;&#25968;&#25454;&#25110;&#27424;&#21442;&#25968;&#21270;&#26497;&#38480;&#19979;&#23545;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#26174;&#33879;&#20302;&#20272;&#12290;&#25105;&#20204;&#20998;&#26512;&#36817;&#30830;&#23450;&#24615;&#12289;&#38169;&#35823;&#35268;&#33539;&#21270;&#21644;&#27424;&#21442;&#25968;&#21270;&#26367;&#20195;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#36825;&#26159;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#24191;&#27867;&#30456;&#20851;&#30340;&#19968;&#20010;&#39046;&#22495;&#12290;&#25105;&#20204;&#35777;&#26126;&#21518;&#39564;&#20998;&#24067;&#24517;&#39035;&#35206;&#30422;&#27599;&#20010;&#35757;&#32451;&#28857;&#65292;&#20197;&#36991;&#20813;&#21457;&#25955;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#23548;&#20986;&#19968;&#20010;&#31526;&#21512;&#36825;&#20010;&#32422;&#26463;&#30340;&#32452;&#21512;&#27169;&#22411;&#12290;&#23545;&#20110;&#32447;&#24615;&#27169;&#22411;&#65292;&#36825;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#39069;&#22806;&#24320;&#38144;&#26368;&#23567;&#12290;&#36825;&#31181;&#39640;&#25928;&#26041;&#27861;&#22312;&#27169;&#22411;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#21407;&#23376;&#23610;&#24230;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expected loss is an upper bound to the model generalization error which admits robust PAC-Bayes bounds for learning. However, loss minimization is known to ignore misspecification, where models cannot exactly reproduce observations. This leads to significant underestimates of parameter uncertainties in the large data, or underparameterized, limit. We analyze the generalization error of near-deterministic, misspecified and underparametrized surrogate models, a regime of broad relevance in science and engineering. We show posterior distributions must cover every training point to avoid a divergent generalization error and derive an ensemble {ansatz} that respects this constraint, which for linear models incurs minimal overhead. The efficient approach is demonstrated on model problems before application to high dimensional datasets in atomistic machine learning. Parameter uncertainties from misspecification survive in the underparametrized limit, giving accurate prediction and boundin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#31639;&#27861;&#31867;&#65292;&#29992;&#20110;&#24178;&#39044;&#33539;&#22260;&#20869;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#28085;&#30422;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#36716;&#21270;&#12290;&#31639;&#27861;&#20445;&#35777;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#21019;&#36896;&#24615;&#22320;&#23558;&#24471;&#20998;&#20989;&#25968;&#19982;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.00849</link><description>&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65306;&#32447;&#24615;&#21644;&#19968;&#33324;&#30340;&#36716;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score-based Causal Representation Learning: Linear and General Transformations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00849
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#31639;&#27861;&#31867;&#65292;&#29992;&#20110;&#24178;&#39044;&#33539;&#22260;&#20869;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#28085;&#30422;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#36716;&#21270;&#12290;&#31639;&#27861;&#20445;&#35777;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#21019;&#36896;&#24615;&#22320;&#23558;&#24471;&#20998;&#20989;&#25968;&#19982;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#38024;&#23545;&#19968;&#33324;&#38750;&#21442;&#25968;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#21644;&#23558;&#28508;&#22312;&#21464;&#37327;&#26144;&#23556;&#21040;&#35266;&#27979;&#21464;&#37327;&#30340;&#26410;&#30693;&#36716;&#21270;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#24178;&#39044;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65288;CRL&#65289;&#12290;&#30740;&#31350;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#30340;&#36716;&#21270;&#12290;&#36825;&#31687;&#35770;&#25991;&#21516;&#26102;&#35752;&#35770;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#20004;&#20010;&#26041;&#38754;&#12290;&#21487;&#35782;&#21035;&#24615;&#26159;&#25351;&#30830;&#23450;&#31639;&#27861;&#19981;&#30456;&#20851;&#30340;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#24674;&#22797;&#30495;&#23454;&#30340;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#21644;&#28508;&#22312;&#22240;&#26524;&#22270;&#12290;&#23454;&#29616;&#24615;&#26159;&#25351;&#31639;&#27861;&#26041;&#38754;&#65292;&#35299;&#20915;&#35774;&#35745;&#31639;&#27861;&#26469;&#23454;&#29616;&#21487;&#35782;&#21035;&#20445;&#35777;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#24471;&#20998;&#20989;&#25968;&#65288;&#21363;&#23494;&#24230;&#20989;&#25968;&#23545;&#25968;&#30340;&#26799;&#24230;&#65289;&#19982;CRL&#20043;&#38388;&#24314;&#31435;&#26032;&#32852;&#31995;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#24471;&#20998;&#20026;&#22522;&#30784;&#30340;&#31639;&#27861;&#31867;&#65292;&#30830;&#20445;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#19987;&#27880;&#20110;&#32447;&#24615;&#36716;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#27599;&#20010;n&#20010;&#38543;&#26426;&#30828;&#24178;&#39044;&#19979;&#35813;&#36716;&#21270;&#30340;&#22240;&#26524;&#34920;&#31034;&#21487;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. Linear and general transformations are investigated. The paper addresses both the \emph{identifiability} and \emph{achievability} aspects. Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them. Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. By drawing novel connections between \emph{score functions} (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a \emph{score-based class of algorithms} that ensures both identifiability and achievability. First, the paper focuses on \emph{linear} transformations and shows that one stochastic hard intervention per n
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#32435;&#20837;&#31639;&#27861;&#39044;&#27979;&#20013;&#65292;&#37325;&#28857;&#22312;&#20110;&#21033;&#29992;&#20154;&#30340;&#21028;&#26029;&#21147;&#21306;&#20998;&#23545;&#20110;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#31639;&#27861;&#26469;&#35828;&#8220;&#30475;&#36215;&#26469;&#30456;&#21516;&#8221;&#30340;&#36755;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.00793</link><description>&lt;p&gt;
&#26080;&#27861;&#21306;&#20998;&#30340;&#21306;&#20998;&#65306;&#31639;&#27861;&#39044;&#27979;&#20013;&#30340;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distinguishing the Indistinguishable: Human Expertise in Algorithmic Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#32435;&#20837;&#31639;&#27861;&#39044;&#27979;&#20013;&#65292;&#37325;&#28857;&#22312;&#20110;&#21033;&#29992;&#20154;&#30340;&#21028;&#26029;&#21147;&#21306;&#20998;&#23545;&#20110;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#31639;&#27861;&#26469;&#35828;&#8220;&#30475;&#36215;&#26469;&#30456;&#21516;&#8221;&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#32435;&#20837;&#31639;&#27861;&#39044;&#27979;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21033;&#29992;&#20154;&#30340;&#21028;&#26029;&#21147;&#26469;&#21306;&#20998;&#37027;&#20123;&#23545;&#20110;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#31639;&#27861;&#26469;&#35828;&#8220;&#30475;&#36215;&#26469;&#30456;&#21516;&#8221;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#26694;&#26550;&#33021;&#22815;&#28548;&#28165;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#19987;&#23478;&#36890;&#24120;&#20855;&#26377;&#20449;&#24687;&#30340;&#35775;&#38382;&#26435;&#38480;&#8212;&#8212;&#29305;&#21035;&#26159;&#20027;&#35266;&#20449;&#24687;&#8212;&#8212;&#32780;&#36825;&#20123;&#20449;&#24687;&#26159;&#31639;&#27861;&#35757;&#32451;&#25968;&#25454;&#20013;&#27809;&#26377;&#32534;&#30721;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#35748;&#35782;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#32452;&#26377;&#21407;&#21017;&#30340;&#31639;&#27861;&#65292;&#20165;&#22312;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#26377;&#25152;&#25913;&#21892;&#26102;&#25165;&#36873;&#25321;&#24615;&#22320;&#32435;&#20837;&#20154;&#31867;&#21453;&#39304;&#12290;&#32463;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#31639;&#27861;&#22312;&#24179;&#22343;&#27700;&#24179;&#19978;&#24448;&#24448;&#20248;&#20110;&#20154;&#31867;&#23545;&#24212;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#20154;&#31867;&#21028;&#26029;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65288;&#21487;&#20197;&#39044;&#20808;&#30830;&#23450;&#65289;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31639;&#27861;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;X&#23556;&#32447;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#23376;&#38598;&#22312;&#24739;&#32773;&#32676;&#20307;&#20013;&#21344;&#25454;&#20102;&#36817;30%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#24335;&#65292;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel framework for incorporating human expertise into algorithmic predictions. Our approach focuses on the use of human judgment to distinguish inputs which `look the same' to any feasible predictive algorithm. We argue that this framing clarifies the problem of human/AI collaboration in prediction tasks, as experts often have access to information -- particularly subjective information -- which is not encoded in the algorithm's training data. We use this insight to develop a set of principled algorithms for selectively incorporating human feedback only when it improves the performance of any feasible predictor. We find empirically that although algorithms often outperform their human counterparts on average, human judgment can significantly improve algorithmic predictions on specific instances (which can be identified ex-ante). In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population. Our approach provides a natural way
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#22522;&#20110;&#27169;&#26495;&#21644;&#38750;&#27169;&#26495;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#27979;&#26041;&#27861;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#27169;&#22411;&#25490;&#21517;&#12289;&#32477;&#23545;&#24471;&#20998;&#21644;&#19982;&#22256;&#24785;&#24230;&#30340;&#20851;&#31995;&#31561;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.00123</link><description>&lt;p&gt;
&#27604;&#36739;&#22522;&#20110;&#27169;&#26495;&#21644;&#38750;&#27169;&#26495;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Comparing Template-based and Template-free Language Model Probing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#22522;&#20110;&#27169;&#26495;&#21644;&#38750;&#27169;&#26495;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#27979;&#26041;&#27861;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#27169;&#22411;&#25490;&#21517;&#12289;&#32477;&#23545;&#24471;&#20998;&#21644;&#19982;&#22256;&#24785;&#24230;&#30340;&#20851;&#31995;&#31561;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#19987;&#23478;&#21046;&#20316;&#30340;&#27169;&#26495;&#21644;&#33258;&#28982;&#21457;&#29983;&#30340;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#27169;&#22411;&#25506;&#27979;&#26041;&#27861;&#30340;&#24046;&#24322;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;16&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;10&#20010;&#33521;&#25991;&#25506;&#27979;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#21253;&#25324;4&#20010;&#22522;&#20110;&#27169;&#26495;&#30340;&#21644;6&#20010;&#38750;&#27169;&#26495;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#38024;&#23545;&#20197;&#19979;&#30740;&#31350;&#38382;&#39064;&#36827;&#34892;&#20102;&#22238;&#31572;&#65306;&#65288;RQ1&#65289;&#27169;&#22411;&#25490;&#21517;&#22312;&#20004;&#31181;&#26041;&#27861;&#20013;&#26159;&#21542;&#19981;&#21516;&#65311;&#65288;RQ2&#65289;&#27169;&#22411;&#30340;&#32477;&#23545;&#24471;&#20998;&#22312;&#20004;&#31181;&#26041;&#27861;&#20013;&#26159;&#21542;&#19981;&#21516;&#65311;&#65288;RQ3&#65289;RQ1&#21644;RQ2&#30340;&#31572;&#26696;&#22312;&#19968;&#33324;&#21644;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#20043;&#38388;&#26159;&#21542;&#19981;&#21516;&#65311;&#25105;&#20204;&#30340;&#21457;&#29616;&#26159;&#65306;1&#65289;&#38500;&#20102;&#39030;&#32423;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#22806;&#65292;&#22522;&#20110;&#27169;&#26495;&#21644;&#38750;&#27169;&#26495;&#26041;&#27861;&#36890;&#24120;&#25490;&#21517;&#19981;&#21516;&#12290;2&#65289;&#19982;&#24179;&#34892;&#30340;&#38750;&#27169;&#26495;&#21644;&#27169;&#26495;&#25552;&#31034;&#30456;&#27604;&#65292;&#20934;&#30830;&#24230;&#19979;&#38477;&#20102;&#26368;&#22810;42%&#12290;3&#65289;&#22312;&#38750;&#27169;&#26495;&#26041;&#27861;&#20013;&#65292;&#22256;&#24785;&#24230;&#19982;&#20934;&#30830;&#24230;&#21576;&#36127;&#30456;&#20851;&#65292;&#20294;&#26159;&#22312;&#22522;&#20110;&#27169;&#26495;&#30340;&#25506;&#27979;&#20013;&#65292;&#23427;&#20204;&#21576;&#27491;&#30456;&#20851;&#65292;&#36825;&#19982;&#30452;&#35273;&#30456;&#21453;&#12290;4&#65289;&#27169;&#22411;&#20542;&#21521;&#20110;&#39044;&#27979;&#30456;&#21516;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
The differences between cloze-task language model (LM) probing with 1) expert-made templates and 2) naturally-occurring text have often been overlooked. Here, we evaluate 16 different LMs on 10 probing English datasets -- 4 template-based and 6 template-free -- in general and biomedical domains to answer the following research questions: (RQ1) Do model rankings differ between the two approaches? (RQ2) Do models' absolute scores differ between the two approaches? (RQ3) Do the answers to RQ1 and RQ2 differ between general and domain-specific models? Our findings are: 1) Template-free and template-based approaches often rank models differently, except for the top domain-specific models. 2) Scores decrease by up to 42% Acc@1 when comparing parallel template-free and template-based prompts. 3) Perplexity is negatively correlated with accuracy in the template-free approach, but, counter-intuitively, they are positively correlated for template-based probing. 4) Models tend to predict the same
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Bridge&#30340;&#27169;&#22411;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#21033;&#29992;&#22810;&#26426;&#26500;&#27979;&#24207;&#25968;&#25454;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22522;&#22240;&#32452;&#26495;&#22359;&#30340;&#21464;&#21270;&#12289;&#27979;&#24207;&#25216;&#26415;&#30340;&#24046;&#24322;&#20197;&#21450;&#25968;&#25454;&#30340;&#39640;&#32500;&#24230;&#21644;&#31232;&#30095;&#24615;&#31561;&#12290;</title><link>https://arxiv.org/abs/2402.00077</link><description>&lt;p&gt;
&#22810;&#26426;&#26500;&#25968;&#25454;&#30340;&#37322;&#25918;&#21147;&#37327;&#65306;&#25972;&#21512;&#21644;&#21327;&#35843;&#36328;&#26426;&#26500;&#30340;&#22522;&#22240;&#32452;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Power of Multi-institutional Data: Integrating and Harmonizing Genomic Data Across Institutions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00077
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Bridge&#30340;&#27169;&#22411;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#21033;&#29992;&#22810;&#26426;&#26500;&#27979;&#24207;&#25968;&#25454;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22522;&#22240;&#32452;&#26495;&#22359;&#30340;&#21464;&#21270;&#12289;&#27979;&#24207;&#25216;&#26415;&#30340;&#24046;&#24322;&#20197;&#21450;&#25968;&#25454;&#30340;&#39640;&#32500;&#24230;&#21644;&#31232;&#30095;&#24615;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#26159;&#30001;&#22522;&#22240;&#31361;&#21464;&#39537;&#21160;&#30340;&#22797;&#26434;&#30142;&#30149;&#65292;&#32959;&#30244;&#27979;&#24207;&#24050;&#25104;&#20026;&#30284;&#30151;&#24739;&#32773;&#20020;&#24202;&#25252;&#29702;&#30340;&#37325;&#35201;&#25163;&#27573;&#12290;&#20986;&#29616;&#30340;&#22810;&#26426;&#26500;&#27979;&#24207;&#25968;&#25454;&#20026;&#23398;&#20064;&#30495;&#23454;&#19990;&#30028;&#30340;&#35777;&#25454;&#20197;&#22686;&#24378;&#31934;&#20934;&#32959;&#30244;&#21307;&#23398;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#36164;&#28304;&#12290;&#30001;&#32654;&#22269;&#30284;&#30151;&#30740;&#31350;&#21327;&#20250;&#39046;&#23548;&#30340;GENIE BPC&#24314;&#31435;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25968;&#25454;&#24211;&#65292;&#23558;&#22522;&#22240;&#32452;&#25968;&#25454;&#19982;&#22810;&#20010;&#30284;&#30151;&#20013;&#24515;&#30340;&#20020;&#24202;&#20449;&#24687;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36825;&#31181;&#22810;&#26426;&#26500;&#27979;&#24207;&#25968;&#25454;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22522;&#22240;&#32452;&#26495;&#22359;&#30340;&#21464;&#21270;&#23548;&#33268;&#22312;&#20351;&#29992;&#24120;&#35265;&#22522;&#22240;&#38598;&#36827;&#34892;&#20998;&#26512;&#26102;&#20449;&#24687;&#20002;&#22833;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#30340;&#27979;&#24207;&#25216;&#26415;&#21644;&#26426;&#26500;&#20043;&#38388;&#30340;&#24739;&#32773;&#24322;&#36136;&#24615;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#12290;&#39640;&#32500;&#25968;&#25454;&#12289;&#31232;&#30095;&#22522;&#22240;&#31361;&#21464;&#27169;&#24335;&#20197;&#21450;&#20010;&#20307;&#22522;&#22240;&#27700;&#24179;&#19978;&#30340;&#24369;&#20449;&#21495;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#20123;&#29616;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Bridge&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cancer is a complex disease driven by genomic alterations, and tumor sequencing is becoming a mainstay of clinical care for cancer patients. The emergence of multi-institution sequencing data presents a powerful resource for learning real-world evidence to enhance precision oncology. GENIE BPC, led by the American Association for Cancer Research, establishes a unique database linking genomic data with clinical information for patients treated at multiple cancer centers. However, leveraging such multi-institutional sequencing data presents significant challenges. Variations in gene panels result in loss of information when the analysis is conducted on common gene sets. Additionally, differences in sequencing techniques and patient heterogeneity across institutions add complexity. High data dimensionality, sparse gene mutation patterns, and weak signals at the individual gene level further complicate matters. Motivated by these real-world challenges, we introduce the Bridge model. It use
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20844;&#24179;&#30340;Wasserstein&#26680;&#24515;&#38598;(FWC)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#22987;&#25968;&#25454;&#38598;&#19982;&#21152;&#26435;&#21512;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#65292;&#24182;&#24378;&#21046;&#23454;&#29616;&#20154;&#21475;&#24179;&#31561;&#65292;&#29983;&#25104;&#20844;&#24179;&#30340;&#21512;&#25104;&#20195;&#34920;&#24615;&#26679;&#26412;&#65292;&#21487;&#29992;&#20110;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2311.05436</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23454;&#29616;&#20844;&#24179;&#30340;&#26680;&#24515;&#38598;
&lt;/p&gt;
&lt;p&gt;
Fair Coresets via Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20844;&#24179;&#30340;Wasserstein&#26680;&#24515;&#38598;(FWC)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#22987;&#25968;&#25454;&#38598;&#19982;&#21152;&#26435;&#21512;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#65292;&#24182;&#24378;&#21046;&#23454;&#29616;&#20154;&#21475;&#24179;&#31561;&#65292;&#29983;&#25104;&#20844;&#24179;&#30340;&#21512;&#25104;&#20195;&#34920;&#24615;&#26679;&#26412;&#65292;&#21487;&#29992;&#20110;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31934;&#28860;&#21644;&#26680;&#24515;&#38598;&#24050;&#25104;&#20026;&#29983;&#25104;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#30340;&#36739;&#23567;&#20195;&#34920;&#24615;&#26679;&#26412;&#38598;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26426;&#22120;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#31038;&#20250;&#23618;&#38754;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#20351;&#24471;&#27169;&#22411;&#26500;&#24314;&#32773;&#24517;&#39035;&#35299;&#20915;&#23384;&#22312;&#20110;&#25968;&#25454;&#20013;&#30340;&#23376;&#32676;&#20307;&#30340;&#22266;&#26377;&#20559;&#35265;&#38382;&#39064;&#12290;&#24403;&#21069;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#30456;&#23545;&#20110;&#21407;&#22987;&#26679;&#26412;&#30340;&#23616;&#37096;&#23646;&#24615;&#26469;&#21019;&#24314;&#20844;&#24179;&#30340;&#21512;&#25104;&#20195;&#34920;&#24615;&#26679;&#26412;&#65292;&#20294;&#20854;&#23545;&#19979;&#28216;&#23398;&#20064;&#36807;&#31243;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20844;&#24179;&#30340;Wasserstein&#26680;&#24515;&#38598;&#65288;FWC&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26680;&#24515;&#38598;&#26041;&#27861;&#65292;&#23427;&#29983;&#25104;&#26082;&#20855;&#26377;&#20844;&#24179;&#24615;&#30340;&#21512;&#25104;&#20195;&#34920;&#24615;&#26679;&#26412;&#65292;&#21448;&#20855;&#26377;&#29992;&#20110;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#30340;&#26679;&#26412;&#32423;&#26435;&#37325;&#12290;FWC&#26368;&#23567;&#21270;&#21407;&#22987;&#25968;&#25454;&#38598;&#19982;&#21152;&#26435;&#21512;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#65292;&#21516;&#26102;&#24378;&#21046;&#23454;&#29616;&#20154;&#21475;&#24179;&#31561;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FWC&#30340;&#26080;&#32422;&#26463;&#29256;&#26412;&#31561;&#20215;&#20110;&#36890;&#24120;&#30340;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;FWC&#30340;&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data distillation and coresets have emerged as popular approaches to generate a smaller representative set of samples for downstream learning tasks to handle large-scale datasets. At the same time, machine learning is being increasingly applied to decision-making processes at a societal level, making it imperative for modelers to address inherent biases towards subgroups present in the data. Current approaches create fair synthetic representative samples by optimizing local properties relative to the original samples, but their effect on downstream learning processes has yet to be explored. In this work, we present fair Wasserstein coresets (FWC), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. FWC minimizes the Wasserstein distance between the original dataset and the weighted synthetic samples while enforcing demographic parity. We show that an unconstrained version of FWC is equiv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#25490;&#21517;&#26631;&#20934;Equal-Opportunity Ranking&#65288;EOR&#65289;&#65292;&#23558;&#24213;&#23618;&#30456;&#20851;&#24615;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24046;&#24322;&#32771;&#34385;&#22312;&#20869;&#65292;&#36890;&#36807;&#32452;&#20869;&#20844;&#24179;&#25277;&#22870;&#23454;&#29616;&#20844;&#24179;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2309.01610</link><description>&lt;p&gt;
&#19981;&#21516;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#20844;&#24179;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Fair Ranking under Disparate Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01610
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#25490;&#21517;&#26631;&#20934;Equal-Opportunity Ranking&#65288;EOR&#65289;&#65292;&#23558;&#24213;&#23618;&#30456;&#20851;&#24615;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24046;&#24322;&#32771;&#34385;&#22312;&#20869;&#65292;&#36890;&#36807;&#32452;&#20869;&#20844;&#24179;&#25277;&#22870;&#23454;&#29616;&#20844;&#24179;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#21517;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20154;&#31867;&#35780;&#20272;&#32773;&#30340;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#21487;&#31649;&#29702;&#30340;&#36873;&#39033;&#23376;&#38598;&#19978;&#12290;&#23427;&#20316;&#20026;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#30340;&#20351;&#29992;&#33539;&#22260;&#20174;&#22312;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#23637;&#31034;&#28508;&#22312;&#30456;&#20851;&#20135;&#21697;&#21040;&#20026;&#20154;&#24037;&#23457;&#26597;&#20248;&#20808;&#22788;&#29702;&#22823;&#23398;&#30003;&#35831;&#12290;&#34429;&#28982;&#25490;&#21517;&#21487;&#20197;&#36890;&#36807;&#23558;&#20851;&#27880;&#38598;&#20013;&#22312;&#26368;&#26377;&#21069;&#36884;&#30340;&#36873;&#39033;&#19978;&#20351;&#20154;&#31867;&#35780;&#20272;&#26356;&#21152;&#39640;&#25928;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#24213;&#23618;&#30456;&#20851;&#24615;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#22312;&#19981;&#21516;&#32452;&#21035;&#30340;&#36873;&#39033;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#25490;&#21517;&#21487;&#33021;&#20250;&#24341;&#20837;&#19981;&#20844;&#24179;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#24046;&#24322;&#20284;&#20046;&#26222;&#36941;&#23384;&#22312;&#65292;&#24120;&#24120;&#23545;&#23569;&#25968;&#32676;&#20307;&#36896;&#25104;&#25439;&#23475;&#65292;&#22240;&#20026;&#36825;&#20123;&#32676;&#20307;&#30340;&#30456;&#20851;&#24615;&#20272;&#35745;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#25110;&#21512;&#36866;&#30340;&#29305;&#24449;&#32780;&#20855;&#26377;&#26356;&#39640;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20844;&#24179;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Equal-Opportunity Ranking&#65288;EOR&#65289;&#20316;&#20026;&#25490;&#21517;&#30340;&#26032;&#20844;&#24179;&#26631;&#20934;&#65292;&#24182;&#23637;&#31034;&#23427;&#23545;&#24212;&#20110;&#22312;&#30456;&#20851;&#36873;&#39033;&#20043;&#38388;&#36827;&#34892;&#32452;&#20869;&#20844;&#24179;&#25277;&#22870;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01610v2 Announce Type: replace  Abstract: Ranking is a ubiquitous method for focusing the attention of human evaluators on a manageable subset of options. Its use as part of human decision-making processes ranges from surfacing potentially relevant products on an e-commerce site to prioritizing college applications for human review. While ranking can make human evaluation more effective by focusing attention on the most promising options, we argue that it can introduce unfairness if the uncertainty of the underlying relevance model differs between groups of options. Unfortunately, such disparity in uncertainty appears widespread, often to the detriment of minority groups for which relevance estimates can have higher uncertainty due to a lack of data or appropriate features. To address this fairness issue, we propose Equal-Opportunity Ranking (EOR) as a new fairness criterion for ranking and show that it corresponds to a group-wise fair lottery among the relevant options even
&lt;/p&gt;</description></item><item><title>ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.10225</link><description>&lt;p&gt;
ChatQA: &#26500;&#24314;GPT-4&#32423;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10225
&lt;/p&gt;
&lt;p&gt;
ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatQA&#65292;&#19968;&#31995;&#21015;&#20855;&#26377;GPT-4&#32423;&#21035;&#20934;&#30830;&#24615;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22788;&#29702;&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22810;&#36718;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#24494;&#35843;&#65292;&#36825;&#26679;&#21487;&#20197;&#25552;&#20379;&#19982;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ChatQA-70B&#21487;&#20197;&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#20998;&#19978;&#36229;&#36807;GPT-4&#65288;54.14 vs. 53.90&#65289;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;OpenAI GPT&#27169;&#22411;&#30340;&#20219;&#20309;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#24418;&#24335;&#21270;&#21644;&#32479;&#19968;&#20102;&#25552;&#39640;&#27867;&#21270;&#24615;&#21644;&#20811;&#26381;&#36807;&#25311;&#21512;&#30340;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.02349</link><description>&lt;p&gt;
&#20998;&#26512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#24615;&#33021;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey Analyzing Generalization in Deep Reinforcement Learning. (arXiv:2401.02349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#24418;&#24335;&#21270;&#21644;&#32479;&#19968;&#20102;&#25552;&#39640;&#27867;&#21270;&#24615;&#21644;&#20811;&#26381;&#36807;&#25311;&#21512;&#30340;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#39640;&#32500;&#29366;&#24577;&#25110;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#38382;&#39064;&#65292;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#25104;&#21151;&#21644;&#20851;&#27880;&#12290;&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30446;&#21069;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#27491;&#22312;&#34987;&#24212;&#29992;&#65292;&#20174;&#21307;&#30103;&#24212;&#29992;&#21040;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65292;&#20294;&#20851;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#27867;&#21270;&#33021;&#21147;&#20173;&#26377;&#35768;&#22810;&#24453;&#35299;&#31572;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#27010;&#36848;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#36935;&#21040;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#23545;&#25552;&#39640;&#27867;&#21270;&#24615;&#21644;&#20811;&#26381;&#29366;&#24577;-&#21160;&#20316;&#20540;&#20989;&#25968;&#20013;&#30340;&#36807;&#25311;&#21512;&#30340;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#24418;&#24335;&#21270;&#21644;&#32479;&#19968;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#30740;&#31350;&#21487;&#20197;&#20026;&#24403;&#21069;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#23637;&#25552;&#20379;&#19968;&#20010;&#31616;&#27905;&#31995;&#32479;&#30340;&#32479;&#19968;&#20998;&#26512;&#65292;&#24182;&#26377;&#21161;&#20110;&#26500;&#24314;&#20581;&#22766;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to self driving vehicles, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will outline the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their robustness and generalization capabilities. Furthermore, we will formalize and unify the diverse solution approaches to increase generalization, and overcome overfitting in state-action value functions. We believe our study can provide a compact systematic unified analysis for the current advancements in deep reinforcement learning, and help to construct robust deep neural policies 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30028;&#25439;&#22833;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#21442;&#32771;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#33258;&#30001;&#21442;&#25968;&#35299;&#20915;&#20102;&#19968;&#20123;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#28789;&#27963;&#30340;&#20551;&#35774;&#20135;&#29983;&#20102;&#26032;&#30340;&#24191;&#20041;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.01148</link><description>&lt;p&gt;
&#26080;&#30028;&#25439;&#22833;&#30340;PAC-Bayes-Chernoff&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayes-Chernoff bounds for unbounded losses. (arXiv:2401.01148v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01148
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30028;&#25439;&#22833;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#21442;&#32771;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#33258;&#30001;&#21442;&#25968;&#35299;&#20915;&#20102;&#19968;&#20123;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#28789;&#27963;&#30340;&#20551;&#35774;&#20135;&#29983;&#20102;&#26032;&#30340;&#24191;&#20041;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26080;&#30028;&#25439;&#22833;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#21442;&#32771;&#30028;&#38480;&#12290;&#36825;&#20010;&#32467;&#26524;&#21487;&#20197;&#29702;&#35299;&#20026;Chernoff&#30028;&#38480;&#30340;PAC-Bayes&#29256;&#26412;&#12290;&#35777;&#26126;&#25216;&#24039;&#20381;&#36182;&#20110;&#36890;&#36807;Cram&#233;r&#21464;&#25442;&#23545;&#25439;&#22833;&#36827;&#34892;&#32479;&#19968;&#36793;&#30028;&#30340;&#23614;&#37096;&#38543;&#26426;&#21464;&#37327;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#25105;&#20204;&#20027;&#35201;&#32467;&#26524;&#30340;&#20004;&#20010;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30028;&#38480;&#35299;&#20915;&#20102;&#35768;&#22810;PAC-Bayes&#30028;&#38480;&#19978;&#30340;&#33258;&#30001;&#21442;&#25968;&#20248;&#21270;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#22312;&#25439;&#22833;&#20989;&#25968;&#19978;&#36827;&#34892;&#28789;&#27963;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#24191;&#20041;&#20102;&#20043;&#21069;&#30340;&#30028;&#38480;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#21270;&#26469;&#33719;&#24471;&#31867;&#20284;Gibbs&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new high-probability PAC-Bayes oracle bound for unbounded losses. This result can be understood as a PAC-Bayes version of the Chernoff bound. The proof technique relies on uniformly bounding the tail of certain random variable based on the Cram\'er transform of the loss. We highlight two applications of our main result. First, we show that our bound solves the open problem of optimizing the free parameter on many PAC-Bayes bounds. Finally, we show that our approach allows working with flexible assumptions on the loss function, resulting in novel bounds that generalize previous ones and can be minimized to obtain Gibbs-like posteriors.
&lt;/p&gt;</description></item><item><title>&#32473;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#30828;&#27880;&#24847;&#21147;&#21644;&#20005;&#26684;&#26410;&#26469;&#25513;&#30721;&#65292;&#24182;&#19988;&#35777;&#26126;&#36825;&#20123;&#32593;&#32476;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#27491;&#26159;&#26080;&#26143;&#35821;&#35328;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36890;&#36807;&#28155;&#21152;&#20301;&#32622;&#23884;&#20837;&#65292;&#36825;&#19968;&#27169;&#22411;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#30740;&#31350;&#20805;&#20998;&#30340;&#35821;&#35328;&#31867;&#21035;&#12290;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#26159;&#24067;&#23572;RASP&#65292;&#36890;&#36807;&#26080;&#26143;&#35821;&#35328;&#30340;&#30740;&#31350;&#65292;&#23558;&#21464;&#25442;&#22120;&#19982;&#19968;&#38454;&#36923;&#36753;&#12289;&#26102;&#24577;&#36923;&#36753;&#21644;&#20195;&#25968;&#33258;&#21160;&#26426;&#29702;&#35770;&#30456;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2310.13897</link><description>&lt;p&gt;
&#25513;&#30721;&#30828;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#21644;&#24067;&#23572;RASP&#20934;&#30830;&#35782;&#21035;&#26080;&#26143;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Hard-Attention Transformers and Boolean RASP Recognize Exactly the Star-Free Languages. (arXiv:2310.13897v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13897
&lt;/p&gt;
&lt;p&gt;
&#32473;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#30828;&#27880;&#24847;&#21147;&#21644;&#20005;&#26684;&#26410;&#26469;&#25513;&#30721;&#65292;&#24182;&#19988;&#35777;&#26126;&#36825;&#20123;&#32593;&#32476;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#27491;&#26159;&#26080;&#26143;&#35821;&#35328;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36890;&#36807;&#28155;&#21152;&#20301;&#32622;&#23884;&#20837;&#65292;&#36825;&#19968;&#27169;&#22411;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#30740;&#31350;&#20805;&#20998;&#30340;&#35821;&#35328;&#31867;&#21035;&#12290;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#26159;&#24067;&#23572;RASP&#65292;&#36890;&#36807;&#26080;&#26143;&#35821;&#35328;&#30340;&#30740;&#31350;&#65292;&#23558;&#21464;&#25442;&#22120;&#19982;&#19968;&#38454;&#36923;&#36753;&#12289;&#26102;&#24577;&#36923;&#36753;&#21644;&#20195;&#25968;&#33258;&#21160;&#26426;&#29702;&#35770;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#30828;&#27880;&#24847;&#21147;&#65288;&#21363;&#25152;&#26377;&#27880;&#24847;&#21147;&#37117;&#38598;&#20013;&#22312;&#19968;&#20010;&#20301;&#32622;&#19978;&#65289;&#21644;&#20005;&#26684;&#30340;&#26410;&#26469;&#25513;&#30721;&#65288;&#21363;&#27599;&#20010;&#20301;&#32622;&#21482;&#19982;&#20005;&#26684;&#24038;&#20391;&#30340;&#20301;&#32622;&#36827;&#34892;&#27880;&#24847;&#21147;&#20132;&#20114;&#65289;&#30340;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#32593;&#32476;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#27491;&#26159;&#26080;&#26143;&#35821;&#35328;&#12290;&#28155;&#21152;&#20301;&#32622;&#23884;&#20837;&#23558;&#34987;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#25193;&#23637;&#21040;&#20854;&#20182;&#30740;&#31350;&#20805;&#20998;&#30340;&#31867;&#21035;&#12290;&#36825;&#20123;&#35777;&#26126;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#26159;&#24067;&#23572;RASP&#65292;&#23427;&#26159;&#19968;&#31181;&#21463;&#38480;&#20110;&#24067;&#23572;&#20540;&#30340;RASP&#21464;&#31181;&#12290;&#36890;&#36807;&#26080;&#26143;&#35821;&#35328;&#65292;&#25105;&#20204;&#23558;&#21464;&#25442;&#22120;&#19982;&#19968;&#38454;&#36923;&#36753;&#12289;&#26102;&#24577;&#36923;&#36753;&#21644;&#20195;&#25968;&#33258;&#21160;&#26426;&#29702;&#35770;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider transformer encoders with hard attention (in which all attention is focused on exactly one position) and strict future masking (in which each position only attends to positions strictly to its left), and prove that the class of languages recognized by these networks is exactly the star-free languages. Adding position embeddings increases the class of recognized languages to other well-studied classes. A key technique in these proofs is Boolean RASP, a variant of RASP that is restricted to Boolean values. Via the star-free languages, we relate transformers to first-order logic, temporal logic, and algebraic automata theory.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32454;&#21270;&#20998;&#26512;&#23398;&#20064;&#29575;&#35843;&#24230;&#26469;&#35299;&#20915;&#23454;&#36341;&#20013;&#23398;&#20064;&#29575;&#35843;&#25972;&#19982;&#29702;&#35770;&#30340;&#19981;&#19968;&#33268;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35266;&#23519;&#21040;&#30340;&#26799;&#24230;&#33539;&#25968;&#36827;&#34892;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#36866;&#24212;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#32454;&#21270;&#35843;&#24230;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07831</link><description>&lt;p&gt;
&#20309;&#26102;&#65292;&#20026;&#20160;&#20040;&#20197;&#21450;&#22810;&#23569;&#65311;&#36890;&#36807;&#32454;&#21270;&#36827;&#34892;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement. (arXiv:2310.07831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07831
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32454;&#21270;&#20998;&#26512;&#23398;&#20064;&#29575;&#35843;&#24230;&#26469;&#35299;&#20915;&#23454;&#36341;&#20013;&#23398;&#20064;&#29575;&#35843;&#25972;&#19982;&#29702;&#35770;&#30340;&#19981;&#19968;&#33268;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35266;&#23519;&#21040;&#30340;&#26799;&#24230;&#33539;&#25968;&#36827;&#34892;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#36866;&#24212;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#32454;&#21270;&#35843;&#24230;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#23398;&#20064;&#29575;&#35843;&#24230;&#19982;&#29702;&#35770;&#25512;&#33616;&#30340;&#20960;&#20046;&#23436;&#20840;&#19981;&#21516;&#12290;&#25105;&#20204;&#32553;&#23567;&#20102;&#22823;&#37096;&#20998;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22240;&#27492;&#33021;&#22815;&#25512;&#23548;&#20986;&#26032;&#30340;&#38382;&#39064;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#35843;&#24230;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#23545;&#24191;&#27867;&#31867;&#21035;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#21253;&#25324;SGD&#65289;&#30340;&#23398;&#20064;&#29575;&#35843;&#24230;&#36827;&#34892;&#32454;&#21270;&#20998;&#26512;&#12290;&#19982;&#22823;&#22810;&#25968;&#21069;&#26399;&#30740;&#31350;&#21482;&#30740;&#31350;&#24179;&#22343;&#36845;&#20195;&#30340;&#25910;&#25947;&#24615;&#19981;&#21516;&#65292;&#25105;&#20204;&#30740;&#31350;&#26368;&#21518;&#19968;&#27425;&#36845;&#20195;&#65292;&#36825;&#26159;&#22823;&#22810;&#25968;&#20154;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#12290;&#24403;&#20165;&#32771;&#34385;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#26102;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#39044;&#27979;&#26368;&#20339;&#36873;&#25321;&#26159;&#32447;&#24615;&#34928;&#20943;&#35843;&#24230;&#65306;&#36825;&#26159;&#19968;&#31181;&#23454;&#36341;&#20013;&#24120;&#29992;&#30340;&#36873;&#25321;&#65292;&#20854;&#23558;&#27493;&#38271;&#19982;&#24403;&#21069;&#36845;&#20195;&#27425;&#25968;t&#21644;&#24635;&#27493;&#25968;T&#25104;&#27604;&#20363;&#22320;&#35774;&#32622;&#20026;1 - t/T&#12290;&#20026;&#20102;&#36229;&#36234;&#36825;&#31181;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#65292;&#25105;&#20204;&#20351;&#29992;&#35266;&#23519;&#21040;&#30340;&#26799;&#24230;&#33539;&#25968;&#26469;&#25512;&#23548;&#36866;&#24212;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#32454;&#21270;&#35843;&#24230;&#12290;&#36825;&#20123;&#32454;&#21270;&#35843;&#24230;&#34920;&#29616;&#20986;&#23398;&#20064;&#29575;&#36880;&#28176;&#22686;&#21152;&#21644;&#23398;&#20064;&#29575;&#36805;&#36895;&#36864;&#28779;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning rate schedules used in practice bear little resemblance to those recommended by theory. We close much of this theory/practice gap, and as a consequence are able to derive new problem-adaptive learning rate schedules. Our key technical contribution is a refined analysis of learning rate schedules for a wide class of optimization algorithms (including SGD). In contrast to most prior works that study the convergence of the average iterate, we study the last iterate, which is what most people use in practice. When considering only worst-case analysis, our theory predicts that the best choice is the linear decay schedule: a popular choice in practice that sets the stepsize proportionally to $1 - t/T$, where $t$ is the current iteration and $T$ is the total number of steps. To go beyond this worst-case analysis, we use the observed gradient norms to derive schedules refined for any particular task. These refined schedules exhibit learning rate warm-up and rapid learning rate anneali
&lt;/p&gt;</description></item><item><title>IMITATE&#26159;&#19968;&#31181;&#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#21307;&#23398;&#25253;&#21578;&#30340;&#23618;&#32423;&#32467;&#26500;&#65292;&#20174;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#19982;&#20998;&#23618;&#21307;&#23398;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#24615;&#21644;&#32467;&#35770;&#24615;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.07355</link><description>&lt;p&gt;
IMITATE: &#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training. (arXiv:2310.07355v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07355
&lt;/p&gt;
&lt;p&gt;
IMITATE&#26159;&#19968;&#31181;&#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#21307;&#23398;&#25253;&#21578;&#30340;&#23618;&#32423;&#32467;&#26500;&#65292;&#20174;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#19982;&#20998;&#23618;&#21307;&#23398;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#24615;&#21644;&#32467;&#35770;&#24615;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#39046;&#22495;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#20174;&#20020;&#24202;&#25253;&#21578;&#21644;&#30456;&#20851;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#33021;&#24573;&#35270;&#20102;&#21033;&#29992;&#20020;&#24202;&#25253;&#21578;&#22266;&#26377;&#30340;&#23618;&#32423;&#32467;&#26500;&#30340;&#26426;&#20250;&#65292;&#36825;&#20123;&#25253;&#21578;&#36890;&#24120;&#34987;&#20998;&#20026;&#25551;&#36848;&#24615;&#20869;&#23481;&#30340;&#8220;&#21457;&#29616;&#8221;&#21644;&#32467;&#35770;&#24615;&#35266;&#23519;&#30340;&#8220;&#21360;&#35937;&#8221;&#12290;&#24403;&#21069;&#30340;&#21307;&#23398;VLP&#26041;&#27861;&#24448;&#24448;&#23558;&#25253;&#21578;&#31616;&#21270;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#23454;&#20307;&#25110;&#20998;&#25955;&#30340;&#26631;&#35760;&#65292;&#32780;&#27809;&#26377;&#21033;&#29992;&#36825;&#31181;&#20016;&#23500;&#30340;&#12289;&#32467;&#26500;&#21270;&#30340;&#26684;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;VLP&#26694;&#26550;&#65292;&#21517;&#20026;IMITATE&#65292;&#29992;&#20110;&#20174;&#21307;&#23398;&#25253;&#21578;&#20013;&#23398;&#20064;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#20174;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#23558;&#36825;&#20123;&#29305;&#24449;&#19982;&#20998;&#23618;&#21307;&#23398;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#24615;&#21644;&#32467;&#35770;&#24615;&#25991;&#26412;&#20998;&#21035;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of medical Vision-Language Pre-training (VLP), significant efforts have been devoted to deriving text and image features from both clinical reports and associated medical images. However, most existing methods may have overlooked the opportunity in leveraging the inherent hierarchical structure of clinical reports, which are generally split into `findings' for descriptive content and `impressions' for conclusive observation. Instead of utilizing this rich, structured format, current medical VLP approaches often simplify the report into either a unified entity or fragmented tokens. In this work, we propose a novel clinical prior guided VLP framework named IMITATE to learn the structure information from medical reports with hierarchical vision-language alignment. The framework derives multi-level visual features from the chest X-ray (CXR) images and separately aligns these features with the descriptive and the conclusive text encoded in the hierarchical medical report. Furth
&lt;/p&gt;</description></item><item><title>&#22312;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#22312;&#23567;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20445;&#23432;&#24615;&#31639;&#27861;&#26469;&#32531;&#35299;&#27492;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20445;&#23432;&#30340;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#22312;&#24635;&#20307;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15178</link><description>&lt;p&gt;
&#20445;&#23432;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conservative World Models. (arXiv:2309.15178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15178
&lt;/p&gt;
&lt;p&gt;
&#22312;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#22312;&#23567;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20445;&#23432;&#24615;&#31639;&#27861;&#26469;&#32531;&#35299;&#27492;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20445;&#23432;&#30340;&#21069;&#21521;-&#21518;&#21521;&#31639;&#27861;&#22312;&#24635;&#20307;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#25215;&#35834;&#22312;&#31163;&#32447;&#39044;&#35757;&#32451;&#38454;&#27573;&#21518;&#65292;&#25552;&#20379;&#33021;&#22815;&#22312;&#20219;&#20309;&#29615;&#22659;&#20013;&#25191;&#34892;&#20219;&#20309;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;&#21069;&#21521;-&#21518;&#21521;&#65288;FB&#65289;&#34920;&#31034;&#22312;&#36825;&#20010;&#29702;&#24819;&#30340;&#23454;&#29616;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21487;&#20197;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#36798;&#21040;&#29305;&#23450;&#20219;&#21153;&#20195;&#29702;&#30340;85%&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#23545;&#20110;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#30340;&#35775;&#38382;&#65292;&#32780;&#22823;&#22810;&#25968;&#30495;&#23454;&#38382;&#39064;&#26080;&#27861;&#26399;&#26395;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#38598;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#24773;&#20917;&#19979;FB&#24615;&#33021;&#22914;&#20309;&#38477;&#20302;&#65292;&#24182;&#36890;&#36807;&#20445;&#23432;&#24615;&#26469;&#20943;&#36731;&#36825;&#31181;&#24773;&#20917;&#65292;&#36825;&#26159;&#19968;&#20010;&#25104;&#29087;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#39046;&#22495;&#21644;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23478;&#26063;&#65292;&#22312;&#24635;&#20307;&#19978;&#36798;&#21040;&#20102;150%&#30340;&#26222;&#36890;FB&#24615;&#33021;&#12290;&#26377;&#20123;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20445;&#23432;&#30340;FB&#31639;&#27861;&#22312;&#27809;&#26377;&#35775;&#38382;&#22870;&#21169;&#26631;&#31614;&#19988;&#38656;&#35201;&#32500;&#25252;&#25152;&#26377;&#20219;&#21153;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#20248;&#20110;&#29305;&#23450;&#20219;&#21153;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot reinforcement learning (RL) promises to provide agents that can perform any task in an environment after an offline pre-training phase. Forward-backward (FB) representations represent remarkable progress towards this ideal, achieving 85% of the performance of task-specific agents in this setting. However, such performance is contingent on access to large and diverse datasets for pre-training, which cannot be expected for most real problems. Here, we explore how FB performance degrades when trained on small datasets that lack diversity, and mitigate it with conservatism, a well-established feature of performant offline RL algorithms. We evaluate our family of methods across various datasets, domains and tasks, reaching 150% of vanilla FB performance in aggregate. Somewhat surprisingly, conservative FB algorithms also outperform the task-specific baseline, despite lacking access to reward labels and being required to maintain policies for all tasks. Conservative FB algorithms p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#25110;&#29983;&#25104;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#20174;&#20840;&#31209;&#30697;&#38453;&#30340;&#20108;&#27425;&#31995;&#32479;&#20013;&#24674;&#22797;&#20449;&#21495;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#38408;&#20540;Wirtinger&#27969;&#31639;&#27861;&#65288;TWF&#65289;&#26469;&#22788;&#29702;&#31232;&#30095;&#20449;&#21495;&#65292;&#24182;&#20351;&#29992;&#35889;&#21021;&#22987;&#21270;&#21644;&#38408;&#20540;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36739;&#23567;&#30340;&#27979;&#37327;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.09032</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#25110;&#29983;&#25104;&#30340;&#20808;&#39564;&#35299;&#20915;&#20840;&#31209;&#30697;&#38453;&#30340;&#20108;&#27425;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Solving Quadratic Systems with Full-Rank Matrices Using Sparse or Generative Priors. (arXiv:2309.09032v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#25110;&#29983;&#25104;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#20174;&#20840;&#31209;&#30697;&#38453;&#30340;&#20108;&#27425;&#31995;&#32479;&#20013;&#24674;&#22797;&#20449;&#21495;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#38408;&#20540;Wirtinger&#27969;&#31639;&#27861;&#65288;TWF&#65289;&#26469;&#22788;&#29702;&#31232;&#30095;&#20449;&#21495;&#65292;&#24182;&#20351;&#29992;&#35889;&#21021;&#22987;&#21270;&#21644;&#38408;&#20540;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36739;&#23567;&#30340;&#27979;&#37327;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20855;&#26377;&#20840;&#31209;&#30697;&#38453;&#30340;&#20108;&#27425;&#31995;&#32479;&#20013;&#24674;&#22797;&#20449;&#21495;x&#22312;&#24212;&#29992;&#20013;&#32463;&#24120;&#20986;&#29616;&#65292;&#27604;&#22914;&#26410;&#20998;&#37197;&#30340;&#36317;&#31163;&#20960;&#20309;&#21644;&#20122;&#27874;&#38271;&#25104;&#20687;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23545;x&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#38024;&#23545;&#39640;&#32500;&#24773;&#20917;&#65288;m &lt;&lt; n&#65289;&#65292;&#20351;&#29992;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#26631;&#20934;&#39640;&#26031;&#30697;&#38453;&#35299;&#20915;&#20102;&#35813;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#32771;&#34385;k-&#31232;&#30095;&#30340;x&#65292;&#24341;&#20837;&#20102;TWF&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#31232;&#30095;&#27700;&#24179;k&#12290;TWF&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#35889;&#21021;&#22987;&#21270;&#65292;&#24403;m = O(k^2log n)&#26102;&#65292;&#30830;&#23450;&#20102;&#19968;&#20010;&#36317;&#31163;x&#36275;&#22815;&#36817;&#30340;&#28857;&#65288;&#21487;&#33021;&#20250;&#26377;&#31526;&#21495;&#32763;&#36716;&#65289;&#65292;&#20197;&#21450;&#20855;&#26377;&#24456;&#22909;&#21021;&#22987;&#21270;&#30340;&#38408;&#20540;&#26799;&#24230;&#19979;&#38477;&#65292;&#35813;&#19979;&#38477;&#20135;&#29983;&#20102;&#19968;&#20010;&#32447;&#24615;&#25910;&#25947;&#21040;x&#30340;&#24207;&#21015;&#65292;&#29992;m = O(klog n)&#20010;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of recovering a signal $\boldsymbol{x} \in \mathbb{R}^n$ from a quadratic system $\{y_i=\boldsymbol{x}^\top\boldsymbol{A}_i\boldsymbol{x},\ i=1,\ldots,m\}$ with full-rank matrices $\boldsymbol{A}_i$ frequently arises in applications such as unassigned distance geometry and sub-wavelength imaging. With i.i.d. standard Gaussian matrices $\boldsymbol{A}_i$, this paper addresses the high-dimensional case where $m\ll n$ by incorporating prior knowledge of $\boldsymbol{x}$. First, we consider a $k$-sparse $\boldsymbol{x}$ and introduce the thresholded Wirtinger flow (TWF) algorithm that does not require the sparsity level $k$. TWF comprises two steps: the spectral initialization that identifies a point sufficiently close to $\boldsymbol{x}$ (up to a sign flip) when $m=O(k^2\log n)$, and the thresholded gradient descent (with a good initialization) that produces a sequence linearly converging to $\boldsymbol{x}$ with $m=O(k\log n)$ measurements. Second, we explore the generative p
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#24335;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#20998;&#25955;&#25968;&#25454;&#30340;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#38480;&#21046;&#21644;&#31169;&#26377;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#38656;&#27714;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#36825;&#19977;&#20010;&#32452;&#20214;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#26045;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FL&#21644;LLM&#38598;&#25104;&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#65292;&#24182;&#20998;&#26512;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21644;&#28508;&#22312;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2307.08925</link><description>&lt;p&gt;
&#32852;&#37030;&#24335;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#20010;&#31435;&#22330;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Federated Large Language Model: A Position Paper. (arXiv:2307.08925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08925
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#24335;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#20998;&#25955;&#25968;&#25454;&#30340;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#38480;&#21046;&#21644;&#31169;&#26377;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#38656;&#27714;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#36825;&#19977;&#20010;&#32452;&#20214;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#26045;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FL&#21644;LLM&#38598;&#25104;&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#65292;&#24182;&#20998;&#26512;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21644;&#28508;&#22312;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#33719;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#24182;&#25214;&#21040;&#20102;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#65292;&#20294;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#24320;&#21457;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#28304;&#20110;&#20844;&#20849;&#39046;&#22495;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#21294;&#20047;&#20197;&#21450;&#23545;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#39033;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#20986;&#29616;&#20102;&#65292;&#23427;&#33021;&#22815;&#22312;&#20445;&#25345;&#20998;&#25955;&#25968;&#25454;&#30340;&#21516;&#26102;&#23454;&#29616;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#24335;LLM&#30340;&#27010;&#24565;&#65292;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#32852;&#37030;&#24335;LLM&#39044;&#35757;&#32451;&#12289;&#32852;&#37030;&#24335;LLM&#24494;&#35843;&#21644;&#32852;&#37030;&#24335;LLM&#25552;&#31034;&#24037;&#31243;&#12290;&#23545;&#20110;&#27599;&#20010;&#32452;&#20214;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23427;&#30456;&#23545;&#20110;&#20256;&#32479;LLM&#35757;&#32451;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#20307;&#30340;&#24037;&#31243;&#31574;&#30053;&#26469;&#23454;&#26045;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FL&#21644;LLM&#38598;&#25104;&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#12290;&#25105;&#20204;&#20998;&#26512;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#30830;&#23450;&#21487;&#33021;&#30340;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Large scale language models (LLM) have received significant attention and found diverse applications across various domains, but their development encounters challenges in real-world scenarios. These challenges arise due to the scarcity of public domain data availability and the need to maintain privacy with respect to private domain data. To address these issues, federated learning (FL) has emerged as a promising technology that enables collaborative training of shared models while preserving decentralized data. We propose the concept of federated LLM, which comprises three key components, i.e., federated LLM pre-training, federated LLM fine-tuning, and federated LLM prompt engineering. For each component, we discuss its advantage over traditional LLM training methods and propose specific engineering strategies for implementation. Furthermore, we explore the novel challenges introduced by the integration of FL and LLM. We analyze existing solutions and identify potential obstacles fac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#38598;&#21512;&#21270;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#26469;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#39640;&#25928;&#32534;&#30721;&#22788;&#29702;&#65292;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.16625</link><description>&lt;p&gt;
&#38598;&#21512;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Set-based Neural Network Encoding. (arXiv:2305.16625v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16625
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#38598;&#21512;&#21270;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#26469;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#39640;&#25928;&#32534;&#30721;&#22788;&#29702;&#65292;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#21040;&#38598;&#21512;&#21644;&#38598;&#21512;&#21040;&#21521;&#37327;&#20989;&#25968;&#26469;&#26377;&#25928;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#36827;&#34892;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#38656;&#35201;&#23545;&#19981;&#21516;&#26550;&#26500;&#32534;&#20889;&#33258;&#23450;&#20041;&#32534;&#30721;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23545;&#28151;&#21512;&#26550;&#26500;&#21644;&#19981;&#21516;&#21442;&#25968;&#22823;&#23567;&#30340;&#27169;&#22411;&#21160;&#24577;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340; SNE&#65288;&#38598;&#21512;&#21270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#65289;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#65292;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#26368;&#32456;&#23558;&#25152;&#26377;&#23618;&#27425;&#32534;&#30721;&#21512;&#24182;&#21040;&#19968;&#36215;&#65292;&#20197;&#33719;&#21462;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#30690;&#37327;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#26469;&#26377;&#25928;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#35813;&#27969;&#27700;&#32447;&#21487;&#26681;&#25454;&#35745;&#31639;&#21644;&#20869;&#23384;&#38480;&#21046;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#20010;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#30340;&#26032;&#20219;&#21153;&#65306;&#36328;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#36866;&#24212;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach to neural network weight encoding for generalization performance prediction that utilizes set-to-set and set-to-vector functions to efficiently encode neural network parameters. Our approach is capable of encoding neural networks in a modelzoo of mixed architecture and different parameter sizes as opposed to previous approaches that require custom encoding models for different architectures. Furthermore, our \textbf{S}et-based \textbf{N}eural network \textbf{E}ncoder (SNE) takes into consideration the hierarchical computational structure of neural networks by utilizing a layer-wise encoding scheme that culminates to encoding all layer-wise encodings to obtain the neural network encoding vector. Additionally, we introduce a \textit{pad-chunk-encode} pipeline to efficiently encode neural network layers that is adjustable to computational and memory constraints. We also introduce two new tasks for neural network generalization performance prediction: cross-dataset a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65288;ILL&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#23545;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#20026;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#38382;&#39064;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.12715</link><description>&lt;p&gt;
&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65306;&#23398;&#20064;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations. (arXiv:2305.12715v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65288;ILL&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#23545;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#20026;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#38382;&#39064;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#23398;&#20064;&#65288;ILL&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#21508;&#31181;&#19981;&#31934;&#30830;&#26631;&#31614;&#37197;&#32622;&#30340;&#32479;&#19968;&#26041;&#27861;&#12290;ILL&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#23545;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#65292;&#23558;&#31934;&#30830;&#26631;&#31614;&#35270;&#20026;&#28508;&#22312;&#21464;&#37327;&#12290;&#19982;&#20197;&#21069;&#35797;&#22270;&#20174;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#20013;&#25512;&#26029;&#27491;&#30830;&#26631;&#31614;&#30340;&#22810;&#21151;&#33021;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;ILL&#26694;&#26550;&#32771;&#34385;&#20102;&#19981;&#31934;&#30830;&#26631;&#31614;&#20449;&#24687;&#24378;&#21152;&#30340;&#25152;&#26377;&#21487;&#33021;&#26631;&#31614;&#65292;&#20801;&#35768;&#23545;&#20219;&#20309;&#19981;&#31934;&#30830;&#26631;&#31614;&#30340;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ILL&#21487;&#20197;&#26080;&#32541;&#22320;&#36866;&#24212;&#21508;&#31181;&#24773;&#20917;&#65292;&#21253;&#25324;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#20197;&#21450;&#36825;&#20123;&#37197;&#32622;&#30340;&#28151;&#21512;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#26041;&#27861;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22788;&#29702;&#19981;&#31934;&#30830;&#26631;&#31614;&#30340;&#25216;&#26415;&#65292;&#26631;&#24535;&#30528;&#31532;&#19968;&#20010;&#32479;&#19968;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the imprecise label learning (ILL) framework, a unified approach to handle various imprecise label configurations, which are commonplace challenges in machine learning tasks. ILL leverages an expectation-maximization (EM) algorithm for the maximum likelihood estimation (MLE) of the imprecise label information, treating the precise labels as latent variables. Compared to previous versatile methods attempting to infer correct labels from the imprecise label information, our ILL framework considers all possible labeling imposed by the imprecise label information, allowing a unified solution to deal with any imprecise labels. With comprehensive experimental results, we demonstrate that ILL can seamlessly adapt to various situations, including partial label learning, semi-supervised learning, noisy label learning, and a mixture of these settings. Notably, our simple method surpasses the existing techniques for handling imprecise labels, marking the first unified 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#65288;&#23884;&#20837;&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#26469;&#23454;&#29616;&#23398;&#20064;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#36127;&#37319;&#26679;&#26041;&#27861;&#24182;&#22312;&#22810;&#39033;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17475</link><description>&lt;p&gt;
&#36229;&#36234;&#36127;&#37319;&#26679;&#30340;&#39640;&#25928;&#20998;&#24067;&#24335;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient distributed representations beyond negative sampling. (arXiv:2303.17475v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#65288;&#23884;&#20837;&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#26469;&#23454;&#29616;&#23398;&#20064;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#36127;&#37319;&#26679;&#26041;&#27861;&#24182;&#22312;&#22810;&#39033;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23398;&#20064;&#20998;&#24067;&#24335;&#34920;&#31034;&#65288;&#20063;&#31216;&#20026;&#23884;&#20837;&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#20010;&#31867;&#20284;&#20110;Word2Vec&#31639;&#27861;&#20013;&#24341;&#20837;&#24182;&#22312;&#22810;&#20010;&#24037;&#20316;&#20013;&#37319;&#29992;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#23454;&#29616;&#12290;&#20248;&#21270;&#35745;&#31639;&#30340;&#29942;&#39048;&#26159;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#35745;&#31639;&#65292;&#36825;&#38656;&#35201;&#19982;&#26679;&#26412;&#22823;&#23567;&#21576;&#20108;&#27425;&#27604;&#20363;&#30340;&#25805;&#20316;&#25968;&#12290;&#36825;&#31181;&#22797;&#26434;&#24230;&#19981;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#25152;&#20197;&#36127;&#37319;&#26679;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19982;&#26679;&#26412;&#22823;&#23567;&#32447;&#24615;&#30456;&#20851;&#30340;&#26102;&#38388;&#20869;&#33719;&#24471;&#20998;&#24067;&#24335;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36127;&#37319;&#26679;&#20250;&#25913;&#21464;&#25439;&#22833;&#20989;&#25968;&#65292;&#22240;&#27492;&#35299;&#20915;&#30340;&#26159;&#19982;&#26368;&#21021;&#25552;&#20986;&#30340;&#19981;&#21516;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#22914;&#20309;&#36890;&#36807;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#65292;&#20174;&#32780;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#26469;&#23398;&#20064;&#20998;&#24067;&#24335;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23884;&#20837;&#36136;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#26041;&#38754;&#20248;&#20110;&#36127;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article describes an efficient method to learn distributed representations, also known as embeddings. This is accomplished minimizing an objective function similar to the one introduced in the Word2Vec algorithm and later adopted in several works. The optimization computational bottleneck is the calculation of the softmax normalization constants for which a number of operations scaling quadratically with the sample size is required. This complexity is unsuited for large datasets and negative sampling is a popular workaround, allowing one to obtain distributed representations in linear time with respect to the sample size. Negative sampling consists, however, in a change of the loss function and hence solves a different optimization problem from the one originally proposed. Our contribution is to show that the sotfmax normalization constants can be estimated in linear time, allowing us to design an efficient optimization strategy to learn distributed representations. We test our ap
&lt;/p&gt;</description></item></channel></rss>