<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20165;&#36816;&#34892;&#26102;&#35299;&#30721;&#22120;&#30340;&#36229;&#32593;&#32476;&#65292;&#19981;&#20381;&#36182;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36328;&#23618;&#21442;&#25968;&#20887;&#20313;&#12290;</title><link>https://arxiv.org/abs/2403.19163</link><description>&lt;p&gt;
D'OH: &#20165;&#35299;&#30721;&#22120;&#38543;&#26426;&#36229;&#32593;&#32476;&#29992;&#20110;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
D'OH: Decoder-Only random Hypernetworks for Implicit Neural Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20165;&#36816;&#34892;&#26102;&#35299;&#30721;&#22120;&#30340;&#36229;&#32593;&#32476;&#65292;&#19981;&#20381;&#36182;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36328;&#23618;&#21442;&#25968;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38544;&#24335;&#20989;&#25968;&#34987;&#21457;&#29616;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#32534;&#30721;&#21508;&#31181;&#33258;&#28982;&#20449;&#21495;&#12290;&#23427;&#20204;&#30340;&#21560;&#24341;&#21147;&#22312;&#20110;&#33021;&#22815;&#32039;&#20945;&#22320;&#34920;&#31034;&#20449;&#21495;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#31163;&#32447;&#35757;&#32451;&#25968;&#25454;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#30340;&#38544;&#24335;&#20559;&#24046;&#26469;&#35299;&#32806;&#20449;&#21495;&#20013;&#30340;&#38544;&#34255;&#20887;&#20313;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#26679;&#19968;&#20010;&#20551;&#35774;&#65306;&#36890;&#36807;&#21033;&#29992;&#23618;&#20043;&#38388;&#23384;&#22312;&#30340;&#20887;&#20313;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#21387;&#32553;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20165;&#36816;&#34892;&#26102;&#35299;&#30721;&#22120;&#30340;&#36229;&#32593;&#32476; - &#23427;&#19981;&#20351;&#29992;&#31163;&#32447;&#35757;&#32451;&#25968;&#25454; - &#26469;&#26356;&#22909;&#22320;&#24314;&#27169;&#36328;&#23618;&#21442;&#25968;&#20887;&#20313;&#12290;&#20808;&#21069;&#22312;&#28145;&#24230;&#38544;&#24335;&#20989;&#25968;&#20013;&#24212;&#29992;&#36229;&#32593;&#32476;&#30340;&#24212;&#29992;&#37117;&#37319;&#29992;&#20102;&#20381;&#36182;&#22823;&#37327;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#21069;&#39304;&#32534;&#30721;&#22120;/&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26080;&#27861;&#27867;&#21270;&#21040;&#35757;&#32451;&#20449;&#21495;&#20043;&#22806;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#21021;&#22987;&#21270;&#36816;&#34892;&#26102;&#28145;&#24230;&#38544;&#24335;&#20989;&#25968;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19163v1 Announce Type: new  Abstract: Deep implicit functions have been found to be an effective tool for efficiently encoding all manner of natural signals. Their attractiveness stems from their ability to compactly represent signals with little to no off-line training data. Instead, they leverage the implicit bias of deep networks to decouple hidden redundancies within the signal. In this paper, we explore the hypothesis that additional compression can be achieved by leveraging the redundancies that exist between layers. We propose to use a novel run-time decoder-only hypernetwork - that uses no offline training data - to better model this cross-layer parameter redundancy. Previous applications of hyper-networks with deep implicit functions have applied feed-forward encoder/decoder frameworks that rely on large offline datasets that do not generalize beyond the signals they were trained on. We instead present a strategy for the initialization of run-time deep implicit func
&lt;/p&gt;</description></item><item><title>skscope&#26159;&#19968;&#20010;Python&#24211;&#65292;&#36890;&#36807;&#21482;&#38656;&#32534;&#20889;&#30446;&#26631;&#20989;&#25968;&#65292;&#23601;&#33021;&#24555;&#36895;&#23454;&#29616;&#31232;&#30095;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#19979;&#65292;&#20854;&#39640;&#25928;&#23454;&#29616;&#20351;&#24471;&#27714;&#35299;&#22120;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#31232;&#30095;&#35299;&#65292;&#36895;&#24230;&#27604;&#22522;&#20934;&#20984;&#27714;&#35299;&#22120;&#24555;80&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.18540</link><description>&lt;p&gt;
skscope&#65306;Python&#20013;&#30340;&#24555;&#36895;&#31232;&#30095;&#32422;&#26463;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
skscope: Fast Sparsity-Constrained Optimization in Python
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18540
&lt;/p&gt;
&lt;p&gt;
skscope&#26159;&#19968;&#20010;Python&#24211;&#65292;&#36890;&#36807;&#21482;&#38656;&#32534;&#20889;&#30446;&#26631;&#20989;&#25968;&#65292;&#23601;&#33021;&#24555;&#36895;&#23454;&#29616;&#31232;&#30095;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#19979;&#65292;&#20854;&#39640;&#25928;&#23454;&#29616;&#20351;&#24471;&#27714;&#35299;&#22120;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#31232;&#30095;&#35299;&#65292;&#36895;&#24230;&#27604;&#22522;&#20934;&#20984;&#27714;&#35299;&#22120;&#24555;80&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31232;&#30095;&#32422;&#26463;&#20248;&#21270;&#65288;SCO&#65289;&#19978;&#24212;&#29992;&#36845;&#20195;&#27714;&#35299;&#22120;&#38656;&#35201;&#32321;&#29712;&#30340;&#25968;&#23398;&#25512;&#23548;&#21644;&#20180;&#32454;&#30340;&#32534;&#31243;/&#35843;&#35797;&#65292;&#36825;&#38480;&#21046;&#20102;&#36825;&#20123;&#27714;&#35299;&#22120;&#30340;&#24191;&#27867;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24211;skscope&#65292;&#20197;&#20811;&#26381;&#27492;&#38556;&#30861;&#12290;&#20511;&#21161;skscope&#65292;&#29992;&#25143;&#21482;&#38656;&#32534;&#20889;&#30446;&#26631;&#20989;&#25968;&#21363;&#21487;&#35299;&#20915;SCO&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20004;&#20010;&#20363;&#23376;&#28436;&#31034;&#20102;skscope&#30340;&#26041;&#20415;&#20043;&#22788;&#65292;&#20854;&#20013;&#21482;&#38656;&#22235;&#34892;&#20195;&#30721;&#23601;&#21487;&#20197;&#35299;&#20915;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#21644;&#36235;&#21183;&#36807;&#28388;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;skscope&#30340;&#39640;&#25928;&#23454;&#29616;&#20351;&#24471;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;&#21487;&#20197;&#24555;&#36895;&#33719;&#24471;&#31232;&#30095;&#35299;&#65292;&#32780;&#26080;&#38656;&#32771;&#34385;&#21442;&#25968;&#31354;&#38388;&#30340;&#39640;&#32500;&#24230;&#12290;&#25968;&#20540;&#23454;&#39564;&#26174;&#31034;&#65292;skscope&#20013;&#30340;&#21487;&#29992;&#27714;&#35299;&#22120;&#21487;&#20197;&#23454;&#29616;&#27604;&#22522;&#20934;&#20984;&#27714;&#35299;&#22120;&#33719;&#24471;&#30340;&#31454;&#20105;&#26494;&#24347;&#35299;&#39640;&#36798;80&#20493;&#30340;&#21152;&#36895;&#24230;&#12290;skscope&#24050;&#32463;&#21457;&#24067;&#22312;Python&#36719;&#20214;&#21253;&#32034;&#24341;&#65288;PyPI&#65289;&#21644;Conda&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18540v1 Announce Type: cross  Abstract: Applying iterative solvers on sparsity-constrained optimization (SCO) requires tedious mathematical deduction and careful programming/debugging that hinders these solvers' broad impact. In the paper, the library skscope is introduced to overcome such an obstacle. With skscope, users can solve the SCO by just programming the objective function. The convenience of skscope is demonstrated through two examples in the paper, where sparse linear regression and trend filtering are addressed with just four lines of code. More importantly, skscope's efficient implementation allows state-of-the-art solvers to quickly attain the sparse solution regardless of the high dimensionality of parameter space. Numerical experiments reveal the available solvers in skscope can achieve up to 80x speedup on the competing relaxation solutions obtained via the benchmarked convex solver. skscope is published on the Python Package Index (PyPI) and Conda, and its 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#28436;&#31034;&#23398;&#20064;&#38598;&#25104;&#21040;&#36816;&#21160;&#29983;&#25104;&#20013;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23454;&#26102;&#29983;&#25104;&#36866;&#24212;&#22797;&#26434;&#29615;&#22659;&#30340;&#36816;&#21160;</title><link>https://arxiv.org/abs/2403.15239</link><description>&lt;p&gt;
&#24341;&#23548;&#35299;&#30721;&#29992;&#20110;&#26426;&#22120;&#20154;&#36816;&#21160;&#29983;&#25104;&#21644;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Guided Decoding for Robot Motion Generation and Adaption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15239
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#28436;&#31034;&#23398;&#20064;&#38598;&#25104;&#21040;&#36816;&#21160;&#29983;&#25104;&#20013;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23454;&#26102;&#29983;&#25104;&#36866;&#24212;&#22797;&#26434;&#29615;&#22659;&#30340;&#36816;&#21160;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#20855;&#26377;&#38556;&#30861;&#29289;&#12289;&#36890;&#36807;&#28857;&#31561;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#39640;&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#33218;&#36816;&#21160;&#29983;&#25104;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#36890;&#36807;&#23558;&#28436;&#31034;&#23398;&#20064;&#65288;LfD&#65289;&#38598;&#25104;&#21040;&#36816;&#21160;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#21462;&#24471;&#20102;&#35813;&#39046;&#22495;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#31181;&#38598;&#25104;&#25903;&#25345;&#26426;&#22120;&#20154;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20801;&#35768;&#26426;&#22120;&#20154;&#20174;&#28436;&#31034;&#36712;&#36857;&#20013;&#23398;&#20064;&#21644;&#27867;&#21270;&#26469;&#20248;&#21270;&#31215;&#32047;&#30340;&#32463;&#39564;&#21033;&#29992;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#27169;&#25311;&#36712;&#36857;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21464;&#25442;&#22120;&#30340;transformer&#26550;&#26500;&#12290;&#36825;&#31181;&#22522;&#20110;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21464;&#25442;&#22120;&#30340;&#26550;&#26500;&#23398;&#20064;&#20102;&#22522;&#26412;&#30340;&#36816;&#21160;&#29983;&#25104;&#25216;&#33021;&#65292;&#24182;&#23558;&#20854;&#35843;&#25972;&#20197;&#28385;&#36275;&#36741;&#21161;&#20219;&#21153;&#21644;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#33258;&#22238;&#24402;&#26041;&#27861;&#23454;&#29616;&#20102;&#29289;&#29702;&#31995;&#32479;&#21453;&#39304;&#30340;&#23454;&#26102;&#38598;&#25104;&#65292;&#22686;&#24378;&#20102;&#36816;&#21160;&#29983;&#25104;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20174;&#21021;&#22987;&#28857;&#21644;&#30446;&#26631;&#28857;&#29983;&#25104;&#36816;&#21160;&#65292;&#21516;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15239v1 Announce Type: cross  Abstract: We address motion generation for high-DoF robot arms in complex settings with obstacles, via points, etc. A significant advancement in this domain is achieved by integrating Learning from Demonstration (LfD) into the motion generation process. This integration facilitates rapid adaptation to new tasks and optimizes the utilization of accumulated expertise by allowing robots to learn and generalize from demonstrated trajectories.   We train a transformer architecture on a large dataset of simulated trajectories. This architecture, based on a conditional variational autoencoder transformer, learns essential motion generation skills and adapts these to meet auxiliary tasks and constraints. Our auto-regressive approach enables real-time integration of feedback from the physical system, enhancing the adaptability and efficiency of motion generation. We show that our model can generate motion from initial and target points, but also that it 
&lt;/p&gt;</description></item><item><title>LS&#26041;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#24179;&#28369;&#25928;&#26524;&#34987;&#21457;&#29616;&#20250;&#36127;&#38754;&#24433;&#21709;&#36873;&#25321;&#24615;&#20998;&#31867;&#65292;&#36890;&#36807;&#24433;&#21709;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#27492;&#30740;&#31350;&#38416;&#26126;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.14715</link><description>&lt;p&gt;
&#29702;&#35299;&#20026;&#20309;&#26631;&#31614;&#24179;&#28369;&#20250;&#38477;&#20302;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#25928;&#26524;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14715
&lt;/p&gt;
&lt;p&gt;
LS&#26041;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#24179;&#28369;&#25928;&#26524;&#34987;&#21457;&#29616;&#20250;&#36127;&#38754;&#24433;&#21709;&#36873;&#25321;&#24615;&#20998;&#31867;&#65292;&#36890;&#36807;&#24433;&#21709;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#27492;&#30740;&#31350;&#38416;&#26126;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#65288;LS&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#22312;&#25552;&#39640;&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#65292;&#24182;&#19988;&#23454;&#29616;&#31616;&#21333;&#12290;"&#30828;"&#30340;one-hot&#26631;&#31614;&#36890;&#36807;&#23558;&#27010;&#29575;&#36136;&#37327;&#22343;&#21248;&#20998;&#37197;&#32473;&#20854;&#20182;&#31867;&#21035;&#26469;&#36827;&#34892;"&#24179;&#28369;&#21270;"&#65292;&#20174;&#32780;&#20943;&#23569;&#36807;&#24230;&#25311;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LS&#22914;&#20309;&#36127;&#38754;&#24433;&#21709;&#36873;&#25321;&#24615;&#20998;&#31867;&#65288;SC&#65289;- &#20854;&#30446;&#26631;&#26159;&#21033;&#29992;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26469;&#25298;&#32477;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#21644;&#26550;&#26500;&#20013;&#20174;&#32463;&#39564;&#19978;&#35777;&#26126;LS&#20250;&#23548;&#33268;SC&#30340;&#19968;&#33268;&#24615;&#38477;&#32423;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;logit&#32423;&#21035;&#30340;&#26799;&#24230;&#26469;&#35299;&#37322;&#36825;&#19968;&#28857;&#65292;&#34920;&#26126;LS&#36890;&#36807;&#22312;&#38169;&#35823;&#27010;&#29575;&#20302;&#26102;&#26356;&#21152;&#27491;&#21017;&#21270;&#26368;&#22823;logit&#65292;&#32780;&#22312;&#38169;&#35823;&#27010;&#29575;&#39640;&#26102;&#26356;&#23569;&#27491;&#21017;&#21270;&#65292;&#21152;&#21095;&#20102;&#36807;&#24230;&#33258;&#20449;&#21644;&#20302;&#33258;&#20449;&#12290;&#36825;&#38416;&#26126;&#20102;&#20197;&#21069;&#25253;&#36947;&#30340;&#24378;&#20998;&#31867;&#22120;&#22312;SC&#20013;&#24615;&#33021;&#19981;&#20339;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14715v1 Announce Type: cross  Abstract: Label smoothing (LS) is a popular regularisation method for training deep neural network classifiers due to its effectiveness in improving test accuracy and its simplicity in implementation. "Hard" one-hot labels are "smoothed" by uniformly distributing probability mass to other classes, reducing overfitting. In this work, we reveal that LS negatively affects selective classification (SC) - where the aim is to reject misclassifications using a model's predictive uncertainty. We first demonstrate empirically across a range of tasks and architectures that LS leads to a consistent degradation in SC. We then explain this by analysing logit-level gradients, showing that LS exacerbates overconfidence and underconfidence by regularising the max logit more when the probability of error is low, and less when the probability of error is high. This elucidates previously reported experimental results where strong classifiers underperform in SC. We
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#27169;&#25311;&#21644;&#20248;&#21270;&#30340;&#20219;&#21153;&#26368;&#20248;&#25968;&#25454;&#39537;&#21160;&#26367;&#20195;&#27169;&#22411;&#26041;&#27861;&#65292;&#22312;eNMPC&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20026;&#23454;&#29616;&#26356;&#20855;&#33021;&#21147;&#30340;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.14425</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#24494;&#20998;&#27169;&#25311;&#21644;&#20248;&#21270;&#30340;&#20219;&#21153;&#26368;&#20248;&#25968;&#25454;&#39537;&#21160;&#26367;&#20195;&#27169;&#22411;&#29992;&#20110;eNMPC
&lt;/p&gt;
&lt;p&gt;
Task-optimal data-driven surrogate models for eNMPC via differentiable simulation and optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14425
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#27169;&#25311;&#21644;&#20248;&#21270;&#30340;&#20219;&#21153;&#26368;&#20248;&#25968;&#25454;&#39537;&#21160;&#26367;&#20195;&#27169;&#22411;&#26041;&#27861;&#65292;&#22312;eNMPC&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20026;&#23454;&#29616;&#26356;&#20855;&#33021;&#21147;&#30340;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25511;&#21046;&#20013;&#20248;&#21270;&#24615;&#33021;&#30340;Koopman&#26367;&#20195;&#27169;&#22411;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#37319;&#29992;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#36129;&#29486;&#30456;&#21453;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#35757;&#32451;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#26426;&#26800;&#27169;&#25311;&#27169;&#22411;&#30340;&#29615;&#22659;&#30340;&#28508;&#22312;&#21487;&#24494;&#24615;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#25991;&#29486;&#24050;&#30693;&#30340;eNMPC&#26696;&#20363;&#30740;&#31350;&#20013;&#20854;&#20182;&#25511;&#21046;&#22120;&#31867;&#22411;&#21644;&#35757;&#32451;&#31639;&#27861;&#32452;&#21512;&#30340;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#22240;&#27492;&#22312;&#20351;&#29992;&#21160;&#24577;&#26367;&#20195;&#27169;&#22411;&#30340;&#26356;&#26377;&#33021;&#21147;&#30340;&#25511;&#21046;&#22120;&#26041;&#38754;&#26500;&#25104;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14425v1 Announce Type: new  Abstract: We present a method for end-to-end learning of Koopman surrogate models for optimal performance in control. In contrast to previous contributions that employ standard reinforcement learning (RL) algorithms, we use a training algorithm that exploits the potential differentiability of environments based on mechanistic simulation models. We evaluate the performance of our method by comparing it to that of other controller type and training algorithm combinations on a literature known eNMPC case study. Our method exhibits superior performance on this problem, thereby constituting a promising avenue towards more capable controllers that employ dynamic surrogate models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29983;&#25104;&#27969;&#34433;&#32676;&#37319;&#26679;&#22120;&#65288;GFACS&#65289;&#65292;&#19968;&#31181;&#32467;&#21512;&#29983;&#25104;&#27969;&#32593;&#32476;&#19982;&#34433;&#32676;&#20248;&#21270;&#26041;&#27861;&#30340;&#31070;&#32463;&#24341;&#23548;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22312;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;ACO&#31639;&#27861;&#24182;&#19982;&#29305;&#23450;&#38382;&#39064;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.07041</link><description>&lt;p&gt;
&#20351;&#29992;GFlowNets&#30340;&#34433;&#32676;&#37319;&#26679;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Ant Colony Sampling with GFlowNets for Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29983;&#25104;&#27969;&#34433;&#32676;&#37319;&#26679;&#22120;&#65288;GFACS&#65289;&#65292;&#19968;&#31181;&#32467;&#21512;&#29983;&#25104;&#27969;&#32593;&#32476;&#19982;&#34433;&#32676;&#20248;&#21270;&#26041;&#27861;&#30340;&#31070;&#32463;&#24341;&#23548;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22312;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;ACO&#31639;&#27861;&#24182;&#19982;&#29305;&#23450;&#38382;&#39064;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29983;&#25104;&#27969;&#34433;&#32676;&#37319;&#26679;&#22120;&#65288;GFACS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#31070;&#32463;&#24341;&#23548;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;GFACS &#23558;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#19982;&#34433;&#32676;&#20248;&#21270;&#65288;ACO&#65289;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;GFlowNets &#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#22312;&#32452;&#21512;&#31354;&#38388;&#20013;&#23398;&#20064;&#26500;&#36896;&#24615;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#22270;&#23454;&#20363;&#19978;&#25552;&#20379;&#20915;&#31574;&#21464;&#37327;&#30340;&#30693;&#24773;&#20808;&#39564;&#20998;&#24067;&#26469;&#22686;&#24378; ACO&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#25216;&#24039;&#32452;&#21512;&#65292;&#21253;&#25324;&#25628;&#32034;&#24341;&#23548;&#30340;&#23616;&#37096;&#25506;&#32034;&#12289;&#33021;&#37327;&#24402;&#19968;&#21270;&#21644;&#33021;&#37327;&#22609;&#24418;&#65292;&#20197;&#25552;&#39640; GFACS &#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GFACS &#22312;&#19971;&#20010;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#32447; ACO &#31639;&#27861;&#65292;&#24182;&#19988;&#22312;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#38382;&#39064;&#29305;&#23450;&#21551;&#21457;&#24335;&#26041;&#27861;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312; \url{https://github.com/ai4co/gfacs} &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07041v1 Announce Type: new  Abstract: This paper introduces the Generative Flow Ant Colony Sampler (GFACS), a novel neural-guided meta-heuristic algorithm for combinatorial optimization. GFACS integrates generative flow networks (GFlowNets) with the ant colony optimization (ACO) methodology. GFlowNets, a generative model that learns a constructive policy in combinatorial spaces, enhance ACO by providing an informed prior distribution of decision variables conditioned on input graph instances. Furthermore, we introduce a novel combination of training tricks, including search-guided local exploration, energy normalization, and energy shaping to improve GFACS. Our experimental results demonstrate that GFACS outperforms baseline ACO algorithms in seven CO tasks and is competitive with problem-specific heuristics for vehicle routing problems. The source code is available at \url{https://github.com/ai4co/gfacs}.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#28145;&#20837;&#25506;&#35752;&#20102;&#36793;&#32536;&#23398;&#20064;(EL)&#20013;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#21508;&#31181;&#26041;&#27861;&#21644;&#26041;&#27861;&#35770;&#65292;&#26088;&#22312;&#32508;&#21512;&#29616;&#26377;&#30693;&#35782;&#65292;&#35782;&#21035;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#26410;&#26469;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02619</link><description>&lt;p&gt;
&#22312;&#36793;&#32536;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Training Machine Learning models at the Edge: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02619
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#28145;&#20837;&#25506;&#35752;&#20102;&#36793;&#32536;&#23398;&#20064;(EL)&#20013;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#21508;&#31181;&#26041;&#27861;&#21644;&#26041;&#27861;&#35770;&#65292;&#26088;&#22312;&#32508;&#21512;&#29616;&#26377;&#30693;&#35782;&#65292;&#35782;&#21035;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#26410;&#26469;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#35745;&#31639;(EC)&#36817;&#24180;&#26469;&#33719;&#24471;&#20102;&#26174;&#33879;&#20851;&#27880;&#65292;&#36890;&#36807;&#22312;&#36793;&#32536;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;(AI)&#33021;&#21147;&#65292;&#25215;&#35834;&#25552;&#39640;&#25928;&#29575;&#12290;&#34429;&#28982;&#20027;&#35201;&#20851;&#27880;&#28857;&#22312;&#36793;&#32536;&#37096;&#32626;&#21644;&#25512;&#26029;&#26426;&#22120;&#23398;&#20064;(ML)&#27169;&#22411;&#65292;&#20294;&#35757;&#32451;&#26041;&#38754;&#20173;&#28982;&#36739;&#23569;&#34987;&#25506;&#35752;&#12290;&#36825;&#39033;&#35843;&#30740;&#28145;&#20837;&#25506;&#35752;&#20102;&#36793;&#32536;&#23398;&#20064;(EL)&#65292;&#29305;&#21035;&#26159;&#22312;&#36793;&#32536;&#20248;&#21270;ML&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#38754;&#12290;&#20854;&#30446;&#26631;&#26159;&#20840;&#38754;&#25506;&#35752;EL&#20013;&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#26041;&#27861;&#35770;&#65292;&#32508;&#21512;&#29616;&#26377;&#30693;&#35782;&#65292;&#35782;&#21035;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#26410;&#26469;&#36235;&#21183;&#12290;&#21033;&#29992;Scopus&#30340;&#39640;&#32423;&#25628;&#32034;&#65292;&#30830;&#23450;&#20102;&#20851;&#20110;EL&#30340;&#30456;&#20851;&#25991;&#29486;&#65292;&#26174;&#31034;&#20102;&#30740;&#31350;&#24037;&#20316;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#26041;&#38754;&#30340;&#32858;&#28966;&#65292;&#29305;&#21035;&#26159;&#32852;&#37030;&#23398;&#20064;(FL)&#12290;&#27492;&#35843;&#30740;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#27604;&#36739;&#29992;&#20110;&#20248;&#21270;&#36793;&#32536;&#23398;&#20064;&#30340;ML&#30340;&#25216;&#26415;&#30340;&#25351;&#21335;&#65292;&#20197;&#21450;&#23545;&#19981;&#21516;&#26694;&#26550;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02619v1 Announce Type: new  Abstract: Edge Computing (EC) has gained significant traction in recent years, promising enhanced efficiency by integrating Artificial Intelligence (AI) capabilities at the edge. While the focus has primarily been on the deployment and inference of Machine Learning (ML) models at the edge, the training aspect remains less explored. This survey delves into Edge Learning (EL), specifically the optimization of ML model training at the edge. The objective is to comprehensively explore diverse approaches and methodologies in EL, synthesize existing knowledge, identify challenges, and highlight future trends. Utilizing Scopus' advanced search, relevant literature on EL was identified, revealing a concentration of research efforts in distributed learning methods, particularly Federated Learning (FL). This survey further provides a guideline for comparing techniques used to optimize ML for edge learning, along with an exploration of different frameworks, 
&lt;/p&gt;</description></item><item><title>&#36870;&#21521;&#36719; Q &#23398;&#20064;&#29992;&#20110;&#33719;&#24471;&#27425;&#20248;&#28436;&#31034;&#30340;&#31163;&#32447;&#27169;&#20223;&#25361;&#25112;&#20102;&#31163;&#32447; IL &#20013;&#26377;&#38480;&#25903;&#25345;&#19987;&#23478;&#28436;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20197;&#21305;&#37197;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#30340;&#21344;&#29992;&#20998;&#24067;</title><link>https://arxiv.org/abs/2402.13147</link><description>&lt;p&gt;
SubIQ: &#36870;&#21521;&#36719; Q &#23398;&#20064;&#29992;&#20110;&#33719;&#24471;&#27425;&#20248;&#28436;&#31034;&#30340;&#31163;&#32447;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13147
&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#36719; Q &#23398;&#20064;&#29992;&#20110;&#33719;&#24471;&#27425;&#20248;&#28436;&#31034;&#30340;&#31163;&#32447;&#27169;&#20223;&#25361;&#25112;&#20102;&#31163;&#32447; IL &#20013;&#26377;&#38480;&#25903;&#25345;&#19987;&#23478;&#28436;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20197;&#21305;&#37197;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#30340;&#21344;&#29992;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#65292;&#26088;&#22312;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#27169;&#20223;&#19987;&#23478;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#36827;&#19968;&#27493;&#20132;&#20114;&#12290;&#22312;&#31163;&#32447; IL &#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22788;&#29702;&#20165;&#28085;&#30422;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#19968;&#23567;&#37096;&#20998;&#30340;&#19987;&#23478;&#28436;&#31034;&#30340;&#26377;&#38480;&#25903;&#25345;&#12290;&#25105;&#20204;&#32771;&#34385;&#31163;&#32447; IL&#65292;&#20854;&#20013;&#19987;&#23478;&#28436;&#31034;&#21463;&#21040;&#38480;&#21046;&#65292;&#20294;&#26159;&#30001;&#26356;&#22823;&#35268;&#27169;&#30340;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#34917;&#20805;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#29992;&#20110;&#27492;&#35774;&#32622;&#30340;&#31163;&#32447; IL &#26041;&#27861;&#22522;&#20110;&#34892;&#20026;&#20811;&#38534;&#25110;&#20998;&#24067;&#21305;&#37197;&#65292;&#20854;&#30446;&#30340;&#26159;&#23558;&#27169;&#20223;&#31574;&#30053;&#30340;&#21344;&#29992;&#20998;&#24067;&#19982;&#19987;&#23478;&#31574;&#30053;&#30340;&#21344;&#29992;&#20998;&#24067;&#21305;&#37197;&#12290;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#22240;&#20026;&#19987;&#23478;&#28436;&#31034;&#26377;&#38480;&#65292;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#21344;&#29992;&#20998;&#24067;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#35268;&#27169;&#26356;&#22823;&#65292;&#26377;&#24456;&#39640;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13147v1 Announce Type: cross  Abstract: We consider offline imitation learning (IL), which aims to mimic the expert's behavior from its demonstration without further interaction with the environment. One of the main challenges in offline IL is dealing with the limited support of expert demonstrations that cover only a small fraction of the state-action spaces. In this work, we consider offline IL, where expert demonstrations are limited but complemented by a larger set of sub-optimal demonstrations of lower expertise levels. Most of the existing offline IL methods developed for this setting are based on behavior cloning or distribution matching, where the aim is to match the occupancy distribution of the imitation policy with that of the expert policy. Such an approach often suffers from over-fitting, as expert demonstrations are limited to accurately represent any occupancy distribution. On the other hand, since sub-optimal sets are much larger, there is a high chance that 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#39134;&#26426;&#26426;&#36523;&#21160;&#24577;&#27700;&#38477;&#36733;&#33655;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#25110;Koopman&#31639;&#23376;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10724</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27700;&#38477;&#36733;&#33655;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Machine Learning based Prediction of Ditching Loads
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#39134;&#26426;&#26426;&#36523;&#21160;&#24577;&#27700;&#38477;&#36733;&#33655;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#25110;Koopman&#31639;&#23376;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#39134;&#26426;&#26426;&#36523;&#21160;&#24577;&#27700;&#38477;&#36733;&#33655;&#30340;&#26041;&#27861;&#12290;&#25152;&#37319;&#29992;&#30340;&#23398;&#20064;&#36807;&#31243;&#20998;&#20026;&#20004;&#37096;&#20998;&#65292;&#21363;&#20351;&#29992;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#65288;CAE&#65289;&#37325;&#26500;&#31354;&#38388;&#36733;&#33655;&#65292;&#20197;&#21450;&#22312;&#38543;&#21518;&#30340;&#37096;&#20998;&#20013;&#36825;&#20123;&#36733;&#33655;&#30340;&#30636;&#26102;&#28436;&#21270;&#12290;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;CAE&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#25110;&#22522;&#20110;Koopman&#31639;&#23376;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#39044;&#27979;&#30636;&#26102;&#34892;&#20026;&#12290;&#35757;&#32451;&#25968;&#25454;&#26159;&#36890;&#36807;von-Karman&#21644;Wagner&#30340;&#21160;&#37327;&#26041;&#27861;&#30340;&#25193;&#23637;&#32534;&#21046;&#30340;&#65292;&#35757;&#32451;&#26041;&#27861;&#30340;&#22522;&#26412;&#21407;&#29702;&#34987;&#31616;&#35201;&#24635;&#32467;&#12290;&#25152;&#28041;&#21450;&#30340;&#24212;&#29992;&#26159;&#25351;DLR-D150&#39134;&#26426;&#30340;&#20840;&#23610;&#23544;&#26426;&#36523;&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#27700;&#24179;&#21644;&#22402;&#30452;&#36827;&#22330;&#36895;&#24230;&#65292;&#20837;&#23556;&#35282;&#20026;6&#176;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#25152;&#26377;&#22235;&#20010;&#30740;&#31350;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#37319;&#29992;LSTM&#32467;&#21512;...&#65288;&#20869;&#23481;&#32570;&#22833;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10724v1 Announce Type: new  Abstract: We present approaches to predict dynamic ditching loads on aircraft fuselages using machine learning. The employed learning procedure is structured into two parts, the reconstruction of the spatial loads using a convolutional autoencoder (CAE) and the transient evolution of these loads in a subsequent part. Different CAE strategies are assessed and combined with either long short-term memory (LSTM) networks or Koopman-operator based methods to predict the transient behaviour. The training data is compiled by an extension of the momentum method of von-Karman and Wagner and the rationale of the training approach is briefly summarised. The application included refers to a full-scale fuselage of a DLR-D150 aircraft for a range of horizontal and vertical approach velocities at 6{\deg} incidence. Results indicate a satisfactory level of predictive agreement for all four investigated surrogate models examined, with the combination of an LSTM an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27491;&#21017;&#21270;&#22238;&#24402;&#65292;&#22312;&#36229;&#21442;&#25968;&#21270;&#33539;&#22260;&#20869;&#65292;&#26681;&#25454;&#29305;&#23450;&#36873;&#25321;&#30340;&#20984;&#20989;&#25968;&#24182;&#36866;&#24403;&#22686;&#21152;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#21487;&#20197;&#23454;&#29616;&#31232;&#30095;&#21644;&#19968;&#20301;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#24615;&#33021;&#20960;&#20046;&#19982;&#26368;&#20339;&#20998;&#31867;&#24615;&#33021;&#30456;&#21516;&#12290;</title><link>https://arxiv.org/abs/2402.10474</link><description>&lt;p&gt;
&#19968;&#20301;&#37327;&#21270;&#21644;&#31232;&#30095;&#21270;&#29992;&#20110;&#22810;&#31867;&#32447;&#24615;&#20998;&#31867;&#30340;&#27491;&#21017;&#21270;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
One-Bit Quantization and Sparsification for Multiclass Linear Classification via Regularized Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10474
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#21270;&#22238;&#24402;&#65292;&#22312;&#36229;&#21442;&#25968;&#21270;&#33539;&#22260;&#20869;&#65292;&#26681;&#25454;&#29305;&#23450;&#36873;&#25321;&#30340;&#20984;&#20989;&#25968;&#24182;&#36866;&#24403;&#22686;&#21152;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#21487;&#20197;&#23454;&#29616;&#31232;&#30095;&#21644;&#19968;&#20301;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#24615;&#33021;&#20960;&#20046;&#19982;&#26368;&#20339;&#20998;&#31867;&#24615;&#33021;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#22238;&#24402;&#20013;&#29992;&#20110;&#22810;&#31867;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#36229;&#21442;&#25968;&#21270;&#33539;&#22260;&#20869;&#65292;&#35757;&#32451;&#25968;&#25454;&#20013;&#19968;&#20123;&#26631;&#35760;&#38169;&#35823;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20026;&#20102;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#38169;&#35823;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#38656;&#35201;&#28155;&#21152;&#19968;&#20010;&#26174;&#24335;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;$\lambda f(w)$&#65292;&#20854;&#20013;$f(\cdot)$&#26159;&#26576;&#20010;&#20984;&#20989;&#25968;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#25968;&#25454;&#26159;&#20174;&#19968;&#20010;&#20855;&#26377;&#30456;&#31561;&#31867;&#22823;&#23567;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#37319;&#26679;&#30340;&#65292;&#24182;&#19988;&#27599;&#20010;&#31867;&#21035;&#30340;&#35757;&#32451;&#26631;&#31614;&#20013;&#26377;&#19968;&#37096;&#20998;&#27604;&#20363;&#20026;$c$&#26159;&#38169;&#35823;&#30340;&#12290;&#22312;&#36825;&#20123;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;$f(\cdot) = \|\cdot\|^2_2$&#19988;$\lambda \to \infty$&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#25105;&#20204;&#32487;&#32493;&#20998;&#26512;&#20102;&#22312;&#22823;$\lambda$&#33539;&#22260;&#20869;$f(\cdot) = \|\cdot\|_1$&#21644;$f(\cdot) = \|\cdot\|_\infty$&#30340;&#20998;&#31867;&#38169;&#35823;&#65292;&#24182;&#19988;&#27880;&#24847;&#21040;&#36890;&#24120;&#21487;&#20197;&#25214;&#21040;&#31232;&#30095;&#21644;&#19968;&#20301;&#35299;&#20915;&#26041;&#26696;&#65292;&#20998;&#21035;&#34920;&#29616;&#20960;&#20046;&#19982;$f(\cdot) = \|\cdot\|^2_2$&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10474v1 Announce Type: new  Abstract: We study the use of linear regression for multiclass classification in the over-parametrized regime where some of the training data is mislabeled. In such scenarios it is necessary to add an explicit regularization term, $\lambda f(w)$, for some convex function $f(\cdot)$, to avoid overfitting the mislabeled data. In our analysis, we assume that the data is sampled from a Gaussian Mixture Model with equal class sizes, and that a proportion $c$ of the training labels is corrupted for each class. Under these assumptions, we prove that the best classification performance is achieved when $f(\cdot) = \|\cdot\|^2_2$ and $\lambda \to \infty$. We then proceed to analyze the classification errors for $f(\cdot) = \|\cdot\|_1$ and $f(\cdot) = \|\cdot\|_\infty$ in the large $\lambda$ regime and notice that it is often possible to find sparse and one-bit solutions, respectively, that perform almost as well as the one corresponding to $f(\cdot) = \|\
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;ExpD3 &#21644; BE-TD3&#65292;&#29992;&#20110;&#35299;&#20915;&#21644;&#21033;&#29992; Actor-Critic &#26041;&#27861;&#20013;&#30340;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.09078</link><description>&lt;p&gt;
&#22312; Actor-Critic &#26041;&#27861;&#20013;&#21033;&#29992;&#20272;&#35745;&#20559;&#24046;&#30340;&#28145;&#24230;&#21452; Q-Learning &#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploiting Estimation Bias in Deep Double Q-Learning for Actor-Critic Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;ExpD3 &#21644; BE-TD3&#65292;&#29992;&#20110;&#35299;&#20915;&#21644;&#21033;&#29992; Actor-Critic &#26041;&#27861;&#20013;&#30340;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#21644;&#21033;&#29992;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013; Actor-Critic &#26041;&#27861;&#20013;&#30340;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#65292;&#20351;&#29992;&#20102;&#28145;&#24230;&#21452; Q-Learning&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#31639;&#27861;&#65306;Expectile Delayed Deep Deterministic Policy Gradient (ExpD3) &#21644; Bias Exploiting - Twin Delayed Deep Deterministic Policy Gradient (BE-TD3)&#12290;ExpD3 &#26088;&#22312;&#36890;&#36807;&#21333;&#19968;&#30340; Q &#20272;&#35745;&#26469;&#20943;&#23569;&#36807;&#24230;&#20272;&#35745;&#20559;&#24046;&#65292;&#24182;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#20043;&#38388;&#25552;&#20379;&#24179;&#34913;&#65292;&#32780; BE-TD3 &#21017;&#26088;&#22312;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#36873;&#25321;&#26368;&#26377;&#21033;&#30340;&#20272;&#35745;&#20559;&#24046;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#31639;&#27861;&#22312;&#19982; TD3 &#31561;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23588;&#20854;&#26159;&#22312;&#20272;&#35745;&#20559;&#24046;&#26174;&#33879;&#24433;&#21709;&#23398;&#20064;&#30340;&#29615;&#22659;&#20013;&#65292;&#21487;&#20197;&#21305;&#25932;&#25110;&#36229;&#36234;&#23427;&#20204;&#12290;&#36825;&#20123;&#32467;&#26524;&#20984;&#26174;&#20102;&#20272;&#35745;&#20559;&#24046;&#23545;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09078v1 Announce Type: cross Abstract: This paper introduces innovative methods in Reinforcement Learning (RL), focusing on addressing and exploiting estimation biases in Actor-Critic methods for continuous control tasks, using Deep Double Q-Learning. We propose two novel algorithms: Expectile Delayed Deep Deterministic Policy Gradient (ExpD3) and Bias Exploiting - Twin Delayed Deep Deterministic Policy Gradient (BE-TD3). ExpD3 aims to reduce overestimation bias with a single $Q$ estimate, offering a balance between computational efficiency and performance, while BE-TD3 is designed to dynamically select the most advantageous estimation bias during training. Our extensive experiments across various continuous control tasks demonstrate the effectiveness of our approaches. We show that these algorithms can either match or surpass existing methods like TD3, particularly in environments where estimation biases significantly impact learning. The results underline the importance of
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#27604;&#20363;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#37319;&#26679;&#21644;&#25237;&#31080;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#24615;&#33021;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22797;&#26434;&#26041;&#27861;&#27491;&#20132;&#12290;</title><link>https://arxiv.org/abs/2402.05120</link><description>&lt;p&gt;
&#26356;&#22810;&#30340;&#20195;&#29702;&#23601;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
More Agents Is All You Need
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05120
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#27604;&#20363;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#37319;&#26679;&#21644;&#25237;&#31080;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#24615;&#33021;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22797;&#26434;&#26041;&#27861;&#27491;&#20132;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#36890;&#36807;&#19968;&#31181;&#37319;&#26679;&#21644;&#25237;&#31080;&#30340;&#26041;&#27861;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Models, LLMs)&#30340;&#24615;&#33021;&#19982;&#23454;&#20363;&#21270;&#30340;&#20195;&#29702;&#25968;&#37327;&#25104;&#27604;&#20363;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#24050;&#26377;&#30340;&#22797;&#26434;&#26041;&#27861;&#36827;&#19968;&#27493;&#22686;&#24378;LLMs&#26159;&#27491;&#20132;&#30340;&#65292;&#32780;&#22686;&#24378;&#30340;&#31243;&#24230;&#19982;&#20219;&#21153;&#30340;&#22256;&#38590;&#31243;&#24230;&#30456;&#20851;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#33021;&#22815;&#20419;&#36827;&#20854;&#21457;&#29983;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#22312;&#20197;&#19979;&#32593;&#22336;: \url{https://anonymous.4open.science/r/more_agent_is_all_you_need}
&lt;/p&gt;
&lt;p&gt;
We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: \url{https://anonymous.4open.science/r/more_agent_is_all_you_need}.
&lt;/p&gt;</description></item><item><title>SMOTE&#26159;&#19968;&#31181;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#24120;&#29992;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#22797;&#21046;&#21407;&#22987;&#23569;&#25968;&#26679;&#26412;&#26469;&#37325;&#26032;&#29983;&#25104;&#21407;&#22987;&#20998;&#24067;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;SMOTE&#30340;&#23494;&#24230;&#22312;&#23569;&#25968;&#26679;&#26412;&#20998;&#24067;&#30340;&#36793;&#30028;&#38468;&#36817;&#36880;&#28176;&#20943;&#23567;&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;BorderLine SMOTE&#31574;&#30053;&#30340;&#21512;&#29702;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;SMOTE&#30456;&#20851;&#31574;&#30053;&#65292;&#24182;&#19982;&#20854;&#20182;&#37325;&#26032;&#24179;&#34913;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#26368;&#32456;&#21457;&#29616;&#65292;&#22312;&#25968;&#25454;&#38598;&#26497;&#24230;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;SMOTE&#12289;&#25552;&#20986;&#30340;&#26041;&#27861;&#25110;&#27424;&#37319;&#26679;&#31243;&#24207;&#26159;&#26368;&#20339;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03819</link><description>&lt;p&gt;
SMOTE&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#65306;&#20851;&#20110;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#30340;&#38480;&#21046;&#21644;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Theoretical and experimental study of SMOTE: limitations and comparisons of rebalancing strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03819
&lt;/p&gt;
&lt;p&gt;
SMOTE&#26159;&#19968;&#31181;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#24120;&#29992;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#22797;&#21046;&#21407;&#22987;&#23569;&#25968;&#26679;&#26412;&#26469;&#37325;&#26032;&#29983;&#25104;&#21407;&#22987;&#20998;&#24067;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;SMOTE&#30340;&#23494;&#24230;&#22312;&#23569;&#25968;&#26679;&#26412;&#20998;&#24067;&#30340;&#36793;&#30028;&#38468;&#36817;&#36880;&#28176;&#20943;&#23567;&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;BorderLine SMOTE&#31574;&#30053;&#30340;&#21512;&#29702;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;SMOTE&#30456;&#20851;&#31574;&#30053;&#65292;&#24182;&#19982;&#20854;&#20182;&#37325;&#26032;&#24179;&#34913;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#26368;&#32456;&#21457;&#29616;&#65292;&#22312;&#25968;&#25454;&#38598;&#26497;&#24230;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;SMOTE&#12289;&#25552;&#20986;&#30340;&#26041;&#27861;&#25110;&#27424;&#37319;&#26679;&#31243;&#24207;&#26159;&#26368;&#20339;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SMOTE&#65288;Synthetic Minority Oversampling Technique&#65289;&#26159;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24120;&#29992;&#30340;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#28176;&#36827;&#24773;&#20917;&#19979;&#65292;SMOTE&#65288;&#40664;&#35748;&#21442;&#25968;&#65289;&#36890;&#36807;&#31616;&#21333;&#22797;&#21046;&#21407;&#22987;&#23569;&#25968;&#26679;&#26412;&#26469;&#37325;&#26032;&#29983;&#25104;&#21407;&#22987;&#20998;&#24067;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#23569;&#25968;&#26679;&#26412;&#20998;&#24067;&#30340;&#25903;&#25345;&#36793;&#30028;&#38468;&#36817;&#65292;SMOTE&#30340;&#23494;&#24230;&#20250;&#20943;&#23567;&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;&#24120;&#35265;&#30340;BorderLine SMOTE&#31574;&#30053;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;SMOTE&#30456;&#20851;&#31574;&#30053;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#29616;&#26377;&#30340;&#37325;&#26032;&#24179;&#34913;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#26377;&#24403;&#25968;&#25454;&#38598;&#26497;&#24230;&#19981;&#24179;&#34913;&#26102;&#25165;&#38656;&#35201;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#12290;&#23545;&#20110;&#36825;&#31181;&#25968;&#25454;&#38598;&#65292;SMOTE&#12289;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25110;&#27424;&#37319;&#26679;&#31243;&#24207;&#26159;&#26368;&#20339;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic Minority Oversampling Technique (SMOTE) is a common rebalancing strategy for handling imbalanced data sets. Asymptotically, we prove that SMOTE (with default parameter) regenerates the original distribution by simply copying the original minority samples. We also prove that SMOTE density vanishes near the boundary of the support of the minority distribution, therefore justifying the common BorderLine SMOTE strategy. Then we introduce two new SMOTE-related strategies, and compare them with state-of-the-art rebalancing procedures. We show that rebalancing strategies are only required when the data set is highly imbalanced. For such data sets, SMOTE, our proposals, or undersampling procedures are the best strategies.
&lt;/p&gt;</description></item><item><title>DeLLMa&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20915;&#31574;&#31934;&#24230;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02392</link><description>&lt;p&gt;
DeLLMa:&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#20915;&#31574;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02392
&lt;/p&gt;
&lt;p&gt;
DeLLMa&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20915;&#31574;&#31934;&#24230;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#12289;&#24037;&#31243;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#39046;&#22495;&#24448;&#24448;&#38754;&#20020;&#20915;&#31574;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#20915;&#31574;&#38382;&#39064;&#19978;&#30452;&#25509;&#20351;&#29992;LLMs&#24448;&#24448;&#25928;&#26524;&#36739;&#24046;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#39064;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeLLMa&#65288;Decision-making Large Language Model assistant&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#31934;&#24230;&#12290;DeLLMa&#21253;&#25324;&#19968;&#20010;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20102;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#20248;&#30340;&#12289;&#21487;&#23457;&#35745;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#30495;&#23454;&#20892;&#19994;&#21644;&#37329;&#34701;&#25968;&#25454;&#30340;&#20915;&#31574;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;DeLLMa&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLMs&#30340;&#20915;&#31574;&#24615;&#33021;&#65292;&#20934;&#30830;&#24615;&#21487;&#25552;&#39640;&#39640;&#36798;40%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine. These fields often grapple with decision-making under uncertainty, a critical yet challenging task. In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases. To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process. We validate our framework on decision-making environments involving real agriculture and finance data. Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#22343;&#21248;&#20998;&#24067;&#20551;&#35774;&#30340;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#23436;&#20840;&#38543;&#26426;&#36873;&#25321;&#20551;&#35774;&#30340;&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#22120;&#65292;&#20197;&#21450;&#39118;&#38505;&#26657;&#27491;&#26041;&#27861;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.15502</link><description>&lt;p&gt;
&#36873;&#21462;&#23436;&#20840;&#38543;&#26426;&#30340;&#20114;&#34917;&#26631;&#31614;&#26159;&#22810;&#31867;&#21035;&#20998;&#31867;&#30340;&#23454;&#29992;&#24369;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Selected-completely-at-random Complementary Label is a Practical Weak Supervision for Multi-class Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15502
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#22343;&#21248;&#20998;&#24067;&#20551;&#35774;&#30340;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#23436;&#20840;&#38543;&#26426;&#36873;&#25321;&#20551;&#35774;&#30340;&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#22120;&#65292;&#20197;&#21450;&#39118;&#38505;&#26657;&#27491;&#26041;&#27861;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#26159;&#19968;&#20010;&#24369;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#20851;&#32852;&#30528;&#19968;&#20010;&#25110;&#22810;&#20010;&#20114;&#34917;&#26631;&#31614;&#65292;&#25351;&#31034;&#20854;&#19981;&#23646;&#20110;&#30340;&#31867;&#21035;&#12290;&#29616;&#26377;&#30340;&#19968;&#33268;&#26041;&#27861;&#20381;&#36182;&#20110;&#22343;&#21248;&#20998;&#24067;&#20551;&#35774;&#26469;&#27169;&#25311;&#20114;&#34917;&#26631;&#31614;&#30340;&#29983;&#25104;&#65292;&#25110;&#32773;&#20381;&#36182;&#20110;&#19968;&#20010;&#26222;&#36890;&#26631;&#31614;&#30340;&#35757;&#32451;&#38598;&#26469;&#20272;&#35745;&#38750;&#22343;&#21248;&#24773;&#20917;&#19979;&#30340;&#36716;&#31227;&#30697;&#38453;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24773;&#20917;&#19979;&#36825;&#20004;&#20010;&#26465;&#20214;&#21487;&#33021;&#19981;&#20250;&#34987;&#28385;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#20110;&#36825;&#20123;&#26465;&#20214;&#12290;&#21463;&#21040;PU&#23398;&#20064;&#25991;&#29486;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23436;&#20840;&#38543;&#26426;&#36873;&#25321;&#20551;&#35774;&#30340;&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#39118;&#38505;&#26657;&#27491;&#26041;&#27861;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#21487;&#20197;&#34987;&#34920;&#36798;&#20026;&#19968;&#32452;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15502v2 Announce Type: replace  Abstract: Complementary-label learning is a weakly supervised learning problem in which each training example is associated with one or multiple complementary labels indicating the classes to which it does not belong. Existing consistent approaches have relied on the uniform distribution assumption to model the generation of complementary labels, or on an ordinary-label training set to estimate the transition matrix in non-uniform cases. However, either condition may not be satisfied in real-world scenarios. In this paper, we propose a novel consistent approach that does not rely on these conditions. Inspired by the positive-unlabeled (PU) learning literature, we propose an unbiased risk estimator based on the Selected Completely At Random assumption for complementary-label learning. We then introduce a risk-correction approach to address overfitting problems. Furthermore, we find that complementary-label learning can be expressed as a set of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#36866;&#24212;Hui-Walter&#33539;&#24335;&#65292;&#23558;&#20256;&#32479;&#24212;&#29992;&#20110;&#27969;&#34892;&#30149;&#23398;&#21644;&#21307;&#23398;&#30340;&#26041;&#27861;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#26080;&#27861;&#33719;&#24471;&#26631;&#31614;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#28508;&#22312;&#31867;&#21035;&#65292;&#24182;&#22312;&#22810;&#20010;&#27979;&#35797;&#20013;&#29420;&#31435;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#30495;&#23454;&#20540;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#22312;&#22788;&#29702;&#22312;&#32447;&#25968;&#25454;&#26102;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09376</link><description>&lt;p&gt;
&#35299;&#38145;&#26080;&#26631;&#31614;&#25968;&#25454;: Hui-Walter&#33539;&#24335;&#22312;&#22312;&#32447;&#21644;&#38745;&#24577;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#35780;&#20272;&#20013;&#30340;&#38598;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unlocking Unlabeled Data: Ensemble Learning with the Hui- Walter Paradigm for Performance Estimation in Online and Static Settings. (arXiv:2401.09376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09376
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#36866;&#24212;Hui-Walter&#33539;&#24335;&#65292;&#23558;&#20256;&#32479;&#24212;&#29992;&#20110;&#27969;&#34892;&#30149;&#23398;&#21644;&#21307;&#23398;&#30340;&#26041;&#27861;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#26080;&#27861;&#33719;&#24471;&#26631;&#31614;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#28508;&#22312;&#31867;&#21035;&#65292;&#24182;&#22312;&#22810;&#20010;&#27979;&#35797;&#20013;&#29420;&#31435;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#30495;&#23454;&#20540;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#22312;&#22788;&#29702;&#22312;&#32447;&#25968;&#25454;&#26102;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#24314;&#27169;&#39046;&#22495;&#65292;&#20174;&#19994;&#20154;&#21592;&#24120;&#24120;&#22312;&#21487;&#35780;&#20272;&#21644;&#35757;&#32451;&#30340;&#20551;&#35774;&#19979;&#24037;&#20316;&#65292;&#21363;&#21487;&#35775;&#38382;&#30340;&#12289;&#38745;&#24577;&#30340;&#12289;&#24102;&#26377;&#26631;&#31614;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20551;&#35774;&#24448;&#24448;&#20559;&#31163;&#20102;&#29616;&#23454;&#65292;&#20854;&#20013;&#30340;&#25968;&#25454;&#21487;&#33021;&#26159;&#31169;&#26377;&#30340;&#12289;&#21152;&#23494;&#30340;&#12289;&#38590;&#20197;&#27979;&#37327;&#30340;&#25110;&#32773;&#27809;&#26377;&#26631;&#31614;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#20256;&#32479;&#24212;&#29992;&#20110;&#27969;&#34892;&#30149;&#23398;&#21644;&#21307;&#23398;&#30340;Hui-Walter&#33539;&#24335;&#35843;&#25972;&#21040;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26469;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#27809;&#26377;&#30495;&#23454;&#20540;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#65292;&#22914;&#20551;&#38451;&#24615;&#29575;&#12289;&#20551;&#38452;&#24615;&#29575;&#21644;&#20808;&#39564;&#27010;&#29575;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#36825;&#31181;&#33539;&#24335;&#26469;&#22788;&#29702;&#22312;&#32447;&#25968;&#25454;&#65292;&#24320;&#36767;&#20102;&#21160;&#24577;&#25968;&#25454;&#29615;&#22659;&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#28508;&#22312;&#31867;&#21035;&#65292;&#20197;&#27169;&#25311;&#22810;&#20010;&#25968;&#25454;&#32676;&#20307;&#65288;&#22914;&#26524;&#27809;&#26377;&#33258;&#28982;&#32676;&#20307;&#21487;&#29992;&#65289;&#65292;&#24182;&#29420;&#31435;&#35757;&#32451;&#27169;&#22411;&#26469;&#22797;&#21046;&#22810;&#27425;&#27979;&#35797;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#25968;&#25454;&#23376;&#38598;&#20043;&#38388;&#20132;&#21449;&#21046;&#34920;&#65292;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#20108;&#20803;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of machine learning and statistical modeling, practitioners often work under the assumption of accessible, static, labeled data for evaluation and training. However, this assumption often deviates from reality where data may be private, encrypted, difficult- to-measure, or unlabeled. In this paper, we bridge this gap by adapting the Hui-Walter paradigm, a method traditionally applied in epidemiology and medicine, to the field of machine learning. This approach enables us to estimate key performance metrics such as false positive rate, false negative rate, and priors in scenarios where no ground truth is available. We further extend this paradigm for handling online data, opening up new possibilities for dynamic data environments. Our methodology involves partitioning data into latent classes to simulate multiple data populations (if natural populations are unavailable) and independently training models to replicate multiple tests. By cross-tabulating binary outcomes across
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20020;&#24202;&#21644;RT-PCR&#25968;&#25454;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26469;&#39044;&#27979;COVID-19&#24739;&#32773;&#24247;&#22797;&#25110;&#27515;&#20129;&#39118;&#38505;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.13925</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#20915;&#31574;&#26862;&#26519;&#65306;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;COVID-19&#24739;&#32773;&#24247;&#22797;&#25110;&#27515;&#20129;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#20020;&#24202;&#21644;RT-PCR&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Decision Forest: A Novel Approach for Predicting Recovery or Decease of COVID-19 Patients with Clinical and RT-PCR. (arXiv:2311.13925v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20020;&#24202;&#21644;RT-PCR&#25968;&#25454;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26469;&#39044;&#27979;COVID-19&#24739;&#32773;&#24247;&#22797;&#25110;&#27515;&#20129;&#39118;&#38505;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#23459;&#24067;&#22823;&#27969;&#34892;&#24050;&#32463;&#32467;&#26463;&#65292;&#20294;COVID-19&#20173;&#28982;&#34987;&#35270;&#20026;&#19968;&#31181;&#22320;&#26041;&#24615;&#30142;&#30149;&#12290;&#36825;&#27425;&#22823;&#27969;&#34892;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26041;&#24335;&#25171;&#20081;&#20102;&#20154;&#20204;&#30340;&#29983;&#27963;&#24182;&#23548;&#33268;&#24191;&#27867;&#30340;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#12290;&#22240;&#27492;&#65292;&#32039;&#24613;&#21307;&#29983;&#26377;&#24517;&#35201;&#30830;&#23450;&#39640;&#39118;&#38505;&#27515;&#20129;&#24739;&#32773;&#65292;&#20197;&#20415;&#20248;&#20808;&#32771;&#34385;&#21307;&#38498;&#35774;&#22791;&#30340;&#20998;&#37197;&#65292;&#23588;&#20854;&#26159;&#22312;&#21307;&#30103;&#36164;&#28304;&#26377;&#38480;&#30340;&#22320;&#21306;&#12290;&#23613;&#31649;&#23384;&#22312;&#21738;&#31181;&#25968;&#25454;&#26368;&#20934;&#30830;&#30340;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#20294;&#24739;&#32773;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#23545;&#20110;&#39044;&#27979;COVID-19&#30149;&#20363;&#30340;&#32467;&#26524;&#26159;&#26377;&#30410;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24819;&#35201;&#26816;&#26597;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#33021;&#22815;&#39044;&#27979;&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20020;&#24202;&#21644;RT-PCR&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#20197;&#30830;&#23450;&#21738;&#20010;&#26356;&#21487;&#38752;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#22235;&#20010;&#19981;&#21516;&#29305;&#24449;&#38598;&#30340;&#38454;&#27573;&#65292;&#24182;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26500;&#24314;&#20102;&#30456;&#24212;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
COVID-19 continues to be considered an endemic disease in spite of the World Health Organization's declaration that the pandemic is over. This pandemic has disrupted people's lives in unprecedented ways and caused widespread morbidity and mortality. As a result, it is important for emergency physicians to identify patients with a higher mortality risk in order to prioritize hospital equipment, especially in areas with limited medical services. The collected data from patients is beneficial to predict the outcome of COVID-19 cases, although there is a question about which data makes the most accurate predictions. Therefore, this study aims to accomplish two main objectives. First, we want to examine whether deep learning algorithms can predict a patient's morality. Second, we investigated the impact of Clinical and RT-PCR on prediction to determine which one is more reliable. We defined four stages with different feature sets and used interpretable deep learning methods to build appropr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#21644;&#37325;&#26032;&#35774;&#35745;&#20102;&#36923;&#36753;-softmax&#20284;&#28982;&#65292;&#36890;&#36807;&#28201;&#24230;&#21442;&#25968;&#25511;&#21046;&#20808;&#39564;&#32622;&#20449;&#27700;&#24179;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#36125;&#21494;&#26031;&#20803;&#23398;&#20064;&#20013;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;&#21516;&#26102;&#35777;&#26126;softmax&#26159;&#36923;&#36753;-softmax&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#36923;&#36753;-softmax&#33021;&#22815;&#24341;&#23548;&#26356;&#22823;&#30340;&#25968;&#25454;&#20998;&#24067;&#23478;&#26063;&#12290;</title><link>http://arxiv.org/abs/2310.10379</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#36125;&#21494;&#26031;&#20803;&#23398;&#20064;&#20013;&#36923;&#36753;-softmax&#20284;&#28982;&#29992;&#20110;&#23569;&#26679;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Revisiting Logistic-softmax Likelihood in Bayesian Meta-Learning for Few-Shot Classification. (arXiv:2310.10379v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#21644;&#37325;&#26032;&#35774;&#35745;&#20102;&#36923;&#36753;-softmax&#20284;&#28982;&#65292;&#36890;&#36807;&#28201;&#24230;&#21442;&#25968;&#25511;&#21046;&#20808;&#39564;&#32622;&#20449;&#27700;&#24179;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#36125;&#21494;&#26031;&#20803;&#23398;&#20064;&#20013;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;&#21516;&#26102;&#35777;&#26126;softmax&#26159;&#36923;&#36753;-softmax&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#36923;&#36753;-softmax&#33021;&#22815;&#24341;&#23548;&#26356;&#22823;&#30340;&#25968;&#25454;&#20998;&#24067;&#23478;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#36890;&#36807;&#23398;&#20064;&#20351;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#35299;&#20915;&#26032;&#38382;&#39064;&#65292;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#34920;&#24449;&#23569;&#26679;&#26412;&#20998;&#31867;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#31867;&#21035;&#39640;&#26031;&#36807;&#31243;&#20998;&#31867;&#20013;&#65292;&#36923;&#36753;-softmax&#20284;&#28982;&#19968;&#30452;&#34987;&#29992;&#20316;softmax&#20284;&#28982;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#22240;&#20026;&#20854;&#20855;&#26377;&#26465;&#20214;&#20849;&#36717;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#36923;&#36753;-softmax&#30340;&#29702;&#35770;&#29305;&#24615;&#19981;&#28165;&#26970;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#36923;&#36753;-softmax&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#20102;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#21644;&#37325;&#26032;&#35774;&#35745;&#20102;&#36923;&#36753;-softmax&#20284;&#28982;&#65292;&#36890;&#36807;&#19968;&#20010;&#28201;&#24230;&#21442;&#25968;&#23454;&#29616;&#23545;&#20808;&#39564;&#32622;&#20449;&#27700;&#24179;&#30340;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#36341;&#30340;&#35282;&#24230;&#35777;&#26126;&#20102;softmax&#21487;&#20197;&#34987;&#35270;&#20026;&#36923;&#36753;-softmax&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#24182;&#19988;&#36923;&#36753;-softmax&#24341;&#23548;&#20102;&#27604;softmax&#26356;&#22823;&#30340;&#25968;&#25454;&#20998;&#24067;&#23478;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning has demonstrated promising results in few-shot classification (FSC) by learning to solve new problems using prior knowledge. Bayesian methods are effective at characterizing uncertainty in FSC, which is crucial in high-risk fields. In this context, the logistic-softmax likelihood is often employed as an alternative to the softmax likelihood in multi-class Gaussian process classification due to its conditional conjugacy property. However, the theoretical property of logistic-softmax is not clear and previous research indicated that the inherent uncertainty of logistic-softmax leads to suboptimal performance. To mitigate these issues, we revisit and redesign the logistic-softmax likelihood, which enables control of the \textit{a priori} confidence level through a temperature parameter. Furthermore, we theoretically and empirically show that softmax can be viewed as a special case of logistic-softmax and logistic-softmax induces a larger family of data distribution than soft
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#35299;&#37322;&#24615;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#30340;&#20851;&#38190;&#37096;&#20998;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07800</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#27880;&#24847;&#21147;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#21450;&#20854;&#23427;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Explainable Attention for Few-shot Learning and Beyond. (arXiv:2310.07800v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#35299;&#37322;&#24615;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#30340;&#20851;&#38190;&#37096;&#20998;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#22686;&#24378;&#23398;&#20064;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#28508;&#21147;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#20013;&#26174;&#33879;&#30340;&#37096;&#20998;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#22312;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#35760;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#23548;&#33268;&#35757;&#32451;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23588;&#20854;&#26377;&#20215;&#20540;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#21551;&#21457;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#23558;AI&#22522;&#32447;&#26292;&#38706;&#20110;&#21407;&#22987;&#25968;&#25454;&#30340;&#20851;&#38190;&#37096;&#20998;&#32780;&#19981;&#26159;&#25972;&#20010;&#36755;&#20837;&#25968;&#25454;&#38598;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#24863;&#30693;&#65292;&#37027;&#20040;&#23427;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26356;&#20934;&#30830;&#12289;&#26356;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#36825;&#20123;&#20449;&#24687;&#24615;&#25968;&#25454;&#37096;&#20998;&#30340;&#20219;&#21153;&#65292;&#21363;&#30828;&#27880;&#24847;&#21147;&#23547;&#25214;&#65292;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24456;&#38590;&#25214;&#21040;&#36825;&#20123;&#20449;&#24687;&#24615;&#21306;&#22495;&#65292;&#21407;&#22240;&#26159;&#22823;&#37327;&#30340;&#35757;&#32451;&#21442;&#25968;&#26080;&#27861;&#20174;&#26377;&#38480;&#30340;&#26679;&#26412;&#20013;&#26377;&#25928;&#23398;&#20064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#30828;&#27880;&#24847;&#21147;&#23547;&#25214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanisms have exhibited promising potential in enhancing learning models by identifying salient portions of input data. This is particularly valuable in scenarios where limited training samples are accessible due to challenges in data collection and labeling. Drawing inspiration from human recognition processes, we posit that an AI baseline's performance could be more accurate and dependable if it is exposed to essential segments of raw data rather than the entire input dataset, akin to human perception. However, the task of selecting these informative data segments, referred to as hard attention finding, presents a formidable challenge. In situations with few training samples, existing studies struggle to locate such informative regions due to the large number of training parameters that cannot be effectively learned from the available limited samples. In this study, we introduce a novel and practical framework for achieving explainable hard attention finding, specifically
&lt;/p&gt;</description></item><item><title>PostRainBench&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#38477;&#27700;&#39044;&#27979;&#22522;&#20934;&#65292;&#32467;&#21512;AI&#21518;&#22788;&#29702;&#25216;&#26415;&#21644;&#20256;&#32479;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#20934;&#30830;&#24615;&#24182;&#35299;&#20915;&#22797;&#26434;&#30340;&#38477;&#27700;&#39044;&#27979;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.02676</link><description>&lt;p&gt;
PostRainBench: &#19968;&#31181;&#20840;&#38754;&#30340;&#38477;&#27700;&#39044;&#27979;&#22522;&#20934;&#21644;&#26032;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PostRainBench: A comprehensive benchmark and a new model for precipitation forecasting. (arXiv:2310.02676v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02676
&lt;/p&gt;
&lt;p&gt;
PostRainBench&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#38477;&#27700;&#39044;&#27979;&#22522;&#20934;&#65292;&#32467;&#21512;AI&#21518;&#22788;&#29702;&#25216;&#26415;&#21644;&#20256;&#32479;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#26041;&#27861;&#65292;&#33021;&#22815;&#22686;&#24378;&#20934;&#30830;&#24615;&#24182;&#35299;&#20915;&#22797;&#26434;&#30340;&#38477;&#27700;&#39044;&#27979;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#38477;&#27700;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#31185;&#23398;&#21644;&#31038;&#20250;&#37325;&#35201;&#24615;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#24191;&#27867;&#37319;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#36182;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#27169;&#25311;&#22522;&#30784;&#29289;&#29702;&#36807;&#31243;&#26041;&#38754;&#26377;&#38480;&#65292;&#20351;&#24471;&#20934;&#30830;&#39044;&#27979;&#22256;&#38590;&#12290;&#23558;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#19982;&#20256;&#32479;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20026;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#20043;&#21069;&#36827;&#34892;&#36807;&#21518;&#22788;&#29702;&#30340;&#23581;&#35797;&#65292;&#20294;&#30001;&#20110;&#19981;&#21516;&#20301;&#32622;&#30340;&#38477;&#27700;&#25968;&#25454;&#22833;&#34913;&#21644;&#22810;&#20010;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20934;&#30830;&#39044;&#27979;&#22823;&#38632;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PostRainBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#21464;&#37327;NWP&#21518;&#22788;&#29702;&#22522;&#20934;&#65292;&#21253;&#25324;&#19977;&#20010;&#29992;&#20110;NWP&#21518;&#22788;&#29702;&#38477;&#27700;&#39044;&#27979;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#28192;&#36947;&#27880;&#24847;&#21147;&#27169;&#22411;CAMT&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate precipitation forecasting is a vital challenge of both scientific and societal importance. Data-driven approaches have emerged as a widely used solution for addressing this challenge. However, solely relying on data-driven approaches has limitations in modeling the underlying physics, making accurate predictions difficult. Coupling AI-based post-processing techniques with traditional Numerical Weather Prediction (NWP) methods offers a more effective solution for improving forecasting accuracy. Despite previous post-processing efforts, accurately predicting heavy rainfall remains challenging due to the imbalanced precipitation data across locations and complex relationships between multiple meteorological variables. To address these limitations, we introduce the PostRainBench, a comprehensive multi-variable NWP post-processing benchmark consisting of three datasets for NWP post-processing-based precipitation forecasting. We propose CAMT, a simple yet effective Channel Attention
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#30340;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.02635</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22522;&#30784;&#65306;&#26397;&#21521;&#20855;&#26377;&#22522;&#30784;&#20808;&#39564;&#36741;&#21161;&#30340;&#20855;&#36523;&#36890;&#29992;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance. (arXiv:2310.02635v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#30340;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#20204;&#24050;&#32463;&#34920;&#26126;&#65292;&#20174;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#26159;&#26500;&#24314;&#36890;&#29992;&#27169;&#22411;&#30340;&#20851;&#38190;&#65292;&#27491;&#22914;&#22312;NLP&#20013;&#25152;&#35265;&#12290;&#20026;&#20102;&#26500;&#24314;&#20855;&#36523;&#36890;&#29992;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21644;&#35768;&#22810;&#20854;&#20182;&#30740;&#31350;&#32773;&#20551;&#35774;&#36825;&#31181;&#22522;&#30784;&#20808;&#39564;&#20063;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#20197;&#36866;&#24403;&#30340;&#20855;&#20307;&#24418;&#24335;&#34920;&#31034;&#36825;&#20123;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#65292;&#20197;&#21450;&#23427;&#20204;&#24212;&#35813;&#22914;&#20309;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#30452;&#35266;&#26377;&#25928;&#30340;&#20855;&#36523;&#20808;&#39564;&#65292;&#21253;&#25324;&#22522;&#30784;&#31574;&#30053;&#12289;&#20215;&#20540;&#21644;&#25104;&#21151;&#22870;&#21169;&#12290;&#25152;&#25552;&#20986;&#30340;&#20808;&#39564;&#26159;&#22522;&#20110;&#30446;&#26631;&#26465;&#20214;&#30340;MDP&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#30001;&#36825;&#20123;&#20808;&#39564;&#36741;&#21161;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#22522;&#30784;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#21629;&#21517;&#20026;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#65288;FRL&#65289;&#65292;&#22240;&#20026;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#26469;&#36827;&#34892;&#25506;&#32034;&#12289;&#23398;&#20064;&#21644;&#24378;&#21270;&#12290;FRL&#30340;&#22909;&#22788;&#26377;&#19977;&#20010;&#12290;(1)&#26679;&#26412;&#25928;&#29575;&#39640;&#12290;&#36890;&#36807;&#22522;&#30784;&#20808;&#39564;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#65292;&#20943;&#23569;&#26679;&#26412;&#20351;&#29992;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, people have shown that large-scale pre-training from internet-scale data is the key to building generalist models, as witnessed in NLP. To build embodied generalist agents, we and many other researchers hypothesize that such foundation prior is also an indispensable component. However, it is unclear what is the proper concrete form to represent those embodied foundation priors and how they should be used in the downstream task. In this paper, we propose an intuitive and effective set of embodied priors that consist of foundation policy, value, and success reward. The proposed priors are based on the goal-conditioned MDP. To verify their effectiveness, we instantiate an actor-critic method assisted by the priors, called Foundation Actor-Critic (FAC). We name our framework as Foundation Reinforcement Learning (FRL), since it completely relies on embodied foundation priors to explore, learn and reinforce. The benefits of FRL are threefold. (1) Sample efficient. With foundation p
&lt;/p&gt;</description></item><item><title>GPSINDy&#26159;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#19982;SINDy&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#27169;&#22411;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#31995;&#32479;&#21160;&#24577;&#21644;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#26041;&#38754;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11076</link><description>&lt;p&gt;
GPSINDy: &#25968;&#25454;&#39537;&#21160;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#26041;&#31243;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
GPSINDy: Data-Driven Discovery of Equations of Motion. (arXiv:2309.11076v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11076
&lt;/p&gt;
&lt;p&gt;
GPSINDy&#26159;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#19982;SINDy&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#27169;&#22411;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#31995;&#32479;&#21160;&#24577;&#21644;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#26041;&#38754;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32771;&#34385;&#20174;&#26377;&#22122;&#22768;&#25968;&#25454;&#20013;&#21457;&#29616;&#21160;&#21147;&#23398;&#31995;&#32479;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#24050;&#30693;&#22122;&#22768;&#23384;&#22312;&#23545;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;&#19968;&#31181;&#38750;&#21442;&#25968;&#23398;&#20064;&#26041;&#27861;&#65289;&#19982;SINDy&#65288;&#19968;&#31181;&#21442;&#25968;&#23398;&#20064;&#26041;&#27861;&#65289;&#30456;&#32467;&#21512;&#65292;&#20174;&#25968;&#25454;&#20013;&#35782;&#21035;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#19982;SINDy&#30456;&#27604;&#22312;&#26377;&#22122;&#22768;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#22312;Lotka-Volterra&#27169;&#22411;&#21644;&#20223;&#30495;&#20013;&#30340;&#21333;&#36718;&#36710;&#21160;&#24577;&#27169;&#22411;&#19978;&#20197;&#21450;&#22312;&#20351;&#29992;&#30828;&#20214;&#25968;&#25454;&#30340;NVIDIA JetRacer&#31995;&#32479;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#36739;&#20110;SINDy&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21457;&#29616;&#31995;&#32479;&#21160;&#24577;&#21644;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#26041;&#38754;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of discovering dynamical system models from noisy data. The presence of noise is known to be a significant problem for symbolic regression algorithms. We combine Gaussian process regression, a nonparametric learning method, with SINDy, a parametric learning approach, to identify nonlinear dynamical systems from data. The key advantages of our proposed approach are its simplicity coupled with the fact that it demonstrates improved robustness properties with noisy data over SINDy. We demonstrate our proposed approach on a Lotka-Volterra model and a unicycle dynamic model in simulation and on an NVIDIA JetRacer system using hardware data. We demonstrate improved performance over SINDy for discovering the system dynamics and predicting future trajectories.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDAN&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65292;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#12290;&#35813;&#32593;&#32476;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#12289;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12902</link><description>&lt;p&gt;
CDAN: &#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement. (arXiv:2308.12902v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDAN&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65292;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#12290;&#35813;&#32593;&#32476;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#12289;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#22270;&#20687;&#20197;&#19981;&#36275;&#30340;&#29031;&#26126;&#20026;&#29305;&#24449;&#65292;&#38754;&#20020;&#28165;&#26224;&#24230;&#20943;&#24369;&#12289;&#39068;&#33394;&#26263;&#28129;&#21644;&#32454;&#33410;&#20943;&#23569;&#30340;&#25361;&#25112;&#12290;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#25913;&#21892;&#20142;&#24230;&#12289;&#23545;&#27604;&#24230;&#21644;&#25972;&#20307;&#24863;&#30693;&#36136;&#37327;&#26469;&#32416;&#27491;&#36825;&#20123;&#38382;&#39064;&#65292;&#20174;&#32780;&#20419;&#36827;&#20934;&#30830;&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65288;CDAN&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#20302;&#20809;&#22270;&#20687;&#12290;CDAN&#23558;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#19982;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#30456;&#32467;&#21512;&#65292;&#37197;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#12290;&#35813;&#26550;&#26500;&#30830;&#20445;&#20102;&#26377;&#25928;&#30340;&#20449;&#24687;&#20256;&#36882;&#21644;&#29305;&#24449;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#39046;&#22495;&#30340;&#26368;&#26032;&#25104;&#26524;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-light images, characterized by inadequate illumination, pose challenges of diminished clarity, muted colors, and reduced details. Low-light image enhancement, an essential task in computer vision, aims to rectify these issues by improving brightness, contrast, and overall perceptual quality, thereby facilitating accurate analysis and interpretation. This paper introduces the Convolutional Dense Attention-guided Network (CDAN), a novel solution for enhancing low-light images. CDAN integrates an autoencoder-based architecture with convolutional and dense blocks, complemented by an attention mechanism and skip connections. This architecture ensures efficient information propagation and feature learning. Furthermore, a dedicated post-processing phase refines color balance and contrast. Our approach demonstrates notable progress compared to state-of-the-art results in low-light image enhancement, showcasing its robustness across a wide range of challenging scenarios. Our model performs 
&lt;/p&gt;</description></item><item><title>Continuous Sweep&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#20108;&#20803;&#37327;&#21270;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#21270;&#31867;&#21035;&#20998;&#24067;&#12289;&#20248;&#21270;&#20915;&#31574;&#36793;&#30028;&#20197;&#21450;&#35745;&#31639;&#22343;&#20540;&#31561;&#26041;&#27861;&#65292;&#23427;&#22312;&#37327;&#21270;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08387</link><description>&lt;p&gt;
Continuous Sweep: &#19968;&#31181;&#25913;&#36827;&#30340;&#20108;&#20803;&#37327;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Continuous Sweep: an improved, binary quantifier. (arXiv:2308.08387v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08387
&lt;/p&gt;
&lt;p&gt;
Continuous Sweep&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#20108;&#20803;&#37327;&#21270;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#21270;&#31867;&#21035;&#20998;&#24067;&#12289;&#20248;&#21270;&#20915;&#31574;&#36793;&#30028;&#20197;&#21450;&#35745;&#31639;&#22343;&#20540;&#31561;&#26041;&#27861;&#65292;&#23427;&#22312;&#37327;&#21270;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#19968;&#31181;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#20854;&#20851;&#27880;&#30340;&#26159;&#20272;&#35745;&#25968;&#25454;&#38598;&#20013;&#31867;&#21035;&#30340;&#26222;&#36941;&#24615;&#65292;&#32780;&#19981;&#26159;&#26631;&#35760;&#20854;&#20010;&#20307;&#35266;&#27979;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Continuous Sweep&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#20108;&#20803;&#37327;&#21270;&#22120;&#65292;&#21463;&#21040;&#34920;&#29616;&#33391;&#22909;&#30340;Median Sweep&#30340;&#21551;&#21457;&#12290;Median Sweep&#30446;&#21069;&#26159;&#26368;&#22909;&#30340;&#20108;&#20803;&#37327;&#21270;&#22120;&#20043;&#19968;&#65292;&#20294;&#25105;&#20204;&#22312;&#19977;&#20010;&#26041;&#38754;&#25913;&#21464;&#20102;&#36825;&#20010;&#37327;&#21270;&#22120;&#65292;&#21363;1&#65289;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#31867;&#21035;&#20998;&#24067;&#32780;&#19981;&#26159;&#32463;&#39564;&#20998;&#24067;&#65292;2&#65289;&#20248;&#21270;&#20915;&#31574;&#36793;&#30028;&#32780;&#19981;&#26159;&#24212;&#29992;&#31163;&#25955;&#30340;&#20915;&#31574;&#35268;&#21017;&#65292;3&#65289;&#35745;&#31639;&#22343;&#20540;&#32780;&#19981;&#26159;&#20013;&#20301;&#25968;&#12290;&#22312;&#19968;&#33324;&#27169;&#22411;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;Continuous Sweep&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#12290;&#36825;&#26159;&#37327;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#39318;&#27425;&#29702;&#35770;&#36129;&#29486;&#20043;&#19968;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25512;&#23548;&#20351;&#25105;&#20204;&#33021;&#22815;&#25214;&#21040;&#26368;&#20248;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#27169;&#25311;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24191;&#27867;&#30340;&#24773;&#20917;&#19979;&#65292;Continuous Sweep&#20248;&#20110;Median Sweep&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantification is a supervised machine learning task, focused on estimating the class prevalence of a dataset rather than labeling its individual observations. We introduce Continuous Sweep, a new parametric binary quantifier inspired by the well-performing Median Sweep. Median Sweep is currently one of the best binary quantifiers, but we have changed this quantifier on three points, namely 1) using parametric class distributions instead of empirical distributions, 2) optimizing decision boundaries instead of applying discrete decision rules, and 3) calculating the mean instead of the median. We derive analytic expressions for the bias and variance of Continuous Sweep under general model assumptions. This is one of the first theoretical contributions in the field of quantification learning. Moreover, these derivations enable us to find the optimal decision boundaries. Finally, our simulation study shows that Continuous Sweep outperforms Median Sweep in a wide range of situations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35748;&#35777;&#30340;&#22810;&#27969;&#31243;&#38646;&#38454;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;MFDOO&#31639;&#27861;&#30340;&#35748;&#35777;&#21464;&#20307;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#36817;&#20284;&#26368;&#20248;&#30340;&#20195;&#20215;&#22797;&#26434;&#24230;&#12290;&#21516;&#26102;&#65292;&#36824;&#32771;&#34385;&#20102;&#26377;&#22122;&#22768;&#35780;&#20272;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.00978</link><description>&lt;p&gt;
&#35748;&#35777;&#30340;&#22810;&#27969;&#31243;&#38646;&#38454;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Certified Multi-Fidelity Zeroth-Order Optimization. (arXiv:2308.00978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35748;&#35777;&#30340;&#22810;&#27969;&#31243;&#38646;&#38454;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;MFDOO&#31639;&#27861;&#30340;&#35748;&#35777;&#21464;&#20307;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#36817;&#20284;&#26368;&#20248;&#30340;&#20195;&#20215;&#22797;&#26434;&#24230;&#12290;&#21516;&#26102;&#65292;&#36824;&#32771;&#34385;&#20102;&#26377;&#22122;&#22768;&#35780;&#20272;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22810;&#27969;&#31243;&#38646;&#38454;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#36817;&#20284;&#27700;&#24179;&#65288;&#20195;&#20215;&#19981;&#21516;&#65289;&#19978;&#35780;&#20272;&#20989;&#25968;$f$&#65292;&#30446;&#26631;&#26159;&#20197;&#23613;&#21487;&#33021;&#20302;&#30340;&#20195;&#20215;&#20248;&#21270;$f$&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;\emph{&#35748;&#35777;}&#31639;&#27861;&#65292;&#23427;&#20204;&#39069;&#22806;&#35201;&#27714;&#36755;&#20986;&#19968;&#20010;&#23545;&#20248;&#21270;&#35823;&#24046;&#30340;&#25968;&#25454;&#39537;&#21160;&#19978;&#30028;&#12290;&#25105;&#20204;&#39318;&#20808;&#20197;&#31639;&#27861;&#21644;&#35780;&#20272;&#29615;&#22659;&#20043;&#38388;&#30340;&#26497;&#23567;&#26497;&#22823;&#21338;&#24328;&#24418;&#24335;&#26469;&#24418;&#24335;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MFDOO&#31639;&#27861;&#30340;&#35748;&#35777;&#21464;&#20307;&#65292;&#24182;&#25512;&#23548;&#20986;&#20854;&#22312;&#20219;&#24847;Lipschitz&#20989;&#25968;$f$&#19978;&#30340;&#20195;&#20215;&#22797;&#26434;&#24230;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#20010;&#20381;&#36182;&#20110;$f$&#30340;&#19979;&#30028;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#20855;&#26377;&#36817;&#20284;&#26368;&#20248;&#30340;&#20195;&#20215;&#22797;&#26434;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#31034;&#20363;&#35299;&#20915;&#20102;&#26377;&#22122;&#22768;&#65288;&#38543;&#26426;&#65289;&#35780;&#20272;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of multi-fidelity zeroth-order optimization, where one can evaluate a function $f$ at various approximation levels (of varying costs), and the goal is to optimize $f$ with the cheapest evaluations possible. In this paper, we study \emph{certified} algorithms, which are additionally required to output a data-driven upper bound on the optimization error. We first formalize the problem in terms of a min-max game between an algorithm and an evaluation environment. We then propose a certified variant of the MFDOO algorithm and derive a bound on its cost complexity for any Lipschitz function $f$. We also prove an $f$-dependent lower bound showing that this algorithm has a near-optimal cost complexity. We close the paper by addressing the special case of noisy (stochastic) evaluations as a direct example.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#23725;&#20989;&#25968;&#30340;&#35282;&#24230;&#20171;&#32461;&#20102;Mercer&#22823;&#35268;&#27169;&#26680;&#26426;&#22120;&#65292;&#36890;&#36807;&#30740;&#31350;&#21738;&#20123;&#26680;&#20989;&#25968;&#21487;&#20197;&#34987;&#20313;&#24358;&#20989;&#25968;&#20056;&#31215;&#36924;&#36817;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#26680;&#26426;&#22120;&#20013;&#30340;&#38590;&#39064;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#28145;&#24230;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22270;&#20687;&#22788;&#29702;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.11925</link><description>&lt;p&gt;
&#22522;&#20110;&#23725;&#20989;&#25968;&#35282;&#24230;&#30340;Mercer&#22823;&#35268;&#27169;&#26680;&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mercer Large-Scale Kernel Machines from Ridge Function Perspective. (arXiv:2307.11925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23725;&#20989;&#25968;&#30340;&#35282;&#24230;&#20171;&#32461;&#20102;Mercer&#22823;&#35268;&#27169;&#26680;&#26426;&#22120;&#65292;&#36890;&#36807;&#30740;&#31350;&#21738;&#20123;&#26680;&#20989;&#25968;&#21487;&#20197;&#34987;&#20313;&#24358;&#20989;&#25968;&#20056;&#31215;&#36924;&#36817;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#26680;&#26426;&#22120;&#20013;&#30340;&#38590;&#39064;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#28145;&#24230;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22270;&#20687;&#22788;&#29702;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20174;&#23725;&#20989;&#25968;&#30340;&#35282;&#24230;&#20171;&#32461;Mercer&#22823;&#35268;&#27169;&#26680;&#26426;&#22120;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;Lin&#21644;Pinkus&#22312;&#23725;&#20989;&#25968;&#30340;&#22522;&#26412;&#24615;&#19978;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;Rachimi&#21644;Recht&#22312;&#36817;&#20284;&#29702;&#35770;&#20013;&#30340;&#26368;&#36817;&#19968;&#31687;&#35770;&#25991;&#30340;&#20027;&#35201;&#23450;&#29702;&#65292;&#21363;&#22823;&#35268;&#27169;&#26680;&#26426;&#22120;&#30340;&#38543;&#26426;&#29305;&#24449;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21738;&#20123;&#26680;&#20989;&#25968;&#21487;&#20197;&#34987;$x$&#21644;$y$&#30340;&#20313;&#24358;&#20989;&#25968;&#20056;&#31215;&#30340;&#21644;&#36924;&#36817;&#65292;&#24182;&#25552;&#20986;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#38556;&#30861;&#12290;&#26412;&#25991;&#30340;&#32467;&#26524;&#21487;&#33021;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#19982;&#22270;&#20687;&#22788;&#29702;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
To present Mercer large-scale kernel machines from a ridge function perspective, we recall the results by Lin and Pinkus from Fundamentality of ridge functions. We consider the main theorem of the recent paper by Rachimi and Recht, 2008, Random features for large-scale kernel machines in terms of the Approximation Theory. We study which kernels can be approximated by a sum of cosine function products with arguments depending on $x$ and $y$ and present the obstacles of such an approach. The results of this article may have various applications in Deep Learning, especially in problems related to Image Processing.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#23569;&#37327;&#25968;&#25454;&#19978;&#35782;&#21035;&#26234;&#33021;&#20307;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#31639;&#27861;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#24335;&#22238;&#24402;&#21644;&#24341;&#20837;&#25429;&#25417;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#20391;&#20449;&#24687;&#32422;&#26463;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20351;&#29992;&#21333;&#20010;&#36712;&#36857;&#30340;&#30701;&#26242;&#36816;&#34892;&#20013;&#20165;&#20165;5&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#24674;&#22797;&#30495;&#23454;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2307.06640</link><description>&lt;p&gt;
&#21457;&#29616;&#26234;&#33021;&#20307;&#22312;&#23569;&#37327;&#25968;&#25454;&#19978;&#30340;&#23398;&#20064;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Discovering How Agents Learn Using Few Data. (arXiv:2307.06640v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#23569;&#37327;&#25968;&#25454;&#19978;&#35782;&#21035;&#26234;&#33021;&#20307;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#31639;&#27861;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#24335;&#22238;&#24402;&#21644;&#24341;&#20837;&#25429;&#25417;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#20391;&#20449;&#24687;&#32422;&#26463;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20351;&#29992;&#21333;&#20010;&#36712;&#36857;&#30340;&#30701;&#26242;&#36816;&#34892;&#20013;&#20165;&#20165;5&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#24674;&#22797;&#30495;&#23454;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#23398;&#20064;&#31639;&#27861;&#26159;&#35774;&#35745;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#23427;&#20204;&#20351;&#24471;&#26234;&#33021;&#20307;&#33021;&#22815;&#20174;&#32463;&#39564;&#21644;&#36807;&#21435;&#30340;&#20132;&#20114;&#20013;&#33258;&#20027;&#23398;&#20064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#21644;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#35782;&#21035;&#35268;&#23450;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#21482;&#38656;&#20351;&#29992;&#21333;&#20010;&#31995;&#32479;&#36712;&#36857;&#30340;&#30701;&#26242;&#31361;&#21457;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22810;&#39033;&#24335;&#22238;&#24402;&#35782;&#21035;&#26234;&#33021;&#20307;&#21160;&#21147;&#23398;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#19978;&#36890;&#36807;&#24341;&#20837;&#25429;&#25417;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#22522;&#26412;&#20551;&#35774;&#25110;&#26399;&#26395;&#30340;&#20391;&#20449;&#24687;&#32422;&#26463;&#26469;&#34917;&#20607;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#21644;&#20248;&#21270;&#32422;&#26463;&#30340;&#24179;&#26041;&#21644;&#35745;&#31639;&#26469;&#23454;&#26045;&#36825;&#20123;&#32422;&#26463;&#65292;&#20174;&#32780;&#24471;&#21040;&#36234;&#26469;&#36234;&#20934;&#30830;&#30340;&#26234;&#33021;&#20307;&#21160;&#21147;&#23398;&#36817;&#20284;&#23618;&#27425;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#20351;&#29992;&#21333;&#20010;&#36712;&#36857;&#30340;&#30701;&#26242;&#36816;&#34892;&#20013;&#30340;5&#20010;&#26679;&#26412;&#65292;&#23601;&#21487;&#20197;&#20934;&#30830;&#24674;&#22797;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#30495;&#23454;&#21160;&#21147;&#23398;&#65292;&#21253;&#25324;&#22343;&#34913;&#36873;&#25321;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized learning algorithms are an essential tool for designing multi-agent systems, as they enable agents to autonomously learn from their experience and past interactions. In this work, we propose a theoretical and algorithmic framework for real-time identification of the learning dynamics that govern agent behavior using a short burst of a single system trajectory. Our method identifies agent dynamics through polynomial regression, where we compensate for limited data by incorporating side-information constraints that capture fundamental assumptions or expectations about agent behavior. These constraints are enforced computationally using sum-of-squares optimization, leading to a hierarchy of increasingly better approximations of the true agent dynamics. Extensive experiments demonstrated that our approach, using only 5 samples from a short run of a single trajectory, accurately recovers the true dynamics across various benchmarks, including equilibrium selection and predictio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#26680;&#27979;&#37327;&#19981;&#30456;&#20284;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#25214;&#21040;&#26368;&#20339;&#26680;&#24102;&#23485;&#12290;&#35813;&#26041;&#27861;&#21487;&#20026;&#29616;&#26377;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#32858;&#31867;&#31639;&#27861;&#25552;&#39640;&#32858;&#31867;&#20934;&#30830;&#24230;&#65292;&#36866;&#29992;&#20110;&#21253;&#21547;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#30340;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.01890</link><description>&lt;p&gt;
&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#30340;&#26680;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Kernel Metric Learning for Clustering Mixed-type Data. (arXiv:2306.01890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01890
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#26680;&#27979;&#37327;&#19981;&#30456;&#20284;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#25214;&#21040;&#26368;&#20339;&#26680;&#24102;&#23485;&#12290;&#35813;&#26041;&#27861;&#21487;&#20026;&#29616;&#26377;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#32858;&#31867;&#31639;&#27861;&#25552;&#39640;&#32858;&#31867;&#20934;&#30830;&#24230;&#65292;&#36866;&#29992;&#20110;&#21253;&#21547;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#30340;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36317;&#31163;&#30340;&#32858;&#31867;&#21644;&#20998;&#31867;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#20197;&#23558;&#28151;&#21512;&#25968;&#20540;&#21644;&#20998;&#31867;&#25968;&#25454;&#20998;&#32452;&#12290;&#39044;&#23450;&#20041;&#30340;&#36317;&#31163;&#27979;&#37327;&#29992;&#20110;&#26681;&#25454;&#23427;&#20204;&#30340;&#19981;&#30456;&#20284;&#24615;&#26469;&#32858;&#31867;&#25968;&#25454;&#28857;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#36866;&#29992;&#20110;&#20855;&#26377;&#32431;&#25968;&#23383;&#23646;&#24615;&#21644;&#20960;&#20010;&#26377;&#24207;&#21644;&#26080;&#24207;&#20998;&#31867;&#25351;&#26631;&#30340;&#25968;&#25454;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20294;&#28151;&#21512;&#22411;&#25968;&#25454;&#30340;&#26368;&#20339;&#36317;&#31163;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#24230;&#37327;&#23558;&#25968;&#23383;&#23646;&#24615;&#36716;&#25442;&#20026;&#20998;&#31867;&#23646;&#24615;&#25110;&#21453;&#20043;&#20134;&#28982;&#12290;&#20182;&#20204;&#23558;&#25968;&#25454;&#28857;&#22788;&#29702;&#20026;&#21333;&#20010;&#23646;&#24615;&#31867;&#22411;&#65292;&#25110;&#32773;&#20998;&#21035;&#35745;&#31639;&#27599;&#20010;&#23646;&#24615;&#20043;&#38388;&#30340;&#36317;&#31163;&#24182;&#23558;&#23427;&#20204;&#30456;&#21152;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#26041;&#27861;&#65292;&#20351;&#29992;&#28151;&#21512;&#26680;&#27979;&#37327;&#19981;&#30456;&#20284;&#24615;&#65292;&#24182;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#26469;&#23547;&#25214;&#26368;&#20339;&#26680;&#24102;&#23485;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#21253;&#21547;&#32431;&#36830;&#32493;&#65292;&#20998;&#31867;&#21644;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#30340;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#32858;&#31867;&#31639;&#27861;&#26102;&#65292;&#25552;&#39640;&#20102;&#32858;&#31867;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distance-based clustering and classification are widely used in various fields to group mixed numeric and categorical data. A predefined distance measurement is used to cluster data points based on their dissimilarity. While there exist numerous distance-based measures for data with pure numerical attributes and several ordered and unordered categorical metrics, an optimal distance for mixed-type data is an open problem. Many metrics convert numerical attributes to categorical ones or vice versa. They handle the data points as a single attribute type or calculate a distance between each attribute separately and add them up. We propose a metric that uses mixed kernels to measure dissimilarity, with cross-validated optimal kernel bandwidths. Our approach improves clustering accuracy when utilized for existing distance-based clustering algorithms on simulated and real-world datasets containing pure continuous, categorical, and mixed-type data.
&lt;/p&gt;</description></item><item><title>&#23545;&#25239;&#35757;&#32451;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25239;&#20987;&#23545;&#25239;&#25200;&#21160;&#30340;&#26631;&#20934;&#26041;&#27861;, &#20854;&#23398;&#20064;&#26426;&#21046;&#23548;&#33268;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29616;&#35937;&#21516;&#26102;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2306.01271</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#20250;&#21516;&#26102;&#20986;&#29616;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29616;&#35937;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Clean Generalization and Robust Overfitting Both Happen in Adversarial Training. (arXiv:2306.01271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01271
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25239;&#20987;&#23545;&#25239;&#25200;&#21160;&#30340;&#26631;&#20934;&#26041;&#27861;, &#20854;&#23398;&#20064;&#26426;&#21046;&#23548;&#33268;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29616;&#35937;&#21516;&#26102;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25239;&#20987;&#23545;&#25239;&#25200;&#21160;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#19982;&#22312;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#29615;&#22659;&#20013;&#20986;&#29616;&#24778;&#20154;&#30340;&#24178;&#20928;&#27867;&#21270;&#33021;&#21147;&#31867;&#20284;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20063;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#24178;&#20928;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#19982;&#24178;&#20928;&#27867;&#21270;&#19981;&#21516;&#30340;&#26159;&#65292;&#23613;&#31649;&#23545;&#25239;&#35757;&#32451;&#33021;&#22815;&#23454;&#29616;&#20302;&#40065;&#26834;&#35757;&#32451;&#35823;&#24046;&#65292;&#20173;&#23384;&#22312;&#26174;&#33879;&#30340;&#40065;&#26834;&#27867;&#21270;&#36317;&#31163;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#25506;&#32034;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#23548;&#33268;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29616;&#35937;&#21516;&#26102;&#21457;&#29983;&#30340;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#36825;&#31181;&#29616;&#35937;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#65292;&#35299;&#37322;&#20102;&#23545;&#25239;&#35757;&#32451;&#22914;&#20309;&#23548;&#33268;&#32593;&#32476;&#23398;&#20064;&#32773;&#36827;&#20837;&#21040;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29366;&#24577;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#36890;&#36807;&#36843;&#20351;&#23398;&#20064;&#22120;&#25104;&#20026;&#24378;&#39044;&#27979;&#32593;&#32476;&#65292;&#23545;&#25239;&#35757;&#32451;&#23558;&#23548;&#33268;&#24178;&#20928;&#27867;&#21270;&#21644;&#40065;&#26834;&#36807;&#25311;&#21512;&#29616;&#35937;&#21516;&#26102;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is a standard method to train deep neural networks to be robust to adversarial perturbation. Similar to surprising $\textit{clean generalization}$ ability in the standard deep learning setting, neural networks trained by adversarial training also generalize well for $\textit{unseen clean data}$. However, in constrast with clean generalization, while adversarial training method is able to achieve low $\textit{robust training error}$, there still exists a significant $\textit{robust generalization gap}$, which promotes us exploring what mechanism leads to both $\textit{clean generalization and robust overfitting (CGRO)}$ during learning process. In this paper, we provide a theoretical understanding of this CGRO phenomenon in adversarial training. First, we propose a theoretical framework of adversarial training, where we analyze $\textit{feature learning process}$ to explain how adversarial training leads network learner to CGRO regime. Specifically, we prove that, u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#25968;&#25454;&#28304;&#30340;&#32852;&#37030;&#25919;&#31574;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#26412;&#22320;&#31574;&#30053;&#32858;&#21512;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21452;&#37325;&#31283;&#20581;&#32447;&#19979;&#31574;&#30053;&#35780;&#20272;&#21644;&#23398;&#20064;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#19981;&#20132;&#25442;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25919;&#31574;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#21518;&#24724;&#19978;&#38480;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#24182;&#29992;&#23454;&#39564;&#32467;&#26524;&#25903;&#25345;&#20102;&#29702;&#35770;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.12407</link><description>&lt;p&gt;
&#24322;&#26500;&#35266;&#27979;&#25968;&#25454;&#19979;&#30340;&#32852;&#37030;&#24369;&#21270;&#25919;&#31574;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Offline Policy Learning with Heterogeneous Observational Data. (arXiv:2305.12407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#25968;&#25454;&#28304;&#30340;&#32852;&#37030;&#25919;&#31574;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#26412;&#22320;&#31574;&#30053;&#32858;&#21512;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21452;&#37325;&#31283;&#20581;&#32447;&#19979;&#31574;&#30053;&#35780;&#20272;&#21644;&#23398;&#20064;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#19981;&#20132;&#25442;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25919;&#31574;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#21518;&#24724;&#19978;&#38480;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#24182;&#29992;&#23454;&#39564;&#32467;&#26524;&#25903;&#25345;&#20102;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22522;&#20110;&#24322;&#26500;&#25968;&#25454;&#28304;&#30340;&#35266;&#27979;&#25968;&#25454;&#23398;&#20064;&#20010;&#24615;&#21270;&#20915;&#31574;&#25919;&#31574;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#20013;&#22830;&#26381;&#21153;&#22120;&#26088;&#22312;&#22312;&#20998;&#24067;&#22312;&#24322;&#26500;&#28304;&#19978;&#30340;&#25968;&#25454;&#19978;&#23398;&#20064;&#19968;&#20010;&#25919;&#31574;&#65292;&#32780;&#19981;&#20132;&#25442;&#23427;&#20204;&#30340;&#21407;&#22987;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#37030;&#25919;&#31574;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#20351;&#29992;&#21452;&#37325;&#31283;&#20581;&#32447;&#19979;&#31574;&#30053;&#35780;&#20272;&#21644;&#23398;&#20064;&#31574;&#30053;&#35757;&#32451;&#30340;&#26412;&#22320;&#31574;&#30053;&#32858;&#21512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#24724;&#20998;&#26512;&#26041;&#27861;&#26469;&#30830;&#31435;&#23545;&#20840;&#23616;&#21518;&#24724;&#27010;&#24565;&#30340;&#26377;&#38480;&#26679;&#26412;&#19978;&#30028;&#65292;&#36825;&#20010;&#20840;&#23616;&#21518;&#24724;&#27010;&#24565;&#36328;&#36234;&#20102;&#23458;&#25143;&#31471;&#30340;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38024;&#23545;&#27599;&#20010;&#21333;&#29420;&#30340;&#23458;&#25143;&#31471;&#24314;&#31435;&#20102;&#30456;&#24212;&#30340;&#23616;&#37096;&#21518;&#24724;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#30001;&#30456;&#23545;&#20110;&#25152;&#26377;&#20854;&#20182;&#23458;&#25143;&#31471;&#30340;&#20998;&#24067;&#21464;&#21270;&#29305;&#24449;&#24615;&#22320;&#25551;&#36848;&#12290;&#25105;&#20204;&#29992;&#23454;&#39564;&#32467;&#26524;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#25552;&#20379;&#20102;&#24322;&#26500;&#23458;&#25143;&#31471;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#20215;&#20540;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning personalized decision policies on observational data from heterogeneous data sources. Moreover, we examine this problem in the federated setting where a central server aims to learn a policy on the data distributed across the heterogeneous sources without exchanging their raw data. We present a federated policy learning algorithm based on aggregation of local policies trained with doubly robust offline policy evaluation and learning strategies. We provide a novel regret analysis for our approach that establishes a finite-sample upper bound on a notion of global regret across a distribution of clients. In addition, for any individual client, we establish a corresponding local regret upper bound characterized by the presence of distribution shift relative to all other clients. We support our theoretical findings with experimental results. Our analysis and experiments provide insights into the value of heterogeneous client participation in federation fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.10947</link><description>&lt;p&gt;
&#20851;&#20110;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#36777;&#25252;
&lt;/p&gt;
&lt;p&gt;
In Defense of Pure 16-bit Floating-Point Neural Networks. (arXiv:2305.10947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#25152;&#38656;&#30340;&#20301;&#25968;&#26159;&#38750;&#24120;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21152;&#24555;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20197;&#24320;&#21457;&#21033;&#29992;&#26356;&#20302;&#31934;&#24230;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#27604;&#22914;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#32431;16&#20301;&#28014;&#28857;&#35774;&#32622;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;&#36896;&#25104;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#28014;&#28857;&#35823;&#24046;&#21644;&#23481;&#24525;&#24230;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#23494;&#20999;&#36924;&#36817;&#32467;&#26524;&#30340;&#26465;&#20214;&#12290;&#36825;&#31181;&#29702;&#35770;&#25506;&#32034;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reducing the number of bits needed to encode the weights and activations of neural networks is highly desirable as it speeds up their training and inference time while reducing memory consumption. For these reasons, research in this area has attracted significant attention toward developing neural networks that leverage lower-precision computing, such as mixed-precision training. Interestingly, none of the existing approaches has investigated pure 16-bit floating-point settings. In this paper, we shed light on the overlooked efficiency of pure 16-bit floating-point neural networks. As such, we provide a comprehensive theoretical analysis to investigate the factors contributing to the differences observed between 16-bit and 32-bit models. We formalize the concepts of floating-point error and tolerance, enabling us to quantitatively explain the conditions under which a 16-bit model can closely approximate the results of its 32-bit counterpart. This theoretical exploration offers perspect
&lt;/p&gt;</description></item><item><title>PI-FL&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.07514</link><description>&lt;p&gt;
PI-FL&#65306;&#20010;&#24615;&#21270;&#21644;&#28608;&#21169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PI-FL: Personalized and Incentivized Federated Learning. (arXiv:2304.07514v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07514
&lt;/p&gt;
&lt;p&gt;
PI-FL&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24212;&#23545;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#32771;&#34385;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#20010;&#24615;&#21270;&#36807;&#31243;&#20197;&#20445;&#25252;&#20854;&#33258;&#27835;&#26435;&#12290;&#20801;&#35768;&#23458;&#25143;&#31471;&#21442;&#19982;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20915;&#31574;&#21464;&#24471;&#37325;&#35201;&#65292;&#22240;&#20026;&#23384;&#22312;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#26080;&#27861;&#33258;&#30001;&#20849;&#20139;&#29983;&#25104;&#33391;&#22909;&#36136;&#37327;&#20010;&#24615;&#21270;&#27169;&#22411;&#25152;&#24517;&#38656;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#25968;&#25454;&#21644;&#36164;&#28304;&#30340;&#23458;&#25143;&#31471;&#19981;&#24895;&#24847;&#22312;&#27809;&#26377;&#21512;&#29702;&#28608;&#21169;&#30340;&#24773;&#20917;&#19979;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PI-FL&#65292;&#36825;&#26159;&#19968;&#20010;&#19968;&#27425;&#24615;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#37197;&#21512;&#19968;&#20010;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#65292;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#12290;PI-FL&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized FL has been widely used to cater to heterogeneity challenges with non-IID data. A primary obstacle is considering the personalization process from the client's perspective to preserve their autonomy. Allowing the clients to participate in personalized FL decisions becomes significant due to privacy and security concerns, where the clients may not be at liberty to share private information necessary for producing good quality personalized models. Moreover, clients with high-quality data and resources are reluctant to participate in the FL process without reasonable incentive. In this paper, we propose PI-FL, a one-shot personalization solution complemented by a token-based incentive mechanism that rewards personalized training. PI-FL outperforms other state-of-the-art approaches and can generate good-quality personalized models while respecting clients' privacy.
&lt;/p&gt;</description></item><item><title>BrainIB&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#24320;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#22312;&#20998;&#26512;fMRI&#22270;&#20687;&#20013;&#30340;&#21151;&#33021;&#36830;&#25509;&#26102;&#33021;&#22815;&#35782;&#21035;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36793;&#32536;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36866;&#29992;&#20110;&#26410;&#35265;&#26679;&#26412;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2205.03612</link><description>&lt;p&gt;
BrainIB&#65306;&#22522;&#20110;&#22270;&#20449;&#24687;&#29942;&#39048;&#30340;&#21487;&#35299;&#37322;&#24615;&#33041;&#32593;&#32476;&#31934;&#31070;&#30142;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
BrainIB: Interpretable Brain Network-based Psychiatric Diagnosis with Graph Information Bottleneck. (arXiv:2205.03612v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03612
&lt;/p&gt;
&lt;p&gt;
BrainIB&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#24320;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#22312;&#20998;&#26512;fMRI&#22270;&#20687;&#20013;&#30340;&#21151;&#33021;&#36830;&#25509;&#26102;&#33021;&#22815;&#35782;&#21035;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36793;&#32536;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36866;&#29992;&#20110;&#26410;&#35265;&#26679;&#26412;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#29983;&#29289;&#26426;&#21046;&#32780;&#38750;&#20027;&#35266;&#30151;&#29366;&#23545;&#31934;&#31070;&#38556;&#30861;&#36827;&#34892;&#35786;&#26029;&#30340;&#26032;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#30340;&#20849;&#35782;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#21151;&#33021;&#36830;&#25509;&#65288;FC&#65289;&#36827;&#34892;&#31934;&#31070;&#38556;&#30861;&#21644;&#20581;&#24247;&#23545;&#29031;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#29992;&#20110;&#30830;&#23450;&#22823;&#33041;&#26631;&#35760;&#29289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35786;&#26029;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#30340;&#24773;&#20917;&#65288;&#30001;&#20110;&#35757;&#32451;&#26679;&#26412;&#19981;&#36275;&#65289;&#65292;&#22312;&#26032;&#30340;&#27979;&#35797;&#29615;&#22659;&#20013;&#34920;&#29616;&#24046;&#12290;&#27492;&#22806;&#65292;&#38590;&#20197;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#12289;&#21487;&#38752;&#30340;&#22823;&#33041;&#29983;&#29289;&#26631;&#35760;&#29289;&#26469;&#35299;&#37322;&#28508;&#22312;&#30340;&#35786;&#26029;&#20915;&#31574;&#12290;&#36825;&#20123;&#38382;&#39064;&#38459;&#30861;&#20102;&#20854;&#21487;&#33021;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BrainIB&#65292;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#33879;&#21517;&#30340;&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#21407;&#29702;&#26469;&#20998;&#26512;&#21151;&#33021;&#30913;&#20849;&#25391;&#22270;&#20687;&#65288;fMRI&#65289;&#12290;BrainIB&#33021;&#22815;&#35782;&#21035;&#22823;&#33041;&#20013;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36793;&#32536;&#65288;&#21363;&#23376;&#22270;&#65289;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#26410;&#35265;&#26679;&#26412;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing a new diagnostic models based on the underlying biological mechanisms rather than subjective symptoms for psychiatric disorders is an emerging consensus. Recently, machine learning-based classifiers using functional connectivity (FC) for psychiatric disorders and healthy controls are developed to identify brain markers. However, existing machine learningbased diagnostic models are prone to over-fitting (due to insufficient training samples) and perform poorly in new test environment. Furthermore, it is difficult to obtain explainable and reliable brain biomarkers elucidating the underlying diagnostic decisions. These issues hinder their possible clinical applications. In this work, we propose BrainIB, a new graph neural network (GNN) framework to analyze functional magnetic resonance images (fMRI), by leveraging the famed Information Bottleneck (IB) principle. BrainIB is able to identify the most informative edges in the brain (i.e., subgraph) and generalizes well to unseen 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20915;&#31574;&#26862;&#26519;&#26500;&#24314;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#26680;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22522;&#20110;&#21494;&#33410;&#28857;&#30456;&#20284;&#24615;&#30340;&#26680;&#24179;&#22343;&#23884;&#20837;&#38543;&#26426;&#26862;&#26519;&#65288;KMERF&#65289;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#25968;&#25454;&#19978;&#37117;&#34920;&#29616;&#20986;&#28176;&#36827;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;KMERF&#22312;&#22810;&#31181;&#39640;&#32500;&#25968;&#25454;&#27979;&#35797;&#20013;&#20248;&#20110;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#26680;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/1812.00029</link><description>&lt;p&gt;
&#36890;&#36807;&#20915;&#31574;&#26862;&#26519;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#26680;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Characteristic Kernels via Decision Forests. (arXiv:1812.00029v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1812.00029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20915;&#31574;&#26862;&#26519;&#26500;&#24314;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#26680;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22522;&#20110;&#21494;&#33410;&#28857;&#30456;&#20284;&#24615;&#30340;&#26680;&#24179;&#22343;&#23884;&#20837;&#38543;&#26426;&#26862;&#26519;&#65288;KMERF&#65289;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#25968;&#25454;&#19978;&#37117;&#34920;&#29616;&#20986;&#28176;&#36827;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;KMERF&#22312;&#22810;&#31181;&#39640;&#32500;&#25968;&#25454;&#27979;&#35797;&#20013;&#20248;&#20110;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#26680;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26862;&#26519;&#34987;&#24191;&#27867;&#29992;&#20110;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;&#26641;&#26041;&#27861;&#30340;&#19968;&#20010;&#36739;&#23569;&#34987;&#30693;&#26195;&#30340;&#29305;&#24615;&#26159;&#21487;&#20197;&#20174;&#26641;&#26500;&#24314;&#30456;&#20284;&#24615;&#30697;&#38453;&#65292;&#24182;&#19988;&#36825;&#20123;&#30456;&#20284;&#24615;&#30697;&#38453;&#26159;&#30001;&#26680;&#35825;&#23548;&#30340;&#12290;&#23613;&#31649;&#23545;&#20110;&#26680;&#30340;&#24212;&#29992;&#21644;&#24615;&#36136;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#30001;&#20915;&#31574;&#26862;&#26519;&#35825;&#23548;&#30340;&#26680;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22522;&#20110;&#21494;&#33410;&#28857;&#30456;&#20284;&#24615;&#30340;&#26680;&#24179;&#22343;&#23884;&#20837;&#38543;&#26426;&#26862;&#26519;&#65288;KMERF&#65289;&#65292;&#23427;&#21487;&#20197;&#20174;&#38543;&#26426;&#26641;&#25110;&#26862;&#26519;&#20013;&#35825;&#23548;&#26680;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#28176;&#36827;&#29305;&#24449;&#26680;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;KMERF&#26680;&#23545;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#25968;&#25454;&#37117;&#26159;&#28176;&#36827;&#29305;&#24449;&#30340;&#12290;&#30001;&#20110;KMERF&#26159;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#65292;&#25105;&#20204;&#24576;&#30097;&#23427;&#23558;&#22312;&#26377;&#38480;&#26679;&#26412;&#25968;&#25454;&#19978;&#32988;&#36807;&#39044;&#20808;&#36873;&#25321;&#30340;&#26680;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;KMERF&#22312;&#21508;&#31181;&#39640;&#32500;&#20004;&#26679;&#26412;&#21644;&#29420;&#31435;&#24615;&#27979;&#35797;&#22330;&#26223;&#20013;&#20960;&#20046;&#21344;&#25454;&#20102;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#26680;&#30340;&#27979;&#35797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision forests are widely used for classification and regression tasks. A lesser known property of tree-based methods is that one can construct a proximity matrix from the tree(s), and these proximity matrices are induced kernels. While there has been extensive research on the applications and properties of kernels, there is relatively little research on kernels induced by decision forests. We construct Kernel Mean Embedding Random Forests (KMERF), which induce kernels from random trees and/or forests using leaf-node proximity. We introduce the notion of an asymptotically characteristic kernel, and prove that KMERF kernels are asymptotically characteristic for both discrete and continuous data. Because KMERF is data-adaptive, we suspected it would outperform kernels selected a priori on finite sample data. We illustrate that KMERF nearly dominates current state-of-the-art kernel-based tests across a diverse range of high-dimensional two-sample and independence testing settings. Furth
&lt;/p&gt;</description></item></channel></rss>