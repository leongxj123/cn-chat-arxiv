<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>TraveLER&#26159;&#19968;&#31181;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27839;&#30528;&#35270;&#39057;&#31227;&#21160;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#25910;&#38598;&#20851;&#38190;&#24103;&#30340;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#35270;&#39057;&#38382;&#31572;&#20013;&#20851;&#38190;&#26102;&#38388;&#25139;&#36873;&#25321;&#21644;&#38169;&#35823;&#26102;&#38388;&#25139;&#35843;&#25972;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2404.01476</link><description>&lt;p&gt;
TraveLER&#65306;&#29992;&#20110;&#35270;&#39057;&#38382;&#31572;&#30340;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TraveLER: A Multi-LMM Agent Framework for Video Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01476
&lt;/p&gt;
&lt;p&gt;
TraveLER&#26159;&#19968;&#31181;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27839;&#30528;&#35270;&#39057;&#31227;&#21160;&#65292;&#24182;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#25910;&#38598;&#20851;&#38190;&#24103;&#30340;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#35270;&#39057;&#38382;&#31572;&#20013;&#20851;&#38190;&#26102;&#38388;&#25139;&#36873;&#25321;&#21644;&#38169;&#35823;&#26102;&#38388;&#25139;&#35843;&#25972;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#35270;&#39057;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#12289;&#22522;&#20110;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#20197;&#24103;&#20026;&#21333;&#20301;&#36827;&#34892;&#22788;&#29702;&#12290;&#34429;&#28982;&#22522;&#20110;&#22270;&#20687;&#30340;&#35270;&#39057;&#26041;&#27861;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#30340;&#23616;&#38480;&#26159;&#23427;&#20204;&#32463;&#24120;&#24573;&#35270;&#20102;&#22914;&#20309;&#36873;&#25321;&#20851;&#38190;&#26102;&#38388;&#25139;&#65292;&#24182;&#19988;&#26080;&#27861;&#22312;&#30830;&#23450;&#38169;&#35823;&#26102;&#38388;&#25139;&#26102;&#36827;&#34892;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#26080;&#27861;&#25552;&#21462;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#32454;&#33410;&#65292;&#32780;&#26159;&#25552;&#20379;&#24103;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#37325;LMM&#20195;&#29702;&#26694;&#26550;&#65292;&#23427;&#27839;&#30528;&#35270;&#39057;&#36827;&#34892;&#31227;&#21160;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#25552;&#38382;&#30340;&#26041;&#24335;&#36845;&#20195;&#22320;&#20174;&#20851;&#38190;&#24103;&#25910;&#38598;&#30456;&#20851;&#20449;&#24687;&#65292;&#30452;&#21040;&#33719;&#24471;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TraveLER&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#21046;&#23450;&#8220;&#36941;&#21382;&#8221;&#35270;&#39057;&#35745;&#21010;&#30340;&#27169;&#22411;&#65292;&#35810;&#38382;&#20851;&#20110;&#21333;&#20010;&#24103;&#30340;&#38382;&#39064;&#20197;&#8220;&#23450;&#20301;&#8221;&#24182;&#23384;&#20648;&#20851;&#38190;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01476v1 Announce Type: cross  Abstract: Recently, Large Multimodal Models (LMMs) have made significant progress in video question-answering using a frame-wise approach by leveraging large-scale, image-based pretraining in a zero-shot manner. While image-based methods for videos have shown impressive performance, a current limitation is that they often overlook how key timestamps are selected and cannot adjust when incorrect timestamps are identified. Moreover, they are unable to extract details relevant to the question, instead providing general descriptions of the frame. To overcome this, we design a multi-LMM agent framework that travels along the video, iteratively collecting relevant information from keyframes through interactive question-asking until there is sufficient information to answer the question. Specifically, we propose TraveLER, a model that can create a plan to "Traverse" through the video, ask questions about individual frames to "Locate" and store key info
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#26368;&#20248;&#26041;&#27861;&#65292;&#21487;&#20197;&#24674;&#22797;&#30001;&#26410;&#30693;&#20219;&#21153;&#20998;&#24067;&#23450;&#20041;&#30340;&#36125;&#21494;&#26031;&#21518;&#39564;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19381</link><description>&lt;p&gt;
&#20851;&#20110;&#36817;&#36125;&#21494;&#26031;&#26368;&#20248;&#31639;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Uncertainty Quantification for Near-Bayes Optimal Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19381
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24120;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#36817;&#20284;&#36125;&#21494;&#26031;&#26368;&#20248;&#26041;&#27861;&#65292;&#21487;&#20197;&#24674;&#22797;&#30001;&#26410;&#30693;&#20219;&#21153;&#20998;&#24067;&#23450;&#20041;&#30340;&#36125;&#21494;&#26031;&#21518;&#39564;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#24314;&#27169;&#20801;&#35768;&#23545;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#65292;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#65292;&#26500;&#24314;&#25110;&#23454;&#29616;&#23427;&#20204;&#30340;&#36125;&#21494;&#26031;&#23545;&#24212;&#26159;&#22256;&#38590;&#30340;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#24120;&#29992;&#30340;ML&#31639;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#39640;&#25928;&#65292;&#24182;&#19988;&#21487;&#33021;&#22312;&#26410;&#30693;&#20219;&#21153;&#20998;&#24067;&#19979;&#25509;&#36817;&#36125;&#21494;&#26031;&#26368;&#20248;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#20351;&#29992;&#35813;&#31639;&#27861;&#26500;&#24314;&#19968;&#20010;&#38789;&#21518;&#39564;&#65292;&#21487;&#20197;&#24674;&#22797;&#30001;&#20219;&#21153;&#20998;&#24067;&#23450;&#20041;&#30340;&#36125;&#21494;&#26031;&#21518;&#39564;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#26159;&#26410;&#30693;&#20294;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36890;&#29992;ML&#31639;&#27861;&#30340;&#23454;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;&#22522;&#20110;&#21508;&#31181;&#38750;NN&#21644;NN&#31639;&#27861;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19381v1 Announce Type: cross  Abstract: Bayesian modelling allows for the quantification of predictive uncertainty which is crucial in safety-critical applications. Yet for many machine learning (ML) algorithms, it is difficult to construct or implement their Bayesian counterpart. In this work we present a promising approach to address this challenge, based on the hypothesis that commonly used ML algorithms are efficient across a wide variety of tasks and may thus be near Bayes-optimal w.r.t. an unknown task distribution. We prove that it is possible to recover the Bayesian posterior defined by the task distribution, which is unknown but optimal in this setting, by building a martingale posterior using the algorithm. We further propose a practical uncertainty quantification method that apply to general ML algorithms. Experiments based on a variety of non-NN and NN algorithms demonstrate the efficacy of our method.
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#33293;&#20837;&#25216;&#26415;&#33021;&#26377;&#25928;&#38544;&#24335;&#27491;&#21017;&#21270;&#39640;&#30246;&#30697;&#38453;&#65292;&#30830;&#20445;&#33293;&#20837;&#21518;&#30340;&#30697;&#38453;&#20855;&#26377;&#23436;&#25972;&#30340;&#21015;&#31209;&#12290;</title><link>https://arxiv.org/abs/2403.12278</link><description>&lt;p&gt;
&#38543;&#26426;&#33293;&#20837;&#38544;&#24335;&#27491;&#21017;&#21270;&#39640;&#30246;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Stochastic Rounding Implicitly Regularizes Tall-and-Thin Matrices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12278
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#33293;&#20837;&#25216;&#26415;&#33021;&#26377;&#25928;&#38544;&#24335;&#27491;&#21017;&#21270;&#39640;&#30246;&#30697;&#38453;&#65292;&#30830;&#20445;&#33293;&#20837;&#21518;&#30340;&#30697;&#38453;&#20855;&#26377;&#23436;&#25972;&#30340;&#21015;&#31209;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#38543;&#26426;&#33293;&#20837;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#27969;&#34892;&#65292;&#25105;&#20204;&#32771;&#34385;&#23454;&#30697;&#38453;$\mathbf{A}$&#30340;&#38543;&#26426;&#36817;&#20284;&#33293;&#20837;&#65292;&#20854;&#20013;&#34892;&#25968;&#36828;&#36828;&#22810;&#20110;&#21015;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#29702;&#35770;&#35777;&#25454;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35780;&#20272;&#25903;&#25345;&#65292;&#39640;&#27010;&#29575;&#19979;&#65292;&#38543;&#26426;&#33293;&#20837;&#30697;&#38453;&#30340;&#26368;&#23567;&#22855;&#24322;&#20540;&#36828;&#31163;&#38646;--&#26080;&#35770;$\mathbf{A}$&#25509;&#36817;&#22855;&#24322;&#36824;&#26159;$\mathbf{A}$&#22855;&#24322;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#38543;&#26426;&#33293;&#20837;\textit{&#38544;&#24335;&#27491;&#21017;&#21270;}&#39640;&#30246;&#30697;&#38453;$\mathbf{A}$&#65292;&#20351;&#24471;&#33293;&#20837;&#21518;&#30340;&#29256;&#26412;&#20855;&#26377;&#23436;&#25972;&#30340;&#21015;&#31209;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#21033;&#29992;&#20102;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#20013;&#30340;&#26377;&#21147;&#32467;&#26524;&#65292;&#20197;&#21450;&#38543;&#26426;&#33293;&#20837;&#35823;&#24046;&#19981;&#38598;&#20013;&#22312;&#20302;&#32500;&#21015;&#31354;&#38388;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12278v1 Announce Type: new  Abstract: Motivated by the popularity of stochastic rounding in the context of machine learning and the training of large-scale deep neural network models, we consider stochastic nearness rounding of real matrices $\mathbf{A}$ with many more rows than columns. We provide novel theoretical evidence, supported by extensive experimental evaluation that, with high probability, the smallest singular value of a stochastically rounded matrix is well bounded away from zero -- regardless of how close $\mathbf{A}$ is to being rank deficient and even if $\mathbf{A}$ is rank-deficient. In other words, stochastic rounding \textit{implicitly regularizes} tall and skinny matrices $\mathbf{A}$ so that the rounded version has full column rank. Our proofs leverage powerful results in random matrix theory, and the idea that stochastic rounding errors do not concentrate in low-dimensional column spaces.
&lt;/p&gt;</description></item><item><title>ViSaRL&#25552;&#20986;&#20102;Visual Saliency-Guided Reinforcement Learning&#65288;&#21463;&#35270;&#35273;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26469;&#26174;&#33879;&#25552;&#39640;RL&#20195;&#29702;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#25104;&#21151;&#29575;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10940</link><description>&lt;p&gt;
ViSaRL&#65306;&#21463;&#20154;&#31867;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ViSaRL: Visual Reinforcement Learning Guided by Human Saliency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10940
&lt;/p&gt;
&lt;p&gt;
ViSaRL&#25552;&#20986;&#20102;Visual Saliency-Guided Reinforcement Learning&#65288;&#21463;&#35270;&#35273;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26469;&#26174;&#33879;&#25552;&#39640;RL&#20195;&#29702;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#25104;&#21151;&#29575;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#39640;&#32500;&#20687;&#32032;&#36755;&#20837;&#22521;&#35757;&#26426;&#22120;&#20154;&#25191;&#34892;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#26159;&#20302;&#25928;&#30340;&#65292;&#22240;&#20026;&#22270;&#20687;&#35266;&#23519;&#20027;&#35201;&#30001;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20449;&#24687;&#32452;&#25104;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#33021;&#22815;&#22312;&#35270;&#35273;&#19978;&#20851;&#27880;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#23545;&#35937;&#21644;&#21306;&#22495;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21463;&#35270;&#35273;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;ViSaRL&#65289;&#12290;&#20351;&#29992;ViSaRL&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26174;&#30528;&#25552;&#39640;&#20102;RL&#20195;&#29702;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#65292;&#21253;&#25324;DeepMind&#25511;&#21046;&#22522;&#20934;&#12289;&#20223;&#30495;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#30340;&#25104;&#21151;&#29575;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#26174;&#33879;&#24615;&#25972;&#21512;&#21040;&#22522;&#20110;CNN&#21644;Transformer&#30340;&#32534;&#30721;&#22120;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20351;&#29992;ViSaRL&#23398;&#20064;&#30340;&#35270;&#35273;&#34920;&#31034;&#23545;&#21508;&#31181;&#35270;&#35273;&#25200;&#21160;&#65292;&#21253;&#25324;&#24863;&#30693;&#22122;&#22768;&#21644;&#22330;&#26223;&#21464;&#21270;&#65292;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;ViSaRL&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#25104;&#21151;&#29575;&#20960;&#20046;&#32763;&#20102;&#19968;&#30058;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10940v1 Announce Type: cross  Abstract: Training robots to perform complex control tasks from high-dimensional pixel input using reinforcement learning (RL) is sample-inefficient, because image observations are comprised primarily of task-irrelevant information. By contrast, humans are able to visually attend to task-relevant objects and areas. Based on this insight, we introduce Visual Saliency-Guided Reinforcement Learning (ViSaRL). Using ViSaRL to learn visual representations significantly improves the success rate, sample efficiency, and generalization of an RL agent on diverse tasks including DeepMind Control benchmark, robot manipulation in simulation and on a real robot. We present approaches for incorporating saliency into both CNN and Transformer-based encoders. We show that visual representations learned using ViSaRL are robust to various sources of visual perturbations including perceptual noise and scene variations. ViSaRL nearly doubles success rate on the real-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;G-NoCL&#65292;&#37319;&#29992;&#29983;&#25104;&#25968;&#25454;&#24182;&#21033;&#29992;DIverSity&#21644;COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#36827;&#34892;&#25968;&#25454;&#34701;&#21512;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10853</link><description>&lt;p&gt;
&#21482;&#35828;&#21517;&#31216;&#65306;&#36890;&#36807;&#25968;&#25454;&#29983;&#25104;&#23454;&#29616;&#20165;&#21033;&#29992;&#31867;&#21035;&#21517;&#31216;&#36827;&#34892;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Just Say the Name: Online Continual Learning with Category Names Only via Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;G-NoCL&#65292;&#37319;&#29992;&#29983;&#25104;&#25968;&#25454;&#24182;&#21033;&#29992;DIverSity&#21644;COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#36827;&#34892;&#25968;&#25454;&#34701;&#21512;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#25104;&#26412;&#36807;&#39640;&#65292;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#21463;&#21040;&#22823;&#35268;&#27169;&#32593;&#32476;&#30417;&#30563;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#24314;&#35758;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#21033;&#29992;&#32593;&#32476;&#25235;&#21462;&#30340;&#25968;&#25454;&#65292;&#20294;&#36825;&#24102;&#26469;&#20102;&#35832;&#22914;&#25968;&#25454;&#19981;&#24179;&#34913;&#12289;&#20351;&#29992;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36830;&#32493;&#32593;&#32476;&#30417;&#30563;&#35757;&#32451;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550; - &#20165;&#20351;&#29992;&#21517;&#31216;&#30340;&#29983;&#25104;&#24335;&#36830;&#32493;&#23398;&#20064;&#65288;G-NoCL&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;G-NoCL&#20351;&#29992;&#19968;&#32452;&#29983;&#25104;&#22120;G&#20197;&#21450;&#23398;&#20064;&#32773;&#12290;&#24403;&#36935;&#21040;&#26032;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#31867;&#21035;&#65289;&#26102;&#65292;G-NoCL&#37319;&#29992;&#26032;&#39062;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;&#25216;&#26415;DIverSity and COmplexity enhancing ensemBlER&#65288;DISCOBER&#65289;&#20174;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#26368;&#20248;&#25277;&#26679;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DISCOBER&#22312;G-NoCL&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#28085;&#30422;&#20102;In-Distributi&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10853v1 Announce Type: cross  Abstract: In real-world scenarios, extensive manual annotation for continual learning is impractical due to prohibitive costs. Although prior arts, influenced by large-scale webly supervised training, suggest leveraging web-scraped data in continual learning, this poses challenges such as data imbalance, usage restrictions, and privacy concerns. Addressing the risks of continual webly supervised training, we present an online continual learning framework - Generative Name only Continual Learning (G-NoCL). The proposed G-NoCL uses a set of generators G along with the learner. When encountering new concepts (i.e., classes), G-NoCL employs the novel sample complexity-guided data ensembling technique DIverSity and COmplexity enhancing ensemBlER (DISCOBER) to optimally sample training data from generated data. Through extensive experimentation, we demonstrate superior performance of DISCOBER in G-NoCL online CL benchmarks, covering both In-Distributi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25209;&#21028;&#24615;&#35780;&#20272;&#20102;&#35821;&#38899;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#25991;&#26412;&#20381;&#36182;&#24615;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#36807;&#20110;&#20851;&#27880;&#35789;&#27719;&#29305;&#24449;&#32780;&#38750;&#39044;&#26399;&#30340;&#35821;&#38899;&#20132;&#38469;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.07767</link><description>&lt;p&gt;
&#36229;&#36234;&#26631;&#31614;&#65306;&#25581;&#31034;&#35821;&#38899;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#25991;&#26412;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech Recognition Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25209;&#21028;&#24615;&#35780;&#20272;&#20102;&#35821;&#38899;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#25991;&#26412;&#20381;&#36182;&#24615;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#36807;&#20110;&#20851;&#27880;&#35789;&#27719;&#29305;&#24449;&#32780;&#38750;&#39044;&#26399;&#30340;&#35821;&#38899;&#20132;&#38469;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#35748;&#30693;&#36127;&#33655;&#21644;&#24773;&#32490;&#31561;&#35821;&#38899;&#20132;&#38469;&#29305;&#24449;&#36234;&#26469;&#36234;&#34987;&#35748;&#21487;&#20026;&#35821;&#38899;&#35782;&#21035;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#36890;&#24120;&#36890;&#36807;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#65288;&#22914;CLSE&#21644;IEMOCAP&#65289;&#36827;&#34892;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#20154;&#23457;&#26597;&#36825;&#20123;&#25968;&#25454;&#38598;&#26159;&#21542;&#23384;&#22312;&#25991;&#26412;&#20381;&#36182;&#24615;&#12290;&#26412;&#25991;&#25209;&#21028;&#24615;&#22320;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#30495;&#27491;&#23398;&#20250;&#35782;&#21035;&#35821;&#38899;&#20132;&#38469;&#29305;&#24449;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#25429;&#25417;&#35789;&#27719;&#29305;&#24449;&#30340;&#26222;&#36941;&#20551;&#35774;&#12290;&#36890;&#36807;&#26816;&#26597;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#30340;&#35789;&#27719;&#37325;&#21472;&#24182;&#27979;&#35797;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29305;&#24449;&#26631;&#31614;&#20013;&#30340;&#26174;&#33879;&#25991;&#26412;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#20687;HuBERT&#36825;&#26679;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#33021;&#26080;&#24847;&#20013;&#19987;&#27880;&#20110;&#35789;&#27719;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#39044;&#26399;&#30340;&#35821;&#38899;&#20132;&#38469;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#21495;&#21484;&#30740;&#31350;&#30028;&#37325;&#26032;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07767v1 Announce Type: cross  Abstract: Paralinguistic traits like cognitive load and emotion are increasingly recognized as pivotal areas in speech recognition research, often examined through specialized datasets like CLSE and IEMOCAP. However, the integrity of these datasets is seldom scrutinized for text-dependency. This paper critically evaluates the prevalent assumption that machine learning models trained on such datasets genuinely learn to identify paralinguistic traits, rather than merely capturing lexical features. By examining the lexical overlap in these datasets and testing the performance of machine learning models, we expose significant text-dependency in trait-labeling. Our results suggest that some machine learning models, especially large pre-trained models like HuBERT, might inadvertently focus on lexical characteristics rather than the intended paralinguistic features. The study serves as a call to action for the research community to reevaluate the relia
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMQ&#30340;&#21019;&#26032;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36880;&#23618;&#20943;&#23567;&#37325;&#26500;&#35823;&#24046;&#26469;&#26377;&#25928;&#38477;&#20302;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#35201;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07134</link><description>&lt;p&gt;
COMQ: &#19968;&#31181;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07134
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMQ&#30340;&#21019;&#26032;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36880;&#23618;&#20943;&#23567;&#37325;&#26500;&#35823;&#24046;&#26469;&#26377;&#25928;&#38477;&#20302;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23384;&#20648;&#35201;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#23558;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#23454;&#29992;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#37096;&#32626;&#26102;&#39640;&#24230;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#27169;&#22411;&#38477;&#33267;&#20302;&#27604;&#29305;&#34920;&#31034;&#32780;&#19981;&#25439;&#23475;&#21407;&#22987;&#20934;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;PTQ&#31639;&#27861;&#31216;&#20026;COMQ&#65292;&#23427;&#36890;&#36807;&#20381;&#27425;&#20943;&#23567;&#36880;&#23618;&#37325;&#26500;&#35823;&#24046;&#26469;&#36827;&#34892;&#22352;&#26631;&#26041;&#21521;&#19978;&#30340;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#25972;&#25968;&#37327;&#21270;&#65292;&#20854;&#20013;&#27599;&#20010;&#37327;&#21270;&#26435;&#37325;&#21487;&#20197;&#20998;&#35299;&#20026;&#19968;&#20010;&#20849;&#20139;&#30340;&#28014;&#28857;&#26631;&#37327;&#21644;&#19968;&#20010;&#25972;&#25968;&#20301;&#32534;&#30721;&#12290;&#22312;&#22266;&#23450;&#23618;&#20869;&#65292;COMQ&#23558;&#25152;&#26377;&#32553;&#25918;&#22240;&#23376;&#21644;&#20301;&#32534;&#30721;&#35270;&#20026;&#37325;&#26500;&#35823;&#24046;&#30340;&#21464;&#37327;&#12290;&#27599;&#27425;&#36845;&#20195;&#37117;&#20250;&#27839;&#30528;&#19968;&#20010;&#22352;&#26631;&#36724;&#25913;&#36827;&#36825;&#20010;&#38169;&#35823;&#65292;&#21516;&#26102;&#20445;&#25345;&#25152;&#26377;&#20854;&#20182;&#21464;&#37327;&#24658;&#23450;&#12290;COMQ&#26131;&#20110;&#20351;&#29992;&#65292;&#26080;&#38656;&#35843;&#25972;&#36229;&#21442;&#25968;&#12290;&#23427;&#21482;&#28041;&#21450;&#28857;&#20056;&#21644;&#22235;&#33293;&#20116;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07134v1 Announce Type: new  Abstract: Post-training quantization (PTQ) has emerged as a practical approach to compress large neural networks, making them highly efficient for deployment. However, effectively reducing these models to their low-bit counterparts without compromising the original accuracy remains a key challenge. In this paper, we propose an innovative PTQ algorithm termed COMQ, which sequentially conducts coordinate-wise minimization of the layer-wise reconstruction errors. We consider the widely used integer quantization, where every quantized weight can be decomposed into a shared floating-point scalar and an integer bit-code. Within a fixed layer, COMQ treats all the scaling factor(s) and bit-codes as the variables of the reconstruction error. Every iteration improves this error along a single coordinate while keeping all other variables constant. COMQ is easy to use and requires no hyper-parameter tuning. It instead involves only dot products and rounding o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;PID&#25511;&#21046;&#22120;&#21644;&#38750;&#36127;&#26356;&#26032;&#35268;&#21017;&#65292;&#35299;&#20915;&#20102;NILM&#25968;&#25454;&#20002;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07012</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Non-Intrusive Load Monitoring with Missing Data Imputation Based on Tensor Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07012
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;PID&#25511;&#21046;&#22120;&#21644;&#38750;&#36127;&#26356;&#26032;&#35268;&#21017;&#65292;&#35299;&#20915;&#20102;NILM&#25968;&#25454;&#20002;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#65288;NILM&#65289;&#22312;&#24314;&#31569;&#33021;&#28304;&#31649;&#29702;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#30830;&#20445;NILM&#25968;&#25454;&#30340;&#39640;&#36136;&#37327;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;NILM&#30340;&#23454;&#38469;&#24212;&#29992;&#38754;&#20020;&#25968;&#25454;&#20002;&#22833;&#30340;&#25361;&#25112;&#65292;&#20005;&#37325;&#24433;&#21709;&#33021;&#28304;&#31649;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#24352;&#37327;&#23436;&#25104;&#65288;TC&#65289;&#27169;&#22411;-&#22522;&#20110;&#31215;&#20998;&#27604;-&#23548;&#25968;&#65288;PID&#65289;&#30340;&#24352;&#37327;&#30340;&#38750;&#36127;&#28508;&#22240;&#23376;&#20998;&#35299;&#65288;PNLFT&#65289;&#26469;&#35299;&#20915;NILM&#25968;&#25454;&#20002;&#22833;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#24605;&#24819;&#65306;1&#65289;&#20026;&#35299;&#20915;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20013;&#28508;&#22312;&#24352;&#37327;&#20998;&#35299;&#65288;LFT&#65289;&#30340;&#25910;&#25947;&#32531;&#24930;&#38382;&#39064;&#65292;&#23398;&#20064;&#36807;&#31243;&#20013;&#24341;&#20837;&#27604;&#20363;-&#31215;&#20998;-&#23548;&#25968;&#25511;&#21046;&#22120;&#12290;PID&#25511;&#21046;&#22120;&#21033;&#29992;&#21382;&#21490;&#20449;&#24687;&#21644;&#24403;&#21069;&#20449;&#24687;&#25511;&#21046;&#23398;&#20064;&#27531;&#24046;&#12290;2&#65289;&#32771;&#34385;&#21040;NILM&#25968;&#25454;&#30340;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#38750;&#36127;&#26356;&#26032;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07012v1 Announce Type: new  Abstract: With the widespread adoption of Non-Intrusive Load Monitoring (NILM) in building energy management, ensuring the high quality of NILM data has become imperative. However, practical applications of NILM face challenges associated with data loss, significantly impacting accuracy and reliability in energy management. This paper addresses the issue of NILM data loss by introducing an innovative tensor completion(TC) model- Proportional-Integral-Derivative (PID)-incorporated Non-negative Latent Factorization of Tensors (PNLFT) with twofold ideas: 1) To tackle the issue of slow convergence in Latent Factorization of Tensors (LFT) using Stochastic Gradient Descent (SGD), a Proportional-Integral-Derivative controller is introduced during the learning process. The PID controller utilizes historical and current information to control learning residuals. 2) Considering the characteristics of NILM data, non-negative update rules are proposed in the 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;"Signature Isolation Forest"&#65292;&#21033;&#29992;&#31895;&#36335;&#24452;&#29702;&#35770;&#30340;&#31614;&#21517;&#21464;&#25442;&#21435;&#38500;&#20102;Functional Isolation Forest&#30340;&#32447;&#24615;&#20869;&#31215;&#21644;&#35789;&#20856;&#36873;&#25321;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04405</link><description>&lt;p&gt;
Signature Isolation Forest
&lt;/p&gt;
&lt;p&gt;
Signature Isolation Forest
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04405
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;"Signature Isolation Forest"&#65292;&#21033;&#29992;&#31895;&#36335;&#24452;&#29702;&#35770;&#30340;&#31614;&#21517;&#21464;&#25442;&#21435;&#38500;&#20102;Functional Isolation Forest&#30340;&#32447;&#24615;&#20869;&#31215;&#21644;&#35789;&#20856;&#36873;&#25321;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Functional Isolation Forest (FIF)&#26159;&#19968;&#31181;&#38024;&#23545;&#21151;&#33021;&#25968;&#25454;&#35774;&#35745;&#30340;&#26368;&#26032;&#19968;&#27969;&#24322;&#24120;&#26816;&#27979;(AD)&#31639;&#27861;&#12290;&#23427;&#20381;&#36182;&#20110;&#19968;&#31181;&#26641;&#20998;&#21306;&#36807;&#31243;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#26354;&#32447;&#35266;&#27979;&#25237;&#24433;&#21040;&#36890;&#36807;&#32447;&#24615;&#20869;&#31215;&#32472;&#21046;&#30340;&#35789;&#20856;&#19978;&#26469;&#35745;&#31639;&#24322;&#24120;&#24471;&#20998;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#8220;Signature Isolation Forest&#8221;&#65292;&#19968;&#31181;&#21033;&#29992;&#31895;&#36335;&#24452;&#29702;&#35770;&#31614;&#21517;&#21464;&#25442;&#30340;&#26032;&#39062;AD&#31639;&#27861;&#31867;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#25552;&#20986;&#20004;&#31181;&#31639;&#27861;&#26469;&#28040;&#38500;FIF&#26045;&#21152;&#30340;&#38480;&#21046;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#29305;&#21035;&#38024;&#23545;FIF&#20869;&#31215;&#30340;&#32447;&#24615;&#24615;&#21644;&#35789;&#20856;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04405v1 Announce Type: cross  Abstract: Functional Isolation Forest (FIF) is a recent state-of-the-art Anomaly Detection (AD) algorithm designed for functional data. It relies on a tree partition procedure where an abnormality score is computed by projecting each curve observation on a drawn dictionary through a linear inner product. Such linear inner product and the dictionary are a priori choices that highly influence the algorithm's performances and might lead to unreliable results, particularly with complex datasets. This work addresses these challenges by introducing \textit{Signature Isolation Forest}, a novel AD algorithm class leveraging the rough path theory's signature transform. Our objective is to remove the constraints imposed by FIF through the proposition of two algorithms which specifically target the linearity of the FIF inner product and the choice of the dictionary. We provide several numerical experiments, including a real-world applications benchmark sho
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550; Diffusion-TS&#65292;&#32467;&#21512;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#21644;&#35299;&#32806;&#26102;&#38388;&#34920;&#31034;&#65292;&#36890;&#36807;&#30452;&#25509;&#37325;&#24314;&#26679;&#26412;&#32780;&#38750;&#22122;&#22768;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#26088;&#22312;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#37322;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01742</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25193;&#25955;&#29992;&#20110;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diffusion-TS: Interpretable Diffusion for General Time Series Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01742
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550; Diffusion-TS&#65292;&#32467;&#21512;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#21644;&#35299;&#32806;&#26102;&#38388;&#34920;&#31034;&#65292;&#36890;&#36807;&#30452;&#25509;&#37325;&#24314;&#26679;&#26412;&#32780;&#38750;&#22122;&#22768;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#26088;&#22312;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#37322;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Denoising diffusion probabilistic models (DDPMs)&#27491;&#36880;&#28176;&#25104;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#27969;&#33539;&#24335;&#65292;&#26368;&#36817;&#24050;&#22312;&#38899;&#39057;&#21512;&#25104;&#12289;&#26102;&#38388;&#24207;&#21015;&#22635;&#34917;&#21644;&#39044;&#27979;&#31561;&#39046;&#22495;&#21462;&#24471;&#31361;&#30772;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-TS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#35299;&#32806;&#26102;&#38388;&#34920;&#31034;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#20854;&#20013;&#20998;&#35299;&#25216;&#26415;&#25351;&#23548;Diffusion-TS&#25429;&#33719;&#26102;&#38388;&#24207;&#21015;&#30340;&#35821;&#20041;&#21547;&#20041;&#65292;&#32780;&#21464;&#21387;&#22120;&#20174;&#22024;&#26434;&#30340;&#27169;&#22411;&#36755;&#20837;&#20013;&#25366;&#25496;&#35814;&#32454;&#30340;&#24207;&#21015;&#20449;&#24687;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#30452;&#25509;&#37325;&#24314;&#26679;&#26412;&#32780;&#19981;&#26159;&#22312;&#27599;&#20010;&#25193;&#25955;&#27493;&#39588;&#20013;&#37325;&#24314;&#22122;&#22768;&#65292;&#24182;&#32467;&#21512;&#20102;&#22522;&#20110;Fourier&#30340;&#25439;&#22833;&#39033;&#12290;&#39044;&#26399;Diffusion-TS&#21487;&#20197;&#29983;&#25104;&#26082;&#20855;&#26377;&#35299;&#37322;&#24615;&#21448;&#30495;&#23454;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#36824;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;Diffusion
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01742v1 Announce Type: cross  Abstract: Denoising diffusion probabilistic models (DDPMs) are becoming the leading paradigm for generative models. It has recently shown breakthroughs in audio synthesis, time series imputation and forecasting. In this paper, we propose Diffusion-TS, a novel diffusion-based framework that generates multivariate time series samples of high quality by using an encoder-decoder transformer with disentangled temporal representations, in which the decomposition technique guides Diffusion-TS to capture the semantic meaning of time series while transformers mine detailed sequential information from the noisy model input. Different from existing diffusion-based approaches, we train the model to directly reconstruct the sample instead of the noise in each diffusion step, combining a Fourier-based loss term. Diffusion-TS is expected to generate time series satisfying both interpretablity and realness. In addition, it is shown that the proposed Diffusion-T
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.18292</link><description>&lt;p&gt;
FSL&#27169;&#22411;&#21487;&#20197;&#22240;&#20026;&#20854;&#20248;&#36234;&#24615;&#24471;&#20998;&#26356;&#39640;
&lt;/p&gt;
&lt;p&gt;
FSL Model can Score Higher as It Is
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18292
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#34987;&#27491;&#30830;&#35782;&#21035;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20542;&#21521;&#20110;&#38754;&#23545;&#38754;&#22320;&#30452;&#35270;&#38754;&#37096;&#35782;&#21035;&#26426;&#65292;&#32780;&#19981;&#26159;&#20391;&#30528;&#38754;&#23545;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#20998;&#31867;&#26412;&#36523;&#23601;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;&#23646;&#20110;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#22312;&#27979;&#35797;&#26399;&#38388;&#23545;&#25197;&#26354;&#21644;&#38750;&#20856;&#22411;&#30340;&#26597;&#35810;&#25110;&#25903;&#25345;&#22270;&#20687;&#20250;&#35753;&#27169;&#22411;&#26356;&#38590;&#27491;&#30830;&#39044;&#27979;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;&#35757;&#32451;&#36807;&#30340;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;FSL&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#20855;&#26377;&#36275;&#22815;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#20855;&#26377;&#23569;&#26679;&#26412;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#25429;&#25417;&#27979;&#35797;&#22270;&#20687;&#30340;&#39118;&#26684;&#25110;&#24418;&#29366;&#65292;&#28982;&#21518;&#35782;&#21035;&#19968;&#20010;&#36866;&#24403;&#30340;&#35757;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18292v1 Announce Type: cross  Abstract: In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable traine
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#20989;&#25968;&#23450;&#20041;&#30340;Boltzmann&#20998;&#24067;&#21644;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20056;&#31215;&#19978;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18012</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#20248;&#21270;&#32422;&#26463;&#25277;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models as Constrained Samplers for Optimization with Unknown Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18012
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#20989;&#25968;&#23450;&#20041;&#30340;Boltzmann&#20998;&#24067;&#21644;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20056;&#31215;&#19978;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;&#20855;&#26377;&#26410;&#30693;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#30340;&#20248;&#21270;&#38382;&#39064;&#22312;&#20998;&#26512;&#23458;&#35266;&#20989;&#25968;&#25110;&#32422;&#26463;&#19981;&#21487;&#29992;&#26102;&#21464;&#24471;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;&#26410;&#30693;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#20294;&#26377;&#38480;&#30740;&#31350;&#20851;&#27880;&#20102;&#32422;&#26463;&#26465;&#20214;&#26410;&#26126;&#30830;&#32473;&#20986;&#30340;&#24773;&#20917;&#12290;&#24573;&#30053;&#36825;&#20123;&#32422;&#26463;&#21487;&#33021;&#23548;&#33268;&#22312;&#23454;&#36341;&#20013;&#19981;&#29616;&#23454;&#30340;&#34394;&#20551;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#26410;&#30693;&#32422;&#26463;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#36827;&#34892;&#20248;&#21270;&#12290;&#20026;&#20102;&#23558;&#20248;&#21270;&#36807;&#31243;&#38480;&#21046;&#22312;&#25968;&#25454;&#27969;&#24418;&#20869;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#26500;&#36896;&#20026;&#36890;&#36807;&#23458;&#35266;&#20989;&#25968;&#23450;&#20041;&#30340;Boltzmann&#20998;&#24067;&#21644;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#20056;&#31215;&#30340;&#25277;&#26679;&#38382;&#39064;&#12290;&#20026;&#20102;&#22686;&#24378;&#25277;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#20197;&#24341;&#23548;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#39044;&#28909;&#65292;&#28982;&#21518;&#26159;Langevin&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18012v1 Announce Type: cross  Abstract: Addressing real-world optimization problems becomes particularly challenging when analytic objective functions or constraints are unavailable. While numerous studies have addressed the issue of unknown objectives, limited research has focused on scenarios where feasibility constraints are not given explicitly. Overlooking these constraints can lead to spurious solutions that are unrealistic in practice. To deal with such unknown constraints, we propose to perform optimization within the data manifold using diffusion models. To constrain the optimization process to the data manifold, we reformulate the original optimization problem as a sampling problem from the product of the Boltzmann distribution defined by the objective function and the data distribution learned by the diffusion model. To enhance sampling efficiency, we propose a two-stage framework that begins with a guided diffusion process for warm-up, followed by a Langevin dyna
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#25972;&#20010;&#20998;&#24067;&#22312;&#22238;&#24402;&#20013;&#30340;&#24615;&#33021;&#25552;&#21319;&#20027;&#35201;&#26469;&#33258;&#20110;&#20248;&#21270;&#30340;&#25913;&#36827;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.13425</link><description>&lt;p&gt;
&#22312;&#22238;&#24402;&#20013;&#25506;&#35752;&#30452;&#26041;&#22270;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Investigating the Histogram Loss in Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13425
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25972;&#20010;&#20998;&#24067;&#22312;&#22238;&#24402;&#20013;&#30340;&#24615;&#33021;&#25552;&#21319;&#20027;&#35201;&#26469;&#33258;&#20110;&#20248;&#21270;&#30340;&#25913;&#36827;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#24120;&#35265;&#30340;&#26159;&#65292;&#22312;&#22238;&#24402;&#20013;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#25972;&#20010;&#20998;&#24067;&#65292;&#21363;&#20351;&#21482;&#38656;&#35201;&#22343;&#20540;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290; &#36825;&#31181;&#39069;&#22806;&#30340;&#24314;&#27169;&#36890;&#24120;&#20250;&#24102;&#26469;&#24615;&#33021;&#22686;&#30410;&#65292;&#20294;&#32972;&#21518;&#30340;&#21407;&#22240;&#23578;&#19981;&#23436;&#20840;&#28165;&#26970;&#12290; &#26412;&#25991;&#30740;&#31350;&#20102;&#22238;&#24402;&#20013;&#30340;&#19968;&#31181;&#26368;&#26032;&#26041;&#27861;&#65292;&#21363;&#30452;&#26041;&#22270;&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;&#20998;&#24067;&#21644;&#28789;&#27963;&#30452;&#26041;&#22270;&#39044;&#27979;&#20043;&#38388;&#30340;&#20132;&#21449;&#29109;&#26469;&#23398;&#20064;&#30446;&#26631;&#21464;&#37327;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290; &#25105;&#20204;&#35774;&#35745;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#20026;&#20160;&#20040;&#20197;&#21450;&#20309;&#26102;&#20250;&#20986;&#29616;&#24615;&#33021;&#22686;&#30410;&#65292;&#20197;&#21450;&#25439;&#22833;&#30340;&#19981;&#21516;&#32452;&#20214;&#22914;&#20309;&#20026;&#27492;&#20570;&#20986;&#36129;&#29486;&#12290; &#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#23398;&#20064;&#20998;&#24067;&#30340;&#22909;&#22788;&#26469;&#33258;&#20110;&#20248;&#21270;&#30340;&#25913;&#36827;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30452;&#26041;&#22270;&#25439;&#22833;&#22312;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13425v1 Announce Type: cross  Abstract: It is becoming increasingly common in regression to train neural networks that model the entire distribution even if only the mean is required for prediction. This additional modeling often comes with performance gain and the reasons behind the improvement are not fully known. This paper investigates a recent approach to regression, the Histogram Loss, which involves learning the conditional distribution of the target variable by minimizing the cross-entropy between a target distribution and a flexible histogram prediction. We design theoretical and empirical analyses to determine why and when this performance gain appears, and how different components of the loss contribute to it. Our results suggest that the benefits of learning distributions in this setup come from improvements in optimization rather than learning a better representation. We then demonstrate the viability of the Histogram Loss in common deep learning applications wi
&lt;/p&gt;</description></item><item><title>&#23558;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#24178;&#39044;&#25152;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.11355</link><description>&lt;p&gt;
&#25913;&#21464;&#20102;&#20160;&#20040;&#65311;&#23558;&#34920;&#24449;&#24178;&#39044;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
What Changed? Converting Representational Interventions to Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11355
&lt;/p&gt;
&lt;p&gt;
&#23558;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#24178;&#39044;&#25152;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34920;&#24449;&#31354;&#38388;&#30340;&#24178;&#39044;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#36825;&#20123;&#26041;&#27861;&#34987;&#29992;&#26469;&#28040;&#38500;&#25110;&#25913;&#21464;&#27169;&#22411;&#34920;&#31034;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#32534;&#30721;&#65292;&#21019;&#24314;&#19968;&#20010;&#21453;&#20107;&#23454;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24178;&#39044;&#25805;&#20316;&#22312;&#34920;&#31034;&#31354;&#38388;&#20869;&#65292;&#20934;&#30830;&#29702;&#35299;&#23427;&#20462;&#25913;&#20102;&#21738;&#20123;&#29305;&#24449;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#21487;&#20197;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#30340;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#23545;&#24212;&#20110;&#32473;&#23450;&#34920;&#31034;&#31354;&#38388;&#24178;&#39044;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#35299;&#37322;&#29992;&#20110;&#32534;&#30721;&#29305;&#23450;&#27010;&#24565;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#21453;&#20107;&#23454;&#21487;&#20197;&#29992;&#20110;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11355v1 Announce Type: new  Abstract: Interventions targeting the representation space of language models (LMs) have emerged as effective means to influence model behavior. These methods are employed, for example, to eliminate or alter the encoding of demographic information such as gender within the model's representations, creating a counterfactual representation. However, since the intervention operates within the representation space, understanding precisely which features it modifies poses a challenge. We show that representation-space counterfactuals can be converted into natural language counterfactuals. We demonstrate that this approach enables us to analyze the linguistic alterations corresponding to a given representation-space intervention and to interpret the features utilized for encoding a specific concept. Moreover, the resulting counterfactuals can be used to mitigate bias in classification.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;TuneTables&#19978;&#19979;&#25991;&#20248;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#65292;&#23558;TabPFN&#25193;&#23637;&#21040;&#19982;&#26356;&#22823;&#25968;&#25454;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#26684;&#20998;&#31867;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;</title><link>https://arxiv.org/abs/2402.11137</link><description>&lt;p&gt;
TuneTables&#65306;&#21487;&#25193;&#23637;&#20808;&#39564;&#25968;&#25454;&#25311;&#21512;&#32593;&#32476;&#30340;&#19978;&#19979;&#25991;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11137
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;TuneTables&#19978;&#19979;&#25991;&#20248;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#65292;&#23558;TabPFN&#25193;&#23637;&#21040;&#19982;&#26356;&#22823;&#25968;&#25454;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#26684;&#20998;&#31867;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#34920;&#26684;&#20998;&#31867;&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#30340;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#20808;&#39564;&#25968;&#25454;&#25311;&#21512;&#32593;&#32476;&#65288;PFN&#65289;&#30340;&#31361;&#30772;&#24615;&#26041;&#27861;&#65292;&#25361;&#25112;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#31867;&#20284;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;PFN&#21033;&#29992;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#22312;&#26032;&#20219;&#21153;&#19978;&#21462;&#24471;&#24378;&#22823;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;PFN&#23384;&#22312;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#37319;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;TabPFN&#22312;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#38750;&#24120;&#24378;&#21170;&#30340;&#24615;&#33021;&#65292;&#20294;&#24182;&#19981;&#36866;&#29992;&#20110;&#25968;&#25454;&#38598;&#22823;&#23567;&#22823;&#20110;1000&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20026;PFN&#24320;&#21457;&#19978;&#19979;&#25991;&#20248;&#21270;&#25216;&#26415;&#65292;&#20811;&#26381;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;PFN&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TuneTables&#65292;&#19968;&#31181;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#36739;&#23567;&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#26032;&#22411;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#12290;TuneTables&#23558;TabPFN&#25193;&#23637;&#21040;&#19982;&#26356;&#22823;&#25968;&#25454;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#26684;&#20998;&#31867;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11137v1 Announce Type: new  Abstract: While tabular classification has traditionally relied on from-scratch training, a recent breakthrough called prior-data fitted networks (PFNs) challenges this approach. Similar to large language models, PFNs make use of pretraining and in-context learning to achieve strong performance on new tasks in a single forward pass. However, current PFNs have limitations that prohibit their widespread adoption. Notably, TabPFN achieves very strong performance on small tabular datasets but is not designed to make predictions for datasets of size larger than 1000. In this work, we overcome these limitations and substantially improve the performance of PFNs by developing context optimization techniques for PFNs. Specifically, we propose TuneTables, a novel prompt-tuning strategy that compresses large datasets into a smaller learned context. TuneTables scales TabPFN to be competitive with state-of-the-art tabular classification methods on larger datas
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24341;&#20837;&#20102;&#21307;&#23398;&#29109;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22522;&#20110;ICD-9&#20195;&#30721;&#30340;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#65292;&#37327;&#21270;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10940</link><description>&lt;p&gt;
&#20020;&#24202;&#31243;&#24207;&#20195;&#30721;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation of clinical procedure codes for medical diagnosis and uncertainty quantification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10940
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24341;&#20837;&#20102;&#21307;&#23398;&#29109;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22522;&#20110;ICD-9&#20195;&#30721;&#30340;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#65292;&#37327;&#21270;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;CDSS&#65289;&#26088;&#22312;&#36890;&#36807;&#23558;&#31995;&#32479;&#29983;&#25104;&#30340;&#24314;&#35758;&#19982;&#21307;&#23398;&#19987;&#19994;&#30693;&#35782;&#32467;&#21512;&#26469;&#22686;&#24378;&#20020;&#24202;&#21307;&#29983;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21307;&#23398;&#29109;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#22522;&#20110;&#25163;&#26415;ICD-9&#20195;&#30721;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26469;&#37327;&#21270;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#19981;&#20165;&#23637;&#31034;&#20102;&#31243;&#24207;&#20195;&#30721;&#19982;&#23454;&#38469;&#21307;&#30103;&#32467;&#26524;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10940v1 Announce Type: new  Abstract: A Clinical Decision Support System (CDSS) is designed to enhance clinician decision-making by combining system-generated recommendations with medical expertise. Given the high costs, intensive labor, and time-sensitive nature of medical treatments, there is a pressing need for efficient decision support, especially in complex emergency scenarios. In these scenarios, where information can be limited, an advanced CDSS framework that leverages AI (artificial intelligence) models to effectively reduce diagnostic uncertainty has utility. Such an AI-enabled CDSS framework with quantified uncertainty promises to be practical and beneficial in the demanding context of real-world medical care. In this study, we introduce the concept of Medical Entropy, quantifying uncertainties in patient outcomes predicted by neural machine translation based on the ICD-9 code of procedures. Our experimental results not only show strong correlations between proce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;&#65292;&#21517;&#20026;aespa&#65292;&#23427;&#22312;&#20445;&#25345;&#23436;&#25972;&#30340;&#27880;&#24847;&#21147;&#24471;&#20998;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#36880;&#23618;&#37327;&#21270;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08958</link><description>&lt;p&gt;
&#36808;&#21521;&#36229;&#22823;&#35268;&#27169;Transformer&#30340;&#19979;&#19968;&#32423;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;&#65292;&#21517;&#20026;aespa&#65292;&#23427;&#22312;&#20445;&#25345;&#23436;&#25972;&#30340;&#27880;&#24847;&#21147;&#24471;&#20998;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#36880;&#23618;&#37327;&#21270;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#24050;&#25104;&#20026;&#22312;&#31227;&#21160;&#35774;&#22791;&#21644;&#30005;&#35270;&#31561;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#36229;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PTQ&#26041;&#26696;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#36164;&#28304;&#65292;&#36825;&#21487;&#33021;&#25104;&#20026;&#23454;&#38469;&#24773;&#20917;&#20013;&#39057;&#32321;&#27169;&#22411;&#26356;&#26032;&#21644;&#22810;&#31181;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#29942;&#39048;&#12290;&#20316;&#20026;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#27425;&#24615;PTQ&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#26377;&#20123;&#21463;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#32771;&#34385;&#21040;Transformer&#20013;&#27880;&#24847;&#21147;&#27169;&#22359;&#20869;&#37096;&#23618;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;PTQ&#31639;&#27861;&#65292;&#23427;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#21483;&#20570;aespa&#65292;&#36890;&#36807;&#22312;&#25928;&#29575;&#19978;&#36827;&#34892;&#36880;&#23618;&#37327;&#21270;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#36328;&#23618;&#20381;&#36182;&#20197;&#20445;&#30041;&#27880;&#24847;&#21147;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08958v1 Announce Type: cross Abstract: With the increasing complexity of generative AI models, post-training quantization (PTQ) has emerged as a promising solution for deploying hyper-scale models on edge devices such as mobile devices and TVs. Existing PTQ schemes, however, consume considerable time and resources, which could be a bottleneck in real situations where frequent model updates and multiple hyper-parameter tunings are required. As a cost-effective alternative, one-shot PTQ schemes have been proposed. Still, the performance is somewhat limited because they cannot consider the inter-layer dependency within the attention module, which is a very important feature of Transformers. In this paper, we thus propose a novel PTQ algorithm that balances accuracy and efficiency. The key idea of the proposed algorithm called aespa is to perform quantization layer-wise for efficiency while considering cross-layer dependency to preserve the attention score. Through extensive exp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#28176;&#21464;&#27969;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31283;&#23450;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#39044;&#27979;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08151</link><description>&lt;p&gt;
&#28176;&#21464;&#27969;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#29992;&#20110;sigmoid&#20998;&#31867;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Gradient-flow adaptive importance sampling for Bayesian leave one out cross-validation for sigmoidal classification models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#28176;&#21464;&#27969;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31283;&#23450;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#39044;&#27979;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#26799;&#24230;&#27969;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;IS&#65289;&#21464;&#25442;&#65292;&#29992;&#20110;&#31283;&#23450;&#36125;&#21494;&#26031;&#20998;&#31867;&#27169;&#22411;&#30340;&#28857;&#32423;&#30041;&#19968;&#20132;&#21449;&#39564;&#35777;&#65288;LOO&#65289;&#39044;&#27979;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#12290;&#21487;&#20197;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#20363;&#22914;&#35745;&#31639;&#19982;AIC&#31867;&#20284;&#30340;LOO&#25110;&#35745;&#31639;LOO ROC / PRC&#26354;&#32447;&#20197;&#21450;&#27966;&#29983;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#22914;AUROC&#21644;AUPRC&#12290;&#36890;&#36807;&#21464;&#20998;&#27861;&#21644;&#26799;&#24230;&#27969;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20004;&#20010;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#21333;&#27493;&#21464;&#25442;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#23558;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#23436;&#25972;&#25968;&#25454;&#21518;&#39564;&#38752;&#36817;&#30446;&#26631;LOO&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#12290;&#36825;&#26679;&#65292;&#21464;&#25442;&#31283;&#23450;&#20102;&#37325;&#35201;&#24615;&#26435;&#37325;&#12290;&#22240;&#20026;&#21464;&#25442;&#28041;&#21450;&#21040;&#20284;&#28982;&#20989;&#25968;&#30340;&#26799;&#24230;&#65292;&#25152;&#20197;&#32467;&#26524;&#30340;&#33945;&#29305;&#21345;&#32599;&#31215;&#20998;&#20381;&#36182;&#20110;&#27169;&#22411;Hessian&#30340;Jacobian&#34892;&#21015;&#24335;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#36825;&#20123;Jacobian&#34892;&#21015;&#24335;&#30340;&#38381;&#21512;&#31934;&#30830;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a set of gradient-flow-guided adaptive importance sampling (IS) transformations to stabilize Monte-Carlo approximations of point-wise leave one out cross-validated (LOO) predictions for Bayesian classification models. One can leverage this methodology for assessing model generalizability by for instance computing a LOO analogue to the AIC or computing LOO ROC/PRC curves and derived metrics like the AUROC and AUPRC. By the calculus of variations and gradient flow, we derive two simple nonlinear single-step transformations that utilize gradient information to shift a model's pre-trained full-data posterior closer to the target LOO posterior predictive distributions. In doing so, the transformations stabilize importance weights. Because the transformations involve the gradient of the likelihood function, the resulting Monte Carlo integral depends on Jacobian determinants with respect to the model Hessian. We derive closed-form exact formulae for these Jacobian determinants in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#23618;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#26497;&#38480;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#26465;&#20214;&#27491;&#23450;&#24452;&#21521;&#22522;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#23454;&#29616;&#22312;&#22352;&#26631;&#36755;&#20837;&#32593;&#32476;&#20013;&#12290;&#36825;&#20026;&#24191;&#27867;&#30340;PINNs&#30740;&#31350;&#24102;&#26469;&#20102;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.06955</link><description>&lt;p&gt;
&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Training dynamics in Physics-Informed Neural Networks with feature mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#23618;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#26497;&#38480;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#26465;&#20214;&#27491;&#23450;&#24452;&#21521;&#22522;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#23454;&#29616;&#22312;&#22352;&#26631;&#36755;&#20837;&#32593;&#32476;&#20013;&#12290;&#36825;&#20026;&#24191;&#27867;&#30340;PINNs&#30740;&#31350;&#24102;&#26469;&#20102;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26631;&#24535;&#24615;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#23613;&#31649;&#20854;&#21464;&#20307;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26469;&#33258;&#26356;&#24191;&#27867;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30740;&#31350;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#32463;&#39564;&#24615;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#36890;&#36807;&#26497;&#38480;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#26469;&#30740;&#31350;&#24102;&#26377;&#29305;&#24449;&#26144;&#23556;&#23618;&#30340;PINNs&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24120;&#29992;&#30340;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#29305;&#24449;&#26144;&#23556;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#26465;&#20214;&#27491;&#23450;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#20316;&#20026;&#26356;&#22909;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#38598;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#36825;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#21487;&#20197;&#36731;&#26494;&#22312;&#22352;&#26631;&#36755;&#20837;&#32593;&#32476;&#20013;&#23454;&#29616;&#65292;&#24182;&#21463;&#30410;&#20110;&#24191;&#27867;&#30340;PINNs&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks (PINNs) have emerged as an iconic machine learning approach for solving Partial Differential Equations (PDEs). Although its variants have achieved significant progress, the empirical success of utilising feature mapping from the wider Implicit Neural Representations studies has been substantially neglected. We investigate the training dynamics of PINNs with a feature mapping layer via the limiting Conjugate Kernel and Neural Tangent Kernel, which sheds light on the convergence and generalisation of the model. We also show the inadequacy of commonly used Fourier-based feature mapping in some scenarios and propose the conditional positive definite Radial Basis Function as a better alternative. The empirical results reveal the efficacy of our method in diverse forward and inverse problem sets. This simple technique can be easily implemented in coordinate input networks and benefits the broad PINNs research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#32454;&#35843;&#22823;&#22411;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#38469;&#38544;&#31169;&#28431;&#27934;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#25968;&#37327;&#20197;&#21450;&#35757;&#32451;&#32467;&#26463;&#26102;&#30340;&#22823;&#26799;&#24230;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2402.06674</link><description>&lt;p&gt;
&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#38469;&#25104;&#21592;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Understanding Practical Membership Privacy of Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06674
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#32454;&#35843;&#22823;&#22411;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#38469;&#38544;&#31169;&#28431;&#27934;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#25968;&#37327;&#20197;&#21450;&#35757;&#32451;&#32467;&#26463;&#26102;&#30340;&#22823;&#26799;&#24230;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#26469;&#31995;&#32479;&#22320;&#27979;&#35797;&#32454;&#35843;&#22823;&#22411;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#38469;&#38544;&#31169;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#29702;&#35299;&#20351;&#25968;&#25454;&#38598;&#21644;&#26679;&#26412;&#23481;&#26131;&#21463;&#21040;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#29305;&#24615;&#12290;&#22312;&#25968;&#25454;&#38598;&#29305;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#25968;&#37327;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#24130;&#24459;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#26159;&#20197;&#25915;&#20987;&#30340;&#30495;&#38451;&#24615;&#29575;&#65288;&#22312;&#20302;&#20551;&#38451;&#24615;&#29575;&#19979;&#27979;&#37327;&#65289;&#26469;&#34913;&#37327;&#30340;&#12290;&#23545;&#20110;&#20010;&#21035;&#26679;&#26412;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#32467;&#26463;&#26102;&#20135;&#29983;&#30340;&#22823;&#26799;&#24230;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We apply a state-of-the-art membership inference attack (MIA) to systematically test the practical privacy vulnerability of fine-tuning large image classification models.We focus on understanding the properties of data sets and samples that make them vulnerable to membership inference. In terms of data set properties, we find a strong power law dependence between the number of examples per class in the data and the MIA vulnerability, as measured by true positive rate of the attack at a low false positive rate. For an individual sample, large gradients at the end of training are strongly correlated with MIA vulnerability.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#24222;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;270M&#21442;&#25968;&#30340;Transformer&#27169;&#22411;&#65292;&#19981;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#25110;&#26174;&#24335;&#25628;&#32034;&#65292;&#21462;&#24471;&#20102;&#22823;&#24072;&#32423;&#27700;&#24179;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#30340;&#25104;&#21151;&#12290;&#27169;&#22411;&#22312;Lichess&#38378;&#30005;&#25112;&#35780;&#20998;&#19978;&#36798;&#21040;&#20102;2895&#65292;&#35299;&#20915;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22269;&#38469;&#35937;&#26827;&#35868;&#39064;&#65292;&#20248;&#20110;AlphaZero&#21644;GPT-3.5-turbo-instruct&#12290;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#23545;&#20110;&#23454;&#29616;&#24378;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#25928;&#26524;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.04494</link><description>&lt;p&gt;
&#19981;&#38656;&#25628;&#32034;&#21363;&#21487;&#23454;&#29616;&#22823;&#24072;&#32423;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;
&lt;/p&gt;
&lt;p&gt;
Grandmaster-Level Chess Without Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#24222;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;270M&#21442;&#25968;&#30340;Transformer&#27169;&#22411;&#65292;&#19981;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#25110;&#26174;&#24335;&#25628;&#32034;&#65292;&#21462;&#24471;&#20102;&#22823;&#24072;&#32423;&#27700;&#24179;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#30340;&#25104;&#21151;&#12290;&#27169;&#22411;&#22312;Lichess&#38378;&#30005;&#25112;&#35780;&#20998;&#19978;&#36798;&#21040;&#20102;2895&#65292;&#35299;&#20915;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22269;&#38469;&#35937;&#26827;&#35868;&#39064;&#65292;&#20248;&#20110;AlphaZero&#21644;GPT-3.5-turbo-instruct&#12290;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#23545;&#20110;&#23454;&#29616;&#24378;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#25928;&#26524;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#31361;&#30772;&#24615;&#25104;&#21151;&#20027;&#35201;&#24402;&#21151;&#20110;&#35268;&#27169;&#21270;&#65292;&#21363;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22823;&#35268;&#27169;&#26550;&#26500;&#21644;&#31354;&#21069;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22269;&#38469;&#35937;&#26827;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#19982;&#20256;&#32479;&#30340;&#20381;&#36182;&#22797;&#26434;&#21551;&#21457;&#24335;&#31639;&#27861;&#12289;&#26174;&#24335;&#25628;&#32034;&#25110;&#20108;&#32773;&#32467;&#21512;&#30340;&#22269;&#38469;&#35937;&#26827;&#24341;&#25806;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;1000&#19975;&#23616;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#20102;&#19968;&#20010;&#25317;&#26377;2.7&#20159;&#21442;&#25968;&#30340;Transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#29992;&#24378;&#22823;&#30340;Stockfish 16&#24341;&#25806;&#25552;&#20379;&#30340;&#21160;&#20316;&#20540;&#26469;&#27880;&#37322;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#26827;&#23616;&#65292;&#20135;&#29983;&#22823;&#32422;150&#20159;&#20010;&#25968;&#25454;&#28857;&#12290;&#25105;&#20204;&#26368;&#22823;&#30340;&#27169;&#22411;&#22312;Lichess&#38378;&#30005;&#25112;Elo&#19978;&#36798;&#21040;&#20102;2895&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22269;&#38469;&#35937;&#26827;&#35868;&#39064;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#29305;&#23450;&#39046;&#22495;&#30340;&#35843;&#25972;&#25110;&#26174;&#24335;&#25628;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;AlphaZero&#30340;&#31574;&#30053;&#21644;&#20215;&#20540;&#32593;&#32476;&#65288;&#26080;MCTS&#65289;&#20197;&#21450;GPT-3.5-turbo-instruct&#12290;&#23545;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#31995;&#32479;&#30740;&#31350;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#21487;&#20197;&#22312;&#35268;&#27169;&#19978;&#21462;&#24471;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent breakthrough successes in machine learning are mainly attributed to scale: namely large-scale attention-based architectures and datasets of unprecedented scale. This paper investigates the impact of training at scale for chess. Unlike traditional chess engines that rely on complex heuristics, explicit search, or a combination of both, we train a 270M parameter transformer model with supervised learning on a dataset of 10 million chess games. We annotate each board in the dataset with action-values provided by the powerful Stockfish 16 engine, leading to roughly 15 billion data points. Our largest model reaches a Lichess blitz Elo of 2895 against humans, and successfully solves a series of challenging chess puzzles, without any domain-specific tweaks or explicit search algorithms. We also show that our model outperforms AlphaZero's policy and value networks (without MCTS) and GPT-3.5-turbo-instruct. A systematic investigation of model and dataset size shows that strong chess 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#21442;&#38543;&#26426;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#25216;&#26415;&#22312;&#38750;&#20984;&#21644;&#20984;&#35774;&#32622;&#19979;&#37117;&#33021;&#21462;&#24471;&#20248;&#20110;&#20808;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#25351;&#20986;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26080;&#27861;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03126</link><description>&lt;p&gt;
&#26080;&#21442;&#38543;&#26426;&#20248;&#21270;&#30340;&#33258;&#30001;&#24230;&#26377;&#22810;&#39640;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Free is Parameter-Free Stochastic Optimization?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03126
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#21442;&#38543;&#26426;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#25216;&#26415;&#22312;&#38750;&#20984;&#21644;&#20984;&#35774;&#32622;&#19979;&#37117;&#33021;&#21462;&#24471;&#20248;&#20110;&#20808;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#25351;&#20986;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26080;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#21442;&#38543;&#26426;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#21487;&#20197;&#23384;&#22312;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#65306;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#19982;&#26368;&#20248;&#35843;&#21442;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#30495;&#23454;&#38382;&#39064;&#21442;&#25968;&#26377;&#24456;&#22810;&#30693;&#35782;&#12290;&#29616;&#26377;&#30340;&#26080;&#21442;&#26041;&#27861;&#21482;&#33021;&#34987;&#35270;&#20026;&#8220;&#37096;&#20998;&#8221;&#26080;&#21442;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#23545;&#30495;&#23454;&#38382;&#39064;&#21442;&#25968;&#26377;&#19968;&#20123;&#38750;&#24179;&#20961;&#30340;&#30693;&#35782;&#65292;&#27604;&#22914;&#38543;&#26426;&#26799;&#24230;&#33539;&#25968;&#30340;&#19978;&#30028;&#12289;&#21040;&#26368;&#23567;&#20540;&#30340;&#36317;&#31163;&#30340;&#19978;&#30028;&#31561;&#12290;&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#25216;&#26415;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#65292;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#26356;&#22797;&#26434;&#30340;&#20808;&#36827;&#31639;&#27861;&#12290;&#22312;&#20855;&#26377;&#22122;&#22768;&#20989;&#25968;&#20540;&#30340;&#20984;&#35774;&#32622;&#19979;&#65292;&#22312;&#36739;&#23567;&#30340;&#22122;&#22768;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#20063;&#25552;&#20379;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#20551;&#35774;&#21482;&#33021;&#35775;&#38382;&#38543;&#26426;&#26799;&#24230;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#20351;&#24471;&#23436;&#20840;&#26080;&#21442;&#30340;&#26041;&#27861;&#26080;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of parameter-free stochastic optimization, inquiring whether, and under what conditions, do fully parameter-free methods exist: these are methods that achieve convergence rates competitive with optimally tuned methods, without requiring significant knowledge of the true problem parameters. Existing parameter-free methods can only be considered ``partially'' parameter-free, as they require some non-trivial knowledge of the true problem parameters, such as a bound on the stochastic gradient norms, a bound on the distance to a minimizer, etc. In the non-convex setting, we demonstrate that a simple hyperparameter search technique results in a fully parameter-free method that outperforms more sophisticated state-of-the-art algorithms. We also provide a similar result in the convex setting with access to noisy function values under mild noise assumptions. Finally, assuming only access to stochastic gradients, we establish a lower bound that renders fully parameter-free s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26680;PCA&#36827;&#34892;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20027;&#25104;&#20998;&#23376;&#31354;&#38388;&#20013;&#24341;&#20837;&#38750;&#32447;&#24615;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#23545;&#20869;&#20998;&#24067;&#21644;&#22806;&#20998;&#24067;&#25968;&#25454;&#30340;&#26377;&#25928;&#21306;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.02949</link><description>&lt;p&gt;
&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#26680;PCA
&lt;/p&gt;
&lt;p&gt;
Kernel PCA for Out-of-Distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26680;PCA&#36827;&#34892;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20027;&#25104;&#20998;&#23376;&#31354;&#38388;&#20013;&#24341;&#20837;&#38750;&#32447;&#24615;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#23545;&#20869;&#20998;&#24067;&#21644;&#22806;&#20998;&#24067;&#25968;&#25454;&#30340;&#26377;&#25928;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#20998;&#24067;&#65288;OoD&#65289;&#26816;&#27979;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30452;&#25509;&#24212;&#29992;&#20110;DNN&#29305;&#24449;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#22312;&#26816;&#27979;&#26469;&#33258;&#20869;&#20998;&#24067;&#65288;InD&#65289;&#25968;&#25454;&#30340;OoD&#25968;&#25454;&#26041;&#38754;&#19981;&#36275;&#22815;&#12290;PCA&#30340;&#22833;&#36133;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#22312;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#31616;&#21333;&#22788;&#29702;&#26080;&#27861;&#24456;&#22909;&#22320;&#23558;OoD&#21644;InD&#20013;&#30340;&#32593;&#32476;&#29305;&#24449;&#20998;&#31163;&#24320;&#26469;&#65292;&#32780;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#26469;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26680;PCA&#65288;KPCA&#65289;&#26694;&#26550;&#36827;&#34892;OoD&#26816;&#27979;&#65292;&#23547;&#25214;OoD&#21644;InD&#29305;&#24449;&#20197;&#26174;&#33879;&#19981;&#21516;&#30340;&#27169;&#24335;&#20998;&#37197;&#30340;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#29305;&#24449;&#26144;&#23556;&#65292;&#22312;KPCA&#20013;&#24341;&#20837;&#38750;&#32447;&#24615;&#20869;&#26680;&#65292;&#20197;&#20419;&#36827;&#22312;&#20027;&#25104;&#20998;&#24352;&#25104;&#30340;&#23376;&#31354;&#38388;&#20013;InD&#21644;OoD&#25968;&#25454;&#20043;&#38388;&#30340;&#21487;&#20998;&#24615;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#22312;&#36825;&#31181;&#23376;&#31354;&#38388;&#20013;&#30340;&#37325;&#26500;&#35823;&#24046;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24471;&#21040;$\mathcal{O}(1)$&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution (OoD) detection is vital for the reliability of Deep Neural Networks (DNNs). Existing works have shown the insufficiency of Principal Component Analysis (PCA) straightforwardly applied on the features of DNNs in detecting OoD data from In-Distribution (InD) data. The failure of PCA suggests that the network features residing in OoD and InD are not well separated by simply proceeding in a linear subspace, which instead can be resolved through proper nonlinear mappings. In this work, we leverage the framework of Kernel PCA (KPCA) for OoD detection, seeking subspaces where OoD and InD features are allocated with significantly different patterns. We devise two feature mappings that induce non-linear kernels in KPCA to advocate the separability between InD and OoD data in the subspace spanned by the principal components. Given any test sample, the reconstruction error in such subspace is then used to efficiently obtain the detection result with $\mathcal{O}(1)$ time comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#65292;&#21363;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#65292;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#32780;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#23548;&#33268;&#20048;&#35266;&#30340;&#24615;&#33021;&#20272;&#35745;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#25104;&#31435;&#12290;</title><link>http://arxiv.org/abs/2401.13796</link><description>&lt;p&gt;
&#19981;&#35201;&#25353;&#25353;&#38062;&#65281;&#25506;&#32034;&#26426;&#22120;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Don't Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning. (arXiv:2401.13796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#65292;&#21363;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#65292;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#32780;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#23548;&#33268;&#20048;&#35266;&#30340;&#24615;&#33021;&#20272;&#35745;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#20026;&#22810;&#20010;&#39046;&#22495;&#25552;&#20379;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;ML&#24037;&#20855;&#30340;&#26085;&#30410;&#21487;&#33719;&#24471;&#24615;&#65292;&#35768;&#22810;&#20174;&#19994;&#32773;&#32570;&#20047;&#28145;&#20837;&#30340;ML&#19987;&#19994;&#30693;&#35782;&#65292;&#37319;&#29992;&#20102;&#8220;&#25353;&#25353;&#38062;&#8221;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#32780;&#24573;&#35270;&#20102;&#24213;&#23618;&#31639;&#27861;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#20294;&#23427;&#24341;&#21457;&#20102;&#23545;&#32467;&#26524;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#65292;&#23548;&#33268;&#20102;&#38169;&#35823;&#30340;&#24615;&#33021;&#35780;&#20272;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;ML&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#25968;&#25454;&#27844;&#38706;&#65292;&#20854;&#20013;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#20102;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#65292;&#29992;&#25143;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#19981;&#25104;&#31435;&#30340;&#20048;&#35266;&#24615;&#33021;&#20272;&#35745;&#12290;&#35780;&#20272;&#24615;&#33021;&#19982;&#23454;&#38469;&#22312;&#26032;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30340;&#24046;&#24322;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#12290;&#26412;&#25991;&#29305;&#21035;&#23558;ML&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#20998;&#20026;&#19981;&#21516;&#31867;&#21035;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has revolutionized various domains, offering predictive capabilities in several areas. However, with the increasing accessibility of ML tools, many practitioners, lacking deep ML expertise, adopt a "push the button" approach, utilizing user-friendly interfaces without a thorough understanding of underlying algorithms. While this approach provides convenience, it raises concerns about the reliability of outcomes, leading to challenges such as incorrect performance evaluation. This paper addresses a critical issue in ML, known as data leakage, where unintended information contaminates the training data, impacting model performance evaluation. Users, due to a lack of understanding, may inadvertently overlook crucial steps, leading to optimistic performance estimates that may not hold in real-world scenarios. The discrepancy between evaluated and actual performance on new data is a significant concern. In particular, this paper categorizes data leakage in ML, discussi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#26799;&#24230;&#19979;&#38477;&#30340;&#24230;&#37327;&#27969;&#29702;&#35770;&#65292;&#23454;&#29616;&#20102;&#22312;&#40654;&#26364;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#27969;&#21160;&#12290;&#20854;&#24212;&#29992;&#20110;&#25968;&#20540;Calabi-Yau&#24230;&#37327;&#65292;&#24182;&#25506;&#35752;&#20102;&#29305;&#24449;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.19870</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#24230;&#37327;&#27969;
&lt;/p&gt;
&lt;p&gt;
Metric Flows with Neural Networks. (arXiv:2310.19870v1 [hep-th])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#26799;&#24230;&#19979;&#38477;&#30340;&#24230;&#37327;&#27969;&#29702;&#35770;&#65292;&#23454;&#29616;&#20102;&#22312;&#40654;&#26364;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#27969;&#21160;&#12290;&#20854;&#24212;&#29992;&#20110;&#25968;&#20540;Calabi-Yau&#24230;&#37327;&#65292;&#24182;&#25506;&#35752;&#20102;&#29305;&#24449;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#31181;&#30001;&#31070;&#32463;&#32593;&#32476;&#26799;&#24230;&#19979;&#38477;&#35825;&#23548;&#30340;&#40654;&#26364;&#24230;&#37327;&#31354;&#38388;&#20013;&#27969;&#21160;&#30340;&#29702;&#35770;&#12290;&#36825;&#37096;&#20998;&#26159;&#21463;&#21040;&#36817;&#26399;&#29992;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;Calabi-Yau&#24230;&#37327;&#30340;&#36827;&#23637;&#30340;&#25512;&#21160;&#65292;&#20063;&#26159;&#30001;&#20110;&#23545;&#31070;&#32463;&#32593;&#32476;&#31354;&#38388;&#20013;&#27969;&#21160;&#30340;&#29702;&#35299;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#30456;&#24212;&#30340;&#24230;&#37327;&#27969;&#21160;&#26041;&#31243;&#65292;&#20854;&#30001;&#24230;&#37327;&#31070;&#32463;&#20999;&#21521;&#26680;&#23450;&#20041;&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38750;&#23616;&#37096;&#23545;&#35937;&#65292;&#20250;&#38543;&#26102;&#38388;&#28436;&#21270;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#32467;&#26500;&#22312;&#26080;&#31351;&#23485;&#24230;&#26497;&#38480;&#19979;&#26680;&#23558;&#21464;&#24471;&#22266;&#23450;&#19988;&#21160;&#24577;&#31616;&#21270;&#12290;&#38468;&#21152;&#20551;&#35774;&#21487;&#23548;&#33268;&#27969;&#21160;&#20013;&#30340;&#23616;&#37096;&#24615;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23454;&#29616;Perelman&#20851;&#20110;&#35299;&#20915;3D Poincar&#233;&#29468;&#24819;&#20013;&#20351;&#29992;&#30340;Ricci&#27969;&#30340;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#24605;&#24819;&#24212;&#29992;&#20110;&#25968;&#20540;Calabi-Yau&#24230;&#37327;&#65292;&#21253;&#25324;&#20851;&#20110;&#29305;&#24449;&#23398;&#20064;&#37325;&#35201;&#24615;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a theory of flows in the space of Riemannian metrics induced by neural network gradient descent. This is motivated in part by recent advances in approximating Calabi-Yau metrics with neural networks and is enabled by recent advances in understanding flows in the space of neural networks. We derive the corresponding metric flow equations, which are governed by a metric neural tangent kernel, a complicated, non-local object that evolves in time. However, many architectures admit an infinite-width limit in which the kernel becomes fixed and the dynamics simplify. Additional assumptions can induce locality in the flow, which allows for the realization of Perelman's formulation of Ricci flow that was used to resolve the 3d Poincar\'e conjecture. We apply these ideas to numerical Calabi-Yau metrics, including a discussion on the importance of feature learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#21464;&#20998;&#25512;&#26029;&#37325;&#26032;&#26694;&#26550;&#20026;&#22312;&#21464;&#20998;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27779;&#29791;&#26031;&#22374;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26469;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#26377;&#25928;&#24615;&#32463;&#36807;&#23454;&#35777;&#23454;&#39564;&#35777;&#23454;&#12290;</title><link>http://arxiv.org/abs/2310.16705</link><description>&lt;p&gt;
&#27779;&#29791;&#26031;&#22374;&#26799;&#24230;&#27969;&#22312;&#21464;&#20998;&#25512;&#26029;&#30340;&#21464;&#20998;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Gradient Flow over Variational Parameter Space for Variational Inference. (arXiv:2310.16705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#21464;&#20998;&#25512;&#26029;&#37325;&#26032;&#26694;&#26550;&#20026;&#22312;&#21464;&#20998;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27779;&#29791;&#26031;&#22374;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26469;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#26377;&#25928;&#24615;&#32463;&#36807;&#23454;&#35777;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#21464;&#20998;&#21442;&#25968;&#34987;&#35843;&#25972;&#20197;&#20351;&#21464;&#20998;&#20998;&#24067;&#19982;&#30495;&#23454;&#21518;&#39564;&#23613;&#21487;&#33021;&#25509;&#36817;&#12290;&#21487;&#20197;&#36890;&#36807;&#40657;&#31665;&#21464;&#20998;&#25512;&#26029;&#20013;&#30340;&#26222;&#36890;&#26799;&#24230;&#19979;&#38477;&#25110;&#33258;&#28982;&#26799;&#24230;&#21464;&#20998;&#25512;&#26029;&#20013;&#30340;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#26469;&#35299;&#20915;&#20248;&#21270;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#21464;&#20998;&#25512;&#26029;&#37325;&#26032;&#26694;&#26550;&#20026;&#22312;&#19968;&#20010;&#8220;&#21464;&#20998;&#21442;&#25968;&#31354;&#38388;&#8221;&#20013;&#23450;&#20041;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27779;&#29791;&#26031;&#22374;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#20248;&#21270;&#25216;&#26415;&#65292;&#21363;&#40657;&#31665;&#21464;&#20998;&#25512;&#26029;&#21644;&#33258;&#28982;&#26799;&#24230;&#21464;&#20998;&#25512;&#26029;&#65292;&#21487;&#20197;&#37325;&#26032;&#35299;&#37322;&#20026;&#25152;&#25552;&#20986;&#30340;&#27779;&#29791;&#26031;&#22374;&#26799;&#24230;&#19979;&#38477;&#30340;&#29305;&#23450;&#23454;&#20363;&#12290;&#20026;&#20102;&#25552;&#39640;&#20248;&#21270;&#25928;&#29575;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#26469;&#25968;&#20540;&#27714;&#35299;&#31163;&#25955;&#26799;&#24230;&#27969;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational inference (VI) can be cast as an optimization problem in which the variational parameters are tuned to closely align a variational distribution with the true posterior. The optimization task can be approached through vanilla gradient descent in black-box VI or natural-gradient descent in natural-gradient VI. In this work, we reframe VI as the optimization of an objective that concerns probability distributions defined over a \textit{variational parameter space}. Subsequently, we propose Wasserstein gradient descent for tackling this optimization problem. Notably, the optimization techniques, namely black-box VI and natural-gradient VI, can be reinterpreted as specific instances of the proposed Wasserstein gradient descent. To enhance the efficiency of optimization, we develop practical methods for numerically solving the discrete gradient flows. We validate the effectiveness of the proposed methods through empirical experiments on a synthetic dataset, supplemented by theore
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#29992;&#20110;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#65292;&#37319;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#21644;&#19968;&#31181;&#26032;&#30340;&#21452;&#25439;&#22833;&#20989;&#25968;&#12290;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#19981;&#21516;&#31995;&#32479;&#24182;&#23547;&#25214;&#26368;&#20339;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#23545;&#21508;&#31181;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11477</link><description>&lt;p&gt;
Robust-MBFD&#65306;&#20351;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#21644;&#19968;&#31181;&#26032;&#30340;&#21452;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#30340;&#31283;&#20581;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Robust-MBFD: A Robust Deep Learning System for Motor Bearing Faults Detection Using Multiple Deep Learning Training Strategies and A Novel Double Loss Function. (arXiv:2310.11477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#29992;&#20110;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#65292;&#37319;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#21644;&#19968;&#31181;&#26032;&#30340;&#21452;&#25439;&#22833;&#20989;&#25968;&#12290;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#19981;&#21516;&#31995;&#32479;&#24182;&#23547;&#25214;&#26368;&#20339;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#23545;&#21508;&#31181;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#65288;MBFD&#65289;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25391;&#21160;&#20449;&#21495;&#35782;&#21035;&#30005;&#26426;&#36724;&#25215;&#30340;&#25925;&#38556;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;MBFD&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;MBFD&#31995;&#32479;&#65292;&#20998;&#21035;&#25506;&#32034;&#20102;&#30417;&#30563;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#36825;&#19977;&#31181;&#35757;&#32451;&#31574;&#30053;&#12290;&#23545;&#25552;&#20986;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21644;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#24182;&#25214;&#20986;&#20102;&#36866;&#29992;&#20110;MBFD&#20219;&#21153;&#30340;&#26368;&#20339;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#32654;&#22269;&#26426;&#26800;&#25925;&#38556;&#39044;&#38450;&#25216;&#26415;&#21327;&#20250;&#65288;MFPT&#65289;&#12289;&#20975;&#26031;&#35199;&#20648;&#22823;&#23398;&#36724;&#25215;&#20013;&#24515;&#65288;CWRU&#65289;&#21644;&#24085;&#24503;&#21338;&#24681;&#22823;&#23398;&#30340;&#30005;&#26426;&#39537;&#21160;&#31995;&#32479;&#36724;&#25215;&#25439;&#20260;&#29366;&#24577;&#30417;&#27979;&#31561;&#19981;&#21516;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive analysis of motor bearing fault detection (MBFD), which involves the task of identifying faults in a motor bearing based on its vibration. To this end, we first propose and evaluate various machine learning based systems for the MBFD task. Furthermore, we propose three deep learning based systems for the MBFD task, each of which explores one of the following training strategies: supervised learning, semi-supervised learning, and unsupervised learning. The proposed machine learning based systems and deep learning based systems are evaluated, compared, and then they are used to identify the best model for the MBFD task. We conducted extensive experiments on various benchmark datasets of motor bearing faults, including those from the American Society for Mechanical Failure Prevention Technology (MFPT), Case Western Reserve University Bearing Center (CWRU), and the Condition Monitoring of Bearing Damage in Electromechanical Drive Systems from Paderborn U
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;&#19968;&#31181;&#37325;&#32622;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;"zapping"&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#25345;&#32493;&#21644;&#36801;&#31227;&#23398;&#20064;&#25928;&#26524;&#65292;&#21516;&#26102;&#20855;&#22791;&#31616;&#21333;&#23454;&#26045;&#21644;&#39640;&#25928;&#35745;&#31639;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.07996</link><description>&lt;p&gt;
&#37325;&#32622;&#24182;&#24536;&#21364;&#65306;&#37325;&#26032;&#23398;&#20064;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#25913;&#21892;&#25345;&#32493;&#21644;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reset It and Forget It: Relearning Last-Layer Weights Improves Continual and Transfer Learning. (arXiv:2310.07996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;&#19968;&#31181;&#37325;&#32622;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;"zapping"&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#25345;&#32493;&#21644;&#36801;&#31227;&#23398;&#20064;&#25928;&#26524;&#65292;&#21516;&#26102;&#20855;&#22791;&#31616;&#21333;&#23454;&#26045;&#21644;&#39640;&#25928;&#35745;&#31639;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26426;&#21046;&#65292;&#33021;&#22815;&#23548;&#33268;&#20855;&#26377;&#26356;&#22909;&#30340;&#25345;&#32493;&#21644;&#36801;&#31227;&#23398;&#20064;&#34920;&#24449;&#12290;&#36825;&#31181;&#26426;&#21046;&#8212;&#8212;&#22312;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#20013;&#21453;&#22797;&#37325;&#32622;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;zapping&#8221;&#8212;&#8212;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#20803;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#22312;&#35768;&#22810;&#19981;&#21516;&#20110;&#20803;&#23398;&#20064;&#21644;&#25345;&#32493;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#20063;&#38750;&#24120;&#36866;&#29992;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#23558;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#36801;&#31227;&#21040;&#19968;&#32452;&#26032;&#30340;&#31867;&#21035;&#65292;&#20165;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;zapping&#36807;&#31243;&#22312;&#26631;&#20934;&#24494;&#35843;&#21644;&#25345;&#32493;&#23398;&#20064;&#35774;&#32622;&#20013;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#36801;&#31227;&#20934;&#30830;&#24615;&#21644;/&#25110;&#26356;&#24555;&#30340;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#23454;&#29616;&#31616;&#21333;&#30340;&#23454;&#26045;&#21644;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20351;&#29992;zapping&#21644;&#39034;&#24207;&#23398;&#20064;&#30340;&#32452;&#21512;&#65292;&#25105;&#20204;&#21487;&#20197;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#30340;&#20803;&#23398;&#20064;&#30456;&#24403;&#30340;&#24615;&#33021;&#32780;&#26080;&#38656;&#26114;&#36149;&#30340;&#39640;&#38454;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work identifies a simple pre-training mechanism that leads to representations exhibiting better continual and transfer learning. This mechanism -- the repeated resetting of weights in the last layer, which we nickname "zapping" -- was originally designed for a meta-continual-learning procedure, yet we show it is surprisingly applicable in many settings beyond both meta-learning and continual learning. In our experiments, we wish to transfer a pre-trained image classifier to a new set of classes, in a few shots. We show that our zapping procedure results in improved transfer accuracy and/or more rapid adaptation in both standard fine-tuning and continual learning settings, while being simple to implement and computationally efficient. In many cases, we achieve performance on par with state of the art meta-learning without needing the expensive higher-order gradients, by using a combination of zapping and sequential learning. An intuitive explanation for the effectiveness of this za
&lt;/p&gt;</description></item><item><title>ProbTS&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#21327;&#21516;&#21644;&#27604;&#36739;&#23450;&#21046;&#31070;&#32463;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12289;&#20248;&#21183;&#21644;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.07446</link><description>&lt;p&gt;
ProbTS&#65306;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#28145;&#24230;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32479;&#19968;&#24037;&#20855;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;
ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting. (arXiv:2310.07446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07446
&lt;/p&gt;
&lt;p&gt;
ProbTS&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#21327;&#21516;&#21644;&#27604;&#36739;&#23450;&#21046;&#31070;&#32463;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12289;&#20248;&#21183;&#21644;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#36825;&#20010;&#39046;&#22495;&#20998;&#21270;&#25104;&#20102;&#20004;&#20010;&#26174;&#33879;&#30340;&#20998;&#25903;&#65306;&#19968;&#20010;&#19987;&#27880;&#20110;&#20026;&#26102;&#38388;&#24207;&#21015;&#23450;&#21046;&#29305;&#23450;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#21478;&#19968;&#20010;&#21033;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#34429;&#28982;&#36825;&#20004;&#20010;&#20998;&#25903;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#25968;&#25454;&#24773;&#26223;&#12289;&#26041;&#27861;&#35770;&#28966;&#28857;&#21644;&#35299;&#30721;&#26041;&#26696;&#19978;&#30340;&#24046;&#24322;&#25552;&#20986;&#20102;&#28145;&#20837;&#32780;&#26410;&#34987;&#25506;&#32034;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30693;&#35782;&#40511;&#27807;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ProbTS&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#21327;&#21516;&#21644;&#27604;&#36739;&#36825;&#20004;&#20010;&#19981;&#21516;&#30340;&#20998;&#25903;&#12290;ProbTS&#20855;&#22791;&#32479;&#19968;&#30340;&#25968;&#25454;&#27169;&#22359;&#12289;&#27169;&#22359;&#21270;&#30340;&#27169;&#22411;&#27169;&#22359;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#22120;&#27169;&#22359;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#37325;&#26032;&#23457;&#35270;&#21644;&#22522;&#20934;&#27979;&#35797;&#36825;&#20004;&#20010;&#20998;&#25903;&#30340;&#39046;&#20808;&#26041;&#27861;&#12290;&#36890;&#36807;ProbTS&#30340;&#23457;&#26597;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#12289;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#20197;&#21450;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-series forecasting serves as a linchpin in a myriad of applications, spanning various domains. With the growth of deep learning, this arena has bifurcated into two salient branches: one focuses on crafting specific neural architectures tailored for time series, and the other harnesses advanced deep generative models for probabilistic forecasting. While both branches have made significant progress, their differences across data scenarios, methodological focuses, and decoding schemes pose profound, yet unexplored, research questions. To bridge this knowledge chasm, we introduce ProbTS, a pioneering toolkit developed to synergize and compare these two distinct branches. Endowed with a unified data module, a modularized model module, and a comprehensive evaluator module, ProbTS allows us to revisit and benchmark leading methods from both branches. The scrutiny with ProbTS highlights their distinct characteristics, relative strengths and weaknesses, and areas that need further explorat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#30340;&#26680;&#36817;&#20284;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#65292;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#34920;&#36798;&#24335;&#65292;&#24182;&#24471;&#20986;&#20102;&#23574;&#38160;&#25351;&#25968;&#30028;&#38480;&#65292;&#25903;&#25345;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#27604;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#26356;&#20855;&#20449;&#24687;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07370</link><description>&lt;p&gt;
&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;: &#26126;&#30830;&#24418;&#24335;&#21644;&#23574;&#38160;&#19981;&#31561;&#24335;
&lt;/p&gt;
&lt;p&gt;
Orthogonal Random Features: Explicit Forms and Sharp Inequalities. (arXiv:2310.07370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07370
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#30340;&#26680;&#36817;&#20284;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#65292;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#34920;&#36798;&#24335;&#65292;&#24182;&#24471;&#20986;&#20102;&#23574;&#38160;&#25351;&#25968;&#30028;&#38480;&#65292;&#25903;&#25345;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#27604;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#26356;&#20855;&#20449;&#24687;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#29305;&#24449;&#36890;&#36807;&#38543;&#26426;&#21270;&#25216;&#26415;&#34987;&#24341;&#20837;&#20197;&#25193;&#23637;&#26680;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#21644;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#34987;&#29992;&#26469;&#36817;&#20284;&#27969;&#34892;&#30340;&#39640;&#26031;&#26680;&#12290;&#21069;&#32773;&#36890;&#36807;&#38543;&#26426;&#39640;&#26031;&#30697;&#38453;&#25191;&#34892;&#65292;&#24182;&#22312;&#24179;&#22343;&#21518;&#24471;&#21040;&#20102;&#23436;&#20840;&#31526;&#21512;&#39640;&#26031;&#26680;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22522;&#20110;&#29992;&#21040;Haar&#27491;&#20132;&#30697;&#38453;&#30340;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#30340;&#26680;&#36817;&#20284;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;&#25105;&#20204;&#20351;&#29992;&#24402;&#19968;&#21270;&#36125;&#22622;&#23572;&#20989;&#25968;&#25552;&#20379;&#20102;&#36825;&#20123;&#37327;&#30340;&#26126;&#30830;&#34920;&#36798;&#24335;&#65292;&#24182;&#25512;&#23548;&#20102;&#25903;&#25345;&#27491;&#20132;&#38543;&#26426;&#29305;&#24449;&#27604;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#23574;&#38160;&#25351;&#25968;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random features have been introduced to scale up kernel methods via randomization techniques. In particular, random Fourier features and orthogonal random features were used to approximate the popular Gaussian kernel. The former is performed by a random Gaussian matrix and leads exactly to the Gaussian kernel after averaging. In this work, we analyze the bias and the variance of the kernel approximation based on orthogonal random features which makes use of Haar orthogonal matrices. We provide explicit expressions for these quantities using normalized Bessel functions and derive sharp exponential bounds supporting the view that orthogonal random features are more informative than random Fourier features.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#23545;&#25509;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03946</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#27169;&#22411;&#25913;&#36827;&#20102;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improved prediction of ligand-protein binding affinities by meta-modeling. (arXiv:2310.03946v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03946
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#23545;&#25509;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#20934;&#30830;&#31579;&#36873;&#20505;&#36873;&#33647;&#29289;&#37197;&#20307;&#19982;&#38774;&#34507;&#30333;&#30340;&#32467;&#21512;&#26159;&#33647;&#29289;&#24320;&#21457;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#65292;&#22240;&#20026;&#31579;&#36873;&#28508;&#22312;&#20505;&#36873;&#29289;&#33021;&#22815;&#33410;&#30465;&#25214;&#33647;&#29289;&#30340;&#26102;&#38388;&#21644;&#36153;&#29992;&#12290;&#36825;&#31181;&#34394;&#25311;&#31579;&#36873;&#37096;&#20998;&#20381;&#36182;&#20110;&#39044;&#27979;&#37197;&#20307;&#21644;&#34507;&#30333;&#36136;&#20043;&#38388;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#26041;&#27861;&#12290;&#37492;&#20110;&#23384;&#22312;&#35768;&#22810;&#35745;&#31639;&#27169;&#22411;&#23545;&#19981;&#21516;&#30446;&#26631;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#32467;&#26524;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#24320;&#21457;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#24050;&#21457;&#34920;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#23545;&#25509;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#26500;&#24314;&#12290;&#22312;&#26500;&#24314;&#36825;&#20010;&#26694;&#26550;&#26102;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35768;&#22810;&#32452;&#21512;&#30340;&#20010;&#21035;&#27169;&#22411;&#12289;&#35757;&#32451;&#25968;&#25454;&#24211;&#20197;&#21450;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#30340;&#20803;&#27169;&#22411;&#26041;&#27861;&#12290;&#25105;&#20204;&#26174;&#31034;&#20986;&#35768;&#22810;&#20803;&#27169;&#22411;&#22312;&#20146;&#21644;&#21147;&#39044;&#27979;&#19978;&#26174;&#33879;&#25913;&#21892;&#20102;&#20010;&#21035;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#20803;&#27169;&#22411;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#32431;&#32467;&#26500;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate screening of candidate drug ligands against target proteins through computational approaches is of prime interest to drug development efforts, as filtering potential candidates would save time and expenses for finding drugs. Such virtual screening depends in part on methods to predict the binding affinity between ligands and proteins. Given many computational models for binding affinity prediction with varying results across targets, we herein develop a meta-modeling framework by integrating published empirical structure-based docking and sequence-based deep learning models. In building this framework, we evaluate many combinations of individual models, training databases, and linear and nonlinear meta-modeling approaches. We show that many of our meta-models significantly improve affinity predictions over individual base models. Our best meta-models achieve comparable performance to state-of-the-art exclusively structure-based deep learning tools. Overall, we demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLPA&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#26041;&#24335;&#23454;&#29616;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#12290;FedLPA&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#21040;&#20840;&#23616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#27425;&#32858;&#21512;&#22312;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00339</link><description>&lt;p&gt;
FedLPA: &#20351;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedLPA: Personalized One-shot Federated Learning with Layer-Wise Posterior Aggregation. (arXiv:2310.00339v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLPA&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#26041;&#24335;&#23454;&#29616;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#12290;FedLPA&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#21040;&#20840;&#23616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#27425;&#32858;&#21512;&#22312;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26412;&#22320;&#23458;&#25143;&#31471;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#39640;&#25928;&#22320;&#32858;&#21512;&#21040;&#26381;&#21153;&#22120;&#19978;&#30340;&#20840;&#23616;&#27169;&#22411;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#35838;&#39064;&#12290;&#26368;&#36817;&#65292;&#21463;&#21040;&#38544;&#31169;&#38382;&#39064;&#20943;&#23569;&#12289;&#28508;&#22312;&#25915;&#20987;&#20943;&#24369;&#21644;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#30340;&#25512;&#21160;&#65292;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#65288;&#21363;&#23558;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#38388;&#30340;&#36890;&#20449;&#38480;&#21046;&#20026;&#19968;&#36718;&#65289;&#22312;&#30740;&#31350;&#32773;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#21333;&#27425;&#32858;&#21512;&#30340;&#24615;&#33021;&#23481;&#26131;&#21463;&#21040;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#22312;&#19968;&#20123;&#23454;&#38469;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#27425;&#32858;&#21512;&#26041;&#27861;&#8212;&#8212;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#65288;FedLPA&#65289;&#12290;FedLPA&#33021;&#22815;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#65292;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#25110;&#26292;&#38706;&#20219;&#20309;&#26426;&#23494;&#30340;&#26412;&#22320;&#20449;&#24687;&#65292;&#27604;&#22914;&#26631;&#31614;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently aggregating trained neural networks from local clients into a global model on a server is a widely researched topic in federated learning. Recently, motivated by diminishing privacy concerns, mitigating potential attacks, and reducing the overhead of communication, one-shot federated learning (i.e., limiting client-server communication into a single round) has gained popularity among researchers. However, the one-shot aggregation performances are sensitively affected by the non-identical training data distribution, which exhibits high statistical heterogeneity in some real-world scenarios. To address this issue, we propose a novel one-shot aggregation method with Layer-wise Posterior Aggregation, named FedLPA. FedLPA aggregates local models to obtain a more accurate global model without requiring extra auxiliary datasets or exposing any confidential local information, e.g., label distributions. To effectively capture the statistics maintained in the biased local datasets in
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;&#24191;&#20041;&#21487;&#21152;&#25928;&#29992;&#32593;&#32476;&#65288;GAUNet&#65289;&#65292;&#29992;&#20110;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#21487;&#20197;&#19982;ASU-DNN&#30456;&#23218;&#32654;&#65292;&#24182;&#19988;&#30456;&#27604;&#20197;&#21069;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16970</link><description>&lt;p&gt;
&#20855;&#26377;&#24191;&#20041;&#21487;&#21152;&#25928;&#29992;&#32593;&#32476;&#30340;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discrete-Choice Model with Generalized Additive Utility Network. (arXiv:2309.16970v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;&#24191;&#20041;&#21487;&#21152;&#25928;&#29992;&#32593;&#32476;&#65288;GAUNet&#65289;&#65292;&#29992;&#20110;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#21487;&#20197;&#19982;ASU-DNN&#30456;&#23218;&#32654;&#65292;&#24182;&#19988;&#30456;&#27604;&#20197;&#21069;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#26159;&#20998;&#26512;&#20915;&#31574;&#34892;&#20026;&#30340;&#24378;&#22823;&#26694;&#26550;&#65292;&#20026;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#20225;&#19994;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20351;&#29992;&#32447;&#24615;&#25928;&#29992;&#20989;&#25968;&#30340;&#22810;&#39033;&#24335;&#36923;&#36753;&#27169;&#22411;&#65288;MNLs&#65289;&#22240;&#20854;&#26131;&#20110;&#20351;&#29992;&#21644;&#21487;&#35299;&#37322;&#24615;&#32780;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20855;&#26377;&#31070;&#32463;&#32593;&#32476;&#65288;&#20363;&#22914;ASU-DNN&#65289;&#30340;MNLs&#65292;&#24182;&#19988;&#22312;&#34892;&#20026;&#36873;&#25321;&#30340;&#39044;&#27979;&#31934;&#24230;&#19978;&#27604;&#20256;&#32479;MNLs&#26356;&#39640;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30001;&#20110;&#22797;&#26434;&#32467;&#26500;&#32780;&#32570;&#20047;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25928;&#29992;&#20989;&#25968;&#65292;&#31216;&#20026;&#24191;&#20041;&#21487;&#21152;&#25928;&#29992;&#32593;&#32476;&#65288;GAUNet&#65289;&#65292;&#29992;&#20110;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;&#19996;&#20140;&#25910;&#38598;&#30340;&#20986;&#34892;&#35843;&#26597;&#25968;&#25454;&#35780;&#20272;&#20102;&#20855;&#26377;GAUNet&#30340;MNL&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;ASU-DNN&#30456;&#24403;&#65292;&#24182;&#19988;&#30456;&#27604;&#20197;&#21069;&#30340;&#27169;&#22411;&#20855;&#26377;&#25913;&#36827;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete-choice models are a powerful framework for analyzing decision-making behavior to provide valuable insights for policymakers and businesses. Multinomial logit models (MNLs) with linear utility functions have been used in practice because they are ease to use and interpretable. Recently, MNLs with neural networks (e.g., ASU-DNN) have been developed, and they have achieved higher prediction accuracy in behavior choice than classical MNLs. However, these models lack interpretability owing to complex structures. We developed utility functions with a novel neural-network architecture based on generalized additive models, named generalized additive utility network ( GAUNet), for discrete-choice models. We evaluated the performance of the MNL with GAUNet using the trip survey data collected in Tokyo. Our models were comparable to ASU-DNN in accuracy and exhibited improved interpretability compared to previous models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#30340;&#20154;&#29289;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;PARADOX&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#20110;&#30495;&#23454;&#20010;&#20307;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#30340;&#25991;&#26412;&#12290;&#35813;&#27169;&#22411;&#20197;&#29992;&#25143;&#30340;&#20154;&#29289;&#24418;&#35937;&#20026;&#26465;&#20214;&#26469;&#32534;&#30721;&#23545;&#35805;&#65292;&#24182;&#29983;&#25104;&#19981;&#24102;&#21333;&#35821;&#21442;&#32771;&#25968;&#25454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#27169;&#22411;&#36824;&#36827;&#34892;&#23545;&#40784;&#65292;&#20351;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#25509;&#36817;&#30495;&#23454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35821;&#20041;&#19978;&#26356;&#26377;&#24847;&#20041;&#65292;&#22312;&#35821;&#35328;&#19978;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2309.02915</link><description>&lt;p&gt;
&#12298;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#30340;&#20154;&#29289;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;&#12299;
&lt;/p&gt;
&lt;p&gt;
Persona-aware Generative Model for Code-mixed Language. (arXiv:2309.02915v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#30340;&#20154;&#29289;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;PARADOX&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#20110;&#30495;&#23454;&#20010;&#20307;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#30340;&#25991;&#26412;&#12290;&#35813;&#27169;&#22411;&#20197;&#29992;&#25143;&#30340;&#20154;&#29289;&#24418;&#35937;&#20026;&#26465;&#20214;&#26469;&#32534;&#30721;&#23545;&#35805;&#65292;&#24182;&#29983;&#25104;&#19981;&#24102;&#21333;&#35821;&#21442;&#32771;&#25968;&#25454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#27169;&#22411;&#36824;&#36827;&#34892;&#23545;&#40784;&#65292;&#20351;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#25509;&#36817;&#30495;&#23454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35821;&#20041;&#19978;&#26356;&#26377;&#24847;&#20041;&#65292;&#22312;&#35821;&#35328;&#19978;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#21644;&#22810;&#35821;&#35328;&#31038;&#20250;&#20013;&#65292;&#20195;&#30721;&#28151;&#21512;&#21644;&#33050;&#26412;&#28151;&#21512;&#38750;&#24120;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#23545;&#20110;&#20195;&#30721;&#28151;&#21512;&#30340;&#20559;&#22909;&#21462;&#20915;&#20110;&#29992;&#25143;&#30340;&#31038;&#20250;&#32463;&#27982;&#22320;&#20301;&#12289;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#21644;&#24403;&#22320;&#29615;&#22659;&#65292;&#32780;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#26102;&#22823;&#22810;&#24573;&#35270;&#20102;&#36825;&#20123;&#22240;&#32032;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#24320;&#21457;&#19968;&#31181;&#20154;&#29289;&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#31867;&#20284;&#20110;&#30495;&#23454;&#20010;&#20307;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20195;&#30721;&#28151;&#21512;&#29983;&#25104;&#30340;&#20154;&#29289;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;&#65288;PARADOX&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#32473;&#23450;&#29992;&#25143;&#30340;&#20154;&#29289;&#24418;&#35937;&#30340;&#26465;&#20214;&#19979;&#23545;&#35805;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#29983;&#25104;&#19981;&#24102;&#21333;&#35821;&#21442;&#32771;&#25968;&#25454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#40784;&#27169;&#22359;&#65292;&#23545;&#29983;&#25104;&#30340;&#24207;&#21015;&#36827;&#34892;&#37325;&#26032;&#26657;&#20934;&#65292;&#20351;&#20854;&#26356;&#25509;&#36817;&#30495;&#23454;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#12290;PARADOX&#29983;&#25104;&#30340;&#20195;&#30721;&#28151;&#21512;&#25991;&#26412;&#22312;&#35821;&#20041;&#19978;&#26356;&#26377;&#24847;&#20041;&#65292;&#22312;&#35821;&#35328;&#19978;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code-mixing and script-mixing are prevalent across online social networks and multilingual societies. However, a user's preference toward code-mixing depends on the socioeconomic status, demographics of the user, and the local context, which existing generative models mostly ignore while generating code-mixed texts. In this work, we make a pioneering attempt to develop a persona-aware generative model to generate texts resembling real-life code-mixed texts of individuals. We propose a Persona-aware Generative Model for Code-mixed Generation, PARADOX, a novel Transformer-based encoder-decoder model that encodes an utterance conditioned on a user's persona and generates code-mixed texts without monolingual reference data. We propose an alignment module that re-calibrates the generated sequence to resemble real-life code-mixed texts. PARADOX generates code-mixed texts that are semantically more meaningful and linguistically more valid. To evaluate the personification capabilities of PARAD
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#32467;&#21512;&#20102;&#22810;&#31181;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#65292;&#24182;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01472</link><description>&lt;p&gt;
&#21453;&#21521;&#31283;&#23450;&#25193;&#25955;&#65306;&#29983;&#25104;&#35813;&#22270;&#20687;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Reverse Stable Diffusion: What prompt was used to generate this image?. (arXiv:2308.01472v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#32467;&#21512;&#20102;&#22810;&#31181;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#65292;&#24182;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65292;&#26368;&#36817;&#21560;&#24341;&#20102;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#20852;&#36259;&#65292;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;&#29983;&#25104;&#36807;&#31243;&#21644;&#22914;&#20309;&#35774;&#35745;&#25552;&#31034;&#20197;&#33719;&#24471;&#25152;&#38656;&#22270;&#20687;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#31995;&#21015;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65288;&#26377;&#21644;&#26080;&#23545;&#25193;&#25955;&#32593;&#32476;&#26435;&#37325;&#36827;&#34892;&#35775;&#38382;&#65289;&#26469;&#22788;&#29702;&#25152;&#25552;&#20986;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#32852;&#21512;&#25552;&#31034;&#22238;&#24402;&#21644;&#22810;&#26631;&#31614;&#35789;&#27719;&#20998;&#31867;&#30446;&#26631;&#65292;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#35838;&#31243;&#23398;&#20064;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#20855;&#26377;&#26356;&#20302;&#26631;&#27880;&#22122;&#22768;&#65288;&#21363;&#26356;&#22909;&#23545;&#40784;&#65289;&#30340;&#22270;&#20687;&#25552;&#31034;&#23545;&#30340;&#23398;&#20064;&#65292;&#24182;&#19988;&#20351;&#29992;&#30456;&#20284;&#24615;&#36827;&#34892;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models such as Stable Diffusion have recently attracted the interest of many researchers, and inverting the diffusion process can play an important role in better understanding the generative process and how to engineer prompts in order to obtain the desired images. To this end, we introduce the new task of predicting the text prompt given an image generated by a generative diffusion model. We combine a series of white-box and black-box models (with and without access to the weights of the diffusion network) to deal with the proposed task. We propose a novel learning framework comprising of a joint prompt regression and multi-label vocabulary classification objective that generates improved prompts. To further improve our method, we employ a curriculum learning procedure that promotes the learning of image-prompt pairs with lower labeling noise (i.e. that are better aligned), and an unsupervised domain-adaptive kernel learning method that uses the similarities b
&lt;/p&gt;</description></item><item><title>TemperatureGAN&#26159;&#19968;&#20010;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20351;&#29992;&#22320;&#38754;&#20197;&#19978;2m&#30340;&#22823;&#27668;&#28201;&#24230;&#25968;&#25454;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#33391;&#22909;&#31354;&#38388;&#34920;&#31034;&#21644;&#19982;&#26172;&#22812;&#21608;&#26399;&#19968;&#33268;&#30340;&#26102;&#38388;&#21160;&#24577;&#30340;&#39640;&#20445;&#30495;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.17248</link><description>&lt;p&gt;
TemperatureGAN: &#21306;&#22495;&#22823;&#27668;&#28201;&#24230;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
TemperatureGAN: Generative Modeling of Regional Atmospheric Temperatures. (arXiv:2306.17248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17248
&lt;/p&gt;
&lt;p&gt;
TemperatureGAN&#26159;&#19968;&#20010;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20351;&#29992;&#22320;&#38754;&#20197;&#19978;2m&#30340;&#22823;&#27668;&#28201;&#24230;&#25968;&#25454;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#33391;&#22909;&#31354;&#38388;&#34920;&#31034;&#21644;&#19982;&#26172;&#22812;&#21608;&#26399;&#19968;&#33268;&#30340;&#26102;&#38388;&#21160;&#24577;&#30340;&#39640;&#20445;&#30495;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#29983;&#25104;&#22120;&#23545;&#20110;&#20272;&#35745;&#27668;&#20505;&#23545;&#21508;&#20010;&#39046;&#22495;&#30340;&#24433;&#21709;&#38750;&#24120;&#26377;&#29992;&#12290;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#27668;&#20505;&#39118;&#38505;&#30340;&#39044;&#27979;&#65292;&#20363;&#22914;&#33021;&#28304;&#31995;&#32479;&#65292;&#38656;&#35201;&#20934;&#30830;&#65288;&#19982;&#22522;&#20934;&#30495;&#23454;&#25968;&#25454;&#26377;&#32479;&#35745;&#30456;&#20284;&#24615;&#65289;&#12289;&#21487;&#38752;&#65288;&#19981;&#20135;&#29983;&#38169;&#35823;&#26679;&#26412;&#65289;&#21644;&#39640;&#25928;&#30340;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#21271;&#32654;&#38470;&#22320;&#25968;&#25454;&#21516;&#21270;&#31995;&#32479;&#30340;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;TemperatureGAN&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#26376;&#20221;&#12289;&#20301;&#32622;&#21644;&#26102;&#38388;&#27573;&#20026;&#26465;&#20214;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20197;&#27599;&#23567;&#26102;&#20998;&#36776;&#29575;&#29983;&#25104;&#22320;&#38754;&#20197;&#19978;2m&#30340;&#22823;&#27668;&#28201;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#21644;&#25351;&#26631;&#26469;&#34913;&#37327;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;TemperatureGAN&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#33391;&#22909;&#31354;&#38388;&#34920;&#31034;&#21644;&#19982;&#24050;&#30693;&#26172;&#22812;&#21608;&#26399;&#19968;&#33268;&#30340;&#26102;&#38388;&#21160;&#24577;&#30340;&#39640;&#20445;&#30495;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic generators are useful for estimating climate impacts on various sectors. Projecting climate risk in various sectors, e.g. energy systems, requires generators that are accurate (statistical resemblance to ground-truth), reliable (do not produce erroneous examples), and efficient. Leveraging data from the North American Land Data Assimilation System, we introduce TemperatureGAN, a Generative Adversarial Network conditioned on months, locations, and time periods, to generate 2m above ground atmospheric temperatures at an hourly resolution. We propose evaluation methods and metrics to measure the quality of generated samples. We show that TemperatureGAN produces high-fidelity examples with good spatial representation and temporal dynamics consistent with known diurnal cycles.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26412;&#22320;&#20998;&#24067;&#22810;&#32593;&#32476;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#38544;&#31169;&#20445;&#25252;&#26469;&#36827;&#34892;&#20849;&#35782;&#31038;&#21306;&#26816;&#27979;&#21644;&#20272;&#35745;&#12290;&#37319;&#29992;&#38543;&#26426;&#21709;&#24212;&#26426;&#21046;&#23545;&#32593;&#32476;&#36793;&#36827;&#34892;&#25200;&#21160;&#65292;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#35889;&#32858;&#31867;&#31639;&#27861;&#22312;&#25200;&#21160;&#37051;&#25509;&#30697;&#38453;&#19978;&#25191;&#34892;&#65292;&#20197;&#38450;&#27490;&#31038;&#21306;&#20043;&#38388;&#30340;&#25269;&#28040;&#12290;&#21516;&#26102;&#65292;&#24320;&#21457;&#20102;&#20004;&#27493;&#20559;&#24046;&#35843;&#25972;&#36807;&#31243;&#26469;&#28040;&#38500;&#25200;&#21160;&#21644;&#32593;&#32476;&#30697;&#38453;&#24102;&#26469;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.15709</link><description>&lt;p&gt;
&#20445;&#25252;&#38544;&#31169;&#30340;&#26412;&#22320;&#20998;&#24067;&#22810;&#32593;&#32476;&#31038;&#21306;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Community Detection for Locally Distributed Multiple Networks. (arXiv:2306.15709v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26412;&#22320;&#20998;&#24067;&#22810;&#32593;&#32476;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#38544;&#31169;&#20445;&#25252;&#26469;&#36827;&#34892;&#20849;&#35782;&#31038;&#21306;&#26816;&#27979;&#21644;&#20272;&#35745;&#12290;&#37319;&#29992;&#38543;&#26426;&#21709;&#24212;&#26426;&#21046;&#23545;&#32593;&#32476;&#36793;&#36827;&#34892;&#25200;&#21160;&#65292;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#35889;&#32858;&#31867;&#31639;&#27861;&#22312;&#25200;&#21160;&#37051;&#25509;&#30697;&#38453;&#19978;&#25191;&#34892;&#65292;&#20197;&#38450;&#27490;&#31038;&#21306;&#20043;&#38388;&#30340;&#25269;&#28040;&#12290;&#21516;&#26102;&#65292;&#24320;&#21457;&#20102;&#20004;&#27493;&#20559;&#24046;&#35843;&#25972;&#36807;&#31243;&#26469;&#28040;&#38500;&#25200;&#21160;&#21644;&#32593;&#32476;&#30697;&#38453;&#24102;&#26469;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#23618;&#32593;&#32476;&#30001;&#20110;&#38544;&#31169;&#12289;&#25152;&#26377;&#26435;&#21644;&#36890;&#20449;&#25104;&#26412;&#30340;&#21407;&#22240;&#65292;&#24120;&#24120;&#20197;&#26412;&#22320;&#21644;&#20998;&#24067;&#24335;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#20998;&#26512;&#12290;&#20851;&#20110;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#27169;&#22411;&#21270;&#32479;&#35745;&#26041;&#27861;&#29992;&#20110;&#31038;&#21306;&#26816;&#27979;&#30340;&#25991;&#29486;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#26412;&#22320;&#23384;&#20648;&#21644;&#35745;&#31639;&#30340;&#32593;&#32476;&#25968;&#25454;&#30340;&#22810;&#23618;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#20849;&#35782;&#31038;&#21306;&#26816;&#27979;&#21644;&#20272;&#35745;&#65292;&#24182;&#37319;&#29992;&#38544;&#31169;&#20445;&#25252;&#12290;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#35889;&#32858;&#31867;&#65288;ppDSC&#65289;&#30340;&#26032;&#31639;&#27861;&#12290;&#20026;&#20102;&#20445;&#25252;&#36793;&#30340;&#38544;&#31169;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#38543;&#26426;&#21709;&#24212;&#65288;RR&#65289;&#26426;&#21046;&#26469;&#25200;&#21160;&#32593;&#32476;&#36793;&#65292;&#35813;&#26426;&#21046;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#30340;&#24378;&#27010;&#24565;&#12290;ppDSC&#31639;&#27861;&#22312;&#24179;&#26041;&#30340;RR&#25200;&#21160;&#37051;&#25509;&#30697;&#38453;&#19978;&#25191;&#34892;&#65292;&#20197;&#38450;&#27490;&#19981;&#21516;&#23618;&#20043;&#38388;&#30340;&#31038;&#21306;&#30456;&#20114;&#25269;&#28040;&#12290;&#20026;&#20102;&#28040;&#38500;RR&#21644;&#24179;&#26041;&#32593;&#32476;&#30697;&#38453;&#25152;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#27493;&#20559;&#24046;&#35843;&#25972;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern multi-layer networks are commonly stored and analyzed in a local and distributed fashion because of the privacy, ownership, and communication costs. The literature on the model-based statistical methods for community detection based on these data is still limited. This paper proposes a new method for consensus community detection and estimation in a multi-layer stochastic block model using locally stored and computed network data with privacy protection. A novel algorithm named privacy-preserving Distributed Spectral Clustering (ppDSC) is developed. To preserve the edges' privacy, we adopt the randomized response (RR) mechanism to perturb the network edges, which satisfies the strong notion of differential privacy. The ppDSC algorithm is performed on the squared RR-perturbed adjacency matrices to prevent possible cancellation of communities among different layers. To remove the bias incurred by RR and the squared network matrices, we develop a two-step bias-adjustment procedure.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CASSLE&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20462;&#25913;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26377;&#21521;&#25237;&#24433;&#32593;&#32476;&#65292;&#21033;&#29992;&#22686;&#24378;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06082</link><description>&lt;p&gt;
&#22686;&#24378;&#24863;&#30693;&#30340;&#26377;&#21521;&#25237;&#24433;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Augmentation-aware Self-supervised Learning with Guided Projector. (arXiv:2306.06082v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CASSLE&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20462;&#25913;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26377;&#21521;&#25237;&#24433;&#32593;&#32476;&#65292;&#21033;&#29992;&#22686;&#24378;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#20581;&#22766;&#34920;&#31034;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;SimCLR&#21644;MoCo&#31561;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#23545;&#24212;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#20445;&#25345;&#19981;&#21464;&#65292;&#33021;&#22815;&#36798;&#21040;&#19982;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#19981;&#21464;&#24615;&#21487;&#33021;&#23545;&#35299;&#20915;&#26576;&#20123;&#19979;&#28216;&#20219;&#21153;&#26377;&#23475;&#65292;&#36825;&#20123;&#20219;&#21153;&#20381;&#36182;&#20110;&#21463;&#21040;&#39044;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30340;&#22686;&#24378;&#24433;&#21709;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#39068;&#33394;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20462;&#25913;&#33258;&#30417;&#30563;&#26550;&#26500;&#30340;&#24120;&#35265;&#32452;&#20214;&#20043;&#19968;&#30340;&#26377;&#21521;&#25237;&#24433;&#32593;&#32476;&#65292;&#26469;&#20419;&#36827;&#34920;&#31034;&#31354;&#38388;&#23545;&#36825;&#20123;&#29305;&#24449;&#30340;&#25935;&#24863;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;&#25237;&#24433;&#22120;&#34917;&#20805;&#26377;&#20851;&#24212;&#29992;&#20110;&#22270;&#20687;&#30340;&#22686;&#24378;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35753;&#25237;&#24433;&#22120;&#22312;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#26102;&#21033;&#29992;&#36825;&#31181;&#36741;&#21161;&#25351;&#23548;&#65292;&#29305;&#24449;&#25552;&#21462;&#22120;&#23398;&#20064;&#22312;&#20854;&#34920;&#31034;&#20013;&#20445;&#30041;&#22686;&#24378;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#26377;&#21521;&#25237;&#24433;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;CASSLE&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) is a powerful technique for learning robust representations from unlabeled data. By learning to remain invariant to applied data augmentations, methods such as SimCLR and MoCo are able to reach quality on par with supervised approaches. However, this invariance may be harmful to solving some downstream tasks which depend on traits affected by augmentations used during pretraining, such as color. In this paper, we propose to foster sensitivity to such characteristics in the representation space by modifying the projector network, a common component of self-supervised architectures. Specifically, we supplement the projector with information about augmentations applied to images. In order for the projector to take advantage of this auxiliary guidance when solving the SSL task, the feature extractor learns to preserve the augmentation information in its representations. Our approach, coined Conditional Augmentation-aware Selfsupervised Learning (CASSLE), is d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#25551;&#36848;&#20013;&#24341;&#20837;&#25200;&#21160;&#26469;&#22686;&#24378;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#26377;&#25928;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#22120;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05079</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25552;&#21319;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Robustness of AI Offensive Code Generators via Data Augmentation. (arXiv:2306.05079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#25551;&#36848;&#20013;&#24341;&#20837;&#25200;&#21160;&#26469;&#22686;&#24378;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#26377;&#25928;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#22120;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25200;&#21160;&#28155;&#21152;&#21040;&#23433;&#20840;&#24615;&#20195;&#30721;&#19978;&#19979;&#25991;&#20013;&#30340;&#20195;&#30721;&#25551;&#36848;&#20013;&#30340;&#26041;&#27861;&#65292;&#21363;&#26469;&#33258;&#21892;&#24847;&#24320;&#21457;&#32773;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#65288;NL&#65289;&#65292;&#24182;&#20998;&#26512;&#20102;&#25200;&#21160;&#22914;&#20309;&#20197;&#21450;&#22312;&#20160;&#20040;&#31243;&#24230;&#19978;&#24433;&#21709;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NL&#25551;&#36848;&#20013;&#30340;&#25200;&#21160;&#39640;&#24230;&#24433;&#21709;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#26041;&#27861;&#25191;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#21363;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#35777;&#26126;&#20854;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a method to add perturbations to the code descriptions, i.e., new inputs in natural language (NL) from well-intentioned developers, in the context of security-oriented code, and analyze how and to what extent perturbations affect the performance of AI offensive code generators. Our experiments show that the performance of the code generators is highly affected by perturbations in the NL descriptions. To enhance the robustness of the code generators, we use the method to perform data augmentation, i.e., to increase the variability and diversity of the training data, proving its effectiveness against both perturbed and non-perturbed code descriptions.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;PAC-Bayes&#35757;&#32451;&#26694;&#26550;&#65292;&#26080;&#38656;&#39069;&#22806;&#27491;&#21017;&#21270;&#21644;&#32593;&#26684;&#25628;&#32034;&#35843;&#25972;&#36229;&#21442;&#25968;&#21363;&#21487;&#36798;&#21040;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#27979;&#35797;&#24615;&#33021;&#65292;&#26174;&#33879;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#33021;&#21147;&#24182;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.19243</link><description>&lt;p&gt;
Auto-tune: &#31070;&#32463;&#32593;&#32476;&#30340;&#20808;&#39564;&#19982;&#21518;&#39564;PAC-Bayes&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Auto-tune: PAC-Bayes Optimization over Prior and Posterior for Neural Networks. (arXiv:2305.19243v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19243
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;PAC-Bayes&#35757;&#32451;&#26694;&#26550;&#65292;&#26080;&#38656;&#39069;&#22806;&#27491;&#21017;&#21270;&#21644;&#32593;&#26684;&#25628;&#32034;&#35843;&#25972;&#36229;&#21442;&#25968;&#21363;&#21487;&#36798;&#21040;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#27979;&#35797;&#24615;&#33021;&#65292;&#26174;&#33879;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#33021;&#21147;&#24182;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#35757;&#32451;&#36807;&#31243;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#25110;Adam&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#21450;&#39069;&#22806;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22914;&#26435;&#37325;&#34928;&#20943;&#12289;Dropout&#25110;&#22122;&#22768;&#27880;&#20837;&#12290;&#36890;&#36807;&#32593;&#26684;&#25628;&#32034;&#35843;&#25972;&#25968;&#37327;&#20247;&#22810;&#30340;&#36229;&#21442;&#25968;&#25165;&#33021;&#36798;&#21040;&#26368;&#20248;&#27867;&#21270;&#65292;&#36825;&#21487;&#33021;&#32791;&#26102;&#65292;&#24182;&#38656;&#35201;&#39069;&#22806;&#30340;&#39564;&#35777;&#25968;&#25454;&#38598;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20999;&#23454;&#21487;&#34892;&#30340;PAC-Bayes&#35757;&#32451;&#26694;&#26550;&#65292;&#20960;&#20046;&#26159;&#26080;&#38656;&#35843;&#25972;&#65292;&#20063;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#27491;&#21017;&#21270;&#65292;&#32780;&#22312;&#23436;&#25104;&#32593;&#26684;&#25628;&#32034;&#21644;&#21152;&#20837;&#39069;&#22806;&#27491;&#21017;&#21270;&#21518;&#65292;&#36798;&#21040;&#20102;&#19982;SGD/Adam&#21487;&#27604;&#36739;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23637;&#31034;&#20102;PAC&#35757;&#32451;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is widely recognized that the generalization ability of neural networks can be greatly enhanced through carefully designing the training procedure. The current state-of-the-art training approach involves utilizing stochastic gradient descent (SGD) or Adam optimization algorithms along with a combination of additional regularization techniques such as weight decay, dropout, or noise injection. Optimal generalization can only be achieved by tuning a multitude of hyperparameters through grid search, which can be time-consuming and necessitates additional validation datasets. To address this issue, we introduce a practical PAC-Bayes training framework that is nearly tuning-free and requires no additional regularization while achieving comparable testing performance to that of SGD/Adam after a complete grid search and with extra regularizations. Our proposed algorithm demonstrates the remarkable potential of PAC training to achieve state-of-the-art performance on deep neural networks wit
&lt;/p&gt;</description></item><item><title>&#24425;&#34425;&#32593;&#32476;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#36890;&#36807;&#23618;&#20869;&#31070;&#32463;&#20803;&#26435;&#37325;&#20114;&#30456;&#29420;&#31435;&#30340;&#23545;&#40784;&#21644;&#38543;&#26426;&#29305;&#24449;&#26144;&#23556;&#26469;&#36827;&#34892;&#32447;&#24615;&#38477;&#32500;&#21644;&#38750;&#32447;&#24615;&#39640;&#32500;&#23884;&#20837;&#65292;&#22312;ImageNet&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.18512</link><description>&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#40657;&#30418;&#20013;&#30340;&#24425;&#34425;
&lt;/p&gt;
&lt;p&gt;
A Rainbow in Deep Network Black Boxes. (arXiv:2305.18512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18512
&lt;/p&gt;
&lt;p&gt;
&#24425;&#34425;&#32593;&#32476;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#36890;&#36807;&#23618;&#20869;&#31070;&#32463;&#20803;&#26435;&#37325;&#20114;&#30456;&#29420;&#31435;&#30340;&#23545;&#40784;&#21644;&#38543;&#26426;&#29305;&#24449;&#26144;&#23556;&#26469;&#36827;&#34892;&#32447;&#24615;&#38477;&#32500;&#21644;&#38750;&#32447;&#24615;&#39640;&#32500;&#23884;&#20837;&#65292;&#22312;ImageNet&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24425;&#34425;&#32593;&#32476;&#20316;&#20026;&#35757;&#32451;&#22909;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32423;&#32852;&#38543;&#26426;&#29305;&#24449;&#26144;&#23556;&#65292;&#20854;&#26435;&#37325;&#20998;&#24067;&#26159;&#21487;&#20197;&#23398;&#20064;&#30340;&#12290;&#23427;&#20551;&#35774;&#19981;&#21516;&#23618;&#20043;&#38388;&#30340;&#26435;&#37325;&#20381;&#36182;&#24615;&#34987;&#20943;&#23569;&#21040;&#23558;&#36755;&#20837;&#28608;&#27963;&#23545;&#20934;&#30340;&#26059;&#36716;&#12290;&#23618;&#20869;&#30340;&#31070;&#32463;&#20803;&#26435;&#37325;&#22312;&#36825;&#31181;&#23545;&#40784;&#21518;&#26159;&#30456;&#20114;&#29420;&#31435;&#30340;&#12290;&#23427;&#20204;&#30340;&#28608;&#27963;&#23450;&#20041;&#20102;&#22312;&#26080;&#31351;&#23485;&#24230;&#26497;&#38480;&#19979;&#21464;&#24471;&#30830;&#23450;&#30340;&#20869;&#26680;&#12290;&#36825;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;ResNets&#20013;&#36890;&#36807;&#25968;&#23383;&#39564;&#35777;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#23398;&#20064;&#30340;&#26435;&#37325;&#20998;&#24067;&#20855;&#26377;&#20302;&#31209;&#21327;&#26041;&#24046;&#12290;&#22240;&#27492;&#65292;&#24425;&#34425;&#32593;&#32476;&#22312;&#32447;&#24615;&#38477;&#32500;&#21644;&#38750;&#32447;&#24615;&#39640;&#32500;&#23884;&#20837;&#19982;&#30333;&#33394;&#38543;&#26426;&#29305;&#24449;&#20043;&#38388;&#20132;&#26367;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20855;&#26377;&#39640;&#26031;&#26435;&#37325;&#20998;&#24067;&#30340;&#39640;&#26031;&#24425;&#34425;&#32593;&#32476;&#23450;&#20041;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20351;&#29992;&#23567;&#27874;&#25955;&#23556;&#32593;&#32476;&#36827;&#34892;CIFAR-10&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#36827;&#34892;&#20102;&#25968;&#23383;&#39564;&#35777;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;SGD&#26356;&#26032;&#26435;&#37325;&#30340;&#21327;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce rainbow networks as a probabilistic model of trained deep neural networks. The model cascades random feature maps whose weight distributions are learned. It assumes that dependencies between weights at different layers are reduced to rotations which align the input activations. Neuron weights within a layer are independent after this alignment. Their activations define kernels which become deterministic in the infinite-width limit. This is verified numerically for ResNets trained on the ImageNet dataset. We also show that the learned weight distributions have low-rank covariances. Rainbow networks thus alternate between linear dimension reductions and non-linear high-dimensional embeddings with white random features. Gaussian rainbow networks are defined with Gaussian weight distributions. These models are validated numerically on image classification on the CIFAR-10 dataset, with wavelet scattering networks. We further show that during training, SGD updates the weight cov
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#33021;&#21147;&#26159;&#19968;&#31181;&#24230;&#37327;&#27169;&#22411;&#26377;&#25928;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#21028;&#26029;&#26159;&#21542;&#38656;&#35201;&#33719;&#21462;&#26356;&#22810;&#25968;&#25454;&#25110;&#32773;&#23547;&#25214;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.17332</link><description>&lt;p&gt;
&#23398;&#20064;&#33021;&#21147;&#65306;&#27169;&#22411;&#26377;&#25928;&#32500;&#24230;&#30340;&#24230;&#37327;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Learning Capacity: A Measure of the Effective Dimensionality of a Model. (arXiv:2305.17332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17332
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#33021;&#21147;&#26159;&#19968;&#31181;&#24230;&#37327;&#27169;&#22411;&#26377;&#25928;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#21028;&#26029;&#26159;&#21542;&#38656;&#35201;&#33719;&#21462;&#26356;&#22810;&#25968;&#25454;&#25110;&#32773;&#23547;&#25214;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#28909;&#21147;&#23398;&#21644;&#25512;&#29702;&#20043;&#38388;&#30340;&#27491;&#24335;&#23545;&#24212;&#20851;&#31995;&#65292;&#23558;&#26679;&#26412;&#25968;&#37327;&#35270;&#20026;&#21453;&#28201;&#24230;&#65292;&#23450;&#20041;&#20102;&#19968;&#31181;&#8220;&#23398;&#20064;&#33021;&#21147;&#8221;&#65292;&#36825;&#26159;&#27169;&#22411;&#26377;&#25928;&#32500;&#24230;&#30340;&#24230;&#37327;&#26041;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#35768;&#22810;&#22312;&#20856;&#22411;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#23398;&#20064;&#33021;&#21147;&#20165;&#21344;&#21442;&#25968;&#25968;&#37327;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#21462;&#20915;&#20110;&#29992;&#20110;&#35757;&#32451;&#30340;&#26679;&#26412;&#25968;&#37327;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#19978;&#19982;&#20174;PAC-Bayesian&#26694;&#26550;&#33719;&#24471;&#30340;&#33021;&#21147;&#27010;&#24565;&#19968;&#33268;&#12290;&#23398;&#20064;&#33021;&#21147;&#20316;&#20026;&#27979;&#35797;&#35823;&#24046;&#30340;&#20989;&#25968;&#19981;&#20250;&#20986;&#29616;&#21452;&#23792;&#19979;&#38477;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#22312;&#38750;&#24120;&#23567;&#21644;&#38750;&#24120;&#22823;&#30340;&#26679;&#26412;&#22823;&#23567;&#22788;&#39281;&#21644;&#65292;&#36825;&#25552;&#20379;&#20102;&#25351;&#23548;&#65292;&#35828;&#26126;&#26159;&#21542;&#24212;&#35813;&#33719;&#21462;&#26356;&#22810;&#25968;&#25454;&#25110;&#32773;&#23547;&#25214;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23398;&#20064;&#33021;&#21147;&#26469;&#29702;&#35299;&#26377;&#25928;&#32500;&#25968;&#65292;&#21363;&#20351;&#26159;&#38750;&#21442;&#25968;&#27169;&#22411;&#65292;&#22914;&#38543;&#26426;&#26862;&#26519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We exploit a formal correspondence between thermodynamics and inference, where the number of samples can be thought of as the inverse temperature, to define a "learning capacity'' which is a measure of the effective dimensionality of a model. We show that the learning capacity is a tiny fraction of the number of parameters for many deep networks trained on typical datasets, depends upon the number of samples used for training, and is numerically consistent with notions of capacity obtained from the PAC-Bayesian framework. The test error as a function of the learning capacity does not exhibit double descent. We show that the learning capacity of a model saturates at very small and very large sample sizes; this provides guidelines, as to whether one should procure more data or whether one should search for new architectures, to improve performance. We show how the learning capacity can be used to understand the effective dimensionality, even for non-parametric models such as random fores
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312; mini-batch &#20013;&#26174;&#24335;&#22320;&#23398;&#20064;&#35823;&#24046;&#30340;&#24207;&#21015;&#30456;&#20851;&#24615;&#65292;&#26469;&#25552;&#39640;&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.17028</link><description>&lt;p&gt;
&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26356;&#22909;Batch&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Better Batch for Deep Probabilistic Time Series Forecasting. (arXiv:2305.17028v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312; mini-batch &#20013;&#26174;&#24335;&#22320;&#23398;&#20064;&#35823;&#24046;&#30340;&#24207;&#21015;&#30456;&#20851;&#24615;&#65292;&#26469;&#25552;&#39640;&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22240;&#20854;&#33021;&#22815;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#36807;&#20110;&#31616;&#21333;&#21270;&#38382;&#39064;&#65292;&#20551;&#35774;&#35823;&#24046;&#36807;&#31243;&#26159;&#19982;&#26102;&#38388;&#26080;&#20851;&#30340;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#35823;&#24046;&#36807;&#31243;&#20013;&#30340;&#24207;&#21015;&#30456;&#20851;&#24615;&#12290;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#20351;&#36825;&#20123;&#27169;&#22411;&#23545;&#20915;&#31574;&#24615;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#20943;&#24369;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#35823;&#24046;&#33258;&#30456;&#20851;&#24615;&#32435;&#20837;&#32771;&#34385;&#65292;&#20197;&#22686;&#24378;&#27010;&#29575;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#36896;&#19968;&#20010;mini-batch&#65292;&#20316;&#20026;$D$&#20010;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#27573;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#26174;&#24335;&#22320;&#23398;&#20064;&#19968;&#20010;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#35206;&#30422;&#20102;&#30456;&#37051;&#26102;&#38388;&#27493;&#20043;&#38388;&#30340;&#35823;&#24046;&#30456;&#20851;&#24615;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#21487;&#29992;&#20110;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#22686;&#24378;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep probabilistic time series forecasting has gained significant attention due to its ability to provide valuable uncertainty quantification for decision-making tasks. However, many existing models oversimplify the problem by assuming the error process is time-independent, thereby overlooking the serial correlation in the error process. This oversight can potentially diminish the accuracy of the forecasts, rendering these models less effective for decision-making purposes. To overcome this limitation, we propose an innovative training method that incorporates error autocorrelation to enhance the accuracy of probabilistic forecasting. Our method involves constructing a mini-batch as a collection of $D$ consecutive time series segments for model training and explicitly learning a covariance matrix over each mini-batch that encodes the error correlation among adjacent time steps. The resulting covariance matrix can be used to improve prediction accuracy and enhance uncertainty quantifica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#20998;&#24067;&#22806;&#26816;&#27979;&#23545;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#21487;&#20197;&#20998;&#35299;&#25104;&#20219;&#21153;&#20869;&#39044;&#27979;&#21644;&#20219;&#21153; ID &#39044;&#27979;&#65292;&#24182;&#19988;&#20219;&#21153; ID &#39044;&#27979;&#19982;&#20998;&#24067;&#22806;&#26816;&#27979;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2304.10038</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#25345;&#32493;&#23398;&#20064;&#65306;&#32479;&#19968;&#26032;&#39062;&#24615;&#26816;&#27979;&#19982;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Open-World Continual Learning: Unifying Novelty Detection and Continual Learning. (arXiv:2304.10038v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#20998;&#24067;&#22806;&#26816;&#27979;&#23545;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#21487;&#20197;&#20998;&#35299;&#25104;&#20219;&#21153;&#20869;&#39044;&#27979;&#21644;&#20219;&#21153; ID &#39044;&#27979;&#65292;&#24182;&#19988;&#20219;&#21153; ID &#39044;&#27979;&#19982;&#20998;&#24067;&#22806;&#26816;&#27979;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528; AI agent &#22312;&#26410;&#30693;&#25110;&#26032;&#22855;&#30340;&#30495;&#23454;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#23427;&#20204;&#38656;&#35201;&#20855;&#22791; (1) &#35748;&#35782;&#24050;&#32463;&#23398;&#20064;&#36807;&#30340;&#29289;&#20307;&#21644;&#26816;&#27979;&#21040;&#20043;&#21069;&#26410;&#35265;&#25110;&#23398;&#20064;&#30340;&#29289;&#20307;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450; (2) &#22686;&#37327;&#22320;&#23398;&#20064;&#26032;&#29289;&#21697;&#65292;&#36880;&#28176;&#21464;&#24471;&#26356;&#26377;&#30693;&#35782;&#21644;&#26356;&#24378;&#22823;&#12290; (1) &#31216;&#20026;&#26032;&#39062;&#24615;&#26816;&#27979;&#25110;&#20998;&#24067;&#22806; (OOD) &#26816;&#27979;&#65292;&#32780; (2) &#31216;&#20026;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064; (CIL)&#65292;&#26159;&#25345;&#32493;&#23398;&#20064; (CL) &#30340;&#19968;&#31181;&#35774;&#32622;&#12290;&#22312;&#29616;&#26377;&#30340;&#30740;&#31350;&#20013;&#65292;OOD &#26816;&#27979;&#21644; CIL &#34987;&#35270;&#20026;&#20004;&#20010;&#23436;&#20840;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102; OOD &#26816;&#27979;&#23454;&#38469;&#19978;&#23545;&#20110; CIL &#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034; CIL &#21487;&#20197;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65306;&#20219;&#21153;&#20869;&#39044;&#27979; (WP) &#21644;&#20219;&#21153; ID &#39044;&#27979;(TP)&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#20102; TP &#19982; OOD &#26816;&#27979;&#30456;&#20851;&#12290;&#20851;&#38190;&#30340;&#29702;&#35770;&#32467;&#26524;&#26159;&#65292;&#26080;&#35770; WP &#21644; OOD &#26816;&#27979;&#65288;&#25110; TP&#65289;&#26159;&#21542;&#30001; CIL &#31639;&#27861;&#26174;&#24335;&#25110;&#38544;&#24335;&#22320;&#23450;&#20041;&#65292;&#22909;&#30340; WP &#21644;&#33391;&#22909;&#30340; OOD &#26816;&#27979;&#25110; TP &#24635;&#26159;&#23384;&#22312;&#23884;&#20837;&#22312;&#20219;&#20309; CIL &#31639;&#27861;&#20013;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI agents are increasingly used in the real open world with unknowns or novelties, they need the ability to (1) recognize objects that (i) they have learned and (ii) detect items that they have not seen or learned before, and (2) learn the new items incrementally to become more and more knowledgeable and powerful. (1) is called novelty detection or out-of-distribution (OOD) detection and (2) is called class incremental learning (CIL), which is a setting of continual learning (CL). In existing research, OOD detection and CIL are regarded as two completely different problems. This paper theoretically proves that OOD detection actually is necessary for CIL. We first show that CIL can be decomposed into two sub-problems: within-task prediction (WP) and task-id prediction (TP). We then prove that TP is correlated with OOD detection. The key theoretical result is that regardless of whether WP and OOD detection (or TP) are defined explicitly or implicitly by a CIL algorithm, good WP and go
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25299;&#23637;&#20102;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;(PINN) &#26469;&#35299;&#20915;&#27714;&#35299;&#38556;&#30861;&#29289;&#30456;&#20851;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#38656;&#35201;&#35299;&#20915;&#25968;&#20540;&#26041;&#27861;&#30340;&#38590;&#24230;&#36739;&#22823;&#65292;&#20294;&#20316;&#32773;&#36890;&#36807;&#23545;&#22810;&#31181;&#24773;&#20917;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;PINN&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03552</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#27714;&#35299;&#38556;&#30861;&#29289;&#30456;&#20851;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
A physics-informed neural network framework for modeling obstacle-related equations. (arXiv:2304.03552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25299;&#23637;&#20102;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;(PINN) &#26469;&#35299;&#20915;&#27714;&#35299;&#38556;&#30861;&#29289;&#30456;&#20851;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#38656;&#35201;&#35299;&#20915;&#25968;&#20540;&#26041;&#27861;&#30340;&#38590;&#24230;&#36739;&#22823;&#65292;&#20294;&#20316;&#32773;&#36890;&#36807;&#23545;&#22810;&#31181;&#24773;&#20917;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;PINN&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#19968;&#20123;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24456;&#22823;&#25104;&#21151;&#65292;&#20294;&#23558;&#20854;&#29992;&#20110;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#12288;&#30340;&#30740;&#31350;&#21017;&#26159;&#36817;&#24180;&#26469;&#30340;&#28909;&#28857;&#65292;&#23588;&#20854;&#22312;&#30446;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#24211;&#65288;&#22914;TensorFlow&#25110;PyTorch&#65289;&#30340;&#25903;&#25345;&#19979;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#21487;&#36890;&#36807;&#35299;&#26512;&#31232;&#30095;&#19988;&#22122;&#22768;&#25968;&#25454;&#26469;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#23558;&#25299;&#23637;PINN&#26469;&#27714;&#35299;&#38556;&#30861;&#29289;&#30456;&#20851;PDE&#65292;&#36825;&#31867;&#26041;&#31243;&#38590;&#24230;&#36739;&#22823;&#65292;&#38656;&#35201;&#21487;&#20197;&#24471;&#21040;&#20934;&#30830;&#35299;&#30340;&#25968;&#20540;&#26041;&#27861;&#12290;&#20316;&#32773;&#22312;&#27491;&#24120;&#21644;&#19981;&#35268;&#21017;&#30340;&#38556;&#30861;&#24773;&#20917;&#19979;&#65292;&#23545;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;PDE&#30340;&#22810;&#20010;&#22330;&#26223;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;PINNs&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been highly successful in some applications. Nevertheless, its use for solving partial differential equations (PDEs) has only been of recent interest with current state-of-the-art machine learning libraries, e.g., TensorFlow or PyTorch. Physics-informed neural networks (PINNs) are an attractive tool for solving partial differential equations based on sparse and noisy data. Here extend PINNs to solve obstacle-related PDEs which present a great computational challenge because they necessitate numerical methods that can yield an accurate approximation of the solution that lies above a given obstacle. The performance of the proposed PINNs is demonstrated in multiple scenarios for linear and nonlinear PDEs subject to regular and irregular obstacles.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21483;&#20570;Robustmix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#32593;&#32476;&#20197;&#20302;&#39057;&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#26469;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;Imagenet-C&#21644;Stylized Imagenet&#31561;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#29366;&#24577;&#24179;&#22343;&#23792;&#20540;&#35823;&#24046;&#65288;mCE&#65289;&#65292;&#22312;&#36991;&#20813;&#35745;&#31639;&#24320;&#38144;&#21644;&#20808;&#39564;&#30693;&#35782;&#30340;&#22823;&#37327;&#22270;&#20687;&#21464;&#25442;&#30340;&#21516;&#26102;&#23545;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#26032;&#36827;&#23637;&#25552;&#20379;&#20102;&#34917;&#20805;&#12290;</title><link>http://arxiv.org/abs/2304.02847</link><description>&lt;p&gt;
Robustmix&#65306;&#36890;&#36807;&#27491;&#21017;&#21270;&#28145;&#24230;&#32593;&#32476;&#30340;&#39057;&#29575;&#20559;&#24046;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustmix: Improving Robustness by Regularizing the Frequency Bias of Deep Nets. (arXiv:2304.02847v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21483;&#20570;Robustmix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#32593;&#32476;&#20197;&#20302;&#39057;&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#26469;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;Imagenet-C&#21644;Stylized Imagenet&#31561;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#29366;&#24577;&#24179;&#22343;&#23792;&#20540;&#35823;&#24046;&#65288;mCE&#65289;&#65292;&#22312;&#36991;&#20813;&#35745;&#31639;&#24320;&#38144;&#21644;&#20808;&#39564;&#30693;&#35782;&#30340;&#22823;&#37327;&#22270;&#20687;&#21464;&#25442;&#30340;&#21516;&#26102;&#23545;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#26032;&#36827;&#23637;&#25552;&#20379;&#20102;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#22312;&#19968;&#31995;&#21015;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23545;&#20110;&#23545;&#20154;&#31867;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#30340;&#25200;&#21160;&#20173;&#28982;&#24456;&#25935;&#24863;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Robustmix&#30340;Mixup&#26032;&#25193;&#23637;&#65292;&#35813;&#25193;&#23637;&#36890;&#36807;&#27491;&#21017;&#21270;&#32593;&#32476;&#20197;&#22522;&#20110;&#20302;&#39057;&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#27491;&#21017;&#21270;&#25913;&#21892;&#20102;&#22312;&#19968;&#31995;&#21015;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#20363;&#22914;Imagenet-C&#21644;Stylized Imagenet&#12290;&#23427;&#20960;&#20046;&#27809;&#26377;&#35745;&#31639;&#24320;&#38144;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#22823;&#37327;&#22270;&#20687;&#21464;&#25442;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#36827;&#19968;&#27493;&#34917;&#20805;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20351;&#29992;EfficientNet-B8&#27169;&#22411;&#21644;RandAugment&#36798;&#21040;&#20102;44.8&#30340;&#26368;&#26032;&#29366;&#24577;&#24179;&#22343;&#23792;&#20540;&#35823;&#24046;&#65288;mCE&#65289;&#65292;&#30456;&#27604;&#22522;&#32447;&#38477;&#20302;&#20102;16&#20010;mCE&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep networks have achieved impressive results on a range of well-curated benchmark datasets. Surprisingly, their performance remains sensitive to perturbations that have little effect on human performance. In this work, we propose a novel extension of Mixup called Robustmix that regularizes networks to classify based on lower-frequency spatial features. We show that this type of regularization improves robustness on a range of benchmarks such as Imagenet-C and Stylized Imagenet. It adds little computational overhead and, furthermore, does not require a priori knowledge of a large set of image transformations. We find that this approach further complements recent advances in model architecture and data augmentation, attaining a state-of-the-art mCE of 44.8 with an EfficientNet-B8 model and RandAugment, which is a reduction of 16 mCE compared to the baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;ODE-Net&#22312;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#32422;&#26463;&#21442;&#25968;ODE&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#24230;&#35770;&#22343;&#22330;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#34920;&#36848;&#65292;&#24182;&#38024;&#23545;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#22120;&#30340;&#23384;&#22312;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.05924</link><description>&lt;p&gt;
ODE-Net&#30340;&#21464;&#20998;&#24418;&#24335;&#65306;&#22343;&#22330;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#21450;&#23384;&#22312;&#24615;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Variational formulations of ODE-Net as a mean-field optimal control problem and existence results. (arXiv:2303.05924v2 [math.AP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;ODE-Net&#22312;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#32422;&#26463;&#21442;&#25968;ODE&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#24230;&#35770;&#22343;&#22330;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#34920;&#36848;&#65292;&#24182;&#38024;&#23545;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#22120;&#30340;&#23384;&#22312;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ODE-Net&#36827;&#34892;&#20102;&#25968;&#23398;&#20998;&#26512;&#65292;&#23427;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#36830;&#32493;&#27169;&#22411;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#29992;ODE&#20195;&#26367;DNN&#28145;&#24230;&#32467;&#26500;&#20316;&#20026;&#36830;&#32493;&#26497;&#38480;&#30340;&#24819;&#27861;&#12290;&#36825;&#20123;&#30740;&#31350;&#23558;ODE-Net&#30340;"&#23398;&#20064;"&#35270;&#20026;&#26368;&#23567;&#21270;&#30001;&#21442;&#25968;ODE&#32422;&#26463;&#30340;"&#25439;&#22833;"&#12290;&#34429;&#28982;&#38656;&#35201;&#20551;&#23450;&#35813;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#23384;&#22312;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#35814;&#32454;&#22320;&#20998;&#26512;&#20102;&#20854;&#23384;&#22312;&#24615;&#12290;&#26412;&#25991;&#23558;ODE-Net&#30340;&#24418;&#24335;&#21270;&#34920;&#36848;&#20026;&#19968;&#31181;&#27979;&#24230;&#35770;&#22343;&#22330;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#27492;&#35752;&#35770;&#20102;&#26368;&#23567;&#21270;&#22120;&#30340;&#23384;&#22312;&#24615;&#32467;&#26524;&#12290;&#24403;&#25551;&#36848;ODE-Net&#21521;&#37327;&#22330;&#30340;&#31070;&#32463;&#32593;&#32476;&#38024;&#23545;&#21487;&#23398;&#20064;&#21442;&#25968;&#26159;&#32447;&#24615;&#30340;&#26102;&#65292;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#22120;&#30340;&#23384;&#22312;&#24615;&#12290;&#35777;&#26126;&#20351;&#29992;&#27979;&#24230;&#35770;&#24418;&#24335;&#21270;&#21644;&#21464;&#20998;&#27861;&#30340;&#30452;&#25509;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#20854;&#27425;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#24819;&#21270;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20197;&#28040;&#38500;&#38024;&#23545;&#22522;&#20110;&#36793;&#30028;&#26465;&#20214;&#30340;"&#24658;&#31561;"ODE&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#19968;&#20123;&#25216;&#26415;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a mathematical analysis of ODE-Net, a continuum model of deep neural networks (DNNs). In recent years, Machine Learning researchers have introduced ideas of replacing the deep structure of DNNs with ODEs as a continuum limit. These studies regard the "learning" of ODE-Net as the minimization of a "loss" constrained by a parametric ODE. Although the existence of a minimizer for this minimization problem needs to be assumed, only a few studies have investigated its existence analytically in detail. In the present paper, the existence of a minimizer is discussed based on a formulation of ODE-Net as a measure-theoretic mean-field optimal control problem. The existence result is proved when a neural network, which describes a vector field of ODE-Net, is linear with respect to learnable parameters. The proof employs the measure-theoretic formulation combined with the direct method of Calculus of Variations. Secondly, an idealized minimization problem is proposed to remove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26080;&#38480;&#32500;&#24230;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#27700;&#24179;&#19978;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#32423;&#25193;&#25955;&#31639;&#27861;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#39640;&#25928;&#22320;&#23398;&#20064;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30456;&#21516;&#25110;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#20135;&#29983;&#27604;&#20256;&#32479;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#24182;&#22788;&#29702;&#30697;&#24418;&#22495;&#12290;</title><link>http://arxiv.org/abs/2303.04772</link><description>&lt;p&gt;
&#22810;&#32423;&#25193;&#25955;&#65306;&#22270;&#20687;&#29983;&#25104;&#30340;&#26080;&#38480;&#32500;&#24230;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation. (arXiv:2303.04772v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26080;&#38480;&#32500;&#24230;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#27700;&#24179;&#19978;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#32423;&#25193;&#25955;&#31639;&#27861;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#39640;&#25928;&#22320;&#23398;&#20064;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30456;&#21516;&#25110;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#20135;&#29983;&#27604;&#20256;&#32479;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#24182;&#22788;&#29702;&#30697;&#24418;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26159;&#36817;&#24180;&#26469;&#22270;&#20687;&#29983;&#25104;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20043;&#19968;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#36890;&#24120;&#22312;&#26377;&#38480;&#32500;&#24230;&#35774;&#32622;&#20013;&#34920;&#36848;&#65292;&#20854;&#20013;&#22270;&#20687;&#34987;&#35270;&#20026;&#20855;&#26377;&#26377;&#38480;&#23610;&#23544;&#30340;&#24352;&#37327;&#12290;&#26412;&#25991;&#22312;&#26080;&#38480;&#32500;&#24230;&#35774;&#32622;&#20013;&#24320;&#21457;&#20102;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21363;&#25105;&#20204;&#23558;&#35757;&#32451;&#25968;&#25454;&#24314;&#27169;&#20026;&#25903;&#25745;&#22312;&#30697;&#24418;&#22495;&#19978;&#30340;&#20989;&#25968;&#12290;&#38500;&#20102;&#36861;&#27714;&#22312;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#29983;&#25104;&#22270;&#20687;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#21019;&#24314;&#19968;&#20010;&#33391;&#22909;&#23450;&#20041;&#30340;&#26080;&#38480;&#32500;&#24230;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#20415;&#21487;&#20197;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#27700;&#24179;&#19978;&#19968;&#33268;&#22320;&#31163;&#25955;&#21270;&#23427;&#12290;&#25105;&#20204;&#24076;&#26395;&#33719;&#24471;&#33021;&#22815;&#27178;&#36328;&#19981;&#21516;&#20998;&#36776;&#29575;&#32423;&#21035;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#39640;&#35757;&#32451;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20811;&#26381;&#24403;&#21069;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#26080;&#38480;&#32500;&#24230;&#35774;&#32622;&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#21069;&#21521;&#36807;&#31243;&#20197;&#30830;&#20445;&#22312;&#26080;&#38480;&#32500;&#24230;&#35774;&#32622;&#20013;&#28508;&#22312;&#20998;&#24067;&#26159;&#33391;&#22909;&#23450;&#20041;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#25193;&#25955;&#31639;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#39640;&#25928;&#22320;&#23398;&#20064;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22810;&#32423;&#27169;&#22411;&#22312;&#30456;&#21516;&#25110;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#20135;&#29983;&#27604;&#20256;&#32479;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#22320;&#29983;&#25104;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#24182;&#22788;&#29702;&#30697;&#24418;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based diffusion models (SBDM) have recently emerged as state-of-the-art approaches for image generation. Existing SBDMs are typically formulated in a finite-dimensional setting, where images are considered as tensors of a finite size. This papers develops SBDMs in the infinite-dimensional setting, that is, we model the training data as functions supported on a rectangular domain. Besides the quest for generating images at ever higher resolution our primary motivation is to create a well-posed infinite-dimensional learning problem so that we can discretize it consistently on multiple resolution levels. We thereby hope to obtain diffusion models that generalize across different resolution levels and improve the efficiency of the training process. We demonstrate how to overcome two shortcomings of current SBDM approaches in the infinite-dimensional setting. First, we modify the forward process to ensure that the latent distribution is well-defined in the infinite-dimensional setting
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Thompson-CHM&#30340;&#28176;&#36817;&#26368;&#20248;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20984;&#21253;&#25104;&#21592;&#38382;&#39064;&#65292;&#19988;&#23558;&#31639;&#27861;&#25193;&#23637;&#21040;&#20102;&#19968;&#32500;&#21644;&#22810;&#32500;&#29615;&#22659;&#20013;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21253;&#25324;&#20572;&#27490;&#35268;&#21017;&#21644;&#37319;&#26679;&#35268;&#21017;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02033</link><description>&lt;p&gt;
&#19968;&#20010;&#28176;&#36817;&#26368;&#20248;&#30340;&#20984;&#21253;&#25104;&#21592;&#38382;&#39064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Asymptotically Optimal Algorithm for the Convex Hull Membership Problem. (arXiv:2302.02033v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Thompson-CHM&#30340;&#28176;&#36817;&#26368;&#20248;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20984;&#21253;&#25104;&#21592;&#38382;&#39064;&#65292;&#19988;&#23558;&#31639;&#27861;&#25193;&#23637;&#21040;&#20102;&#19968;&#32500;&#21644;&#22810;&#32500;&#29615;&#22659;&#20013;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21253;&#25324;&#20572;&#27490;&#35268;&#21017;&#21644;&#37319;&#26679;&#35268;&#21017;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#20984;&#21253;&#25104;&#21592;&#38382;&#39064;&#30340;&#32431;&#25506;&#32034;&#35774;&#32622;&#19982;&#20984;&#21253;&#22343;&#20540;&#30340;&#26377;&#38480;&#20998;&#24067;&#38598;&#21512;&#20013;&#26377;&#25928;&#20934;&#30830;&#22320;&#30830;&#23450;&#32473;&#23450;&#28857;&#26159;&#21542;&#22312;&#20984;&#21253;&#20013;&#30456;&#20851;&#12290;&#25105;&#20204;&#22312;&#19968;&#32500;&#29615;&#22659;&#20013;&#23436;&#20840;&#21051;&#30011;&#20102;&#20984;&#21253;&#25104;&#21592;&#38382;&#39064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#28176;&#36817;&#26368;&#20248;&#31639;&#27861;&#65292;&#21517;&#20026;Thompson-CHM&#65292;&#20854;&#27169;&#22359;&#21270;&#35774;&#35745;&#21253;&#25324;&#20572;&#27490;&#35268;&#21017;&#21644;&#37319;&#26679;&#35268;&#21017;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#31639;&#27861;&#25193;&#23637;&#21040;&#20102;&#19968;&#20123;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#25991;&#29486;&#20013;&#24191;&#20041;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;Thompson-CHM&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#25193;&#23637;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;&#31639;&#27861;&#30340;&#32463;&#39564;&#34892;&#20026;&#19982;&#25105;&#20204;&#22312;&#23454;&#38469;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#29702;&#35770;&#32467;&#26524;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies the pure-exploration setting for the convex hull membership (CHM) problem where one aims to efficiently and accurately determine if a given point lies in the convex hull of means of a finite set of distributions. We give a complete characterization of the sample complexity of the CHM problem in the one-dimensional setting. We present the first asymptotically optimal algorithm called Thompson-CHM, whose modular design consists of a stopping rule and a sampling rule. In addition, we extend the algorithm to settings that generalize several important problems in the multi-armed bandit literature. Furthermore, we discuss the extension of Thompson-CHM to higher dimensions. Finally, we provide numerical experiments to demonstrate the empirical behavior of the algorithm matches our theoretical results for realistic time horizons.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#26631;&#24230;&#22312;&#32447;&#23398;&#20064;&#30340;&#31561;&#35843;&#33410;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20855;&#26377;&#24555;&#36895;&#12289;&#33258;&#36866;&#24212;&#12289;&#38543;&#26102;&#38543;&#22320;&#21644;&#26080;&#26631;&#24230;&#30340;&#29305;&#28857;&#65292;&#24182;&#21487;&#20197;&#33258;&#21160;&#36866;&#24212;&#36951;&#25022;&#30340;&#36895;&#29575;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22312;&#32447;&#26657;&#27491;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2112.14586</link><description>&lt;p&gt;
&#20351;&#29992;&#20110;&#26080;&#26631;&#24230;&#22312;&#32447;&#23398;&#20064;&#30340;&#31561;&#35843;&#33410;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Isotuning With Applications To Scale-Free Online Learning. (arXiv:2112.14586v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14586
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#26631;&#24230;&#22312;&#32447;&#23398;&#20064;&#30340;&#31561;&#35843;&#33410;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20855;&#26377;&#24555;&#36895;&#12289;&#33258;&#36866;&#24212;&#12289;&#38543;&#26102;&#38543;&#22320;&#21644;&#26080;&#26631;&#24230;&#30340;&#29305;&#28857;&#65292;&#24182;&#21487;&#20197;&#33258;&#21160;&#36866;&#24212;&#36951;&#25022;&#30340;&#36895;&#29575;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22312;&#32447;&#26657;&#27491;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25193;&#23637;&#21644;&#32467;&#21512;&#20102;&#25991;&#29486;&#20013;&#30340;&#20960;&#31181;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#24555;&#36895;&#12289;&#33258;&#36866;&#24212;&#12289;&#38543;&#26102;&#38543;&#22320;&#21644;&#26080;&#26631;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#26080;&#26631;&#24230;&#30340;&#36951;&#25022;&#30028;&#38480;&#24517;&#39035;&#19982;&#26368;&#22823;&#25439;&#22833;&#25104;&#32447;&#24615;&#20851;&#31995;&#65292;&#19981;&#35770;&#26159;&#23545;&#20110;&#22823;&#25439;&#22833;&#36824;&#26159;&#23545;&#20110;&#38750;&#24120;&#23567;&#30340;&#25439;&#22833;&#12290;&#33258;&#36866;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#34920;&#26126;&#31639;&#27861;&#21487;&#20197;&#21033;&#29992;&#31616;&#21333;&#30340;&#25968;&#25454;&#24182;&#21487;&#33021;&#20855;&#26377;&#24120;&#25968;&#36951;&#25022;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#23613;&#21487;&#33021;&#23569;&#20381;&#36182;&#21442;&#25968;&#30340;&#24555;&#36895;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#24212;&#35813;&#26159;&#38543;&#26102;&#21487;&#29992;&#30340;&#65292;&#22240;&#27492;&#19981;&#20381;&#36182;&#20110;&#26102;&#38388;&#33539;&#22260;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#21644;&#20027;&#35201;&#24037;&#20855;&#26159;&#31561;&#35843;&#33410;&#25216;&#26415;&#65292;&#23427;&#26159;&#24179;&#34913;&#36951;&#25022;&#26435;&#34913;&#30340;&#24605;&#24819;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#24037;&#20855;&#26469;&#36731;&#26494;&#35774;&#35745;&#21644;&#20998;&#26512;&#36825;&#26679;&#30340;&#23398;&#20064;&#36895;&#24230;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#36951;&#25022;&#30340;&#36895;&#29575;&#65288;&#26080;&#35770;&#26159;&#24120;&#25968;&#12289;$O(\log T)$&#12289;$O(\sqrt{T})$&#31561;&#65289;&#65292;&#24182;&#19988;&#22312;&#21516;&#26679;&#30340;&#35266;&#23519;&#37327;&#19978;&#27604;&#22312;&#20107;&#21518;&#36873;&#25321;&#30340;&#26368;&#20248;&#23398;&#20064;&#36895;&#24230;&#39640;&#20986;2&#20493;&#12290;&#31532;&#20108;&#20010;&#24037;&#20855;&#26159;&#22312;&#32447;&#26657;&#27491;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;...
&lt;/p&gt;
&lt;p&gt;
We extend and combine several tools of the literature to design fast, adaptive, anytime and scale-free online learning algorithms. Scale-free regret bounds must scale linearly with the maximum loss, both toward large losses and toward very small losses. Adaptive regret bounds demonstrate that an algorithm can take advantage of easy data and potentially have constant regret. We seek to develop fast algorithms that depend on as few parameters as possible, in particular they should be anytime and thus not depend on the time horizon. Our first and main tool, isotuning, is a generalization of the idea of balancing the trade-off of the regret. We develop a set of tools to design and analyze such learning rates easily and show that they adapts automatically to the rate of the regret (whether constant, $O(\log T)$, $O(\sqrt{T})$, etc.) within a factor 2 of the optimal learning rate in hindsight for the same observed quantities. The second tool is an online correction, which allows us to obtain
&lt;/p&gt;</description></item></channel></rss>