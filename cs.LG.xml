<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#24314;&#31569;&#35774;&#35745;&#20013;&#24320;&#21019;&#20102;&#26032;&#30340;&#26041;&#27861;&#35770;&#33539;&#24335;&#65292;&#26174;&#33879;&#25193;&#23637;&#20102;&#35774;&#35745;&#36807;&#31243;&#30340;&#21019;&#26032;&#28508;&#21147;&#21644;&#25928;&#29575;&#65292;&#36890;&#36807;&#24191;&#27867;&#24212;&#29992;&#29983;&#25104;&#24335;AI&#25216;&#26415;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;2D&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;3D&#27169;&#22411;&#65292;&#24182;&#23457;&#35270;&#20854;&#22312;&#19981;&#21516;&#38454;&#27573;&#30340;&#24433;&#21709;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2404.01335</link><description>&lt;p&gt;
&#24314;&#31569;&#35774;&#35745;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Architectural Design: A Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01335
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#24314;&#31569;&#35774;&#35745;&#20013;&#24320;&#21019;&#20102;&#26032;&#30340;&#26041;&#27861;&#35770;&#33539;&#24335;&#65292;&#26174;&#33879;&#25193;&#23637;&#20102;&#35774;&#35745;&#36807;&#31243;&#30340;&#21019;&#26032;&#28508;&#21147;&#21644;&#25928;&#29575;&#65292;&#36890;&#36807;&#24191;&#27867;&#24212;&#29992;&#29983;&#25104;&#24335;AI&#25216;&#26415;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;2D&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;3D&#27169;&#22411;&#65292;&#24182;&#23457;&#35270;&#20854;&#22312;&#19981;&#21516;&#38454;&#27573;&#30340;&#24433;&#21709;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01335v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25688;&#35201;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#24314;&#31569;&#35774;&#35745;&#20013;&#24320;&#21019;&#20102;&#26032;&#30340;&#26041;&#27861;&#35770;&#33539;&#24335;&#65292;&#26174;&#33879;&#25193;&#23637;&#20102;&#35774;&#35745;&#36807;&#31243;&#30340;&#21019;&#26032;&#28508;&#21147;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#19968;&#36235;&#21183;&#21463;&#30410;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#25991;&#31456;&#20840;&#38754;&#22238;&#39038;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#31361;&#20986;&#20102;&#22312;&#29983;&#25104;2D&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;3D&#27169;&#22411;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23457;&#26597;&#26469;&#33258;2020&#24180;&#30340;&#26368;&#26032;&#25991;&#29486;&#65292;&#26412;&#25991;&#23457;&#35270;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#24314;&#31569;&#35774;&#35745;&#30340;&#19981;&#21516;&#38454;&#27573;&#30340;&#24433;&#21709;&#65292;&#20174;&#29983;&#25104;&#21021;&#22987;&#24314;&#31569;3D&#24418;&#24335;&#21040;&#29983;&#25104;&#26368;&#32456;&#24314;&#31569;&#22270;&#20687;&#12290;&#30740;&#31350;&#22686;&#38271;&#30340;&#26126;&#26174;&#36235;&#21183;&#34920;&#26126;&#24314;&#31569;&#35774;&#35745;&#39046;&#22495;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#20542;&#21521;&#19981;&#26029;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01335v1 Announce Type: cross  Abstract: Generative Artificial Intelligence (AI) has pioneered new methodological paradigms in architectural design, significantly expanding the innovative potential and efficiency of the design process. This paper explores the extensive applications of generative AI technologies in architectural design, a trend that has benefited from the rapid development of deep generative models. This article provides a comprehensive review of the basic principles of generative AI and large-scale models and highlights the applications in the generation of 2D images, videos, and 3D models. In addition, by reviewing the latest literature from 2020, this paper scrutinizes the impact of generative AI technologies at different stages of architectural design, from generating initial architectural 3D forms to producing final architectural imagery. The marked trend of research growth indicates an increasing inclination within the architectural design community towa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22810;&#28304;&#29615;&#22659;&#20013;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#26631;&#20934;K&#25240;&#20132;&#21449;&#39564;&#35777;&#21644;&#30041;&#20986;&#28304;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#26356;&#20840;&#38754;&#21644;&#30495;&#23454;&#30340;&#31934;&#30830;&#24230;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;</title><link>https://arxiv.org/abs/2403.15012</link><description>&lt;p&gt;
&#20020;&#24202;&#26426;&#22120;&#23398;&#20064;&#20013;&#22810;&#28304;&#20132;&#21449;&#39564;&#35777;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Empirical investigation of multi-source cross-validation in clinical machine learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22810;&#28304;&#29615;&#22659;&#20013;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#26631;&#20934;K&#25240;&#20132;&#21449;&#39564;&#35777;&#21644;&#30041;&#20986;&#28304;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#26356;&#20840;&#38754;&#21644;&#30495;&#23454;&#30340;&#31934;&#30830;&#24230;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20020;&#24202;&#39044;&#27979;&#27169;&#22411;&#26159;&#22312;&#26469;&#33258;&#21333;&#19968;&#26469;&#28304;&#65288;&#22914;&#21307;&#38498;&#65289;&#30340;&#24739;&#32773;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#12290;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#21487;&#36890;&#36807;&#37325;&#22797;&#38543;&#26426;&#25286;&#20998;&#25968;&#25454;&#26469;&#20272;&#35745;&#36825;&#20123;&#27169;&#22411;&#22312;&#26469;&#33258;&#21516;&#19968;&#26469;&#28304;&#30340;&#26032;&#24739;&#32773;&#19978;&#30340;&#31934;&#30830;&#24230;&#12290;&#28982;&#32780;&#65292;&#19982;&#37096;&#32626;&#27169;&#22411;&#21040;&#25968;&#25454;&#38598;&#20013;&#26410;&#20195;&#34920;&#30340;&#28304;&#22836;&#65288;&#22914;&#26032;&#21307;&#38498;&#65289;&#33719;&#24471;&#30340;&#31934;&#30830;&#24230;&#30456;&#27604;&#65292;&#36825;&#20123;&#20272;&#35745;&#24448;&#24448;&#36807;&#20110;&#20048;&#35266;&#12290;&#22810;&#28304;&#21307;&#30103;&#25968;&#25454;&#38598;&#30340;&#19981;&#26029;&#22686;&#21152;&#20026;&#36890;&#36807;&#22522;&#20110;&#28304;&#22836;&#30340;&#20132;&#21449;&#39564;&#35777;&#35774;&#35745;&#33719;&#24471;&#26356;&#20840;&#38754;&#21644;&#30495;&#23454;&#30340;&#39044;&#26399;&#31934;&#30830;&#24230;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15012v1 Announce Type: new  Abstract: Traditionally, machine learning-based clinical prediction models have been trained and evaluated on patient data from a single source, such as a hospital. Cross-validation methods can be used to estimate the accuracy of such models on new patients originating from the same source, by repeated random splitting of the data. However, such estimates tend to be highly overoptimistic when compared to accuracy obtained from deploying models to sources not represented in the dataset, such as a new hospital. The increasing availability of multi-source medical datasets provides new opportunities for obtaining more comprehensive and realistic evaluations of expected accuracy through source-level cross-validation designs.   In this study, we present a systematic empirical evaluation of standard K-fold cross-validation and leave-source-out cross-validation methods in a multi-source setting. We consider the task of electrocardiogram based cardiovascul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#23545;&#25239;&#25552;&#31034;&#26694;&#26550;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#26377;&#38480;&#25968;&#25454;&#35843;&#25972;&#36755;&#20837;&#24207;&#21015;&#65292;&#26174;&#33879;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#23545;&#25239;&#24615;&#30456;&#20851;&#30340;&#25991;&#26412;&#30417;&#30563;&#12290;</title><link>https://arxiv.org/abs/2403.14774</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#23569;&#26679;&#26412;&#23545;&#25239;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Adversarial Prompt Learning on Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#23545;&#25239;&#25552;&#31034;&#26694;&#26550;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#26377;&#38480;&#25968;&#25454;&#35843;&#25972;&#36755;&#20837;&#24207;&#21015;&#65292;&#26174;&#33879;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#23545;&#25239;&#24615;&#30456;&#20851;&#30340;&#25991;&#26412;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#24494;&#19981;&#21487;&#35265;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#21463;&#21040;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#36890;&#36807;&#23558;&#23545;&#25239;&#24615;&#35270;&#35273;&#29305;&#24449;&#19982;&#25991;&#26412;&#30417;&#30563;&#23545;&#40784;&#26469;&#23454;&#29616;&#38646;&#26679;&#26412;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#21253;&#25324;&#37325;&#22823;&#36866;&#24212;&#25104;&#26412;&#12289;&#27425;&#20248;&#25991;&#26412;&#30417;&#30563;&#21644;&#26410;&#21463;&#25511;&#21046;&#30340;&#33258;&#28982;&#27867;&#21270;&#33021;&#21147;&#22312;&#20869;&#30340;&#22810;&#20010;&#38382;&#39064;&#65292;&#23427;&#20204;&#20173;&#28982;&#19981;&#23613;&#20154;&#24847;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#23545;&#25239;&#25552;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#26377;&#38480;&#30340;&#25968;&#25454;&#35843;&#25972;&#36755;&#20837;&#24207;&#21015;&#20351;&#24471;&#23545;&#25239;&#40065;&#26834;&#24615;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#23545;&#25239;&#30456;&#20851;&#30340;&#25991;&#26412;&#30417;&#30563;&#65292;&#35813;&#30417;&#30563;&#26159;&#20174;&#23545;&#25239;&#24615;&#31034;&#20363;&#20013;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#65292;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#22810;&#27169;&#24577;&#29305;&#24449;&#19968;&#33268;&#24615;&#24182;&#40723;&#21169;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14774v1 Announce Type: cross  Abstract: The vulnerability of deep neural networks to imperceptible adversarial perturbations has attracted widespread attention. Inspired by the success of vision-language foundation models, previous efforts achieved zero-shot adversarial robustness by aligning adversarial visual features with text supervision. However, in practice, they are still unsatisfactory due to several issues, including heavy adaptation cost, suboptimal text supervision, and uncontrolled natural generalization capacity. In this paper, to address these issues, we propose a few-shot adversarial prompt framework where adapting input sequences with limited data makes significant adversarial robustness improvement. Specifically, we achieve this by providing adversarially correlated text supervision that is end-to-end learned from adversarial examples. We also propose a novel training objective that enhances the consistency of multi-modal features while encourages differenti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#24494;&#36719;&#24320;&#23637;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#65292;&#29992;&#20110;&#33258;&#21160;&#23545;&#27604;&#23398;&#20064;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.12641</link><description>&lt;p&gt;
&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#33258;&#21160;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Automated Contrastive Learning Strategy Search for Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24494;&#36719;&#24320;&#23637;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#65292;&#29992;&#20110;&#33258;&#21160;&#23545;&#27604;&#23398;&#20064;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#24050;&#25104;&#20026;&#26102;&#38388;&#24207;&#21015;&#30340;&#20027;&#35201;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#22823;&#22810;&#25968;&#26041;&#27861;&#20391;&#37325;&#20110;&#36890;&#36807;&#20154;&#31867;&#21551;&#21457;&#24335;&#26041;&#27861;&#25163;&#21160;&#26500;&#24314;&#29305;&#23450;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65288;CLS&#65289;&#20197;&#24212;&#29992;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#24320;&#21457;CLS&#36890;&#24120;&#38656;&#35201;&#23545;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#26377;&#36807;&#22810;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20363;&#22914;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#23545;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#30340;&#19987;&#19994;&#35748;&#30693;&#65292;&#20197;&#21450;&#22823;&#37327;&#30340;&#20154;&#21147;&#21644;&#22823;&#37327;&#23454;&#39564;&#26469;&#30830;&#23450;&#35814;&#32454;&#30340;&#23398;&#20064;&#37197;&#32622;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24494;&#36719;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#23454;&#36341;&#65292;&#35813;&#23454;&#36341;&#33258;&#21160;&#23398;&#20064;&#23545;&#27604;&#23398;&#20064;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#30340;&#34920;&#31034;&#65292;&#21363;&#33258;&#21160;&#21270;&#23545;&#27604;&#23398;&#20064;&#65288;AutoCL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12641v1 Announce Type: new  Abstract: In recent years, Contrastive Learning (CL) has become a predominant representation learning paradigm for time series. Most existing methods in the literature focus on manually building specific Contrastive Learning Strategies (CLS) by human heuristics for certain datasets and tasks. However, manually developing CLS usually require excessive prior knowledge about the datasets and tasks, e.g., professional cognition of the medical time series in healthcare, as well as huge human labor and massive experiments to determine the detailed learning configurations. In this paper, we present an Automated Machine Learning (AutoML) practice at Microsoft, which automatically learns to contrastively learn representations for various time series datasets and tasks, namely Automated Contrastive Learning (AutoCL). We first construct a principled universal search space of size over 3x1012, covering data augmentation, embedding transformation, contrastive 
&lt;/p&gt;</description></item><item><title>CASPER&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#20851;&#31995;&#24863;&#30693;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25554;&#34917;&#38382;&#39064;&#65292;&#36991;&#20813;&#36807;&#24230;&#21033;&#29992;&#38750;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#39640;&#25968;&#25454;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11960</link><description>&lt;p&gt;
CASPER&#65306;&#22240;&#26524;&#20851;&#31995;&#24863;&#30693;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;
&lt;/p&gt;
&lt;p&gt;
CASPER: Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11960
&lt;/p&gt;
&lt;p&gt;
CASPER&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#20851;&#31995;&#24863;&#30693;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25554;&#34917;&#38382;&#39064;&#65292;&#36991;&#20813;&#36807;&#24230;&#21033;&#29992;&#38750;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#39640;&#25968;&#25454;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11960v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25552;&#35201;&#65306;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#26159;&#29702;&#35299;&#20154;&#31867;&#27963;&#21160;&#21450;&#20854;&#24433;&#21709;&#30340;&#22522;&#30784;&#65292;&#36890;&#24120;&#36890;&#36807;&#25918;&#32622;&#22312;&#19981;&#21516;&#20301;&#32622;&#30340;&#30417;&#27979;&#20256;&#24863;&#22120;&#25910;&#38598;&#12290;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#30001;&#20110;&#21508;&#31181;&#25925;&#38556;&#32780;&#23548;&#33268;&#30340;&#32570;&#22833;&#20540;&#65292;&#36825;&#23545;&#25968;&#25454;&#20998;&#26512;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#20102;&#22635;&#34917;&#32570;&#22833;&#20540;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#12290;&#22312;&#24674;&#22797;&#29305;&#23450;&#25968;&#25454;&#28857;&#26102;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20542;&#21521;&#20110;&#32771;&#34385;&#19982;&#35813;&#28857;&#30456;&#20851;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#26080;&#35770;&#23427;&#20204;&#26159;&#21542;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#12290;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#65292;&#21253;&#25324;&#19968;&#20123;&#26410;&#30693;&#28151;&#26434;&#22240;&#32032;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#20363;&#22914;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32972;&#26223;&#22122;&#22768;&#21644;&#26500;&#24314;&#30340;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#38750;&#22240;&#26524;&#24555;&#25463;&#36793;&#12290;&#36825;&#20123;&#28151;&#26434;&#22240;&#32032;&#21487;&#33021;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#24320;&#36767;&#21453;&#21521;&#36335;&#24452;&#65292;&#25442;&#21477;&#35805;&#35828;&#65292;&#23427;&#20204;&#24314;&#31435;&#20102;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#38750;&#22240;&#26524;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11960v1 Announce Type: new  Abstract: Spatiotemporal time series is the foundation of understanding human activities and their impacts, which is usually collected via monitoring sensors placed at different locations. The collected data usually contains missing values due to various failures, which have significant impact on data analysis. To impute the missing values, a lot of methods have been introduced. When recovering a specific data point, most existing methods tend to take into consideration all the information relevant to that point regardless of whether they have a cause-and-effect relationship. During data collection, it is inevitable that some unknown confounders are included, e.g., background noise in time series and non-causal shortcut edges in the constructed sensor network. These confounders could open backdoor paths between the input and output, in other words, they establish non-causal correlations between the input and output. Over-exploiting these non-causa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#20998;&#24067;&#24335;&#26102;&#38388;&#24046;&#20998;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05811</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#26102;&#38388;&#24046;&#20998;&#30340;&#32479;&#35745;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Statistical Efficiency of Distributional Temporal Difference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05811
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#20998;&#24067;&#24335;&#26102;&#38388;&#24046;&#20998;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;(DRL)&#20851;&#27880;&#30340;&#26159;&#36820;&#22238;&#30340;&#23436;&#25972;&#20998;&#24067;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22343;&#20540;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#32463;&#39564;&#25104;&#21151;&#12290;&#39046;&#22495;DRL&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#20043;&#19968;&#26159;&#20998;&#24067;&#24335;&#31574;&#30053;&#35780;&#20272;&#65292;&#28041;&#21450;&#20272;&#35745;&#32473;&#23450;&#31574;&#30053;pi&#30340;&#36820;&#22238;&#20998;&#24067;&#951;^pi&#12290;&#30456;&#24212;&#22320;&#25552;&#20986;&#20102;&#20998;&#24067;&#26102;&#38388;&#24046;&#20998;(TD)&#31639;&#27861;&#65292;&#36825;&#26159;&#32463;&#20856;RL&#25991;&#29486;&#20013;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#30340;&#24310;&#20280;&#12290;&#22312;&#34920;&#26684;&#26696;&#20363;&#20013;&#65292;citet{rowland2018analysis}&#21644;citet{rowland2023analysis}&#20998;&#21035;&#35777;&#26126;&#20102;&#20004;&#20010;&#20998;&#24067;&#24335;TD&#23454;&#20363;&#21363;&#20998;&#31867;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;(CTD)&#21644;&#20998;&#20301;&#25968;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;(QTD)&#30340;&#28176;&#36817;&#25910;&#25947;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#20998;&#24067;&#24335;TD&#30340;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340; dis
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05811v1 Announce Type: cross  Abstract: Distributional reinforcement learning (DRL), which cares about the full distribution of returns instead of just the mean, has achieved empirical success in various domains. One of the core tasks in the field of DRL is distributional policy evaluation, which involves estimating the return distribution $\eta^\pi$ for a given policy $\pi$. A distributional temporal difference (TD) algorithm has been accordingly proposed, which is an extension of the temporal difference algorithm in the classic RL literature. In the tabular case, \citet{rowland2018analysis} and \citet{rowland2023analysis} proved the asymptotic convergence of two instances of distributional TD, namely categorical temporal difference algorithm (CTD) and quantile temporal difference algorithm (QTD), respectively. In this paper, we go a step further and analyze the finite-sample performance of distributional TD. To facilitate theoretical analysis, we propose non-parametric dis
&lt;/p&gt;</description></item><item><title>&#29992;&#21344;&#29992;&#24230;&#27979;&#37327;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#65292;&#36890;&#36807;&#32771;&#34385;&#20195;&#29702;&#19982;&#30495;&#23454;&#22870;&#21169;&#20043;&#38388;&#22823;&#30340;&#29366;&#24577;&#21344;&#29992;&#24230;&#20559;&#24046;&#26469;&#36991;&#20813;&#28508;&#22312;&#30340;&#28798;&#38590;&#21518;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.03185</link><description>&lt;p&gt;
&#29992;&#21344;&#29992;&#24230;&#27979;&#37327;&#27491;&#21017;&#21270;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Preventing Reward Hacking with Occupancy Measure Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03185
&lt;/p&gt;
&lt;p&gt;
&#29992;&#21344;&#29992;&#24230;&#27979;&#37327;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#65292;&#36890;&#36807;&#32771;&#34385;&#20195;&#29702;&#19982;&#30495;&#23454;&#22870;&#21169;&#20043;&#38388;&#22823;&#30340;&#29366;&#24577;&#21344;&#29992;&#24230;&#20559;&#24046;&#26469;&#36991;&#20813;&#28508;&#22312;&#30340;&#28798;&#38590;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#29702;&#26681;&#25454;&#19968;&#20010;&#8220;&#20195;&#29702;&#8221;&#22870;&#21169;&#20989;&#25968;&#65288;&#21487;&#33021;&#26159;&#25163;&#21160;&#25351;&#23450;&#25110;&#23398;&#20064;&#30340;&#65289;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30456;&#23545;&#20110;&#26410;&#30693;&#30340;&#30495;&#23454;&#22870;&#21169;&#21364;&#34920;&#29616;&#31967;&#31957;&#26102;&#65292;&#23601;&#20250;&#21457;&#29983;&#22870;&#21169;&#27450;&#39575;&#12290;&#30001;&#20110;&#30830;&#20445;&#20195;&#29702;&#21644;&#30495;&#23454;&#22870;&#21169;&#20043;&#38388;&#33391;&#22909;&#23545;&#40784;&#26497;&#20026;&#22256;&#38590;&#65292;&#39044;&#38450;&#22870;&#21169;&#27450;&#39575;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20445;&#23432;&#22320;&#20248;&#21270;&#20195;&#29702;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#29305;&#21035;&#20851;&#27880;&#20110;&#36890;&#36807;&#24809;&#32602;&#20182;&#20204;&#30340;&#34892;&#20026;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#24378;&#21046;&#35753;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#34920;&#29616;&#31867;&#20284;&#20110;&#8220;&#23433;&#20840;&#8221;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#34892;&#20026;&#20998;&#24067;&#30340;&#27491;&#21017;&#21270;&#24182;&#19981;&#24635;&#26159;&#26377;&#25928;&#65292;&#22240;&#20026;&#22312;&#21333;&#20010;&#29366;&#24577;&#19979;&#34892;&#20026;&#20998;&#24067;&#30340;&#24494;&#23567;&#21464;&#21270;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#65292;&#32780;&#36739;&#22823;&#30340;&#21464;&#21270;&#21487;&#33021;&#24182;&#19981;&#20195;&#34920;&#20219;&#20309;&#21361;&#38505;&#27963;&#21160;&#12290;&#25105;&#20204;&#30340;&#35265;&#35299;&#26159;&#65292;&#24403;&#22870;&#21169;&#27450;&#39575;&#26102;&#65292;&#20195;&#29702;&#35775;&#38382;&#30340;&#29366;&#24577;&#19982;&#23433;&#20840;&#31574;&#30053;&#36798;&#21040;&#30340;&#29366;&#24577;&#25130;&#28982;&#19981;&#21516;&#65292;&#23548;&#33268;&#29366;&#24577;&#21344;&#29992;&#24230;&#30340;&#24040;&#22823;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03185v1 Announce Type: cross  Abstract: Reward hacking occurs when an agent performs very well with respect to a "proxy" reward function (which may be hand-specified or learned), but poorly with respect to the unknown true reward. Since ensuring good alignment between the proxy and true reward is extremely difficult, one approach to prevent reward hacking is optimizing the proxy conservatively. Prior work has particularly focused on enforcing the learned policy to behave similarly to a "safe" policy by penalizing the KL divergence between their action distributions (AD). However, AD regularization doesn't always work well since a small change in action distribution at a single state can lead to potentially calamitous outcomes, while large changes might not be indicative of any dangerous activity. Our insight is that when reward hacking, the agent visits drastically different states from those reached by the safe policy, causing large deviations in state occupancy measure (OM
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#38750;&#24120;&#39640;&#30340;dropout&#29575;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#65292;&#36825;&#36229;&#20986;&#20102;&#38598;&#25104;&#21644;&#26435;&#37325;&#24179;&#22343;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00946</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#24120;&#22823;&#30340;Dropout&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning with Very Large Dropout
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00946
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#38750;&#24120;&#39640;&#30340;dropout&#29575;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#65292;&#36825;&#36229;&#20986;&#20102;&#38598;&#25104;&#21644;&#26435;&#37325;&#24179;&#22343;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20170;&#22825;&#19981;&#21487;&#33021;&#20551;&#35013;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#19982;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#36981;&#24490;&#30456;&#21516;&#20998;&#24067;&#30340;&#35266;&#24565;&#26159;&#20860;&#23481;&#30340;&#12290;&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#20351;&#29992;&#38750;&#24120;&#39640;&#30340;&#20002;&#24323;&#29575;&#26469;&#33719;&#24471;&#36825;&#31181;&#20016;&#23500;&#34920;&#31034;&#65292;&#23613;&#31649;&#20351;&#29992;&#36825;&#26679;&#30340;&#20002;&#24323;&#29575;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#28145;&#24230;&#32593;&#32476;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20294;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#19981;&#20165;&#26159;&#21487;&#33021;&#30340;&#65292;&#32780;&#19988;&#23454;&#29616;&#20102;&#36229;&#36234;&#38598;&#25104;&#21644;&#26435;&#37325;&#24179;&#22343;&#26041;&#27861;&#30340;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00946v1 Announce Type: new  Abstract: It is impossible today to pretend that the practice of machine learning is compatible with the idea that training and testing data follow the same distribution. Several authors have recently used ensemble techniques to show how scenarios involving multiple data distributions are best served by representations that are both richer than those obtained by regularizing for the best in-distribution performance, and richer than those obtained under the influence of the implicit sparsity bias of common stochastic gradient procedures.   This contribution investigates the use of very high dropout rates instead of ensembles to obtain such rich representations. Although training a deep network from scratch using such dropout rates is virtually impossible, fine-tuning a large pre-trained model under such conditions is not only possible but also achieves out-of-distribution performances that exceed those of both ensembles and weight averaging methods
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;SUB-SAMPLE-Q&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#23616;&#37096;&#20195;&#29702;&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#22312;&#25351;&#25968;&#32423;&#21035;&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#26368;&#20339;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19982;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#30340;&#25351;&#25968;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2403.00222</link><description>&lt;p&gt;
&#23384;&#22312;&#22823;&#35268;&#27169;&#23616;&#37096;&#20195;&#29702;&#30340;&#20840;&#23616;&#20915;&#31574;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Reinforcement Learning for Global Decision Making in the Presence of Local Agents at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00222
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;SUB-SAMPLE-Q&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#23616;&#37096;&#20195;&#29702;&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#22312;&#25351;&#25968;&#32423;&#21035;&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#26368;&#20339;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19982;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#30340;&#25351;&#25968;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23384;&#22312;&#35768;&#22810;&#23616;&#37096;&#20195;&#29702;&#30340;&#20840;&#23616;&#20915;&#31574;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20840;&#23616;&#20915;&#31574;&#32773;&#20570;&#20986;&#24433;&#21709;&#25152;&#26377;&#23616;&#37096;&#20195;&#29702;&#30340;&#20915;&#31574;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#26368;&#22823;&#21270;&#20840;&#23616;&#21644;&#23616;&#37096;&#20195;&#29702;&#22870;&#21169;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#25193;&#23637;&#24615;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#29366;&#24577;/&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#21487;&#33021;&#20250;&#38543;&#20195;&#29702;&#25968;&#37327;&#25351;&#25968;&#22686;&#38271;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SUB-SAMPLE-Q&#31639;&#27861;&#65292;&#22312;&#27492;&#31639;&#27861;&#20013;&#65292;&#20840;&#23616;&#20195;&#29702;&#23545;$k\leq n$&#20010;&#23616;&#37096;&#20195;&#29702;&#36827;&#34892;&#23376;&#37319;&#26679;&#20197;&#22312;&#20165;&#25351;&#25968;&#20110;$k$&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#26368;&#20339;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19982;&#25351;&#25968;&#20110;$n$&#30340;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#30340;&#25351;&#25968;&#21152;&#36895;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#30528;&#23376;&#37319;&#26679;&#20195;&#29702;&#25968;$k$&#30340;&#22686;&#21152;&#65292;&#23398;&#21040;&#30340;&#31574;&#30053;&#23558;&#25910;&#25947;&#20110;&#39034;&#24207;&#20026;$\tilde{O}(1/\sqrt{k}+\epsilon_{k,m})$&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00222v1 Announce Type: new  Abstract: We study reinforcement learning for global decision-making in the presence of many local agents, where the global decision-maker makes decisions affecting all local agents, and the objective is to learn a policy that maximizes the rewards of both the global and the local agents. Such problems find many applications, e.g. demand response, EV charging, queueing, etc. In this setting, scalability has been a long-standing challenge due to the size of the state/action space which can be exponential in the number of agents. This work proposes the SUB-SAMPLE-Q algorithm where the global agent subsamples $k\leq n$ local agents to compute an optimal policy in time that is only exponential in $k$, providing an exponential speedup from standard methods that are exponential in $n$. We show that the learned policy converges to the optimal policy in the order of $\tilde{O}(1/\sqrt{k}+\epsilon_{k,m})$ as the number of sub-sampled agents $k$ increases, 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;Transformer&#20013;&#27880;&#24847;&#21147;&#22836;&#21644;MLP&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#29305;&#23450;&#19978;&#19979;&#25991;&#19979;&#28608;&#27963;&#29305;&#23450;token&#39044;&#27979;&#30340;&#26426;&#21046;&#65292;&#20174;&#32780;&#38416;&#26126;&#22312;LLMs&#20013;&#27880;&#24847;&#21147;&#22914;&#20309;&#20419;&#25104;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#19987;&#38376;&#21270;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.15055</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#35299;&#37322;&#19978;&#19979;&#25991;&#26597;&#25214;&#65306;&#25506;&#31350;&#27880;&#24847;&#21147;-MLP&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15055
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;Transformer&#20013;&#27880;&#24847;&#21147;&#22836;&#21644;MLP&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#29305;&#23450;&#19978;&#19979;&#25991;&#19979;&#28608;&#27963;&#29305;&#23450;token&#39044;&#27979;&#30340;&#26426;&#21046;&#65292;&#20174;&#32780;&#38416;&#26126;&#22312;LLMs&#20013;&#27880;&#24847;&#21147;&#22914;&#20309;&#20419;&#25104;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#19987;&#38376;&#21270;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#22836;&#21644;Multilayer Perceptron&#20013;&#19987;&#38376;&#39044;&#27979;&#29305;&#23450;token&#30340;"next-token"&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#20419;&#20351;&#20687;GPT-4&#36825;&#26679;&#30340;LLM&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#20869;&#37096;&#65292;&#25105;&#20204;&#21487;&#20197;&#38416;&#26126;&#28608;&#27963;&#26576;&#20123;next-token&#31070;&#32463;&#20803;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#30830;&#23450;&#20102;&#35782;&#21035;&#19982;&#39044;&#27979;&#29305;&#23450;token&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#30340;attention heads&#65292;&#36890;&#36807;&#27531;&#24046;&#36830;&#25509;&#28608;&#27963;&#30456;&#20851;&#32852;&#30340;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#22312;&#36739;&#26089;&#30340;&#23618;&#20013;&#22987;&#32456;&#28608;&#27963;&#30456;&#21516;next-token&#31070;&#32463;&#20803;&#30340;attention heads&#12290;&#25506;&#32034;&#36825;&#20123;&#19981;&#21516;&#30340;&#28608;&#27963;&#27169;&#24335;&#25581;&#31034;&#20102;&#20026;&#19981;&#21516;&#35821;&#35328;&#19978;&#19979;&#25991;&#19987;&#38376;&#21270;&#30340;&#22836;&#19982;&#29983;&#25104;&#26576;&#20123;tokens&#30456;&#20851;&#32852;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#31070;&#32463;&#35299;&#37322;&#21644;&#25506;&#27979;&#23396;&#31435;&#30340;&#32452;&#20214;&#65292;&#20197;&#38416;&#26126;&#27880;&#24847;&#21147;&#22914;&#20309;&#20351;LLMs&#20013;&#30340;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#19987;&#38376;&#22788;&#29702;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15055v1 Announce Type: cross  Abstract: In this paper, we investigate the interplay between attention heads and specialized "next-token" neurons in the Multilayer Perceptron that predict specific tokens. By prompting an LLM like GPT-4 to explain these model internals, we can elucidate attention mechanisms that activate certain next-token neurons. Our analysis identifies attention heads that recognize contexts relevant to predicting a particular token, activating the associated neuron through the residual connection. We focus specifically on heads in earlier layers consistently activating the same next-token neuron across similar prompts. Exploring these differential activation patterns reveals that heads that specialize for distinct linguistic contexts are tied to generating certain tokens. Overall, our method combines neural explanations and probing isolated components to illuminate how attention enables context-dependent, specialized processing in LLMs.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#25915;&#20987;LLMs&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26799;&#24230;&#24341;&#23548;&#21518;&#38376;&#35302;&#21457;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#34920;&#26126;&#25104;&#21151;&#22320;&#30772;&#22351;&#27169;&#22411;&#36755;&#20986;&#65292;&#20165;&#25913;&#21464;1%&#30340;&#25351;&#23548;&#35843;&#20248;&#26679;&#26412;&#21363;&#21487;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#29575;&#36798;&#21040;&#32422;80&#65285;&#12290;</title><link>https://arxiv.org/abs/2402.13459</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#25351;&#23548;&#35843;&#20248;&#26399;&#38388;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning to Poison Large Language Models During Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13459
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#25915;&#20987;LLMs&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26799;&#24230;&#24341;&#23548;&#21518;&#38376;&#35302;&#21457;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#34920;&#26126;&#25104;&#21151;&#22320;&#30772;&#22351;&#27169;&#22411;&#36755;&#20986;&#65292;&#20165;&#25913;&#21464;1%&#30340;&#25351;&#23548;&#35843;&#20248;&#26679;&#26412;&#21363;&#21487;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#29575;&#36798;&#21040;&#32422;80&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#26631;&#24535;&#30528;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#37325;&#22823;&#31361;&#30772;&#12290;&#34429;&#28982;&#23427;&#20204;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;LLMs&#38754;&#20020;&#30528;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#30340;&#28431;&#27934;&#65292;&#20854;&#20013;&#23545;&#25163;&#23558;&#21518;&#38376;&#35302;&#21457;&#22120;&#25554;&#20837;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#25805;&#32437;&#36755;&#20986;&#20197;&#36827;&#34892;&#24694;&#24847;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#65292;&#26088;&#22312;&#21033;&#29992;&#25351;&#23548;&#35843;&#20248;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#35782;&#21035;LLMs&#20013;&#30340;&#39069;&#22806;&#23433;&#20840;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#24341;&#23548;&#21518;&#38376;&#35302;&#21457;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#35782;&#21035;&#25932;&#23545;&#35302;&#21457;&#22120;&#65292;&#30830;&#20445;&#23545;&#20256;&#32479;&#38450;&#24481;&#25163;&#27573;&#30340;&#35268;&#36991;&#65292;&#21516;&#26102;&#20445;&#25345;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;LLMs&#21644;&#20219;&#21153;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#34920;&#26126;&#22312;&#30772;&#22351;&#27169;&#22411;&#36755;&#20986;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#25104;&#21151;&#29575;&#65307;&#20165;&#23545;4,000&#20010;&#25351;&#23548;&#35843;&#20248;&#26679;&#26412;&#20013;&#30340;1&#65285;&#36827;&#34892;&#27880;&#20837;&#23601;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#29575;&#65288;PDR&#65289;&#32422;&#20026;80&#65285;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39640;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13459v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has marked significant achievements in language processing and reasoning capabilities. Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where adversaries insert backdoor triggers into training data to manipulate outputs for malicious purposes. This work further identifies additional security risks in LLMs by designing a new data poisoning attack tailored to exploit the instruction tuning process. We propose a novel gradient-guided backdoor trigger learning approach to identify adversarial triggers efficiently, ensuring an evasion of detection by conventional defenses while maintaining content integrity. Through experimental validation across various LLMs and tasks, our strategy demonstrates a high success rate in compromising model outputs; poisoning only 1\% of 4,000 instruction tuning samples leads to a Performance Drop Rate (PDR) of around 80\%. Our work high
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#25361;&#25112;&#21450;&#23545;&#31574;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.12617</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#65306;&#25361;&#25112;&#19982;&#23545;&#31574;
&lt;/p&gt;
&lt;p&gt;
Generative AI Security: Challenges and Countermeasures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12617
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#25361;&#25112;&#21450;&#23545;&#31574;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12617v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#35768;&#22810;&#34892;&#19994;&#30340;&#19981;&#26029;&#25193;&#23637;&#24341;&#21457;&#20102;&#20154;&#20204;&#30340;&#20852;&#22859;&#21644;&#22686;&#21152;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25152;&#24102;&#26469;&#30340;&#29420;&#29305;&#23433;&#20840;&#25361;&#25112;&#65292;&#24182;&#27010;&#36848;&#20102;&#31649;&#29702;&#36825;&#20123;&#39118;&#38505;&#30340;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12617v1 Announce Type: cross  Abstract: Generative AI's expanding footprint across numerous industries has led to both excitement and increased scrutiny. This paper delves into the unique security challenges posed by Generative AI, and outlines potential research directions for managing these risks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;&#65288;mePS&#65289;&#65292;&#36890;&#36807;&#22312;&#36229;&#22270;&#19978;&#22810;&#20010;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#35299;&#20915;&#20102;&#25237;&#24433;&#27169;&#25311;&#65288;PS&#65289;&#26080;&#27861;&#27169;&#25311;&#21516;&#26102;&#32467;&#21512;&#22810;&#20010;&#27010;&#24565;&#30340;&#24605;&#32500;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10192</link><description>&lt;p&gt;
&#20511;&#37492;&#22810;&#20307;&#29289;&#29702;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Multi-Excitation Projective Simulation with a Many-Body Physics Inspired Inductive Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10192
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;&#65288;mePS&#65289;&#65292;&#36890;&#36807;&#22312;&#36229;&#22270;&#19978;&#22810;&#20010;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#35299;&#20915;&#20102;&#25237;&#24433;&#27169;&#25311;&#65288;PS&#65289;&#26080;&#27861;&#27169;&#25311;&#21516;&#26102;&#32467;&#21512;&#22810;&#20010;&#27010;&#24565;&#30340;&#24605;&#32500;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#20381;&#36182;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#27491;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#19981;&#36879;&#26126;&#30340;&#12289;&#31867;&#20284;&#20110;&#31070;&#35861;&#33324;&#30340;&#29305;&#24615;&#65292;&#20351;&#24471;&#35299;&#37322;&#21644;&#29702;&#35299;&#23427;&#20204;&#30340;&#20915;&#31574;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#20010;&#38382;&#39064;&#23548;&#33268;&#20102;&#34987;&#31216;&#20026;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#35813;&#39046;&#22495;&#20013;&#30340;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#25237;&#24433;&#27169;&#25311;&#65288;PS&#65289;&#65292;&#23558;&#24605;&#32500;&#36807;&#31243;&#24314;&#27169;&#20026;&#19968;&#20010;&#22312;&#20855;&#26377;&#27010;&#24565;&#38468;&#21152;&#30340;&#39030;&#28857;&#30340;&#22270;&#19978;&#30340;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#12290;&#34429;&#28982;&#36825;&#31181;&#25551;&#36848;&#20855;&#26377;&#21508;&#31181;&#22909;&#22788;&#65292;&#21253;&#25324;&#37327;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#19981;&#33021;&#33258;&#28982;&#22320;&#29992;&#26469;&#27169;&#25311;&#21516;&#26102;&#32467;&#21512;&#22810;&#20010;&#27010;&#24565;&#30340;&#24605;&#32500;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#28608;&#21457;&#25237;&#24433;&#27169;&#25311;&#65288;mePS&#65289;&#30340;&#25512;&#24191;&#65292;&#23427;&#23558;&#24605;&#32500;&#36807;&#31243;&#35270;&#20026;&#36229;&#22270;&#19978;&#22810;&#20010;&#31890;&#23376;&#30340;&#38543;&#26426;&#28216;&#36208;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10192v1 Announce Type: cross  Abstract: With the impressive progress of deep learning, applications relying on machine learning are increasingly being integrated into daily life. However, most deep learning models have an opaque, oracle-like nature making it difficult to interpret and understand their decisions. This problem led to the development of the field known as eXplainable Artificial Intelligence (XAI). One method in this field known as Projective Simulation (PS) models a chain-of-thought as a random walk of a particle on a graph with vertices that have concepts attached to them. While this description has various benefits, including the possibility of quantization, it cannot be naturally used to model thoughts that combine several concepts simultaneously. To overcome this limitation, we introduce Multi-Excitation Projective Simulation (mePS), a generalization that considers a chain-of-thought to be a random walk of several particles on a hypergraph. A definition for
&lt;/p&gt;</description></item><item><title>&#22240;&#26524;&#29305;&#24449;&#19981;&#33021;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#39046;&#22495;&#65292;&#39044;&#27979;&#22120;&#20351;&#29992;&#25152;&#26377;&#29305;&#24449;&#30340;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.09891</link><description>&lt;p&gt;
&#39044;&#27979;&#22240;&#26524;&#29305;&#24449;&#19981;&#33021;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Predictors from causal features do not generalize better to new domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09891
&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#29305;&#24449;&#19981;&#33021;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#39046;&#22495;&#65292;&#39044;&#27979;&#22120;&#20351;&#29992;&#25152;&#26377;&#29305;&#24449;&#30340;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#65292;&#22522;&#20110;&#22240;&#26524;&#29305;&#24449;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#28085;&#30422;&#20581;&#24247;&#12289;&#23601;&#19994;&#12289;&#25945;&#32946;&#12289;&#31038;&#20250;&#31119;&#21033;&#21644;&#25919;&#27835;&#31561;&#24212;&#29992;&#30340;16&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#26377;&#22810;&#20010;&#39046;&#22495;&#65292;&#25105;&#20204;&#21487;&#20197;&#27979;&#35797;&#19968;&#20010;&#22312;&#19968;&#20010;&#39046;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21478;&#19968;&#20010;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;&#23545;&#20110;&#27599;&#20010;&#39044;&#27979;&#20219;&#21153;&#65292;&#25105;&#20204;&#36873;&#25321;&#23545;&#39044;&#27979;&#30446;&#26631;&#26377;&#22240;&#26524;&#24433;&#21709;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#27979;&#35797;&#22522;&#20110;&#22240;&#26524;&#29305;&#24449;&#35757;&#32451;&#30340;&#27169;&#22411;&#26159;&#21542;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#26356;&#22909;&#22320;&#27867;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26080;&#35770;&#26159;&#21542;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#65292;&#20351;&#29992;&#25152;&#26377;&#21487;&#29992;&#29305;&#24449;&#30340;&#39044;&#27979;&#22120;&#37117;&#27604;&#20351;&#29992;&#22240;&#26524;&#29305;&#24449;&#30340;&#39044;&#27979;&#22120;&#22312;&#39046;&#22495;&#20869;&#22806;&#30340;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#32780;&#19988;&#65292;&#21363;&#20351;&#26159;&#20174;&#19968;&#20010;&#39046;&#22495;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#30340;&#20934;&#30830;&#24615;&#32477;&#23545;&#19979;&#38477;&#23545;&#20110;&#22240;&#26524;&#39044;&#27979;&#22120;&#26469;&#35828;&#20063;&#19981;&#27604;&#20351;&#29992;&#25152;&#26377;&#29305;&#24449;&#30340;&#27169;&#22411;&#26356;&#22909;&#12290;&#22914;&#26524;&#30446;&#26631;&#26159;&#22312;&#26032;&#39046;&#22495;&#20013;&#27867;&#21270;&#65292;&#23454;&#36341;&#20013;&#20351;&#29992;&#25152;&#26377;&#29305;&#24449;&#30340;&#39044;&#27979;&#22120;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09891v1 Announce Type: new  Abstract: We study how well machine learning models trained on causal features generalize across domains. We consider 16 prediction tasks on tabular datasets covering applications in health, employment, education, social benefits, and politics. Each dataset comes with multiple domains, allowing us to test how well a model trained in one domain performs in another. For each prediction task, we select features that have a causal influence on the target of prediction. Our goal is to test the hypothesis that models trained on causal features generalize better across domains. Without exception, we find that predictors using all available features, regardless of causality, have better in-domain and out-of-domain accuracy than predictors using causal features. Moreover, even the absolute drop in accuracy from one domain to the other is no better for causal predictors than for models that use all features. If the goal is to generalize to new domains, prac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#22312;&#36229;&#31435;&#26041;&#20307;&#25968;&#25454;&#27969;&#19978;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#31934;&#24230;&#30028;&#38480;&#65292;&#20063;&#25512;&#24191;&#20102;&#20043;&#21069;&#20851;&#20110;&#35745;&#25968;&#26597;&#35810;&#30340;&#36830;&#32493;&#21457;&#24067;&#27169;&#22411;&#30340;&#24037;&#20316;&#65292;&#20165;&#38656;&#35201;&#39069;&#22806;&#30340;&#22810;&#39033;&#24335;&#23545;&#25968;&#22240;&#23376;&#12290;</title><link>https://arxiv.org/abs/2402.08012</link><description>&lt;p&gt;
&#22312;&#32447;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Online Differentially Private Synthetic Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#22312;&#36229;&#31435;&#26041;&#20307;&#25968;&#25454;&#27969;&#19978;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#31934;&#24230;&#30028;&#38480;&#65292;&#20063;&#25512;&#24191;&#20102;&#20043;&#21069;&#20851;&#20110;&#35745;&#25968;&#26597;&#35810;&#30340;&#36830;&#32493;&#21457;&#24067;&#27169;&#22411;&#30340;&#24037;&#20316;&#65292;&#20165;&#38656;&#35201;&#39069;&#22806;&#30340;&#22810;&#39033;&#24335;&#23545;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#23545;&#20110;&#22312;&#36229;&#31435;&#26041;&#20307;$[0,1]^d$&#20869;&#30340;&#25968;&#25454;&#27969;&#21644;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#27599;&#20010;&#26102;&#38388;$t$&#37117;&#29983;&#25104;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#35813;&#31639;&#27861;&#22312;1-Wasserstein&#36317;&#31163;&#19978;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#31934;&#24230;&#30028;&#38480;&#65306;&#24403;$d\geq 2$&#26102;&#20026;$O(t^{-1/d}\log(t)$&#65292;&#24403;$d=1$&#26102;&#20026;$O(t^{-1}\log^{4.5}(t)$&#12290;&#36825;&#20010;&#32467;&#26524;&#23558;&#20043;&#21069;&#20851;&#20110;&#35745;&#25968;&#26597;&#35810;&#30340;&#36830;&#32493;&#21457;&#24067;&#27169;&#22411;&#30340;&#24037;&#20316;&#25512;&#24191;&#21040;&#21253;&#25324;Lipschitz&#26597;&#35810;&#12290;&#19982;&#31163;&#32447;&#24773;&#20917;&#19981;&#21516;&#65292;&#31163;&#32447;&#24773;&#20917;&#19979;&#25972;&#20010;&#25968;&#25454;&#38598;&#19968;&#27425;&#24615;&#21487;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#38656;&#35201;&#22312;&#31934;&#24230;&#30028;&#38480;&#20013;&#39069;&#22806;&#30340;&#22810;&#39033;&#24335;&#23545;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a polynomial-time algorithm for online differentially private synthetic data generation. For a data stream within the hypercube $[0,1]^d$ and an infinite time horizon, we develop an online algorithm that generates a differentially private synthetic dataset at each time $t$. This algorithm achieves a near-optimal accuracy bound of $O(t^{-1/d}\log(t))$ for $d\geq 2$ and $O(t^{-1}\log^{4.5}(t))$ for $d=1$ in the 1-Wasserstein distance. This result generalizes the previous work on the continual release model for counting queries to include Lipschitz queries. Compared to the offline case, where the entire dataset is available at once, our approach requires only an extra polylog factor in the accuracy bound.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#26367;&#25442;&#29616;&#26377;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#29305;&#23450;&#32452;&#20214;&#12290;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#22312;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12289;&#24863;&#30693;&#12289;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#31561;&#26041;&#38754;&#25198;&#28436;&#20102;&#37325;&#35201;&#35282;&#33394;&#12290;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#23545;&#23454;&#38469;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#24433;&#21709;&#20063;&#34987;&#35752;&#35770;&#21040;&#12290;</title><link>https://arxiv.org/abs/2402.05741</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#24212;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Real-World Robot Applications of Foundation Models: A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#26367;&#25442;&#29616;&#26377;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#29305;&#23450;&#32452;&#20214;&#12290;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#22312;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#12289;&#24863;&#30693;&#12289;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#31561;&#26041;&#38754;&#25198;&#28436;&#20102;&#37325;&#35201;&#35282;&#33394;&#12290;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#23545;&#23454;&#38469;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#24433;&#21709;&#20063;&#34987;&#35752;&#35770;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#31561;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#20026;&#19981;&#21516;&#20219;&#21153;&#21644;&#27169;&#24577;&#30340;&#28789;&#27963;&#24212;&#29992;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;&#23427;&#20204;&#30340;&#24433;&#21709;&#28085;&#30422;&#20102;&#21253;&#25324;&#21307;&#30103;&#12289;&#25945;&#32946;&#21644;&#26426;&#22120;&#20154;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#65292;&#37325;&#28857;&#26159;&#26367;&#25442;&#29616;&#26377;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#29305;&#23450;&#32452;&#20214;&#12290;&#24635;&#32467;&#28085;&#30422;&#20102;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#30340;&#24863;&#30693;&#12289;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#31561;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#26410;&#26469;&#25361;&#25112;&#21644;&#23545;&#23454;&#38469;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in foundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities. Their impact spans various fields, including healthcare, education, and robotics. This paper provides an overview of the practical application of foundation models in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems. The summary encompasses the perspective of input-output relationships in foundation models, as well as their role in perception, motion planning, and control within the field of robotics. This paper concludes with a discussion of future challenges and implications for practical robot applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#27010;&#29575;&#39564;&#35777;AI&#31995;&#32479;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#24378;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#39564;&#35777;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35768;&#22810;&#26377;&#36259;&#23646;&#24615;&#65292;&#22914;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#25110;&#21333;&#35843;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04892</link><description>&lt;p&gt;
&#36890;&#36807;&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#30340;&#27010;&#29575;&#39564;&#35777;AI&#31995;&#32479;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Probabilistic Verification of AI Systems via Weighted Model Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#27010;&#29575;&#39564;&#35777;AI&#31995;&#32479;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#24378;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#39564;&#35777;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35768;&#22810;&#26377;&#36259;&#23646;&#24615;&#65292;&#22914;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#25110;&#21333;&#35843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#24418;&#24335;&#39564;&#35777;&#65288;PFV) AI&#31995;&#32479;&#36824;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#23545;&#20110;&#29305;&#23450;&#31867;&#21035;&#30340;&#27169;&#22411;&#21644;/&#25110;&#23646;&#24615;&#65292;&#26041;&#27861;&#20165;&#38480;&#20110;&#29305;&#23450;&#30340;&#31639;&#27861;&#32780;&#24050;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#65288;WMI&#65289;&#30340;AI&#31995;&#32479;PFV&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#21487;&#20197;&#38750;&#24120;&#36890;&#29992;&#22320;&#23450;&#20041;&#38382;&#39064;&#12290;&#20851;&#38190;&#26159;&#65292;&#36825;&#31181;&#32422;&#31616;&#21487;&#20197;&#22312;&#19981;&#20570;&#36807;&#24378;&#30340;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#39564;&#35777;&#35768;&#22810;&#26377;&#36259;&#30340;&#23646;&#24615;&#65292;&#22914;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#25110;&#21333;&#35843;&#24615;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#29616;&#25104;&#30340;WMI&#27714;&#35299;&#22120;&#35299;&#20915;&#22810;&#20010;&#39564;&#35777;&#20219;&#21153;&#26469;&#25903;&#25345;&#36825;&#31181;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#65292;&#28982;&#21518;&#35752;&#35770;&#19982;&#36825;&#20010;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#30456;&#20851;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The probabilistic formal verification (PFV) of AI systems is in its infancy. So far, approaches have been limited to ad-hoc algorithms for specific classes of models and/or properties.   We propose a unifying framework for the PFV of AI systems based onWeighted Model Integration (WMI), which allows to frame the problem in very general terms.   Crucially, this reduction enables the verification of many properties of interest, like fairness, robustness or monotonicity, over a wide range of machine learning models, without making strong distributional assumptions.   We support the generality of the approach by solving multiple verification tasks with a single, off-the-shelf WMI solver, then discuss the scalability challenges and research directions related to this promising framework.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#22270;&#31070;&#32463;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#24615;&#28431;&#27934;&#65292;&#36890;&#36807;&#36793;&#37325;&#26500;&#25915;&#20987;&#21487;&#20197;&#25512;&#26029;&#20986;&#25935;&#24863;&#30340;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#25506;&#35752;&#20102;&#22122;&#22768;&#32858;&#21512;&#26426;&#21046;&#20135;&#29983;&#30340;&#38544;&#31169;&#22270;&#34920;&#31034;&#23545;&#35813;&#25915;&#20987;&#30340;&#38887;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04033</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#24418;&#34920;&#31034;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
On provable privacy vulnerabilities of graph representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04033
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#22270;&#31070;&#32463;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#24615;&#28431;&#27934;&#65292;&#36890;&#36807;&#36793;&#37325;&#26500;&#25915;&#20987;&#21487;&#20197;&#25512;&#26029;&#20986;&#25935;&#24863;&#30340;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#25506;&#35752;&#20102;&#22122;&#22768;&#32858;&#21512;&#26426;&#21046;&#20135;&#29983;&#30340;&#38544;&#31169;&#22270;&#34920;&#31034;&#23545;&#35813;&#25915;&#20987;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;(GRL)&#23545;&#20110;&#20174;&#22797;&#26434;&#30340;&#32593;&#32476;&#32467;&#26500;&#20013;&#25552;&#21462;&#27934;&#35265;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#34920;&#31034;&#20013;&#21487;&#33021;&#23384;&#22312;&#38544;&#31169;&#28431;&#27934;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#24615;&#28431;&#27934;&#65292;&#21487;&#20197;&#36890;&#36807;&#36793;&#37325;&#26500;&#25915;&#20987;&#25512;&#26029;&#20986;&#25935;&#24863;&#30340;&#25299;&#25169;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#35299;&#20915;&#20102;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#36793;&#37325;&#26500;&#25915;&#20987;(COSERA)&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;&#35777;&#26126;&#38543;&#30528;&#22270;&#30340;&#35268;&#27169;&#22686;&#21152;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#20197;&#23436;&#32654;&#22320;&#37325;&#26500;&#31232;&#30095;&#30340;Erdos Renyi&#22270;&#19982;&#29420;&#31435;&#38543;&#26426;&#29305;&#24449;&#12290;&#21453;&#20043;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31232;&#30095;&#24615;&#23545;COSERA&#30340;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;&#22122;&#22768;&#32858;&#21512;(NAG)&#26426;&#21046;&#20135;&#29983;&#30340;(&#21487;&#35777;&#26126;&#30340;)&#38544;&#31169;&#22270;&#34920;&#31034;&#23545;COSERA&#25915;&#20987;&#30340;&#38887;&#24615;&#12290;&#25105;&#20204;&#23454;&#35777;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Graph representation learning (GRL) is critical for extracting insights from complex network structures, but it also raises security concerns due to potential privacy vulnerabilities in these representations. This paper investigates the structural vulnerabilities in graph neural models where sensitive topological information can be inferred through edge reconstruction attacks. Our research primarily addresses the theoretical underpinnings of cosine-similarity-based edge reconstruction attacks (COSERA), providing theoretical and empirical evidence that such attacks can perfectly reconstruct sparse Erdos Renyi graphs with independent random features as graph size increases. Conversely, we establish that sparsity is a critical factor for COSERA's effectiveness, as demonstrated through analysis and experiments on stochastic block models. Finally, we explore the resilience of (provably) private graph representations produced via noisy aggregation (NAG) mechanism against COSERA. We empirical
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#23478;&#26063;&#65292;&#31216;&#20026;&#22122;&#22768;&#25193;&#25955;&#20998;&#31867;&#22120;&#65288;NDCs&#65289;&#65292;&#20854;&#20855;&#26377;&#26368;&#26032;&#30340;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;&#25193;&#25955;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#20998;&#31867;&#39640;&#26031;&#21463;&#25439;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#19982;&#38543;&#26426;&#24179;&#28369;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20102;&#20855;&#26377;&#38750;&#24120;&#37327;Lipschitzness&#30340;&#24179;&#28369;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;NDCs&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02316</link><description>&lt;p&gt;
&#20320;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Your Diffusion Model is Secretly a Certifiably Robust Classifier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02316
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#23478;&#26063;&#65292;&#31216;&#20026;&#22122;&#22768;&#25193;&#25955;&#20998;&#31867;&#22120;&#65288;NDCs&#65289;&#65292;&#20854;&#20855;&#26377;&#26368;&#26032;&#30340;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;&#25193;&#25955;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#20998;&#31867;&#39640;&#26031;&#21463;&#25439;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#19982;&#38543;&#26426;&#24179;&#28369;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20102;&#20855;&#26377;&#38750;&#24120;&#37327;Lipschitzness&#30340;&#24179;&#28369;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;NDCs&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#25193;&#25955;&#27169;&#22411;&#34987;&#20316;&#20026;&#40065;&#26834;&#20998;&#31867;&#30340;&#29983;&#25104;&#22120;&#20998;&#31867;&#22120;&#25152;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25193;&#25955;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#32508;&#21512;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#65292;&#36825;&#35753;&#25105;&#20204;&#24576;&#30097;&#23427;&#20204;&#26159;&#21542;&#20250;&#23481;&#26131;&#21463;&#21040;&#26410;&#26469;&#26356;&#24378;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#23478;&#26063;&#65292;&#21629;&#21517;&#20026;&#22122;&#22768;&#25193;&#25955;&#20998;&#31867;&#22120;&#65288;NDCs&#65289;&#65292;&#20854;&#20855;&#26377;&#26368;&#26032;&#30340;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#36825;&#20123;&#20998;&#24067;&#30340;&#35777;&#25454;&#19979;&#30028;&#65288;ELBOs&#65289;&#65292;&#21033;&#29992;ELBO&#36817;&#20284;&#20284;&#28982;&#24230;&#37327;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#23450;&#29702;&#35745;&#31639;&#20998;&#31867;&#27010;&#29575;&#65292;&#23558;&#25193;&#25955;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#20998;&#31867;&#39640;&#26031;&#21463;&#25439;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25512;&#24191;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#19982;&#38543;&#26426;&#24179;&#28369;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20855;&#26377;&#38750;&#24120;&#37327;Lipschitzness&#30340;&#24179;&#28369;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;NDCs&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#35748;&#35777;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#36798;&#21040;80%&#30340;...
&lt;/p&gt;
&lt;p&gt;
Diffusion models are recently employed as generative classifiers for robust classification. However, a comprehensive theoretical understanding of the robustness of diffusion classifiers is still lacking, leading us to question whether they will be vulnerable to future stronger attacks. In this study, we propose a new family of diffusion classifiers, named Noised Diffusion Classifiers~(NDCs), that possess state-of-the-art certified robustness. Specifically, we generalize the diffusion classifiers to classify Gaussian-corrupted data by deriving the evidence lower bounds (ELBOs) for these distributions, approximating the likelihood using the ELBO, and calculating classification probabilities via Bayes' theorem. We integrate these generalized diffusion classifiers with randomized smoothing to construct smoothed classifiers possessing non-constant Lipschitzness. Experimental results demonstrate the superior certified robustness of our proposed NDCs. Notably, we are the first to achieve 80\%
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65288;LFMs&#65289;&#20013;&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#25351;&#20986;&#20102;&#20174;&#26377;&#20559;&#35265;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;LFMs&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;UIM&#26694;&#26550;&#65292;&#26088;&#22312;&#29702;&#35299;LFMs&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#24182;&#35299;&#37322;&#20854;&#20013;&#30340;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.01909</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Catastrophic Inheritance of Large Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01909
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65288;LFMs&#65289;&#20013;&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#25351;&#20986;&#20102;&#20174;&#26377;&#20559;&#35265;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;LFMs&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;UIM&#26694;&#26550;&#65292;&#26088;&#22312;&#29702;&#35299;LFMs&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#24182;&#35299;&#37322;&#20854;&#20013;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65288;LFMs&#65289;&#22768;&#31216;&#20855;&#26377;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#20154;&#20204;&#23545;&#23427;&#20204;&#22312;&#26426;&#22120;&#23398;&#20064;&#20197;&#21450;&#20854;&#20182;&#21508;&#20010;&#23398;&#31185;&#20013;&#30340;&#31070;&#31192;&#21644;&#38590;&#20197;&#35299;&#37322;&#30340;&#28508;&#21147;&#25552;&#20986;&#20102;&#26497;&#22823;&#20851;&#20999;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#65292;&#21363;LFMs&#20013;&#26681;&#28145;&#33922;&#22266;&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#25551;&#36848;&#20102;&#20174;&#26377;&#20559;&#35265;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;LFMs&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#30340;&#24369;&#28857;&#21644;&#38480;&#21046;&#65292;&#21253;&#25324;&#21463;&#25439;&#12289;&#38271;&#23614;&#12289;&#26377;&#22122;&#38899;&#12289;&#36229;&#20986;&#20998;&#24067;&#31561;&#26679;&#26412;&#12290;&#36825;&#31181;&#32487;&#25215;&#21487;&#33021;&#23545;&#19979;&#28216;&#24212;&#29992;&#20135;&#29983;&#28798;&#38590;&#24615;&#24433;&#21709;&#65292;&#22914;&#20559;&#35265;&#12289;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12289;&#24615;&#33021;&#19979;&#38477;&#12289;&#23433;&#20840;&#28431;&#27934;&#12289;&#38544;&#31169;&#27844;&#38706;&#21644;&#20215;&#20540;&#35823;&#24046;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20010;&#38382;&#39064;&#32972;&#21518;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;UIM&#26694;&#26550;&#65292;&#26469;&#29702;&#35299;LFMs&#30340;&#28798;&#38590;&#24615;&#32487;&#25215;&#38382;&#39064;&#65292;&#21253;&#25324;&#26469;&#33258;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#36866;&#24212;&#30340;&#32487;&#25215;&#20869;&#23481;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large foundation models (LFMs) are claiming incredible performances. Yet great concerns have been raised about their mythic and uninterpreted potentials not only in machine learning, but also in various other disciplines. In this position paper, we propose to identify a neglected issue deeply rooted in LFMs: Catastrophic Inheritance, describing the weaknesses and limitations inherited from biased large-scale pre-training data to behaviors of LFMs on the downstream tasks, including samples that are corrupted, long-tailed, noisy, out-of-distributed, to name a few. Such inheritance can potentially cause catastrophes to downstream applications, such as bias, lack of generalization, deteriorated performance, security vulnerability, privacy leakage, and value misalignment. We discuss the challenges behind this issue and propose UIM, a framework to Understand the catastrophic inheritance of LFMs from both pre-training and downstream adaptation, Interpret the implications of catastrophic inher
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#65288;QAS&#65289;&#30340;&#24615;&#33021;&#21487;&#20197;&#24471;&#20197;&#25552;&#21319;&#65292;&#32780;&#19981;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#26631;&#35760;&#12290;</title><link>https://arxiv.org/abs/2401.11576</link><description>&lt;p&gt;
&#21033;&#29992;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Quantum Architecture Search with Unsupervised Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11576
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#65288;QAS&#65289;&#30340;&#24615;&#33021;&#21487;&#20197;&#24471;&#20197;&#25552;&#21319;&#65292;&#32780;&#19981;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#65288;QAS&#65289;&#20195;&#34920;&#20102;&#19968;&#31181;&#21069;&#27839;&#26041;&#27861;&#65292;&#26377;&#26395;&#22312;&#22024;&#26434;&#30340;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#19978;&#23454;&#29616;&#28508;&#22312;&#30340;&#37327;&#23376;&#20248;&#21183;&#12290;&#22823;&#22810;&#25968;QAS&#31639;&#27861;&#23558;&#23427;&#20204;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#25628;&#32034;&#31639;&#27861;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#22240;&#27492;&#36890;&#24120;&#38656;&#35201;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#35780;&#20272;&#22823;&#37327;&#30340;&#37327;&#23376;&#30005;&#36335;&#12290;&#22522;&#20110;&#39044;&#27979;&#30340;QAS&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#26681;&#25454;&#30005;&#36335;&#32467;&#26500;&#20272;&#35745;&#30005;&#36335;&#30340;&#24615;&#33021;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#39640;&#24615;&#33021;&#30340;&#39044;&#27979;&#22120;&#36890;&#24120;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#26631;&#35760;&#65292;&#20197;&#33719;&#24471;&#22823;&#37327;&#24102;&#26631;&#31614;&#30340;&#37327;&#23376;&#30005;&#36335;&#12290;&#26368;&#36817;&#65292;&#19968;&#20010;&#32463;&#20856;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;Arch2vec&#21551;&#21457;&#25105;&#20204;&#65292;&#34920;&#26126;&#26550;&#26500;&#25628;&#32034;&#21487;&#20197;&#20174;&#23558;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#19982;&#25628;&#32034;&#36807;&#31243;&#20998;&#31163;&#20013;&#33719;&#30410;&#12290;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26159;&#21542;&#33021;&#24110;&#21161;QAS
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11576v2 Announce Type: replace-cross  Abstract: Utilizing unsupervised representation learning for quantum architecture search (QAS) represents a cutting-edge approach poised to realize potential quantum advantage on Noisy Intermediate-Scale Quantum (NISQ) devices. Most QAS algorithms combine their search space and search algorithms together and thus generally require evaluating a large number of quantum circuits during the search process. Predictor-based QAS algorithms can alleviate this problem by directly estimating the performance of circuits according to their structures. However, a high-performance predictor generally requires very time-consuming labeling to obtain a large number of labeled quantum circuits. Recently, a classical neural architecture search algorithm Arch2vec inspires us by showing that architecture search can benefit from decoupling unsupervised representation learning from the search process. Whether unsupervised representation learning can help QAS w
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20811;&#26381;&#32852;&#37030;&#23398;&#20064;&#20013;&#36890;&#20449;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;FL&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21516;&#26102;&#20943;&#23569;&#36890;&#20449;&#36127;&#25285;&#12290;</title><link>https://arxiv.org/abs/2210.01708</link><description>&lt;p&gt;
&#20811;&#26381;&#36890;&#20449;&#32422;&#26463;&#65292;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#20013;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conquering the Communication Constraints to Enable Large Pre-Trained Models in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.01708
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20811;&#26381;&#32852;&#37030;&#23398;&#20064;&#20013;&#36890;&#20449;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;FL&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21516;&#26102;&#20943;&#23569;&#36890;&#20449;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26088;&#22312;&#22312;&#26412;&#22320;&#35774;&#22791;&#19978;&#21327;&#21147;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#38656;&#35201;&#23545;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#20013;&#24515;&#21270;&#35775;&#38382;&#30340;&#26377;&#21069;&#26223;&#30340;&#33539;&#24335;&#12290;&#22312;&#20856;&#22411;&#30340;FL&#33539;&#24335;&#65288;&#20363;&#22914;FedAvg&#65289;&#20013;&#65292;&#27599;&#19968;&#36718;&#27169;&#22411;&#26435;&#37325;&#37117;&#20250;&#34987;&#21457;&#36865;&#21040;&#21442;&#19982;&#23458;&#25143;&#31471;&#24182;&#22238;&#20256;&#21040;&#26381;&#21153;&#22120;&#12290;&#26368;&#36817;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20248;&#21270;&#21644;&#25910;&#25947;&#25913;&#36827;&#26041;&#38754;&#23637;&#31034;&#20102;&#20351;&#29992;&#23567;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#39044;&#35757;&#32451;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#24378;&#22823;&#65292;&#20294;&#20063;&#25317;&#26377;&#26356;&#22810;&#21442;&#25968;&#12290;&#22312;&#20256;&#32479;&#30340;FL&#20013;&#65292;&#20849;&#20139;&#24040;&#22823;&#30340;&#27169;&#22411;&#26435;&#37325;&#21487;&#20197;&#36805;&#36895;&#32473;&#31995;&#32479;&#24102;&#26469;&#24040;&#22823;&#30340;&#36890;&#20449;&#36127;&#25285;&#65292;&#23588;&#20854;&#26159;&#22914;&#26524;&#37319;&#29992;&#26356;&#21152;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#33021;&#21542;&#25214;&#21040;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;FL&#20013;&#21551;&#29992;&#36825;&#20123;&#24378;&#22823;&#19988;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23454;&#29616;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;&#36890;&#20449;&#36127;&#25285;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.01708v3 Announce Type: replace  Abstract: Federated learning (FL) has emerged as a promising paradigm for enabling the collaborative training of models without centralized access to the raw data on local devices. In the typical FL paradigm (e.g., FedAvg), model weights are sent to and from the server each round to participating clients. Recently, the use of small pre-trained models has been shown effective in federated learning optimization and improving convergence. However, recent state-of-the-art pre-trained models are getting more capable but also have more parameters. In conventional FL, sharing the enormous model weights can quickly put a massive communication burden on the system, especially if more capable models are employed. Can we find a solution to enable those strong and readily-available pre-trained models in FL to achieve excellent performance while simultaneously reducing the communication burden? To this end, we investigate the use of parameter-efficient fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#26469;&#35299;&#20915;&#29699;&#38754;&#19978;&#30340;&#25955;&#28857;&#25968;&#25454;&#25311;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#21152;&#26435;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#36924;&#36817;&#24615;&#33021;&#65292;&#25104;&#21151;&#25512;&#23548;&#20986;&#20102;&#24102;&#26435;&#37325;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#26368;&#20248;&#35823;&#24046;&#20272;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#19968;&#20123;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.15294</link><description>&lt;p&gt;
&#29699;&#38754;&#19978;&#25955;&#28857;&#25968;&#25454;&#25311;&#21512;&#30340;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integral Operator Approaches for Scattered Data Fitting on Spheres. (arXiv:2401.15294v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#26469;&#35299;&#20915;&#29699;&#38754;&#19978;&#30340;&#25955;&#28857;&#25968;&#25454;&#25311;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#21152;&#26435;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#36924;&#36817;&#24615;&#33021;&#65292;&#25104;&#21151;&#25512;&#23548;&#20986;&#20102;&#24102;&#26435;&#37325;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#26368;&#20248;&#35823;&#24046;&#20272;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#19968;&#20123;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#20102;&#29699;&#38754;&#19978;&#30340;&#25955;&#28857;&#25968;&#25454;&#25311;&#21512;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#21152;&#26435;&#35889;&#28388;&#27874;&#31639;&#27861;&#65288;&#21253;&#25324;Tikhonov&#27491;&#21017;&#21270;&#12289;Landaweber&#36845;&#20195;&#12289;&#35889;&#25130;&#26029;&#21644;&#36845;&#20195;Tikhonov&#65289;&#22312;&#25311;&#21512;&#21487;&#33021;&#23384;&#22312;&#30340;&#26080;&#30028;&#38543;&#26426;&#22122;&#22768;&#30340;&#22024;&#26434;&#25968;&#25454;&#26102;&#30340;&#36924;&#36817;&#24615;&#33021;&#12290;&#20026;&#20102;&#20998;&#26512;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#65292;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#25955;&#28857;&#25968;&#25454;&#25311;&#21512;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#37319;&#26679;&#19981;&#31561;&#24335;&#26041;&#27861;&#21644;&#35268;&#33539;&#38598;&#26041;&#27861;&#30340;&#24310;&#20280;&#12290;&#36890;&#36807;&#25552;&#20379;&#31639;&#23376;&#24046;&#24322;&#21644;&#25968;&#20540;&#31215;&#20998;&#35268;&#21017;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#25512;&#23548;&#20986;&#24102;&#26435;&#37325;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;Sobolev&#31867;&#22411;&#35823;&#24046;&#20272;&#35745;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#35823;&#24046;&#20272;&#35745;&#19981;&#21463;&#25991;&#29486;&#20013;Tikhonov&#27491;&#21017;&#21270;&#30340;&#39281;&#21644;&#29616;&#35937;&#12289;&#29616;&#26377;&#35823;&#24046;&#20998;&#26512;&#20013;&#30340;&#26412;&#22320;&#31354;&#38388;&#23631;&#38556;&#21644;&#19981;&#21516;&#23884;&#20837;&#31354;&#38388;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#32780;&#27835;&#20043;&#30340;&#26041;&#26696;&#65292;&#20197;&#25552;&#21319;&#21152;&#26435;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#25928;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on scattered data fitting problems on spheres. We study the approximation performance of a class of weighted spectral filter algorithms, including Tikhonov regularization, Landaweber iteration, spectral cut-off, and iterated Tikhonov, in fitting noisy data with possibly unbounded random noise. For the analysis, we develop an integral operator approach that can be regarded as an extension of the widely used sampling inequality approach and norming set method in the community of scattered data fitting. After providing an equivalence between the operator differences and quadrature rules, we succeed in deriving optimal Sobolev-type error estimates of weighted spectral filter algorithms. Our derived error estimates do not suffer from the saturation phenomenon for Tikhonov regularization in the literature, native-space-barrier for existing error analysis and adapts to different embedding spaces. We also propose a divide-and-conquer scheme to equip weighted spectral filter 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#26144;&#23556;&#19982;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65292;&#22312;&#20165;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#26102;&#38388;&#27493;&#38271;&#20725;&#30828;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.04372</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#26144;&#23556;&#36827;&#34892;&#31283;&#23450;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Stable generative modeling using diffusion maps. (arXiv:2401.04372v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#26144;&#23556;&#19982;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65292;&#22312;&#20165;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#26102;&#38388;&#27493;&#38271;&#20725;&#30828;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#20165;&#26377;&#36275;&#22815;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#21487;&#24471;&#21040;&#30340;&#26410;&#30693;&#20998;&#24067;&#20013;&#25277;&#26679;&#30340;&#38382;&#39064;&#12290;&#22312;&#29983;&#25104;&#24314;&#27169;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#26679;&#30340;&#35774;&#32622;&#26368;&#36817;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#26144;&#23556;&#21644;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#25193;&#25955;&#26144;&#23556;&#29992;&#20110;&#20174;&#21487;&#29992;&#30340;&#35757;&#32451;&#26679;&#26412;&#20013;&#36817;&#20284;&#24471;&#21040;&#28418;&#31227;&#39033;&#65292;&#28982;&#21518;&#22312;&#31163;&#25955;&#26102;&#38388;&#30340;&#26391;&#20043;&#19975;&#37319;&#26679;&#22120;&#20013;&#23454;&#29616;&#29983;&#25104;&#26032;&#26679;&#26412;&#12290;&#36890;&#36807;&#23558;&#26680;&#24102;&#23485;&#35774;&#32622;&#20026;&#19982;&#26410;&#35843;&#25972;&#30340;&#26391;&#20043;&#19975;&#31639;&#27861;&#20013;&#20351;&#29992;&#30340;&#26102;&#38388;&#27493;&#38271;&#21305;&#37197;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#36991;&#20813;&#36890;&#24120;&#19982;&#26102;&#38388;&#27493;&#38271;&#20725;&#30828;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30456;&#20851;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35010;&#27493;&#39588;&#26041;&#26696;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#26679;&#26412;&#20445;&#25345;&#22312;&#35757;&#32451;&#26679;&#26412;&#30340;&#20984;&#21253;&#20869;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#33258;&#28982;&#22320;&#25193;&#23637;&#20026;&#29983;&#25104;&#26465;&#20214;&#26679;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sampling from an unknown distribution for which only a sufficiently large number of training samples are available. Such settings have recently drawn considerable interest in the context of generative modelling. In this paper, we propose a generative model combining diffusion maps and Langevin dynamics. Diffusion maps are used to approximate the drift term from the available training samples, which is then implemented in a discrete-time Langevin sampler to generate new samples. By setting the kernel bandwidth to match the time step size used in the unadjusted Langevin algorithm, our method effectively circumvents any stability issues typically associated with time-stepping stiff stochastic differential equations. More precisely, we introduce a novel split-step scheme, ensuring that the generated samples remain within the convex hull of the training samples. Our framework can be naturally extended to generate conditional samples. We demonstrate the performance
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#26102;&#38388;&#21464;&#21270;&#22270;&#36793;&#32536;&#20449;&#21495;&#39044;&#27979;&#31639;&#27861;&#65292;&#21033;&#29992;&#32447;&#22270;&#36716;&#25442;&#36793;&#32536;&#20449;&#21495;&#20026;&#36793;&#21040;&#39030;&#28857;&#23545;&#20598;&#33410;&#28857;&#65292;&#20351;&#24471;&#20449;&#21495;&#21487;&#20197;&#20351;&#29992;&#24050;&#26377;&#30340;GSP&#27010;&#24565;&#36827;&#34892;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2311.00656</link><description>&lt;p&gt;
&#36890;&#36807;&#32447;&#22270;&#36716;&#25442;&#22312;&#32447;&#20272;&#35745;&#22270;&#36793;&#32536;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Online Signal Estimation on the Graph Edges via Line Graph Transformation. (arXiv:2311.00656v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00656
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#26102;&#38388;&#21464;&#21270;&#22270;&#36793;&#32536;&#20449;&#21495;&#39044;&#27979;&#31639;&#27861;&#65292;&#21033;&#29992;&#32447;&#22270;&#36716;&#25442;&#36793;&#32536;&#20449;&#21495;&#20026;&#36793;&#21040;&#39030;&#28857;&#23545;&#20598;&#33410;&#28857;&#65292;&#20351;&#24471;&#20449;&#21495;&#21487;&#20197;&#20351;&#29992;&#24050;&#26377;&#30340;GSP&#27010;&#24565;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32447;&#22270;&#24402;&#19968;&#21270;&#26368;&#23567;&#22343;&#26041;(LGNLMS)&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#26102;&#38388;&#21464;&#21270;&#22270;&#36793;&#32536;&#20449;&#21495;&#30340;&#39044;&#27979;&#12290;LGNLMS&#21033;&#29992;&#32447;&#22270;&#23558;&#22270;&#36793;&#32536;&#20449;&#21495;&#36716;&#25442;&#20026;&#20854;&#36793;&#21040;&#39030;&#28857;&#23545;&#20598;&#33410;&#28857;&#12290;&#36825;&#20351;&#24471;&#36793;&#32536;&#20449;&#21495;&#21487;&#20197;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;GSP&#27010;&#24565;&#36827;&#34892;&#22788;&#29702;&#65292;&#32780;&#26080;&#38656;&#22312;&#22270;&#36793;&#32536;&#19978;&#37325;&#26032;&#23450;&#20041;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Line Graph Normalized Least Mean Square (LGNLMS) algorithm for online time-varying graph edge signals prediction. LGNLMS utilizes the Line Graph to transform graph edge signals into the node of its edge-to-vertex dual. This enables edge signals to be processed using established GSP concepts without redefining them on graph edges.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#21518;&#39564;&#37319;&#26679;&#23398;&#20064;&#31639;&#27861;&#22312;&#24207;&#21015;&#21270;POMDPs&#20013;&#30340;&#36951;&#25022;&#24615;&#33021;&#65292;&#24182;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#22810;&#39033;&#24335;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.10107</link><description>&lt;p&gt;
&#21518;&#39564;&#37319;&#26679;&#23398;&#20064;&#31639;&#27861;&#22312;&#24207;&#21015;&#21270;POMDPs&#20013;&#30340;&#36951;&#25022;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Regret Analysis of the Posterior Sampling-based Learning Algorithm for Episodic POMDPs. (arXiv:2310.10107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#21518;&#39564;&#37319;&#26679;&#23398;&#20064;&#31639;&#27861;&#22312;&#24207;&#21015;&#21270;POMDPs&#20013;&#30340;&#36951;&#25022;&#24615;&#33021;&#65292;&#24182;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#22810;&#39033;&#24335;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#27604;&#20110;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#65292;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#30340;&#23398;&#20064;&#30001;&#20110;&#35266;&#23519;&#25968;&#25454;&#38590;&#20197;&#35299;&#35835;&#32780;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#26410;&#30693;&#36716;&#31227;&#21644;&#35266;&#27979;&#27169;&#22411;&#30340;POMDPs&#20013;&#30340;&#24207;&#21015;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#21518;&#39564;&#37319;&#26679;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;PSRL&#65289;&#22312;POMDPs&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#35777;&#26126;&#20854;&#36125;&#21494;&#26031;&#36951;&#25022;&#38543;&#30528;&#24207;&#21015;&#30340;&#25968;&#37327;&#30340;&#24179;&#26041;&#26681;&#32780;&#32553;&#23567;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#36951;&#25022;&#38543;&#30528;&#26102;&#38388;&#38271;&#24230;$H$&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#19979;&#30028;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;POMDP&#26159;&#27424;&#23436;&#22791;&#19988;&#24369;&#21487;&#35782;&#21035;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#65292;&#30456;&#27604;&#20110;arXiv:2204.08967&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#25913;&#36827;&#20102;&#36951;&#25022;&#30028;&#32422;$\Omega(H^2\sqrt{SA})$&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to Markov Decision Processes (MDPs), learning in Partially Observable Markov Decision Processes (POMDPs) can be significantly harder due to the difficulty of interpreting observations. In this paper, we consider episodic learning problems in POMDPs with unknown transition and observation models. We consider the Posterior Sampling-based Reinforcement Learning (PSRL) algorithm for POMDPs and show that its Bayesian regret scales as the square root of the number of episodes. In general, the regret scales exponentially with the horizon length $H$, and we show that this is inevitable by providing a lower bound. However, under the condition that the POMDP is undercomplete and weakly revealing, we establish a polynomial Bayesian regret bound that improves the regret bound by a factor of $\Omega(H^2\sqrt{SA})$ over the recent result by arXiv:2204.08967.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#26041;&#27861;&#65288;XAIG-R&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#21644;&#28151;&#21512;&#26694;&#26550;&#26469;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#22788;&#29702;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2307.07840</link><description>&lt;p&gt;
RegExplainer: &#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#29983;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task. (arXiv:2307.07840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#26041;&#27861;&#65288;XAIG-R&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#21644;&#28151;&#21512;&#26694;&#26550;&#26469;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#22788;&#29702;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22238;&#24402;&#26159;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#65292;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25512;&#29702;&#36807;&#31243;&#36890;&#24120;&#26159;&#19981;&#21487;&#35299;&#37322;&#30340;&#12290;&#29616;&#26377;&#30340;&#35299;&#37322;&#25216;&#26415;&#22823;&#22810;&#38480;&#20110;&#29702;&#35299;&#20998;&#31867;&#20219;&#21153;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23547;&#27714;&#35299;&#37322;&#26469;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65288;XAIG-R&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#20998;&#24067;&#20559;&#31227;&#21644;&#36830;&#32493;&#26377;&#24207;&#30340;&#20915;&#31574;&#36793;&#30028;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#24335;&#25903;&#25345;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#24212;&#23545;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;&#20026;&#20102;&#20174;&#32463;&#39564;&#19978;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph regression is a fundamental task and has received increasing attention in a wide range of graph learning tasks. However, the inference process is often not interpretable. Most existing explanation techniques are limited to understanding GNN behaviors in classification tasks. In this work, we seek an explanation to interpret the graph regression models (XAIG-R). We show that existing methods overlook the distribution shifting and continuously ordered decision boundary, which hinders them away from being applied in the regression tasks. To address these challenges, we propose a novel objective based on the information bottleneck theory and introduce a new mix-up framework, which could support various GNNs in a model-agnostic manner. We further present a contrastive learning strategy to tackle the continuously ordered labels in regression task. To empirically verify the effectiveness of the proposed method, we introduce three benchmark datasets and a real-life dataset for evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#24314;&#31435;&#25928;&#29992;&#29702;&#35770;&#65292;&#26088;&#22312;&#22522;&#20110;&#19968;&#33324;&#24615;&#25351;&#26631;&#23450;&#37327;&#35780;&#20272;&#21512;&#25104;&#31639;&#27861;&#30340;&#25928;&#29992;&#65292;&#25928;&#29992;&#25351;&#26631;&#30340;&#20998;&#26512;&#30028;&#38480;&#25581;&#31034;&#20102;&#25351;&#26631;&#25910;&#25947;&#30340;&#20851;&#38190;&#26465;&#20214;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21482;&#35201;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#35268;&#33539;&#26159;&#27491;&#30830;&#30340;&#65292;&#21512;&#25104;&#29305;&#24449;&#20998;&#24067;&#19981;&#19968;&#23450;&#19982;&#21407;&#22987;&#29305;&#24449;&#20998;&#24067;&#30456;&#21516;&#65292;&#25928;&#29992;&#25351;&#26631;&#20250;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2305.10015</link><description>&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#25928;&#29992;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Utility Theory of Synthetic Data Generation. (arXiv:2305.10015v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#24314;&#31435;&#25928;&#29992;&#29702;&#35770;&#65292;&#26088;&#22312;&#22522;&#20110;&#19968;&#33324;&#24615;&#25351;&#26631;&#23450;&#37327;&#35780;&#20272;&#21512;&#25104;&#31639;&#27861;&#30340;&#25928;&#29992;&#65292;&#25928;&#29992;&#25351;&#26631;&#30340;&#20998;&#26512;&#30028;&#38480;&#25581;&#31034;&#20102;&#25351;&#26631;&#25910;&#25947;&#30340;&#20851;&#38190;&#26465;&#20214;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21482;&#35201;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#35268;&#33539;&#26159;&#27491;&#30830;&#30340;&#65292;&#21512;&#25104;&#29305;&#24449;&#20998;&#24067;&#19981;&#19968;&#23450;&#19982;&#21407;&#22987;&#29305;&#24449;&#20998;&#24067;&#30456;&#21516;&#65292;&#25928;&#29992;&#25351;&#26631;&#20250;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#30340;&#25928;&#29992;&#23545;&#20110;&#34913;&#37327;&#21512;&#25104;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#32467;&#26524;&#20391;&#37325;&#20110;&#23545;&#21512;&#25104;&#25968;&#25454;&#25928;&#29992;&#30340;&#32463;&#39564;&#35780;&#20272;&#65292;&#32780;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#31639;&#27861;&#22914;&#20309;&#24433;&#21709;&#25928;&#29992;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#24314;&#31435;&#25928;&#29992;&#29702;&#35770;&#65292;&#26088;&#22312;&#22522;&#20110;&#19968;&#33324;&#24615;&#25351;&#26631;&#23450;&#37327;&#35780;&#20272;&#21512;&#25104;&#31639;&#27861;&#30340;&#25928;&#29992;&#12290;&#35813;&#25351;&#26631;&#23450;&#20041;&#20026;&#22312;&#21512;&#25104;&#21644;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#27867;&#21270;&#30340;&#32477;&#23545;&#24046;&#24322;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#35813;&#25928;&#29992;&#25351;&#26631;&#30340;&#20998;&#26512;&#30028;&#38480;&#26469;&#30740;&#31350;&#25351;&#26631;&#25910;&#25947;&#30340;&#20851;&#38190;&#26465;&#20214;&#12290;&#19968;&#20010;&#26377;&#36259;&#30340;&#32467;&#26524;&#26159;&#65292;&#21482;&#35201;&#19979;&#28216;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#35268;&#33539;&#26159;&#27491;&#30830;&#30340;&#65292;&#21512;&#25104;&#29305;&#24449;&#20998;&#24067;&#19981;&#19968;&#23450;&#19982;&#21407;&#22987;&#29305;&#24449;&#20998;&#24067;&#30456;&#21516;&#65292;&#21017;&#35813;&#25928;&#29992;&#25351;&#26631;&#20250;&#25910;&#25947;&#12290;&#21478;&#19968;&#20010;&#37325;&#35201;&#30340;&#25928;&#29992;&#25351;&#26631;&#22522;&#20110;&#21512;&#25104;&#21644;&#21407;&#22987;&#25968;&#25454;&#20043;&#38388;&#28508;&#22312;&#30340;&#22240;&#26524;&#26426;&#21046;&#19968;&#33268;&#24615;&#12290;&#35813;&#29702;&#35770;&#20351;&#29992;&#20960;&#31181;&#21512;&#25104;&#31639;&#27861;&#36827;&#34892;&#35828;&#26126;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#25928;&#29992;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the utility of synthetic data is critical for measuring the effectiveness and efficiency of synthetic algorithms. Existing results focus on empirical evaluations of the utility of synthetic data, whereas the theoretical understanding of how utility is affected by synthetic data algorithms remains largely unexplored. This paper establishes utility theory from a statistical perspective, aiming to quantitatively assess the utility of synthetic algorithms based on a general metric. The metric is defined as the absolute difference in generalization between models trained on synthetic and original datasets. We establish analytical bounds for this utility metric to investigate critical conditions for the metric to converge. An intriguing result is that the synthetic feature distribution is not necessarily identical to the original one for the convergence of the utility metric as long as the model specification in downstream learning tasks is correct. Another important utility metri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#36870;&#26144;&#23556;B-KRnet&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25968;&#25454;&#25110;PDE&#30340;&#23494;&#24230;&#20272;&#35745;/&#36817;&#20284;&#65292;&#30001;&#20110;&#20854;&#23450;&#20041;&#22312;&#26377;&#30028;&#22495;&#19978;&#65292;&#22240;&#27492;&#27604;KRnet&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.09063</link><description>&lt;p&gt;
&#26377;&#30028;KRnet&#21450;&#20854;&#22312;&#23494;&#24230;&#20272;&#35745;&#21644;&#36817;&#20284;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bounded KRnet and its applications to density estimation and approximation. (arXiv:2305.09063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#36870;&#26144;&#23556;B-KRnet&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25968;&#25454;&#25110;PDE&#30340;&#23494;&#24230;&#20272;&#35745;/&#36817;&#20284;&#65292;&#30001;&#20110;&#20854;&#23450;&#20041;&#22312;&#26377;&#30028;&#22495;&#19978;&#65292;&#22240;&#27492;&#27604;KRnet&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#26377;&#30028;&#22495;&#19978;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#36870;&#26144;&#23556;&#65292;&#31216;&#20026;B-KRnet&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25968;&#25454;&#25110;PDE&#65288;&#20363;&#22914;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#21644;Keller-Segel&#26041;&#31243;&#65289;&#30340;&#23494;&#24230;&#20272;&#35745;/&#36817;&#20284;&#12290;&#19982;KRnet&#31867;&#20284;&#65292;B-KRnet&#30340;&#32467;&#26500;&#23558;Knothe-Rosenblatt&#37325;&#25490;&#30340;&#19977;&#35282;&#24418;&#24418;&#24335;&#36716;&#21270;&#20026;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#12290;B-KRnet&#21644;KRnet&#20043;&#38388;&#30340;&#20027;&#35201;&#21306;&#21035;&#26159;B-KRnet&#23450;&#20041;&#22312;&#36229;&#31435;&#26041;&#20307;&#19978;&#65292;&#32780;KRnet&#23450;&#20041;&#22312;&#25972;&#20010;&#31354;&#38388;&#19978;&#65292;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#22312;B-KRnet&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#26469;&#20445;&#25345;&#31934;&#30830;&#30340;&#21487;&#36870;&#24615;&#12290;&#23558;B-KRnet&#29992;&#20316;&#20256;&#36755;&#26144;&#23556;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65288;PDF&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23545;&#24212;&#20110;&#20808;&#39564;&#65288;&#22343;&#21248;&#65289;&#20998;&#24067;&#22312;&#36229;&#31435;&#26041;&#20307;&#19978;&#30340;&#25512;&#31227;&#12290;&#20026;&#20102;&#36817;&#20284;&#35745;&#31639;&#22495;&#19978;&#23450;&#20041;&#30340;PDF&#65292;B-KRnet&#27604;KRnet&#26356;&#26377;&#25928;&#12290;&#36890;&#36807;&#32806;&#21512;KRnet&#21644;B-KRnet&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#22312;&#39640;&#32500;&#22495;&#19978;&#23450;&#20041;&#19968;&#20010;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop an invertible mapping, called B-KRnet, on a bounded domain and apply it to density estimation/approximation for data or the solutions of PDEs such as the Fokker-Planck equation and the Keller-Segel equation. Similar to KRnet, the structure of B-KRnet adapts the triangular form of the Knothe-Rosenblatt rearrangement into a normalizing flow model. The main difference between B-KRnet and KRnet is that B-KRnet is defined on a hypercube while KRnet is defined on the whole space, in other words, we introduce a new mechanism in B-KRnet to maintain the exact invertibility. Using B-KRnet as a transport map, we obtain an explicit probability density function (PDF) model that corresponds to the pushforward of a prior (uniform) distribution on the hypercube. To approximate PDFs defined on a bounded computational domain, B-KRnet is more effective than KRnet. By coupling KRnet and B-KRnet, we can also define a deep generative model on a high-dimensional domain where some di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#37327;&#23376;&#35745;&#31639;&#26426;&#33021;&#21147;&#20989;&#25968;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#36817;&#26399;&#37327;&#23376;&#22788;&#29702;&#22120;&#19978;&#37327;&#23376;&#30005;&#36335;&#30340;&#25104;&#21151;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.10650</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning a quantum computer's capability using convolutional neural networks. (arXiv:2304.10650v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#37327;&#23376;&#35745;&#31639;&#26426;&#33021;&#21147;&#20989;&#25968;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#36817;&#26399;&#37327;&#23376;&#22788;&#29702;&#22120;&#19978;&#37327;&#23376;&#30005;&#36335;&#30340;&#25104;&#21151;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#37327;&#23376;&#22788;&#29702;&#22120;&#30340;&#35745;&#31639;&#33021;&#21147;&#21463;&#30828;&#20214;&#38169;&#35823;&#24433;&#21709;&#23548;&#33268;&#35745;&#31639;&#22833;&#36133;&#12290;&#25105;&#20204;&#25506;&#35752;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#22788;&#29702;&#22120;&#33021;&#21147;&#20989;&#25968;&#30340;&#36817;&#20284;&#20540;&#12290;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#37327;&#23376;&#22788;&#29702;&#22120;&#30340;&#33021;&#21147;&#20989;&#25968;&#65292;&#20351;&#24471;&#36817;&#26399;&#37327;&#23376;&#22788;&#29702;&#22120;&#19978;&#30340;&#37327;&#23376;&#30005;&#36335;&#21487;&#20197;&#24555;&#36895;&#21487;&#38752;&#22320;&#39044;&#27979;&#25104;&#21151;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computational power of contemporary quantum processors is limited by hardware errors that cause computations to fail. In principle, each quantum processor's computational capabilities can be described with a capability function that quantifies how well a processor can run each possible quantum circuit (i.e., program), as a map from circuits to the processor's success rates on those circuits. However, capability functions are typically unknown and challenging to model, as the particular errors afflicting a specific quantum processor are a priori unknown and difficult to completely characterize. In this work, we investigate using artificial neural networks to learn an approximation to a processor's capability function. We explore how to define the capability function, and we explain how data for training neural networks can be efficiently obtained for a capability function defined using process fidelity. We then investigate using convolutional neural networks to model a quantum compu
&lt;/p&gt;</description></item><item><title>TargetCall&#36890;&#36807;&#39044;&#22522;&#35843;&#36807;&#28388;&#65292;&#28040;&#38500;&#20102;basecalling&#20013;&#30340;&#28010;&#36153;&#35745;&#31639;&#65292;&#25552;&#39640;&#20102;&#22522;&#22240;&#32452;&#20998;&#26512;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.04953</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#22522;&#35843;&#36807;&#28388;&#28040;&#38500;basecalling&#20013;&#30340;&#28010;&#36153;&#35745;&#31639;&#30340;TargetCall
&lt;/p&gt;
&lt;p&gt;
TargetCall: Eliminating the Wasted Computation in Basecalling via Pre-Basecalling Filtering. (arXiv:2212.04953v2 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04953
&lt;/p&gt;
&lt;p&gt;
TargetCall&#36890;&#36807;&#39044;&#22522;&#35843;&#36807;&#28388;&#65292;&#28040;&#38500;&#20102;basecalling&#20013;&#30340;&#28010;&#36153;&#35745;&#31639;&#65292;&#25552;&#39640;&#20102;&#22522;&#22240;&#32452;&#20998;&#26512;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Basecalling&#26159;&#32435;&#31859;&#23380;&#27979;&#24207;&#20998;&#26512;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#65292;&#23427;&#23558;&#32435;&#31859;&#23380;&#27979;&#24207;&#20202;&#30340;&#21407;&#22987;&#20449;&#21495;&#36716;&#25442;&#20026;&#26680;&#37240;&#24207;&#21015;&#65292;&#21363;reads&#12290;&#26368;&#20808;&#36827;&#30340;basecallers&#20351;&#29992;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#39640;&#24230;&#30340;basecalling&#20934;&#30830;&#24615;&#12290;&#36825;&#20351;&#24471;basecalling&#22312;&#35745;&#31639;&#19978;&#25928;&#29575;&#20302;&#19979;&#19988;&#20869;&#23384;&#28040;&#32791;&#22823;&#65292;&#25104;&#20026;&#25972;&#20010;&#22522;&#22240;&#32452;&#20998;&#26512;&#27969;&#31243;&#30340;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#26469;&#35828;&#65292;&#22823;&#22810;&#25968;reads&#19982;&#24863;&#20852;&#36259;&#30340;&#21442;&#32771;&#22522;&#22240;&#32452;&#19981;&#21305;&#37197;&#65288;&#21363;&#30446;&#26631;&#21442;&#32771;&#22522;&#22240;&#32452;&#65289;&#65292;&#22240;&#27492;&#20250;&#22312;&#21518;&#32493;&#30340;&#22522;&#22240;&#32452;&#27969;&#31243;&#27493;&#39588;&#20013;&#34987;&#20002;&#24323;&#65292;&#28010;&#36153;&#20102;basecalling&#30340;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TargetCall&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#28040;&#38500;basecalling&#20013;&#28010;&#36153;&#35745;&#31639;&#30340;&#39044;&#22522;&#35843;&#36807;&#28388;&#22120;&#12290;TargetCall&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;basecalling&#20043;&#21069;&#20002;&#24323;&#19981;&#20250;&#19982;&#30446;&#26631;&#21442;&#32771;&#22522;&#22240;&#32452;&#21305;&#37197;&#30340;reads&#65288;&#21363;&#38750;&#30446;&#26631;reads&#65289;&#12290;TargetCall&#30001;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#65288;1&#65289;LightCall&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#31070;&#32463;&#32593;&#32476;basecaller&#65292;&#20135;&#29983;&#22122;&#22768;reads&#65307;
&lt;/p&gt;
&lt;p&gt;
Basecalling is an essential step in nanopore sequencing analysis where the raw signals of nanopore sequencers are converted into nucleotide sequences, i.e., reads. State-of-the-art basecallers employ complex deep learning models to achieve high basecalling accuracy. This makes basecalling computationally-inefficient and memory-hungry; bottlenecking the entire genome analysis pipeline. However, for many applications, the majority of reads do no match the reference genome of interest (i.e., target reference) and thus are discarded in later steps in the genomics pipeline, wasting the basecalling computation. To overcome this issue, we propose TargetCall, the first pre-basecalling filter to eliminate the wasted computation in basecalling. TargetCall's key idea is to discard reads that will not match the target reference (i.e., off-target reads) prior to basecalling. TargetCall consists of two main components: (1) LightCall, a lightweight neural network basecaller that produces noisy reads;
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#20803;&#20114;&#25554;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;Few-Task Meta-Learning&#38382;&#39064;&#20013;&#20219;&#21153;&#25968;&#37327;&#23569;&#24102;&#26469;&#30340;&#29942;&#39048;&#65292;&#21516;&#26102;&#35813;&#26041;&#27861;&#23545;&#39046;&#22495;&#19981;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2205.09990</link><description>&lt;p&gt;
&#38024;&#23545;Few-Task Meta-Learning&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#20803;&#20114;&#25554;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Set-based Meta-Interpolation for Few-Task Meta-Learning. (arXiv:2205.09990v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#20803;&#20114;&#25554;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;Few-Task Meta-Learning&#38382;&#39064;&#20013;&#20219;&#21153;&#25968;&#37327;&#23569;&#24102;&#26469;&#30340;&#29942;&#39048;&#65292;&#21516;&#26102;&#35813;&#26041;&#27861;&#23545;&#39046;&#22495;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#22312;&#32473;&#23450;&#23569;&#37327;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20803;&#35757;&#32451;&#20219;&#21153;&#30340;&#25968;&#37327;&#20173;&#28982;&#38656;&#35201;&#24456;&#22823;&#65292;&#25165;&#33021;&#22312;&#20803;&#27979;&#35797;&#26399;&#38388;&#36827;&#34892;&#27867;&#21270;&#65292;&#36825;&#23545;&#20110;&#21482;&#26377;&#23569;&#37327;&#20219;&#21153;&#30340;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#26469;&#35828;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#29942;&#39048;&#65292;&#21407;&#22240;&#21253;&#25324;&#26500;&#24314;&#20219;&#21153;&#30340;&#22256;&#38590;&#21644;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#20803;&#20114;&#25554;&#26041;&#27861;Meta-Interpolation&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning approaches enable machine learning systems to adapt to new tasks given few examples by leveraging knowledge from related tasks. However, a large number of meta-training tasks are still required for generalization to unseen tasks during meta-testing, which introduces a critical bottleneck for real-world problems that come with only few tasks, due to various reasons including the difficulty and cost of constructing tasks. Recently, several task augmentation methods have been proposed to tackle this issue using domain-specific knowledge to design augmentation techniques to densify the meta-training task distribution. However, such reliance on domain-specific knowledge renders these methods inapplicable to other domains. While Manifold Mixup based task augmentation methods are domain-agnostic, we empirically find them ineffective on non-image domains. To tackle these limitations, we propose a novel domain-agnostic task augmentation method, Meta-Interpolation, which utilizes e
&lt;/p&gt;</description></item></channel></rss>