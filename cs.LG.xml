<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>FlexLLM&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#22312;&#21516;&#19968;&#36845;&#20195;&#20013;&#20849;&#21516;&#25552;&#20379;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35831;&#27714;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#32423;&#24494;&#35843;&#26426;&#21046;&#23454;&#29616;&#20849;&#20139;GPU&#36164;&#28304;&#30340;&#39640;&#25928;&#21033;&#29992;</title><link>https://arxiv.org/abs/2402.18789</link><description>&lt;p&gt;
FlexLLM&#65306;&#19968;&#31181;&#29992;&#20110;&#20849;&#21516;&#25552;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18789
&lt;/p&gt;
&lt;p&gt;
FlexLLM&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#22312;&#21516;&#19968;&#36845;&#20195;&#20013;&#20849;&#21516;&#25552;&#20379;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35831;&#27714;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#32423;&#24494;&#35843;&#26426;&#21046;&#23454;&#29616;&#20849;&#20139;GPU&#36164;&#28304;&#30340;&#39640;&#25928;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Parameter-efficient finetuning&#65288;PEFT&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20026;&#19981;&#21516;&#20219;&#21153;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#24120;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#20250;&#20026;&#29992;&#25143;&#21019;&#24314;&#21333;&#29420;&#30340;&#31995;&#32479;&#65292;&#20197;&#25191;&#34892;PEFT&#27169;&#22411;&#24494;&#35843;&#21644;&#25512;&#29702;&#20219;&#21153;&#12290;&#36825;&#26159;&#22240;&#20026;&#29616;&#26377;&#31995;&#32479;&#26080;&#27861;&#22788;&#29702;&#21253;&#21547;&#25512;&#29702;&#21644;PEFT&#24494;&#35843;&#35831;&#27714;&#28151;&#21512;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;&#22240;&#27492;&#65292;&#20849;&#20139;&#30340;GPU&#36164;&#28304;&#21033;&#29992;&#19981;&#36275;&#65292;&#23548;&#33268;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FlexLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#22312;&#21516;&#19968;&#36845;&#20195;&#20013;&#20026;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35831;&#27714;&#25552;&#20379;&#26381;&#21153;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21033;&#29992;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#20114;&#34917;&#24615;&#36136;&#65292;&#24182;&#21033;&#29992;&#20849;&#20139;&#30340;GPU&#36164;&#28304;&#26469;&#20849;&#21516;&#36816;&#34892;&#23427;&#20204;&#65292;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;&#20849;&#21516;&#25552;&#20379;&#30340;&#26041;&#27861;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;FlexLLM&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#35760;&#32423;&#24494;&#35843;&#26426;&#21046;&#65292;&#23558;&#24207;&#21015;&#30340;&#24494;&#35843;&#35745;&#31639;&#20998;&#35299;&#20026;&#26356;&#23567;&#30340;&#26631;&#35760;&#32423;&#35745;&#31639;&#65292;&#24182;&#20351;&#29992;&#20381;&#36182;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18789v1 Announce Type: cross  Abstract: Parameter-efficient finetuning (PEFT) is a widely used technique to adapt large language models for different tasks. Service providers typically create separate systems for users to perform PEFT model finetuning and inference tasks. This is because existing systems cannot handle workloads that include a mix of inference and PEFT finetuning requests. As a result, shared GPU resources are underutilized, leading to inefficiencies. To address this problem, we present FlexLLM, the first system that can serve inference and parameter-efficient finetuning requests in the same iteration. Our system leverages the complementary nature of these two tasks and utilizes shared GPU resources to run them jointly, using a method called co-serving. To achieve this, FlexLLM introduces a novel token-level finetuning mechanism, which breaks down the finetuning computation of a sequence into smaller token-level computations and uses dependent parallelization
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#32423;&#20302;&#31209;&#30697;&#38453;&#20013;&#30340;&#22240;&#23376;&#25311;&#21512;&#12289;&#31209;&#20998;&#37197;&#21644;&#20998;&#21106;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#24320;&#28304;&#36719;&#20214;&#21253;&#12290;</title><link>http://arxiv.org/abs/2310.19214</link><description>&lt;p&gt;
&#22312;&#22810;&#32423;&#20302;&#31209;&#30697;&#38453;&#20013;&#36827;&#34892;&#22240;&#23376;&#25311;&#21512;&#12289;&#31209;&#20998;&#37197;&#21644;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Factor Fitting, Rank Allocation, and Partitioning in Multilevel Low Rank Matrices. (arXiv:2310.19214v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#32423;&#20302;&#31209;&#30697;&#38453;&#20013;&#30340;&#22240;&#23376;&#25311;&#21512;&#12289;&#31209;&#20998;&#37197;&#21644;&#20998;&#21106;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#24320;&#28304;&#36719;&#20214;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22810;&#32423;&#20302;&#31209;&#65288;MLR&#65289;&#30697;&#38453;&#65292;&#23450;&#20041;&#20026;&#19968;&#31995;&#21015;&#30697;&#38453;&#30340;&#34892;&#21644;&#21015;&#30340;&#25490;&#21015;&#65292;&#27599;&#20010;&#30697;&#38453;&#37117;&#26159;&#21069;&#19968;&#20010;&#30697;&#38453;&#30340;&#22359;&#23545;&#35282;&#20462;&#27491;&#65292;&#25152;&#26377;&#22359;&#20197;&#22240;&#23376;&#24418;&#24335;&#32473;&#20986;&#20302;&#31209;&#30697;&#38453;&#12290;MLR&#30697;&#38453;&#25193;&#23637;&#20102;&#20302;&#31209;&#30697;&#38453;&#30340;&#27010;&#24565;&#65292;&#20294;&#23427;&#20204;&#20849;&#20139;&#35768;&#22810;&#29305;&#24615;&#65292;&#20363;&#22914;&#25152;&#38656;&#24635;&#23384;&#20648;&#31354;&#38388;&#21644;&#30697;&#38453;&#21521;&#37327;&#20056;&#27861;&#30340;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#29992;Frobenius&#33539;&#25968;&#25311;&#21512;&#32473;&#23450;&#30697;&#38453;&#21040;MLR&#30697;&#38453;&#30340;&#19977;&#20010;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#26159;&#22240;&#23376;&#25311;&#21512;&#65292;&#36890;&#36807;&#35843;&#25972;MLR&#30697;&#38453;&#30340;&#22240;&#23376;&#26469;&#35299;&#20915;&#12290;&#31532;&#20108;&#20010;&#38382;&#39064;&#26159;&#31209;&#20998;&#37197;&#65292;&#22312;&#27599;&#20010;&#32423;&#21035;&#20013;&#36873;&#25321;&#22359;&#30340;&#31209;&#65292;&#28385;&#36275;&#24635;&#31209;&#30340;&#32473;&#23450;&#20540;&#65292;&#20197;&#20445;&#25345;MLR&#30697;&#38453;&#25152;&#38656;&#30340;&#24635;&#23384;&#20648;&#31354;&#38388;&#12290;&#26368;&#21518;&#19968;&#20010;&#38382;&#39064;&#26159;&#36873;&#25321;&#34892;&#21644;&#21015;&#30340;&#23618;&#27425;&#20998;&#21106;&#65292;&#20197;&#21450;&#31209;&#21644;&#22240;&#23376;&#12290;&#26412;&#25991;&#38468;&#24102;&#20102;&#19968;&#20010;&#24320;&#28304;&#36719;&#20214;&#21253;&#65292;&#23454;&#29616;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider multilevel low rank (MLR) matrices, defined as a row and column permutation of a sum of matrices, each one a block diagonal refinement of the previous one, with all blocks low rank given in factored form. MLR matrices extend low rank matrices but share many of their properties, such as the total storage required and complexity of matrix-vector multiplication. We address three problems that arise in fitting a given matrix by an MLR matrix in the Frobenius norm. The first problem is factor fitting, where we adjust the factors of the MLR matrix. The second is rank allocation, where we choose the ranks of the blocks in each level, subject to the total rank having a given value, which preserves the total storage needed for the MLR matrix. The final problem is to choose the hierarchical partition of rows and columns, along with the ranks and factors. This paper is accompanied by an open source package that implements the proposed methods.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#26816;&#27979;&#21644;&#36716;&#25442;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#26041;&#38754;&#27880;&#37325;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#22270;&#12289;&#38598;&#25104;&#26799;&#24230;&#21644;&#27169;&#22411;&#21453;&#39304;&#31561;&#25216;&#26415;&#65292;&#22312;&#26816;&#27979;&#38454;&#27573;&#26377;&#21161;&#20110;&#35782;&#21035;&#23545;&#23545;&#25239;&#24615;&#20998;&#31867;&#26377;&#36129;&#29486;&#30340;&#26174;&#33879;&#29305;&#24449;&#21644;&#25200;&#21160;&#35789;&#35821;&#65292;&#24182;&#22312;&#36716;&#25442;&#38454;&#27573;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#27169;&#22411;&#21453;&#39304;&#26469;&#29983;&#25104;&#25200;&#21160;&#35789;&#35821;&#30340;&#26368;&#20339;&#26367;&#20195;&#65292;&#20197;&#23558;&#23545;&#25239;&#24615;&#31034;&#20363;&#36716;&#25442;&#20026;&#27491;&#24120;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.01225</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;
&lt;/p&gt;
&lt;p&gt;
Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT). (arXiv:2307.01225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01225
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#26816;&#27979;&#21644;&#36716;&#25442;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#26041;&#38754;&#27880;&#37325;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#22270;&#12289;&#38598;&#25104;&#26799;&#24230;&#21644;&#27169;&#22411;&#21453;&#39304;&#31561;&#25216;&#26415;&#65292;&#22312;&#26816;&#27979;&#38454;&#27573;&#26377;&#21161;&#20110;&#35782;&#21035;&#23545;&#23545;&#25239;&#24615;&#20998;&#31867;&#26377;&#36129;&#29486;&#30340;&#26174;&#33879;&#29305;&#24449;&#21644;&#25200;&#21160;&#35789;&#35821;&#65292;&#24182;&#22312;&#36716;&#25442;&#38454;&#27573;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#27169;&#22411;&#21453;&#39304;&#26469;&#29983;&#25104;&#25200;&#21160;&#35789;&#35821;&#30340;&#26368;&#20339;&#26367;&#20195;&#65292;&#20197;&#23558;&#23545;&#25239;&#24615;&#31034;&#20363;&#36716;&#25442;&#20026;&#27491;&#24120;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#22914;BERT&#12289;Roberta&#12289;T5&#21644;GPT-3&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20110;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#33030;&#24369;&#24615;&#25552;&#20986;&#20102;&#23433;&#20840;&#39118;&#38505;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#24456;&#38590;&#29702;&#35299;&#23545;&#25239;&#24615;&#20998;&#31867;&#24182;&#35782;&#21035;&#27169;&#22411;&#30340;&#28431;&#27934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;&#26694;&#26550;&#12290;&#23427;&#19987;&#27880;&#20110;&#22312;&#26816;&#27979;&#21644;&#36716;&#25442;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#26102;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#12290;IT-DT&#21033;&#29992;&#27880;&#24847;&#21147;&#22270;&#12289;&#38598;&#25104;&#26799;&#24230;&#21644;&#27169;&#22411;&#21453;&#39304;&#31561;&#25216;&#26415;&#36827;&#34892;&#35299;&#37322;&#24615;&#26816;&#27979;&#12290;&#36825;&#26377;&#21161;&#20110;&#35782;&#21035;&#23545;&#23545;&#25239;&#24615;&#20998;&#31867;&#26377;&#36129;&#29486;&#30340;&#26174;&#33879;&#29305;&#24449;&#21644;&#25200;&#21160;&#35789;&#35821;&#12290;&#22312;&#36716;&#25442;&#38454;&#27573;&#65292;IT-DT&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#27169;&#22411;&#21453;&#39304;&#26469;&#29983;&#25104;&#25200;&#21160;&#35789;&#35821;&#30340;&#26368;&#20339;&#26367;&#20195;&#12290;&#36890;&#36807;&#25214;&#21040;&#21512;&#36866;&#30340;&#26367;&#25442;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#23545;&#25239;&#24615;&#31034;&#20363;&#36716;&#25442;&#20026;&#27491;&#24120;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based text classifiers like BERT, Roberta, T5, and GPT-3 have shown impressive performance in NLP. However, their vulnerability to adversarial examples poses a security risk. Existing defense methods lack interpretability, making it hard to understand adversarial classifications and identify model vulnerabilities. To address this, we propose the Interpretability and Transparency-Driven Detection and Transformation (IT-DT) framework. It focuses on interpretability and transparency in detecting and transforming textual adversarial examples. IT-DT utilizes techniques like attention maps, integrated gradients, and model feedback for interpretability during detection. This helps identify salient features and perturbed words contributing to adversarial classifications. In the transformation phase, IT-DT uses pre-trained embeddings and model feedback to generate optimal replacements for perturbed words. By finding suitable substitutions, we aim to convert adversarial examples into
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#65292;&#22914;&#20309;&#22312;&#26399;&#26395;&#21644;&#23614;&#37096;&#39118;&#38505;&#20043;&#38388;&#20570;&#20986;&#26368;&#20248;&#26435;&#34913;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#23454;&#29616;&#26368;&#22351;&#21644;&#23454;&#20363;&#30456;&#20851;&#30340;&#20248;&#24322;&#34920;&#29616;&#65292;&#24182;&#19988;&#33021;&#22815;&#26368;&#23567;&#21270;&#36951;&#25022;&#23614;&#37096;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.04341</link><description>&lt;p&gt;
&#38543;&#26426;&#36172;&#21338;&#26426;&#20013;&#30340;&#36951;&#25022;&#20998;&#24067;&#65306;&#26399;&#26395;&#21644;&#23614;&#37096;&#39118;&#38505;&#20043;&#38388;&#30340;&#26368;&#20248;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Regret Distribution in Stochastic Bandits: Optimal Trade-off between Expectation and Tail Risk. (arXiv:2304.04341v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04341
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#65292;&#22914;&#20309;&#22312;&#26399;&#26395;&#21644;&#23614;&#37096;&#39118;&#38505;&#20043;&#38388;&#20570;&#20986;&#26368;&#20248;&#26435;&#34913;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#23454;&#29616;&#26368;&#22351;&#21644;&#23454;&#20363;&#30456;&#20851;&#30340;&#20248;&#24322;&#34920;&#29616;&#65292;&#24182;&#19988;&#33021;&#22815;&#26368;&#23567;&#21270;&#36951;&#25022;&#23614;&#37096;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#20013;&#65292;&#36951;&#25022;&#20998;&#24067;&#30340;&#26399;&#26395;&#21644;&#23614;&#37096;&#39118;&#38505;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#23436;&#20840;&#21051;&#30011;&#20102;&#31574;&#30053;&#35774;&#35745;&#20013;&#19977;&#20010;&#26399;&#26395;&#24615;&#36136;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65306;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#24615;&#65292;&#23454;&#20363;&#30456;&#20851;&#30340;&#19968;&#33268;&#24615;&#21644;&#36731;&#23614;&#39118;&#38505;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26399;&#26395;&#36951;&#25022;&#30340;&#39034;&#24207;&#22914;&#20309;&#24433;&#21709;&#36951;&#25022;&#23614;&#37096;&#27010;&#29575;&#30340;&#34928;&#20943;&#29575;&#65292;&#21516;&#26102;&#21253;&#25324;&#20102;&#26368;&#22351;&#24773;&#20917;&#21644;&#23454;&#20363;&#30456;&#20851;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#20197;&#34920;&#24449;&#23545;&#20110;&#20219;&#20309;&#36951;&#25022;&#38408;&#20540;&#30340;&#26368;&#20248;&#36951;&#25022;&#23614;&#37096;&#27010;&#29575;&#12290;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#20219;&#20309;&#32473;&#23450;&#30340;$\alpha \in [1/2, 1)$&#21644;$\beta \in [0, \alpha]$&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#24179;&#22343;&#26399;&#26395;&#36951;&#25022;$\tilde O(T^\alpha)$&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;$\alpha$-&#26368;&#20248;&#21644;&#26399;&#26395;&#36951;&#25022;$\tilde O(T^\beta)$&#30340;&#23454;&#20363;&#30456;&#20851;&#30340;$\beta$-&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#20139;&#26377;&#19968;&#23450;&#30340;&#27010;&#29575;&#21487;&#20197;&#36991;&#20813;$\tilde O(T^\delta)$&#30340;&#36951;&#25022;($\delta \geq \alpha$&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#21644;$\delta \geq \beta$&#22312;&#23454;&#20363;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;)&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the trade-off between expectation and tail risk for regret distribution in the stochastic multi-armed bandit problem. We fully characterize the interplay among three desired properties for policy design: worst-case optimality, instance-dependent consistency, and light-tailed risk. We show how the order of expected regret exactly affects the decaying rate of the regret tail probability for both the worst-case and instance-dependent scenario. A novel policy is proposed to characterize the optimal regret tail probability for any regret threshold. Concretely, for any given $\alpha\in[1/2, 1)$ and $\beta\in[0, \alpha]$, our policy achieves a worst-case expected regret of $\tilde O(T^\alpha)$ (we call it $\alpha$-optimal) and an instance-dependent expected regret of $\tilde O(T^\beta)$ (we call it $\beta$-consistent), while enjoys a probability of incurring an $\tilde O(T^\delta)$ regret ($\delta\geq\alpha$ in the worst-case scenario and $\delta\geq\beta$ in the instance-dependent s
&lt;/p&gt;</description></item></channel></rss>