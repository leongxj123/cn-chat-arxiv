<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#22312;&#26377;&#38480;&#25104;&#23545;&#20559;&#22909;&#27604;&#36739;&#19979;&#30740;&#31350;&#24230;&#37327;&#23398;&#20064;&#65292;&#34920;&#26126;&#34429;&#28982;&#26080;&#27861;&#23398;&#20064;&#21333;&#20010;&#29702;&#24819;&#39033;&#30446;&#65292;&#20294;&#24403;&#27604;&#36739;&#23545;&#35937;&#34920;&#29616;&#20986;&#20302;&#32500;&#32467;&#26500;&#26102;&#65292;&#27599;&#20010;&#29992;&#25143;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#38480;&#21046;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#30340;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.19629</link><description>&lt;p&gt;
&#26377;&#38480;&#25104;&#23545;&#20559;&#22909;&#27604;&#36739;&#19979;&#30340;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Metric Learning from Limited Pairwise Preference Comparisons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19629
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#25104;&#23545;&#20559;&#22909;&#27604;&#36739;&#19979;&#30740;&#31350;&#24230;&#37327;&#23398;&#20064;&#65292;&#34920;&#26126;&#34429;&#28982;&#26080;&#27861;&#23398;&#20064;&#21333;&#20010;&#29702;&#24819;&#39033;&#30446;&#65292;&#20294;&#24403;&#27604;&#36739;&#23545;&#35937;&#34920;&#29616;&#20986;&#20302;&#32500;&#32467;&#26500;&#26102;&#65292;&#27599;&#20010;&#29992;&#25143;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#38480;&#21046;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#29702;&#24819;&#28857;&#27169;&#22411;&#19979;&#30340;&#20559;&#22909;&#27604;&#36739;&#20013;&#30340;&#24230;&#37327;&#23398;&#20064;&#65292;&#20854;&#20013;&#29992;&#25143;&#22914;&#26524;&#19968;&#20010;&#39033;&#30446;&#27604;&#20854;&#28508;&#22312;&#29702;&#24819;&#39033;&#30446;&#26356;&#25509;&#36817;&#65292;&#21017;&#26356;&#21916;&#27426;&#35813;&#39033;&#30446;&#12290;&#36825;&#20123;&#39033;&#30446;&#23884;&#20837;&#21040;&#20855;&#26377;&#26410;&#30693;&#39532;&#27663;&#36317;&#31163;&#30340;$\mathbb{R}^d$&#20013;&#65292;&#35813;&#36317;&#31163;&#22312;&#29992;&#25143;&#38388;&#20849;&#20139;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#27599;&#20010;&#29992;&#25143;$\mathcal{O}(d)$&#20010;&#25104;&#23545;&#27604;&#36739;&#21487;&#20197;&#21516;&#26102;&#24674;&#22797;&#24230;&#37327;&#21644;&#29702;&#24819;&#39033;&#30446;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#26377;$o(d)$&#30340;&#26377;&#38480;&#27604;&#36739;&#39044;&#31639;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21363;&#20351;&#24050;&#30693;&#23398;&#20064;&#21333;&#20010;&#29702;&#24819;&#39033;&#30446;&#29616;&#22312;&#19981;&#20877;&#21487;&#33021;&#65292;&#24230;&#37327;&#26159;&#21542;&#20173;&#28982;&#21487;&#20197;&#24674;&#22797;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#33324;&#26469;&#35828;&#65292;$o(d)$&#27604;&#36739;&#19981;&#20250;&#25581;&#31034;&#26377;&#20851;&#24230;&#37327;&#30340;&#20449;&#24687;&#65292;&#21363;&#20351;&#29992;&#25143;&#25968;&#37327;&#26080;&#38480;&#12290;&#28982;&#32780;&#65292;&#24403;&#27604;&#36739;&#30340;&#39033;&#30446;&#34920;&#29616;&#20986;&#20302;&#32500;&#32467;&#26500;&#26102;&#65292;&#27599;&#20010;&#29992;&#25143;&#37117;&#21487;&#20197;&#26377;&#21161;&#20110;&#23398;&#20064;&#38480;&#21046;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#30340;&#24230;&#37327;&#65292;&#36825;&#26679;&#24230;&#37327;&#23601;&#21487;&#20197;&#34987;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19629v1 Announce Type: new  Abstract: We study metric learning from preference comparisons under the ideal point model, in which a user prefers an item over another if it is closer to their latent ideal item. These items are embedded into $\mathbb{R}^d$ equipped with an unknown Mahalanobis distance shared across users. While recent work shows that it is possible to simultaneously recover the metric and ideal items given $\mathcal{O}(d)$ pairwise comparisons per user, in practice we often have a limited budget of $o(d)$ comparisons. We study whether the metric can still be recovered, even though it is known that learning individual ideal items is now no longer possible. We show that in general, $o(d)$ comparisons reveals no information about the metric, even with infinitely many users. However, when comparisons are made over items that exhibit low-dimensional structure, each user can contribute to learning the metric restricted to a low-dimensional subspace so that the metric
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.18717</link><description>&lt;p&gt;
&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Learning for Deep Causal Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18717
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#22238;&#31572;&#8220;&#22914;&#26524;$y$&#21464;&#20026;$z$&#65292;$x$&#20250;&#22914;&#20309;&#21464;&#21270;&#65311;&#8221;&#36825;&#31867;&#38382;&#39064;&#30340;&#27169;&#22411;&#23545;&#20110;&#25512;&#21160;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#33021;&#22815;&#35299;&#20915;&#36825;&#31867;&#21453;&#20107;&#23454;&#38382;&#39064;&#30340;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#30446;&#21069;&#35201;&#27714;&#25152;&#26377;&#30456;&#20851;&#21464;&#37327;&#22343;&#24050;&#34987;&#35266;&#23519;&#21040;&#65292;&#24182;&#19988;&#30456;&#24212;&#30340;&#26631;&#31614;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21487;&#29992;&#12290;&#25105;&#20204;&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18717v1 Announce Type: cross  Abstract: Developing models that can answer questions of the form "How would $x$ change if $y$ had been $z$?" is fundamental for advancing medical image analysis. Training causal generative models that address such counterfactual questions, though, currently requires that all relevant variables have been observed and that corresponding labels are available in training data. However, clinical data may not have complete records for all patients and state of the art causal generative models are unable to take full advantage of this. We thus develop, for the first time, a semi-supervised deep causal generative model that exploits the causal relationships between variables to maximise the use of all available data. We explore this in the setting where each sample is either fully labelled or fully unlabelled, as well as the more clinically realistic case of having different labels missing for each sample. We leverage techniques from causal inference t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#24863;&#30693;3D&#24418;&#29366;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;2D&#24179;&#38754;&#34920;&#31034;&#22686;&#24378;&#24314;&#27169;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#24418;&#29366;&#34920;&#31034;&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#36830;&#32493;&#26377;&#21521;&#36317;&#31163;&#22330;&#34920;&#31034;&#65292;&#20174;&#32780;&#30830;&#20445;&#31354;&#38388;&#19968;&#33268;&#24615;&#21644;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.18241</link><description>&lt;p&gt;
NeuSDFusion: &#19968;&#31181;&#31354;&#38388;&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;3D&#24418;&#29366;&#30340;&#23436;&#25104;&#12289;&#37325;&#24314;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18241
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#24863;&#30693;3D&#24418;&#29366;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;2D&#24179;&#38754;&#34920;&#31034;&#22686;&#24378;&#24314;&#27169;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#24418;&#29366;&#34920;&#31034;&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#36830;&#32493;&#26377;&#21521;&#36317;&#31163;&#22330;&#34920;&#31034;&#65292;&#20174;&#32780;&#30830;&#20445;&#31354;&#38388;&#19968;&#33268;&#24615;&#21644;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#24418;&#29366;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#26465;&#20214;&#21644;&#32422;&#26463;&#30340;&#21019;&#26032;&#24615;3D&#20869;&#23481;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#23558;3D&#24418;&#29366;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#23616;&#37096;&#32452;&#20214;&#65292;&#23558;&#27599;&#20010;&#20803;&#32032;&#23396;&#31435;&#22788;&#29702;&#32780;&#19981;&#32771;&#34385;&#31354;&#38388;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;3D&#25968;&#25454;&#34920;&#31034;&#21644;&#24418;&#29366;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#22810;&#26679;&#24615;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#29983;&#25104;&#39640;&#24230;&#22810;&#26679;&#21270;&#19988;&#31526;&#21512;&#25351;&#23450;&#32422;&#26463;&#30340;3D&#24418;&#29366;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#24863;&#30693;3D&#24418;&#29366;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;2D&#24179;&#38754;&#34920;&#31034;&#26469;&#22686;&#24378;3D&#24418;&#29366;&#24314;&#27169;&#12290;&#20026;&#30830;&#20445;&#31354;&#38388;&#19968;&#33268;&#24615;&#24182;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#31181;&#28151;&#21512;&#24418;&#29366;&#34920;&#31034;&#25216;&#26415;&#65292;&#30452;&#25509;&#20351;&#29992;&#27491;&#20132;&#30340;2D&#24179;&#38754;&#23398;&#20064;3D&#24418;&#29366;&#30340;&#36830;&#32493;&#26377;&#21521;&#36317;&#31163;&#22330;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20256;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18241v1 Announce Type: cross  Abstract: 3D shape generation aims to produce innovative 3D content adhering to specific conditions and constraints. Existing methods often decompose 3D shapes into a sequence of localized components, treating each element in isolation without considering spatial consistency. As a result, these approaches exhibit limited versatility in 3D data representation and shape generation, hindering their ability to generate highly diverse 3D shapes that comply with the specified constraints. In this paper, we introduce a novel spatial-aware 3D shape generation framework that leverages 2D plane representations for enhanced 3D shape modeling. To ensure spatial coherence and reduce memory usage, we incorporate a hybrid shape representation technique that directly learns a continuous signed distance field representation of the 3D shape using orthogonal 2D planes. Additionally, we meticulously enforce spatial correspondences across distinct planes using a tra
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#26377;&#25928;&#36827;&#34892;&#25506;&#32034;&#65292;&#38500;&#20102;&#29305;&#23450;&#37197;&#32622;&#19979;&#30340;GPT-4&#20855;&#26377;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#34920;&#29616;&#19981;&#31283;&#23450;&#12290;</title><link>https://arxiv.org/abs/2403.15371</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#20013;&#30340;&#25506;&#32034;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can large language models explore in-context?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15371
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#26377;&#25928;&#36827;&#34892;&#25506;&#32034;&#65292;&#38500;&#20102;&#29305;&#23450;&#37197;&#32622;&#19979;&#30340;GPT-4&#20855;&#26377;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#34920;&#29616;&#19981;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36827;&#34892;&#25506;&#32034;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#24378;&#21270;&#23398;&#20064;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#25105;&#20204;&#20851;&#27880;&#29616;&#26377;LLMs&#30340;&#21407;&#29983;&#24615;&#33021;&#65292;&#27809;&#26377;&#36827;&#34892;&#35757;&#32451;&#24178;&#39044;&#12290;&#25105;&#20204;&#23558;LLMs&#37096;&#32626;&#20026;&#31616;&#21333;&#22810;&#33218;&#32769;&#34382;&#26426;&#29615;&#22659;&#20013;&#30340;&#20195;&#29702;&#65292;&#24182;&#23436;&#20840;&#22312;&#19978;&#19979;&#25991;&#20013;&#25351;&#23450;&#29615;&#22659;&#25551;&#36848;&#21644;&#20132;&#20114;&#21382;&#21490;&#65292;&#21363;&#22312;LLM&#25552;&#31034;&#20869;&#37096;&#36827;&#34892;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#23545;GPT-3.5&#12289;GPT-4&#21644;Llama2&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24182;&#27809;&#26377;&#31283;&#20581;&#22320;&#36827;&#34892;&#25506;&#32034;&#65306;i&#65289;&#22312;&#25105;&#20204;&#30340;&#25152;&#26377;&#23454;&#39564;&#20013;&#65292;&#21482;&#26377;&#19968;&#20010;&#37197;&#32622;&#23548;&#33268;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#65306;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#22806;&#37096;&#24635;&#32467;&#30340;&#20132;&#20114;&#21382;&#21490;&#30340;GPT-4&#65292;&#36825;&#20123;&#34987;&#21576;&#29616;&#20026;&#20805;&#20998;&#32479;&#35745;&#30340;&#24773;&#20917;&#65307;ii&#65289;&#25152;&#26377;&#20854;&#20182;&#37197;&#32622;&#37117;&#27809;&#26377;&#20135;&#29983;&#31283;&#20581;&#30340;&#25506;&#32034;&#34892;&#20026;&#65292;&#21253;&#25324;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#20854;&#20182;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15371v1 Announce Type: cross  Abstract: We investigate the extent to which contemporary Large Language Models (LLMs) can engage in exploration, a core capability in reinforcement learning and decision making. We focus on native performance of existing LLMs, without training interventions. We deploy LLMs as agents in simple multi-armed bandit environments, specifying the environment description and interaction history entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5, GPT-4, and Llama2, using a variety of prompt designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Across all of our experiments, only one configuration resulted in satisfactory exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally summarized interaction history, presented as sufficient statistics; ii) All other configurations did not result in robust exploratory behavior, including those with chain-of-thou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;OTSeg&#20013;&#30340;Multi-Prompts Sinkhorn Attention&#26426;&#21046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#26469;&#21305;&#37197;&#30456;&#20851;&#20687;&#32032;&#23884;&#20837;&#65292;&#20174;&#32780;&#25552;&#21319;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14183</link><description>&lt;p&gt;
OTSeg&#65306;&#22810;&#25552;&#31034;Sinkhorn&#27880;&#24847;&#21147;&#29992;&#20110;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14183
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;OTSeg&#20013;&#30340;Multi-Prompts Sinkhorn Attention&#26426;&#21046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#26469;&#21305;&#37197;&#30456;&#20851;&#20687;&#32032;&#23884;&#20837;&#65292;&#20174;&#32780;&#25552;&#21319;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#30340;&#26368;&#26032;&#25104;&#21151;&#35777;&#26126;&#20102;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#30693;&#35782;&#36716;&#31227;&#21040;&#20687;&#32032;&#32423;&#20998;&#31867;&#26469;&#36827;&#34892;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;CLIP&#30693;&#35782;&#26469;&#32039;&#23494;&#23545;&#40784;&#25991;&#26412;&#23884;&#20837;&#21644;&#20687;&#32032;&#23884;&#20837;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OTSeg&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26088;&#22312;&#22686;&#24378;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#21305;&#37197;&#30456;&#20851;&#20687;&#32032;&#23884;&#20837;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#31639;&#27861;&#30340;&#22810;&#25552;&#31034;Sinkhorn&#65288;MPS&#65289;&#65292;&#36825;&#20351;&#24471;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#20851;&#27880;&#22270;&#20687;&#20687;&#32032;&#20869;&#30340;&#21508;&#31181;&#35821;&#20041;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#21463;&#21040;Sinkformers&#22312;&#21333;&#27169;&#24577;&#35774;&#32622;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MPS&#30340;&#25193;&#23637;&#65292;&#31216;&#20026;&#22810;&#25552;&#31034;Sinkhorn&#27880;&#24847;&#21147;&#65288;MPSA&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#21462;&#20195;&#20102;Transformer&#26694;&#26550;&#20013;&#22810;&#27169;&#24577;&#35774;&#32622;&#20013;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14183v1 Announce Type: cross  Abstract: The recent success of CLIP has demonstrated promising results in zero-shot semantic segmentation by transferring muiltimodal knowledge to pixel-level classification. However, leveraging pre-trained CLIP knowledge to closely align text embeddings with pixel embeddings still has limitations in existing approaches. To address this issue, we propose OTSeg, a novel multimodal attention mechanism aimed at enhancing the potential of multiple text prompts for matching associated pixel embeddings. We first propose Multi-Prompts Sinkhorn (MPS) based on the Optimal Transport (OT) algorithm, which leads multiple text prompts to selectively focus on various semantic features within image pixels. Moreover, inspired by the success of Sinkformers in unimodal settings, we introduce the extension of MPS, called Multi-Prompts Sinkhorn Attention (MPSA), which effectively replaces cross-attention mechanisms within Transformer framework in multimodal settin
&lt;/p&gt;</description></item><item><title>BaCon&#36890;&#36807;&#24179;&#34913;&#29305;&#24449;&#32423;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#35268;&#33539;&#20102;&#23454;&#20363;&#34920;&#31034;&#30340;&#20998;&#24067;&#65292;&#22312;&#35299;&#20915;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.12986</link><description>&lt;p&gt;
BaCon&#65306;&#36890;&#36807;&#24179;&#34913;&#29305;&#24449;&#32423;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BaCon: Boosting Imbalanced Semi-supervised Learning via Balanced Feature-Level Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12986
&lt;/p&gt;
&lt;p&gt;
BaCon&#36890;&#36807;&#24179;&#34913;&#29305;&#24449;&#32423;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#35268;&#33539;&#20102;&#23454;&#20363;&#34920;&#31034;&#30340;&#20998;&#24067;&#65292;&#22312;&#35299;&#20915;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#20943;&#23569;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#23545;&#22823;&#37327;&#26631;&#27880;&#30340;&#38656;&#27714;&#65292;&#20294;&#26159;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#26356;&#29616;&#23454;&#30340;&#25361;&#25112;&#8212;&#8212;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#20173;&#28982;&#36739;&#23569;&#34987;&#25506;&#35752;&#12290;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;(CISSL)&#20013;&#65292;&#19981;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#20250;&#21152;&#21095;&#30001;&#19981;&#21487;&#38752;&#20266;&#26631;&#31614;&#24341;&#20837;&#30340;&#20559;&#35265;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#25110;&#37325;&#37319;&#26679;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#20381;&#36182;&#20110;&#26377;&#20559;&#30340;&#39592;&#24178;&#34920;&#31034;&#65292;&#20854;&#24615;&#33021;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;&#19968;&#20123;&#20854;&#20182;&#26041;&#27861;&#30830;&#23454;&#36827;&#34892;&#20102;&#29305;&#24449;&#32423;&#35843;&#25972;&#65292;&#27604;&#22914;&#29305;&#24449;&#34701;&#21512;&#65292;&#20294;&#21487;&#33021;&#24341;&#20837;&#19981;&#21033;&#30340;&#22122;&#22768;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#26356;&#24179;&#34913;&#30340;&#29305;&#24449;&#20998;&#24067;&#23545;CISSL&#38382;&#39064;&#30340;&#22909;&#22788;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#29305;&#24449;&#32423;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;(BaCon)&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#27604;&#26041;&#24335;&#30452;&#25509;&#35268;&#33539;&#20102;&#23454;&#20363;&#34920;&#31034;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12986v1 Announce Type: cross  Abstract: Semi-supervised Learning (SSL) reduces the need for extensive annotations in deep learning, but the more realistic challenge of imbalanced data distribution in SSL remains largely unexplored. In Class Imbalanced Semi-supervised Learning (CISSL), the bias introduced by unreliable pseudo-labels can be exacerbated by imbalanced data distributions. Most existing methods address this issue at instance-level through reweighting or resampling, but the performance is heavily limited by their reliance on biased backbone representation. Some other methods do perform feature-level adjustments like feature blending but might introduce unfavorable noise. In this paper, we discuss the bonus of a more balanced feature distribution for the CISSL problem, and further propose a Balanced Feature-Level Contrastive Learning method (BaCon). Our method directly regularizes the distribution of instances' representations in a well-designed contrastive manner. 
&lt;/p&gt;</description></item><item><title>EnvGen&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#33258;&#36866;&#24212;&#21019;&#24314;&#35757;&#32451;&#29615;&#22659;&#65292;&#24110;&#21161;&#23567;&#22411;&#20855;&#36523;&#20307;RL&#20195;&#29702;&#22312;&#24369;&#28857;&#26041;&#38754;&#23398;&#20064;&#26377;&#29992;&#25216;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12014</link><description>&lt;p&gt;
EnvGen: &#36890;&#36807;LLMs&#29983;&#25104;&#21644;&#35843;&#25972;&#29615;&#22659;&#20197;&#35757;&#32451;&#20855;&#36523;&#20307;&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12014
&lt;/p&gt;
&lt;p&gt;
EnvGen&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#33258;&#36866;&#24212;&#21019;&#24314;&#35757;&#32451;&#29615;&#22659;&#65292;&#24110;&#21161;&#23567;&#22411;&#20855;&#36523;&#20307;RL&#20195;&#29702;&#22312;&#24369;&#28857;&#26041;&#38754;&#23398;&#20064;&#26377;&#29992;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#20851;&#36890;&#36807;&#20114;&#21160;&#36827;&#34892;&#20855;&#36523;&#20307;&#23398;&#20064;&#30340;&#26368;&#26032;&#26041;&#27861;&#30452;&#25509;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20195;&#29702;&#65292;&#20197;&#30830;&#23450;&#29615;&#22659;&#20013;&#30340;&#19979;&#19968;&#27493;&#12290;LLM&#20195;&#29702;&#30001;&#20110;&#20854;&#19990;&#30028;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#27604;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#20197;&#24448;&#36739;&#23567;&#30340;&#20195;&#29702;&#34920;&#29616;&#26356;&#24378;&#65307;&#20294;&#39057;&#32321;&#35843;&#29992;LLMs&#36895;&#24230;&#24930;&#19988;&#26114;&#36149;&#12290;&#25105;&#20204;&#25552;&#20986;EnvGen&#65292;&#19968;&#20010;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#31034;&#19968;&#20010;LLM&#29983;&#25104;&#35757;&#32451;&#29615;&#22659;&#65292;&#20351;&#20195;&#29702;&#21487;&#20197;&#24555;&#36895;&#24182;&#34892;&#23398;&#20064;&#19981;&#21516;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLM&#33719;&#24471;&#20219;&#21153;&#25551;&#36848;&#21644;&#27169;&#25311;&#22120;&#30446;&#26631;&#65292;&#28982;&#21518;&#34987;&#35201;&#27714;&#29983;&#25104;&#19968;&#32452;&#29615;&#22659;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12014v1 Announce Type: cross  Abstract: Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive. Instead of directly employing LLMs as agents, can we use LLMs' reasoning capabilities to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at? We propose EnvGen, a novel framework to address this question. First, we prompt an LLM to generate training environments that allow agents to quickly learn different tasks in parallel. Concretely, the LLM is given the task description and simulator objectives that the agents should learn and is then asked to generate a set of environment configurations (e.g
&lt;/p&gt;</description></item><item><title>&#20174;&#25511;&#21046;&#29702;&#35770;&#30340;&#35282;&#24230;&#65292;&#25552;&#20379;&#20102;Roesser&#31867;&#22411;&#30340;2-D&#21367;&#31215;&#23618;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#65292;&#20855;&#26377;&#26368;&#23567;&#21270;&#30340;&#29366;&#24577;&#25968;&#37327;&#65292;&#22312;$c_\mathrm{in}=c_\mathrm{out}$&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#65292;&#24182;&#36827;&#19968;&#27493;&#23454;&#29616;&#20102;&#25193;&#24352;&#12289;&#36328;&#36234;&#21644;N-D&#21367;&#31215;&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.11938</link><description>&lt;p&gt;
Roesser&#31867;&#22411;&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#29992;&#20110;&#21367;&#31215;&#23618;
&lt;/p&gt;
&lt;p&gt;
State space representations of the Roesser type for convolutional layers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11938
&lt;/p&gt;
&lt;p&gt;
&#20174;&#25511;&#21046;&#29702;&#35770;&#30340;&#35282;&#24230;&#65292;&#25552;&#20379;&#20102;Roesser&#31867;&#22411;&#30340;2-D&#21367;&#31215;&#23618;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#65292;&#20855;&#26377;&#26368;&#23567;&#21270;&#30340;&#29366;&#24577;&#25968;&#37327;&#65292;&#22312;$c_\mathrm{in}=c_\mathrm{out}$&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#65292;&#24182;&#36827;&#19968;&#27493;&#23454;&#29616;&#20102;&#25193;&#24352;&#12289;&#36328;&#36234;&#21644;N-D&#21367;&#31215;&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25511;&#21046;&#29702;&#35770;&#30340;&#35282;&#24230;&#30475;&#65292;&#21367;&#31215;&#23618;&#65288;&#31070;&#32463;&#32593;&#32476;&#30340;&#65289;&#26159;2-D&#65288;&#25110;N-D&#65289;&#32447;&#24615;&#26102;&#19981;&#21464;&#21160;&#24577;&#31995;&#32479;&#12290;&#21367;&#31215;&#23618;&#36890;&#24120;&#36890;&#36807;&#21367;&#31215;&#26680;&#34920;&#31034;&#65292;&#23545;&#24212;&#20110;&#21160;&#24577;&#31995;&#32479;&#36890;&#36807;&#20854;&#33033;&#20914;&#21709;&#24212;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#25511;&#21046;&#29702;&#35770;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#20363;&#22914;&#28041;&#21450;&#32447;&#24615;&#30697;&#38453;&#19981;&#31561;&#24335;&#30340;&#24037;&#20855;&#65292;&#38656;&#35201;&#19968;&#20010;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26126;&#30830;&#25552;&#20379;&#20102;Roesser&#31867;&#22411;&#30340;2-D&#21367;&#31215;&#23618;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#65292;&#20855;&#26377;$c_\mathrm{in}r_1+c_\mathrm{out}r_2$&#20010;&#29366;&#24577;&#65292;&#20854;&#20013;$c_\mathrm{in}/c_\mathrm{out}$&#26159;&#23618;&#30340;&#36755;&#20837;/&#36755;&#20986;&#36890;&#36947;&#25968;&#65292;$r_1/r_2$ &#34920;&#31034;&#21367;&#31215;&#26680;&#30340;&#23485;&#24230;/&#38271;&#24230;&#12290;&#23545;&#20110;$c_\mathrm{in}=c_\mathrm{out}$&#65292;&#24050;&#32463;&#35777;&#26126;&#36825;&#31181;&#34920;&#31034;&#26159;&#26368;&#23567;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#25193;&#24352;&#12289;&#36328;&#36234;&#21644;N-D&#21367;&#31215;&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11938v1 Announce Type: cross  Abstract: From the perspective of control theory, convolutional layers (of neural networks) are 2-D (or N-D) linear time-invariant dynamical systems. The usual representation of convolutional layers by the convolution kernel corresponds to the representation of a dynamical system by its impulse response. However, many analysis tools from control theory, e.g., involving linear matrix inequalities, require a state space representation. For this reason, we explicitly provide a state space representation of the Roesser type for 2-D convolutional layers with $c_\mathrm{in}r_1 + c_\mathrm{out}r_2$ states, where $c_\mathrm{in}$/$c_\mathrm{out}$ is the number of input/output channels of the layer and $r_1$/$r_2$ characterizes the width/length of the convolution kernel. This representation is shown to be minimal for $c_\mathrm{in} = c_\mathrm{out}$. We further construct state space representations for dilated, strided, and N-D convolutions.
&lt;/p&gt;</description></item><item><title>StainFuser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#23558;&#26579;&#33394;&#26631;&#20934;&#21270;&#38382;&#39064;&#35270;&#20026;&#39118;&#26684;&#36801;&#31227;&#20219;&#21153;&#65292;&#26080;&#38656;&#25163;&#24037;&#21046;&#20316;&#39068;&#33394;&#32452;&#20998;&#65292;&#22312;2&#30334;&#19975;&#22810;&#20010;&#32452;&#32455;&#23398;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#32467;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09302</link><description>&lt;p&gt;
StainFuser&#65306;&#22312;&#22810;&#21513;&#21152;&#20687;&#32032;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#25511;&#21046;&#25193;&#25955;&#20197;&#21152;&#24555;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
StainFuser: Controlling Diffusion for Faster Neural Style Transfer in Multi-Gigapixel Histology Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09302
&lt;/p&gt;
&lt;p&gt;
StainFuser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#23558;&#26579;&#33394;&#26631;&#20934;&#21270;&#38382;&#39064;&#35270;&#20026;&#39118;&#26684;&#36801;&#31227;&#20219;&#21153;&#65292;&#26080;&#38656;&#25163;&#24037;&#21046;&#20316;&#39068;&#33394;&#32452;&#20998;&#65292;&#22312;2&#30334;&#19975;&#22810;&#20010;&#32452;&#32455;&#23398;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#32467;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26579;&#33394;&#26631;&#20934;&#21270;&#31639;&#27861;&#26088;&#22312;&#23558;&#28304;&#22810;&#21513;&#21152;&#20687;&#32032;&#32452;&#32455;&#23398;&#22270;&#20687;&#30340;&#39068;&#33394;&#21644;&#24378;&#24230;&#29305;&#24449;&#36716;&#25442;&#20026;&#19982;&#30446;&#26631;&#22270;&#20687;&#30456;&#21305;&#37197;&#65292;&#20174;&#32780;&#20943;&#36731;&#22270;&#20687;&#20013;&#29992;&#20110;&#31361;&#20986;&#26174;&#31034;&#32454;&#32990;&#32452;&#20998;&#30340;&#26579;&#33394;&#21058;&#22806;&#35266;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;StainFuser&#65292;&#23558;&#36825;&#20010;&#38382;&#39064;&#35270;&#20026;&#19968;&#20010;&#39118;&#26684;&#36801;&#31227;&#20219;&#21153;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#28040;&#38500;&#20102;&#25163;&#24037;&#21046;&#20316;&#39068;&#33394;&#32452;&#20998;&#30340;&#38656;&#35201;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#20026;&#39640;&#36136;&#37327;&#36716;&#25442;&#31579;&#36873;&#20102;&#36804;&#20170;&#20026;&#27490;&#21253;&#21547;&#36229;&#36807;200&#19975;&#20010;&#32452;&#32455;&#23398;&#22270;&#20687;&#30340;&#26368;&#22823;&#26579;&#33394;&#26631;&#20934;&#21270;&#25968;&#25454;&#38598;SPI-2M&#65292;&#24182;&#36827;&#34892;&#20102;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#35757;&#32451;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#35757;&#32451;&#21518;&#65292;StainFuser&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;GAN&#21644;&#25163;&#24037;&#21046;&#20316;&#26041;&#27861;&#30340;&#26631;&#20934;&#21270;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#29992;&#20316;te&#26102;&#65292;&#23427;&#25913;&#21892;&#20102;&#32454;&#32990;&#26680;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09302v1 Announce Type: cross  Abstract: Stain normalization algorithms aim to transform the color and intensity characteristics of a source multi-gigapixel histology image to match those of a target image, mitigating inconsistencies in the appearance of stains used to highlight cellular components in the images. We propose a new approach, StainFuser, which treats this problem as a style transfer task using a novel Conditional Latent Diffusion architecture, eliminating the need for handcrafted color components. With this method, we curate SPI-2M the largest stain normalization dataset to date of over 2 million histology images with neural style transfer for high-quality transformations. Trained on this data, StainFuser outperforms current state-of-the-art GAN and handcrafted methods in terms of the quality of normalized images. Additionally, compared to existing approaches, it improves the performance of nuclei instance segmentation and classification models when used as a te
&lt;/p&gt;</description></item><item><title>&#20998;&#21449;&#27880;&#24847;&#21147;&#26159;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#21333;&#19978;&#19979;&#25991;&#25209;&#37327;&#25277;&#26679;&#29615;&#22659;&#24320;&#21457;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20998;&#25104;&#20004;&#20010;&#29420;&#31435;&#30340;&#25805;&#20316;&#26469;&#20943;&#23569;&#20887;&#20313;&#20869;&#23384;IO&#25104;&#26412;&#65292;&#25552;&#39640;&#25928;&#29575;&#24182;&#38477;&#20302;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2403.08845</link><description>&lt;p&gt;
&#21333;&#19978;&#19979;&#25991;&#22823;&#25209;&#37327;&#25277;&#26679;&#30340;&#20998;&#21449;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Bifurcated Attention for Single-Context Large-Batch Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08845
&lt;/p&gt;
&lt;p&gt;
&#20998;&#21449;&#27880;&#24847;&#21147;&#26159;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#21333;&#19978;&#19979;&#25991;&#25209;&#37327;&#25277;&#26679;&#29615;&#22659;&#24320;&#21457;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20998;&#25104;&#20004;&#20010;&#29420;&#31435;&#30340;&#25805;&#20316;&#26469;&#20943;&#23569;&#20887;&#20313;&#20869;&#23384;IO&#25104;&#26412;&#65292;&#25552;&#39640;&#25928;&#29575;&#24182;&#38477;&#20302;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#21449;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21333;&#19978;&#19979;&#25991;&#25209;&#37327;&#25277;&#26679;&#29615;&#22659;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20943;&#23569;&#20887;&#20313;&#30340;&#20869;&#23384;IO&#25104;&#26412;&#65292;&#36825;&#26159;&#39640;&#25209;&#37327;&#22823;&#23567;&#21644;&#38271;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#24310;&#36831;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#20998;&#21449;&#27880;&#24847;&#21147;&#36890;&#36807;&#22312;&#22686;&#37327;&#35299;&#30721;&#26399;&#38388;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#21010;&#20998;&#20026;&#20004;&#20010;&#19981;&#21516;&#30340;GEMM&#25805;&#20316;&#65292;&#20998;&#21035;&#19987;&#27880;&#20110;&#26469;&#33258;&#39044;&#22635;&#20805;&#30340;KV&#32531;&#23384;&#20197;&#21450;&#35299;&#30721;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#30830;&#20445;&#20102;&#31934;&#30830;&#30340;&#35745;&#31639;&#65292;&#24182;&#32500;&#25345;&#24120;&#35268;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35745;&#31639;&#36127;&#36733;&#65288;FLOPs&#65289;&#65292;&#20294;&#20943;&#23569;&#20102;&#20869;&#23384;IO&#12290;&#20998;&#21449;&#27880;&#24847;&#21147;&#36824;&#19982;&#20943;&#23569;KV&#32531;&#23384;&#20869;&#23384;IO&#24050;&#30693;&#30340;&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#26426;&#21046;&#20860;&#23481;&#65292;&#36827;&#19968;&#27493;&#23454;&#29616;&#26356;&#39640;&#30340;&#25209;&#37327;&#22823;&#23567;&#21644;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#30001;&#27492;&#24102;&#26469;&#30340;&#25928;&#29575;&#23548;&#33268;&#26356;&#20302;&#30340;&#24310;&#36831;&#65292;&#25913;&#21892;&#20102;&#23454;&#26102;&#24212;&#29992;&#30340;&#36866;&#29992;&#24615;&#65292;&#20363;&#22914;&#23454;&#29616;&#22823;&#35268;&#27169;&#24182;&#34892;&#30340;&#31572;&#26696;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08845v1 Announce Type: cross  Abstract: In our study, we present bifurcated attention, a method developed for language model inference in single-context batch sampling contexts. This approach aims to reduce redundant memory IO costs, a significant factor in latency for high batch sizes and long context lengths. Bifurcated attention achieves this by dividing the attention mechanism during incremental decoding into two distinct GEMM operations, focusing on the KV cache from prefill and the decoding process. This method ensures precise computation and maintains the usual computational load (FLOPs) of standard attention mechanisms, but with reduced memory IO. Bifurcated attention is also compatible with multi-query attention mechanism known for reduced memory IO for KV cache, further enabling higher batch size and context length. The resulting efficiency leads to lower latency, improving suitability for real-time applications, e.g., enabling massively-parallel answer generation 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;LD3M&#65289;&#65292;&#32467;&#21512;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#21644;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#21644;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.03881</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Latent Dataset Distillation with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03881
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;LD3M&#65289;&#65292;&#32467;&#21512;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#21644;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#21644;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#25968;&#25454;&#38598;&#24102;&#26469;&#23384;&#20648;&#25361;&#25112;&#65292;&#24182;&#19988;&#21253;&#21547;&#19968;&#20123;&#38750;&#24433;&#21709;&#21147;&#26679;&#26412;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#34987;&#24573;&#30053;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#26368;&#32456;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#20986;&#29616;&#20102;&#23558;&#25968;&#25454;&#38598;&#20449;&#24687;&#33976;&#39311;&#25104;&#19968;&#32452;&#21387;&#32553;&#26679;&#26412;&#65288;&#21512;&#25104;&#26679;&#26412;&#65289;&#65292;&#21363;&#33976;&#39311;&#25968;&#25454;&#38598;&#30340;&#27010;&#24565;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#36873;&#25321;&#29992;&#20110;&#36830;&#25509;&#21407;&#22987;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26550;&#26500;&#65288;&#36890;&#24120;&#26159;ConvNet&#65289;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#26550;&#26500;&#19982;&#33976;&#39311;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#21017;&#26368;&#32456;&#20934;&#30830;&#24615;&#20250;&#38477;&#20302;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#20363;&#22914;128x128&#21450;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03881v1 Announce Type: cross  Abstract: The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets. However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model. In response to these limitations, the concept of distilling the information on a dataset into a condensed set of (synthetic) samples, namely a distilled dataset, emerged. One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets. However, the final accuracy is lower if the employed model architecture differs from the model used during distillation. Another challenge is the generation of high-resolution images, e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation with Diffusion Models (LD3M) that combine diffusion in latent space with dataset distillation to tackle both chal
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37325;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#23548;&#33268;&#20102;&#20248;&#21270;&#21160;&#24577;&#19978;&#30340;&#22256;&#38590;&#65292;Adam&#21644;&#22522;&#20110;&#31526;&#21495;&#30340;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20248;&#20110;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.19449</link><description>&lt;p&gt;
Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models
&lt;/p&gt;
&lt;p&gt;
Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19449
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37325;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#23548;&#33268;&#20102;&#20248;&#21270;&#21160;&#24577;&#19978;&#30340;&#22256;&#38590;&#65292;Adam&#21644;&#22522;&#20110;&#31526;&#21495;&#30340;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20248;&#20110;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#37325;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;Adam&#22312;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#34920;&#29616;&#20248;&#20110;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#20110;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#37325;&#23614;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26102;&#65292;&#19982;&#19981;&#24120;&#35265;&#21333;&#35789;&#30456;&#20851;&#30340;&#25439;&#22833;&#19979;&#38477;&#36895;&#24230;&#27604;&#19982;&#24120;&#35265;&#21333;&#35789;&#30456;&#20851;&#30340;&#25439;&#22833;&#19979;&#38477;&#36895;&#24230;&#24930;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#26679;&#26412;&#26469;&#33258;&#30456;&#23545;&#19981;&#24120;&#35265;&#30340;&#21333;&#35789;&#65292;&#24179;&#22343;&#25439;&#22833;&#20540;&#22312;&#26799;&#24230;&#19979;&#38477;&#26102;&#19979;&#38477;&#36895;&#24230;&#36739;&#24930;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;Adam&#21644;&#22522;&#20110;&#31526;&#21495;&#30340;&#26041;&#27861;&#21364;&#19981;&#21463;&#27492;&#38382;&#39064;&#24433;&#21709;&#65292;&#24182;&#25913;&#21892;&#20102;&#25152;&#26377;&#31867;&#21035;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#26550;&#26500;&#21644;&#25968;&#25454;&#31867;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#34892;&#20026;&#30830;&#23454;&#26159;&#30001;&#31867;&#21035;&#19981;&#24179;&#34913;&#24341;&#36215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19449v1 Announce Type: cross  Abstract: Adam has been shown to outperform gradient descent in optimizing large language transformers empirically, and by a larger margin than on other tasks, but it is unclear why this happens. We show that the heavy-tailed class imbalance found in language modeling tasks leads to difficulties in the optimization dynamics. When training with gradient descent, the loss associated with infrequent words decreases slower than the loss associated with frequent ones. As most samples come from relatively infrequent words, the average loss decreases slowly with gradient descent. On the other hand, Adam and sign-based methods do not suffer from this problem and improve predictions on all classes. To establish that this behavior is indeed caused by class imbalance, we show empirically that it persist through different architectures and data types, on language transformers, vision CNNs, and linear models. We further study this phenomenon on a linear clas
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#29992;&#20110;&#32479;&#19968;&#19981;&#21516;&#27169;&#22411;&#30340;&#20219;&#21153;&#23884;&#20837;&#65292;&#20351;&#24471;&#20219;&#21153;&#23884;&#20837;&#21487;&#20197;&#36328;&#36234;&#21508;&#31181;&#27169;&#22411;&#65292;&#24182;&#22312;&#21333;&#19968;&#21521;&#37327;&#31354;&#38388;&#20869;&#36827;&#34892;&#27604;&#36739;&#21644;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.14522</link><description>&lt;p&gt;
&#36328;&#36234;&#22810;&#20010;&#27169;&#22411;&#30340;&#32479;&#19968;&#20219;&#21153;&#23884;&#20837;&#65306;&#24357;&#21512;&#22522;&#20110;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#23427;&#27169;&#22411;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14522
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#29992;&#20110;&#32479;&#19968;&#19981;&#21516;&#27169;&#22411;&#30340;&#20219;&#21153;&#23884;&#20837;&#65292;&#20351;&#24471;&#20219;&#21153;&#23884;&#20837;&#21487;&#20197;&#36328;&#36234;&#21508;&#31181;&#27169;&#22411;&#65292;&#24182;&#22312;&#21333;&#19968;&#21521;&#37327;&#31354;&#38388;&#20869;&#36827;&#34892;&#27604;&#36739;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23884;&#20837;&#26159;&#19968;&#31181;&#25429;&#25417;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#30340;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#27169;&#22411;&#32534;&#36753;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#39046;&#22495;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32479;&#19968;&#20219;&#21153;&#23884;&#20837;&#65288;FUTE&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#21327;&#35843;&#26469;&#33258;&#21508;&#31181;&#27169;&#22411;&#65288;&#21253;&#25324;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20855;&#26377;&#19981;&#21516;&#25552;&#31034;&#30340;LLMs&#65289;&#30340;&#20219;&#21153;&#23884;&#20837;&#65292;&#20351;&#20854;&#22788;&#20110;&#21333;&#19968;&#21521;&#37327;&#31354;&#38388;&#12290;&#36825;&#31181;&#32479;&#19968;&#24615;&#20351;&#24471;&#21487;&#20197;&#27604;&#36739;&#21644;&#20998;&#26512;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#20219;&#21153;&#23884;&#20837;&#26041;&#27861;&#22312;&#35299;&#20915;&#22810;&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#33539;&#22260;&#21644;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14522v1 Announce Type: new  Abstract: Task embedding, a meta-learning technique that captures task-specific information, has become prevalent, especially in areas such as multi-task learning, model editing, and interpretability. However, it faces challenges with the emergence of prompt-guided Large Language Models (LLMs) operating in a gradientfree manner. Existing task embedding methods rely on fine-tuned, task-specific language models, which hinders the adaptability of task embeddings across diverse models, especially prompt-based LLMs. To unleash the power of task embedding in the era of LLMs, we propose a framework for unified task embeddings (FUTE), harmonizing task embeddings from various models, including smaller language models and LLMs with varied prompts, within a single vector space. Such uniformity enables the comparison and analysis of similarities amongst different models, extending the scope and utility of existing task embedding methods in addressing multi-mo
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#19968;&#31867;&#24102;&#26377;&#38271;&#26399;&#32422;&#26463;&#30340;&#22312;&#32447;&#24230;&#37327;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22312;&#21487;&#25345;&#32493;&#33021;&#28304;&#21644;&#35745;&#31639;&#31995;&#32479;&#20013;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#24212;&#29992;&#20013;&#30340;&#26368;&#20248;&#31454;&#20105;&#31639;&#27861;&#21644;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.14012</link><description>&lt;p&gt;
&#22312;&#28385;&#36275;&#38271;&#26399;&#32422;&#26463;&#26465;&#20214;&#19979;&#36861;&#36880;&#20984;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Chasing Convex Functions with Long-term Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14012
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#19968;&#31867;&#24102;&#26377;&#38271;&#26399;&#32422;&#26463;&#30340;&#22312;&#32447;&#24230;&#37327;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22312;&#21487;&#25345;&#32493;&#33021;&#28304;&#21644;&#35745;&#31639;&#31995;&#32479;&#20013;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#24212;&#29992;&#20013;&#30340;&#26368;&#20248;&#31454;&#20105;&#31639;&#27861;&#21644;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#19968;&#31867;&#24102;&#26377;&#38271;&#26399;&#32422;&#26463;&#30340;&#22312;&#32447;&#24230;&#37327;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#19968;&#20010;&#22312;&#32447;&#29609;&#23478;&#22312;&#24230;&#37327;&#31354;&#38388;$(X,d)$&#20013;&#20570;&#20986;&#20915;&#31574;$\mathbf{x}_t$&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20182;&#20204;&#30340;&#21629;&#20013;&#25104;&#26412;$f_t(\mathbf{x}_t)$&#21644;&#30001;&#24230;&#37327;&#30830;&#23450;&#30340;&#20999;&#25442;&#25104;&#26412;&#12290;&#22312;&#26102;&#38388;&#36328;&#24230;$T$&#20869;&#65292;&#29609;&#23478;&#24517;&#39035;&#28385;&#36275;&#38271;&#26399;&#38656;&#27714;&#32422;&#26463;$\sum_{t} c(\mathbf{x}_t) \geq 1$&#65292;&#20854;&#20013;$c(\mathbf{x}_t)$&#34920;&#31034;&#26102;&#38388;$t$&#26102;&#28385;&#36275;&#30340;&#38656;&#27714;&#27604;&#20363;&#12290;&#36825;&#31867;&#38382;&#39064;&#22312;&#21487;&#25345;&#32493;&#33021;&#28304;&#21644;&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#20013;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#20855;&#20307;&#23454;&#20363;&#35774;&#35745;&#20102;&#26368;&#20248;&#30340;&#31454;&#20105;&#31639;&#27861;&#21644;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14012v1 Announce Type: cross  Abstract: We introduce and study a family of online metric problems with long-term constraints. In these problems, an online player makes decisions $\mathbf{x}_t$ in a metric space $(X,d)$ to simultaneously minimize their hitting cost $f_t(\mathbf{x}_t)$ and switching cost as determined by the metric. Over the time horizon $T$, the player must satisfy a long-term demand constraint $\sum_{t} c(\mathbf{x}_t) \geq 1$, where $c(\mathbf{x}_t)$ denotes the fraction of demand satisfied at time $t$. Such problems can find a wide array of applications to online resource allocation in sustainable energy and computing systems. We devise optimal competitive and learning-augmented algorithms for specific instantiations of these problems, and further show that our proposed algorithms perform well in numerical experiments.
&lt;/p&gt;</description></item><item><title>STENCIL&#21033;&#29992;&#27425;&#27169;&#20114;&#20449;&#24687;&#36873;&#25321;&#24369;&#26631;&#35760;&#30340;&#31232;&#26377;&#31867;&#23454;&#20363;&#65292;&#24182;&#36890;&#36807;&#26631;&#27880;&#32773;&#24378;&#26631;&#35760;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#21644;&#31232;&#26377;&#31867;F-1&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.13468</link><description>&lt;p&gt;
STENCIL&#65306;&#22522;&#20110;&#27425;&#27169;&#20114;&#20449;&#24687;&#30340;&#20919;&#21551;&#21160;&#20027;&#21160;&#23398;&#20064;&#24369;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13468
&lt;/p&gt;
&lt;p&gt;
STENCIL&#21033;&#29992;&#27425;&#27169;&#20114;&#20449;&#24687;&#36873;&#25321;&#24369;&#26631;&#35760;&#30340;&#31232;&#26377;&#31867;&#23454;&#20363;&#65292;&#24182;&#36890;&#36807;&#26631;&#27880;&#32773;&#24378;&#26631;&#35760;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#21644;&#31232;&#26377;&#31867;F-1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;NLP&#24212;&#29992;&#20013;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#38656;&#35201;&#26356;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#35745;&#25968;&#22686;&#21152;&#26102;&#12290;&#20027;&#21160;&#23398;&#20064;&#35797;&#22270;&#25366;&#25496;&#21644;&#27880;&#37322;&#26410;&#26631;&#35760;&#30340;&#23454;&#20363;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#24555;&#36895;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#65292;&#26159;&#20943;&#23569;&#27880;&#37322;&#25104;&#26412;&#30340;&#24120;&#35265;&#36873;&#25321;&#65307;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#24120;&#24573;&#35270;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#24182;&#19988;&#35201;&#20040;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#21021;&#22987;&#26631;&#27880;&#25968;&#25454;&#65292;&#35201;&#20040;&#35201;&#27714;&#25913;&#36827;&#31232;&#26377;&#31867;&#20043;&#21069;&#38656;&#35201;&#22810;&#36718;&#20027;&#21160;&#23398;&#20064;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STENCIL&#65292;&#23427;&#21033;&#29992;&#19968;&#32452;&#25991;&#26412;&#31034;&#20363;&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;&#27425;&#27169;&#20114;&#20449;&#24687;&#26469;&#36873;&#25321;&#19968;&#32452;&#24369;&#26631;&#35760;&#30340;&#31232;&#26377;&#31867;&#23454;&#20363;&#65292;&#28982;&#21518;&#30001;&#26631;&#27880;&#32773;&#23545;&#20854;&#36827;&#34892;&#24378;&#26631;&#35760;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;STENCIL&#22312;&#22810;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23558;&#25972;&#20307;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;10%-24%&#65292;&#23558;&#31232;&#26377;&#31867;F-1&#20998;&#25968;&#25552;&#39640;&#20102;17%-40%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13468v1 Announce Type: cross  Abstract: As supervised fine-tuning of pre-trained models within NLP applications increases in popularity, larger corpora of annotated data are required, especially with increasing parameter counts in large language models. Active learning, which attempts to mine and annotate unlabeled instances to improve model performance maximally fast, is a common choice for reducing the annotation cost; however, most methods typically ignore class imbalance and either assume access to initial annotated data or require multiple rounds of active learning selection before improving rare classes. We present STENCIL, which utilizes a set of text exemplars and the recently proposed submodular mutual information to select a set of weakly labeled rare-class instances that are then strongly labeled by an annotator. We show that STENCIL improves overall accuracy by $10\%-24\%$ and rare-class F-1 score by $17\%-40\%$ on multiple text classification datasets over commo
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#22238;&#28779;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21487;&#25913;&#21892;&#27010;&#29575;&#25968;&#20540;&#26041;&#27861;&#22312;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#21442;&#25968;&#20248;&#21270;&#25910;&#25947;&#24615;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#21442;&#25968;&#30340;&#21487;&#38752;&#20272;&#35745;</title><link>https://arxiv.org/abs/2402.12231</link><description>&lt;p&gt;
&#25193;&#25955;&#22238;&#28779;&#25913;&#21892;&#27010;&#29575;&#31215;&#20998;&#22120;&#23545;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#21442;&#25968;&#20272;&#35745;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Diffusion Tempering Improves Parameter Estimation with Probabilistic Integrators for Ordinary Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12231
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#22238;&#28779;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21487;&#25913;&#21892;&#27010;&#29575;&#25968;&#20540;&#26041;&#27861;&#22312;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#21442;&#25968;&#20248;&#21270;&#25910;&#25947;&#24615;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#21442;&#25968;&#30340;&#21487;&#38752;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25551;&#36848;&#31185;&#23398;&#20013;&#30340;&#21160;&#24577;&#31995;&#32479;&#65292;&#20294;&#30830;&#23450;&#35299;&#37322;&#23454;&#39564;&#27979;&#37327;&#32467;&#26524;&#30340;&#21442;&#25968;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#25955;&#22238;&#28779;&#36825;&#19968;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23427;&#38024;&#23545;ODEs&#20013;&#30340;&#27010;&#29575;&#25968;&#20540;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#26799;&#24230;&#20248;&#21270;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#24615;&#12290;&#36890;&#36807;&#36845;&#20195;&#20943;&#23569;&#27010;&#29575;&#31215;&#20998;&#22120;&#30340;&#19968;&#20010;&#22122;&#22768;&#21442;&#25968;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26356;&#21487;&#38752;&#22320;&#25910;&#25947;&#21040;&#30495;&#23454;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#19981;&#21516;&#22797;&#26434;&#24615;&#30340;&#21160;&#24577;&#31995;&#32479;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#23637;&#31034;&#23427;&#23545;&#20110;&#20855;&#26377;&#23454;&#38469;&#30456;&#20851;&#21442;&#25968;&#25968;&#37327;&#30340;Hodgkin-Huxley&#27169;&#22411;&#33719;&#24471;&#21487;&#38752;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12231v1 Announce Type: new  Abstract: Ordinary differential equations (ODEs) are widely used to describe dynamical systems in science, but identifying parameters that explain experimental measurements is challenging. In particular, although ODEs are differentiable and would allow for gradient-based parameter optimization, the nonlinear dynamics of ODEs often lead to many local minima and extreme sensitivity to initial conditions. We therefore propose diffusion tempering, a novel regularization technique for probabilistic numerical methods which improves convergence of gradient-based parameter optimization in ODEs. By iteratively reducing a noise parameter of the probabilistic integrator, the proposed method converges more reliably to the true parameters. We demonstrate that our method is effective for dynamical systems of different complexity and show that it obtains reliable parameter estimates for a Hodgkin-Huxley model with a practically relevant number of parameters.
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#25351;&#20196;&#35843;&#20248;&#26159;&#19968;&#20010;&#22686;&#24378;&#20854;&#23454;&#29992;&#24615;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#26696;&#24573;&#35270;&#20102;&#29983;&#25104;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SafeCoder&#65292;&#36890;&#36807;&#23433;&#20840;&#24494;&#35843;&#21644;&#26631;&#20934;&#25351;&#20196;&#35843;&#20248;&#30456;&#32467;&#21512;&#65292;&#26469;&#20248;&#21270;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09497</link><description>&lt;p&gt;
&#23433;&#20840;&#20195;&#30721;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning for Secure Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09497
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#31243;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#25351;&#20196;&#35843;&#20248;&#26159;&#19968;&#20010;&#22686;&#24378;&#20854;&#23454;&#29992;&#24615;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#26696;&#24573;&#35270;&#20102;&#29983;&#25104;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SafeCoder&#65292;&#36890;&#36807;&#23433;&#20840;&#24494;&#35843;&#21644;&#26631;&#20934;&#25351;&#20196;&#35843;&#20248;&#30456;&#32467;&#21512;&#65292;&#26469;&#20248;&#21270;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;(LMs)&#22312;&#26085;&#24120;&#21644;&#19987;&#19994;&#29615;&#22659;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#35748;&#21487;&#65292;&#23588;&#20854;&#22312;&#32534;&#31243;&#20013;&#12290;&#25351;&#20196;&#35843;&#20248;&#26159;&#19968;&#31181;&#20851;&#38190;&#30340;&#36807;&#31243;&#65292;&#36890;&#36807;&#35757;&#32451;LMs&#36981;&#24490;&#29992;&#25143;&#25351;&#20196;&#21644;&#20154;&#31867;&#20559;&#22909;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#24378;&#20102;LMs&#30340;&#23454;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#26696;&#24573;&#35270;&#20102;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#29983;&#25104;&#20195;&#30721;&#30340;&#23433;&#20840;&#24615;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#25351;&#20196;&#35843;&#20248;&#30340;LMs&#20063;&#32463;&#24120;&#20135;&#29983;&#19981;&#23433;&#20840;&#30340;&#20195;&#30721;&#65292;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SafeCoder&#26469;&#22635;&#34917;&#36825;&#20010;&#24046;&#36317;&#12290;SafeCoder&#20351;&#29992;&#19968;&#20010;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23433;&#20840;&#20026;&#20013;&#24515;&#30340;&#24494;&#35843;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#25910;&#38598;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23558;&#23433;&#20840;&#24494;&#35843;&#19982;&#26631;&#20934;&#30340;&#25351;&#20196;&#35843;&#20248;&#30456;&#32467;&#21512;&#65292;&#20197;&#20415;&#21516;&#26102;&#20248;&#21270;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;SafeCoder&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09497v1 Announce Type: cross  Abstract: Modern language models (LMs) have gained widespread acceptance in everyday and professional contexts, particularly in programming. An essential procedure enabling this adoption is instruction tuning, which substantially enhances LMs' practical utility by training them to follow user instructions and human preferences. However, existing instruction tuning schemes overlook a crucial aspect: the security of generated code. As a result, even the state-of-the-art instruction-tuned LMs frequently produce unsafe code, posing significant security risks. In this work, we introduce SafeCoder to address this gap. SafeCoder performs security-centric fine-tuning using a diverse and high-quality dataset that we collected using an automated pipeline. We integrate the security fine-tuning with standard instruction tuning, to facilitate a joint optimization of both security and utility. Despite its simplicity, we show that SafeCoder is effective across
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#21463;&#38480;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#35774;&#32622;&#25439;&#22833;&#19978;&#38480;&#26469;&#23547;&#25214;&#26368;&#20339;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24179;&#22343;&#24615;&#33021;&#20248;&#21270;&#23548;&#33268;&#29305;&#23450;&#26102;&#38388;&#27493;&#39588;&#19978;&#35823;&#24046;&#36807;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09373</link><description>&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#25439;&#22833;&#22609;&#36896;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Loss Shaping Constraints for Long-Term Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09373
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#21463;&#38480;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#35774;&#32622;&#25439;&#22833;&#19978;&#38480;&#26469;&#23547;&#25214;&#26368;&#20339;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24179;&#22343;&#24615;&#33021;&#20248;&#21270;&#23548;&#33268;&#29305;&#23450;&#26102;&#38388;&#27493;&#39588;&#19978;&#35823;&#24046;&#36807;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#39044;&#27979;&#22810;&#20010;&#27493;&#39588;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#26377;&#22823;&#37327;&#30340;&#25991;&#29486;&#65292;&#20294;&#32463;&#20856;&#21644;&#26368;&#36817;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#26368;&#23567;&#21270;&#39044;&#27979;&#31383;&#21475;&#19978;&#30340;&#24615;&#33021;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#39044;&#27979;&#27493;&#39588;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#30340;&#38169;&#35823;&#20998;&#24067;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22312;&#24120;&#35265;&#39044;&#27979;&#22522;&#20934;&#19978;&#35757;&#32451;&#30340;&#26368;&#36817;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#24179;&#22343;&#24615;&#33021;&#20248;&#21270;&#21487;&#33021;&#23548;&#33268;&#29305;&#23450;&#26102;&#38388;&#27493;&#39588;&#19978;&#30340;&#38169;&#35823;&#36807;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#21463;&#38480;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25214;&#21040;&#22312;&#24179;&#22343;&#24615;&#33021;&#19978;&#26368;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#20445;&#25345;&#29992;&#25143;&#23450;&#20041;&#30340;&#25439;&#22833;&#19978;&#38480;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;&#25439;&#22833;&#22609;&#36896;&#32422;&#26463;&#65292;&#22240;&#20026;&#23427;&#23545;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#30340;&#25439;&#22833;&#26045;&#21152;&#32422;&#26463;&#65292;&#24182;&#21033;&#29992;&#26368;&#36817;&#30340;&#23545;&#20598;&#24615;&#32467;&#26524;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09373v1 Announce Type: new Abstract: Several applications in time series forecasting require predicting multiple steps ahead. Despite the vast amount of literature in the topic, both classical and recent deep learning based approaches have mostly focused on minimising performance averaged over the predicted window. We observe that this can lead to disparate distributions of errors across forecasting steps, especially for recent transformer architectures trained on popular forecasting benchmarks. That is, optimising performance on average can lead to undesirably large errors at specific time-steps. In this work, we present a Constrained Learning approach for long-term time series forecasting that aims to find the best model in terms of average performance that respects a user-defined upper bound on the loss at each time-step. We call our approach loss shaping constraints because it imposes constraints on the loss at each time step, and leverage recent duality results to show 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23545;&#27604;&#20102;&#20154;&#31867;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34892;&#20026;&#24046;&#24322;&#65292;&#21457;&#29616;&#20154;&#31867;&#20855;&#26377;&#21363;&#26102;&#27010;&#25324;&#33021;&#21147;&#65292;&#32780;DNNs&#23384;&#22312;&#28382;&#21518;&#27010;&#25324;&#29616;&#35937;&#65292;&#36825;&#34920;&#26126;&#20102;&#34920;&#31034;&#20998;&#27495;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.09303</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#30340;&#21363;&#26102;&#27010;&#25324;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28382;&#21518;&#27010;&#25324;&#8212;&#8212;&#34920;&#31034;&#20998;&#27495;&#30340;&#35777;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
Immediate generalisation in humans but a generalisation lag in deep neural networks$\unicode{x2014}$evidence for representational divergence?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09303
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#27604;&#20102;&#20154;&#31867;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34892;&#20026;&#24046;&#24322;&#65292;&#21457;&#29616;&#20154;&#31867;&#20855;&#26377;&#21363;&#26102;&#27010;&#25324;&#33021;&#21147;&#65292;&#32780;DNNs&#23384;&#22312;&#28382;&#21518;&#27010;&#25324;&#29616;&#35937;&#65292;&#36825;&#34920;&#26126;&#20102;&#34920;&#31034;&#20998;&#27495;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#22312;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#23545;&#27604;&#20102;&#20154;&#31867;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#35768;&#22810;&#34892;&#20026;&#27604;&#36739;&#12290;&#36890;&#24120;&#65292;&#27604;&#36739;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#23398;&#20064;&#36807;&#31243;&#30340;&#26368;&#32456;&#32467;&#26524;&#65292;&#36890;&#36807;&#27979;&#37327;&#21644;&#27604;&#36739;&#30446;&#26631;&#31867;&#21035;&#34920;&#31034;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34920;&#31034;&#22914;&#20309;&#24418;&#25104;&#21363;&#20854;&#36807;&#31243;&#8212;&#8212;&#21363;&#22312;&#33719;&#21462;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#21464;&#21270;&#21644;&#20013;&#38388;&#38454;&#27573;&#8212;&#8212;&#24448;&#24448;&#23569;&#26377;&#30452;&#25509;&#21644;&#23454;&#35777;&#30340;&#27604;&#36739;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#23545;&#20154;&#31867;&#35266;&#23519;&#32773;&#21644;&#19981;&#21516;&#32463;&#20856;&#19982;&#26368;&#26032;&#25216;&#26415;&#30340;DNNs&#20013;&#21487;&#36716;&#31227;&#34920;&#31034;&#26159;&#22914;&#20309;&#34987;&#33719;&#21462;&#30340;&#30340;&#35814;&#32454;&#35843;&#26597;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#38480;&#30340;&#30417;&#30563;&#23398;&#20064;&#29615;&#22659;&#65292;&#35813;&#29615;&#22659;&#20013;&#25105;&#20204;&#23545;&#40784;&#20102;&#23398;&#20064;&#30456;&#20851;&#30340;&#21442;&#25968;&#65292;&#22914;&#36215;&#22987;&#28857;&#12289;&#36755;&#20837;&#27169;&#24335;&#12289;&#21487;&#29992;&#36755;&#20837;&#25968;&#25454;&#20197;&#21450;&#25552;&#20379;&#30340;&#21453;&#39304;&#12290;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#25105;&#20204;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09303v1 Announce Type: cross Abstract: Recent research has seen many behavioral comparisons between humans and deep neural networks (DNNs) in the domain of image classification. Often, comparison studies focus on the end-result of the learning process by measuring and comparing the similarities in the representations of object categories once they have been formed. However, the process of how these representations emerge$\unicode{x2014}$that is, the behavioral changes and intermediate stages observed during the acquisition$\unicode{x2014}$is less often directly and empirically compared.   Here we report a detailed investigation of how transferable representations are acquired in human observers and various classic and state-of-the-art DNNs. We develop a constrained supervised learning environment in which we align learning-relevant parameters such as starting point, input modality, available input data and the feedback provided. Across the whole learning process we evaluate 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32769;&#34382;&#26426;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#21644;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22312;&#20943;&#23569;&#21518;&#24724;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06963</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;
&lt;/p&gt;
&lt;p&gt;
Tree Ensembles for Contextual Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32769;&#34382;&#26426;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#21644;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22312;&#20943;&#23569;&#21518;&#24724;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32769;&#34382;&#26426;&#26041;&#27861;&#65292;&#19978;&#20449;&#24515;&#30028;&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#65292;&#25972;&#21512;&#21040;&#26631;&#20934;&#21644;&#32452;&#21512;&#35774;&#32622;&#20013;&#12290;&#36890;&#36807;&#20351;&#29992;&#27969;&#34892;&#30340;&#26641;&#38598;&#25104;&#26041;&#27861;XGBoost&#36827;&#34892;&#22810;&#27425;&#23454;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#24403;&#24212;&#29992;&#20110;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#36947;&#36335;&#32593;&#32476;&#23548;&#33322;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#26102;&#65292;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20943;&#23569;&#21518;&#24724;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework for contextual multi-armed bandits based on tree ensembles. Our framework integrates two widely used bandit methods, Upper Confidence Bound and Thompson Sampling, for both standard and combinatorial settings. We demonstrate the effectiveness of our framework via several experimental studies, employing XGBoost, a popular tree ensemble method. Compared to state-of-the-art methods based on neural networks, our methods exhibit superior performance in terms of both regret minimization and computational runtime, when applied to benchmark datasets and the real-world application of navigation over road networks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.06165</link><description>&lt;p&gt;
&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#34920;&#31034;&#26469;&#36827;&#34892;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Contrastive Feature Representations for Facial Action Unit Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06165
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#65288;AU&#65289;&#26816;&#27979;&#30340;&#20027;&#35201;&#26041;&#27861;&#28041;&#21450;&#30417;&#30563;&#30340;&#22810;&#26631;&#31614;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24120;&#24120;&#23545;AU&#30340;&#20687;&#32032;&#32423;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#25552;&#20986;&#20102;&#24456;&#22823;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#22122;&#22768;AU&#26631;&#31614;&#65292;&#36825;&#31181;&#20570;&#27861;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#22686;&#24378;&#12290;&#30446;&#26631;&#26159;&#22312;AU&#26816;&#27979;&#39046;&#22495;&#20013;&#25670;&#33073;&#20256;&#32479;&#30340;&#20687;&#32032;&#32423;&#23398;&#20064;&#33539;&#24335;&#65292;&#33719;&#24471;&#21028;&#21035;&#29305;&#24449;&#12290;&#20026;&#20102;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#30417;&#30563;&#20449;&#21495;&#12290;&#36825;&#31181;&#22686;&#24378;&#26159;&#36890;&#36807;&#27491;&#26679;&#26412;&#25277;&#26679;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#27491;&#26679;&#26412;&#23545;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#27599;&#20010;AU&#31867;&#22411;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we empl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#65292;&#39564;&#35777;&#20102;GPT-4&#29983;&#25104;&#30340;&#21465;&#36848;&#22312;&#20256;&#36798;&#29983;&#27963;&#20107;&#20214;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#21465;&#36848;&#33021;&#22815;&#36275;&#22815;&#20256;&#36798;&#25552;&#31034;&#30340;&#24847;&#22270;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#21465;&#36848;&#12290;</title><link>https://arxiv.org/abs/2402.05435</link><description>&lt;p&gt;
GPT-4&#20351;&#29992;&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#29983;&#25104;&#29983;&#27963;&#20107;&#20214;&#30340;&#21465;&#36848;&#65306;&#19968;&#39033;&#39564;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#65292;&#39564;&#35777;&#20102;GPT-4&#29983;&#25104;&#30340;&#21465;&#36848;&#22312;&#20256;&#36798;&#29983;&#27963;&#20107;&#20214;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#21465;&#36848;&#33021;&#22815;&#36275;&#22815;&#20256;&#36798;&#25552;&#31034;&#30340;&#24847;&#22270;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#21465;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#21508;&#31181;&#21465;&#36848;&#26041;&#38754;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#20419;&#36827;&#20102;&#23545;&#20854;&#22312;&#21465;&#36848;&#24418;&#24335;&#20013;&#20256;&#36798;&#29983;&#27963;&#20107;&#20214;&#25928;&#26524;&#30340;&#31995;&#32479;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#38646;-shot&#32467;&#26500;&#21270;&#21465;&#20107;&#25552;&#31034;&#65292;&#20351;&#29992;OpenAI&#30340;GPT-4&#29983;&#25104;&#20102;24,000&#20010;&#21465;&#36848;&#12290;&#20174;&#36825;&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#25163;&#21160;&#20998;&#31867;&#20102;2,880&#20010;&#21465;&#36848;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#20256;&#36798;&#20986;&#29983;&#12289;&#27515;&#20129;&#12289;&#25307;&#32856;&#21644;&#35299;&#38599;&#20107;&#20214;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;87.43%&#30340;&#21465;&#36848;&#36275;&#22815;&#20256;&#36798;&#32467;&#26500;&#21270;&#25552;&#31034;&#30340;&#24847;&#22270;&#12290;&#20026;&#20102;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#21465;&#36848;&#65292;&#25105;&#20204;&#23545;&#20998;&#31867;&#25968;&#25454;&#38598;&#35757;&#32451;&#21644;&#39564;&#35777;&#20102;&#20061;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#23545;&#21097;&#20313;21,120&#20010;&#21465;&#36848;&#30340;&#20998;&#31867;&#39044;&#27979;&#20998;&#26512;&#12290;&#25152;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23558;&#26377;&#25928;&#30340;&#21465;&#36848;&#20998;&#31867;&#20026;&#26377;&#25928;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#21516;&#26102;&#23558;&#26080;&#25928;&#30340;&#21465;&#36848;&#20998;&#31867;&#20026;&#26080;&#25928;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#19981;&#20165;&#25512;&#36827;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#36824;&#25552;&#20379;&#20102;&#33258;&#21160;&#35782;&#21035;&#26377;&#25928;&#21465;&#36848;&#30340;&#26377;&#30410;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) play a pivotal role in generating vast arrays of narratives, facilitating a systematic exploration of their effectiveness for communicating life events in narrative form. In this study, we employ a zero-shot structured narrative prompt to generate 24,000 narratives using OpenAI's GPT-4. From this dataset, we manually classify 2,880 narratives and evaluate their validity in conveying birth, death, hiring, and firing events. Remarkably, 87.43% of the narratives sufficiently convey the intention of the structured prompt. To automate the identification of valid and invalid narratives, we train and validate nine Machine Learning models on the classified datasets. Leveraging these models, we extend our analysis to predict the classifications of the remaining 21,120 narratives. All the ML models excelled at classifying valid narratives as valid, but experienced challenges at simultaneously classifying invalid narratives as invalid. Our findings not only advance th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;AI&#27169;&#22411;&#30340;&#30830;&#20999;&#31639;&#27861;&#65292;&#26426;&#21046;&#35299;&#37322;&#24615;&#65288;MI&#65289;&#26088;&#22312;&#29702;&#35299;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#24494;&#19981;&#36275;&#36947;&#30340;&#34892;&#20026;&#21644;&#33021;&#21147;&#65292;&#32780;&#24573;&#35270;&#20102;&#38544;&#34255;&#22312;&#32593;&#32476;&#20869;&#37096;&#30340;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21628;&#21505;&#30740;&#31350;&#30028;&#26397;&#30528;&#26032;&#30340;&#26694;&#26550;&#21162;&#21147;&#65292;&#30740;&#31350;&#36825;&#20123;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.03855</link><description>&lt;p&gt;
&#12298;&#23450;&#20301;&#35770;&#25991;&#65306;&#25506;&#32034;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#26032;&#26694;&#26550;&#12299;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Toward New Frameworks for Studying Model Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03855
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;AI&#27169;&#22411;&#30340;&#30830;&#20999;&#31639;&#27861;&#65292;&#26426;&#21046;&#35299;&#37322;&#24615;&#65288;MI&#65289;&#26088;&#22312;&#29702;&#35299;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#24494;&#19981;&#36275;&#36947;&#30340;&#34892;&#20026;&#21644;&#33021;&#21147;&#65292;&#32780;&#24573;&#35270;&#20102;&#38544;&#34255;&#22312;&#32593;&#32476;&#20869;&#37096;&#30340;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21628;&#21505;&#30740;&#31350;&#30028;&#26397;&#30528;&#26032;&#30340;&#26694;&#26550;&#21162;&#21147;&#65292;&#30740;&#31350;&#36825;&#20123;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#21046;&#35299;&#37322;&#24615;&#65288;MI&#65289;&#26088;&#22312;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;AI&#27169;&#22411;&#23398;&#20064;&#30340;&#30830;&#20999;&#31639;&#27861;&#26469;&#29702;&#35299;&#27169;&#22411;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;MI&#30740;&#31350;&#30340;&#34892;&#20026;&#21644;&#33021;&#21147;&#37117;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#21644;&#31526;&#21495;&#23545;&#40784;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#33021;&#21147;&#24182;&#19981;&#37027;&#20040;&#24494;&#19981;&#36275;&#36947;&#65292;&#36825;&#20026;&#30740;&#31350;&#32593;&#32476;&#20869;&#37096;&#30340;&#38544;&#34255;&#34920;&#31034;&#20316;&#20026;&#20998;&#26512;&#21333;&#20301;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25991;&#29486;&#22238;&#39038;&#65292;&#23545;&#29305;&#24449;&#21644;&#34892;&#20026;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#30340;&#34920;&#31034;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#21644;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20123;&#22522;&#26412;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#35752;&#35770;&#21644;&#25506;&#32034;&#24615;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30740;&#31350;&#34920;&#31034;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#65292;&#24182;&#19988;&#30446;&#21069;MI&#20013;&#24314;&#31435;&#30340;&#26041;&#27861;&#19981;&#36275;&#20197;&#29702;&#35299;&#34920;&#31034;&#65292;&#22240;&#27492;&#25512;&#21160;&#30740;&#31350;&#30028;&#26397;&#30528;&#30740;&#31350;&#34920;&#31034;&#30340;&#26032;&#26694;&#26550;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mechanistic interpretability (MI) aims to understand AI models by reverse-engineering the exact algorithms neural networks learn. Most works in MI so far have studied behaviors and capabilities that are trivial and token-aligned. However, most capabilities are not that trivial, which advocates for the study of hidden representations inside these networks as the unit of analysis. We do a literature review, formalize representations for features and behaviors, highlight their importance and evaluation, and perform some basic exploration in the mechanistic interpretability of representations. With discussion and exploratory results, we justify our position that studying representations is an important and under-studied field, and that currently established methods in MI are not sufficient to understand representations, thus pushing for the research community to work toward new frameworks for studying representations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;ReLU&#32593;&#32476;&#22312;&#21152;&#26435;&#34928;&#20943;&#27491;&#21017;&#21270;&#19979;&#21450;&#20854;&#20984;&#26494;&#24347;&#20043;&#38388;&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#65292;&#35777;&#26126;&#20102;&#24403;&#35757;&#32451;&#25968;&#25454;&#26159;&#38543;&#26426;&#30340;&#26102;&#20505;&#65292;&#30456;&#23545;&#26368;&#20248;&#24615;&#24046;&#36317;&#21487;&#20197;&#34987;&#19968;&#20010;$O(\sqrt{\log n})$&#30340;&#22240;&#23376;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#23616;&#37096;&#26799;&#24230;&#26041;&#27861;&#20960;&#20046;&#32943;&#23450;&#20250;&#25910;&#25947;&#21040;&#35757;&#32451;&#25439;&#22833;&#36739;&#20302;&#30340;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.03625</link><description>&lt;p&gt;
ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20984;&#26494;&#24347;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36924;&#36817;&#20840;&#23616;&#26368;&#20248;&#35299;
&lt;/p&gt;
&lt;p&gt;
Convex Relaxations of ReLU Neural Networks Approximate Global Optima in Polynomial Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;ReLU&#32593;&#32476;&#22312;&#21152;&#26435;&#34928;&#20943;&#27491;&#21017;&#21270;&#19979;&#21450;&#20854;&#20984;&#26494;&#24347;&#20043;&#38388;&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#65292;&#35777;&#26126;&#20102;&#24403;&#35757;&#32451;&#25968;&#25454;&#26159;&#38543;&#26426;&#30340;&#26102;&#20505;&#65292;&#30456;&#23545;&#26368;&#20248;&#24615;&#24046;&#36317;&#21487;&#20197;&#34987;&#19968;&#20010;$O(\sqrt{\log n})$&#30340;&#22240;&#23376;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#23616;&#37096;&#26799;&#24230;&#26041;&#27861;&#20960;&#20046;&#32943;&#23450;&#20250;&#25910;&#25947;&#21040;&#35757;&#32451;&#25439;&#22833;&#36739;&#20302;&#30340;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;ReLU&#32593;&#32476;&#22312;&#21152;&#26435;&#34928;&#20943;&#27491;&#21017;&#21270;&#19979;&#21450;&#20854;&#20984;&#26494;&#24347;&#20043;&#38388;&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#35757;&#32451;&#25968;&#25454;&#26159;&#38543;&#26426;&#30340;&#26102;&#20505;&#65292;&#21407;&#22987;&#38382;&#39064;&#19982;&#20854;&#20984;&#26494;&#24347;&#20043;&#38388;&#30340;&#30456;&#23545;&#26368;&#20248;&#24615;&#24046;&#36317;&#21487;&#20197;&#34987;&#19968;&#20010;$O(\sqrt{\log n})$&#30340;&#22240;&#23376;&#30028;&#38480;&#65292;&#20854;&#20013;$n$&#26159;&#35757;&#32451;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340;&#24212;&#29992;&#21487;&#20197;&#23548;&#20986;&#19968;&#20010;&#21487;&#34892;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20445;&#35777;&#22312;&#23545;&#25968;&#22240;&#23376;&#33539;&#22260;&#20869;&#35299;&#20915;&#21407;&#22987;&#30340;&#38750;&#20984;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#21442;&#25968;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#19979;&#65292;&#23616;&#37096;&#26799;&#24230;&#26041;&#27861;&#20960;&#20046;&#32943;&#23450;&#20250;&#25910;&#25947;&#21040;&#35757;&#32451;&#25439;&#22833;&#36739;&#20302;&#30340;&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#30456;&#23545;&#20110;&#29616;&#26377;&#32467;&#26524;&#32780;&#35328;&#26159;&#25351;&#25968;&#32423;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#25581;&#31034;&#20102;&#20026;&#20160;&#20040;&#23616;&#37096;&#26799;&#24230;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the optimality gap between two-layer ReLU networks regularized with weight decay and their convex relaxations. We show that when the training data is random, the relative optimality gap between the original problem and its relaxation can be bounded by a factor of $O(\sqrt{\log n})$, where $n$ is the number of training samples. A simple application leads to a tractable polynomial-time algorithm that is guaranteed to solve the original non-convex problem up to a logarithmic factor. Moreover, under mild assumptions, we show that with random initialization on the parameters local gradient methods almost surely converge to a point that has low training loss. Our result is an exponential improvement compared to existing results and sheds new light on understanding why local gradient methods work well.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#22686;&#24378;Transformer RNNs&#23545;&#39034;&#24207;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#21442;&#25968;&#25968;&#37327;&#26368;&#23567;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.02625</link><description>&lt;p&gt;
&#29992;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#22686;&#24378;Transformer RNNs
&lt;/p&gt;
&lt;p&gt;
Enhancing Transformer RNNs with Multiple Temporal Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02625
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#22686;&#24378;Transformer RNNs&#23545;&#39034;&#24207;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#22312;&#21442;&#25968;&#25968;&#37327;&#26368;&#23567;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#26550;&#26500;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#20854;&#23545;&#39034;&#24207;&#25968;&#25454;&#30340;&#29702;&#35299;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#32500;&#25252;&#20808;&#21069;&#36935;&#21040;&#30340;&#25991;&#26412;&#30340;&#22810;&#26679;&#26102;&#38388;&#35270;&#22270;&#65292;&#26174;&#33879;&#20016;&#23500;&#20102;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#32435;&#20837;&#20102;Receptance Weighted Key Value&#65288;RWKV&#65289;&#26550;&#26500;&#65292;&#35299;&#20915;&#20102;&#35813;&#26550;&#26500;&#22312;&#21333;&#20010;&#38544;&#34255;&#29366;&#24577;&#20013;&#20445;&#30041;&#25152;&#26377;&#21382;&#21490;&#20449;&#24687;&#30340;&#22266;&#26377;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#26368;&#23569;&#65288;&#20165;&#20026;&#26368;&#21021;&#21442;&#25968;&#25968;&#37327;&#30340;0.04%&#65289;&#65292;&#20063;&#23454;&#29616;&#20102;&#27492;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#22810;&#20010;&#26102;&#38388;&#35270;&#35282;&#25152;&#38656;&#30340;&#39069;&#22806;&#21442;&#25968;&#32463;&#36807;&#24494;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#23436;&#20840;&#39044;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#22312;&#25552;&#31034;&#25512;&#26029;&#36807;&#31243;&#20013;&#20445;&#25345;&#20102;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the concept of multiple temporal perspectives, a novel approach applicable to Recurrent Neural Network (RNN) architectures for enhancing their understanding of sequential data. This method involves maintaining diverse temporal views of previously encountered text, significantly enriching the language models' capacity to interpret context. To show the efficacy of this approach, we incorporate it into the Receptance Weighted Key Value (RWKV) architecture, addressing its inherent challenge of retaining all historical information within a single hidden state. Notably, this improvement is achieved with a minimal increase in the number of parameters --even as little as $0.04\%$ of the original number of parameters. Further, the additional parameters necessary for the multiple temporal perspectives are fine-tuned with minimal computational overhead, avoiding the need for a full pre-training. The resulting model maintains linear computational complexity during prompt inference, en
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#22825;&#27668;&#39044;&#27979;&#30340;&#20892;&#19994;&#25512;&#33616;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#23391;&#21152;&#25289;&#22269;&#20892;&#19994;&#38754;&#20020;&#30340;&#22825;&#27668;&#19981;&#21033;&#22240;&#32032;&#23545;&#31918;&#39135;&#29983;&#20135;&#30340;&#24433;&#21709;&#65292;&#20197;&#23454;&#29616;&#30408;&#21033;&#12289;&#21487;&#25345;&#32493;&#21644;&#20892;&#27665;&#21451;&#22909;&#30340;&#20892;&#19994;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2401.11410</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20892;&#19994;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#22810;&#21464;&#37327;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Agricultural Recommendation System based on Deep Learning: A Multivariate Weather Forecasting Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11410
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#22825;&#27668;&#39044;&#27979;&#30340;&#20892;&#19994;&#25512;&#33616;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#23391;&#21152;&#25289;&#22269;&#20892;&#19994;&#38754;&#20020;&#30340;&#22825;&#27668;&#19981;&#21033;&#22240;&#32032;&#23545;&#31918;&#39135;&#29983;&#20135;&#30340;&#24433;&#21709;&#65292;&#20197;&#23454;&#29616;&#30408;&#21033;&#12289;&#21487;&#25345;&#32493;&#21644;&#20892;&#27665;&#21451;&#22909;&#30340;&#20892;&#19994;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23391;&#21152;&#25289;&#22269;&#20027;&#35201;&#26159;&#19968;&#20010;&#20892;&#19994;&#22269;&#23478;&#65292;&#20892;&#19994;&#37096;&#38376;&#23545;&#20110;&#21152;&#24555;&#32463;&#27982;&#22686;&#38271;&#21644;&#20445;&#38556;&#20154;&#27665;&#31918;&#39135;&#23433;&#20840;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#34429;&#28982;&#23391;&#21152;&#25289;&#22269;&#21171;&#21160;&#23494;&#38598;&#22411;&#20892;&#19994;&#21462;&#24471;&#20102;&#31918;&#39135;&#20135;&#37327;&#31283;&#27493;&#22686;&#38271;&#65292;&#20294;&#24120;&#24120;&#21463;&#21040;&#19981;&#21033;&#22825;&#27668;&#26465;&#20214;&#30340;&#24433;&#21709;&#65292;&#22914;&#26292;&#38632;&#12289;&#20302;&#28201;&#21644;&#24178;&#26097;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#22240;&#32032;&#20005;&#37325;&#24433;&#21709;&#20102;&#31918;&#39135;&#29983;&#20135;&#65292;&#20351;&#24471;&#22269;&#23478;&#30340;&#31918;&#39135;&#23433;&#20840;&#21463;&#21040;&#23041;&#32961;&#12290;&#20026;&#20102;&#23454;&#29616;&#30408;&#21033;&#12289;&#21487;&#25345;&#32493;&#19988;&#20892;&#27665;&#21451;&#22909;&#30340;&#20892;&#19994;&#23454;&#36341;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#20316;&#29289;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11410v2 Announce Type: replace-cross  Abstract: Bangladesh is predominantly an agricultural country, where the agrarian sector plays an essential role in accelerating economic growth and enabling the food security of the people. The performance of this sector has an overwhelming impact on the primary macroeconomic objectives like food security, employment generation, poverty alleviation, human resources development, and other economic and social forces. Although Bangladesh's labor-intensive agriculture has achieved steady increases in food grain production, it often suffered from unfavorable weather conditions such as heavy rainfall, low temperature, and drought. Consequently, these factors hinder the production of food substantially, putting the country's overall food security in danger. In order to have a profitable, sustainable, and farmer-friendly agricultural practice, this paper proposes a context-based crop recommendation system powered by a weather forecast model. Wi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#25490;&#21517;&#26631;&#20934;Equal-Opportunity Ranking&#65288;EOR&#65289;&#65292;&#23558;&#24213;&#23618;&#30456;&#20851;&#24615;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24046;&#24322;&#32771;&#34385;&#22312;&#20869;&#65292;&#36890;&#36807;&#32452;&#20869;&#20844;&#24179;&#25277;&#22870;&#23454;&#29616;&#20844;&#24179;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2309.01610</link><description>&lt;p&gt;
&#19981;&#21516;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#20844;&#24179;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Fair Ranking under Disparate Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01610
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#25490;&#21517;&#26631;&#20934;Equal-Opportunity Ranking&#65288;EOR&#65289;&#65292;&#23558;&#24213;&#23618;&#30456;&#20851;&#24615;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24046;&#24322;&#32771;&#34385;&#22312;&#20869;&#65292;&#36890;&#36807;&#32452;&#20869;&#20844;&#24179;&#25277;&#22870;&#23454;&#29616;&#20844;&#24179;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#21517;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20154;&#31867;&#35780;&#20272;&#32773;&#30340;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#21487;&#31649;&#29702;&#30340;&#36873;&#39033;&#23376;&#38598;&#19978;&#12290;&#23427;&#20316;&#20026;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#30340;&#20351;&#29992;&#33539;&#22260;&#20174;&#22312;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#23637;&#31034;&#28508;&#22312;&#30456;&#20851;&#20135;&#21697;&#21040;&#20026;&#20154;&#24037;&#23457;&#26597;&#20248;&#20808;&#22788;&#29702;&#22823;&#23398;&#30003;&#35831;&#12290;&#34429;&#28982;&#25490;&#21517;&#21487;&#20197;&#36890;&#36807;&#23558;&#20851;&#27880;&#38598;&#20013;&#22312;&#26368;&#26377;&#21069;&#36884;&#30340;&#36873;&#39033;&#19978;&#20351;&#20154;&#31867;&#35780;&#20272;&#26356;&#21152;&#39640;&#25928;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#24213;&#23618;&#30456;&#20851;&#24615;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#22312;&#19981;&#21516;&#32452;&#21035;&#30340;&#36873;&#39033;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#25490;&#21517;&#21487;&#33021;&#20250;&#24341;&#20837;&#19981;&#20844;&#24179;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#24046;&#24322;&#20284;&#20046;&#26222;&#36941;&#23384;&#22312;&#65292;&#24120;&#24120;&#23545;&#23569;&#25968;&#32676;&#20307;&#36896;&#25104;&#25439;&#23475;&#65292;&#22240;&#20026;&#36825;&#20123;&#32676;&#20307;&#30340;&#30456;&#20851;&#24615;&#20272;&#35745;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#25110;&#21512;&#36866;&#30340;&#29305;&#24449;&#32780;&#20855;&#26377;&#26356;&#39640;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20844;&#24179;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Equal-Opportunity Ranking&#65288;EOR&#65289;&#20316;&#20026;&#25490;&#21517;&#30340;&#26032;&#20844;&#24179;&#26631;&#20934;&#65292;&#24182;&#23637;&#31034;&#23427;&#23545;&#24212;&#20110;&#22312;&#30456;&#20851;&#36873;&#39033;&#20043;&#38388;&#36827;&#34892;&#32452;&#20869;&#20844;&#24179;&#25277;&#22870;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01610v2 Announce Type: replace  Abstract: Ranking is a ubiquitous method for focusing the attention of human evaluators on a manageable subset of options. Its use as part of human decision-making processes ranges from surfacing potentially relevant products on an e-commerce site to prioritizing college applications for human review. While ranking can make human evaluation more effective by focusing attention on the most promising options, we argue that it can introduce unfairness if the uncertainty of the underlying relevance model differs between groups of options. Unfortunately, such disparity in uncertainty appears widespread, often to the detriment of minority groups for which relevance estimates can have higher uncertainty due to a lack of data or appropriate features. To address this fairness issue, we propose Equal-Opportunity Ranking (EOR) as a new fairness criterion for ranking and show that it corresponds to a group-wise fair lottery among the relevant options even
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#20108;&#20803;&#20998;&#31867;&#24615;&#33021;&#30340;&#20869;&#22312;&#25968;&#25454;&#38480;&#21046;&#21644;&#19978;&#30028;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#24182;&#36827;&#34892;&#20102;&#29702;&#35770;&#25512;&#29702;&#21644;&#23454;&#35777;&#26816;&#39564;&#65292;&#21457;&#29616;&#29702;&#35770;&#19978;&#38480;&#26159;&#21487;&#20197;&#34987;&#36798;&#21040;&#30340;&#65292;&#24182;&#35745;&#31639;&#20986;&#20102;&#19977;&#20010;&#24120;&#29992;&#35780;&#20272;&#25351;&#26631;&#30340;&#31934;&#30830;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.17036</link><description>&lt;p&gt;
&#20108;&#20803;&#20998;&#31867;&#24615;&#33021;&#20013;&#30340;&#20869;&#22312;&#25968;&#25454;&#38480;&#21046;&#21644;&#19978;&#30028;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Data Constraints and Upper Bounds in Binary Classification Performance. (arXiv:2401.17036v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17036
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20108;&#20803;&#20998;&#31867;&#24615;&#33021;&#30340;&#20869;&#22312;&#25968;&#25454;&#38480;&#21046;&#21644;&#19978;&#30028;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#24182;&#36827;&#34892;&#20102;&#29702;&#35770;&#25512;&#29702;&#21644;&#23454;&#35777;&#26816;&#39564;&#65292;&#21457;&#29616;&#29702;&#35770;&#19978;&#38480;&#26159;&#21487;&#20197;&#34987;&#36798;&#21040;&#30340;&#65292;&#24182;&#35745;&#31639;&#20986;&#20102;&#19977;&#20010;&#24120;&#29992;&#35780;&#20272;&#25351;&#26631;&#30340;&#31934;&#30830;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#32452;&#32455;&#30340;&#32467;&#26500;&#34987;&#24191;&#27867;&#35748;&#20026;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35748;&#20026;&#32473;&#23450;&#25968;&#25454;&#38598;&#19978;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#26368;&#22823;&#28508;&#21147;&#20027;&#35201;&#21463;&#21040;&#25968;&#25454;&#30340;&#20869;&#22312;&#29305;&#24615;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#29702;&#35770;&#25512;&#29702;&#21644;&#23454;&#35777;&#26816;&#39564;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26631;&#20934;&#30446;&#26631;&#20989;&#25968;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#20108;&#20803;&#20998;&#31867;&#22120;&#65292;&#24471;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#32467;&#35770;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#20108;&#20803;&#20998;&#31867;&#24615;&#33021;&#30340;&#29702;&#35770;&#19978;&#38480;&#26159;&#21487;&#20197;&#34987;&#29702;&#35770;&#19978;&#36798;&#21040;&#30340;&#12290;&#36825;&#20010;&#19978;&#38480;&#20195;&#34920;&#20102;&#23398;&#20064;&#25439;&#22833;&#21644;&#35780;&#20272;&#25351;&#26631;&#20043;&#38388;&#30340;&#21487;&#35745;&#31639;&#24179;&#34913;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35745;&#31639;&#20102;&#19977;&#20010;&#24120;&#29992;&#35780;&#20272;&#25351;&#26631;&#30340;&#31934;&#30830;&#19978;&#38480;&#65292;&#25581;&#31034;&#20102;&#19982;&#25105;&#20204;&#24635;&#20307;&#35770;&#28857;&#30340;&#26681;&#26412;&#19968;&#33268;&#24615;&#65306;&#19978;&#30028;&#19982;&#20869;&#22312;&#25968;&#25454;&#38480;&#21046;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
The structure of data organization is widely recognized as having a substantial influence on the efficacy of machine learning algorithms, particularly in binary classification tasks. Our research provides a theoretical framework suggesting that the maximum potential of binary classifiers on a given dataset is primarily constrained by the inherent qualities of the data. Through both theoretical reasoning and empirical examination, we employed standard objective functions, evaluative metrics, and binary classifiers to arrive at two principal conclusions. Firstly, we show that the theoretical upper bound of binary classification performance on actual datasets can be theoretically attained. This upper boundary represents a calculable equilibrium between the learning loss and the metric of evaluation. Secondly, we have computed the precise upper bounds for three commonly used evaluation metrics, uncovering a fundamental uniformity with our overarching thesis: the upper bound is intricately 
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#20998;&#26512;&#20013;&#20855;&#26377;&#35786;&#26029;&#21644;&#39044;&#27979;&#30340;&#28508;&#21147;&#65292;&#24403;&#21069;&#30740;&#31350;&#32858;&#28966;&#20110;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#65292;&#20197;&#21450;&#23545;&#35813;&#30142;&#30149;&#30340;&#20998;&#31867;&#12289;&#20998;&#32423;&#12289;&#20998;&#23376;&#26631;&#35760;&#39044;&#27979;&#21644;&#29983;&#23384;&#39044;&#27979;&#31561;&#20020;&#24202;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.15022</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#20998;&#26512;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based analysis of glioma tissue sections: a review. (arXiv:2401.15022v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15022
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#20998;&#26512;&#20013;&#20855;&#26377;&#35786;&#26029;&#21644;&#39044;&#27979;&#30340;&#28508;&#21147;&#65292;&#24403;&#21069;&#30740;&#31350;&#32858;&#28966;&#20110;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#65292;&#20197;&#21450;&#23545;&#35813;&#30142;&#30149;&#30340;&#20998;&#31867;&#12289;&#20998;&#32423;&#12289;&#20998;&#23376;&#26631;&#35760;&#39044;&#27979;&#21644;&#29983;&#23384;&#39044;&#27979;&#31561;&#20020;&#24202;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33014;&#36136;&#30244;&#30340;&#35786;&#26029;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#20351;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#33014;&#36136;&#30244;&#32452;&#32455;&#36827;&#34892;&#32452;&#32455;&#23398;&#35780;&#20272;&#65292;&#20026;&#35786;&#26029;&#21644;&#39044;&#27979;&#32467;&#26524;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#23545;&#24403;&#21069;&#30740;&#31350;&#30340;&#29616;&#29366;&#36827;&#34892;&#27010;&#36848;&#65292;&#26412;&#32508;&#36848;&#23545;70&#20010;&#20844;&#24320;&#21487;&#24471;&#30340;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36825;&#20123;&#35770;&#25991;&#20851;&#20110;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#26579;&#33394;&#30340;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#65292;&#28085;&#30422;&#20102;&#20998;&#31867;&#65288;16/70&#65289;&#65292;&#20998;&#32423;&#65288;23/70&#65289;&#65292;&#20998;&#23376;&#26631;&#35760;&#39044;&#27979;&#65288;13/70&#65289;&#21644;&#29983;&#23384;&#39044;&#27979;&#65288;27/70&#65289;&#31561;&#35786;&#26029;&#20219;&#21153;&#12290;&#25152;&#26377;&#30340;&#30740;&#31350;&#37117;&#22312;&#26041;&#27861;&#23398;&#26041;&#38754;&#21450;&#20854;&#20020;&#24202;&#36866;&#29992;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#23545;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#36827;&#34892;&#35780;&#20272;&#12290;&#22810;&#25968;&#30740;&#31350;&#65288;49/70&#65289;&#22522;&#20110;&#20844;&#24320;&#30340;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#21644;&#20302;&#32423;&#21035;&#33014;&#36136;&#30244;&#25968;&#25454;&#38598;&#65292;&#20165;&#26377;&#23569;&#25968;&#30740;&#31350;&#20351;&#29992;&#20854;&#20182;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the diagnosis of gliomas has become increasingly complex. Histological assessment of glioma tissue using modern machine learning techniques offers new opportunities to support diagnosis and outcome prediction. To give an overview of the current state of research, this review examines 70 publicly available research studies on machine learning-based analysis of stained human glioma tissue sections, covering the diagnostic tasks of subtyping (16/70), grading (23/70), molecular marker prediction (13/70), and survival prediction (27/70). All studies were reviewed with regard to methodological aspects as well as clinical applicability. It was found that the focus of current research is the assessment of hematoxylin and eosin-stained tissue sections of adult-type diffuse gliomas. The majority of studies (49/70) are based on the publicly available glioblastoma and low-grade glioma datasets from The Cancer Genome Atlas (TCGA) and only a few studies employed other datasets in is
&lt;/p&gt;</description></item><item><title>DISTINQT&#26159;&#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.10158</link><description>&lt;p&gt;
DISTINQT: &#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#38544;&#31169;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DISTINQT: A Distributed Privacy Aware Learning Framework for QoS Prediction for Future Mobile and Wireless Networks. (arXiv:2401.10158v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10158
&lt;/p&gt;
&lt;p&gt;
DISTINQT&#26159;&#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
5G&#21644;6G&#20197;&#21518;&#30340;&#32593;&#32476;&#23558;&#25903;&#25345;&#20381;&#36182;&#19968;&#23450;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#30340;&#26032;&#30340;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29992;&#20363;&#21644;&#24212;&#29992;&#31243;&#24207;&#12290;&#21450;&#26102;&#39044;&#27979;QoS&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65288;&#22914;&#36710;&#36742;&#36890;&#20449;&#65289;&#23588;&#20026;&#37325;&#35201;&#12290;&#23613;&#31649;&#30452;&#21040;&#26368;&#36817;&#65292;QoS&#39044;&#27979;&#19968;&#30452;&#30001;&#38598;&#20013;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35299;&#20915;&#26041;&#26696;&#23436;&#25104;&#65292;&#20294;&#24050;&#32463;&#20986;&#29616;&#20102;&#19968;&#20123;&#38544;&#31169;&#12289;&#35745;&#31639;&#21644;&#36816;&#33829;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#26367;&#20195;&#26041;&#26696;&#24050;&#32463;&#20986;&#29616;&#65288;&#22914;&#20998;&#21106;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#23558;&#22797;&#26434;&#24230;&#36739;&#20302;&#30340;AI&#20219;&#21153;&#20998;&#24067;&#22312;&#33410;&#28857;&#20043;&#38388;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#26410;&#26469;&#26080;&#32447;&#32593;&#32476;&#30340;&#24322;&#26500;&#24615;&#65292;&#24403;&#28041;&#21450;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#26102;&#65292;&#20250;&#20986;&#29616;&#26032;&#30340;&#25361;&#25112;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISTINQT&#30340;&#38754;&#21521;QoS&#39044;&#27979;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond 5G and 6G networks are expected to support new and challenging use cases and applications that depend on a certain level of Quality of Service (QoS) to operate smoothly. Predicting the QoS in a timely manner is of high importance, especially for safety-critical applications as in the case of vehicular communications. Although until recent years the QoS prediction has been carried out by centralized Artificial Intelligence (AI) solutions, a number of privacy, computational, and operational concerns have emerged. Alternative solutions have been surfaced (e.g. Split Learning, Federated Learning), distributing AI tasks of reduced complexity across nodes, while preserving the privacy of the data. However, new challenges rise when it comes to scalable distributed learning approaches, taking into account the heterogeneous nature of future wireless networks. The current work proposes DISTINQT, a privacy-aware distributed learning framework for QoS prediction. Our framework supports mult
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24555;&#36895;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#23545;&#21738;&#20123;&#29305;&#24449;&#26356;&#20559;&#22909;&#65292;&#21363;&#21487;&#39044;&#27979;&#24615;&#21644;&#21487;&#29992;&#24615;&#22914;&#20309;&#30456;&#20114;&#24433;&#21709;&#27169;&#22411;&#30340;&#29305;&#24449;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.16228</link><description>&lt;p&gt;
&#20851;&#20110;&#24555;&#36895;&#23398;&#20064;&#30340;&#22522;&#30784;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Foundations of Shortcut Learning. (arXiv:2310.16228v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16228
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24555;&#36895;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#23545;&#21738;&#20123;&#29305;&#24449;&#26356;&#20559;&#22909;&#65292;&#21363;&#21487;&#39044;&#27979;&#24615;&#21644;&#21487;&#29992;&#24615;&#22914;&#20309;&#30456;&#20114;&#24433;&#21709;&#27169;&#22411;&#30340;&#29305;&#24449;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#20016;&#23500;&#30340;&#29305;&#24449;&#12290;&#27169;&#22411;&#20351;&#29992;&#21738;&#20123;&#29305;&#24449;&#19981;&#20165;&#21462;&#20915;&#20110;&#39044;&#27979;&#33021;&#21147; - &#19968;&#20010;&#29305;&#24449;&#21487;&#38752;&#22320;&#25351;&#31034;&#35757;&#32451;&#38598;&#26631;&#31614;&#30340;&#31243;&#24230;&#65292;&#36824;&#21462;&#20915;&#20110;&#21487;&#29992;&#24615; - &#19968;&#20010;&#29305;&#24449;&#21487;&#20197;&#20174;&#36755;&#20837;&#20013;&#34987;&#36731;&#26494;&#25552;&#21462;&#25110;&#21033;&#29992;&#30340;&#31243;&#24230;&#12290;&#26377;&#20851;&#24555;&#36895;&#23398;&#20064;&#30340;&#25991;&#29486;&#24050;&#32463;&#25351;&#20986;&#20102;&#27169;&#22411;&#20559;&#22909;&#19968;&#20010;&#29305;&#24449;&#32780;&#19981;&#26159;&#21478;&#19968;&#20010;&#29305;&#24449;&#30340;&#20363;&#23376;&#65292;&#20363;&#22914;&#22312;&#32441;&#29702;&#21644;&#24418;&#29366;&#20043;&#38388;&#20197;&#21450;&#22312;&#22270;&#20687;&#32972;&#26223;&#21644;&#21069;&#26223;&#23545;&#35937;&#20043;&#38388;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#27979;&#35797;&#20851;&#20110;&#21738;&#20123;&#36755;&#20837;&#23646;&#24615;&#23545;&#20110;&#27169;&#22411;&#26356;&#23481;&#26131;&#33719;&#21462;&#30340;&#20551;&#35774;&#65292;&#24182;&#31995;&#32479;&#22320;&#30740;&#31350;&#39044;&#27979;&#33021;&#21147;&#21644;&#21487;&#29992;&#24615;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#26469;&#22609;&#36896;&#27169;&#22411;&#30340;&#29305;&#24449;&#20351;&#29992;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26368;&#23567;&#30340;&#12289;&#26126;&#30830;&#30340;&#29983;&#25104;&#26694;&#26550;&#26469;&#21512;&#25104;&#20855;&#26377;&#20004;&#20010;&#28508;&#22312;&#29305;&#24449;&#30340;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#36825;&#20004;&#20010;&#29305;&#24449;&#22312;&#39044;&#27979;&#33021;&#21147;&#21644;&#25105;&#20204;&#20551;&#35774;&#19982;&#21487;&#29992;&#24615;&#26377;&#20851;&#30340;&#22240;&#32032;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#37327;&#21270;&#20102;&#27169;&#22411;&#30340;&#24555;&#25463;&#20559;&#24046; - &#23427;&#36807;&#24230;&#20381;&#36182;&#24555;&#25463;&#65288;&#26356;&#21487;&#29992;&#12289;&#19981;&#22826;&#39044;&#27979;&#65289;&#29305;&#24449;&#32780;&#24573;&#35270;&#20102;&#26680;&#24515;&#65288;&#19981;&#22826;&#21487;&#29992;)&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep-learning models can extract a rich assortment of features from data. Which features a model uses depends not only on predictivity-how reliably a feature indicates train-set labels-but also on availability-how easily the feature can be extracted, or leveraged, from inputs. The literature on shortcut learning has noted examples in which models privilege one feature over another, for example texture over shape and image backgrounds over foreground objects. Here, we test hypotheses about which input properties are more available to a model, and systematically study how predictivity and availability interact to shape models' feature use. We construct a minimal, explicit generative framework for synthesizing classification datasets with two latent features that vary in predictivity and in factors we hypothesize to relate to availability, and quantify a model's shortcut bias-its over-reliance on the shortcut (more available, less predictive) feature at the expense of the core (less avail
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#23427;&#23558;&#29289;&#29702;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#65292;&#23545;&#20110;&#31649;&#29702;&#27700;&#36164;&#28304;&#20197;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#31561;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.05227</link><description>&lt;p&gt;
&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#38761;&#21629;&#31185;&#23398;&#33539;&#24335;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;&#36807;&#31243;&#30340;&#27700;&#25991;&#23398;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Physics-aware Machine Learning Revolutionizes Scientific Paradigm for Machine Learning and Process-based Hydrology. (arXiv:2310.05227v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05227
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#23427;&#23558;&#29289;&#29702;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#65292;&#23545;&#20110;&#31649;&#29702;&#27700;&#36164;&#28304;&#20197;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#31561;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#23545;&#20110;&#35299;&#20915;&#27700;&#36164;&#28304;&#31649;&#29702;&#20013;&#30340;&#31185;&#23398;&#21644;&#31038;&#20250;&#25361;&#25112;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#20026;&#27668;&#20505;&#21464;&#21270;&#30340;&#21160;&#24577;&#24433;&#21709;&#19979;&#12290;&#29616;&#26377;&#30340;&#35780;&#35770;&#20027;&#35201;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#28982;&#32780;&#27700;&#25991;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#29420;&#31435;&#30340;&#33539;&#24335;&#23384;&#22312;&#26126;&#26174;&#30340;&#21306;&#21035;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20197;&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#21464;&#38761;&#24615;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#36825;&#31181;&#35748;&#30693;&#38556;&#30861;&#65292;&#24182;&#38761;&#26032;&#20102;&#36825;&#20004;&#20010;&#39046;&#22495;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#35780;&#35770;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#31038;&#21306;&#65288;PaML&#65289;&#65292;&#23558;&#20808;&#21069;&#30340;&#29289;&#29702;&#30693;&#35782;&#25110;&#22522;&#20110;&#29289;&#29702;&#30340;&#24314;&#27169;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20174;&#29289;&#29702;&#25968;&#25454;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#12289;&#29289;&#29702;&#20449;&#24687;&#22788;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#12289;&#29289;&#29702;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#29702;&#24863;&#30693;&#28151;&#21512;&#23398;&#20064;&#22235;&#20010;&#26041;&#38754;&#20998;&#26512;&#20102;&#36825;&#20123;PaML&#26041;&#27861;&#12290;PaML&#20419;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20551;&#35774;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate hydrological understanding and water cycle prediction are crucial for addressing scientific and societal challenges associated with the management of water resources, particularly under the dynamic influence of anthropogenic climate change. Existing reviews predominantly concentrate on the development of machine learning (ML) in this field, yet there is a clear distinction between hydrology and ML as separate paradigms. Here, we introduce physics-aware ML as a transformative approach to overcome the perceived barrier and revolutionize both fields. Specifically, we present a comprehensive review of the physics-aware ML methods, building a structured community (PaML) of existing methodologies that integrate prior physical knowledge or physics-based modeling into ML. We systematically analyze these PaML methodologies with respect to four aspects: physical data-guided ML, physics-informed ML, physics-embedded ML, and physics-aware hybrid learning. PaML facilitates ML-aided hypothe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20462;&#25913;&#32479;&#35745;&#24615;&#27495;&#35270;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#30001;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#30340;&#21487;&#21512;&#21516;&#21270;&#20449;&#24565;&#65292;&#32473;&#30417;&#31649;&#32773;&#25552;&#20379;&#20102;&#19968;&#31181;&#36229;&#36807;&#32943;&#23450;&#34892;&#21160;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35201;&#27714;&#20844;&#21496;&#36873;&#21462;&#19968;&#20010;&#24179;&#34913;&#19981;&#21516;&#32676;&#20307;&#30495;&#27491;&#38451;&#24615;&#29575;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#23454;&#29616;&#26426;&#20250;&#24179;&#31561;&#26469;&#28040;&#38500;&#32479;&#35745;&#24615;&#27495;&#35270;&#12290;</title><link>http://arxiv.org/abs/2310.04585</link><description>&lt;p&gt;
&#26426;&#20250;&#24179;&#31561;&#23545;&#32479;&#35745;&#24615;&#27495;&#35270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Equal Opportunity on Statistical Discrimination. (arXiv:2310.04585v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20462;&#25913;&#32479;&#35745;&#24615;&#27495;&#35270;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#30001;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#30340;&#21487;&#21512;&#21516;&#21270;&#20449;&#24565;&#65292;&#32473;&#30417;&#31649;&#32773;&#25552;&#20379;&#20102;&#19968;&#31181;&#36229;&#36807;&#32943;&#23450;&#34892;&#21160;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35201;&#27714;&#20844;&#21496;&#36873;&#21462;&#19968;&#20010;&#24179;&#34913;&#19981;&#21516;&#32676;&#20307;&#30495;&#27491;&#38451;&#24615;&#29575;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#23454;&#29616;&#26426;&#20250;&#24179;&#31561;&#26469;&#28040;&#38500;&#32479;&#35745;&#24615;&#27495;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20462;&#25913;&#20102;Coate&#21644;Loury&#65288;1993&#65289;&#30340;&#32463;&#20856;&#32479;&#35745;&#24615;&#27495;&#35270;&#27169;&#22411;&#65292;&#20551;&#35774;&#20844;&#21496;&#23545;&#20010;&#20307;&#26410;&#35266;&#23519;&#21040;&#30340;&#31867;&#21035;&#30340;&#20449;&#24565;&#26159;&#30001;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#30340;&#65292;&#22240;&#27492;&#26159;&#21487;&#21512;&#21516;&#21270;&#30340;&#12290;&#36825;&#25193;&#23637;&#20102;&#30417;&#31649;&#32773;&#30340;&#24037;&#20855;&#31665;&#65292;&#36229;&#20986;&#20102;&#20687;&#32943;&#23450;&#34892;&#21160;&#36825;&#26679;&#30340;&#26080;&#20449;&#24565;&#35268;&#23450;&#12290;&#21487;&#21512;&#21516;&#21270;&#30340;&#20449;&#24565;&#20351;&#24471;&#35201;&#27714;&#20844;&#21496;&#36873;&#25321;&#19968;&#20010;&#20915;&#31574;&#31574;&#30053;&#65292;&#20351;&#24471;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#30495;&#27491;&#38451;&#24615;&#29575;&#30456;&#31561;&#65288;&#31639;&#27861;&#20844;&#24179;&#25991;&#29486;&#20013;&#25152;&#31216;&#30340;&#26426;&#20250;&#24179;&#31561;&#65289;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#32943;&#23450;&#34892;&#21160;&#19981;&#19968;&#23450;&#33021;&#28040;&#38500;&#32479;&#35745;&#24615;&#27495;&#35270;&#65292;&#20294;&#26412;&#25991;&#34920;&#26126;&#23454;&#26045;&#26426;&#20250;&#24179;&#31561;&#21487;&#20197;&#20570;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
I modify the canonical statistical discrimination model of Coate and Loury (1993) by assuming the firm's belief about an individual's unobserved class is machine learning-generated and, therefore, contractible. This expands the toolkit of a regulator beyond belief-free regulations like affirmative action. Contractible beliefs make it feasible to require the firm to select a decision policy that equalizes true positive rates across groups -- what the algorithmic fairness literature calls equal opportunity. While affirmative action does not necessarily end statistical discrimination, I show that imposing equal opportunity does.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#22270;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#31038;&#21306;&#32467;&#26500;&#26159;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#20855;&#26377;&#31038;&#20250;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#31185;&#23398;&#23478;&#20204;&#20570;&#20986;&#20102;&#21162;&#21147;&#65292;&#20294;&#23578;&#26410;&#25214;&#21040;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#31038;&#21306;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#21508;&#31181;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#21508;&#31181;&#32593;&#32476;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.11798</link><description>&lt;p&gt;
&#22270;&#20013;&#31038;&#21306;&#26816;&#27979;&#30340;&#32508;&#21512;&#35780;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review of Community Detection in Graphs. (arXiv:2309.11798v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#22270;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#31038;&#21306;&#32467;&#26500;&#26159;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#20855;&#26377;&#31038;&#20250;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#31185;&#23398;&#23478;&#20204;&#20570;&#20986;&#20102;&#21162;&#21147;&#65292;&#20294;&#23578;&#26410;&#25214;&#21040;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#31038;&#21306;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#21508;&#31181;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#21508;&#31181;&#32593;&#32476;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#30740;&#31350;&#26174;&#33879;&#20419;&#36827;&#20102;&#25105;&#20204;&#23545;&#30495;&#23454;&#19990;&#30028;&#22270;&#30340;&#31038;&#21306;&#32467;&#26500;&#30340;&#29702;&#35299;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#31038;&#20250;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;&#23613;&#31649;&#36328;&#23398;&#31185;&#31185;&#23398;&#23478;&#31038;&#21306;&#30340;&#21162;&#21147;&#65292;&#20294;&#23578;&#26410;&#25214;&#21040;&#19968;&#20010;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#32508;&#36848;&#35814;&#32454;&#20171;&#32461;&#20102;&#22270;&#20013;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#30340;&#32452;&#32455;&#21644;&#21151;&#33021;&#36215;&#30528;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#31038;&#21306;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#23427;&#25351;&#30340;&#26159;&#23558;&#39030;&#28857;&#21010;&#20998;&#20026;&#20855;&#26377;&#24378;&#20869;&#37096;&#36830;&#25509;&#21644;&#36739;&#24369;&#36830;&#25509;&#30340;&#38598;&#32676;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#38416;&#36848;&#65292;&#21253;&#25324;&#25105;&#20204;&#35774;&#35745;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#31038;&#21306;&#26816;&#27979;&#22312;&#21508;&#31181;&#32593;&#32476;&#20013;&#30340;&#30495;&#23454;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of complex networks has significantly advanced our understanding of community structures which serves as a crucial feature of real-world graphs. Detecting communities in graphs is a challenging problem with applications in sociology, biology, and computer science. Despite the efforts of an interdisciplinary community of scientists, a satisfactory solution to this problem has not yet been achieved. This review article delves into the topic of community detection in graphs, which serves as a crucial role in understanding the organization and functioning of complex systems. We begin by introducing the concept of community structure, which refers to the arrangement of vertices into clusters, with strong internal connections and weaker connections between clusters. Then, we provide a thorough exposition of various community detection methods, including a new method designed by us. Additionally, we explore real-world applications of community detection in diverse networks. In concl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;&#36793;&#32536;&#24212;&#29992;&#20013;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#32676;&#20307;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#36328;&#22495;&#32676;&#20307;&#37325;&#35201;&#24615;&#26469;&#20943;&#36731;&#20840;&#23616;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#24182;&#20445;&#25345;&#38544;&#31169;&#21644;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.07085</link><description>&lt;p&gt;
&#22312;&#24322;&#26500;&#35774;&#22791;&#19978;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32676;&#20307;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Group Bias in Federated Learning for Heterogeneous Devices. (arXiv:2309.07085v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;&#36793;&#32536;&#24212;&#29992;&#20013;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#32676;&#20307;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#36328;&#22495;&#32676;&#20307;&#37325;&#35201;&#24615;&#26469;&#20943;&#36731;&#20840;&#23616;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#24182;&#20445;&#25345;&#38544;&#31169;&#21644;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#27491;&#22312;&#20998;&#24067;&#24335;&#36793;&#32536;&#24212;&#29992;&#20013;&#23853;&#38706;&#22836;&#35282;&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36793;&#32536;&#37096;&#32626;&#26159;&#24322;&#26500;&#30340;&#65292;&#21363;&#23427;&#20204;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#29615;&#22659;&#22312;&#37096;&#32626;&#20013;&#21508;&#19981;&#30456;&#21516;&#12290;&#36825;&#31181;&#36793;&#32536;&#24322;&#26500;&#36829;&#21453;&#20102;&#26412;&#22320;&#25968;&#25454;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#29420;&#31435;&#19988;&#20998;&#24067;&#30456;&#21516; (IID) &#30340;&#29305;&#24615;&#65292;&#20135;&#29983;&#20102;&#26377;&#20559;&#35265;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#21363;&#23545;&#29305;&#23450;&#31038;&#21306;&#25110;&#32676;&#20307;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#20915;&#31574;&#21644;&#27495;&#35270;&#12290;&#29616;&#26377;&#30340;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#21482;&#20851;&#27880;&#38750;IID&#25968;&#25454;&#20013;&#30001;&#26631;&#31614;&#24322;&#26500;&#24341;&#36215;&#30340;&#20559;&#35265;&#65292;&#24182;&#27809;&#26377;&#32771;&#34385;&#30001;&#29305;&#24449;&#24322;&#26500;&#23548;&#33268;&#30340;&#39046;&#22495;&#21464;&#21270;&#65292;&#20063;&#27809;&#26377;&#35299;&#20915;&#20840;&#23616;&#32676;&#20307;&#20844;&#24179;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20445;&#25252;&#38544;&#31169;&#21644;&#19981;&#22686;&#21152;&#36164;&#28304;&#21033;&#29992;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#65292;&#20943;&#23569;&#32676;&#20307;&#20559;&#35265;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#21033;&#29992;&#24179;&#22343;&#26465;&#20214;&#27010;&#29575;&#26469;&#35745;&#31639;&#36328;&#22495;&#32676;&#20307;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning is emerging as a privacy-preserving model training approach in distributed edge applications. As such, most edge deployments are heterogeneous in nature i.e., their sensing capabilities and environments vary across deployments. This edge heterogeneity violates the independence and identical distribution (IID) property of local data across clients and produces biased global models i.e. models that contribute to unfair decision-making and discrimination against a particular community or a group. Existing bias mitigation techniques only focus on bias generated from label heterogeneity in non-IID data without accounting for domain variations due to feature heterogeneity and do not address global group-fairness property.  Our work proposes a group-fair FL framework that minimizes group-bias while preserving privacy and without resource utilization overhead. Our main idea is to leverage average conditional probabilities to compute a cross-domain group \textit{importance we
&lt;/p&gt;</description></item><item><title>ABC123&#26159;&#19968;&#31181;&#26080;&#38656;&#20351;&#29992;&#31034;&#20363;&#36827;&#34892;&#35757;&#32451;&#25110;&#25512;&#26029;&#30340;&#22810;&#31867;&#21035;&#31867;&#21035;&#26080;&#20851;&#35745;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#33539;&#24335;&#65292;&#23427;&#22312;&#22810;&#31181;&#23545;&#35937;&#21516;&#26102;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.04820</link><description>&lt;p&gt;
ABC&#31616;&#21333;&#22914;123&#65306;&#19968;&#31181;&#29992;&#20110;&#26080;&#20808;&#20363;&#22810;&#31867;&#21035;&#31867;&#21035;&#26080;&#20851;&#35745;&#25968;&#30340;&#30450;&#30446;&#35745;&#25968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ABC Easy as 123: A Blind Counter for Exemplar-Free Multi-Class Class-agnostic Counting. (arXiv:2309.04820v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04820
&lt;/p&gt;
&lt;p&gt;
ABC123&#26159;&#19968;&#31181;&#26080;&#38656;&#20351;&#29992;&#31034;&#20363;&#36827;&#34892;&#35757;&#32451;&#25110;&#25512;&#26029;&#30340;&#22810;&#31867;&#21035;&#31867;&#21035;&#26080;&#20851;&#35745;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#33539;&#24335;&#65292;&#23427;&#22312;&#22810;&#31181;&#23545;&#35937;&#21516;&#26102;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#21035;&#26080;&#20851;&#35745;&#25968;&#26041;&#27861;&#21487;&#20197;&#23545;&#20219;&#24847;&#31867;&#21035;&#30340;&#23545;&#35937;&#36827;&#34892;&#35745;&#25968;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#23454;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#33021;&#36866;&#29992;&#20110;&#38656;&#35201;&#19968;&#32452;&#29305;&#23450;&#31867;&#22411;&#30340;&#31034;&#20363;&#25110;&#22270;&#20687;&#20013;&#20165;&#21253;&#21547;&#19968;&#31181;&#31867;&#22411;&#23545;&#35937;&#30340;&#24773;&#20917;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#23616;&#38480;&#20043;&#19968;&#26159;&#32570;&#20047;&#36866;&#29992;&#20110;&#22810;&#31181;&#23545;&#35937;&#21516;&#26102;&#23384;&#22312;&#30340;&#35745;&#25968;&#35774;&#32622;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#31867;&#21035;&#12289;&#31867;&#21035;&#26080;&#20851;&#35745;&#25968;&#25968;&#25454;&#38598;&#65288;MCAC&#65289;&#20197;&#21450;&#19968;&#31181;&#21517;&#20026;ABC123&#30340;&#30450;&#30446;&#35745;&#25968;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#35757;&#32451;&#25110;&#25512;&#26029;&#36807;&#31243;&#20013;&#19981;&#20351;&#29992;&#29305;&#23450;&#31867;&#22411;&#31034;&#20363;&#26469;&#21516;&#26102;&#35745;&#25968;&#22810;&#31181;&#23545;&#35937;&#12290;ABC123&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#22312;&#35745;&#25968;&#38454;&#27573;&#21518;&#25214;&#21040;&#31034;&#20363;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#23548;&#26679;&#26412;&#26469;&#24341;&#23548;&#35745;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ABC123&#22312;MCAC&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#36827;&#34892;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-agnostic counting methods enumerate objects of an arbitrary class, providing tremendous utility in many fields. Prior works have limited usefulness as they require either a set of examples of the type to be counted or that the image contains only a single type of object. A significant factor in these shortcomings is the lack of a dataset to properly address counting in settings with more than one kind of object present. To address these issues, we propose the first Multi-class, Class-Agnostic Counting dataset (MCAC) and A Blind Counter (ABC123), a method that can count multiple types of objects simultaneously without using examples of type during training or inference. ABC123 introduces a new paradigm where instead of requiring exemplars to guide the enumeration, examples are found after the counting stage to help a user understand the generated outputs. We show that ABC123 outperforms contemporary methods on MCAC without the requirement of human in-the-loop annotations. We also 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#23433;&#20840;&#32771;&#34385;&#20316;&#20026;&#32467;&#26500;&#20803;&#32032;&#65292;&#20197;&#31995;&#32479;&#32508;&#21512;&#30340;&#26041;&#24335;&#30830;&#20445;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#19968;&#27010;&#24565;&#19981;&#20165;&#19982;&#29616;&#26377;&#30340;&#23433;&#20840;&#26631;&#20934;&#30456;&#22865;&#21512;&#65292;&#36824;&#20026;AI&#23433;&#20840;&#30456;&#20851;&#30340;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#26631;&#20934;&#25552;&#20379;&#20102;&#26032;&#30340;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.03774</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#23433;&#20840;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Safety Concerns in Automated Driving Perception. (arXiv:2309.03774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#23433;&#20840;&#32771;&#34385;&#20316;&#20026;&#32467;&#26500;&#20803;&#32032;&#65292;&#20197;&#31995;&#32479;&#32508;&#21512;&#30340;&#26041;&#24335;&#30830;&#20445;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#19968;&#27010;&#24565;&#19981;&#20165;&#19982;&#29616;&#26377;&#30340;&#23433;&#20840;&#26631;&#20934;&#30456;&#22865;&#21512;&#65292;&#36824;&#20026;AI&#23433;&#20840;&#30456;&#20851;&#30340;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#26631;&#20934;&#25552;&#20379;&#20102;&#26032;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#20197;&#21450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#24863;&#30693;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#23548;&#33268;&#20102;&#23545;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#24212;&#29992;&#30340;&#22686;&#21152;&#38656;&#27714;&#12290;&#36825;&#31867;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#38656;&#35201;&#32771;&#34385;DNNs&#30340;&#29420;&#29305;&#23646;&#24615;&#12290;&#20026;&#20102;&#20197;&#31995;&#32479;&#32508;&#21512;&#30340;&#26041;&#24335;&#30830;&#20445;&#22522;&#20110;DNNs&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#23433;&#20840;&#32771;&#34385;&#20316;&#20026;&#36866;&#24403;&#30340;&#32467;&#26500;&#20803;&#32032;&#12290;&#19968;&#26041;&#38754;&#65292;&#23433;&#20840;&#32771;&#34385;&#30340;&#27010;&#24565;&#35774;&#35745;&#19982;&#29616;&#26377;&#30340;&#19982;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#23433;&#20840;&#30456;&#20851;&#30340;&#26631;&#20934;&#22914;ISO 21448&#65288;SOTIF&#65289;&#38750;&#24120;&#22865;&#21512;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#24050;&#32463;&#28608;&#21457;&#20102;&#20960;&#31687;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#21363;&#23558;&#20986;&#21488;&#30340;&#20851;&#20110;AI&#23433;&#20840;&#30340;&#26631;&#20934;&#65292;&#22914;ISO PAS 8800&#12290;&#34429;&#28982;&#23433;&#20840;&#32771;&#34385;&#30340;&#27010;&#24565;&#20197;&#21069;&#24050;&#32463;&#34987;&#20171;&#32461;&#36807;&#65292;&#20294;&#26412;&#25991;&#23545;&#20854;&#36827;&#34892;&#20102;&#25193;&#23637;&#21644;&#20248;&#21270;&#65292;&#20511;&#37492;&#20102;&#21508;&#20010;&#39046;&#22495;&#21644;&#23433;&#20840;&#19987;&#23478;&#30340;&#21453;&#39304;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in the field of deep learning and impressive performance of deep neural networks (DNNs) for perception have resulted in an increased demand for their use in automated driving (AD) systems. The safety of such systems is of utmost importance and thus requires to consider the unique properties of DNNs.  In order to achieve safety of AD systems with DNN-based perception components in a systematic and comprehensive approach, so-called safety concerns have been introduced as a suitable structuring element. On the one hand, the concept of safety concerns is -- by design -- well aligned to existing standards relevant for safety of AD systems such as ISO 21448 (SOTIF). On the other hand, it has already inspired several academic publications and upcoming standards on AI safety such as ISO PAS 8800.  While the concept of safety concerns has been previously introduced, this paper extends and refines it, leveraging feedback from various domain and safety experts in the field. In par
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#25512;&#21160;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#31934;&#32454;&#35821;&#35328;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;WhyShift&#23454;&#39564;&#24179;&#21488;&#65292;&#24182;&#35752;&#35770;&#20102;$Y|X$-&#20559;&#31227;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.05284</link><description>&lt;p&gt;
&#20851;&#20110;&#38656;&#35201;&#25551;&#36848;&#20998;&#24067;&#20559;&#31227;&#30340;&#35821;&#35328;&#65306;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets. (arXiv:2307.05284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05284
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#25512;&#21160;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#31934;&#32454;&#35821;&#35328;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;WhyShift&#23454;&#39564;&#24179;&#21488;&#65292;&#24182;&#35752;&#35770;&#20102;$Y|X$-&#20559;&#31227;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#20998;&#24067;&#20559;&#31227;&#38656;&#35201;&#19981;&#21516;&#30340;&#31639;&#27861;&#21644;&#25805;&#20316;&#24178;&#39044;&#12290;&#26041;&#27861;&#30740;&#31350;&#24517;&#39035;&#20197;&#20854;&#25152;&#28041;&#21450;&#30340;&#20855;&#20307;&#20559;&#31227;&#20026;&#22522;&#30784;&#12290;&#23613;&#31649;&#26032;&#20852;&#30340;&#22522;&#20934;&#25968;&#25454;&#20026;&#23454;&#35777;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#38544;&#21547;&#22320;&#20851;&#27880;&#21327;&#21464;&#37327;&#20559;&#31227;&#65292;&#24182;&#19988;&#23454;&#35777;&#21457;&#29616;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#20559;&#31227;&#31867;&#22411;&#65292;&#20363;&#22914;&#65292;&#24403;$Y|X$&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#20043;&#21069;&#20851;&#20110;&#31639;&#27861;&#24615;&#33021;&#30340;&#35266;&#23519;&#21487;&#33021;&#26080;&#25928;&#12290;&#25105;&#20204;&#23545;5&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;86,000&#20010;&#27169;&#22411;&#37197;&#32622;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#19968;&#31181;&#31934;&#32454;&#30340;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#35821;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;WhyShift&#65292;&#19968;&#20010;&#30001;&#31574;&#21010;&#30340;&#30495;&#23454;&#19990;&#30028;&#20559;&#31227;&#27979;&#35797;&#24179;&#21488;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#23545;&#25105;&#20204;&#22522;&#20934;&#24615;&#33021;&#30340;&#20559;&#31227;&#31867;&#22411;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#30001;&#20110;$Y|X$-&#20559;&#31227;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#24456;&#24120;&#35265;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21463;&#21040;&#26368;&#22823;$Y|X$-&#20559;&#31227;&#24433;&#21709;&#30340;&#21327;&#21464;&#37327;&#21306;&#22495;&#65292;&#24182;&#35752;&#35770;&#20102;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different distribution shifts require different algorithmic and operational interventions. Methodological research must be grounded by the specific shifts they address. Although nascent benchmarks provide a promising empirical foundation, they implicitly focus on covariate shifts, and the validity of empirical findings depends on the type of shift, e.g., previous observations on algorithmic performance can fail to be valid when the $Y|X$ distribution changes. We conduct a thorough investigation of natural shifts in 5 tabular datasets over 86,000 model configurations, and find that $Y|X$-shifts are most prevalent. To encourage researchers to develop a refined language for distribution shifts, we build WhyShift, an empirical testbed of curated real-world shifts where we characterize the type of shift we benchmark performance over. Since $Y|X$-shifts are prevalent in tabular settings, we identify covariate regions that suffer the biggest $Y|X$-shifts and discuss implications for algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#38754;&#21521;&#24322;&#26500;HPC&#24179;&#21488;&#30340;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#21253;&#25324;GPU&#12289;TPU&#12289;FPGA&#12289;ASIC&#12289;&#31070;&#32463;&#22788;&#29702;&#21333;&#20803;&#21644;RISC-V&#31561;&#65292;&#21516;&#26102;&#20063;&#28085;&#30422;&#20102;&#26032;&#20852;&#20869;&#23384;&#25216;&#26415;&#21644;&#35745;&#31639;&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.15552</link><description>&lt;p&gt;
&#38754;&#21521;&#24322;&#26500;HPC&#24179;&#21488;&#30340;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#22120;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms. (arXiv:2306.15552v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#38754;&#21521;&#24322;&#26500;HPC&#24179;&#21488;&#30340;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#21253;&#25324;GPU&#12289;TPU&#12289;FPGA&#12289;ASIC&#12289;&#31070;&#32463;&#22788;&#29702;&#21333;&#20803;&#21644;RISC-V&#31561;&#65292;&#21516;&#26102;&#20063;&#28085;&#30422;&#20102;&#26032;&#20852;&#20869;&#23384;&#25216;&#26415;&#21644;&#35745;&#31639;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#24212;&#29992;&#20013;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#35821;&#38899;&#35782;&#21035;&#20013;&#25104;&#20026;&#30828;&#20214;&#21152;&#36895;&#22120;&#26368;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#32508;&#36848;&#24635;&#32467;&#21644;&#20998;&#31867;&#20102;&#35774;&#35745;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#22120;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20197;&#28385;&#36275;HPC&#24212;&#29992;&#30340;&#24615;&#33021;&#35201;&#27714;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#24378;&#35843;&#20102;&#25903;&#25345;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#21253;&#25324;&#19981;&#20165;&#38480;&#20110;&#22522;&#20110;GPU&#21644;TPU&#30340;&#21152;&#36895;&#22120;&#65292;&#36824;&#21253;&#25324;&#22522;&#20110;FPGA&#21644;ASIC&#30340;&#29305;&#23450;&#35774;&#35745;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#12289;&#31070;&#32463;&#22788;&#29702;&#21333;&#20803;&#12289;&#22522;&#20110;&#24320;&#25918;&#30828;&#20214;RISC-V&#30340;&#21152;&#36895;&#22120;&#21644;&#21327;&#22788;&#29702;&#22120;&#12290;&#26412;&#32508;&#36848;&#36824;&#25551;&#36848;&#20102;&#22522;&#20110;&#26032;&#20852;&#20869;&#23384;&#25216;&#26415;&#21644;&#35745;&#31639;&#33539;&#24335;&#30340;&#21152;&#36895;&#22120;&#65292;&#20363;&#22914;3D&#22534;&#21472;&#22788;&#29702;&#22120;&#20869;&#23384;&#12289;&#38750;&#26131;&#22833;&#24615;&#23384;&#20648;&#22120;&#65288;&#20027;&#35201;&#26159;&#30005;&#38459;&#24335;&#38543;&#26426;&#23384;&#21462;&#23384;&#20648;&#22120;&#21644;&#30456;&#21464;&#23384;&#20648;&#22120;&#65289;&#23454;&#29616;&#20869;&#23384;&#35745;&#31639;&#65292;&#31070;&#32463;&#24418;&#24577;&#23398;&#22788;&#29702;&#21333;&#20803;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent trends in deep learning (DL) imposed hardware accelerators as the most viable solution for several classes of high-performance computing (HPC) applications such as image classification, computer vision, and speech recognition. This survey summarizes and classifies the most recent advances in designing DL accelerators suitable to reach the performance requirements of HPC applications. In particular, it highlights the most advanced approaches to support deep learning accelerations including not only GPU and TPU-based accelerators but also design-specific hardware accelerators such as FPGA-based and ASIC-based accelerators, Neural Processing Units, open hardware RISC-V-based accelerators and co-processors. The survey also describes accelerators based on emerging memory technologies and computing paradigms, such as 3D-stacked Processor-In-Memory, non-volatile memories (mainly, Resistive RAM and Phase Change Memories) to implement in-memory computing, Neuromorphic Processing Units, a
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25216;&#33021;&#31579;&#36873;&#19982;&#20248;&#21270;&#30340;Skill-Critic&#31639;&#27861;&#33021;&#22815;&#25552;&#39640;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#19979;&#24378;&#21270;&#23398;&#20064;&#20013;&#20302;&#23618;&#31574;&#30053;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08388</link><description>&lt;p&gt;
Skill-Critic: &#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#25216;&#33021;&#30340;&#31579;&#36873;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Skill-Critic: Refining Learned Skills for Reinforcement Learning. (arXiv:2306.08388v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08388
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25216;&#33021;&#31579;&#36873;&#19982;&#20248;&#21270;&#30340;Skill-Critic&#31639;&#27861;&#33021;&#22815;&#25552;&#39640;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#19979;&#24378;&#21270;&#23398;&#20064;&#20013;&#20302;&#23618;&#31574;&#30053;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#26102;&#38388;&#25277;&#35937;&#23558;&#19968;&#20010;&#31574;&#30053;&#20998;&#20026;&#22810;&#20010;&#23618;&#27425;&#65292;&#21152;&#24555;&#38271;&#26399;&#20915;&#31574;&#30340;&#36895;&#24230;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#29615;&#22659;&#20013;&#65292;&#25216;&#33021;&#21363;&#21407;&#22987;&#21160;&#20316;&#30340;&#24207;&#21015;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#26377;&#26395;&#30340;&#32467;&#26524;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#25216;&#33021;&#30340;&#28508;&#22312;&#31354;&#38388;&#21644;&#31574;&#30053;&#26159;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#65292;&#20294;&#30001;&#20110;&#28436;&#31034;&#35206;&#30422;&#33539;&#22260;&#20302;&#25110;&#20998;&#24067;&#36716;&#31227;&#65292;&#25152;&#24471;&#21040;&#30340;&#20302;&#23618;&#31574;&#30053;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Fine-tuning&#20302;&#23618;&#31574;&#30053;&#19982;&#39640;&#23618;&#25216;&#33021;&#36873;&#25321;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;Skill-Critic&#31639;&#27861;&#20248;&#21270;&#20102;&#20302;&#23618;&#21644;&#39640;&#23618;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20174;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#21021;&#22987;&#21270;&#21644;&#35268;&#33539;&#21270;&#65292;&#20197;&#24341;&#23548;&#32852;&#21512;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#31232;&#30095;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;Gran Turismo Sport&#20013;&#26032;&#30340;&#31232;&#30095;&#22870;&#21169;&#33258;&#20027;&#36187;&#36710;&#20219;&#21153;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;Skill-Critic&#30340;&#20302;&#23618;&#31574;&#30053;Fine-tuning&#21644;&#28436;&#31034;&#24341;&#23548;&#31574;&#30053;&#21021;&#22987;&#21270;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical reinforcement learning (RL) can accelerate long-horizon decision-making by temporally abstracting a policy into multiple levels. Promising results in sparse reward environments have been seen with skills, i.e. sequences of primitive actions. Typically, a skill latent space and policy are discovered from offline data, but the resulting low-level policy can be unreliable due to low-coverage demonstrations or distribution shifts. As a solution, we propose fine-tuning the low-level policy in conjunction with high-level skill selection. Our Skill-Critic algorithm optimizes both the low and high-level policies; these policies are also initialized and regularized by the latent space learned from offline demonstrations to guide the joint policy optimization. We validate our approach in multiple sparse RL environments, including a new sparse reward autonomous racing task in Gran Turismo Sport. The experiments show that Skill-Critic's low-level policy fine-tuning and demonstration-g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20108;&#27425;&#38181;&#26494;&#24347;&#19979;&#65292;&#21487;&#20197;&#25214;&#21040;&#26368;&#31232;&#30095;&#30340;&#21521;&#37327;&#65292;&#24471;&#21040;&#20102;&#21487;&#38752;&#30340;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.04647</link><description>&lt;p&gt;
&#21387;&#32553;&#24863;&#30693;&#65306;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Compressed Sensing: A Discrete Optimization Approach. (arXiv:2306.04647v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20108;&#27425;&#38181;&#26494;&#24347;&#19979;&#65292;&#21487;&#20197;&#25214;&#21040;&#26368;&#31232;&#30095;&#30340;&#21521;&#37327;&#65292;&#24471;&#21040;&#20102;&#21487;&#38752;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65292;&#21363;&#25214;&#21040;&#26368;&#31232;&#30095;&#30340;&#21521;&#37327;&#65292;&#35813;&#21521;&#37327;&#28385;&#36275;&#19968;&#32452;&#32447;&#24615;&#27979;&#37327;&#65292;&#21516;&#26102;&#36798;&#21040;&#19968;&#23450;&#30340;&#25968;&#20540;&#23481;&#38480;&#12290;&#21387;&#32553;&#24863;&#30693;&#26159;&#32479;&#35745;&#23398;&#12289;&#36816;&#31609;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26680;&#24515;&#38382;&#39064;&#65292;&#24212;&#29992;&#20110;&#20449;&#21495;&#22788;&#29702;&#12289;&#25968;&#25454;&#21387;&#32553;&#21644;&#22270;&#20687;&#37325;&#24314;&#31561;&#39046;&#22495;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;$\ell_2$&#27491;&#21017;&#21270;&#30340;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65292;&#23558;&#20854;&#20316;&#20026;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#38181;&#35268;&#21010;&#26469;&#37325;&#26032;&#23450;&#20041;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#27492;&#38382;&#39064;&#30340;&#20108;&#27425;&#38181;&#26494;&#24347;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#28201;&#21644;&#38480;&#21046;&#19979;&#65292;&#24471;&#21040;&#30340;&#26494;&#24347;&#31561;&#20215;&#20110;&#28145;&#20837;&#30740;&#31350;&#30340;&#22522;&#30784;&#36861;&#36394;&#21435;&#22122;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#23450;&#26494;&#24347;&#26469;&#21152;&#24378;&#20108;&#27425;&#38181;&#26494;&#24347;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#20108;&#27425;&#38181;&#26494;&#24347;&#26469;&#35299;&#20915;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#30340;&#23454;&#20363;&#65292;&#20197;&#30830;&#35777;&#30340;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#31934;&#30830;&#30340;&#65292;&#24182;&#19988;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the Compressed Sensing (CS) problem, which is the problem of finding the most sparse vector that satisfies a set of linear measurements up to some numerical tolerance. CS is a central problem in Statistics, Operations Research and Machine Learning which arises in applications such as signal processing, data compression and image reconstruction. We introduce an $\ell_2$ regularized formulation of CS which we reformulate as a mixed integer second order cone program. We derive a second order cone relaxation of this problem and show that under mild conditions on the regularization parameter, the resulting relaxation is equivalent to the well studied basis pursuit denoising problem. We present a semidefinite relaxation that strengthens the second order cone relaxation and develop a custom branch-and-bound algorithm that leverages our second order cone relaxation to solve instances of CS to certifiable optimality. Our numerical results show that our approach produces solutions that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22810;&#38454;&#27573;&#20307;&#31995;&#32467;&#26500;&#21644;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#19987;&#38376;&#29992;&#20110;&#22522;&#20110;&#36816;&#21160;&#23398;&#25968;&#25454;&#30340;&#34892;&#21160;&#20998;&#21106;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#22312;&#19977;&#20010;&#25163;&#26415;&#32541;&#21512;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2303.07814</link><description>&lt;p&gt;
&#22522;&#20110;&#36816;&#21160;&#23398;&#25968;&#25454;&#30340;&#25163;&#26415;&#34892;&#20026;&#20999;&#20998;
&lt;/p&gt;
&lt;p&gt;
Kinematic Data-Based Action Segmentation for Surgical Applications. (arXiv:2303.07814v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22810;&#38454;&#27573;&#20307;&#31995;&#32467;&#26500;&#21644;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#19987;&#38376;&#29992;&#20110;&#22522;&#20110;&#36816;&#21160;&#23398;&#25968;&#25454;&#30340;&#34892;&#21160;&#20998;&#21106;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#22312;&#19977;&#20010;&#25163;&#26415;&#32541;&#21512;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#21160;&#20999;&#20998;&#26159;&#39640;&#32423;&#27969;&#31243;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#36890;&#24120;&#22312;&#35270;&#39057;&#25110;&#20174;&#21508;&#31181;&#20256;&#24863;&#22120;&#33719;&#21462;&#30340;&#36816;&#21160;&#23398;&#25968;&#25454;&#19978;&#25191;&#34892;&#12290;&#22312;&#25163;&#26415;&#36807;&#31243;&#20013;&#65292;&#34892;&#21160;&#20999;&#20998;&#23545;&#20110;&#24037;&#20316;&#27969;&#20998;&#26512;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#19982;&#36816;&#21160;&#23398;&#25968;&#25454;&#30456;&#20851;&#30340;&#34892;&#21160;&#20998;&#21106;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#22810;&#38454;&#27573;&#20307;&#31995;&#32467;&#26500;&#65292;MS-TCN-BiLSTM&#21644;MS-TCN-BiGRU&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36816;&#21160;&#23398;&#25968;&#25454;&#12290; &#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30001;&#20855;&#26377;&#38454;&#20869;&#35268;&#21017;&#21270;&#21644;&#21452;&#21521;LSTM&#25110;GRU&#30340;&#32454;&#21270;&#38454;&#27573;&#30340;&#39044;&#27979;&#29983;&#25104;&#22120;&#32452;&#25104;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;World Frame Rotation&#21644;Horizontal-Flip&#65292;&#21033;&#29992;&#36816;&#21160;&#23398;&#25968;&#25454;&#30340;&#24378;&#20960;&#20309;&#32467;&#26500;&#26469;&#25552;&#39640;&#31639;&#27861;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25163;&#26415;&#32541;&#21512;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65306;&#21487;&#21464;&#32452;&#32455;&#27169;&#25311;&#65288;VTS&#65289;&#25968;&#25454;&#38598;&#21644;&#26032;&#25512;&#20986;&#30340;&#32928;&#36947;&#20462;&#22797;&#27169;&#25311;&#65288;BRS&#65289;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Action segmentation is a challenging task in high-level process analysis, typically performed on video or kinematic data obtained from various sensors. In the context of surgical procedures, action segmentation is critical for workflow analysis algorithms. This work presents two contributions related to action segmentation on kinematic data. Firstly, we introduce two multi-stage architectures, MS-TCN-BiLSTM and MS-TCN-BiGRU, specifically designed for kinematic data. The architectures consist of a prediction generator with intra-stage regularization and Bidirectional LSTM or GRU-based refinement stages. Secondly, we propose two new data augmentation techniques, World Frame Rotation and Horizontal-Flip, which utilize the strong geometric structure of kinematic data to improve algorithm performance and robustness. We evaluate our models on three datasets of surgical suturing tasks: the Variable Tissue Simulation (VTS) Dataset and the newly introduced Bowel Repair Simulation (BRS) Dataset,
&lt;/p&gt;</description></item><item><title>SynthMorph&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#19978;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#37319;&#29992;&#20102;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#22270;&#20687;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;</title><link>http://arxiv.org/abs/2301.11329</link><description>&lt;p&gt;
SynthMorph&#23454;&#29616;&#30340;&#32771;&#34385;&#35299;&#21078;&#32467;&#26500;&#21644;&#26080;&#20851;&#37319;&#38598;&#26041;&#27861;&#30340;&#32852;&#21512;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
Anatomy-aware and acquisition-agnostic joint registration with SynthMorph. (arXiv:2301.11329v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11329
&lt;/p&gt;
&lt;p&gt;
SynthMorph&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#19978;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#37319;&#29992;&#20102;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#22270;&#20687;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#23556;&#22270;&#20687;&#37197;&#20934;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#22522;&#30707;&#12290;&#34429;&#28982;&#20256;&#32479;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#20248;&#31168;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#20026;&#27599;&#19968;&#23545;&#22270;&#20687;&#36827;&#34892;&#32791;&#26102;&#30340;&#20248;&#21270;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23558;&#22270;&#20687;&#23545;&#26144;&#23556;&#21040;&#36755;&#20986;&#21464;&#25442;&#30340;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35780;&#20272;&#36825;&#20010;&#20989;&#25968;&#26159;&#24555;&#36895;&#30340;&#65292;&#20294;&#25429;&#25417;&#22823;&#30340;&#21464;&#25442;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#32780;&#19988;&#22914;&#26524;&#27979;&#35797;&#22270;&#20687;&#30340;&#29305;&#24449;&#20174;&#35757;&#32451;&#39046;&#22495;&#21464;&#21270;&#65292;&#22914;&#20998;&#36776;&#29575;&#65292;&#32593;&#32476;&#24448;&#24448;&#20250;&#20986;&#29616;&#22256;&#38590;&#12290;&#22823;&#22810;&#25968;&#20223;&#23556;&#26041;&#27861;&#26159;&#23545;&#35299;&#21078;&#32467;&#26500;&#26080;&#30693;&#30340;&#65292;&#24847;&#21619;&#30528;&#22914;&#26524;&#31639;&#27861;&#32771;&#34385;&#22270;&#20687;&#20013;&#30340;&#25152;&#26377;&#32467;&#26500;&#65292;&#37197;&#20934;&#20250;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#36890;&#36807;SynthMorph&#35299;&#20915;&#20102;&#36825;&#20123;&#32570;&#28857;&#65292;&#23427;&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#36827;&#34892;&#25805;&#20316;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#30340;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#30340;&#22270;&#20687;&#26469;&#35757;&#32451;&#32593;&#32476;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#30340;&#22810;&#26679;&#21270;&#37319;&#38598;&#35268;&#33539;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20248;&#21270;&#32593;&#32476;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#20854;&#33021;&#22815;&#32771;&#34385;&#19981;&#21516;&#30340;&#35299;&#21078;&#29305;&#24449;&#21644;&#23398;&#20064;&#25269;&#21046;&#37319;&#38598;&#29305;&#23450;&#38480;&#21046;&#30340;&#21464;&#25442;&#12290;&#36890;&#36807;&#36825;&#20123;&#21019;&#26032;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Affine image registration is a cornerstone of medical-image analysis. While classical algorithms can achieve excellent accuracy, they solve a time-consuming optimization for every image pair. Deep-learning (DL) methods learn a function that maps an image pair to an output transform. Evaluating the function is fast, but capturing large transforms can be challenging, and networks tend to struggle if a test-image characteristic shifts from the training domain, such as resolution. Most affine methods are agnostic to anatomy, meaning the registration will be inaccurate if algorithms consider all structures in the image.  We address these shortcomings with SynthMorph, an easy-to-use DL tool for joint affine-deformable registration of any brain image without preprocessing, right off the MRI scanner. First, we leverage a strategy to train networks with wildly varying images synthesized from label maps, yielding robust performance across acquisition specifics unseen at training. Second, we opti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20102;&#35299;&#20915;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#25805;&#32437;&#20013;&#30340;&#21487;&#20998;&#31163;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#23450;&#20041;&#22522;&#20110;&#30456;&#20851;&#25552;&#31034;&#30340;&#35821;&#26009;&#24211;&#23376;&#31354;&#38388;&#26469;&#33719;&#21462;&#29305;&#23450;&#22270;&#20687;&#29305;&#24449;&#24182;&#24341;&#20837;CLIP&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#65288;PAE&#65289;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#22788;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.03919</link><description>&lt;p&gt;
CLIP-PAE&#65306;&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#20197;&#25552;&#21462;&#30456;&#20851;&#29305;&#24449;&#29992;&#20110;&#21487;&#20998;&#31163;&#12289;&#21487;&#35299;&#37322;&#12289;&#21487;&#25511;&#30340;&#25991;&#26412;&#25351;&#23548;&#33080;&#37096;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable, and Controllable Text-Guided Face Manipulation. (arXiv:2210.03919v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03919
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20102;&#35299;&#20915;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#25805;&#32437;&#20013;&#30340;&#21487;&#20998;&#31163;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#23450;&#20041;&#22522;&#20110;&#30456;&#20851;&#25552;&#31034;&#30340;&#35821;&#26009;&#24211;&#23376;&#31354;&#38388;&#26469;&#33719;&#21462;&#29305;&#23450;&#22270;&#20687;&#29305;&#24449;&#24182;&#24341;&#20837;CLIP&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#65288;PAE&#65289;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#22788;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24341;&#20837;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#21040;&#20849;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#36825;&#25171;&#24320;&#20102;&#19968;&#20010;&#22823;&#38376;&#65292;&#21363;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#25991;&#23383;&#35828;&#26126;&#26469;&#25805;&#20316;&#36755;&#20837;&#22270;&#20687;&#30340;&#20016;&#23500;&#25991;&#23398;&#36164;&#26009;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32852;&#21512;&#31354;&#38388;&#20013;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#23558;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#36890;&#24120;&#20250;&#23548;&#33268;&#32467;&#26524;&#22270;&#20687;&#20013;&#20986;&#29616;&#24847;&#22806;&#30340;&#20266;&#24433;&#12290;&#23545;&#20110;&#25805;&#32437;&#26469;&#35828;&#65292;&#21487;&#20998;&#31163;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#20063;&#24456;&#38590;&#20445;&#35777;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23450;&#20041;&#30001;&#30456;&#20851;&#25552;&#31034;&#23637;&#24320;&#30340;&#35821;&#26009;&#24211;&#23376;&#31354;&#38388;&#26469;&#25429;&#33719;&#29305;&#23450;&#30340;&#22270;&#20687;&#29305;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CLIP&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#65288;PAE&#65289;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#25552;&#39640;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#25805;&#32437;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#33539;&#20363;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#35745;&#31639;&#21644;&#36866;&#24212;&#65292;&#24182;&#24179;&#31283;&#22320;&#34701;&#20837;&#21040;&#20219;&#20309;&#22522;&#20110;CLIP&#30340;&#22270;&#20687;&#25805;&#20316;&#31639;&#27861;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently introduced Contrastive Language-Image Pre-Training (CLIP) bridges images and text by embedding them into a joint latent space. This opens the door to ample literature that aims to manipulate an input image by providing a textual explanation. However, due to the discrepancy between image and text embeddings in the joint space, using text embeddings as the optimization target often introduces undesired artifacts in the resulting images. Disentanglement, interpretability, and controllability are also hard to guarantee for manipulation. To alleviate these problems, we propose to define corpus subspaces spanned by relevant prompts to capture specific image characteristics. We introduce CLIP Projection-Augmentation Embedding (PAE) as an optimization target to improve the performance of text-guided image manipulation. Our method is a simple and general paradigm that can be easily computed and adapted, and smoothly incorporated into any CLIP-based image manipulation algorithm. To demo
&lt;/p&gt;</description></item></channel></rss>