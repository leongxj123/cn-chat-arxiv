<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#20998;&#23376;&#21147;&#23398;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;Grappa&#65292;&#36890;&#36807;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#21644;transformer&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#65292;&#33021;&#22312;&#29616;&#26377;MD&#24341;&#25806;&#20013;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2404.00050</link><description>&lt;p&gt;
Grappa--&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#23376;&#21147;&#23398;&#21183;&#22330;
&lt;/p&gt;
&lt;p&gt;
Grappa -- A Machine Learned Molecular Mechanics Force Field
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00050
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#20998;&#23376;&#21147;&#23398;&#21442;&#25968;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;Grappa&#65292;&#36890;&#36807;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#21644;transformer&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#65292;&#33021;&#22312;&#29616;&#26377;MD&#24341;&#25806;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#22823;&#20998;&#23376;&#31995;&#32479;&#38656;&#35201;&#26082;&#20934;&#30830;&#21448;&#39640;&#25928;&#30340;&#21147;&#22330;&#12290;&#36817;&#24180;&#26469;&#65292;E(3)&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#21147;&#22330;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24352;&#21147;&#65292;&#20294;&#20854;&#20195;&#20215;&#20173;&#27604;&#20256;&#32479;&#30340;&#20998;&#23376;&#21147;&#23398;&#65288;MM&#65289;&#21147;&#22330;&#26114;&#36149;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#21644;&#20855;&#26377;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#20301;&#32622;&#32534;&#30721;&#30340;transformer&#26469;&#39044;&#27979;&#20998;&#23376;&#22270;&#20013;&#30340;MM&#21442;&#25968;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#21147;&#22330;Grappa&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#24050;&#24314;&#31435;&#30340;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#30340;MM&#21147;&#22330;&#65292;&#24182;&#19988;&#33021;&#20197;&#30456;&#21516;&#30340;&#35745;&#31639;&#25928;&#29575;&#22312;&#29616;&#26377;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#24341;&#25806;&#22914;GROMACS&#21644;OpenMM&#20013;&#20351;&#29992;&#12290;&#23427;&#21487;&#20197;&#39044;&#27979;&#23567;&#20998;&#23376;&#12289;&#32957;&#12289;RNA&#30340;&#33021;&#37327;&#21644;&#21147;--&#23637;&#31034;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#21040;&#26410;&#30693;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00050v1 Announce Type: cross  Abstract: Simulating large molecular systems over long timescales requires force fields that are both accurate and efficient. In recent years, E(3) equivariant neural networks have lifted the tension between computational efficiency and accuracy of force fields, but they are still several orders of magnitude more expensive than classical molecular mechanics (MM) force fields.   Here, we propose a novel machine learning architecture to predict MM parameters from the molecular graph, employing a graph attentional neural network and a transformer with symmetry-preserving positional encoding. The resulting force field, Grappa, outperforms established and other machine-learned MM force fields in terms of accuracy at the same computational efficiency and can be used in existing Molecular Dynamics (MD) engines like GROMACS and OpenMM. It predicts energies and forces of small molecules, peptides, RNA and - showcasing its extensibility to uncharted regio
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26377;&#26395;&#25552;&#20379;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24212;&#27880;&#24847;&#20854;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#31215;&#26497;&#37319;&#21462;&#31574;&#30053;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.14814</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#26426;&#20250;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
The opportunities and risks of large language models in mental health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14814
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26377;&#26395;&#25552;&#20379;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24212;&#27880;&#24847;&#20854;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#31215;&#26497;&#37319;&#21462;&#31574;&#30053;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#21457;&#29983;&#29575;&#27491;&#22312;&#19978;&#21319;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#29616;&#26377;&#30340;&#24515;&#29702;&#20445;&#20581;&#27169;&#24335;&#26080;&#27861;&#20805;&#20998;&#25193;&#23637;&#20197;&#28385;&#36275;&#38656;&#27714;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#20855;&#26377;&#21019;&#36896;&#26032;&#39062;&#12289;&#22823;&#35268;&#27169;&#35299;&#20915;&#26041;&#26696;&#20197;&#25903;&#25345;&#24515;&#29702;&#20581;&#24247;&#30340;&#25215;&#35834;&#24863;&#21040;&#20048;&#35266;&#12290;&#23613;&#31649;&#23427;&#20204;&#36824;&#22788;&#20110;&#21021;&#26399;&#38454;&#27573;&#65292;LLMs&#24050;&#34987;&#24212;&#29992;&#20110;&#19982;&#24515;&#29702;&#20581;&#24247;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#24050;&#26377;&#25991;&#29486;&#20013;&#20851;&#20110;&#21033;&#29992;LLMs&#25552;&#20379;&#24515;&#29702;&#20581;&#24247;&#25945;&#32946;&#12289;&#35780;&#20272;&#21644;&#24178;&#39044;&#30340;&#21162;&#21147;&#65292;&#24182;&#31361;&#20986;&#20102;&#27599;&#20010;&#39046;&#22495;&#20013;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#30340;&#20851;&#38190;&#26426;&#20250;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23558;LLMs&#24212;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#25152;&#20276;&#38543;&#30340;&#39118;&#38505;&#65292;&#24182;&#40723;&#21169;&#37319;&#29992;&#31574;&#30053;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#23545;&#20110;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#36843;&#20999;&#38656;&#27714;&#24517;&#39035;&#19982;&#36127;&#36131;&#20219;&#30340;&#24515;&#29702;&#20581;&#24247;LLMs&#30340;&#24320;&#21457;&#12289;&#27979;&#35797;&#21644;&#37096;&#32626;&#30456;&#24179;&#34913;&#12290;&#29305;&#21035;&#20851;&#38190;&#30340;&#26159;&#30830;&#20445;&#24515;&#29702;&#20581;&#24247;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14814v1 Announce Type: cross  Abstract: Global rates of mental health concerns are rising and there is increasing realization that existing models of mental healthcare will not adequately expand to meet the demand. With the emergence of large language models (LLMs) has come great optimism regarding their promise to create novel, large-scale solutions to support mental health. Despite their nascence, LLMs have already been applied to mental health-related tasks. In this review, we summarize the extant literature on efforts to use LLMs to provide mental health education, assessment, and intervention and highlight key opportunities for positive impact in each area. We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks. The urgent need for mental health support must be balanced with responsible development, testing, and deployment of mental health LLMs. Especially critical is ensuring that mental he
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignRec&#30340;&#23545;&#40784;&#21644;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25512;&#33616;&#30446;&#26631;&#20998;&#35299;&#20026;&#19977;&#20010;&#23545;&#40784;&#37096;&#20998;&#65292;&#23454;&#29616;&#20869;&#23481;&#20869;&#37096;&#23545;&#40784;&#12289;&#20869;&#23481;&#19982;&#20998;&#31867;ID&#20043;&#38388;&#30340;&#23545;&#40784;&#20197;&#21450;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.12384</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#25512;&#33616;&#30340;&#23545;&#40784;&#21644;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Aligning and Training Framework for Multimodal Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12384
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignRec&#30340;&#23545;&#40784;&#21644;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#25512;&#33616;&#30446;&#26631;&#20998;&#35299;&#20026;&#19977;&#20010;&#23545;&#40784;&#37096;&#20998;&#65292;&#23454;&#29616;&#20869;&#23481;&#20869;&#37096;&#23545;&#40784;&#12289;&#20869;&#23481;&#19982;&#20998;&#31867;ID&#20043;&#38388;&#30340;&#23545;&#40784;&#20197;&#21450;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#23186;&#20307;&#24212;&#29992;&#30340;&#21457;&#23637;&#65292;&#22810;&#27169;&#24577;&#25512;&#33616;&#27491;&#22312;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#21033;&#29992;&#36229;&#36234;&#29992;&#25143;&#20132;&#20114;&#30340;&#20016;&#23500;&#19978;&#19979;&#25991;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#35270;&#20026;&#36741;&#21161;&#65292;&#29992;&#20110;&#24110;&#21161;&#23398;&#20064;ID&#29305;&#24449;&#65307;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#20869;&#23481;&#29305;&#24449;&#21644;ID&#29305;&#24449;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#24046;&#36317;&#65292;&#30452;&#25509;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#20316;&#20026;&#36741;&#21161;&#20351;&#29992;&#20250;&#23548;&#33268;&#29992;&#25143;&#21644;&#39033;&#30446;&#34920;&#31034;&#30340;&#19981;&#23545;&#40784;&#12290;&#26412;&#25991;&#39318;&#20808;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignRec&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;AlignRec&#20013;&#65292;&#25512;&#33616;&#30446;&#26631;&#34987;&#20998;&#35299;&#20026;&#19977;&#20010;&#23545;&#40784;&#37096;&#20998;&#65292;&#21363;&#20869;&#23481;&#20869;&#37096;&#23545;&#40784;&#65292;&#20869;&#23481;&#19982;&#20998;&#31867;ID&#20043;&#38388;&#30340;&#23545;&#40784;&#20197;&#21450;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#27599;&#20010;&#23545;&#40784;&#37096;&#20998;&#37117;&#30001;&#29305;&#23450;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#34920;&#24449;&#65292;&#24182;&#25972;&#21512;&#21040;&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12384v1 Announce Type: cross  Abstract: With the development of multimedia applications, multimodal recommendations are playing an essential role, as they can leverage rich contexts beyond user interactions. Existing methods mainly regard multimodal information as an auxiliary, using them to help learn ID features; however, there exist semantic gaps among multimodal content features and ID features, for which directly using multimodal information as an auxiliary would lead to misalignment in representations of users and items. In this paper, we first systematically investigate the misalignment issue in multimodal recommendations, and propose a solution named AlignRec. In AlignRec, the recommendation objective is decomposed into three alignments, namely alignment within contents, alignment between content and categorical ID, and alignment between users and items. Each alignment is characterized by a specific objective function and is integrated into our multimodal recommendat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65292;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.09918</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Attention-based Class-Conditioned Alignment for Multi-Source Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09918
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65292;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#65288;OD&#65289;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#33268;&#21147;&#20110;&#36890;&#36807;&#20419;&#36827;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#29305;&#24449;&#23545;&#40784;&#26469;&#32531;&#35299;&#20998;&#24067;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#20801;&#35768;&#21033;&#29992;&#22810;&#20010;&#24102;&#27880;&#37322;&#30340;&#28304;&#25968;&#25454;&#38598;&#21644;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#26469;&#25552;&#39640;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;OD MSDA&#26041;&#27861;&#20197;&#19968;&#31181;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#26041;&#24335;&#25191;&#34892;&#29305;&#24449;&#23545;&#40784;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#25353;&#31867;&#21035;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#22024;&#26434;&#30340;&#20266;&#26631;&#31614;&#32780;&#23548;&#33268;&#38169;&#35823;&#31215;&#32047;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;MSDA&#65292;&#35813;&#26041;&#26696;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09918v1 Announce Type: cross  Abstract: Domain adaptation methods for object detection (OD) strive to mitigate the impact of distribution shifts by promoting feature alignment across source and target domains. Multi-source domain adaptation (MSDA) allows leveraging multiple annotated source datasets, and unlabeled target data to improve the accuracy and robustness of the detection model. Most state-of-the-art MSDA methods for OD perform feature alignment in a class-agnostic manner. This is challenging since the objects have unique modal information due to variations in object appearance across domains. A recent prototype-based approach proposed a class-wise alignment, yet it suffers from error accumulation due to noisy pseudo-labels which can negatively affect adaptation with imbalanced data. To overcome these limitations, we propose an attention-based class-conditioned alignment scheme for MSDA that aligns instances of each object category across domains. In particular, an 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31062;&#21338;&#22827;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#23545;&#24212;&#30340;&#26446;&#38597;&#26222;&#35834;&#22827;&#35777;&#20070;&#65292;&#20197;&#26368;&#22823;&#21270;&#21306;&#22495;&#21560;&#24341;&#21147;&#65292;&#24182;&#23562;&#37325;&#28608;&#21169;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.08448</link><description>&lt;p&gt;
&#12298;&#22522;&#20110;&#28436;&#21592;&#35780;&#35770;&#32773;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#26446;&#38597;&#26222;&#35834;&#22827;&#25511;&#21046;&#12299;
&lt;/p&gt;
&lt;p&gt;
Actor-Critic Physics-informed Neural Lyapunov Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08448
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31062;&#21338;&#22827;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#23545;&#24212;&#30340;&#26446;&#38597;&#26222;&#35834;&#22827;&#35777;&#20070;&#65292;&#20197;&#26368;&#22823;&#21270;&#21306;&#22495;&#21560;&#24341;&#21147;&#65292;&#24182;&#23562;&#37325;&#28608;&#21169;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20855;&#26377;&#21487;&#35777;&#20445;&#35777;&#30340;&#31283;&#23450;&#21270;&#20219;&#21153;&#25511;&#21046;&#31574;&#30053;&#26159;&#38750;&#32447;&#24615;&#25511;&#21046;&#20013;&#30340;&#19968;&#20010;&#38271;&#26399;&#38382;&#39064;&#12290;&#20851;&#38190;&#30340;&#24615;&#33021;&#25351;&#26631;&#26159;&#20135;&#29983;&#21306;&#22495;&#21560;&#24341;&#21147;&#30340;&#22823;&#23567;&#65292;&#36825;&#22522;&#26412;&#19978;&#20805;&#24403;&#20102;&#23553;&#38381;&#29615;&#31995;&#32479;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#24377;&#24615;&#8220;&#36793;&#30028;&#8221;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#19968;&#20010;&#31283;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#20197;&#21450;&#20854;&#23545;&#24212;&#30340;&#26446;&#38597;&#26222;&#35834;&#22827;&#35777;&#20070;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20135;&#29983;&#30340;&#21306;&#22495;&#21560;&#24341;&#21147;&#65292;&#21516;&#26102;&#23562;&#37325;&#28608;&#21169;&#32422;&#26463;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#20043;&#22788;&#22312;&#20110;&#20351;&#29992;&#31062;&#21338;&#22827;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#65292;&#35813;&#26041;&#31243;&#31934;&#30830;&#22320;&#34920;&#24449;&#20102;&#32473;&#23450;&#25511;&#21046;&#31574;&#30053;&#30340;&#30495;&#23454;&#21306;&#22495;&#21560;&#24341;&#21147;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36981;&#24490;&#28436;&#21592;&#35780;&#35770;&#32773;&#27169;&#24335;&#65292;&#25105;&#20204;&#22312;&#25913;&#36827;&#25511;&#21046;&#31574;&#30053;&#65288;&#28436;&#21592;&#65289;&#21644;&#23398;&#20064;&#31062;&#21338;&#22827;&#20989;&#25968;&#65288;&#35780;&#35770;&#32773;&#65289;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35843;&#29992;SMT&#27714;&#35299;&#22120;&#35745;&#31639;&#20986;&#26368;&#22823;&#30340;&#21487;&#35777;&#21306;&#22495;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08448v1 Announce Type: new  Abstract: Designing control policies for stabilization tasks with provable guarantees is a long-standing problem in nonlinear control. A crucial performance metric is the size of the resulting region of attraction, which essentially serves as a robustness "margin" of the closed-loop system against uncertainties. In this paper, we propose a new method to train a stabilizing neural network controller along with its corresponding Lyapunov certificate, aiming to maximize the resulting region of attraction while respecting the actuation constraints. Crucial to our approach is the use of Zubov's Partial Differential Equation (PDE), which precisely characterizes the true region of attraction of a given control policy. Our framework follows an actor-critic pattern where we alternate between improving the control policy (actor) and learning a Zubov function (critic). Finally, we compute the largest certifiable region of attraction by invoking an SMT solver
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#36866;&#21512;&#30340;Q&#36845;&#20195;&#30340;&#25209;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#23454;&#29616;&#30446;&#26631;&#26102;&#19981;&#20135;&#29983;&#25104;&#26412;&#30340;&#38382;&#39064;&#20013;&#65292;&#20854;&#26679;&#26412;&#25968;&#37327;&#38656;&#27714;&#19982;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#25104;&#27604;&#20363;&#65292;&#33021;&#22815;&#25552;&#20379;&#19982;&#26368;&#20248;&#21487;&#36798;&#25104;&#26412;&#25104;&#27604;&#20363;&#30340;&#8220;&#23567;&#25104;&#26412;&#8221;&#30028;&#38480;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#22312;&#37027;&#20123;&#26368;&#20248;&#31574;&#30053;&#21487;&#38752;&#23454;&#29616;&#30446;&#26631;&#30340;&#38382;&#39064;&#20013;&#65292;FQI-LOG&#27604;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;FQI&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.05385</link><description>&lt;p&gt;
&#22312;&#25209;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20999;&#25442;&#25439;&#22833;&#20989;&#25968;&#26469;&#38477;&#20302;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Switching the Loss Reduces the Cost in Batch Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05385
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#36866;&#21512;&#30340;Q&#36845;&#20195;&#30340;&#25209;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#23454;&#29616;&#30446;&#26631;&#26102;&#19981;&#20135;&#29983;&#25104;&#26412;&#30340;&#38382;&#39064;&#20013;&#65292;&#20854;&#26679;&#26412;&#25968;&#37327;&#38656;&#27714;&#19982;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#25104;&#27604;&#20363;&#65292;&#33021;&#22815;&#25552;&#20379;&#19982;&#26368;&#20248;&#21487;&#36798;&#25104;&#26412;&#25104;&#27604;&#20363;&#30340;&#8220;&#23567;&#25104;&#26412;&#8221;&#30028;&#38480;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#22312;&#37027;&#20123;&#26368;&#20248;&#31574;&#30053;&#21487;&#38752;&#23454;&#29616;&#30446;&#26631;&#30340;&#38382;&#39064;&#20013;&#65292;FQI-LOG&#27604;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;FQI&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#65288;FQI-LOG&#65289;&#26469;&#35757;&#32451;&#36866;&#21512;&#30340;Q&#36845;&#20195;&#30340;&#25209;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;FQI-LOG&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#31574;&#30053;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#19982;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#25104;&#27604;&#20363;&#65292;&#23545;&#20110;&#37027;&#20123;&#36890;&#36807;&#26368;&#20248;&#34892;&#20026;&#23454;&#29616;&#30446;&#26631;&#19988;&#19981;&#20135;&#29983;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#20026;&#38646;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#25209;RL&#20013;&#35777;&#26126;&#20855;&#26377;&#19982;&#26368;&#20248;&#21487;&#36798;&#25104;&#26412;&#25104;&#27604;&#20363;&#30340;&#8220;&#23567;&#25104;&#26412;&#8221;&#30028;&#38480;&#30340;&#19968;&#33324;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#32463;&#39564;&#19978;&#39564;&#35777;&#65292;FQI-LOG&#22312;&#37027;&#20123;&#26368;&#20248;&#31574;&#30053;&#21487;&#38752;&#22320;&#23454;&#29616;&#30446;&#26631;&#30340;&#38382;&#39064;&#19978;&#20351;&#29992;&#30340;&#26679;&#26412;&#27604;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;FQI&#35201;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05385v1 Announce Type: new  Abstract: We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch reinforcement learning (RL). We show that the number of samples needed to learn a near-optimal policy with FQI-LOG scales with the accumulated cost of the optimal policy, which is zero in problems where acting optimally achieves the goal and incurs no cost. In doing so, we provide a general framework for proving $\textit{small-cost}$ bounds, i.e. bounds that scale with the optimal achievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG uses fewer samples than FQI trained with squared loss on problems where the optimal policy reliably achieves the goal.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#30693;&#20803;&#23398;&#20064;&#36825;&#19968;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30693;&#35782;&#20849;&#20139;&#65292;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#25269;&#24481;&#35266;&#27979;&#22122;&#22768;&#12290;</title><link>https://arxiv.org/abs/2402.16105</link><description>&lt;p&gt;
&#36890;&#30693;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Informed Meta-Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#30693;&#20803;&#23398;&#20064;&#36825;&#19968;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30693;&#35782;&#20849;&#20139;&#65292;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#25269;&#24481;&#35266;&#27979;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#30427;&#34892;&#30340;&#22024;&#26434;&#21644;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#31361;&#20986;&#30340;&#25361;&#25112;&#22312;&#20110;&#26377;&#25928;&#22320;&#34701;&#21512;&#20419;&#36827;&#25968;&#25454;&#25928;&#29575;&#21644;&#31283;&#20581;&#24615;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#20803;&#23398;&#20064;&#21644;&#36890;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#20004;&#31181;&#23558;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#26041;&#27861;&#12290;&#21069;&#32773;&#20381;&#36182;&#20110;&#19968;&#31181;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#20808;&#39564;&#26469;&#28304;&#65292;&#32780;&#21518;&#32773;&#21463;&#19987;&#23478;&#30693;&#35782;&#30340;&#24418;&#24335;&#21270;&#34920;&#31034;&#24341;&#23548;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#33539;&#24335;&#65292;&#36890;&#30693;&#20803;&#23398;&#20064;&#65292;&#26088;&#22312;&#23454;&#29616;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#36328;&#20219;&#21153;&#30693;&#35782;&#20849;&#20139;&#30340;&#20114;&#34917;&#24615;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#36890;&#30693;&#20803;&#23398;&#20064;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#36825;&#19968;&#26694;&#26550;&#30340;&#20855;&#20307;&#23454;&#20363;--&#36890;&#30693;&#31070;&#32463;&#36807;&#31243;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#35828;&#26126;&#24615;&#21644;&#26356;&#22823;&#35268;&#27169;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#30693;&#20803;&#23398;&#20064;&#22312;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#25269;&#24481;&#35266;&#27979;&#22122;&#22768;&#26041;&#38754;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16105v1 Announce Type: new  Abstract: In noisy and low-data regimes prevalent in real-world applications, an outstanding challenge of machine learning lies in effectively incorporating inductive biases that promote data efficiency and robustness. Meta-learning and informed ML stand out as two approaches for incorporating prior knowledge into the ML pipeline. While the former relies on a purely data-driven source of priors, the latter is guided by a formal representation of expert knowledge. This paper introduces a novel hybrid paradigm, informed meta-learning, seeking complementarity in cross-task knowledge sharing of humans and machines. We establish the foundational components of informed meta-learning and present a concrete instantiation of this framework--the Informed Neural Process. Through a series of illustrative and larger-scale experiments, we demonstrate the potential benefits of informed meta-learning in improving data efficiency and robustness to observational no
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31574;&#30053;&#32467;&#26500;&#20808;&#39564;&#30340;&#39640;&#25928;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#31995;&#32479;&#65292;&#36890;&#36807;&#20869;&#22806;&#29615;&#30340;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25235;&#21462;&#12289;&#35266;&#23519;&#21644;&#25918;&#32622;&#22312;&#24863;&#30693;&#22122;&#22768;&#20013;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.15402</link><description>&lt;p&gt;
&#25235;&#21462;&#12289;&#35266;&#23519;&#21644;&#25918;&#32622;&#65306;&#20855;&#26377;&#31574;&#30053;&#32467;&#26500;&#20808;&#39564;&#30340;&#39640;&#25928;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;
&lt;/p&gt;
&lt;p&gt;
Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy Structure Prior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15402
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31574;&#30053;&#32467;&#26500;&#20808;&#39564;&#30340;&#39640;&#25928;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#31995;&#32479;&#65292;&#36890;&#36807;&#20869;&#22806;&#29615;&#30340;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25235;&#21462;&#12289;&#35266;&#23519;&#21644;&#25918;&#32622;&#22312;&#24863;&#30693;&#22122;&#22768;&#20013;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#20219;&#21153;&#65292;&#21363;&#26426;&#22120;&#20154;&#24212;&#37325;&#26032;&#37197;&#32622;&#29289;&#20307;&#21040;&#30001;RGB-D&#22270;&#20687;&#25351;&#23450;&#30340;&#26399;&#26395;&#30446;&#26631;&#37197;&#32622;&#20013;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22359;&#26469;&#25506;&#32034;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#24863;&#30693;&#35823;&#24046;&#25935;&#24863;&#65292;&#24182;&#19988;&#36739;&#23569;&#20851;&#27880;&#20219;&#21153;&#32423;&#24615;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#26377;&#25928;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#24863;&#30693;&#22122;&#22768;&#20013;&#37325;&#26032;&#25490;&#21015;&#26410;&#30693;&#29289;&#20307;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#25581;&#31034;&#20102;&#22122;&#22768;&#24863;&#30693;&#22914;&#20309;&#20197;&#20998;&#31163;&#30340;&#26041;&#24335;&#24433;&#21709;&#25235;&#21462;&#21644;&#25918;&#32622;&#65292;&#24182;&#23637;&#31034;&#36825;&#26679;&#30340;&#20998;&#31163;&#32467;&#26500;&#19981;&#23481;&#26131;&#25913;&#21892;&#20219;&#21153;&#30340;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#20998;&#31163;&#32467;&#26500;&#20316;&#20026;&#20808;&#39564;&#30340;GSP&#65292;&#19968;&#20010;&#21452;&#29615;&#31995;&#32479;&#12290;&#23545;&#20110;&#20869;&#29615;&#65292;&#25105;&#20204;&#23398;&#20064;&#20027;&#21160;&#35266;&#23519;&#31574;&#30053;&#20197;&#25552;&#39640;&#25918;&#32622;&#30340;&#24863;&#30693;&#12290;&#23545;&#20110;&#22806;&#29615;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#25235;&#21462;&#31574;&#30053;&#65292;&#24847;&#35782;&#21040;&#29289;&#20307;&#21305;&#37197;&#21644;&#25235;&#21462;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15402v1 Announce Type: cross  Abstract: We focus on the task of unknown object rearrangement, where a robot is supposed to re-configure the objects into a desired goal configuration specified by an RGB-D image. Recent works explore unknown object rearrangement systems by incorporating learning-based perception modules. However, they are sensitive to perception error, and pay less attention to task-level performance. In this paper, we aim to develop an effective system for unknown object rearrangement amidst perception noise. We theoretically reveal the noisy perception impacts grasp and place in a decoupled way, and show such a decoupled structure is non-trivial to improve task optimality. We propose GSP, a dual-loop system with the decoupled structure as prior. For the inner loop, we learn an active seeing policy for self-confident object matching to improve the perception of place. For the outer loop, we learn a grasp policy aware of object matching and grasp capability gu
&lt;/p&gt;</description></item><item><title>ChunkAttention&#26159;&#19968;&#31181;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#24182;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#34892;&#26102;&#25913;&#21892;&#20869;&#23384;&#21033;&#29992;&#29575;&#30340;KV&#32531;&#23384;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#20197;&#25552;&#39640;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15220</link><description>&lt;p&gt;
ChunkAttention: &#20855;&#26377;&#21069;&#32512;&#24863;&#30693;KV&#32531;&#23384;&#21644;&#20004;&#38454;&#27573;&#20998;&#21306;&#30340;&#39640;&#25928;&#33258;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15220
&lt;/p&gt;
&lt;p&gt;
ChunkAttention&#26159;&#19968;&#31181;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#24182;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#34892;&#26102;&#25913;&#21892;&#20869;&#23384;&#21033;&#29992;&#29575;&#30340;KV&#32531;&#23384;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#20197;&#25552;&#39640;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23545;&#20110;&#38271;&#24207;&#21015;&#26469;&#35828;&#26159;&#25512;&#29702;&#24310;&#36831;&#30340;&#19968;&#20010;&#26174;&#33879;&#26469;&#28304;&#12290;&#22312;&#22810;&#31199;&#25143;LLMs&#26381;&#21153;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;LLM&#35831;&#27714;&#22312;&#21069;&#32512;&#20013;&#20849;&#20139;&#31995;&#32479;&#25552;&#31034;&#30340;&#27010;&#29575;&#65292;&#21487;&#20197;&#20248;&#21270;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25805;&#20316;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChunkAttention&#65292;&#19968;&#31181;&#20855;&#26377;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#26816;&#27979;&#22810;&#20010;&#35831;&#27714;&#20043;&#38388;&#21305;&#37197;&#30340;&#25552;&#31034;&#21069;&#32512;&#65292;&#24182;&#20849;&#20139;&#23427;&#20204;&#30340;&#38190;/&#20540;&#24352;&#37327;&#20197;&#25913;&#36827;KV&#32531;&#23384;&#30340;&#20869;&#23384;&#21033;&#29992;&#29575;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#25972;&#20307;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#65292;&#24182;&#23558;&#23427;&#20204;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#26469;&#23454;&#29616;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;KV&#32531;&#23384;&#20043;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#33258;&#27880;&#24847;&#21147;&#20869;&#26680;&#65292;&#20854;&#20013;&#23454;&#29616;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#65292;&#20197;&#25913;&#21892;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15220v1 Announce Type: cross  Abstract: Self-attention is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences. In multi-tenant LLMs serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the p
&lt;/p&gt;</description></item><item><title>&#20998;&#23618;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;HiSS&#65289;&#26159;&#19968;&#31181;&#38024;&#23545;&#36830;&#32493;&#24207;&#21015;&#21040;&#24207;&#21015;&#24314;&#27169;&#30340;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#22534;&#21472;&#30340;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.10211</link><description>&lt;p&gt;
&#38024;&#23545;&#36830;&#32493;&#24207;&#21015;&#21040;&#24207;&#21015;&#24314;&#27169;&#30340;&#20998;&#23618;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10211
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;HiSS&#65289;&#26159;&#19968;&#31181;&#38024;&#23545;&#36830;&#32493;&#24207;&#21015;&#21040;&#24207;&#21015;&#24314;&#27169;&#30340;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#22534;&#21472;&#30340;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10211v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#20174;&#21407;&#22987;&#24863;&#30693;&#25968;&#25454;&#30340;&#24207;&#21015;&#25512;&#29702;&#26159;&#20174;&#21307;&#30103;&#35774;&#22791;&#21040;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#24120;&#24120;&#28041;&#21450;&#20351;&#29992;&#38271;&#24207;&#21015;&#30340;&#21407;&#22987;&#20256;&#24863;&#22120;&#25968;&#25454;&#65288;&#20363;&#22914;&#30913;&#21147;&#35745;&#65292;&#21387;&#38459;&#22120;&#65289;&#26469;&#39044;&#27979;&#29702;&#24819;&#30340;&#29289;&#29702;&#37327;&#24207;&#21015;&#65288;&#20363;&#22914;&#21147;&#37327;&#65292;&#24815;&#24615;&#27979;&#37327;&#65289;&#12290;&#34429;&#28982;&#32463;&#20856;&#26041;&#27861;&#23545;&#20110;&#23616;&#37096;&#32447;&#24615;&#39044;&#27979;&#38382;&#39064;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#22312;&#20351;&#29992;&#23454;&#38469;&#20256;&#24863;&#22120;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20123;&#20256;&#24863;&#22120;&#36890;&#24120;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#21463;&#21040;&#22806;&#30028;&#21464;&#37327;&#65288;&#20363;&#22914;&#25391;&#21160;&#65289;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#34920;&#29616;&#20986;&#25968;&#25454;&#30456;&#20851;&#28418;&#31227;&#12290;&#23545;&#20110;&#35768;&#22810;&#38382;&#39064;&#26469;&#35828;&#65292;&#39044;&#27979;&#20219;&#21153;&#21463;&#21040;&#31232;&#32570;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#33719;&#21462;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#38656;&#35201;&#26114;&#36149;&#30340;&#35774;&#22791;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#23618;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;HiSS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27010;&#24565;&#19978;&#31616;&#21333;&#12289;&#20840;&#26032;&#30340;&#36830;&#32493;&#39034;&#24207;&#39044;&#27979;&#25216;&#26415;&#12290;HiSS&#23558;&#32467;&#26500;&#21270;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22534;&#21472;&#22312;&#19968;&#36215;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#26242;&#23450;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10211v1 Announce Type: new  Abstract: Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a tempor
&lt;/p&gt;</description></item><item><title>DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02563</link><description>&lt;p&gt;
DefInt&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#22788;&#29702;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02563
&lt;/p&gt;
&lt;p&gt;
DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#33021;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22914;&#36830;&#38145;&#25512;&#29702;&#65288;CoT&#65289;&#21644;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#20027;&#35201;&#20851;&#27880;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24573;&#35270;&#20102;&#19981;&#26029;&#22686;&#21152;&#30340;&#26631;&#35760;&#25104;&#26412;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#24040;&#22823;&#35299;&#31354;&#38388;&#30340;&#24320;&#25918;&#24615;&#23454;&#38469;&#20219;&#21153;&#26469;&#35828;&#21487;&#33021;&#29305;&#21035;&#38382;&#39064;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#30340;&#21452;&#36807;&#31243;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65288;DefInt&#65289;&#65292;&#20197;&#37322;&#25918;&#28151;&#21512;LLMs&#30340;&#21327;&#21516;&#28508;&#21147;&#12290;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;DefInt&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20302;&#25104;&#26412;&#30340;&#25512;&#29702;&#24605;&#36335;&#65292;&#31867;&#20284;&#20110;&#8220;&#31995;&#32479;1&#8221;&#20135;&#29983;&#30340;&#24555;&#36895;&#30452;&#35273;&#12290;&#22914;&#26524;&#36825;&#20123;&#30452;&#35273;&#34987;&#35748;&#20026;&#20302;&#32622;&#20449;&#24230;&#65292;&#21017;DefInt&#23558;&#35843;&#29992;&#25918;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#24605;&#25512;&#29702;&#20316;&#20026;&#8220;&#31995;&#32479;2&#8221;&#30340;&#24178;&#39044;&#65292;&#21487;&#20197;&#35206;&#30422;&#40664;&#35748;&#24605;&#32771;&#24182;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#23454;&#39564;&#22312;&#20116;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;DefInt&#35770;&#25991;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on fiv
&lt;/p&gt;</description></item><item><title>KGLens &#26159;&#19968;&#20010;&#26088;&#22312;&#34913;&#37327;&#30693;&#35782;&#22270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#23545;&#40784;&#31243;&#24230;&#30340;&#26694;&#26550;&#65292;&#24110;&#21161;&#25214;&#20986;LLMs&#30456;&#23545;&#20110;&#30693;&#35782;&#22270;&#30340;&#30693;&#35782;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>https://arxiv.org/abs/2312.11539</link><description>&lt;p&gt;
KGLens&#65306;&#19968;&#20010;&#21442;&#25968;&#21270;&#30693;&#35782;&#22270;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#30693;&#36947;&#21644;&#19981;&#30693;&#36947;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11539
&lt;/p&gt;
&lt;p&gt;
KGLens &#26159;&#19968;&#20010;&#26088;&#22312;&#34913;&#37327;&#30693;&#35782;&#22270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#23545;&#40784;&#31243;&#24230;&#30340;&#26694;&#26550;&#65292;&#24110;&#21161;&#25214;&#20986;LLMs&#30456;&#23545;&#20110;&#30693;&#35782;&#22270;&#30340;&#30693;&#35782;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34913;&#37327;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#26159;&#35780;&#20272;&#20107;&#23454;&#24615;&#24182;&#35782;&#21035;LLMs&#30340;&#30693;&#35782;&#30450;&#28857;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#23558;KGs&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#21644;&#39640;&#25928;&#35780;&#20272;&#36825;&#20123;&#24191;&#27867;&#19988;&#22797;&#26434;&#30340;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KGLens--&#19968;&#20010;&#26088;&#22312;&#34913;&#37327;KGs&#21644;LLMs&#20043;&#38388;&#23545;&#40784;&#31243;&#24230;&#65292;&#24182;&#25214;&#20986;LLMs&#30456;&#23545;&#20110;KGs&#30340;&#30693;&#35782;&#32570;&#38519;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;KGLens&#20855;&#26377;&#19968;&#20010;&#22270;&#24341;&#23548;&#30340;&#38382;&#39064;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#23558;KGs&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#21442;&#25968;&#21270;KG&#32467;&#26500;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#21152;&#24555;KG&#30340;&#36941;&#21382;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Wikidata&#30340;&#19977;&#20010;&#39046;&#22495;&#29305;&#23450;KG&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#20123;KG&#21253;&#25324;&#36229;&#36807;19,000&#26465;&#36793;&#65292;700&#20010;&#20851;&#31995;&#21644;21,000&#20010;&#23454;&#20307;&#12290;&#25105;&#20204;&#36328;&#36234;8&#20010;LLMs&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;KGLens&#19981;&#20165;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11539v2 Announce Type: replace  Abstract: Measuring the alignment between a Knowledge Graph (KG) and Large Language Models (LLMs) is an effective method to assess the factualness and identify the knowledge blind spots of LLMs. However, this approach encounters two primary challenges including the translation of KGs into natural language and the efficient evaluation of these extensive and complex structures. In this paper, we present KGLens--a novel framework aimed at measuring the alignment between KGs and LLMs, and pinpointing the LLMs' knowledge deficiencies relative to KGs. KGLens features a graph-guided question generator for converting KGs into natural language, along with a carefully designed sampling strategy based on parameterized KG structure to expedite KG traversal. We conducted experiments using three domain-specific KGs from Wikidata, which comprise over 19,000 edges, 700 relations, and 21,000 entities. Our analysis across eight LLMs reveals that KGLens not only
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#26694;&#26550;&#65292;&#35780;&#20272;&#31227;&#21160;&#22240;&#32032;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#65292;&#20135;&#29983;&#20855;&#26377;&#19981;&#21516;&#31227;&#21160;&#34892;&#20026;&#30340;&#20301;&#32622;&#24207;&#21015;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#35299;&#37322;&#39044;&#27979;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2311.11749</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#25581;&#31034;&#34892;&#20026;&#23545;&#31227;&#21160;&#39044;&#27979;&#32593;&#32476;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Revealing behavioral impact on mobility prediction networks through causal interventions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11749
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#24178;&#39044;&#26694;&#26550;&#65292;&#35780;&#20272;&#31227;&#21160;&#22240;&#32032;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#65292;&#20135;&#29983;&#20855;&#26377;&#19981;&#21516;&#31227;&#21160;&#34892;&#20026;&#30340;&#20301;&#32622;&#24207;&#21015;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#35299;&#37322;&#39044;&#27979;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#31227;&#21160;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#34987;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#20351;&#29992;&#65292;&#28982;&#32780;&#23427;&#20204;&#22797;&#26434;&#30340;&#20869;&#37096;&#36816;&#20316;&#26041;&#24335;&#32473;&#21487;&#35299;&#37322;&#24615;&#24102;&#26469;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#29702;&#35299;&#21508;&#31181;&#31227;&#21160;&#34892;&#20026;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#26041;&#38754;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#22240;&#26524;&#24178;&#39044;&#26694;&#26550;&#65292;&#35780;&#20272;&#19982;&#19979;&#19968;&#20010;&#20301;&#32622;&#39044;&#27979;&#30456;&#20851;&#30340;&#31227;&#21160;&#22240;&#32032;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709; -- &#36825;&#26159;&#19968;&#39033;&#19987;&#27880;&#20110;&#39044;&#27979;&#20010;&#20307;&#21363;&#23558;&#21040;&#36798;&#20301;&#32622;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20351;&#29992;&#20010;&#20307;&#31227;&#21160;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#20301;&#32622;&#35775;&#38382;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#24178;&#39044;&#23427;&#20204;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#26469;&#25511;&#21046;&#34892;&#20026;&#21160;&#24577;&#12290;&#25105;&#20204;&#20351;&#29992;&#31227;&#21160;&#25351;&#26631;&#35780;&#20272;&#24178;&#39044;&#20301;&#32622;&#24207;&#21015;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#35757;&#32451;&#33391;&#22909;&#30340;&#32593;&#32476;&#20013;&#20197;&#20998;&#26512;&#24615;&#33021;&#21464;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20135;&#29983;&#20855;&#26377;&#19981;&#21516;&#31227;&#21160;&#34892;&#20026;&#30340;&#20301;&#32622;&#24207;&#21015;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#39044;&#27979;&#32593;&#32476;&#30340;&#29702;&#35299;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11749v2 Announce Type: replace-cross  Abstract: Deep neural networks are increasingly utilized in mobility prediction tasks, yet their intricate internal workings pose challenges for interpretability, especially in comprehending how various aspects of mobility behavior affect predictions. This study introduces a causal intervention framework to assess the impact of mobility-related factors on neural networks designed for next location prediction -- a task focusing on predicting the immediate next location of an individual. To achieve this, we employ individual mobility models to generate synthetic location visit sequences and control behavior dynamics by intervening in their data generation process. We evaluate the interventional location sequences using mobility metrics and input them into well-trained networks to analyze performance variations. The results demonstrate the effectiveness in producing location sequences with distinct mobility behaviors, thereby facilitating t
&lt;/p&gt;</description></item><item><title>&#23618;&#32423;&#22870;&#21169;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22870;&#21169;&#35774;&#35745;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20445;&#35777;&#35825;&#23548;&#20986;&#26681;&#25454;&#20559;&#22909;&#20851;&#31995;&#26159;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#23637;&#31034;&#20854;&#24555;&#36895;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2212.03733</link><description>&lt;p&gt;
&#23618;&#32423;&#22870;&#21169;&#20989;&#25968;&#65306;&#35268;&#23450;&#21644;&#24555;&#36895;&#23398;&#20064;&#25152;&#38656;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Tiered Reward Functions: Specifying and Fast Learning of Desired Behavior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.03733
&lt;/p&gt;
&lt;p&gt;
&#23618;&#32423;&#22870;&#21169;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22870;&#21169;&#35774;&#35745;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20445;&#35777;&#35825;&#23548;&#20986;&#26681;&#25454;&#20559;&#22909;&#20851;&#31995;&#26159;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#23637;&#31034;&#20854;&#24555;&#36895;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#26368;&#22823;&#21270;&#22870;&#21169;&#20449;&#21495;&#12290;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20316;&#20026;&#20154;&#31867;&#30340;&#20219;&#21153;&#26159;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#34920;&#36798;&#25152;&#26399;&#26395;&#30340;&#34892;&#20026;&#65292;&#24182;&#20351;&#20195;&#29702;&#33021;&#22815;&#36805;&#36895;&#23398;&#20064;&#36825;&#31181;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20219;&#21153;&#20013;&#36798;&#21040;&#33391;&#22909;&#29366;&#24577;&#21644;&#36991;&#20813;&#19981;&#33391;&#29366;&#24577;&#30340;&#22870;&#21169;&#35774;&#35745;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#31574;&#30053;&#31354;&#38388;&#30340;&#37096;&#20998;&#25490;&#24207;&#65292;&#20197;&#35299;&#20915;&#34892;&#20026;&#20559;&#22909;&#20013;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#26356;&#20542;&#21521;&#20110;&#33021;&#26356;&#24555;&#36895;&#22320;&#21040;&#36798;&#33391;&#22909;&#29366;&#24577;&#24182;&#20197;&#26356;&#39640;&#30340;&#27010;&#29575;&#21040;&#36798;&#65292;&#21516;&#26102;&#33021;&#26356;&#38271;&#26102;&#38388;&#22320;&#36991;&#20813;&#19981;&#33391;&#29366;&#24577;&#30340;&#31574;&#30053;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23618;&#32423;&#22870;&#21169;&#65292;&#19968;&#31867;&#19982;&#29615;&#22659;&#26080;&#20851;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#34920;&#26126;&#23427;&#20445;&#35777;&#35825;&#23548;&#20986;&#26681;&#25454;&#25105;&#20204;&#30340;&#20559;&#22909;&#20851;&#31995;&#26159;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23618;&#32423;&#22870;&#21169;&#21487;&#20197;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#20351;&#29992;&#22810;&#20010;&#34920;&#26684;&#36827;&#34892;&#35780;&#20272;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.03733v2 Announce Type: replace-cross  Abstract: Reinforcement-learning agents seek to maximize a reward signal through environmental interactions. As humans, our job in the learning process is to design reward functions to express desired behavior and enable the agent to learn such behavior swiftly. In this work, we consider the reward-design problem in tasks formulated as reaching desirable states and avoiding undesirable states. To start, we propose a strict partial ordering of the policy space to resolve trade-offs in behavior preference. We prefer policies that reach the good states faster and with higher probability while avoiding the bad states longer. Next, we introduce Tiered Reward, a class of environment-independent reward functions and show it is guaranteed to induce policies that are Pareto-optimal according to our preference relation. Finally, we demonstrate that Tiered Reward can lead to fast learning by evaluating on several environments using multiple tabular
&lt;/p&gt;</description></item><item><title>MoE-Infinity&#26159;&#19968;&#31181;&#25104;&#26412;&#39640;&#25928;&#30340;MoE&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#28608;&#27963;&#24863;&#30693;&#30340;&#19987;&#23478;&#21368;&#36733;&#21644;&#32531;&#23384;&#25216;&#26415;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#24310;&#36831;&#65292;&#24182;&#25552;&#39640;&#20102;&#25104;&#26412;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14361</link><description>&lt;p&gt;
MoE-Infinity&#65306;&#29992;&#20110;&#39640;&#25928;MoE&#26381;&#21153;&#30340;&#28608;&#27963;&#24863;&#30693;&#19987;&#23478;&#21368;&#36733;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE Serving. (arXiv:2401.14361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14361
&lt;/p&gt;
&lt;p&gt;
MoE-Infinity&#26159;&#19968;&#31181;&#25104;&#26412;&#39640;&#25928;&#30340;MoE&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#28608;&#27963;&#24863;&#30693;&#30340;&#19987;&#23478;&#21368;&#36733;&#21644;&#32531;&#23384;&#25216;&#26415;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#24310;&#36831;&#65292;&#24182;&#25552;&#39640;&#20102;&#25104;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MoE-Infinity&#65292;&#19968;&#31181;&#25104;&#26412;&#39640;&#25928;&#30340;&#19987;&#23478;&#28151;&#21512;(MoE)&#26381;&#21153;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#28608;&#27963;&#24863;&#30693;&#30340;&#19987;&#23478;&#21368;&#36733;&#12290;MoE-Infinity&#20855;&#26377;&#24207;&#21015;&#32423;&#19987;&#23478;&#28608;&#27963;&#36861;&#36394;&#30340;&#29305;&#28857;&#65292;&#36825;&#26159;&#19968;&#31181;&#25797;&#38271;&#35782;&#21035;&#31232;&#30095;&#28608;&#27963;&#24182;&#25429;&#25417;MoE&#25512;&#29702;&#30340;&#26102;&#38388;&#23616;&#37096;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#36825;&#20123;&#36861;&#36394;&#65292;MoE-Infinity&#25191;&#34892;&#20102;&#26032;&#39062;&#30340;&#28608;&#27963;&#24863;&#30693;&#19987;&#23478;&#39044;&#21462;&#21644;&#32531;&#23384;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#36890;&#24120;&#19982;&#21368;&#36733;&#19987;&#23478;&#30456;&#20851;&#30340;&#24310;&#36831;&#24320;&#38144;&#65292;&#25552;&#39640;&#20102;&#25104;&#26412;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#38598;&#32676;&#20013;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;MoE-Infinity&#20248;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#31995;&#32479;&#21644;&#26041;&#27861;&#65292;&#23545;&#20110;&#21508;&#31181;MoEs&#65292;&#23558;&#24310;&#36831;&#38477;&#20302;&#20102;420&#20493;&#65292;&#23558;&#37096;&#32626;&#25104;&#26412;&#38477;&#20302;&#20102;8&#20493;&#20197;&#19978;&#12290;MoE-Infinity&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/TorchMoE/MoE-Infinity&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents MoE-Infinity, a cost-efficient mixture-of-expert (MoE) serving system that realizes activation-aware expert offloading. MoE-Infinity features sequence-level expert activation tracing, a new approach adept at identifying sparse activations and capturing the temporal locality of MoE inference. By analyzing these traces, MoE-Infinity performs novel activation-aware expert prefetching and caching, substantially reducing the latency overheads usually associated with offloading experts for improved cost performance. Extensive experiments in a cluster show that MoE-Infinity outperforms numerous existing systems and approaches, reducing latency by 4 20X and decreasing deployment costs by over 8X for various MoEs. MoE-Infinity's source code is publicly available at https://github.com/TorchMoE/MoE-Infinity
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#21518;&#38376;&#65292;&#36890;&#36807;&#25910;&#38598;&#29992;&#20110;&#35757;&#32451;&#30340;&#21518;&#38376;&#24182;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#26469;&#39044;&#27979;&#21518;&#38376;&#65292;&#21462;&#24471;&#20102;&#27604;Gurobi&#21644;&#20808;&#21069;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.10467</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Learning Backdoors for Mixed Integer Programs with Contrastive Learning. (arXiv:2401.10467v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#21518;&#38376;&#65292;&#36890;&#36807;&#25910;&#38598;&#29992;&#20110;&#35757;&#32451;&#30340;&#21518;&#38376;&#24182;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#26469;&#39044;&#27979;&#21518;&#38376;&#65292;&#21462;&#24471;&#20102;&#27604;Gurobi&#21644;&#20808;&#21069;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#20026;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#24182;&#20351;&#29992;&#20998;&#25903;&#23450;&#30028;&#26041;&#27861;&#36827;&#34892;&#27714;&#35299;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#23384;&#22312;MIP&#21518;&#38376;&#65292;&#21363;&#19968;&#23567;&#32452;&#21464;&#37327;&#65292;&#22914;&#26524;&#20248;&#20808;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#22312;&#23427;&#20204;&#19978;&#36827;&#34892;&#20998;&#25903;&#65292;&#21017;&#21487;&#20197;&#21152;&#24555;&#36816;&#34892;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;&#33021;&#25552;&#39640;&#36816;&#34892;&#26102;&#38388;&#30340;&#39640;&#36136;&#37327;&#21518;&#38376;&#20173;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#25490;&#21517;&#23398;&#20064;&#20272;&#35745;&#38543;&#26426;&#37319;&#26679;&#30340;&#21518;&#38376;&#30456;&#23545;&#27714;&#35299;&#22120;&#36895;&#24230;&#65292;&#28982;&#21518;&#20915;&#23450;&#26159;&#21542;&#20351;&#29992;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26041;&#27861;&#25910;&#38598;&#29992;&#20110;&#35757;&#32451;&#30340;&#21518;&#38376;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#38543;&#26426;&#37319;&#26679;&#65292;&#24182;&#19988;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#26469;&#39044;&#27979;&#21518;&#38376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#24120;&#35265;&#30340;MIP&#38382;&#39064;&#39046;&#22495;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#34920;&#29616;&#20986;&#23545;&#27604;Gurobi&#21644;&#20808;&#21069;&#27169;&#22411;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world problems can be efficiently modeled as Mixed Integer Programs (MIPs) and solved with the Branch-and-Bound method. Prior work has shown the existence of MIP backdoors, small sets of variables such that prioritizing branching on them when possible leads to faster running times. However, finding high-quality backdoors that improve running times remains an open question. Previous work learns to estimate the relative solver speed of randomly sampled backdoors through ranking and then decide whether to use it. In this paper, we utilize the Monte-Carlo tree search method to collect backdoors for training, rather than relying on random sampling, and adapt a contrastive learning framework to train a Graph Attention Network model to predict backdoors. Our method, evaluated on four common MIP problem domains, demonstrates performance improvements over both Gurobi and previous models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#24110;&#21161;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;&#22788;&#29702;&#36873;&#25321;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.06925</link><description>&lt;p&gt;
&#29992;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Latent Selection with Structural Causal Models. (arXiv:2401.06925v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#24110;&#21161;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;&#22788;&#29702;&#36873;&#25321;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#20559;&#20506;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#22914;&#26524;&#19981;&#27491;&#30830;&#22788;&#29702;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCMs&#65289;&#36827;&#34892;&#26465;&#20214;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#20197;&#20174;&#22240;&#26524;&#30340;&#35282;&#24230;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26465;&#20214;&#25805;&#20316;&#23558;&#20855;&#26377;&#26126;&#30830;&#28508;&#22312;&#36873;&#25321;&#26426;&#21046;&#30340;SCM&#36716;&#25442;&#20026;&#27809;&#26377;&#27492;&#31867;&#36873;&#25321;&#26426;&#21046;&#30340;SCM&#65292;&#36825;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32534;&#30721;&#20102;&#26681;&#25454;&#21407;&#22987;SCM&#36873;&#25321;&#30340;&#20122;&#24635;&#20307;&#30340;&#22240;&#26524;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#26465;&#20214;&#25805;&#20316;&#20445;&#25345;SCMs&#30340;&#31616;&#27905;&#24615;&#65292;&#26080;&#29615;&#24615;&#21644;&#32447;&#24615;&#24615;&#65292;&#24182;&#19982;&#36793;&#38469;&#21270;&#25805;&#20316;&#30456;&#31526;&#21512;&#12290;&#30001;&#20110;&#36825;&#20123;&#29305;&#24615;&#19982;&#36793;&#38469;&#21270;&#21644;&#24178;&#39044;&#32467;&#21512;&#36215;&#26469;&#65292;&#26465;&#20214;&#25805;&#20316;&#20026;&#22312;&#28508;&#22312;&#32454;&#33410;&#24050;&#32463;&#21435;&#38500;&#30340;&#22240;&#26524;&#27169;&#22411;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#36890;&#36807;&#20363;&#23376;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#22240;&#26524;&#25512;&#26029;&#30340;&#32463;&#20856;&#32467;&#26524;&#25512;&#24191;&#20197;&#21253;&#25324;&#36873;&#25321;&#20559;&#20506;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selection bias is ubiquitous in real-world data, and can lead to misleading results if not dealt with properly. We introduce a conditioning operation on Structural Causal Models (SCMs) to model latent selection from a causal perspective. We show that the conditioning operation transforms an SCM with the presence of an explicit latent selection mechanism into an SCM without such selection mechanism, which partially encodes the causal semantics of the selected subpopulation according to the original SCM. Furthermore, we show that this conditioning operation preserves the simplicity, acyclicity, and linearity of SCMs, and commutes with marginalization. Thanks to these properties, combined with marginalization and intervention, the conditioning operation offers a valuable tool for conducting causal reasoning tasks within causal models where latent details have been abstracted away. We demonstrate by example how classical results of causal inference can be generalized to include selection b
&lt;/p&gt;</description></item><item><title>HiMTM&#26159;&#19968;&#31181;&#38754;&#21521;&#38271;&#26399;&#39044;&#27979;&#30340;&#20998;&#23618;&#22810;&#23610;&#24230;&#23631;&#34109;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#23618;&#22810;&#23610;&#24230;&#21464;&#21387;&#22120;&#65292;&#35299;&#32806;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65292;&#22810;&#23610;&#24230;&#23631;&#34109;&#37325;&#26500;&#21644;&#36328;&#23610;&#24230;&#27880;&#24847;&#24494;&#35843;&#31561;&#32452;&#25104;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2401.05012</link><description>&lt;p&gt;
HiMTM: &#38754;&#21521;&#38271;&#26399;&#39044;&#27979;&#30340;&#20998;&#23618;&#22810;&#23610;&#24230;&#23631;&#34109;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
HiMTM: Hierarchical Multi-Scale Masked Time Series Modeling for Long-Term Forecasting. (arXiv:2401.05012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05012
&lt;/p&gt;
&lt;p&gt;
HiMTM&#26159;&#19968;&#31181;&#38754;&#21521;&#38271;&#26399;&#39044;&#27979;&#30340;&#20998;&#23618;&#22810;&#23610;&#24230;&#23631;&#34109;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#23618;&#22810;&#23610;&#24230;&#21464;&#21387;&#22120;&#65292;&#35299;&#32806;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65292;&#22810;&#23610;&#24230;&#23631;&#34109;&#37325;&#26500;&#21644;&#36328;&#23610;&#24230;&#27880;&#24847;&#24494;&#35843;&#31561;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36817;&#26399;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36259;&#28608;&#22686;&#65292;&#36825;&#20123;&#27169;&#22411;&#36866;&#24212;&#21508;&#31181;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#36825;&#26159;&#20540;&#24471;&#27880;&#24847;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#23610;&#24230;&#24615;&#36136;&#65292;&#36825;&#26159;&#36827;&#34892;&#31934;&#30830;&#39044;&#27979;&#25152;&#24517;&#38656;&#30340;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiMTM&#65292;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#38271;&#26399;&#39044;&#27979;&#35774;&#35745;&#30340;&#20998;&#23618;&#22810;&#23610;&#24230;&#23631;&#34109;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#12290;&#20855;&#20307;&#21253;&#25324;&#22235;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#20998;&#23618;&#22810;&#23610;&#24230;&#21464;&#21387;&#22120;&#65288;HMT&#65289;&#25429;&#25417;&#19981;&#21516;&#23610;&#24230;&#30340;&#26102;&#38388;&#20449;&#24687;&#65307;&#65288;2&#65289;&#35299;&#32806;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;DED&#65289;&#35753;&#32534;&#30721;&#22120;&#19987;&#27880;&#20110;&#29305;&#24449;&#25552;&#21462;&#65292;&#32780;&#35299;&#30721;&#22120;&#19987;&#27880;&#20110;&#20551;&#35774;&#20219;&#21153;&#65307;&#65288;3&#65289;&#22810;&#23610;&#24230;&#23631;&#34109;&#37325;&#26500;&#65288;MMR&#65289;&#20026;&#39044;&#35757;&#32451;&#25552;&#20379;&#22810;&#38454;&#27573;&#30340;&#30417;&#30563;&#20449;&#21495;&#65307;&#65288;4&#65289;&#36328;&#23610;&#24230;&#27880;&#24847;&#24494;&#35843;&#65288;CSA-FT&#65289;&#20197;&#25429;&#25417;&#19981;&#21516;&#23610;&#24230;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#36827;&#34892;&#39044;&#27979;&#12290;&#32508;&#19978;&#25152;&#36848;&#65292;&#36825;&#20123;&#37096;&#20214;&#20849;&#21516;&#26500;&#25104;&#20102;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting is crucial and challenging in the real world. The recent surge in interest regarding time series foundation models, which cater to a diverse array of downstream tasks, is noteworthy. However, existing methods often overlook the multi-scale nature of time series, an aspect crucial for precise forecasting. To bridge this gap, we propose HiMTM, a hierarchical multi-scale masked time series modeling method designed for long-term forecasting. Specifically, it comprises four integral components: (1) hierarchical multi-scale transformer (HMT) to capture temporal information at different scales; (2) decoupled encoder-decoder (DED) forces the encoder to focus on feature extraction, while the decoder to focus on pretext tasks; (3) multi-scale masked reconstruction (MMR) provides multi-stage supervision signals for pre-training; (4) cross-scale attention fine-tuning (CSA-FT) to capture dependencies between different scales for forecasting. Collectively, these components en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SC-MIL&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#26469;&#21516;&#26102;&#25913;&#36827;&#29305;&#24449;&#23884;&#20837;&#21644;&#23454;&#20363;&#30456;&#20851;&#24615;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00048</link><description>&lt;p&gt;
SC-MIL: &#29992;&#20110;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#31232;&#30095;&#32534;&#30721;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2311.00048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SC-MIL&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#26469;&#21516;&#26102;&#25913;&#36827;&#29305;&#24449;&#23884;&#20837;&#21644;&#23454;&#20363;&#30456;&#20851;&#24615;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#22312;&#24369;&#30417;&#30563;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSI&#65289;&#20998;&#31867;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#20856;&#22411;&#30340;MIL&#26041;&#27861;&#21253;&#25324;&#29305;&#24449;&#23884;&#20837;&#37096;&#20998;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#23558;&#23454;&#20363;&#23884;&#20837;&#21040;&#29305;&#24449;&#20013;&#65292;&#20197;&#21450;MIL&#32858;&#21512;&#22120;&#65292;&#23558;&#23454;&#20363;&#23884;&#20837;&#32452;&#21512;&#25104;&#39044;&#27979;&#32467;&#26524;&#12290;&#30446;&#21069;&#30340;&#37325;&#28857;&#26159;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;&#36825;&#20123;&#37096;&#20998;&#65292;&#24182;&#21333;&#29420;&#24314;&#27169;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#32534;&#30721;&#30340;MIL&#65288;SC-MIL&#65289;&#65292;&#21516;&#26102;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#26041;&#38754;&#12290;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#36890;&#36807;&#23558;&#23454;&#20363;&#34920;&#31034;&#20026;&#36807;&#23436;&#22791;&#23383;&#20856;&#20013;&#21407;&#23376;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#26469;&#25429;&#25417;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#31232;&#30095;&#24615;&#21487;&#20197;&#36890;&#36807;&#25233;&#21046;&#19981;&#30456;&#20851;&#30340;&#23454;&#20363;&#32780;&#20445;&#30041;&#26368;&#30456;&#20851;&#30340;&#23454;&#20363;&#65292;&#20174;&#32780;&#22686;&#24378;&#23454;&#20363;&#30340;&#29305;&#24449;&#23884;&#20837;&#12290;&#20026;&#20102;&#25913;&#21892;&#20256;&#32479;&#30340;&#29305;&#24449;&#23884;&#20837;&#21644;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;we proposed a sparsely coded MIL.
&lt;/p&gt;
&lt;p&gt;
Multiple Instance Learning (MIL) has been widely used in weakly supervised whole slide image (WSI) classification. Typical MIL methods include a feature embedding part that embeds the instances into features via a pre-trained feature extractor and the MIL aggregator that combines instance embeddings into predictions. The current focus has been directed toward improving these parts by refining the feature embeddings through self-supervised pre-training and modeling the correlations between instances separately. In this paper, we proposed a sparsely coded MIL (SC-MIL) that addresses those two aspects at the same time by leveraging sparse dictionary learning. The sparse dictionary learning captures the similarities of instances by expressing them as a sparse linear combination of atoms in an over-complete dictionary. In addition, imposing sparsity help enhance the instance feature embeddings by suppressing irrelevant instances while retaining the most relevant ones. To make the convention
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#38750;&#24179;&#31283;&#24773;&#20917;&#19979;&#65292;&#25506;&#32034;&#20102;&#26102;&#38388;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36741;&#21161;&#21464;&#37327;&#35266;&#27979;&#30340;&#26041;&#27861;&#26469;&#24674;&#22797;&#29420;&#31435;&#30340;&#28508;&#22312;&#20998;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.18615</link><description>&lt;p&gt;
&#26410;&#30693;&#38750;&#24179;&#31283;&#24773;&#20917;&#19979;&#30340;&#26102;&#38388;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Temporally Disentangled Representation Learning under Unknown Nonstationarity. (arXiv:2310.18615v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#38750;&#24179;&#31283;&#24773;&#20917;&#19979;&#65292;&#25506;&#32034;&#20102;&#26102;&#38388;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36741;&#21161;&#21464;&#37327;&#35266;&#27979;&#30340;&#26041;&#27861;&#26469;&#24674;&#22797;&#29420;&#31435;&#30340;&#28508;&#22312;&#20998;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#26102;&#24310;&#28508;&#22312;&#22240;&#26524;&#24433;&#21709;&#30340;&#26102;&#24207;&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#32467;&#26500;&#22312;&#31283;&#24577;&#24773;&#20917;&#19979;&#24050;&#32463;&#24314;&#31435;&#20102;&#26377;&#20851;&#22240;&#26524;&#30456;&#20851;&#28508;&#22312;&#21464;&#37327;&#35299;&#32544;&#30340;&#24378;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#24179;&#31283;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30740;&#31350;&#21482;&#37096;&#20998;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35201;&#20040;&#21033;&#29992;&#35266;&#27979;&#21040;&#30340;&#36741;&#21161;&#21464;&#37327;&#65288;&#22914;&#31867;&#21035;&#26631;&#31614;&#21644;/&#25110;&#22495;&#32034;&#24341;&#65289;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#65292;&#35201;&#20040;&#20551;&#35774;&#31616;&#21270;&#30340;&#28508;&#22312;&#22240;&#26524;&#21160;&#21147;&#23398;&#12290;&#36825;&#20004;&#32773;&#38480;&#21046;&#20102;&#26041;&#27861;&#30340;&#36866;&#29992;&#33539;&#22260;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#26102;&#38388;&#24310;&#36831;&#30456;&#20851;&#36807;&#31243;&#30340;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#22312;&#19981;&#35266;&#23519;&#36741;&#21161;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#20174;&#38750;&#32447;&#24615;&#28151;&#21512;&#20013;&#24674;&#22797;&#29420;&#31435;&#30340;&#28508;&#22312;&#20998;&#37327;&#65292;&#20294;&#21487;&#33021;&#23384;&#22312;&#25490;&#21015;&#21644;&#20998;&#37327;&#32423;&#36716;&#25442;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#20272;&#35745;&#26694;&#26550;NCTRL&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In unsupervised causal representation learning for sequential data with time-delayed latent causal influences, strong identifiability results for the disentanglement of causally-related latent variables have been established in stationary settings by leveraging temporal structure. However, in nonstationary setting, existing work only partially addressed the problem by either utilizing observed auxiliary variables (e.g., class labels and/or domain indexes) as side information or assuming simplified latent causal dynamics. Both constrain the method to a limited range of scenarios. In this study, we further explored the Markov Assumption under time-delayed causally related process in nonstationary setting and showed that under mild conditions, the independent latent components can be recovered from their nonlinear mixture up to a permutation and a component-wise transformation, without the observation of auxiliary variables. We then introduce NCTRL, a principled estimation framework, to r
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#37051;&#36817;&#24615;&#26469;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#36741;&#30456;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12428</link><description>&lt;p&gt;
&#23454;&#29616;&#38543;&#26426;&#26862;&#26519;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#22686;&#24378;&#65306;&#22522;&#20110;&#37051;&#36817;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Enhanced Local Explainability of Random Forests: a Proximity-Based Approach. (arXiv:2310.12428v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12428
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#37051;&#36817;&#24615;&#26469;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#36741;&#30456;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#27169;&#22411;&#30340;&#26679;&#26412;&#22806;&#24615;&#33021;&#65292;&#21033;&#29992;&#20102;&#20219;&#20309;RF&#37117;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#33258;&#36866;&#24212;&#21152;&#26435;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#27169;&#22411;&#30340;&#20107;&#23454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;RF&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23398;&#21040;&#30340;&#28857;&#20043;&#38388;&#30340;&#37051;&#36817;&#24615;&#65292;&#23558;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#37325;&#20889;&#20026;&#35757;&#32451;&#25968;&#25454;&#28857;&#30446;&#26631;&#26631;&#31614;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#12290;&#36825;&#31181;&#32447;&#24615;&#24615;&#36136;&#26377;&#21161;&#20110;&#22312;&#35757;&#32451;&#38598;&#35266;&#27979;&#20013;&#20026;&#20219;&#20309;&#27169;&#22411;&#39044;&#27979;&#29983;&#25104;&#23646;&#24615;&#65292;&#20174;&#32780;&#20026;RF&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#34917;&#20805;&#20102;SHAP&#31561;&#24050;&#26377;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21017;&#20026;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#19978;&#30340;&#27169;&#22411;&#39044;&#27979;&#29983;&#25104;&#23646;&#24615;&#12290;&#25105;&#20204;&#22312;&#35757;&#32451;&#20110;&#32654;&#22269;&#20844;&#21496;&#20538;&#21048;&#20132;&#26131;&#25968;&#25454;&#30340;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#21508;&#31181;&#29616;&#26377;&#30340;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We initiate a novel approach to explain the out of sample performance of random forest (RF) models by exploiting the fact that any RF can be formulated as an adaptive weighted K nearest-neighbors model. Specifically, we use the proximity between points in the feature space learned by the RF to re-write random forest predictions exactly as a weighted average of the target labels of training data points. This linearity facilitates a local notion of explainability of RF predictions that generates attributions for any model prediction across observations in the training set, and thereby complements established methods like SHAP, which instead generates attributions for a model prediction across dimensions of the feature space. We demonstrate this approach in the context of a bond pricing model trained on US corporate bond trades, and compare our approach to various existing approaches to model explainability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;V-nets&#65289;&#30340;&#24191;&#27867;&#26694;&#26550;&#65292;&#24182;&#35299;&#37322;&#20102;&#23427;&#20204;&#19982;&#36229;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.07716</link><description>&lt;p&gt;
&#29702;&#35299;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#19982;&#23454;&#25968;&#21644;&#36229;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Understanding Vector-Valued Neural Networks and Their Relationship with Real and Hypercomplex-Valued Neural Networks. (arXiv:2309.07716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;V-nets&#65289;&#30340;&#24191;&#27867;&#26694;&#26550;&#65292;&#24182;&#35299;&#37322;&#20102;&#23427;&#20204;&#19982;&#36229;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#32500;&#20449;&#21495;&#21644;&#22270;&#20687;&#22788;&#29702;&#26041;&#38754;&#26377;&#35768;&#22810;&#25104;&#21151;&#30340;&#24212;&#29992;&#65292;&#20294;&#22823;&#22810;&#25968;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#30001;&#65288;&#22810;&#32500;&#65289;&#23454;&#25968;&#25968;&#32452;&#34920;&#31034;&#30340;&#25968;&#25454;&#12290;&#29305;&#24449;&#36890;&#36947;&#20043;&#38388;&#30340;&#20114;&#30456;&#20851;&#36890;&#24120;&#34987;&#26399;&#26395;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#21644;&#20180;&#32454;&#30340;&#35757;&#32451;&#12290;&#30456;&#21453;&#65292;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#34987;&#35774;&#35745;&#25104;&#22788;&#29702;&#21521;&#37327;&#25968;&#32452;&#65292;&#24182;&#33258;&#28982;&#22320;&#32771;&#34385;&#29305;&#24449;&#36890;&#36947;&#20043;&#38388;&#30340;&#20114;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#65292;&#36890;&#24120;&#27604;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#24378;&#30340;&#35757;&#32451;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#20010;&#24191;&#27867;&#30340;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#31216;&#20026;V-nets&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#36229;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#34987;&#35270;&#20026;&#20855;&#26377;&#39069;&#22806;&#20195;&#25968;&#23646;&#24615;&#30340;&#21521;&#37327;&#20540;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#35299;&#37322;&#20102;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#19982;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrelation between feature channels. Consequently, they usually have fewer parameters and often undergo more robust training than traditional neural networks. This paper aims to present a broad framework for vector-valued neural networks, referred to as V-nets. In this context, hypercomplex-valued neural networks are regarded as vector-valued models with additional algebraic properties. Furthermore, this paper explains the relationship between vector-valued and traditional neural networks. Precisely, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32463;&#27982;&#38750;&#32447;&#24615;MPC&#30340;Koopman&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#25511;&#21046;&#24615;&#33021;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.01674</link><description>&lt;p&gt;
&#32463;&#27982;&#38750;&#32447;&#24615;MPC&#30340;Koopman&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear MPC. (arXiv:2308.01674v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32463;&#27982;&#38750;&#32447;&#24615;MPC&#30340;Koopman&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#25511;&#21046;&#24615;&#33021;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#65288;&#32463;&#27982;&#65289;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;&#65288;e&#65289;NMPC&#65289;&#38656;&#35201;&#22312;&#25152;&#26377;&#30456;&#20851;&#29366;&#24577;&#31354;&#38388;&#21306;&#22495;&#37117;&#20855;&#26377;&#36275;&#22815;&#20934;&#30830;&#24615;&#30340;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#36824;&#24517;&#39035;&#35745;&#31639;&#25104;&#26412;&#36275;&#22815;&#20302;&#20197;&#30830;&#20445;&#23454;&#26102;&#21487;&#34892;&#24615;&#12290;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26367;&#20195;&#26426;&#21046;&#27169;&#22411;&#21487;&#20197;&#29992;&#26469;&#20943;&#23569;&#65288;e&#65289;NMPC&#30340;&#35745;&#31639;&#36127;&#25285;&#65307;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#31995;&#32479;&#36776;&#35782;&#20197;&#22312;&#27169;&#25311;&#26679;&#26412;&#19978;&#33719;&#24471;&#26368;&#22823;&#24179;&#22343;&#39044;&#27979;&#20934;&#30830;&#24615;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20316;&#20026;&#23454;&#38469;&#65288;e&#65289;NMPC&#30340;&#19968;&#37096;&#20998;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#26368;&#20339;&#65288;e&#65289;NMPC&#24615;&#33021;&#30340;&#21160;&#24577;&#26367;&#20195;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#32780;&#24471;&#21040;&#20855;&#26377;&#25511;&#21046;&#24615;&#33021;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#33391;&#22909;&#24179;&#34913;&#30340;&#39044;&#27979;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#22522;&#20110;&#24050;&#24314;&#31435;&#30340;&#38750;&#32447;&#24615;&#36830;&#32493;&#25605;&#25292;&#21453;&#24212;&#22120;&#27169;&#22411;&#30340;&#24212;&#29992;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
(Economic) nonlinear model predictive control ((e)NMPC) requires dynamic system models that are sufficiently accurate in all relevant state-space regions. These models must also be computationally cheap enough to ensure real-time tractability. Data-driven surrogate models for mechanistic models can be used to reduce the computational burden of (e)NMPC; however, such models are typically trained by system identification for maximum average prediction accuracy on simulation samples and perform suboptimally as part of actual (e)NMPC. We present a method for end-to-end reinforcement learning of dynamic surrogate models for optimal performance in (e)NMPC applications, resulting in predictive controllers that strike a favorable balance between control performance and computational demand. We validate our method on two applications derived from an established nonlinear continuous stirred-tank reactor model. We compare the controller performance to that of MPCs utilizing models trained by the 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#31435;&#20445;&#38505;&#29702;&#36180;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#25193;&#23637;&#20102;split conformal prediction&#25216;&#26415;&#21040;&#20004;&#38454;&#27573;&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#20316;&#20026;&#20005;&#37325;&#24615;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#34955;&#22806;&#26426;&#21046;&#28040;&#38500;&#20102;&#26657;&#20934;&#38598;&#30340;&#38656;&#35201;&#65292;&#24182;&#23454;&#29616;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#23485;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.13124</link><description>&lt;p&gt;
&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#30340;&#31526;&#21512;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction for frequency-severity modeling. (arXiv:2307.13124v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13124
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#31435;&#20445;&#38505;&#29702;&#36180;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#25193;&#23637;&#20102;split conformal prediction&#25216;&#26415;&#21040;&#20004;&#38454;&#27573;&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#20316;&#20026;&#20005;&#37325;&#24615;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#34955;&#22806;&#26426;&#21046;&#28040;&#38500;&#20102;&#26657;&#20934;&#38598;&#30340;&#38656;&#35201;&#65292;&#24182;&#23454;&#29616;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#23485;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#31435;&#20445;&#38505;&#29702;&#36180;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#23558;&#20998;&#21106;&#31526;&#21512;&#24615;&#39044;&#27979;&#25216;&#26415;&#25193;&#23637;&#21040;&#20004;&#38454;&#27573;&#39057;&#29575;-&#20005;&#37325;&#24615;&#24314;&#27169;&#39046;&#22495;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#24403;&#22522;&#30784;&#20005;&#37325;&#24615;&#27169;&#22411;&#26159;&#38543;&#26426;&#26862;&#26519;&#26102;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20004;&#38454;&#27573;&#20998;&#21106;&#31526;&#21512;&#24615;&#39044;&#27979;&#36807;&#31243;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#34955;&#22806;&#26426;&#21046;&#28040;&#38500;&#26657;&#20934;&#38598;&#30340;&#38656;&#35201;&#65292;&#24182;&#23454;&#29616;&#20855;&#26377;&#33258;&#36866;&#24212;&#23485;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a nonparametric model-agnostic framework for building prediction intervals of insurance claims, with finite sample statistical guarantees, extending the technique of split conformal prediction to the domain of two-stage frequency-severity modeling. The effectiveness of the framework is showcased with simulated and real datasets. When the underlying severity model is a random forest, we extend the two-stage split conformal prediction procedure, showing how the out-of-bag mechanism can be leveraged to eliminate the need for a calibration set and to enable the production of prediction intervals with adaptive width.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;U-Net&#36827;&#34892;&#32974;&#20799;&#22836;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;MobileNet&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#24182;&#23545;&#26377;&#38480;&#30340;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30456;&#23218;&#32654;&#30340;&#20998;&#21106;&#24615;&#33021;&#65292;&#19988;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.09067</link><description>&lt;p&gt;
&#35780;&#20272;&#20351;&#29992;U-Net&#36827;&#34892;&#32974;&#20799;&#22836;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24494;&#35843;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image Segmentation with U-Net. (arXiv:2307.09067v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;U-Net&#36827;&#34892;&#32974;&#20799;&#22836;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;MobileNet&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#24182;&#23545;&#26377;&#38480;&#30340;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30456;&#23218;&#32654;&#30340;&#20998;&#21106;&#24615;&#33021;&#65292;&#19988;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32974;&#20799;&#22836;&#20998;&#21106;&#26159;&#27979;&#37327;&#22922;&#23072;&#26399;&#38388;&#32974;&#20799;&#22836;&#22260;(HC)&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#26159;&#30417;&#27979;&#32974;&#20799;&#29983;&#38271;&#30340;&#37325;&#35201;&#29983;&#29289;&#27979;&#23450;&#23398;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#29983;&#25104;&#29983;&#29289;&#23398;&#27979;&#23450;&#26159;&#32791;&#26102;&#19988;&#32467;&#26524;&#19981;&#19968;&#33268;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32454;&#35843;(U-Net&#32593;&#32476;&#21644;&#36731;&#37327;&#32423;&#30340;MobileNet&#20316;&#20026;&#32534;&#30721;&#22120;)&#23545;&#19968;&#32452;&#26377;&#38480;&#30340;&#32974;&#20799;&#22836;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;CNN&#32593;&#32476;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#32454;&#35843;&#31574;&#30053;&#22312;&#35757;&#32451;&#21442;&#25968;&#20943;&#23569;85.8%&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;&#24182;&#19988;&#65292;&#25105;&#20204;&#30340;&#32454;&#35843;&#31574;&#30053;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fetal head segmentation is a crucial step in measuring the fetal head circumference (HC) during gestation, an important biometric in obstetrics for monitoring fetal growth. However, manual biometry generation is time-consuming and results in inconsistent accuracy. To address this issue, convolutional neural network (CNN) models have been utilized to improve the efficiency of medical biometry. But training a CNN network from scratch is a challenging task, we proposed a Transfer Learning (TL) method. Our approach involves fine-tuning (FT) a U-Net network with a lightweight MobileNet as the encoder to perform segmentation on a set of fetal head ultrasound (US) images with limited effort. This method addresses the challenges associated with training a CNN network from scratch. It suggests that our proposed FT strategy yields segmentation performance that is comparable when trained with a reduced number of parameters by 85.8%. And our proposed FT strategy outperforms other strategies with s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#32534;&#30721;&#20132;&#20114;&#26469;&#20934;&#30830;&#19988;&#31616;&#26126;&#22320;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#25512;&#29702;&#36923;&#36753;&#30340;&#26041;&#27861;&#12290;&#38024;&#23545;&#27492;&#30446;&#30340;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21363;AND&#20132;&#20114;&#21644;OR&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#35774;&#35745;&#20986;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13312</link><description>&lt;p&gt;
&#25216;&#26415;&#31508;&#35760;&#65306;&#23450;&#20041;&#21644;&#37327;&#21270;DNN&#30340;AND-OR&#20132;&#20114;&#20197;&#36827;&#34892;&#20934;&#30830;&#21644;&#31616;&#26126;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Technical Note: Defining and Quantifying AND-OR Interactions for Faithful and Concise Explanation of DNNs. (arXiv:2304.13312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#32534;&#30721;&#20132;&#20114;&#26469;&#20934;&#30830;&#19988;&#31616;&#26126;&#22320;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#25512;&#29702;&#36923;&#36753;&#30340;&#26041;&#27861;&#12290;&#38024;&#23545;&#27492;&#30446;&#30340;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21363;AND&#20132;&#20114;&#21644;OR&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#35774;&#35745;&#20986;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#32534;&#30721;&#20132;&#20114;&#26469;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#25512;&#29702;&#36923;&#36753;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#37325;&#26032;&#24605;&#32771;&#20132;&#20114;&#30340;&#23450;&#20041;&#65292;&#28982;&#21518;&#27491;&#24335;&#23450;&#20041;&#20102;&#22522;&#20110;&#20132;&#20114;&#30340;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#31616;&#27905;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21363;AND&#20132;&#20114;&#21644;OR&#20132;&#20114;&#12290;&#38024;&#23545;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;AND&#65288;OR&#65289;&#20132;&#20114;&#22312;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;AND&#65288;OR&#65289;&#20851;&#31995;&#25928;&#24212;&#26041;&#38754;&#30340;&#21807;&#19968;&#24615;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;AND-OR&#20132;&#20114;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#25216;&#26415;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;DNN&#30340;&#25512;&#29702;&#36923;&#36753;&#21487;&#20197;&#36890;&#36807;&#19968;&#32452;&#31526;&#21495;&#27010;&#24565;&#20934;&#30830;&#32780;&#31616;&#26126;&#22320;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this technical note, we aim to explain a deep neural network (DNN) by quantifying the encoded interactions between input variables, which reflects the DNN's inference logic. Specifically, we first rethink the definition of interactions, and then formally define faithfulness and conciseness for interaction-based explanation. To this end, we propose two kinds of interactions, i.e., the AND interaction and the OR interaction. For faithfulness, we prove the uniqueness of the AND (OR) interaction in quantifying the effect of the AND (OR) relationship between input variables. Besides, based on AND-OR interactions, we design techniques to boost the conciseness of the explanation, while not hurting the faithfulness. In this way, the inference logic of a DNN can be faithfully and concisely explained by a set of symbolic concepts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#36827;&#34892;&#20809;&#23398;&#31995;&#32479;&#38750;&#32447;&#24615;&#36890;&#36947;&#34917;&#20607;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;Transformer&#30340;&#35760;&#24518;&#20851;&#27880;&#33021;&#21147;&#21644;&#24182;&#34892;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#34917;&#20607;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#20449;&#24687;&#25513;&#30721;&#65292;&#29992;&#20110;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13119</link><description>&lt;p&gt;
&#21033;&#29992;Transformer&#36827;&#34892;&#20809;&#23398;&#31995;&#32479;&#38750;&#32447;&#24615;&#36890;&#36947;&#34917;&#20607;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Transformers for Nonlinear Channel Compensation in Optical Systems. (arXiv:2304.13119v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#36827;&#34892;&#20809;&#23398;&#31995;&#32479;&#38750;&#32447;&#24615;&#36890;&#36947;&#34917;&#20607;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;Transformer&#30340;&#35760;&#24518;&#20851;&#27880;&#33021;&#21147;&#21644;&#24182;&#34892;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#34917;&#20607;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#20449;&#24687;&#25513;&#30721;&#65292;&#29992;&#20110;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#36890;&#36947;&#22343;&#34913;&#26041;&#27861;&#65292;&#29992;&#20110;&#30456;&#24178;&#38271;&#36317;&#31163;&#20256;&#36755;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#30452;&#25509;&#20851;&#27880;&#19968;&#31995;&#21015;&#31526;&#21495;&#20043;&#38388;&#30340;&#35760;&#24518;&#65292;&#22240;&#27492;Transformer&#21487;&#20197;&#19982;&#24182;&#34892;&#32467;&#26500;&#26377;&#25928;&#22320;&#37197;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#32534;&#30721;&#22120;&#37096;&#20998;&#30340;Transformer&#23454;&#29616;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#22343;&#34913;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#19981;&#21516;&#36229;&#21442;&#25968;&#33539;&#22260;&#20869;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#22788;&#29702;&#31526;&#21495;&#22359;&#65292;&#24182;&#20180;&#32454;&#36873;&#25321;&#35201;&#19968;&#36215;&#22788;&#29702;&#30340;&#32534;&#30721;&#22120;&#36755;&#20986;&#23376;&#38598;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#38750;&#32447;&#24615;&#34917;&#20607;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#32447;&#24615;&#25200;&#21160;&#29702;&#35770;&#30340;&#29289;&#29702;&#23398;&#20449;&#24687;&#25513;&#30721;&#65292;&#29992;&#20110;&#38477;&#20302;Transformer&#38750;&#32447;&#24615;&#22343;&#34913;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new nonlinear channel equalization method for the coherent long-haul transmission based on Transformers. We show that due to their capability to attend directly to the memory across a sequence of symbols, Transformers can be used effectively with a parallelized structure. We present an implementation of encoder part of Transformer for nonlinear equalization and analyze its performance over a wide range of different hyper-parameters. It is shown that by processing blocks of symbols at each iteration and carefully selecting subsets of the encoder's output to be processed together, an efficient nonlinear compensation can be achieved. We also propose the use of a physic-informed mask inspired by nonlinear perturbation theory for reducing the computational complexity of Transformer nonlinear equalization.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#21450;&#20854;&#26041;&#27861;&#65292;&#20854;&#20013;&#26679;&#26412;&#27867;&#21270;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26410;&#26469;&#38656;&#35201;&#37325;&#28857;&#20851;&#27880;&#20943;&#23569;&#36807;&#25311;&#21512;&#65307;&#20998;&#24067;&#27867;&#21270;&#19982;&#39046;&#22495;&#27867;&#21270;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#22256;&#38590;&#30340;&#26679;&#26412;&#25110;&#20998;&#24067;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2209.01610</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#65306;&#32508;&#36848;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Generalization in Neural Networks: A Broad Survey. (arXiv:2209.01610v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01610
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#21450;&#20854;&#26041;&#27861;&#65292;&#20854;&#20013;&#26679;&#26412;&#27867;&#21270;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26410;&#26469;&#38656;&#35201;&#37325;&#28857;&#20851;&#27880;&#20943;&#23569;&#36807;&#25311;&#21512;&#65307;&#20998;&#24067;&#27867;&#21270;&#19982;&#39046;&#22495;&#27867;&#21270;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#22256;&#38590;&#30340;&#26679;&#26412;&#25110;&#20998;&#24067;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#27010;&#24565;&#12289;&#24314;&#27169;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#21253;&#25324;&#26679;&#26412;&#12289;&#20998;&#24067;&#12289;&#39046;&#22495;&#12289;&#20219;&#21153;&#12289;&#27169;&#24577;&#21644;&#33539;&#22260;&#19978;&#30340;&#27867;&#21270;&#12290;&#22312;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;&#26368;&#26032;&#25913;&#36827;&#37117;&#20943;&#23567;&#20102;&#35757;&#32451;&#35823;&#24046;&#65292;&#32780;&#36807;&#25311;&#21512;&#20445;&#25345;&#19981;&#21464;&#65307;&#38543;&#30528;&#20960;&#20046;&#25152;&#26377;&#30340;&#35757;&#32451;&#35823;&#24046;&#34987;&#28040;&#38500;&#65292;&#26410;&#26469;&#30340;&#36827;&#23637;&#23558;&#38656;&#35201;&#38598;&#20013;&#20851;&#27880;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#20174;&#32479;&#35745;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;(2)&#20998;&#24067;&#27867;&#21270;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#26679;&#26412;&#26435;&#37325;&#25110;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#30340;&#21464;&#21270;&#65307;&#22240;&#27492;&#65292;&#22312;&#39046;&#22495;&#27867;&#21270;&#25104;&#21151;&#30340;&#25216;&#26415;&#26377;&#21487;&#33021;&#24212;&#29992;&#20110;&#22256;&#38590;&#30340;&#26679;&#26412;&#25110;&#20998;&#24067;&#27867;&#21270;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;(3)&#39046;&#22495;&#27867;&#21270;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#26368;&#36817;&#30340;&#36827;&#23637;&#21644;&#20016;&#23500;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reviews concepts, modeling approaches, and recent findings along a spectrum of different levels of abstraction of neural network models including generalization across (1) Samples, (2) Distributions, (3) Domains, (4) Tasks, (5) Modalities, and (6) Scopes. Results on (1) sample generalization show that, in the case of ImageNet, nearly all the recent improvements reduced training error while overfitting stayed flat; with nearly all the training error eliminated, future progress will require a focus on reducing overfitting. Perspectives from statistics highlight how (2) distribution generalization can be viewed alternately as a change in sample weights or a change in the input-output relationship; thus, techniques that have been successful in domain generalization have the potential to be applied to difficult forms of sample or distribution generalization. Transfer learning approaches to (3) domain generalization are summarized, as are recent advances and the wealth of domain a
&lt;/p&gt;</description></item></channel></rss>