<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;HTTP&#21709;&#24212;&#22836;&#35774;&#35745;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#22312;&#36328;&#27983;&#35272;&#22120;&#29615;&#22659;&#19979;&#26377;&#25928;&#26816;&#27979;Web&#36861;&#36394;&#22120;&#65292;&#32467;&#26524;&#22312;Chrome&#21644;Firefox&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01240</link><description>&lt;p&gt;
&#36229;&#36234;&#35831;&#27714;&#65306;&#21033;&#29992;HTTP&#21709;&#24212;&#22836;&#22312;&#19981;&#24179;&#34913;&#29615;&#22659;&#20013;&#36827;&#34892;&#36328;&#27983;&#35272;&#22120;Web&#36861;&#36394;&#22120;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Beyond the Request: Harnessing HTTP Response Headers for Cross-Browser Web Tracker Classification in an Imbalanced Setting
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;HTTP&#21709;&#24212;&#22836;&#35774;&#35745;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#22312;&#36328;&#27983;&#35272;&#22120;&#29615;&#22659;&#19979;&#26377;&#25928;&#26816;&#27979;Web&#36861;&#36394;&#22120;&#65292;&#32467;&#26524;&#22312;Chrome&#21644;Firefox&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19975;&#32500;&#32593;&#30340;&#36830;&#36890;&#24615;&#20027;&#35201;&#24402;&#22240;&#20110;HTTP&#21327;&#35758;&#65292;&#20854;&#20013;&#30340;HTTP&#28040;&#24687;&#25552;&#20379;&#20102;&#26377;&#20851;&#32593;&#32476;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#20449;&#24687;&#22836;&#23383;&#27573;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;Web&#36861;&#36394;&#12290;&#23613;&#31649;&#24050;&#26377;&#30740;&#31350;&#21033;&#29992;HTTP/S&#35831;&#27714;&#28040;&#24687;&#26469;&#35782;&#21035;Web&#36861;&#36394;&#22120;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;HTTP/S&#21709;&#24212;&#22836;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35774;&#35745;&#20351;&#29992;HTTP/S&#21709;&#24212;&#22836;&#36827;&#34892;Web&#36861;&#36394;&#22120;&#26816;&#27979;&#30340;&#26377;&#25928;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#27983;&#35272;&#22120;&#25193;&#23637;&#31243;&#24207;T.EX&#33719;&#21462;&#30340;Chrome&#12289;Firefox&#21644;Brave&#27983;&#35272;&#22120;&#30340;&#25968;&#25454;&#20316;&#20026;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;Chrome&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;11&#20010;&#30417;&#30563;&#27169;&#22411;&#65292;&#24182;&#22312;&#25152;&#26377;&#27983;&#35272;&#22120;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;Chrome&#21644;Firefox&#19978;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12289;F1&#20998;&#25968;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#26368;&#23567;&#23545;&#25968;&#25439;&#22833;&#35823;&#24046;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;Brave&#27983;&#35272;&#22120;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#20854;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#29305;&#24449;&#38598;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;Web&#36861;&#36394;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The World Wide Web's connectivity is greatly attributed to the HTTP protocol, with HTTP messages offering informative header fields that appeal to disciplines like web security and privacy, especially concerning web tracking. Despite existing research employing HTTP/S request messages to identify web trackers, HTTP/S response headers are often overlooked. This study endeavors to design effective machine learning classifiers for web tracker detection using HTTP/S response headers. Data from the Chrome, Firefox, and Brave browsers, obtained through the traffic monitoring browser extension T.EX, serves as our data set. Eleven supervised models were trained on Chrome data and tested across all browsers. The results demonstrated high accuracy, F1-score, precision, recall, and minimal log-loss error for Chrome and Firefox, but subpar performance on Brave, potentially due to its distinct data distribution and feature set. The research suggests that these classifiers are viable for detecting w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#35299;&#32544;&#31163;&#25955;&#22270;&#33258;&#32534;&#30721;&#22120;(DGA)&#21644;&#35299;&#32544;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;(DVGA)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#35299;&#32544;&#34920;&#31034;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01143</link><description>&lt;p&gt;
&#29992;&#35299;&#32544;&#31163;&#25955;&#22270;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#32593;&#32476;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Network Representations with Disentangled Graph Auto-Encoder
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#35299;&#32544;&#31163;&#25955;&#22270;&#33258;&#32534;&#30721;&#22120;(DGA)&#21644;&#35299;&#32544;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;(DVGA)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#35299;&#32544;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
(&#21464;&#20998;)&#22270;&#33258;&#32534;&#30721;&#22120;&#24191;&#27867;&#29992;&#20110;&#23398;&#20064;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#22270;&#30340;&#24418;&#25104;&#26159;&#19968;&#20010;&#30001;&#28508;&#22312;&#22240;&#32032;&#24433;&#21709;&#30340;&#22797;&#26434;&#21644;&#24322;&#36136;&#30340;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;&#22522;&#26412;&#19978;&#26159;&#25972;&#20307;&#30340;&#65292;&#24573;&#35270;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#32416;&#32544;&#12290;&#36825;&#19981;&#20165;&#20351;&#24471;&#22270;&#20998;&#26512;&#20219;&#21153;&#19981;&#22826;&#26377;&#25928;&#65292;&#32780;&#19988;&#20351;&#24471;&#29702;&#35299;&#21644;&#35299;&#37322;&#36825;&#20123;&#34920;&#31034;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#29992;(&#21464;&#20998;)&#22270;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#35299;&#32544;&#30340;&#22270;&#34920;&#31034;&#38754;&#20020;&#30528;&#37325;&#35201;&#25361;&#25112;&#65292;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35299;&#32544;&#31163;&#25955;&#22270;&#33258;&#32534;&#30721;&#22120;(DGA)&#21644;&#35299;&#32544;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;(DVGA)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#26469;&#23398;&#20064;&#35299;&#32544;&#34920;&#31034;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#35299;&#32544;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#20351;&#29992;&#22810;&#36890;&#36947;&#28040;&#24687;&#20256;&#36882;&#23618;&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#32858;&#21512;&#19982;&#27599;&#20010;&#33410;&#28857;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The (variational) graph auto-encoder is extensively employed for learning representations of graph-structured data. However, the formation of real-world graphs is a complex and heterogeneous process influenced by latent factors. Existing encoders are fundamentally holistic, neglecting the entanglement of latent factors. This not only makes graph analysis tasks less effective but also makes it harder to understand and explain the representations. Learning disentangled graph representations with (variational) graph auto-encoder poses significant challenges, and remains largely unexplored in the existing literature. In this article, we introduce the Disentangled Graph Auto-Encoder (DGA) and Disentangled Variational Graph Auto-Encoder (DVGA), approaches that leverage generative models to learn disentangled representations. Specifically, we first design a disentangled graph convolutional network with multi-channel message-passing layers, as the encoder aggregating information related to eac
&lt;/p&gt;</description></item><item><title>JailbreakBench&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#25239;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#29425;&#30340;&#24320;&#25918;&#22522;&#20934;&#65292;&#25552;&#20379;&#26032;&#30340;&#25968;&#25454;&#38598;&#12289;&#23545;&#25239;&#25552;&#31034;&#21644;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2404.01318</link><description>&lt;p&gt;
JailbreakBench: &#19968;&#20010;&#29992;&#20110;&#23545;&#25239;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#29425;&#30340;&#24320;&#25918;&#40065;&#26834;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01318
&lt;/p&gt;
&lt;p&gt;
JailbreakBench&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#25239;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#29425;&#30340;&#24320;&#25918;&#22522;&#20934;&#65292;&#25552;&#20379;&#26032;&#30340;&#25968;&#25454;&#38598;&#12289;&#23545;&#25239;&#25552;&#31034;&#21644;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#29425;&#25915;&#20987;&#20250;&#23548;&#33268;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#12289;&#19981;&#36947;&#24503;&#25110;&#20196;&#20154;&#21453;&#24863;&#30340;&#20869;&#23481;&#12290;&#35780;&#20272;&#36825;&#20123;&#25915;&#20987;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#25216;&#26415;&#24182;&#26410;&#20805;&#20998;&#35299;&#20915;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;JailbreakBench&#65292;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#65292;&#21253;&#25324;&#20855;&#26377;100&#20010;&#29420;&#29305;&#34892;&#20026;&#30340;&#26032;&#36234;&#29425;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;JBB-Behaviors&#65289;&#12289;&#19968;&#32452;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#25552;&#31034;&#65288;&#31216;&#20026;&#36234;&#29425;&#24037;&#20214;&#65289;&#21644;&#19968;&#20010;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01318v1 Announce Type: cross  Abstract: Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) a new jailbreaking dataset containing 100 unique behaviors, which we call JBB-Behaviors; (2) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (3) a standardized evaluation framework that i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102; Generalized Latent Equilibrium (GLE)&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#29289;&#29702;&#21160;&#24577;&#23616;&#37096;&#26102;&#31354;&#20449;&#29992;&#20998;&#37197;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.16933</link><description>&lt;p&gt;
&#36890;&#36807;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#22823;&#33041;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Backpropagation through space, time, and the brain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16933
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102; Generalized Latent Equilibrium (GLE)&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#29289;&#29702;&#21160;&#24577;&#23616;&#37096;&#26102;&#31354;&#20449;&#29992;&#20998;&#37197;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38656;&#35201;&#26681;&#25454;&#23427;&#20204;&#23545;&#35299;&#20915;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#26469;&#35843;&#25972;&#21333;&#20010;&#31361;&#35302;&#12290;&#28982;&#32780;&#65292;&#26080;&#35770;&#26159;&#29983;&#29289;&#36824;&#26159;&#20154;&#24037;&#30340;&#29289;&#29702;&#31070;&#32463;&#31995;&#32479;&#37117;&#21463;&#21040;&#26102;&#31354;&#23616;&#38480;&#12290;&#36825;&#26679;&#30340;&#32593;&#32476;&#22914;&#20309;&#25191;&#34892;&#39640;&#25928;&#30340;&#20449;&#29992;&#20998;&#37197;&#65292;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20173;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#38169;&#35823;&#30340;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#20960;&#20046;&#26222;&#36941;&#34987;&#31354;&#38388;&#65288;BP&#65289;&#21644;&#26102;&#38388;&#65288;BPTT&#65289;&#20004;&#31181;&#26041;&#24335;&#32473;&#20986;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;BP(TT)&#34987;&#24191;&#27867;&#35748;&#20026;&#20381;&#36182;&#20110;&#19981;&#20855;&#29983;&#29289;&#23398;&#24847;&#20041;&#30340;&#20551;&#35774;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#26102;&#31354;&#23616;&#38480;&#24615;&#65292;&#32780;&#27491;&#21521;&#20256;&#25773;&#27169;&#22411;&#65292;&#22914;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#65288;RTRL&#65289;&#65292;&#21017;&#21463;&#21040;&#20869;&#23384;&#32422;&#26463;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24191;&#20041;&#28508;&#22312;&#24179;&#34913;&#65288;GLE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#31070;&#32463;&#20803;&#29289;&#29702;&#21160;&#24577;&#32593;&#32476;&#23436;&#20840;&#23616;&#37096;&#26102;&#31354;&#20449;&#29992;&#20998;&#37197;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;&#25105;&#20204;&#20174;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16933v1 Announce Type: cross  Abstract: Effective learning in neuronal networks requires the adaptation of individual synapses given their relative contribution to solving a task. However, physical neuronal systems -- whether biological or artificial -- are constrained by spatio-temporal locality. How such networks can perform efficient credit assignment, remains, to a large extent, an open question. In Machine Learning, the answer is almost universally given by the error backpropagation algorithm, through both space (BP) and time (BPTT). However, BP(TT) is well-known to rely on biologically implausible assumptions, in particular with respect to spatiotemporal (non-)locality, while forward-propagation models such as real-time recurrent learning (RTRL) suffer from prohibitive memory constraints. We introduce Generalized Latent Equilibrium (GLE), a computational framework for fully local spatio-temporal credit assignment in physical, dynamical networks of neurons. We start by 
&lt;/p&gt;</description></item><item><title>GLAD&#26159;&#19968;&#20010;&#22312;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36866;&#24212;&#25193;&#25955;&#26725;&#32467;&#26500;&#23398;&#20064;&#20854;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#20110;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#30340;&#20998;&#35299;&#65292;&#22312;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16883</link><description>&lt;p&gt;
&#24102;&#25193;&#25955;&#26725;&#30340;&#31163;&#25955;&#28508;&#22312;&#22270;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Discrete Latent Graph Generative Modeling with Diffusion Bridges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16883
&lt;/p&gt;
&lt;p&gt;
GLAD&#26159;&#19968;&#20010;&#22312;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36866;&#24212;&#25193;&#25955;&#26725;&#32467;&#26500;&#23398;&#20064;&#20854;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#20110;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#30340;&#20998;&#35299;&#65292;&#22312;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#20110;&#22312;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#27169;&#22411;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#65292;&#36804;&#20170;&#34920;&#29616;&#20986;&#30340;&#24615;&#33021;&#20047;&#21892;&#21487;&#38472;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GLAD&#65292;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#22270;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#28508;&#22312;&#31354;&#38388;&#22270;&#29983;&#25104;&#27169;&#22411;&#19981;&#21516;&#65292;GLAD&#22312;&#20445;&#30041;&#22270;&#32467;&#26500;&#30340;&#31163;&#25955;&#24615;&#36136;&#26041;&#38754;&#36816;&#34892;&#65292;&#26080;&#38656;&#36827;&#34892;&#35832;&#22914;&#28508;&#22312;&#31354;&#38388;&#36830;&#32493;&#24615;&#31561;&#19981;&#33258;&#28982;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25193;&#25955;&#26725;&#35843;&#25972;&#21040;&#20854;&#32467;&#26500;&#65292;&#26469;&#23398;&#20064;&#25105;&#20204;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#12290;&#36890;&#36807;&#22312;&#36866;&#24403;&#26500;&#24314;&#30340;&#28508;&#22312;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#25105;&#20204;&#36991;&#20813;&#20381;&#36182;&#20110;&#24120;&#29992;&#20110;&#22312;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#25805;&#20316;&#30340;&#27169;&#22411;&#20013;&#30340;&#20998;&#35299;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#22270;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#26126;&#26174;&#23637;&#31034;&#20102;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20248;&#36234;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#29983;&#25104;&#24615;&#33021;&#65292;&#20351;GLA
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16883v1 Announce Type: new  Abstract: Learning graph generative models over latent spaces has received less attention compared to models that operate on the original data space and has so far demonstrated lacklustre performance. We present GLAD a latent space graph generative model. Unlike most previous latent space graph generative models, GLAD operates on a discrete latent space that preserves to a significant extent the discrete nature of the graph structures making no unnatural assumptions such as latent space continuity. We learn the prior of our discrete latent space by adapting diffusion bridges to its structure. By operating over an appropriately constructed latent space we avoid relying on decompositions that are often used in models that operate in the original data space. We present experiments on a series of graph benchmark datasets which clearly show the superiority of the discrete latent space and obtain state of the art graph generative performance, making GLA
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#21152;&#36895;&#35270;&#39057;&#20998;&#26512;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36731;&#37327;&#32423;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#36827;&#34892;&#37096;&#32626;&#25512;&#29702;&#65292;&#21033;&#29992;&#26356;&#22823;&#30340;&#8220;&#25945;&#24072;&#8221;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#26631;&#35760;&#65292;&#23454;&#29616;&#23545;&#19981;&#26029;&#21464;&#21270;&#22330;&#26223;&#30340;&#25345;&#32493;&#33258;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.14353</link><description>&lt;p&gt;
DaCapo&#65306;&#21152;&#24555;&#33258;&#20027;&#31995;&#32479;&#22312;&#35270;&#39057;&#20998;&#26512;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DaCapo: Accelerating Continuous Learning in Autonomous Systems for Video Analytics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14353
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#21152;&#36895;&#35270;&#39057;&#20998;&#26512;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36731;&#37327;&#32423;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#36827;&#34892;&#37096;&#32626;&#25512;&#29702;&#65292;&#21033;&#29992;&#26356;&#22823;&#30340;&#8220;&#25945;&#24072;&#8221;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#26631;&#35760;&#65292;&#23454;&#29616;&#23545;&#19981;&#26029;&#21464;&#21270;&#22330;&#26223;&#30340;&#25345;&#32493;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#35270;&#39057;&#20998;&#26512;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12289;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#21644;&#23433;&#38450;&#26426;&#22120;&#20154;&#31561;&#33258;&#20027;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#30005;&#27744;&#21151;&#29575;&#65292;&#23454;&#38469;&#37096;&#32626;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25345;&#32493;&#23398;&#20064;&#21033;&#29992;&#22312;&#37096;&#32626;&#65288;&#25512;&#29702;&#65289;&#20013;&#30340;&#36731;&#37327;&#32423;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#65292;&#21033;&#29992;&#26356;&#22823;&#30340;&#8220;&#25945;&#24072;&#8221;&#27169;&#22411;&#23545;&#37319;&#26679;&#25968;&#25454;&#36827;&#34892;&#26631;&#35760;&#65288;&#26631;&#35760;&#65289;&#65292;&#24182;&#19981;&#26029;&#37325;&#26032;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#20197;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14353v1 Announce Type: cross  Abstract: Deep neural network (DNN) video analytics is crucial for autonomous systems such as self-driving vehicles, unmanned aerial vehicles (UAVs), and security robots. However, real-world deployment faces challenges due to their limited computational resources and battery power. To tackle these challenges, continuous learning exploits a lightweight "student" model at deployment (inference), leverages a larger "teacher" model for labeling sampled data (labeling), and continuously retrains the student model to adapt to changing scenarios (retraining). This paper highlights the limitations in state-of-the-art continuous learning systems: (1) they focus on computations for retraining, while overlooking the compute needs for inference and labeling, (2) they rely on power-hungry GPUs, unsuitable for battery-operated autonomous systems, and (3) they are located on a remote centralized server, intended for multi-tenant scenarios, again unsuitable for
&lt;/p&gt;</description></item><item><title>RoPE&#22312;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22806;&#25512;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;ImageNet-1k&#12289;COCO&#26816;&#27979;&#21644;ADE-20k&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13298</link><description>&lt;p&gt;
&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Rotary Position Embedding for Vision Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13298
&lt;/p&gt;
&lt;p&gt;
RoPE&#22312;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22806;&#25512;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;ImageNet-1k&#12289;COCO&#26816;&#27979;&#21644;ADE-20k&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65288;RoPE&#65289;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;Transformer&#30340;&#38271;&#24230;&#22806;&#25512;&#12290;&#28982;&#32780;&#65292;RoPE&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#65292;&#23613;&#31649;RoPE&#20284;&#20046;&#33021;&#22815;&#20687;&#35821;&#35328;&#39046;&#22495;&#19968;&#26679;&#22686;&#24378;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;ViT&#65289;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#23545;&#23558;RoPE&#24212;&#29992;&#20110;ViT&#26102;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21033;&#29992;RoPE&#22312;2D&#35270;&#35273;&#25968;&#25454;&#19978;&#30340;&#23454;&#38469;&#23454;&#29616;&#12290;&#20998;&#26512;&#26174;&#31034;&#65292;RoPE&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22806;&#25512;&#24615;&#33021;&#65292;&#21363;&#22312;&#25512;&#26029;&#26102;&#22312;&#22686;&#21152;&#22270;&#20687;&#20998;&#36776;&#29575;&#30340;&#21516;&#26102;&#20445;&#25345;&#31934;&#24230;&#12290;&#26368;&#32456;&#23548;&#33268;&#20102;ImageNet-1k&#12289;COCO&#26816;&#27979;&#21644;ADE-20k&#20998;&#21106;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30456;&#20449;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23558;RoPE&#24212;&#29992;&#20110;ViT&#30340;&#35814;&#23613;&#25351;&#23548;&#65292;&#25215;&#35834;&#36890;&#36807;&#26368;&#23567;&#30340;&#39069;&#22806;&#35745;&#31639;&#24320;&#38144;&#25552;&#39640;&#39592;&#24178;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;&#32593;&#22336;https://&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13298v1 Announce Type: cross  Abstract: Rotary Position Embedding (RoPE) performs remarkably on language models, especially for length extrapolation of Transformers. However, the impacts of RoPE on computer vision domains have been underexplored, even though RoPE appears capable of enhancing Vision Transformer (ViT) performance in a way similar to the language domain. This study provides a comprehensive analysis of RoPE when applied to ViTs, utilizing practical implementations of RoPE for 2D vision data. The analysis reveals that RoPE demonstrates impressive extrapolation performance, i.e., maintaining precision while increasing image resolution at inference. It eventually leads to performance improvement for ImageNet-1k, COCO detection, and ADE-20k segmentation. We believe this study provides thorough guidelines to apply RoPE into ViT, promising improved backbone performance with minimal extra computational overhead. Our code and pre-trained models are available at https://
&lt;/p&gt;</description></item><item><title>MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.02694</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#24863;&#30693;&#35821;&#20041;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Privacy-Aware Semantic Cache for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02694
&lt;/p&gt;
&lt;p&gt;
MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#12289;Google Bard&#12289;Claude&#21644;Llama 2&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25628;&#32034;&#24341;&#25806;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36896;&#25104;&#20102;&#24322;&#24120;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MeanCache&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#20197;&#30830;&#23450;&#32531;&#23384;&#21629;&#20013;&#25110;&#26410;&#21629;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02694v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2 have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters and inference on these models also demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries, leading to unacceptable false hit-and-miss rates.   This paper introduces MeanCache, a semantic cache for LLMs that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. MeanCache lever
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#28508;&#22312;&#34920;&#31034;&#30340;&#20108;&#20998;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29983;&#24577;&#32593;&#32476;&#20013;&#30340;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#24341;&#20837;&#39069;&#22806;&#30340;HSIC&#24809;&#32602;&#39033;&#65292;&#30830;&#20445;&#20102;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#19982;&#36830;&#32493;&#21464;&#37327;&#30340;&#29420;&#31435;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02011</link><description>&lt;p&gt;
&#20844;&#24179;&#28508;&#22312;&#34920;&#31034;&#30340;&#20108;&#20998;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20197;&#35299;&#20915;&#29983;&#24577;&#32593;&#32476;&#20013;&#30340;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Bipartite Graph Variational Auto-Encoder with Fair Latent Representation to Account for Sampling Bias in Ecological Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#28508;&#22312;&#34920;&#31034;&#30340;&#20108;&#20998;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29983;&#24577;&#32593;&#32476;&#20013;&#30340;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#24341;&#20837;&#39069;&#22806;&#30340;HSIC&#24809;&#32602;&#39033;&#65292;&#30830;&#20445;&#20102;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#19982;&#36830;&#32493;&#21464;&#37327;&#30340;&#29420;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#23884;&#20837;&#26469;&#34920;&#31034;&#20108;&#20998;&#32593;&#32476;&#65292;&#20197;&#35299;&#20915;&#30740;&#31350;&#29983;&#24577;&#32593;&#32476;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#27604;&#22914;&#36830;&#25509;&#26893;&#29289;&#21644;&#20256;&#31881;&#32773;&#31561;&#32593;&#32476;&#65292;&#38656;&#32771;&#34385;&#35768;&#22810;&#21327;&#21464;&#37327;&#65292;&#23588;&#20854;&#35201;&#25511;&#21046;&#25277;&#26679;&#20559;&#24046;&#12290;&#25105;&#20204;&#23558;&#21464;&#20998;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#35843;&#25972;&#20026;&#20108;&#20998;&#24773;&#20917;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#23884;&#20837;&#65292;&#20854;&#20013;&#20004;&#32452;&#33410;&#28857;&#30340;&#20301;&#32622;&#22522;&#20110;&#23427;&#20204;&#30340;&#36830;&#25509;&#27010;&#29575;&#12290;&#25105;&#20204;&#23558;&#22312;&#31038;&#20250;&#23398;&#20013;&#24120;&#32771;&#34385;&#30340;&#20844;&#24179;&#24615;&#26694;&#26550;&#36716;&#21270;&#20026;&#29983;&#24577;&#23398;&#20013;&#30340;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;&#65288;HSIC&#65289;&#20316;&#20026;&#39069;&#22806;&#24809;&#32602;&#39033;&#65292;&#25105;&#20204;&#30830;&#20445;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#19982;&#36830;&#32493;&#21464;&#37327;&#65288;&#19982;&#25277;&#26679;&#36807;&#31243;&#30456;&#20851;&#65289;&#26080;&#20851;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#25913;&#21464;&#25105;&#20204;&#23545;&#29983;&#24577;&#32593;&#32476;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02011v1 Announce Type: cross  Abstract: We propose a method to represent bipartite networks using graph embeddings tailored to tackle the challenges of studying ecological networks, such as the ones linking plants and pollinators, where many covariates need to be accounted for, in particular to control for sampling bias. We adapt the variational graph auto-encoder approach to the bipartite case, which enables us to generate embeddings in a latent space where the two sets of nodes are positioned based on their probability of connection. We translate the fairness framework commonly considered in sociology in order to address sampling bias in ecology. By incorporating the Hilbert-Schmidt independence criterion (HSIC) as an additional penalty term in the loss we optimize, we ensure that the structure of the latent space is independent of continuous variables, which are related to the sampling process. Finally, we show how our approach can change our understanding of ecological n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32467;&#21512;&#39034;&#24207;&#21270;&#30340;MCMC&#32467;&#26500;&#23398;&#20064;&#25216;&#26415;&#21644;&#26799;&#24230;&#22270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#23558;&#22240;&#26524;&#32467;&#26500;&#25512;&#26029;&#38382;&#39064;&#20998;&#35299;&#20026;&#21464;&#37327;&#25299;&#25169;&#39034;&#24207;&#25512;&#26029;&#21644;&#21464;&#37327;&#29238;&#33410;&#28857;&#38598;&#21512;&#25512;&#26029;&#65292;&#21516;&#26102;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#22240;&#26524;&#26426;&#21046;&#24314;&#27169;&#23454;&#29616;&#31934;&#30830;&#36793;&#32536;&#21270;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;Rao-Blackwell&#21270;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.14781</link><description>&lt;p&gt;
Rao-Blackwellising Bayesian Causal Inference
&lt;/p&gt;
&lt;p&gt;
Rao-Blackwellising Bayesian Causal Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32467;&#21512;&#39034;&#24207;&#21270;&#30340;MCMC&#32467;&#26500;&#23398;&#20064;&#25216;&#26415;&#21644;&#26799;&#24230;&#22270;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#23558;&#22240;&#26524;&#32467;&#26500;&#25512;&#26029;&#38382;&#39064;&#20998;&#35299;&#20026;&#21464;&#37327;&#25299;&#25169;&#39034;&#24207;&#25512;&#26029;&#21644;&#21464;&#37327;&#29238;&#33410;&#28857;&#38598;&#21512;&#25512;&#26029;&#65292;&#21516;&#26102;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#22240;&#26524;&#26426;&#21046;&#24314;&#27169;&#23454;&#29616;&#31934;&#30830;&#36793;&#32536;&#21270;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;Rao-Blackwell&#21270;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#65292;&#21363;&#25512;&#26029;&#29992;&#20110;&#19979;&#28216;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#22240;&#26524;&#27169;&#22411;&#30340;&#21518;&#39564;&#27010;&#29575;&#65292;&#26500;&#25104;&#20102;&#19968;&#20010;&#22312;&#25991;&#29486;&#20013;&#40092;&#26377;&#25506;&#35752;&#30340;&#38590;&#35299;&#30340;&#35745;&#31639;&#25512;&#26029;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#22522;&#20110;&#39034;&#24207;&#30340;MCMC&#32467;&#26500;&#23398;&#20064;&#25216;&#26415;&#19982;&#26368;&#36817;&#26799;&#24230;&#22270;&#23398;&#20064;&#30340;&#36827;&#23637;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#30340;&#38382;&#39064;&#20998;&#35299;&#20026;(i)&#25512;&#26029;&#21464;&#37327;&#20043;&#38388;&#30340;&#25299;&#25169;&#39034;&#24207;&#20197;&#21450;(ii)&#25512;&#26029;&#27599;&#20010;&#21464;&#37327;&#30340;&#29238;&#33410;&#28857;&#38598;&#21512;&#12290;&#24403;&#38480;&#21046;&#27599;&#20010;&#21464;&#37327;&#30340;&#29238;&#33410;&#28857;&#25968;&#37327;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23436;&#20840;&#36793;&#32536;&#21270;&#29238;&#33410;&#28857;&#38598;&#21512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#26469;&#24314;&#27169;&#26410;&#30693;&#30340;&#22240;&#26524;&#26426;&#21046;&#65292;&#20174;&#32780;&#20801;&#35768;&#20854;&#31934;&#30830;&#36793;&#32536;&#21270;&#12290;&#36825;&#24341;&#20837;&#20102;&#19968;&#20010;Rao-Blackwell&#21270;&#26041;&#26696;&#65292;&#20854;&#20013;&#38500;&#20102;&#22240;&#26524;&#39034;&#24207;&#20043;&#22806;&#65292;&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#32452;&#20214;&#37117;&#34987;&#28040;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14781v1 Announce Type: cross  Abstract: Bayesian causal inference, i.e., inferring a posterior over causal models for the use in downstream causal reasoning tasks, poses a hard computational inference problem that is little explored in literature. In this work, we combine techniques from order-based MCMC structure learning with recent advances in gradient-based graph learning into an effective Bayesian causal inference framework. Specifically, we decompose the problem of inferring the causal structure into (i) inferring a topological order over variables and (ii) inferring the parent sets for each variable. When limiting the number of parents per variable, we can exactly marginalise over the parent sets in polynomial time. We further use Gaussian processes to model the unknown causal mechanisms, which also allows their exact marginalisation. This introduces a Rao-Blackwellization scheme, where all components are eliminated from the model, except for the causal order, for whi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#20849;&#20139;&#30340;&#31163;&#25955;&#27010;&#24565;&#26469;&#20943;&#36731;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#26631;&#35760;&#23376;&#32452;&#65292;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#28040;&#38500;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.13368</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#27010;&#24565;&#21457;&#29616;&#20943;&#36731;&#34394;&#20551;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Concept Discovery Mitigates Spurious Correlations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13368
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#20849;&#20139;&#30340;&#31163;&#25955;&#27010;&#24565;&#26469;&#20943;&#36731;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#26631;&#35760;&#23376;&#32452;&#65292;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#28040;&#38500;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#23481;&#26131;&#20135;&#29983;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#20135;&#29983;&#33030;&#24369;&#30340;&#39044;&#27979;&#24182;&#24341;&#20837;&#24847;&#22806;&#30340;&#20559;&#35265;&#12290;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#36890;&#24120;&#28041;&#21450;&#20381;&#36182;&#20808;&#39564;&#30693;&#35782;&#21644;&#32676;&#32452;&#27880;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#32780;&#36825;&#20123;&#20449;&#24687;&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#21487;&#33021;&#24182;&#19981;&#23481;&#26131;&#33719;&#24471;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26080;&#30417;&#30563;&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#19982;&#20943;&#36731;&#34394;&#20551;&#30456;&#20851;&#24615;&#20043;&#38388;&#30340;&#19968;&#31181;&#26032;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#26159;&#30452;&#25509;&#25512;&#26029;&#19982;&#26631;&#31614;&#20855;&#26377;&#19981;&#21516;&#30456;&#20851;&#24615;&#30340;&#23376;&#32452;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#21457;&#29616;&#27010;&#24565;&#65306;&#22312;&#36755;&#20837;&#26679;&#26412;&#20043;&#38388;&#20849;&#20139;&#30340;&#31163;&#25955;&#24605;&#24819;&#12290;&#20511;&#21161;&#29616;&#26377;&#30340;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#23398;&#20064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CoBalT&#65306;&#19968;&#31181;&#27010;&#24565;&#24179;&#34913;&#25216;&#26415;&#65292;&#26377;&#25928;&#20943;&#36731;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#32780;&#26080;&#38656;&#20154;&#31867;&#23545;&#23376;&#32452;&#36827;&#34892;&#26631;&#35760;&#12290;&#22312;&#27700;&#40479;&#12289;CelebA&#21644;ImageNet-9&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#38024;&#23545;&#23376;&#32676;&#20307;&#21464;&#21270;&#30340;&#35780;&#20272;&#34920;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13368v1 Announce Type: new  Abstract: Models prone to spurious correlations in training data often produce brittle predictions and introduce unintended biases. Addressing this challenge typically involves methods relying on prior knowledge and group annotation to remove spurious correlations, which may not be readily available in many applications. In this paper, we establish a novel connection between unsupervised object-centric learning and mitigation of spurious correlations. Instead of directly inferring sub-groups with varying correlations with labels, our approach focuses on discovering concepts: discrete ideas that are shared across input samples. Leveraging existing object-centric representation learning, we introduce CoBalT: a concept balancing technique that effectively mitigates spurious correlations without requiring human labeling of subgroups. Evaluation across the Waterbirds, CelebA and ImageNet-9 benchmark datasets for subpopulation shifts demonstrate superio
&lt;/p&gt;</description></item><item><title>FairProof&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#26469;&#20844;&#24320;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#65292;&#21516;&#26102;&#20445;&#25345;&#26426;&#23494;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;ZKPs&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#35748;&#35777;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12572</link><description>&lt;p&gt;
FairProof&#65306;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#23494;&#21644;&#21487;&#35748;&#35777;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
FairProof : Confidential and Certifiable Fairness for Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12572
&lt;/p&gt;
&lt;p&gt;
FairProof&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#26469;&#20844;&#24320;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#31995;&#32479;&#65292;&#21516;&#26102;&#20445;&#25345;&#26426;&#23494;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;ZKPs&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#35748;&#35777;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#31038;&#20250;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#28982;&#32780;&#27861;&#24459;&#21644;&#38544;&#31169;&#38382;&#39064;&#35201;&#27714;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#38656;&#35201;&#20445;&#23494;&#12290;&#22240;&#27492;&#65292;&#28040;&#36153;&#32773;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#23646;&#24615;&#36234;&#26469;&#36234;&#19981;&#20449;&#20219;&#65292;&#28040;&#36153;&#32773;&#36890;&#24120;&#26159;&#27169;&#22411;&#39044;&#27979;&#30340;&#25509;&#25910;&#32773;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FairProof - &#19968;&#31181;&#31995;&#32479;&#65292;&#20351;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#65288;&#19968;&#31181;&#23494;&#30721;&#21407;&#35821;&#65289;&#26469;&#20844;&#24320;&#39564;&#35777;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#26426;&#23494;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#21512;&#20110;ZKPs&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#35748;&#35777;&#31639;&#27861;&#65292;&#24182;&#22312;&#35813;&#31995;&#32479;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;Gnark&#20013;&#23454;&#29616;&#20102;FairProof&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#26159;&#23454;&#38469;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12572v1 Announce Type: cross  Abstract: Machine learning models are increasingly used in societal applications, yet legal and privacy concerns demand that they very often be kept confidential. Consequently, there is a growing distrust about the fairness properties of these models in the minds of consumers, who are often at the receiving end of model predictions. To this end, we propose FairProof - a system that uses Zero-Knowledge Proofs (a cryptographic primitive) to publicly verify the fairness of a model, while maintaining confidentiality. We also propose a fairness certification algorithm for fully-connected neural networks which is befitting to ZKPs and is used in this system. We implement FairProof in Gnark and demonstrate empirically that our system is practically feasible.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23616;&#37096;&#34394;&#21457;&#29616;&#29575;&#25511;&#21046;&#30340;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;$O(\sqrt{T})$&#30340;&#21518;&#24724;&#29575;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;&#21518;&#24724;&#29575;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#25913;&#36827;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.11425</link><description>&lt;p&gt;
&#22312;&#32447;&#23616;&#37096;&#34394;&#21457;&#29616;&#29575;&#25511;&#21046;&#65306;&#19968;&#31181;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Local False Discovery Rate Control: A Resource Allocation Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11425
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23616;&#37096;&#34394;&#21457;&#29616;&#29575;&#25511;&#21046;&#30340;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;$O(\sqrt{T})$&#30340;&#21518;&#24724;&#29575;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;&#21518;&#24724;&#29575;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#25913;&#36827;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#23616;&#37096;&#34394;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#20010;&#27979;&#35797;&#34987;&#39034;&#24207;&#36827;&#34892;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#24635;&#26399;&#26395;&#30340;&#21457;&#29616;&#27425;&#25968;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#28041;&#21450;&#25509;&#21463;/&#25298;&#32477;&#20915;&#31574;&#65292;&#20174;&#39640;&#23618;&#27425;&#26469;&#30475;&#65292;&#36825;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#24102;&#26377;&#39069;&#22806;&#19981;&#30830;&#23450;&#24615;&#30340;&#22312;&#32447;&#32972;&#21253;&#38382;&#39064;&#65292;&#21363;&#38543;&#26426;&#39044;&#31639;&#34917;&#20805;&#12290;&#25105;&#20204;&#20174;&#19968;&#33324;&#30340;&#21040;&#36798;&#20998;&#24067;&#24320;&#22987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;$O(\sqrt{T})$&#30340;&#21518;&#24724;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#36825;&#31181;&#21518;&#24724;&#29575;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#25913;&#36827;&#30340;&#26469;&#34917;&#20805;&#36825;&#19968;&#32467;&#26524;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#28966;&#28857;&#36716;&#21521;&#31163;&#25955;&#21040;&#36798;&#20998;&#24067;&#12290;&#25105;&#20204;&#21457;&#29616;&#35768;&#22810;&#29616;&#26377;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#25991;&#29486;&#20013;&#30340;&#37325;&#26032;&#35299;&#20915;&#21551;&#21457;&#24335;&#34429;&#28982;&#22312;&#20856;&#22411;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#26377;&#30028;&#30340;&#25439;&#22833;&#65292;&#20294;&#21487;&#33021;&#20250;&#36896;&#25104;$\Omega(\sqrt{T})$&#29978;&#33267;$\Omega(T)$&#30340;&#21518;&#24724;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;&#20856;&#22411;&#31574;&#30053;&#24448;&#24448;&#22826;&#36807;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11425v1 Announce Type: cross  Abstract: We consider the problem of online local false discovery rate (FDR) control where multiple tests are conducted sequentially, with the goal of maximizing the total expected number of discoveries. We formulate the problem as an online resource allocation problem with accept/reject decisions, which from a high level can be viewed as an online knapsack problem, with the additional uncertainty of random budget replenishment. We start with general arrival distributions and propose a simple policy that achieves a $O(\sqrt{T})$ regret. We complement the result by showing that such regret rate is in general not improvable. We then shift our focus to discrete arrival distributions. We find that many existing re-solving heuristics in the online resource allocation literature, albeit achieve bounded loss in canonical settings, may incur a $\Omega(\sqrt{T})$ or even a $\Omega(T)$ regret. With the observation that canonical policies tend to be too op
&lt;/p&gt;</description></item><item><title>&#20998;&#23618;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;HiSS&#65289;&#26159;&#19968;&#31181;&#38024;&#23545;&#36830;&#32493;&#24207;&#21015;&#21040;&#24207;&#21015;&#24314;&#27169;&#30340;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#22534;&#21472;&#30340;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.10211</link><description>&lt;p&gt;
&#38024;&#23545;&#36830;&#32493;&#24207;&#21015;&#21040;&#24207;&#21015;&#24314;&#27169;&#30340;&#20998;&#23618;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10211
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;HiSS&#65289;&#26159;&#19968;&#31181;&#38024;&#23545;&#36830;&#32493;&#24207;&#21015;&#21040;&#24207;&#21015;&#24314;&#27169;&#30340;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#22534;&#21472;&#30340;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10211v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#20174;&#21407;&#22987;&#24863;&#30693;&#25968;&#25454;&#30340;&#24207;&#21015;&#25512;&#29702;&#26159;&#20174;&#21307;&#30103;&#35774;&#22791;&#21040;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#24120;&#24120;&#28041;&#21450;&#20351;&#29992;&#38271;&#24207;&#21015;&#30340;&#21407;&#22987;&#20256;&#24863;&#22120;&#25968;&#25454;&#65288;&#20363;&#22914;&#30913;&#21147;&#35745;&#65292;&#21387;&#38459;&#22120;&#65289;&#26469;&#39044;&#27979;&#29702;&#24819;&#30340;&#29289;&#29702;&#37327;&#24207;&#21015;&#65288;&#20363;&#22914;&#21147;&#37327;&#65292;&#24815;&#24615;&#27979;&#37327;&#65289;&#12290;&#34429;&#28982;&#32463;&#20856;&#26041;&#27861;&#23545;&#20110;&#23616;&#37096;&#32447;&#24615;&#39044;&#27979;&#38382;&#39064;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#22312;&#20351;&#29992;&#23454;&#38469;&#20256;&#24863;&#22120;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20123;&#20256;&#24863;&#22120;&#36890;&#24120;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#21463;&#21040;&#22806;&#30028;&#21464;&#37327;&#65288;&#20363;&#22914;&#25391;&#21160;&#65289;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#34920;&#29616;&#20986;&#25968;&#25454;&#30456;&#20851;&#28418;&#31227;&#12290;&#23545;&#20110;&#35768;&#22810;&#38382;&#39064;&#26469;&#35828;&#65292;&#39044;&#27979;&#20219;&#21153;&#21463;&#21040;&#31232;&#32570;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#33719;&#21462;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#38656;&#35201;&#26114;&#36149;&#30340;&#35774;&#22791;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#23618;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;HiSS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27010;&#24565;&#19978;&#31616;&#21333;&#12289;&#20840;&#26032;&#30340;&#36830;&#32493;&#39034;&#24207;&#39044;&#27979;&#25216;&#26415;&#12290;HiSS&#23558;&#32467;&#26500;&#21270;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22534;&#21472;&#22312;&#19968;&#36215;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#26242;&#23450;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10211v1 Announce Type: new  Abstract: Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a tempor
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#30340;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#25968;&#25454;&#28304;&#20043;&#38388;&#36136;&#37327;&#21644;&#20840;&#38754;&#24615;&#24046;&#24322;&#32473;&#31995;&#32479;&#20248;&#21270;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04146</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#36890;&#36807;&#28508;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04146
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#30340;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#25968;&#25454;&#28304;&#20043;&#38388;&#36136;&#37327;&#21644;&#20840;&#38754;&#24615;&#24046;&#24322;&#32473;&#31995;&#32479;&#20248;&#21270;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20986;&#29616;&#65292;&#21508;&#20010;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#24050;&#32463;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26367;&#20195;&#27169;&#22411;&#26469;&#24314;&#27169;&#26469;&#33258;&#22823;&#37327;&#20449;&#24687;&#28304;&#65288;&#25968;&#25454;&#65289;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#36825;&#31181;&#22686;&#21152;&#23548;&#33268;&#20102;&#24320;&#21457;&#20986;&#29992;&#20110;&#25191;&#34892;&#29305;&#23450;&#21151;&#33021;&#30340;&#20248;&#36234;&#31995;&#32479;&#25152;&#38656;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#30340;&#26174;&#33879;&#38477;&#20302;&#12290;&#36825;&#26679;&#30340;&#26367;&#20195;&#27169;&#22411;&#24448;&#24448;&#24191;&#27867;&#22320;&#34701;&#21512;&#22810;&#20010;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#33021;&#26159;&#21457;&#34920;&#30340;&#35770;&#25991;&#12289;&#19987;&#21033;&#12289;&#24320;&#25918;&#36164;&#28304;&#24211;&#25110;&#20854;&#20182;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#20449;&#24687;&#26469;&#28304;&#30340;&#22522;&#30784;&#29289;&#29702;&#21442;&#25968;&#30340;&#36136;&#37327;&#21644;&#20840;&#38754;&#24615;&#30340;&#24046;&#24322;&#65292;&#21487;&#33021;&#23545;&#31995;&#32479;&#20248;&#21270;&#36807;&#31243;&#20135;&#29983;&#21518;&#32493;&#24433;&#21709;&#65292;&#21364;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#65288;LVGP&#65289;&#30340;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of artificial intelligence (AI) and machine learning (ML), various domains of science and engineering communites has leveraged data-driven surrogates to model complex systems from numerous sources of information (data). The proliferation has led to significant reduction in cost and time involved in development of superior systems designed to perform specific functionalities. A high proposition of such surrogates are built extensively fusing multiple sources of data, may it be published papers, patents, open repositories, or other resources. However, not much attention has been paid to the differences in quality and comprehensiveness of the known and unknown underlying physical parameters of the information sources that could have downstream implications during system optimization. Towards resolving this issue, a multi-source data fusion framework based on Latent Variable Gaussian Process (LVGP) is proposed. The individual data sources are tagged as a characteristic cate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36328;&#39046;&#22495;&#25968;&#25454;&#24046;&#24322;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03094</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#36890;&#36807;&#22686;&#24378;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36328;&#39046;&#22495;&#25968;&#25454;&#24046;&#24322;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#65288;CD-FSOD&#65289;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#20934;&#30830;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#29992;&#26368;&#23569;&#30340;&#26631;&#35760;&#26679;&#26412;&#26469;&#26816;&#27979;&#26032;&#39046;&#22495;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#24320;&#38598;&#26816;&#27979;&#22120;&#65288;&#20363;&#22914;DE-ViT&#65289;&#22312;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#21644;&#20256;&#32479;&#30340;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#31867;&#21035;&#65292;&#25105;&#20204;&#33258;&#28982;&#20250;&#25552;&#20986;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;1&#65289;&#36825;&#31181;&#24320;&#38598;&#26816;&#27979;&#26041;&#27861;&#33021;&#21542;&#23481;&#26131;&#22320;&#25512;&#24191;&#21040;CD-FSOD&#65311;2&#65289;&#22914;&#26524;&#19981;&#33021;&#65292;&#22914;&#20309;&#22312;&#38754;&#23545;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#24322;&#26102;&#22686;&#24378;&#24320;&#38598;&#26041;&#27861;&#30340;&#32467;&#26524;&#65311;&#20026;&#20102;&#22238;&#31572;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20010;&#34913;&#37327;&#39046;&#22495;&#24046;&#24322;&#30340;&#25351;&#26631;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;&#22810;&#26679;&#39046;&#22495;&#24230;&#37327;&#20540;&#30340;&#26032;&#30340;CD-FSOD&#22522;&#20934;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#22495;&#22806;&#25968;&#25454;&#38598;&#20013;&#35266;&#23519;&#21040;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#34920;&#26126;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;CD-FSOD&#19978;&#22833;&#36133;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenge of cross-domain few-shot object detection (CD-FSOD), aiming to develop an accurate object detector for novel domains with minimal labeled examples. While transformer-based open-set detectors e.g., DE-ViT~\cite{zhang2023detect} have excelled in both open-vocabulary object detection and traditional few-shot object detection, detecting categories beyond those seen during training, we thus naturally raise two key questions: 1) can such open-set detection methods easily generalize to CD-FSOD? 2) If no, how to enhance the results of open-set methods when faced with significant domain gaps? To address the first question, we introduce several metrics to quantify domain variances and establish a new CD-FSOD benchmark with diverse domain metric values. Some State-Of-The-Art (SOTA) open-set object detection methods are evaluated on this benchmark, with evident performance degradation observed across out-of-domain datasets. This indicates the failure of adopting 
&lt;/p&gt;</description></item><item><title>LHRS-Bot &#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;(VGI)&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#36817;&#26399;MLLM&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#26410;&#23545;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#36827;&#34892;&#20805;&#20998;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02544</link><description>&lt;p&gt;
LHRS-Bot&#65306;&#21033;&#29992;VGI&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#36965;&#24863;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02544
&lt;/p&gt;
&lt;p&gt;
LHRS-Bot &#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;(VGI)&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#36817;&#26399;MLLM&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#26410;&#23545;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#36827;&#34892;&#20805;&#20998;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38761;&#21629;&#24615;&#33021;&#21147;&#24320;&#21019;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24182;&#20419;&#36827;&#20102;&#22312;&#21508;&#20010;&#19987;&#19994;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#36965;&#24863;&#65288;RS&#65289;&#39046;&#22495;&#20013;&#65292;&#36817;&#26399;&#30340;MLLM&#21162;&#21147;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#21040;&#36965;&#24863;&#22270;&#20687;&#20013;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;RS&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;LHRS-Align&#65292;&#20197;&#21450;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;RS&#29305;&#23450;&#25351;&#23548;&#25968;&#25454;&#38598;LHRS-Instruct&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;&#65288;VGI&#65289;&#21644;&#20840;&#29699;&#21487;&#29992;&#30340;RS&#22270;&#20687;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LHRS-Bot&#65292;&#19968;&#31181;&#38024;&#23545;RS&#22270;&#20687;&#29702;&#35299;&#30340;MLLM&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22312;&#32447;&#24179;&#21488;&#20013;&#19982;&#24178;&#25200;&#36827;&#34892;&#30340;&#23454;&#39564;&#12290;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#65292;&#23398;&#20064;&#32773;&#20998;&#37197;&#19981;&#21516;&#30340;&#33218;&#32473;&#27599;&#20010;&#23454;&#39564;&#21333;&#20803;&#65292;&#26681;&#25454;&#21333;&#20803;&#20043;&#38388;&#30340;&#31354;&#38388;&#36317;&#31163;&#21644;&#23545;&#25163;&#36873;&#25321;&#30340;&#21305;&#37197;&#20989;&#25968;&#26469;&#20915;&#23450;&#27599;&#20010;&#21333;&#20803;&#22312;&#27599;&#36718;&#30340;&#22238;&#25253;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36716;&#25442;&#25919;&#31574;&#33021;&#22815;&#23454;&#29616;&#26368;&#20339;&#30340;&#39044;&#26399;&#36951;&#25022;&#65292;&#20294;&#20219;&#20309;&#36716;&#25442;&#25919;&#31574;&#37117;&#20250;&#36973;&#21463;&#19968;&#23450;&#30340;&#36951;&#25022;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.01845</link><description>&lt;p&gt;
&#20855;&#26377;&#24178;&#25200;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Multi-Armed Bandits with Interference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01845
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22312;&#32447;&#24179;&#21488;&#20013;&#19982;&#24178;&#25200;&#36827;&#34892;&#30340;&#23454;&#39564;&#12290;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#65292;&#23398;&#20064;&#32773;&#20998;&#37197;&#19981;&#21516;&#30340;&#33218;&#32473;&#27599;&#20010;&#23454;&#39564;&#21333;&#20803;&#65292;&#26681;&#25454;&#21333;&#20803;&#20043;&#38388;&#30340;&#31354;&#38388;&#36317;&#31163;&#21644;&#23545;&#25163;&#36873;&#25321;&#30340;&#21305;&#37197;&#20989;&#25968;&#26469;&#20915;&#23450;&#27599;&#20010;&#21333;&#20803;&#22312;&#27599;&#36718;&#30340;&#22238;&#25253;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36716;&#25442;&#25919;&#31574;&#33021;&#22815;&#23454;&#29616;&#26368;&#20339;&#30340;&#39044;&#26399;&#36951;&#25022;&#65292;&#20294;&#20219;&#20309;&#36716;&#25442;&#25919;&#31574;&#37117;&#20250;&#36973;&#21463;&#19968;&#23450;&#30340;&#36951;&#25022;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#22312;&#32447;&#24179;&#21488;&#19978;&#65292;&#19982;&#24178;&#25200;&#36827;&#34892;&#23454;&#39564;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20197;&#24448;&#26377;&#20851;&#24178;&#25200;&#23454;&#39564;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#25919;&#31574;&#30340;&#26368;&#32456;&#36755;&#20986;&#19978;&#65292;&#32780;&#23545;&#20110;&#32047;&#35745;&#24615;&#33021;&#21017;&#20102;&#35299;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#20855;&#26377;&#24178;&#25200;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#8221;&#65288;MABI&#65289;&#38382;&#39064;&#65292;&#22312;&#26102;&#38388;&#27573;&#20026;T&#36718;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#32773;&#20026;N&#20010;&#23454;&#39564;&#21333;&#20803;&#20013;&#30340;&#27599;&#20010;&#20998;&#37197;&#19968;&#20010;&#33218;&#12290;&#27599;&#20010;&#21333;&#20803;&#22312;&#27599;&#19968;&#36718;&#30340;&#22238;&#25253;&#21462;&#20915;&#20110;&#8220;&#25152;&#26377;&#8221;&#21333;&#20803;&#30340;&#27835;&#30103;&#26041;&#24335;&#65292;&#32780;&#21333;&#20803;&#20043;&#38388;&#30340;&#31354;&#38388;&#36317;&#31163;&#20250;&#23548;&#33268;&#21333;&#20803;&#30340;&#24433;&#21709;&#21147;&#36880;&#28176;&#34928;&#20943;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#36890;&#29992;&#35774;&#32622;&#65292;&#20854;&#20013;&#22238;&#25253;&#20989;&#25968;&#30001;&#23545;&#25163;&#36873;&#25321;&#65292;&#24182;&#19988;&#22312;&#36718;&#27425;&#21644;&#21333;&#20803;&#20043;&#38388;&#21487;&#20197;&#20219;&#24847;&#21464;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#36716;&#25442;&#25919;&#31574;&#33021;&#22815;&#23545;&#26368;&#20339;&#22266;&#23450;&#33218;&#25919;&#31574;&#23454;&#29616;&#26368;&#20248;&#30340;&#8220;&#39044;&#26399;&#8221;&#36951;&#25022;&#65292;&#36951;&#25022;&#20540;&#20026;$O(\sqrt T)$&#12290;&#28982;&#32780;&#65292;&#20219;&#20309;&#19968;&#20010;&#36716;&#25442;&#25919;&#31574;&#30340;&#36951;&#25022;&#65288;&#20316;&#20026;&#19968;&#20010;&#38543;&#26426;&#21464;&#37327;&#65289;&#37117;&#20250;&#36973;&#21463;&#19968;&#23450;&#30340;&#36951;&#25022;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimentation with interference poses a significant challenge in contemporary online platforms. Prior research on experimentation with interference has concentrated on the final output of a policy. The cumulative performance, while equally crucial, is less well understood. To address this gap, we introduce the problem of {\em Multi-armed Bandits with Interference} (MABI), where the learner assigns an arm to each of $N$ experimental units over a time horizon of $T$ rounds. The reward of each unit in each round depends on the treatments of {\em all} units, where the influence of a unit decays in the spatial distance between units. Furthermore, we employ a general setup wherein the reward functions are chosen by an adversary and may vary arbitrarily across rounds and units. We first show that switchback policies achieve an optimal {\em expected} regret $\tilde O(\sqrt T)$ against the best fixed-arm policy. Nonetheless, the regret (as a random variable) for any switchback policy suffers 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#24212;&#29992;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#26377;&#25928;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2401.16650</link><description>&lt;p&gt;
&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22238;&#25918;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Augmenting Replay in World Models for Continual Reinforcement Learning. (arXiv:2401.16650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#24212;&#29992;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#26377;&#25928;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#29615;&#22659;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#25104;&#21151;&#30340;&#31995;&#32479;&#24212;&#35813;&#36866;&#24403;&#24179;&#34913;&#20445;&#25345;&#24050;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#20195;&#29702;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#35201;&#27714;&#12290;&#39318;&#36827;&#20808;&#20986;&#32531;&#20914;&#21306;&#36890;&#24120;&#29992;&#20110;&#22686;&#24378;&#27492;&#31867;&#35774;&#32622;&#20013;&#30340;&#23398;&#20064;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#20869;&#23384;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#22686;&#24378;&#26041;&#27861;&#24212;&#29992;&#20110;&#27492;&#32531;&#20914;&#21306;&#20013;&#65292;&#20197;&#32531;&#35299;&#20869;&#23384;&#38480;&#21046;&#65292;&#24182;&#19982;&#22522;&#20110;&#19990;&#30028;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#65292;&#35780;&#20272;&#20854;&#22312;&#20419;&#36827;&#36830;&#32493;&#23398;&#20064;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;Procgen&#21644;Atari&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#28508;&#22312;&#19990;&#30028;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#30340;&#20998;&#24067;&#21305;&#37197;&#22686;&#24378;&#21487;&#20197;&#25104;&#21151;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#24182;&#38750;&#23436;&#20840;&#26080;&#25032;&#21487;&#20987;&#65292;
&lt;/p&gt;
&lt;p&gt;
In continual RL, the environment of a reinforcement learning (RL) agent undergoes change. A successful system should appropriately balance the conflicting requirements of retaining agent performance on already learned tasks, stability, whilst learning new tasks, plasticity. The first-in-first-out buffer is commonly used to enhance learning in such settings but requires significant memory. We explore the application of an augmentation to this buffer which alleviates the memory constraints, and use it with a world model model-based reinforcement learning algorithm, to evaluate its effectiveness in facilitating continual learning. We evaluate the effectiveness of our method in Procgen and Atari RL benchmarks and show that the distribution matching augmentation to the replay-buffer used in the context of latent world models can successfully prevent catastrophic forgetting with significantly reduced computational overhead. Yet, we also find such a solution to not be entirely infallible, and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#28014;&#28857;&#36816;&#31639;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#20108;&#36827;&#21046;&#38408;&#20540;&#21333;&#20803;&#25110;ReLU&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#35760;&#24518;&#20219;&#20309;&#23454;&#25968;&#36755;&#20837;/&#36755;&#20986;&#23545;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23567;&#35823;&#24046;&#20869;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.15121</link><description>&lt;p&gt;
ReLU&#21644;Step&#32593;&#32476;&#22312;&#28014;&#28857;&#36816;&#31639;&#19979;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Expressive Power of ReLU and Step Networks under Floating-Point Operations. (arXiv:2401.15121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15121
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#28014;&#28857;&#36816;&#31639;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#20108;&#36827;&#21046;&#38408;&#20540;&#21333;&#20803;&#25110;ReLU&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#35760;&#24518;&#20219;&#20309;&#23454;&#25968;&#36755;&#20837;/&#36755;&#20986;&#23545;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23567;&#35823;&#24046;&#20869;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32467;&#26524;&#20551;&#35774;&#23454;&#25968;&#36755;&#20837;&#21644;&#21442;&#25968;&#20197;&#21450;&#22312;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#36807;&#31243;&#20013;&#36827;&#34892;&#31934;&#30830;&#36816;&#31639;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#22312;&#21482;&#33021;&#34920;&#31034;&#23454;&#25968;&#30340;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#65292;&#24182;&#19988;&#36827;&#34892;&#19981;&#31934;&#30830;&#30340;&#36816;&#31639;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#26356;&#23454;&#38469;&#30340;&#35774;&#32622;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65306;&#20351;&#29992;&#28014;&#28857;&#25968;&#21644;&#28014;&#28857;&#36816;&#31639;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#32452;&#32467;&#26524;&#20551;&#35774;&#28014;&#28857;&#36816;&#31639;&#20013;&#65292;&#28014;&#28857;&#25968;&#30340;&#26377;&#25928;&#20301;&#25968;&#30001;&#26377;&#38480;&#20301;&#34920;&#31034;&#65292;&#20294;&#20854;&#25351;&#25968;&#21487;&#20197;&#21462;&#20219;&#20309;&#25972;&#25968;&#20540;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#20108;&#36827;&#21046;&#38408;&#20540;&#21333;&#20803;&#25110;ReLU&#21487;&#20197;&#35760;&#24518;&#20219;&#20309;&#26377;&#38480;&#30340;&#36755;&#20837;/&#36755;&#20986;&#23545;&#65292;&#24182;&#21487;&#20197;&#22312;&#23567;&#35823;&#24046;&#20869;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#28014;&#28857;&#36816;&#31639;&#19979;&#20851;&#20110;&#35760;&#24518;&#21644;&#36890;&#29992;&#36924;&#36817;&#30340;&#31867;&#20284;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of the expressive power of neural networks has investigated the fundamental limits of neural networks. Most existing results assume real-valued inputs and parameters as well as exact operations during the evaluation of neural networks. However, neural networks are typically executed on computers that can only represent a tiny subset of the reals and apply inexact operations. In this work, we analyze the expressive power of neural networks under a more realistic setup: when we use floating-point numbers and operations. Our first set of results assumes floating-point operations where the significand of a float is represented by finite bits but its exponent can take any integer value. Under this setup, we show that neural networks using a binary threshold unit or ReLU can memorize any finite input/output pairs and can approximate any continuous function within a small error. We also show similar results on memorization and universal approximation when floating-point operations u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;ConTextual&#65292;&#29992;&#20110;&#35780;&#20272;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#65292;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.13311</link><description>&lt;p&gt;
ConTextual: &#22312;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#35780;&#20272;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models. (arXiv:2401.13311v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;ConTextual&#65292;&#29992;&#20110;&#35780;&#20272;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#65292;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#28041;&#21450;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#22797;&#26434;&#20219;&#21153;&#65292;&#20363;&#22914;&#22312;&#20844;&#20849;&#22330;&#25152;&#23548;&#33322;&#22320;&#22270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ConTextual&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#19987;&#38376;&#35774;&#35745;&#30340;&#25351;&#20196;&#65292;&#29992;&#20110;&#35780;&#20272;LMMs&#22312;&#25191;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;ConTextual&#24378;&#35843;&#20102;&#22810;&#26679;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#65288;&#20363;&#22914;&#26102;&#38388;&#38405;&#35835;&#12289;&#23548;&#33322;&#12289;&#36141;&#29289;&#31561;&#65289;&#65292;&#35201;&#27714;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#20339;&#34920;&#29616;&#30340;LMM&#65292;GPT-4V(ision)&#65292;&#19982;&#20154;&#31867;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;30.8%&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#20351;&#29992;&#20154;&#31867;&#35780;&#20272;&#25351;&#20986;&#22312;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#34429;&#28982;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#65288;&#22914;&#27169;&#22240;&#21644;&#24341;&#25991;&#35299;&#37322;&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#25972;&#20307;&#24615;&#33021;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in AI have led to the development of large multimodal models (LMMs) capable of processing complex tasks involving joint reasoning over text and visual content in the image (e.g., navigating maps in public places). This paper introduces ConTextual, a novel benchmark comprising instructions designed explicitly to evaluate LMMs' ability to perform context-sensitive text-rich visual reasoning. ConTextual emphasizes diverse real-world scenarios (e.g., time-reading, navigation, shopping and more) demanding a deeper understanding of the interactions between textual and visual elements. Our findings reveal a significant performance gap of 30.8% between the best-performing LMM, GPT-4V(ision), and human capabilities using human evaluation indicating substantial room for improvement in context-sensitive text-rich visual reasoning. Notably, while GPT-4V excelled in abstract categories like meme and quote interpretation, its overall performance still lagged behind humans. In add
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#28508;&#22312;&#25193;&#25955;&#26041;&#27861;&#65292;&#29992;&#20110;&#20840;&#26223;&#20998;&#21106;&#21644;&#36974;&#32617;&#20462;&#22797;&#65292;&#36890;&#36807;&#31616;&#21270;&#26550;&#26500;&#26469;&#36991;&#20813;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#27169;&#22411;&#35299;&#38145;&#36974;&#32617;&#20462;&#22797;&#21151;&#33021;&#65292;&#20855;&#26377;&#24212;&#29992;&#20110;&#20132;&#20114;&#24335;&#20998;&#21106;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10227</link><description>&lt;p&gt;
&#19968;&#20010;&#31616;&#21333;&#30340;&#28508;&#22312;&#25193;&#25955;&#26041;&#27861;&#24212;&#29992;&#20110;&#20840;&#26223;&#20998;&#21106;&#21644;&#36974;&#32617;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting. (arXiv:2401.10227v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10227
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#28508;&#22312;&#25193;&#25955;&#26041;&#27861;&#65292;&#29992;&#20110;&#20840;&#26223;&#20998;&#21106;&#21644;&#36974;&#32617;&#20462;&#22797;&#65292;&#36890;&#36807;&#31616;&#21270;&#26550;&#26500;&#26469;&#36991;&#20813;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#27169;&#22411;&#35299;&#38145;&#36974;&#32617;&#20462;&#22797;&#21151;&#33021;&#65292;&#20855;&#26377;&#24212;&#29992;&#20110;&#20132;&#20114;&#24335;&#20998;&#21106;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26223;&#21644;&#23454;&#20363;&#20998;&#21106;&#32593;&#32476;&#36890;&#24120;&#36890;&#36807;&#19987;&#38376;&#30340;&#30446;&#26631;&#26816;&#27979;&#27169;&#22359;&#65292;&#22797;&#26434;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#29305;&#27530;&#30340;&#21518;&#22788;&#29702;&#27493;&#39588;&#26469;&#35757;&#32451;&#65292;&#20197;&#22788;&#29702;&#23454;&#20363;&#36974;&#32617;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Panoptic and instance segmentation networks are often trained with specialized object detection modules, complex loss functions, and ad-hoc post-processing steps to handle the permutation-invariance of the instance masks. This work builds upon Stable Diffusion and proposes a latent diffusion approach for panoptic segmentation, resulting in a simple architecture which omits these complexities. Our training process consists of two steps: (1) training a shallow autoencoder to project the segmentation masks to latent space; (2) training a diffusion model to allow image-conditioned sampling in latent space. The use of a generative model unlocks the exploration of mask completion or inpainting, which has applications in interactive segmentation. The experimental validation yields promising results for both panoptic segmentation and mask inpainting. While not setting a new state-of-the-art, our model's simplicity, generality, and mask completion capability are desirable properties.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#35748;&#30693;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#20027;&#35201;&#20851;&#27880;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;(AD)&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#36890;&#36807;&#20998;&#26512;&#20070;&#20889;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26816;&#27979;&#20986;&#32454;&#24494;&#30340;&#21464;&#21270;&#65292;&#36825;&#23545;&#20110;&#26089;&#26399;AD&#30340;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.06697</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#35748;&#30693;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65306;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning in the Cognitive Domain: Alzheimer's Disease Study. (arXiv:2401.06697v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#35748;&#30693;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#20027;&#35201;&#20851;&#27880;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;(AD)&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#36890;&#36807;&#20998;&#26512;&#20070;&#20889;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26816;&#27979;&#20986;&#32454;&#24494;&#30340;&#21464;&#21270;&#65292;&#36825;&#23545;&#20110;&#26089;&#26399;AD&#30340;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;(AD)&#26159;&#26368;&#24120;&#35265;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#33041;&#30142;&#30149;&#65292;&#23548;&#33268;&#32769;&#24180;&#20154;&#20013;&#26174;&#33879;&#30340;&#35748;&#30693;&#21151;&#33021;&#25439;&#23475;&#12290;&#35748;&#30693;&#21151;&#33021;&#25439;&#23475;&#21487;&#20197;&#34920;&#29616;&#20026;&#21508;&#31181;&#24515;&#29702;&#33021;&#21147;&#30340;&#19979;&#38477;&#65292;&#22914;&#27880;&#24847;&#21147;&#12289;&#35760;&#24518;&#21644;&#20854;&#20182;&#39640;&#32423;&#35748;&#30693;&#33021;&#21147;&#12290;&#36825;&#20123;&#32570;&#38519;&#20250;&#20005;&#37325;&#24433;&#21709;&#20010;&#20307;&#29702;&#35299;&#20449;&#24687;&#12289;&#33719;&#21462;&#26032;&#30693;&#35782;&#21644;&#26377;&#25928;&#27807;&#36890;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#21463;&#21040;&#35748;&#30693;&#21151;&#33021;&#25439;&#23475;&#24433;&#21709;&#30340;&#27963;&#21160;&#20043;&#19968;&#26159;&#20070;&#20889;&#12290;&#36890;&#36807;&#20998;&#26512;&#20070;&#20889;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#21253;&#25324;&#21387;&#21147;&#12289;&#36895;&#24230;&#21644;&#31354;&#38388;&#32452;&#32455;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26816;&#27979;&#21040;&#32454;&#24494;&#30340;&#21464;&#21270;&#65292;&#21487;&#33021;&#34920;&#26126;&#26089;&#26399;&#30340;&#35748;&#30693;&#21151;&#33021;&#25439;&#23475;&#65292;&#29305;&#21035;&#26159;AD&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#32463;&#20856;&#30340;&#20154;&#24037;&#26234;&#33021;(AI)&#26041;&#27861;&#65292;&#36890;&#36807;&#20070;&#20889;&#20998;&#26512;&#26469;&#26816;&#27979;&#32769;&#24180;&#20154;&#20013;&#30340;AD&#12290;&#28982;&#32780;&#65292;&#20808;&#36827;&#30340;AI&#26041;&#27861;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#22240;&#20026;&#25968;&#25454;&#30340;&#35268;&#27169;&#22686;&#21152;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease (AD) is the most prevalent neurodegenerative brain disorder, which results in significant cognitive impairments, especially in the elderly population. Cognitive impairments can manifest as a decline in various mental faculties, such as concentration, memory, and other higher-order cognitive abilities. These deficits can significantly impact an individual's capacity to comprehend information, acquire new knowledge, and communicate effectively. One of the affected activities due to cognitive impairments is handwriting. By analyzing different aspects of handwriting, including pressure, velocity, and spatial organization, researchers can detect subtle alterations that might indicate early-stage cognitive impairments, especially AD. Recently, several classical artificial intelligence (AI) approaches have been proposed for detecting AD in elderly individuals through handwriting analysis. However, advanced AI methods require more computational power as the size of the data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#22238;&#24402;&#26041;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;&#65292;&#23545;&#26680;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#21644;&#20854;&#20182;&#20998;&#26512;&#35889;&#31639;&#27861;&#22312;&#26680;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#36827;&#34892;&#20102;&#20840;&#38754;&#29305;&#24449;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#35757;&#32451;&#23485;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#36129;&#29486;-&#20998;&#26512;&#21151;&#33021;&#35770;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.01599</link><description>&lt;p&gt;
&#20998;&#26512;&#35889;&#31639;&#27861;&#22312;&#24130;&#24459;&#34928;&#20943;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Generalization Error Curves for Analytic Spectral Algorithms under Power-law Decay. (arXiv:2401.01599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#22238;&#24402;&#26041;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;&#65292;&#23545;&#26680;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#21644;&#20854;&#20182;&#20998;&#26512;&#35889;&#31639;&#27861;&#22312;&#26680;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#36827;&#34892;&#20102;&#20840;&#38754;&#29305;&#24449;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#35757;&#32451;&#23485;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#36129;&#29486;-&#20998;&#26512;&#21151;&#33021;&#35770;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26576;&#20123;&#26680;&#22238;&#24402;&#26041;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;&#26088;&#22312;&#30830;&#23450;&#22312;&#19981;&#21516;&#28304;&#26465;&#20214;&#12289;&#22122;&#22768;&#27700;&#24179;&#21644;&#27491;&#21017;&#21270;&#21442;&#25968;&#36873;&#25321;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#30830;&#20999;&#39034;&#24207;&#65292;&#32780;&#19981;&#26159;&#26368;&#23567;&#21270;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#20005;&#26684;&#32473;&#20986;&#20102;&#26680;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65288;&#20197;&#21450;&#22823;&#31867;&#20998;&#26512;&#35889;&#31639;&#27861;&#65289;&#22312;&#26680;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;&#30340;&#23436;&#25972;&#29305;&#24449;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#26680;&#25554;&#20540;&#30340;&#36817;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#28548;&#28165;&#20855;&#26377;&#26356;&#39640;&#36164;&#26684;&#30340;&#26680;&#22238;&#24402;&#31639;&#27861;&#30340;&#39281;&#21644;&#25928;&#24212;&#65292;&#31561;&#31561;&#12290;&#30001;&#20110;&#31070;&#32463;&#20999;&#32447;&#26680;&#29702;&#35770;&#30340;&#24110;&#21161;&#65292;&#36825;&#20123;&#32467;&#26524;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25105;&#20204;&#23545;&#35757;&#32451;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#36129;&#29486;&#65292;&#21363;&#20998;&#26512;&#21151;&#33021;&#35770;&#35777;&#65292;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization error curve of certain kernel regression method aims at determining the exact order of generalization error with various source condition, noise level and choice of the regularization parameter rather than the minimax rate. In this work, under mild assumptions, we rigorously provide a full characterization of the generalization error curves of the kernel gradient descent method (and a large class of analytic spectral algorithms) in kernel regression. Consequently, we could sharpen the near inconsistency of kernel interpolation and clarify the saturation effects of kernel regression algorithms with higher qualification, etc. Thanks to the neural tangent kernel theory, these results greatly improve our understanding of the generalization behavior of training the wide neural networks. A novel technical contribution, the analytic functional argument, might be of independent interest.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#37325;&#26032;&#22312;PyTorch&#19978;&#23454;&#29616;&#20102;VMAF&#65292;&#19982;&#26631;&#20934;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;VMAF&#21333;&#20301;&#19978;&#30340;&#24046;&#24322;&#23567;&#20110;$10^{-2}$&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;VMAF&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#35813;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#19981;&#20250;&#23548;&#33268;&#26799;&#24230;&#19981;&#33391;&#12290;</title><link>http://arxiv.org/abs/2310.15578</link><description>&lt;p&gt;
&#22312;PyTorch&#19978;&#37325;&#26032;&#23454;&#29616;&#30340;VMAF&#65306;&#19968;&#20123;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
VMAF Re-implementation on PyTorch: Some Experimental Results. (arXiv:2310.15578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15578
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#37325;&#26032;&#22312;PyTorch&#19978;&#23454;&#29616;&#20102;VMAF&#65292;&#19982;&#26631;&#20934;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;VMAF&#21333;&#20301;&#19978;&#30340;&#24046;&#24322;&#23567;&#20110;$10^{-2}$&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;VMAF&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#35813;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#19981;&#20250;&#23548;&#33268;&#26799;&#24230;&#19981;&#33391;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26631;&#20934;&#30340;VMAF&#23454;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;PyTorch&#26694;&#26550;&#23454;&#29616;VMAF&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#36825;&#20010;&#23454;&#29616;&#65292;&#19982;&#26631;&#20934;&#30340;(libvmaf)&#36827;&#34892;&#27604;&#36739;&#65292;VMAF&#21333;&#20301;&#19978;&#30340;&#24046;&#24322;&#23567;&#20110;$10^{-2}$&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;VMAF&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#35813;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#19981;&#20250;&#23548;&#33268;&#26799;&#24230;&#19981;&#33391;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on the standard VMAF implementation we propose an implementation of VMAF using PyTorch framework. For this implementation comparisons with the standard (libvmaf) show the discrepancy $\lesssim 10^{-2}$ in VMAF units. We investigate gradients computation when using VMAF as an objective function and demonstrate that training using this function does not result in ill-behaving gradients.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24341;&#23548;&#32858;&#31867;&#12290;&#36890;&#36807;&#21033;&#29992;WordNet&#30340;&#25991;&#26412;&#35821;&#20041;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#22270;&#20687;&#32858;&#31867;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.11989</link><description>&lt;p&gt;
&#24102;&#26377;&#22806;&#37096;&#24341;&#23548;&#30340;&#22270;&#20687;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Image Clustering with External Guidance. (arXiv:2310.11989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11989
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24341;&#23548;&#32858;&#31867;&#12290;&#36890;&#36807;&#21033;&#29992;WordNet&#30340;&#25991;&#26412;&#35821;&#20041;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#22270;&#20687;&#32858;&#31867;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#30340;&#26680;&#24515;&#26159;&#34701;&#20837;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#26500;&#24314;&#30417;&#30563;&#20449;&#21495;&#12290;&#20174;&#22522;&#20110;&#25968;&#25454;&#32039;&#23494;&#24615;&#30340;&#32463;&#20856;k&#22343;&#20540;&#21040;&#26368;&#36817;&#30340;&#22522;&#20110;&#33258;&#30417;&#30563;&#24341;&#23548;&#30340;&#23545;&#27604;&#32858;&#31867;&#65292;&#32858;&#31867;&#26041;&#27861;&#30340;&#36827;&#27493;&#19982;&#30417;&#30563;&#20449;&#21495;&#30340;&#21457;&#23637;&#20869;&#22312;&#22320;&#30456;&#23545;&#24212;&#12290;&#30446;&#21069;&#65292;&#24456;&#22810;&#24037;&#20316;&#24050;&#32463;&#33268;&#21147;&#20110;&#20174;&#25968;&#25454;&#20013;&#25366;&#25496;&#20869;&#37096;&#30417;&#30563;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#20016;&#23500;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#20363;&#22914;&#35821;&#20041;&#25551;&#36848;&#65292;&#33258;&#28982;&#22320;&#20419;&#36827;&#20102;&#32858;&#31867;&#65292;&#21364;&#34987;&#36951;&#25022;&#22320;&#24573;&#35270;&#20102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#20316;&#20026;&#26032;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#24341;&#23548;&#32858;&#31867;&#65292;&#21363;&#20351;&#23427;&#20284;&#20046;&#19982;&#32473;&#23450;&#30340;&#25968;&#25454;&#26080;&#20851;&#12290;&#20026;&#20102;&#23454;&#29616;&#21644;&#39564;&#35777;&#25105;&#20204;&#30340;&#24819;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22806;&#37096;&#24341;&#23548;&#30340;&#32858;&#31867;&#26041;&#27861;&#65288;&#25991;&#26412;&#36741;&#21161;&#32858;&#31867;&#65292;TAC&#65289;&#65292;&#23427;&#21033;&#29992;WordNet&#30340;&#25991;&#26412;&#35821;&#20041;&#26469;&#20419;&#36827;&#22270;&#20687;&#32858;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TAC&#39318;&#20808;&#36873;&#25321;&#24182;&#26816;&#32034;&#26368;&#33021;&#21306;&#20998;&#22270;&#20687;&#30340;WordNet&#21517;&#35789;&#20197;&#22686;&#24378;&#32858;&#31867;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The core of clustering is incorporating prior knowledge to construct supervision signals. From classic k-means based on data compactness to recent contrastive clustering guided by self-supervision, the evolution of clustering methods intrinsically corresponds to the progression of supervision signals. At present, substantial efforts have been devoted to mining internal supervision signals from data. Nevertheless, the abundant external knowledge such as semantic descriptions, which naturally conduces to clustering, is regrettably overlooked. In this work, we propose leveraging external knowledge as a new supervision signal to guide clustering, even though it seems irrelevant to the given data. To implement and validate our idea, we design an externally guided clustering method (Text-Aided Clustering, TAC), which leverages the textual semantics of WordNet to facilitate image clustering. Specifically, TAC first selects and retrieves WordNet nouns that best distinguish images to enhance th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25805;&#25511;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#12289;&#27604;&#36739;&#21644;&#36870;&#21521;&#25628;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#36825;&#20123;&#20869;&#22312;&#30340;&#24369;&#28857;&#65306;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#39640;&#25928;&#22320;&#25805;&#25511;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2309.14402</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#29702;&#23398;&#65306;&#31532;3.2&#37096;&#20998;&#65292;&#30693;&#35782;&#25805;&#25511;
&lt;/p&gt;
&lt;p&gt;
Physics of Language Models: Part 3.2, Knowledge Manipulation. (arXiv:2309.14402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25805;&#25511;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#12289;&#27604;&#36739;&#21644;&#36870;&#21521;&#25628;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#36825;&#20123;&#20869;&#22312;&#30340;&#24369;&#28857;&#65306;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#39640;&#25928;&#22320;&#25805;&#25511;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23384;&#20648;&#22823;&#37327;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#22312;&#20351;&#29992;&#36825;&#20123;&#30693;&#35782;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25805;&#25511;&#20854;&#23384;&#20648;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#22235;&#31181;&#25805;&#25511;&#31867;&#22411;&#65306;&#26816;&#32034;&#65288;&#20363;&#22914;&#65292;&#8220;A&#30340;&#23646;&#24615;X&#26159;&#20160;&#20040;&#8221;&#65289;&#12289;&#20998;&#31867;&#65288;&#20363;&#22914;&#65292;&#8220;A&#30340;&#23646;&#24615;X&#26159;&#22855;&#25968;&#36824;&#26159;&#20598;&#25968;&#8221;&#65289;&#12289;&#27604;&#36739;&#65288;&#20363;&#22914;&#65292;&#8220;&#22312;&#23646;&#24615;X&#20013;A&#26159;&#21542;&#22823;&#20110;B&#8221;&#65289;&#21644;&#36870;&#21521;&#25628;&#32034;&#65288;&#20363;&#22914;&#65292;&#8220;&#21738;&#20010;&#20154;&#30340;&#23646;&#24615;X&#31561;&#20110;T&#8221;&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20687;GPT2/3/4&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#20998;&#31867;&#25110;&#27604;&#36739;&#20219;&#21153;&#20013;&#24456;&#38590;&#32988;&#20219;&#65292;&#38500;&#38750;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#37319;&#29992;&#20102;Chain of Thoughts&#65288;CoTs&#65289;&#12290;&#26080;&#35770;&#25552;&#31034;&#26159;&#20160;&#20040;&#65292;&#23427;&#20204;&#22312;&#36870;&#21521;&#30693;&#35782;&#25628;&#32034;&#20013;&#34920;&#29616;&#37117;&#24456;&#24046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#20010;&#20026;&#25511;&#21046;&#23454;&#39564;&#32780;&#35774;&#35745;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35777;&#23454;&#20102;&#36825;&#20123;&#20869;&#22312;&#30340;&#24369;&#28857;&#65306;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#39640;&#25928;&#22320;&#25805;&#25511;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models can store vast amounts of factual knowledge, but their ability to use this knowledge for logical reasoning remains questionable. This paper explores a language model's ability to manipulate its stored knowledge during inference. We focus on four manipulation types: retrieval (e.g., "What is person A's attribute X"), classification (e.g., "Is A's attribute X even or odd?"), comparison (e.g., "Is A greater than B in attribute X?") and inverse search (e.g., "Which person's attribute X equals T?")  We observe that pre-trained language models like GPT2/3/4 excel in knowledge retrieval but struggle with simple classification or comparison tasks unless Chain of Thoughts (CoTs) are employed during both training and inference. They also perform poorly in inverse knowledge search, irrespective of the prompts. Our primary contribution is a synthetic dataset for a controlled experiment that confirms these inherent weaknesses: a language model cannot efficiently manipulate knowledge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22522;&#20110;GPT&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#23558;&#39046;&#22495;&#30693;&#35782;&#24211;&#19982;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#20197;&#25552;&#39640;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#24211;&#21644;&#35780;&#20272;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#23398;&#29983;&#21644;&#39046;&#22495;&#19987;&#23478;&#23545;&#20110;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#22238;&#31572;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2309.12367</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;GPT&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#30740;&#31350;&#39046;&#22495;&#30693;&#35782;&#24211;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Examining the Influence of Varied Levels of Domain Knowledge Base Inclusion in GPT-based Intelligent Tutors. (arXiv:2309.12367v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22522;&#20110;GPT&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#23558;&#39046;&#22495;&#30693;&#35782;&#24211;&#19982;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#20197;&#25552;&#39640;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#24211;&#21644;&#35780;&#20272;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#23398;&#29983;&#21644;&#39046;&#22495;&#19987;&#23478;&#23545;&#20110;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#22238;&#31572;&#36827;&#34892;&#20102;&#39564;&#35777;&#21644;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20419;&#36827;&#20102;&#20855;&#26377;&#22797;&#26434;&#23545;&#35805;&#33021;&#21147;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;LLM&#23545;&#26597;&#35810;&#30340;&#22238;&#31572;&#32463;&#24120;&#19981;&#20934;&#30830;&#65292;&#36825;&#38480;&#21046;&#20102;&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#19982;LLM&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#38598;&#25104;&#20197;&#22686;&#21152;&#22238;&#31572;&#21487;&#38752;&#24615;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#24211;&#65292;&#25945;&#32946;&#30417;&#30563;&#21592;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#35838;&#31243;&#65292;&#35813;&#35838;&#31243;&#20250;&#34987;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#33258;&#21160;&#22788;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#35780;&#20272;&#23454;&#39564;&#65292;&#23398;&#29983;&#21442;&#19982;&#32773;&#38656;&#35201;&#22238;&#31572;&#26377;&#20851;&#20154;&#24037;&#26234;&#33021;&#35838;&#31243;&#30340;&#38382;&#39064;&#12290; GPT-4&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20855;&#26377;&#19981;&#21516;&#23618;&#27425;&#30340;KB&#35775;&#38382;&#26435;&#38480;&#65292;&#24182;&#30001;&#20154;&#31867;&#39046;&#22495;&#19987;&#23478;&#35780;&#20272;&#36825;&#20123;&#22238;&#31572;&#12290;&#26368;&#21518;&#65292;&#23398;&#29983;&#23545;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#22238;&#31572;&#36827;&#34892;&#20102;&#19982;&#39046;&#22495;&#19987;&#23478;&#30340;&#20132;&#21449;&#39564;&#35777;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#21508;&#31181;&#25945;&#23398;&#33021;&#21147;&#36827;&#34892;&#20102;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have facilitated the development of chatbots with sophisticated conversational capabilities. However, LLMs exhibit frequent inaccurate responses to queries, hindering applications in educational settings. In this paper, we investigate the effectiveness of integrating a knowledge base (KB) with LLM intelligent tutors to increase response reliability. To achieve this, we design a scaleable KB that affords educational supervisors seamless integration of lesson curricula, which is automatically processed by the intelligent tutoring system. We then detail an evaluation, where student participants were presented with questions about the artificial intelligence curriculum to respond to. GPT-4 intelligent tutors with varying hierarchies of KB access and human domain experts then assessed these responses. Lastly, students cross-examined the intelligent tutors' responses to the domain experts' and ranked their various pedagogical abilities. Res
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#20248;&#21270;&#20986;&#20215;&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#26041;&#26696;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15193</link><description>&lt;p&gt;
&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning in Repeated Multi-Unit Pay-As-Bid Auctions. (arXiv:2307.15193v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#20248;&#21270;&#20986;&#20215;&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#26041;&#26696;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#30899;&#25490;&#25918;&#20132;&#26131;&#26041;&#26696;&#12289;&#22269;&#20538;&#25293;&#21334;&#21644;&#37319;&#36141;&#25293;&#21334;&#30340;&#21551;&#21457;&#65292;&#36825;&#20123;&#37117;&#28041;&#21450;&#25293;&#21334;&#21516;&#36136;&#30340;&#22810;&#20010;&#21333;&#20301;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22914;&#20309;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#22312;&#27599;&#20010;&#25293;&#21334;&#20013;&#65292;&#22823;&#37327;&#65288;&#30456;&#21516;&#30340;&#65289;&#29289;&#21697;&#23558;&#34987;&#20998;&#37197;&#32473;&#26368;&#39640;&#30340;&#20986;&#20215;&#65292;&#27599;&#20010;&#20013;&#26631;&#20215;&#31561;&#20110;&#20986;&#20215;&#26412;&#36523;&#12290;&#30001;&#20110;&#34892;&#21160;&#31354;&#38388;&#30340;&#32452;&#21512;&#24615;&#36136;&#65292;&#23398;&#20064;&#22914;&#20309;&#22312;&#20184;&#36153;&#25293;&#21334;&#20013;&#20986;&#20215;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20851;&#27880;&#31163;&#32447;&#35774;&#32622;&#65292;&#20854;&#20013;&#25237;&#26631;&#20154;&#36890;&#36807;&#21482;&#33021;&#35775;&#38382;&#20854;&#20182;&#25237;&#26631;&#20154;&#36807;&#21435;&#25552;&#20132;&#30340;&#20986;&#20215;&#26469;&#20248;&#21270;&#20182;&#20204;&#30340;&#20986;&#20215;&#21521;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31163;&#32447;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#21487;&#20197;&#20351;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#26041;&#26696;&#26469;&#33719;&#24471;&#12290;&#25105;&#20204;&#21033;&#29992;DP&#26041;&#26696;&#30340;&#32467;&#26500;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by Carbon Emissions Trading Schemes, Treasury Auctions, and Procurement Auctions, which all involve the auctioning of homogeneous multiple units, we consider the problem of learning how to bid in repeated multi-unit pay-as-bid auctions. In each of these auctions, a large number of (identical) items are to be allocated to the largest submitted bids, where the price of each of the winning bids is equal to the bid itself. The problem of learning how to bid in pay-as-bid auctions is challenging due to the combinatorial nature of the action space. We overcome this challenge by focusing on the offline setting, where the bidder optimizes their vector of bids while only having access to the past submitted bids by other bidders. We show that the optimal solution to the offline problem can be obtained using a polynomial time dynamic programming (DP) scheme. We leverage the structure of the DP scheme to design online learning algorithms with polynomial time and space complexity under fu
&lt;/p&gt;</description></item><item><title>MOCA&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#25513;&#30721;&#24335;&#22312;&#32447;&#30721;&#26412;&#20998;&#37197;&#26469;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;&#12290;&#23427;&#21516;&#26102;&#20855;&#22791;&#33391;&#22909;&#30340;&#35821;&#22659;&#25512;&#29702;&#23646;&#24615;&#21644;&#23545;&#22270;&#20687;&#25200;&#21160;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#20302;&#26679;&#26412;&#35774;&#32622;&#21644;&#21508;&#31181;&#35780;&#20272;&#21327;&#35758;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#35757;&#32451;&#36895;&#24230;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#24555;3&#20493;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2307.09361</link><description>&lt;p&gt;
MOCA: &#33258;&#30417;&#30563;&#23398;&#20064;&#36890;&#36807;&#39044;&#27979;&#25513;&#30721;&#24335;&#22312;&#32447;&#30721;&#26412;&#20998;&#37197;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments. (arXiv:2307.09361v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09361
&lt;/p&gt;
&lt;p&gt;
MOCA&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#25513;&#30721;&#24335;&#22312;&#32447;&#30721;&#26412;&#20998;&#37197;&#26469;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;&#12290;&#23427;&#21516;&#26102;&#20855;&#22791;&#33391;&#22909;&#30340;&#35821;&#22659;&#25512;&#29702;&#23646;&#24615;&#21644;&#23545;&#22270;&#20687;&#25200;&#21160;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#20302;&#26679;&#26412;&#35774;&#32622;&#21644;&#21508;&#31181;&#35780;&#20272;&#21327;&#35758;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#35757;&#32451;&#36895;&#24230;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#24555;3&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#21487;&#20197;&#29992;&#20110;&#32531;&#35299;Vision Transformer&#32593;&#32476;&#23545;&#22823;&#22411;&#20840;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#36138;&#23146;&#38656;&#27714;&#12290;&#19981;&#21516;&#31867;&#21035;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#20855;&#26377;&#33391;&#22909;&#35821;&#22659;&#25512;&#29702;&#23646;&#24615;&#30340;&#34920;&#31034;&#65292;&#20363;&#22914;&#20351;&#29992;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#31574;&#30053;&#65292;&#25110;&#32773;&#23545;&#22270;&#20687;&#25200;&#21160;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#34920;&#31034;&#65292;&#20363;&#22914;&#20351;&#29992;&#23545;&#27604;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#38454;&#27573;&#12289;&#29420;&#31435;&#30340;&#26041;&#27861;MOCA&#65292;&#20351;&#29992;&#22522;&#20110;&#39640;&#32423;&#29305;&#24449;&#65288;&#32780;&#19981;&#26159;&#20687;&#32032;&#32423;&#32454;&#33410;&#65289;&#23450;&#20041;&#30340;&#26032;&#22411;&#25513;&#30721;&#21644;&#39044;&#27979;&#30446;&#26631;&#26469;&#32479;&#19968;&#36825;&#20004;&#31181;&#26399;&#26395;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20197;&#21327;&#21516;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#24335;&#26377;&#25928;&#22320;&#24212;&#29992;&#36825;&#20004;&#31181;&#23398;&#20064;&#33539;&#24335;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#22312;&#20302;&#26679;&#26412;&#35774;&#32622;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#35780;&#20272;&#21327;&#35758;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20854;&#35757;&#32451;&#36895;&#24230;&#33267;&#23569;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#24555;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning can be used for mitigating the greedy needs of Vision Transformer networks for very large fully-annotated datasets. Different classes of self-supervised learning offer representations with either good contextual reasoning properties, e.g., using masked image modeling strategies, or invariance to image perturbations, e.g., with contrastive methods. In this work, we propose a single-stage and standalone method, MOCA, which unifies both desired properties using novel mask-and-predict objectives defined with high-level features (instead of pixel-level details). Moreover, we show how to effectively employ both learning paradigms in a synergistic and computation-efficient way. Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;DP-SGD&#20998;&#26512;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.00310</link><description>&lt;p&gt;
&#26799;&#24230;&#30456;&#20284;&#65306;&#25935;&#24863;&#24230;&#32463;&#24120;&#34987;&#36807;&#39640;&#20272;&#35745;&#22312;DP-SGD&#20013;
&lt;/p&gt;
&lt;p&gt;
Gradients Look Alike: Sensitivity is Often Overestimated in DP-SGD. (arXiv:2307.00310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;DP-SGD&#20998;&#26512;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#26159;&#31169;&#26377;&#28145;&#24230;&#23398;&#20064;&#30340;&#26631;&#20934;&#31639;&#27861;&#12290;&#34429;&#28982;&#24050;&#30693;&#20854;&#38544;&#31169;&#20998;&#26512;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#26159;&#32039;&#23494;&#30340;&#65292;&#20294;&#26159;&#19968;&#20123;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#26174;&#33879;&#20943;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;DP-SGD&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#25429;&#25417;&#21040;&#22312;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#30456;&#20284;&#37051;&#23621;&#30340;&#28857;&#20139;&#21463;&#26356;&#22909;&#38544;&#31169;&#24615;&#30340;&#30452;&#35273;&#12290;&#24418;&#24335;&#19978;&#26469;&#35828;&#65292;&#36825;&#26159;&#36890;&#36807;&#20462;&#25913;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#35745;&#31639;&#24471;&#21040;&#30340;&#27169;&#22411;&#26356;&#26032;&#30340;&#27599;&#27493;&#38544;&#31169;&#24615;&#20998;&#26512;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#32452;&#21512;&#23450;&#29702;&#65292;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20010;&#26032;&#30340;&#27599;&#27493;&#20998;&#26512;&#26469;&#25512;&#29702;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#12290;&#24635;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#39062;&#30340;DP-SGD&#20998;&#26512;&#20351;&#25105;&#20204;&#33021;&#22815;&#27491;&#24335;&#22320;&#26174;&#31034;DP-SGD&#23545;&#35768;&#22810;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#27844;&#28431;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private stochastic gradient descent (DP-SGD) is the canonical algorithm for private deep learning. While it is known that its privacy analysis is tight in the worst-case, several empirical results suggest that when training on common benchmark datasets, the models obtained leak significantly less privacy for many datapoints. In this paper, we develop a new analysis for DP-SGD that captures the intuition that points with similar neighbors in the dataset enjoy better privacy than outliers. Formally, this is done by modifying the per-step privacy analysis of DP-SGD to introduce a dependence on the distribution of model updates computed from a training dataset. We further develop a new composition theorem to effectively use this new per-step analysis to reason about an entire training run. Put all together, our evaluation shows that this novel DP-SGD analysis allows us to now formally show that DP-SGD leaks significantly less privacy for many datapoints. In particular, we ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#20854;&#36827;&#34892;&#25915;&#20987;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24494;&#35843;&#40065;&#26834;&#30340;&#20027;&#24178;&#65292;&#21487;&#20197;&#26377;&#38480;&#30340;&#35745;&#31639;&#20195;&#20215;&#35757;&#32451;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.12941</link><description>&lt;p&gt;
&#40065;&#26834;&#35821;&#20041;&#20998;&#21106;&#65306;&#24378;&#40065;&#26834;&#24615;&#25915;&#20987;&#21644;&#24555;&#36895;&#35757;&#32451;&#40065;&#26834;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust Semantic Segmentation: Strong Adversarial Attacks and Fast Training of Robust Models. (arXiv:2306.12941v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#20854;&#36827;&#34892;&#25915;&#20987;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24494;&#35843;&#40065;&#26834;&#30340;&#20027;&#24178;&#65292;&#21487;&#20197;&#26377;&#38480;&#30340;&#35745;&#31639;&#20195;&#20215;&#35757;&#32451;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#37327;&#30340;&#24037;&#20316;&#24050;&#32463;&#38598;&#20013;&#22312;&#35774;&#35745;&#38024;&#23545;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19978;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#23384;&#22312;&#29992;&#20110;&#25915;&#20987;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25915;&#20987;&#20998;&#21106;&#27169;&#22411;&#30340;&#20219;&#21153;&#29305;&#23450;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#35780;&#20272;&#21327;&#35758;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#39640;&#20272;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#33267;&#20170;&#26368;&#25104;&#21151;&#30340;&#33719;&#24471;&#40065;&#26834;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26080;&#27861;&#25104;&#21151;&#24212;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#22240;&#20026;&#35201;&#23398;&#20064;&#30340;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#27604;&#22270;&#20687;&#20998;&#31867;&#26356;&#39640;&#30340;&#35745;&#31639;&#37327;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#22312;&#40065;&#26834;ImageNet&#20998;&#31867;&#22120;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#40065;&#26834;&#30340;&#20027;&#24178;&#65292;&#20197;&#26377;&#38480;&#30340;&#35745;&#31639;&#20195;&#20215;&#35757;&#32451;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
While a large amount of work has focused on designing adversarial attacks against image classifiers, only a few methods exist to attack semantic segmentation models. We show that attacking segmentation models presents task-specific challenges, for which we propose novel solutions. Our final evaluation protocol outperforms existing methods, and shows that those can overestimate the robustness of the models. Additionally, so far adversarial training, the most successful way for obtaining robust image classifiers, could not be successfully applied to semantic segmentation. We argue that this is because the task to be learned is more challenging, and requires significantly higher computational effort than for image classification. As a remedy, we show that by taking advantage of recent advances in robust ImageNet classifiers, one can train adversarially robust segmentation models at limited computational cost by fine-tuning robust backbones.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#39044;&#27979;&#38598;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#35774;&#35745;&#26041;&#27861;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#21333;&#19968;&#26631;&#31614;&#39044;&#27979;&#65292;&#23427;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#22120;&#26500;&#24314;&#39044;&#27979;&#38598;&#65292;&#24182;&#24341;&#23548;&#20154;&#31867;&#19987;&#23478;&#20174;&#20013;&#36873;&#25321;&#26631;&#31614;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.03928</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#39044;&#27979;&#38598;&#35774;&#35745;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Designing Decision Support Systems Using Counterfactual Prediction Sets. (arXiv:2306.03928v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#39044;&#27979;&#38598;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#35774;&#35745;&#26041;&#27861;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#21333;&#19968;&#26631;&#31614;&#39044;&#27979;&#65292;&#23427;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#22120;&#26500;&#24314;&#39044;&#27979;&#38598;&#65292;&#24182;&#24341;&#23548;&#20154;&#31867;&#19987;&#23478;&#20174;&#20013;&#36873;&#25321;&#26631;&#31614;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#20219;&#21153;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#36890;&#24120;&#34987;&#35774;&#35745;&#29992;&#20110;&#39044;&#27979;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#30340;&#20540;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#39044;&#27979;&#24182;&#19981;&#23436;&#32654;&#65292;&#36825;&#20123;&#31995;&#32479;&#36824;&#38656;&#35201;&#35753;&#20154;&#31867;&#19987;&#23478;&#20102;&#35299;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#39044;&#27979;&#26469;&#26356;&#26032;&#33258;&#24049;&#30340;&#39044;&#27979;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#34987;&#35777;&#26126;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26368;&#36817;&#26377;&#20154;&#35748;&#20026;&#65292;&#21478;&#19968;&#31181;&#31867;&#22411;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#21487;&#33021;&#20250;&#36991;&#24320;&#36825;&#20010;&#25361;&#25112;&#12290;&#36825;&#20123;&#31995;&#32479;&#19981;&#26159;&#25552;&#20379;&#21333;&#20010;&#26631;&#31614;&#39044;&#27979;&#65292;&#32780;&#26159;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#22120;&#26500;&#24314;&#19968;&#32452;&#26631;&#31614;&#39044;&#27979;&#20540;&#65292;&#21363;&#39044;&#27979;&#38598;&#65292;&#24182;&#24378;&#21046;&#35201;&#27714;&#19987;&#23478;&#20174;&#39044;&#27979;&#38598;&#20013;&#39044;&#27979;&#19968;&#20010;&#26631;&#31614;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#35780;&#20272;&#36804;&#20170;&#20173;&#20381;&#36182;&#20110;&#26679;&#24335;&#21270;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#36825;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#25215;&#35834;&#30340;&#36136;&#30097;&#12290;&#26412;&#25991;&#20174;&#22312;&#32447;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#31181;&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision support systems for classification tasks are predominantly designed to predict the value of the ground truth labels. However, since their predictions are not perfect, these systems also need to make human experts understand when and how to use these predictions to update their own predictions. Unfortunately, this has been proven challenging. In this context, it has been recently argued that an alternative type of decision support systems may circumvent this challenge. Rather than providing a single label prediction, these systems provide a set of label prediction values constructed using a conformal predictor, namely a prediction set, and forcefully ask experts to predict a label value from the prediction set. However, the design and evaluation of these systems have so far relied on stylized expert models, questioning their promise. In this paper, we revisit the design of this type of systems from the perspective of online learning and develop a methodology that does not requi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20803;&#32032;&#38500;&#27861;&#30340;&#21487;&#23398;&#20064;&#33293;&#20837;&#26426;&#21046;FlexRound&#65292;&#20351;&#24471;&#21518;&#35757;&#32451;&#37327;&#21270;&#26102;&#26356;&#22909;&#22320;&#37325;&#26500;&#27599;&#20010;&#23618;&#25110;&#22359;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#20844;&#20849;&#37327;&#21270;&#32593;&#26684;&#22823;&#23567;&#20197;&#21450;&#27599;&#20010;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#19981;&#21516;&#27604;&#20363;&#23610;&#12290;</title><link>http://arxiv.org/abs/2306.00317</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#32032;&#38500;&#27861;&#30340;&#21487;&#23398;&#20064;&#33293;&#20837;&#29992;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization. (arXiv:2306.00317v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20803;&#32032;&#38500;&#27861;&#30340;&#21487;&#23398;&#20064;&#33293;&#20837;&#26426;&#21046;FlexRound&#65292;&#20351;&#24471;&#21518;&#35757;&#32451;&#37327;&#21270;&#26102;&#26356;&#22909;&#22320;&#37325;&#26500;&#27599;&#20010;&#23618;&#25110;&#22359;&#30340;&#36755;&#20986;&#65292;&#24182;&#19988;&#33021;&#22815;&#23398;&#20064;&#20844;&#20849;&#37327;&#21270;&#32593;&#26684;&#22823;&#23567;&#20197;&#21450;&#27599;&#20010;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#19981;&#21516;&#27604;&#20363;&#23610;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#24050;&#32463;&#22312;&#36164;&#28304;&#26377;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#19982;&#37327;&#21270;&#24863;&#30693;&#22521;&#35757;&#19981;&#21516;&#65292;&#23436;&#20840;&#19981;&#38656;&#35201;&#20840;&#38754;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25110;&#31471;&#21040;&#31471;&#22521;&#35757;&#12290;&#22240;&#20026;&#22522;&#20110;&#37325;&#26500;&#27599;&#20010;&#23618;&#25110;&#22359;&#36755;&#20986;&#30340;PTQ&#26041;&#26696;&#25928;&#26524;&#26174;&#30528;&#20197;&#22686;&#24378;&#37327;&#21270;&#27169;&#22411;&#24615;&#33021;&#65292;&#25152;&#20197;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#24320;&#21457;&#20102;&#31639;&#27861;&#26469;&#35774;&#35745;&#21644;&#23398;&#20064;&#19968;&#31181;&#26032;&#30340;&#26435;&#37325;&#33293;&#20837;&#26041;&#26696;&#65292;&#20197;&#26356;&#22909;&#22320;&#37325;&#26500;&#27599;&#20010;&#23618;&#25110;&#22359;&#30340;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26032;&#30340;PTQ&#26435;&#37325;&#33293;&#20837;&#26426;&#21046;&#65292;&#21517;&#20026;FlexRound&#65292;&#20854;&#22522;&#20110;&#20803;&#32032;&#38500;&#27861;&#32780;&#19981;&#26159;&#20856;&#22411;&#30340;&#20803;&#32032;&#21152;&#27861;&#65292;&#20174;&#32780;&#20351;FlexRound&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#20844;&#20849;&#37327;&#21270;&#32593;&#26684;&#22823;&#23567;&#20197;&#21450;&#27599;&#20010;&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#19981;&#21516;&#27604;&#20363;&#23610;&#12290;&#30001;&#20110;&#20803;&#32032;&#38500;&#27861;&#20135;&#29983;&#30340;&#23548;&#25968;&#30340;&#20114;&#34917;&#35268;&#21017;&#65292;FlexRound&#22312;&#26356;&#26032;&#20854;&#30456;&#20851;&#39044;&#35757;&#32451;&#26435;&#37325;&#26102;&#22825;&#29983;&#33021;&#22815;&#21033;&#29992;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) has been gaining popularity for the deployment of deep neural networks on resource-limited devices since unlike quantization-aware training, neither a full training dataset nor end-to-end training is required at all. As PTQ schemes based on reconstructing each layer or block output turn out to be effective to enhance quantized model performance, recent works have developed algorithms to devise and learn a new weight-rounding scheme so as to better reconstruct each layer or block output. In this work, we propose a simple yet effective new weight-rounding mechanism for PTQ, coined FlexRound, based on element-wise division instead of typical element-wise addition such that FlexRound enables jointly learning a common quantization grid size as well as a different scale for each pre-trained weight. Thanks to the reciprocal rule of derivatives induced by element-wise division, FlexRound is inherently able to exploit pre-trained weights when updating their corr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#25110;&#32773;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;965&#20010;&#20803;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;GBDT&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.02997</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20309;&#26102;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#32988;&#36807;&#22686;&#24378;&#26641;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Do Neural Nets Outperform Boosted Trees on Tabular Data?. (arXiv:2305.02997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02997
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#25110;&#32773;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;965&#20010;&#20803;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;GBDT&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24120;&#29992;&#30340;&#25968;&#25454;&#31867;&#22411;&#20043;&#19968;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#20154;&#20204;&#20173;&#22312;&#31215;&#26497;&#35752;&#35770;NN&#26159;&#21542;&#36890;&#24120;&#20248;&#20110;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#24037;&#20316;&#35201;&#20040;&#35748;&#20026;GBDT&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#19968;&#36143;&#20248;&#20110;NN&#65292;&#35201;&#20040;&#35748;&#20026;NN&#20248;&#20110;GBDT&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#38382;&#65306;'&#36825;&#37325;&#35201;&#21527;&#65311;'&#25105;&#20204;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#27604;&#36739;19&#31181;&#31639;&#27861;&#65292;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;'NN vs. GBDT'&#20105;&#35770;&#34987;&#36807;&#20998;&#24378;&#35843;&#65306;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#30456;&#24403;&#22810;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#35201;&#20040;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#35201;&#20040;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;965&#20010;&#20803;&#29305;&#24449;&#65292;&#20197;&#30830;&#23450;&#25968;&#25454;&#38598;&#30340;&#21738;&#20123;&#29305;&#24615;&#20351;NN&#25110;GBDT&#26356;&#36866;&#21512;&#34920;&#29616;&#33391;&#22909;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;GBDT&#35201;&#27604;NN&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and ask, 'does it matter?' We conduct the largest tabular data analysis to date, by comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than selecting the best algorithm. Next, we analyze 965 metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32463;&#27982;&#23398;&#20013;&#20004;&#31181;&#26426;&#21046;&#30340;&#21487;&#23398;&#20064;&#24615;&#65306;&#33756;&#21333;&#25277;&#22870;&#21644;&#20004;&#37096;&#20998;&#31080;&#20215;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#36825;&#20004;&#31181;&#26426;&#21046;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.11700</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#22823;&#21270;&#33756;&#21333;&#25277;&#22870;&#21644;&#20004;&#37096;&#20998;&#31080;&#20215;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Learning Revenue Maximizing Menus of Lotteries and Two-Part Tariffs. (arXiv:2302.11700v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11700
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32463;&#27982;&#23398;&#20013;&#20004;&#31181;&#26426;&#21046;&#30340;&#21487;&#23398;&#20064;&#24615;&#65306;&#33756;&#21333;&#25277;&#22870;&#21644;&#20004;&#37096;&#20998;&#31080;&#20215;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#36825;&#20004;&#31181;&#26426;&#21046;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#22312;&#23398;&#20064;&#29702;&#35770;&#21644;&#35745;&#31639;&#32463;&#27982;&#23398;&#20132;&#21449;&#39046;&#22495;&#20013;&#36817;&#24180;&#26469;&#34028;&#21187;&#21457;&#23637;&#30340;&#19968;&#31995;&#21015;&#24037;&#20316;&#65292;&#25512;&#36827;&#20102;&#32463;&#27982;&#23398;&#20013;&#20004;&#31867;&#26426;&#21046;&#30340;&#21487;&#23398;&#20064;&#24615;&#30740;&#31350;&#65292;&#20998;&#21035;&#26159;&#33756;&#21333;&#25277;&#22870;&#21644;&#20004;&#37096;&#20998;&#31080;&#20215;&#12290;&#21069;&#32773;&#26159;&#19968;&#31867;&#26088;&#22312;&#38144;&#21806;&#22810;&#20010;&#29289;&#21697;&#30340;&#38543;&#26426;&#26426;&#21046;&#65292;&#24050;&#30693;&#33021;&#22815;&#23454;&#29616;&#36229;&#20986;&#30830;&#23450;&#24615;&#26426;&#21046;&#30340;&#25910;&#30410;&#65292;&#32780;&#21518;&#32773;&#21017;&#26159;&#38024;&#23545;&#38144;&#21806;&#21333;&#20010;&#29289;&#21697;&#22810;&#20010;&#21333;&#20301;&#65288;&#21103;&#26412;&#65289;&#30340;&#35774;&#35745;&#65292;&#36866;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22330;&#26223;&#65292;&#22914;&#27773;&#36710;&#25110;&#33258;&#34892;&#36710;&#20849;&#20139;&#26381;&#21153;&#31561;&#12290;&#25105;&#20204;&#20851;&#27880;&#22914;&#20309;&#20174;&#20080;&#23478;&#20272;&#20540;&#25968;&#25454;&#20013;&#23398;&#20064;&#20986;&#39640;&#25910;&#30410;&#30340;&#36825;&#31867;&#26426;&#21046;&#65292;&#28085;&#30422;&#22810;&#31181;&#20998;&#24067;&#35774;&#32622;&#65292;&#26082;&#26377;&#30452;&#25509;&#33719;&#24471;&#20080;&#23478;&#20272;&#20540;&#26679;&#26412;&#30340;&#24773;&#20917;&#65292;&#20063;&#26377;&#26356;&#20855;&#25361;&#25112;&#24615;&#12289;&#30740;&#31350;&#36739;&#23569;&#30340;&#22312;&#32447;&#35774;&#32622;&#65292;&#20854;&#20013;&#20080;&#23478;&#19968;&#20010;&#25509;&#19968;&#20010;&#21040;&#26469;&#65292;&#24182;&#19988;&#23545;&#20182;&#20204;&#30340;&#20272;&#20540;&#27809;&#26377;&#20998;&#24067;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#33756;&#21333;&#25277;&#22870;&#21644;&#20004;&#37096;&#20998;&#31080;&#20215;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We advance a recently flourishing line of work at the intersection of learning theory and computational economics by studying the learnability of two classes of mechanisms prominent in economics, namely menus of lotteries and two-part tariffs. The former is a family of randomized mechanisms designed for selling multiple items, known to achieve revenue beyond deterministic mechanisms, while the latter is designed for selling multiple units (copies) of a single item with applications in real-world scenarios such as car or bike-sharing services. We focus on learning high-revenue mechanisms of this form from buyer valuation data in both distributional settings, where we have access to buyers' valuation samples up-front, and the more challenging and less-studied online settings, where buyers arrive one-at-a-time and no distributional assumption is made about their values.  Our main contribution is proposing the first online learning algorithms for menus of lotteries and two-part tariffs wit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25345;&#32493;&#35889;&#35268;&#33539;&#21270;&#22120;&#65288;CaSpeR&#65289;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24449;&#36827;&#34892;&#35268;&#33539;&#21270;&#65292;&#22312;&#38754;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#26102;&#25913;&#21892;&#20102;&#22522;&#20110;&#37325;&#28436;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.03345</link><description>&lt;p&gt;
&#28508;&#22312;&#35889;&#35268;&#33539;&#21270;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Latent Spectral Regularization for Continual Learning. (arXiv:2301.03345v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25345;&#32493;&#35889;&#35268;&#33539;&#21270;&#22120;&#65288;CaSpeR&#65289;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24449;&#36827;&#34892;&#35268;&#33539;&#21270;&#65292;&#22312;&#38754;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#26102;&#25913;&#21892;&#20102;&#22522;&#20110;&#37325;&#28436;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#29289;&#26234;&#33021;&#22312;&#19968;&#29983;&#20013;&#38543;&#30528;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#31215;&#32047;&#32780;&#26377;&#26426;&#22320;&#22686;&#38271;&#65292;&#20294;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#26102;&#23481;&#26131;&#36973;&#21463;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;&#22522;&#20110;&#37325;&#28436;&#30340;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#30340;&#19968;&#31181;&#22810;&#21151;&#33021;&#21487;&#38752;&#35299;&#20915;&#26041;&#26696;&#65307;&#28982;&#32780;&#65292;&#31361;&#28982;&#30340;&#36755;&#20837;&#20013;&#26029;&#21644;&#20869;&#23384;&#38480;&#21046;&#20250;&#25913;&#21464;&#23427;&#20204;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#23398;&#20064;&#32773;&#28508;&#22312;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24449;&#26469;&#30740;&#31350;&#36825;&#19968;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#37325;&#26032;&#28436;&#31034;&#30340;&#19981;&#21516;&#31867;&#21035;&#30340;&#25968;&#25454;&#28857;&#36234;&#26469;&#36234;&#28151;&#21512;&#65292;&#24178;&#25200;&#20102;&#20998;&#31867;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#23427;&#23545;&#28508;&#22312;&#31354;&#38388;&#30340;&#25289;&#26222;&#25289;&#26031;&#35889;&#26045;&#21152;&#20102;&#36739;&#24369;&#30340;&#35201;&#27714;&#65292;&#20419;&#36827;&#20102;&#20998;&#21306;&#34892;&#20026;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25345;&#32493;&#35889;&#35268;&#33539;&#21270;&#22120;&#65288;CaSpeR&#65289;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#22522;&#20110;&#37325;&#28436;&#30340;CL&#26041;&#27861;&#36731;&#26494;&#32452;&#21512;&#65292;&#24182;&#25552;&#39640;&#20102;SOTA&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While biological intelligence grows organically as new knowledge is gathered throughout life, Artificial Neural Networks forget catastrophically whenever they face a changing training data distribution. Rehearsal-based Continual Learning (CL) approaches have been established as a versatile and reliable solution to overcome this limitation; however, sudden input disruptions and memory constraints are known to alter the consistency of their predictions. We study this phenomenon by investigating the geometric characteristics of the learner's latent space and find that replayed data points of different classes increasingly mix up, interfering with classification. Hence, we propose a geometric regularizer that enforces weak requirements on the Laplacian spectrum of the latent space, promoting a partitioning behavior. We show that our proposal, called Continual Spectral Regularizer (CaSpeR), can be easily combined with any rehearsal-based CL approach and improves the performance of SOTA meth
&lt;/p&gt;</description></item></channel></rss>