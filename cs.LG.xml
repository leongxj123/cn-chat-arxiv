<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>DeNetDM &#26159;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#19987;&#23478;&#20056;&#31215;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#22312;&#21019;&#24314;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#21518;&#65292;&#23558;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#21435;&#20559;&#35265;&#27169;&#22411;&#65292;&#30456;&#27604;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21462;&#24471;&#26356;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.19863</link><description>&lt;p&gt;
DeNetDM: &#36890;&#36807;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#26469;&#28040;&#38500;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
DeNetDM: Debiasing by Network Depth Modulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19863
&lt;/p&gt;
&lt;p&gt;
DeNetDM &#26159;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#19987;&#23478;&#20056;&#31215;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#22312;&#21019;&#24314;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#21518;&#65292;&#23558;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#21435;&#20559;&#35265;&#27169;&#22411;&#65292;&#30456;&#27604;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21462;&#24471;&#26356;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31070;&#32463;&#32593;&#32476;&#22312;&#20559;&#35265;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#26080;&#24847;&#38388;&#23398;&#20064;&#21040;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#23454;&#29616;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#30446;&#21069;&#35299;&#20915;&#36825;&#31181;&#20559;&#35265;&#30340;&#26041;&#27861;&#36890;&#24120;&#21253;&#25324;&#21033;&#29992;&#20559;&#35265;&#27880;&#37322;&#12289;&#26681;&#25454;&#20266;&#20559;&#35265;&#26631;&#31614;&#36827;&#34892;&#21152;&#26435;&#37325;&#12289;&#25110;&#36890;&#36807;&#22686;&#24378;&#25216;&#26415;&#22686;&#21152;&#20559;&#35265;&#20914;&#31361;&#25968;&#25454;&#28857;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DeNetDM&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35266;&#23519;&#32467;&#26524;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#20248;&#20808;&#23398;&#20064;&#26680;&#24515;&#23646;&#24615;&#65292;&#32780;&#26356;&#28145;&#23618;&#27425;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#33719;&#21462;&#19981;&#21516;&#20449;&#24687;&#26102;&#24378;&#35843;&#20559;&#35265;&#12290;&#25105;&#20204;&#21033;&#29992;&#20174;&#19987;&#23478;&#20056;&#31215;&#20013;&#25512;&#23548;&#20986;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#21019;&#24314;&#20102;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#65292;&#28982;&#21518;&#29992;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#30340;&#21435;&#20559;&#35265;&#27169;&#22411;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19863v1 Announce Type: new  Abstract: When neural networks are trained on biased datasets, they tend to inadvertently learn spurious correlations, leading to challenges in achieving strong generalization and robustness. Current approaches to address such biases typically involve utilizing bias annotations, reweighting based on pseudo-bias labels, or enhancing diversity within bias-conflicting data points through augmentation techniques. We introduce DeNetDM, a novel debiasing method based on the observation that shallow neural networks prioritize learning core attributes, while deeper ones emphasize biases when tasked with acquiring distinct information. Using a training paradigm derived from Product of Experts, we create both biased and debiased branches with deep and shallow architectures and then distill knowledge to produce the target debiased model. Extensive experiments and analyses demonstrate that our approach outperforms current debiasing techniques, achieving a not
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#65292;&#26088;&#22312;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.19181</link><description>&lt;p&gt;
&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#25490;&#21517;&#22120;
&lt;/p&gt;
&lt;p&gt;
Make Large Language Model a Better Ranker
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#65292;&#26088;&#22312;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#26174;&#33879;&#22686;&#24378;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#25512;&#33616;&#31995;&#32479;&#65288;RSs&#65289;&#27010;&#24565;&#21644;&#24320;&#21457;&#26041;&#24335;&#21457;&#29983;&#20102;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#28857;&#23545;&#28857;&#21644;&#25104;&#23545;&#25512;&#33616;&#33539;&#24335;&#19978;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#22120;&#20013;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#19968;&#20123;&#30740;&#31350;&#34429;&#28982;&#28145;&#20837;&#30740;&#31350;&#20102;&#21015;&#34920;&#22411;&#26041;&#27861;&#65292;&#20294;&#22312;&#25490;&#21517;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#19968;&#19981;&#36275;&#24402;&#22240;&#20110;&#25490;&#21517;&#21644;&#35821;&#35328;&#29983;&#25104;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#12290;ALRO&#26088;&#22312;&#24357;&#21512;LLMs&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#24494;&#22937;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;ALRO&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#26159;&#24341;&#20837;&#20102;&#36719;lambda&#20540;lo
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19181v1 Announce Type: cross  Abstract: The evolution of Large Language Models (LLMs) has significantly enhanced capabilities across various fields, leading to a paradigm shift in how Recommender Systems (RSs) are conceptualized and developed. However, existing research primarily focuses on point-wise and pair-wise recommendation paradigms. These approaches prove inefficient in LLM-based recommenders due to the high computational cost of utilizing Large Language Models. While some studies have delved into list-wise approaches, they fall short in ranking tasks. This shortfall is attributed to the misalignment between the objectives of ranking and language generation. To this end, this paper introduces the Language Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks within recommender systems. A key feature of ALRO is the introduction of soft lambda lo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837; BorealTC &#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#26032;&#39062;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;-Mamba&#20307;&#31995;&#32467;&#26500;&#22312;&#21271;&#26041;&#26862;&#26519;&#22320;&#24418;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.16877</link><description>&lt;p&gt;
&#24863;&#30693;&#21147;&#23601;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#65306;&#21271;&#26041;&#26862;&#26519;&#30340;&#22320;&#24418;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Proprioception Is All You Need: Terrain Classification for Boreal Forests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16877
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837; BorealTC &#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#26032;&#39062;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;-Mamba&#20307;&#31995;&#32467;&#26500;&#22312;&#21271;&#26041;&#26862;&#26519;&#22320;&#24418;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#39046;&#22495;&#26426;&#22120;&#20154;&#23398;&#30740;&#31350;&#24378;&#35843;&#20102;&#25269;&#24481;&#19981;&#21516;&#31867;&#22411;&#22320;&#24418;&#30340;&#37325;&#35201;&#24615;&#12290;&#21271;&#26041;&#26862;&#26519;&#29305;&#21035;&#21463;&#21040;&#35768;&#22810;&#38480;&#21046;&#26426;&#21160;&#24615;&#30340;&#22320;&#24418;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#22320;&#24418;&#24212;&#35813;&#22312;&#36234;&#37326;&#33258;&#20027;&#23548;&#33322;&#20013;&#21152;&#20197;&#32771;&#34385;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#22320;&#29699;&#19978;&#26368;&#22823;&#30340;&#38470;&#22320;&#29983;&#29289;&#32676;&#33853;&#20043;&#19968;&#65292;&#21271;&#26041;&#26862;&#26519;&#26159;&#39044;&#35745;&#33258;&#20027;&#36710;&#36742;&#23558;&#26085;&#30410;&#26222;&#21450;&#30340;&#22320;&#21306;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;BorealTC&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#24863;&#30693;&#21147;&#30340;&#22320;&#24418;&#20998;&#31867;&#65288;TC&#65289;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35760;&#24405;&#20102;Husky A200&#30340;116&#20998;&#38047;&#30340;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#12289;&#30005;&#26426;&#30005;&#27969;&#21644;&#36718;&#32974;&#37324;&#31243;&#25968;&#25454;&#65292;&#37325;&#28857;&#20851;&#27880;&#20856;&#22411;&#30340;&#21271;&#26041;&#26862;&#26519;&#22320;&#24418;&#65292;&#29305;&#21035;&#26159;&#38634;&#12289;&#20912;&#21644;&#28132;&#27877;&#22756;&#12290;&#32467;&#21512;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19982;&#21478;&#19968;&#20010;&#26469;&#33258;&#26368;&#26032;&#25216;&#26415;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;TC t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16877v1 Announce Type: cross  Abstract: Recent works in field robotics highlighted the importance of resiliency against different types of terrains. Boreal forests, in particular, are home to many mobility-impeding terrains that should be considered for off-road autonomous navigation. Also, being one of the largest land biomes on Earth, boreal forests are an area where autonomous vehicles are expected to become increasingly common. In this paper, we address this issue by introducing BorealTC, a publicly available dataset for proprioceptive-based terrain classification (TC). Recorded with a Husky A200, our dataset contains 116 min of Inertial Measurement Unit (IMU), motor current, and wheel odometry data, focusing on typical boreal forest terrains, notably snow, ice, and silty loam. Combining our dataset with another dataset from the state-of-the-art, we evaluate both a Convolutional Neural Network (CNN) and the novel state space model (SSM)-based Mamba architecture on a TC t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CrossTimeNet&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29305;&#24615;&#24046;&#24322;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#33021;&#26377;&#25928;&#36716;&#25442;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.12372</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#39044;&#35757;&#32451;&#23398;&#20064;&#21487;&#20256;&#36882;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning Transferable Time Series Classifier with Cross-Domain Pre-training from Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12372
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CrossTimeNet&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29305;&#24615;&#24046;&#24322;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#33021;&#26377;&#25928;&#36716;&#25442;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;SSL&#65289;&#30340;&#36827;&#23637;&#26174;&#33879;&#25512;&#21160;&#20102;&#21487;&#20256;&#36882;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#36825;&#23545;&#20110;&#22686;&#24378;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#22312;&#36328;&#39046;&#22495;SSL&#39044;&#35757;&#32451;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38169;&#36807;&#20102;&#38598;&#25104;&#19981;&#21516;&#39046;&#22495;&#27169;&#24335;&#21644;&#29305;&#24449;&#30340;&#23453;&#36149;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CrossTimeNet&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#36328;&#39046;&#22495;SSL&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#21508;&#31181;&#39046;&#22495;&#23398;&#20064;&#21487;&#20256;&#36882;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#24378;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12372v1 Announce Type: new  Abstract: Advancements in self-supervised pre-training (SSL) have significantly advanced the field of learning transferable time series representations, which can be very useful in enhancing the downstream task. Despite being effective, most existing works struggle to achieve cross-domain SSL pre-training, missing valuable opportunities to integrate patterns and features from different domains. The main challenge lies in the significant differences in the characteristics of time-series data across different domains, such as variations in the number of channels and temporal resolution scales. To address this challenge, we propose CrossTimeNet, a novel cross-domain SSL learning framework to learn transferable knowledge from various domains to largely benefit the target downstream task. One of the key characteristics of CrossTimeNet is the newly designed time series tokenization module, which could effectively convert the raw time series into a seque
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#35302;&#35273;&#20449;&#24687;&#32435;&#20837;&#27169;&#20223;&#23398;&#20064;&#24179;&#21488;&#20197;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#36890;&#36807;&#35757;&#32451;&#26426;&#22120;&#20154;&#20195;&#29702;&#25554;&#25300;USB&#30005;&#32518;&#65292;&#23454;&#29616;&#20102;&#22312;&#24494;&#32454;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.11898</link><description>&lt;p&gt;
&#35270;&#35273;-&#35302;&#35273;&#39044;&#35757;&#32451;&#29992;&#20110;&#25554;&#25300;&#30005;&#32518;
&lt;/p&gt;
&lt;p&gt;
Visuo-Tactile Pretraining for Cable Plugging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#35302;&#35273;&#20449;&#24687;&#32435;&#20837;&#27169;&#20223;&#23398;&#20064;&#24179;&#21488;&#20197;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#36890;&#36807;&#35757;&#32451;&#26426;&#22120;&#20154;&#20195;&#29702;&#25554;&#25300;USB&#30005;&#32518;&#65292;&#23454;&#29616;&#20102;&#22312;&#24494;&#32454;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35302;&#35273;&#20449;&#24687;&#26159;&#36827;&#34892;&#31934;&#32454;&#25805;&#32437;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#20316;&#20026;&#20154;&#31867;&#65292;&#25105;&#20204;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#35302;&#35273;&#20449;&#24687;&#26469;&#29702;&#35299;&#21608;&#22260;&#30340;&#29289;&#20307;&#20197;&#21450;&#22914;&#20309;&#19982;&#20854;&#20114;&#21160;&#12290;&#25105;&#20204;&#19981;&#20165;&#20351;&#29992;&#35302;&#25720;&#26469;&#25191;&#34892;&#25805;&#32437;&#20219;&#21153;&#65292;&#36824;&#29992;&#23427;&#26469;&#23398;&#20064;&#22914;&#20309;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#21019;&#24314;&#33021;&#22815;&#23398;&#20064;&#20197;&#20154;&#31867;&#25110;&#36229;&#20154;&#31867;&#27700;&#24179;&#23436;&#25104;&#25805;&#32437;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#65292;&#25105;&#20204;&#38656;&#35201;&#27491;&#30830;&#22320;&#23558;&#35302;&#35273;&#20449;&#24687;&#34701;&#20837;&#25216;&#33021;&#25191;&#34892;&#21644;&#25216;&#33021;&#23398;&#20064;&#20013;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#35302;&#35273;&#20449;&#24687;&#32435;&#20837;&#27169;&#20223;&#23398;&#20064;&#24179;&#21488;&#20197;&#25552;&#39640;&#22797;&#26434;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30528;&#25163;&#35299;&#20915;&#25554;&#25300;USB&#30005;&#32518;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#39033;&#20381;&#36182;&#20110;&#24494;&#35266;&#35270;&#35273;-&#35302;&#35273;&#21327;&#20316;&#30340;&#29087;&#32451;&#25805;&#32437;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#35302;&#35273;&#20449;&#24687;&#32435;&#20837;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#20154;&#20195;&#29702;&#25554;&#25300;USB&#30005;&#32518;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11898v1 Announce Type: cross  Abstract: Tactile information is a critical tool for fine-grain manipulation. As humans, we rely heavily on tactile information to understand objects in our environments and how to interact with them. We use touch not only to perform manipulation tasks but also to learn how to perform these tasks. Therefore, to create robotic agents that can learn to complete manipulation tasks at a human or super-human level of performance, we need to properly incorporate tactile information into both skill execution and skill learning. In this paper, we investigate how we can incorporate tactile information into imitation learning platforms to improve performance on complex tasks. To do this, we tackle the challenge of plugging in a USB cable, a dexterous manipulation task that relies on fine-grain visuo-tactile serving. By incorporating tactile information into imitation learning frameworks, we are able to train a robotic agent to plug in a USB cable - a firs
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#36125;&#21494;&#26031;&#26410;&#26469;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#36234;&#37326;&#22320;&#22270;&#30340;&#21046;&#20316;&#65292;&#20026;&#38271;&#31243;&#39044;&#27979;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2403.11876</link><description>&lt;p&gt;
&#28145;&#24230;&#36125;&#21494;&#26031;&#26410;&#26469;&#34701;&#21512;&#29992;&#20110;&#33258;&#30417;&#30563;&#12289;&#39640;&#20998;&#36776;&#29575;&#12289;&#36234;&#37326;&#22320;&#22270;&#21046;&#20316;
&lt;/p&gt;
&lt;p&gt;
Deep Bayesian Future Fusion for Self-Supervised, High-Resolution, Off-Road Mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#36125;&#21494;&#26031;&#26410;&#26469;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#36234;&#37326;&#22320;&#22270;&#30340;&#21046;&#20316;&#65292;&#20026;&#38271;&#31243;&#39044;&#27979;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#28304;&#21463;&#38480;&#30340;&#36234;&#37326;&#36710;&#36742;&#30340;&#20256;&#24863;&#22120;&#20998;&#36776;&#29575;&#26377;&#38480;&#65292;&#36825;&#32473;&#21487;&#38752;&#30340;&#36234;&#37326;&#33258;&#20027;&#24615;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#34701;&#21512;&#26410;&#26469;&#20449;&#24687;&#65288;&#21363;&#26410;&#26469;&#34701;&#21512;&#65289;&#36827;&#34892;&#33258;&#30417;&#30563;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#21033;&#29992;&#26410;&#26469;&#20449;&#24687;&#20197;&#21450;&#25163;&#24037;&#21046;&#20316;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#30452;&#25509;&#30417;&#30563;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#21487;&#31359;&#36234;&#24615;&#20272;&#35745;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#20010;&#26356;&#20026;&#36890;&#29992;&#30340;&#21457;&#23637;&#26041;&#21521; - &#36890;&#36807;&#26410;&#26469;&#34701;&#21512;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#26102;&#38388;&#39640;&#25928;&#22320;&#23436;&#25104;&#26368;&#39640;&#20998;&#36776;&#29575;&#65288;&#21363;&#27599;&#20687;&#32032;2&#21400;&#31859;&#65289;BEV&#22320;&#22270;&#65292;&#21487;&#29992;&#20110;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#38271;&#31243;&#39044;&#27979;&#12290;&#20026;&#27492;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#26410;&#26469;&#34701;&#21512;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#65288;RGB / &#39640;&#24230;&#65289;&#21407;&#22987;&#31232;&#30095;&#22122;&#38899;&#36755;&#20837;&#21644;&#22522;&#20110;&#22320;&#22270;&#30340;&#23494;&#38598;&#26631;&#31614;&#30340;&#25104;&#23545;&#25968;&#25454;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#36866;&#24212;&#20256;&#24863;&#22120;&#30340;&#22122;&#22768;&#21644;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11876v1 Announce Type: cross  Abstract: The limited sensing resolution of resource-constrained off-road vehicles poses significant challenges towards reliable off-road autonomy. To overcome this limitation, we propose a general framework based on fusing the future information (i.e. future fusion) for self-supervision. Recent approaches exploit this future information alongside the hand-crafted heuristics to directly supervise the targeted downstream tasks (e.g. traversability estimation). However, in this paper, we opt for a more general line of development - time-efficient completion of the highest resolution (i.e. 2cm per pixel) BEV map in a self-supervised manner via future fusion, which can be used for any downstream tasks for better longer range prediction. To this end, first, we create a high-resolution future-fusion dataset containing pairs of (RGB / height) raw sparse and noisy inputs and map-based dense labels. Next, to accommodate the noise and sparsity of the sens
&lt;/p&gt;</description></item><item><title>I3SB&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#32467;&#21512;&#25439;&#22351;&#30340;&#22270;&#20687;&#25913;&#21892;&#32441;&#29702;&#24674;&#22797;&#65292;&#22312;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.06069</link><description>&lt;p&gt;
&#38544;&#24335;&#22270;&#20687;&#23545;&#22270;&#20687;Schrodinger&#26725;&#29992;&#20110;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Implicit Image-to-Image Schrodinger Bridge for CT Super-Resolution and Denoising
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06069
&lt;/p&gt;
&lt;p&gt;
I3SB&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#32467;&#21512;&#25439;&#22351;&#30340;&#22270;&#20687;&#25913;&#21892;&#32441;&#29702;&#24674;&#22797;&#65292;&#22312;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#22312;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#32780;&#24471;&#21040;&#35748;&#21487;&#65292;&#28982;&#32780;&#65292;&#20854;&#20174;&#39640;&#26031;&#22122;&#22768;&#24320;&#22987;&#30340;&#36845;&#20195;&#21435;&#22122;&#36807;&#31243;&#24448;&#24448;&#23548;&#33268;&#25512;&#26029;&#36895;&#24230;&#24930;&#12290;&#20316;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#22270;&#20687;&#23545;&#22270;&#20687;Schr&#246;dinger&#26725;&#65288;I2SB&#65289;&#20174;&#25439;&#22351;&#30340;&#22270;&#20687;&#24320;&#22987;&#21021;&#22987;&#21270;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#38598;&#25104;&#20102;&#26377;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25216;&#26415;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#38544;&#24335;&#22270;&#20687;&#23545;&#22270;&#20687;Schr&#246;dinger&#26725;&#65288;I3SB&#65289;&#25193;&#23637;&#20102;I2SB&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#19968;&#29983;&#25104;&#27493;&#39588;&#20013;&#32435;&#20837;&#25439;&#22351;&#30340;&#22270;&#20687;&#65292;&#23558;&#20854;&#29983;&#25104;&#36807;&#31243;&#36716;&#25442;&#20026;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#12290;&#36825;&#31181;&#22686;&#24378;&#20351;&#24471;I3SB&#33021;&#22815;&#22312;&#23569;&#37327;&#29983;&#25104;&#27493;&#39588;&#20013;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#32441;&#29702;&#24674;&#22797;&#30340;&#22270;&#20687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;&#20219;&#21153;&#19978;&#24471;&#21040;&#39564;&#35777;&#65292;&#24182;&#36229;&#36234;&#20102;&#21253;&#25324;&#26377;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#20869;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06069v1 Announce Type: cross  Abstract: Conditional diffusion models have gained recognition for their effectiveness in image restoration tasks, yet their iterative denoising process, starting from Gaussian noise, often leads to slow inference speeds. As a promising alternative, the Image-to-Image Schr\"odinger Bridge (I2SB) initializes the generative process from corrupted images and integrates training techniques from conditional diffusion models. In this study, we extended the I2SB method by introducing the Implicit Image-to-Image Schrodinger Bridge (I3SB), transitioning its generative process to a non-Markovian process by incorporating corrupted images in each generative step. This enhancement empowers I3SB to generate images with better texture restoration using a small number of generative steps. The proposed method was validated on CT super-resolution and denoising tasks and outperformed existing methods, including the conditional denoising diffusion probabilistic mod
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PromptKD&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#23454;&#29616;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#65292;&#26080;&#38656;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12842</link><description>&lt;p&gt;
PromptKD&#65306;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#20026;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12842
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PromptKD&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#23454;&#29616;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#65292;&#26080;&#38656;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#23545;&#25512;&#29702;&#25104;&#26412;&#30340;&#25285;&#24551;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#19968;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#38024;&#23545;LLMs&#36825;&#26679;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;KD&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#65292;&#32780;&#25552;&#21462;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#27169;&#22411;&#30340;KD&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#24615;&#33021;&#65292;&#22312;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptKD&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972; - &#22312;KD&#20013;&#39318;&#27425;&#20986;&#29616; - &#20351;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20256;&#36882;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#12290;&#19982;&#20808;&#21069;&#20998;&#31867;&#24037;&#20316;&#19981;&#21516;&#65292;&#20808;&#21069;&#37027;&#20123;&#38656;&#35201;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#20197;&#25552;&#21462;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#65292;PromptKD&#36890;&#36807;&#28155;&#21152;&#23569;&#37327;&#25552;&#31034;&#26631;&#35760;&#65292;&#24182;&#20165;&#36890;&#36807;&#23398;&#29983;&#25351;&#23548;&#35843;&#25972;&#25552;&#31034;&#26469;&#36798;&#21040;&#31867;&#20284;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12842v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Ex
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#22914;&#20309;&#22312;&#35757;&#32451;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#20195;&#30721;&#21253;&#21547;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#23457;&#35745;&#26102;&#30340;&#29256;&#26435;&#20405;&#26435;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09299</link><description>&lt;p&gt;
&#26410;&#32463;&#26412;&#20154;&#21516;&#24847;&#30340;&#35757;&#32451;&#65306;&#22312;&#35757;&#32451;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#20195;&#30721;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;
Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#22914;&#20309;&#22312;&#35757;&#32451;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#20195;&#30721;&#21253;&#21547;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#23457;&#35745;&#26102;&#30340;&#29256;&#26435;&#20405;&#26435;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#23457;&#35745;&#36890;&#36807;&#39564;&#35777;&#24320;&#21457;&#30340;&#20195;&#30721;&#26159;&#21542;&#31526;&#21512;&#26631;&#20934;&#12289;&#27861;&#35268;&#21644;&#29256;&#26435;&#20445;&#25252;&#65292;&#30830;&#20445;&#20854;&#19981;&#21253;&#21547;&#26469;&#33258;&#21463;&#20445;&#25252;&#26469;&#28304;&#30340;&#20195;&#30721;&#12290;&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#32534;&#30721;&#21161;&#25163;&#30340;&#20986;&#29616;&#32473;&#20195;&#30721;&#23457;&#35745;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#26469;&#33258;&#20844;&#24320;&#21487;&#29992;&#30340;&#26469;&#28304;&#12290;&#36825;&#24341;&#21457;&#20102;&#30693;&#35782;&#20135;&#26435;&#20405;&#26435;&#38382;&#39064;&#65292;&#22240;&#20026;&#24320;&#21457;&#32773;&#30340;&#20195;&#30721;&#24050;&#21253;&#21547;&#22312;&#25968;&#25454;&#38598;&#20013;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;LLMs&#24320;&#21457;&#30340;&#20195;&#30721;&#23457;&#35745;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25105;&#20204;&#26080;&#27861;&#20934;&#30830;&#30830;&#23450;&#24320;&#21457;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;LLM&#26159;&#21542;&#24050;&#32463;&#22312;&#29305;&#23450;&#30340;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20195;&#30721;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#22240;&#20026;&#25105;&#20204;&#26080;&#27861;&#33719;&#24471;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#37492;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20445;&#23494;&#24615;&#65292;&#20256;&#32479;&#30340;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#31561;&#26041;&#27861;&#26080;&#27861;&#30830;&#20445;&#29256;&#26435;&#20405;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09299v1 Announce Type: cross Abstract: Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing. The dataset for training these models is mainly collected from publicly available sources. This raises the issue of intellectual property infringement as developers' codes are already included in the dataset. Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. To add
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HyperBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20013;&#24341;&#20837;&#36229;&#22270;&#24863;&#30693;&#23618;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#38590;&#20197;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07309</link><description>&lt;p&gt;
HyperBERT:&#23558;&#28151;&#21512;&#36229;&#22270;&#24863;&#30693;&#23618;&#19982;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#23646;&#24615;&#36229;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HyperBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20013;&#24341;&#20837;&#36229;&#22270;&#24863;&#30693;&#23618;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#38590;&#20197;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#36890;&#36807;&#22797;&#26434;&#30340;&#25299;&#25169;&#32467;&#26500;&#26631;&#35760;&#65292;&#34920;&#36798;&#22810;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;&#36229;&#36793;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#36229;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#25991;&#26412;&#23646;&#24615;&#36229;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#21516;&#26102;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#20840;&#37096;&#20869;&#23481;&#21644;&#33410;&#28857;&#23646;&#24615;&#20013;&#30340;&#20016;&#23500;&#35821;&#35328;&#23646;&#24615;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20026;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#36827;&#19968;&#27493;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#24341;&#20837;&#19987;&#38376;&#30340;&#36229;&#22270;&#24863;&#30693;&#23618;&#12290;&#36825;&#20123;&#23618;&#23558;&#39640;&#38454;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#24341;&#20837;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#21033;&#29992;&#36229;&#22270;&#32467;&#26500;&#20013;&#30340;&#39640;&#38454;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#25991;&#26412;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges. Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of node classification on text-attributed hypergraphs have garnered increasing research attention. However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the nodes attributes, which largely hampers their effectiveness and generalizability. To overcome these challenges, we explore ways to further augment a pretrained BERT model with specialized hypergraph-aware layers for the task of node classification. Such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. In this paper, we
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;CTRL&#65292;&#36890;&#36807;&#20248;&#21270;&#36215;&#28857;&#21644;&#31934;&#32454;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#21305;&#37197;&#26041;&#21521;&#23548;&#33268;&#30340;&#35757;&#32451;&#36712;&#36857;&#20559;&#24046;&#21644;&#32047;&#31215;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04924</link><description>&lt;p&gt;
&#20004;&#20010;&#20132;&#26131;&#19981;&#20250;&#22256;&#25200;&#65306;&#36890;&#36807;&#26500;&#36896;&#21512;&#29702;&#30340;&#26799;&#24230;&#21305;&#37197;&#26469;&#21387;&#32553;&#22270;&#34920;
&lt;/p&gt;
&lt;p&gt;
Two Trades is not Baffled: Condense Graph via Crafting Rational Gradient Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;CTRL&#65292;&#36890;&#36807;&#20248;&#21270;&#36215;&#28857;&#21644;&#31934;&#32454;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#21305;&#37197;&#26041;&#21521;&#23548;&#33268;&#30340;&#35757;&#32451;&#36712;&#36857;&#20559;&#24046;&#21644;&#32047;&#31215;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#22270;&#34920;&#19978;&#35757;&#32451;&#24050;&#32463;&#22312;&#22270;&#34920;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20294;&#20854;&#25104;&#26412;&#21644;&#23384;&#20648;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20316;&#20026;&#26368;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#20043;&#19968;&#65292;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#21305;&#37197;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#23558;&#23436;&#25972;&#30340;&#22270;&#34920;&#21387;&#32553;&#25104;&#26356;&#31616;&#27905;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#21512;&#25104;&#38598;&#12290;&#23613;&#31649;&#20196;&#20154;&#40723;&#33310;&#65292;&#20294;&#36825;&#20123;&#31574;&#30053;&#20027;&#35201;&#24378;&#35843;&#26799;&#24230;&#30340;&#21305;&#37197;&#26041;&#21521;&#65292;&#20174;&#32780;&#23548;&#33268;&#35757;&#32451;&#36712;&#36857;&#30340;&#20559;&#24046;&#12290;&#36825;&#31181;&#20559;&#24046;&#36827;&#19968;&#27493;&#30001;&#21387;&#32553;&#21644;&#35780;&#20272;&#38454;&#27573;&#20043;&#38388;&#30340;&#24046;&#24322;&#25918;&#22823;&#65292;&#26368;&#32456;&#23548;&#33268;&#32047;&#31215;&#35823;&#24046;&#65292;&#23545;&#21387;&#32553;&#22270;&#34920;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L} trajectory&#65288;\textbf{CTRL}&#65289;&#30340;&#26032;&#22411;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#25509;&#36817;&#21407;&#22987;&#25968;&#25454;&#38598;&#29305;&#24449;&#20998;&#24067;&#30340;&#20248;&#21270;&#36215;&#28857;&#21644;&#19968;&#20010;&#26356;&#31934;&#32454;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training on large-scale graphs has achieved remarkable results in graph representation learning, but its cost and storage have raised growing concerns. As one of the most promising directions, graph condensation methods address these issues by employing gradient matching, aiming to condense the full graph into a more concise yet information-rich synthetic set. Though encouraging, these strategies primarily emphasize matching directions of the gradients, which leads to deviations in the training trajectories. Such deviations are further magnified by the differences between the condensation and evaluation phases, culminating in accumulated errors, which detrimentally affect the performance of the condensed graphs. In light of this, we propose a novel graph condensation method named \textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L} trajectory (\textbf{CTRL}), which offers an optimized starting point closer to the original dataset's feature distribution and a more refined strategy for 
&lt;/p&gt;</description></item><item><title>"Lens"&#26159;&#19968;&#20010;&#22522;&#20110;T5&#26550;&#26500;&#30340;&#22522;&#30784;&#32593;&#32476;&#27969;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#34920;&#31034;&#65292;&#33021;&#22815;&#22312;&#27969;&#37327;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#31934;&#30830;&#30340;&#39044;&#27979;&#21644;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.03646</link><description>&lt;p&gt;
Lens: &#32593;&#32476;&#27969;&#37327;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lens: A Foundation Model for Network Traffic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03646
&lt;/p&gt;
&lt;p&gt;
"Lens"&#26159;&#19968;&#20010;&#22522;&#20110;T5&#26550;&#26500;&#30340;&#22522;&#30784;&#32593;&#32476;&#27969;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#34920;&#31034;&#65292;&#33021;&#22815;&#22312;&#27969;&#37327;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#31934;&#30830;&#30340;&#39044;&#27979;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#27969;&#37327;&#26159;&#25351;&#36890;&#36807;&#20114;&#32852;&#32593;&#25110;&#36830;&#25509;&#35745;&#31639;&#26426;&#30340;&#20219;&#20309;&#31995;&#32479;&#21457;&#36865;&#21644;&#25509;&#25910;&#30340;&#20449;&#24687;&#37327;&#12290;&#20998;&#26512;&#21644;&#29702;&#35299;&#32593;&#32476;&#27969;&#37327;&#23545;&#20110;&#25552;&#39640;&#32593;&#32476;&#23433;&#20840;&#21644;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#21253;&#30340;&#29305;&#27530;&#29305;&#24615;&#65292;&#22914;&#24322;&#26500;&#26631;&#22836;&#21644;&#32570;&#20047;&#35821;&#20041;&#30340;&#21152;&#23494;&#36127;&#36733;&#65292;&#32593;&#32476;&#27969;&#37327;&#30340;&#20998;&#26512;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#25429;&#25417;&#27969;&#37327;&#30340;&#28508;&#22312;&#35821;&#20041;&#65292;&#19968;&#20123;&#30740;&#31350;&#37319;&#29992;&#20102;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#25110;&#35299;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#20174;&#22823;&#35268;&#27169;&#30340;&#27969;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#21482;&#22312;&#27969;&#37327;&#29702;&#35299;&#65288;&#20998;&#31867;&#65289;&#25110;&#27969;&#37327;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Lens&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#30784;&#30340;&#32593;&#32476;&#27969;&#37327;&#27169;&#22411;&#65292;&#21033;&#29992;T5&#26550;&#26500;&#20174;&#22823;&#35268;&#27169;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#39044;&#35757;&#32451;&#34920;&#31034;&#12290;&#20511;&#21161;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#30340;&#20248;&#21183;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#23454;&#29616;&#31934;&#30830;&#30340;&#27969;&#37327;&#39044;&#27979;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network traffic refers to the amount of information being sent and received over the internet or any system that connects computers. Analyzing and understanding network traffic is vital for improving network security and management. However, the analysis of network traffic poses great challenges due to the unique characteristics of data packets, such as heterogeneous headers and encrypted payload lacking semantics. To capture the latent semantics of traffic, a few studies have adopted pre-training techniques based on the Transformer encoder or decoder to learn the representations from large-scale traffic data. However, these methods typically excel only in traffic understanding (classification) or traffic generation tasks. To address this issue, we develop Lens, a foundational network traffic model that leverages the T5 architecture to learn the pre-trained representations from large-scale unlabeled data. Harnessing the strength of the encoder-decoder framework, which captures the glob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36328;&#39046;&#22495;&#25968;&#25454;&#24046;&#24322;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03094</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#36890;&#36807;&#22686;&#24378;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36328;&#39046;&#22495;&#25968;&#25454;&#24046;&#24322;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#65288;CD-FSOD&#65289;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#20934;&#30830;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#29992;&#26368;&#23569;&#30340;&#26631;&#35760;&#26679;&#26412;&#26469;&#26816;&#27979;&#26032;&#39046;&#22495;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#24320;&#38598;&#26816;&#27979;&#22120;&#65288;&#20363;&#22914;DE-ViT&#65289;&#22312;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#21644;&#20256;&#32479;&#30340;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#31867;&#21035;&#65292;&#25105;&#20204;&#33258;&#28982;&#20250;&#25552;&#20986;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;1&#65289;&#36825;&#31181;&#24320;&#38598;&#26816;&#27979;&#26041;&#27861;&#33021;&#21542;&#23481;&#26131;&#22320;&#25512;&#24191;&#21040;CD-FSOD&#65311;2&#65289;&#22914;&#26524;&#19981;&#33021;&#65292;&#22914;&#20309;&#22312;&#38754;&#23545;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#24322;&#26102;&#22686;&#24378;&#24320;&#38598;&#26041;&#27861;&#30340;&#32467;&#26524;&#65311;&#20026;&#20102;&#22238;&#31572;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20010;&#34913;&#37327;&#39046;&#22495;&#24046;&#24322;&#30340;&#25351;&#26631;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;&#22810;&#26679;&#39046;&#22495;&#24230;&#37327;&#20540;&#30340;&#26032;&#30340;CD-FSOD&#22522;&#20934;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#22495;&#22806;&#25968;&#25454;&#38598;&#20013;&#35266;&#23519;&#21040;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#34920;&#26126;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;CD-FSOD&#19978;&#22833;&#36133;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenge of cross-domain few-shot object detection (CD-FSOD), aiming to develop an accurate object detector for novel domains with minimal labeled examples. While transformer-based open-set detectors e.g., DE-ViT~\cite{zhang2023detect} have excelled in both open-vocabulary object detection and traditional few-shot object detection, detecting categories beyond those seen during training, we thus naturally raise two key questions: 1) can such open-set detection methods easily generalize to CD-FSOD? 2) If no, how to enhance the results of open-set methods when faced with significant domain gaps? To address the first question, we introduce several metrics to quantify domain variances and establish a new CD-FSOD benchmark with diverse domain metric values. Some State-Of-The-Art (SOTA) open-set object detection methods are evaluated on this benchmark, with evident performance degradation observed across out-of-domain datasets. This indicates the failure of adopting 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#22312;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23569;&#31867;&#21035;&#22270;&#20687;&#20998;&#31867;&#26102;&#29983;&#25104;&#38750;&#31354;&#27867;&#21270;&#30028;&#38480;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#21457;&#23637;&#20102;&#38024;&#23545;&#20989;&#25968;&#31354;&#38388;&#21644;&#20855;&#26377;&#19968;&#33324;Lipschitz&#28608;&#27963;&#20989;&#25968;&#30340;CNNs&#30340;&#26032;&#30340;Talagrand&#21387;&#32553;&#24341;&#29702;&#12290;</title><link>https://arxiv.org/abs/2208.04284</link><description>&lt;p&gt;
&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#28145;&#24230;&#23398;&#20064;&#19968;&#33324;&#21270;&#30028;&#38480;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Rademacher Complexity-based Generalization Bounds for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.04284
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#22312;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23569;&#31867;&#21035;&#22270;&#20687;&#20998;&#31867;&#26102;&#29983;&#25104;&#38750;&#31354;&#27867;&#21270;&#30028;&#38480;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#21457;&#23637;&#20102;&#38024;&#23545;&#20989;&#25968;&#31354;&#38388;&#21644;&#20855;&#26377;&#19968;&#33324;Lipschitz&#28608;&#27963;&#20989;&#25968;&#30340;CNNs&#30340;&#26032;&#30340;Talagrand&#21387;&#32553;&#24341;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#36827;&#34892;&#20998;&#31867;&#23569;&#37327;&#31867;&#21035;&#22270;&#20687;&#38750;&#31354;&#27867;&#21270;&#30028;&#38480;&#12290;&#26032;&#30340;Talagrand&#21387;&#32553;&#24341;&#29702;&#30340;&#21457;&#23637;&#23545;&#20110;&#39640;&#32500;&#26144;&#23556;&#20989;&#25968;&#31354;&#38388;&#21644;&#20855;&#26377;&#19968;&#33324;Lipschitz&#28608;&#27963;&#20989;&#25968;&#30340;CNNs&#26159;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Rademacher&#22797;&#26434;&#24230;&#19981;&#20381;&#36182;&#20110;CNNs&#30340;&#32593;&#32476;&#38271;&#24230;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35832;&#22914;ReLU&#65292;Leaky ReLU&#65292;Parametric Rectifier Linear Unit&#65292;Sigmoid&#21644;Tanh&#31561;&#29305;&#23450;&#31867;&#22411;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that the Rademacher complexity-based approach can generate non-vacuous generalisation bounds on Convolutional Neural Networks (CNNs) for classifying a small number of classes of images. The development of new Talagrand's contraction lemmas for high-dimensional mappings between function spaces and CNNs for general Lipschitz activation functions is a key technical contribution. Our results show that the Rademacher complexity does not depend on the network length for CNNs with some special types of activation functions such as ReLU, Leaky ReLU, Parametric Rectifier Linear Unit, Sigmoid, and Tanh.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#21644;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#30340;&#36830;&#32493;&#26102;&#31354;&#27169;&#22411;&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#20855;&#26377;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#20998;&#36776;&#29575;&#19981;&#21464;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#30701;&#26399;&#26102;&#38388;&#24207;&#21015;&#21644;&#38271;&#26399;&#32479;&#35745;&#25968;&#25454;&#19978;&#39640;&#25928;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2311.11798</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#31354;&#27169;&#22411;&#30340;&#22522;&#20110;&#26799;&#24230;&#21644;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#30340;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Operator Learning for Continuous Spatial-Temporal Model with Gradient-Based and Derivative-Free Optimization Methods. (arXiv:2311.11798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#21644;&#26080;&#23548;&#25968;&#20248;&#21270;&#26041;&#27861;&#30340;&#36830;&#32493;&#26102;&#31354;&#27169;&#22411;&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#20855;&#26377;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#20998;&#36776;&#29575;&#19981;&#21464;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#30701;&#26399;&#26102;&#38388;&#24207;&#21015;&#21644;&#38271;&#26399;&#32479;&#35745;&#25968;&#25454;&#19978;&#39640;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24037;&#31243;&#24212;&#29992;&#20013;&#65292;&#24120;&#24120;&#20351;&#29992;&#20559;&#24494;&#20998;&#26041;&#31243;&#23545;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#36827;&#34892;&#26102;&#31354;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#31639;&#23376;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#28857;&#26159;&#23545;&#20110;&#31354;&#38388;&#21644;&#26102;&#38388;&#31163;&#25955;&#21270;&#30340;&#20998;&#36776;&#29575;&#19981;&#21464;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#19981;&#21516;&#26102;&#38388;&#20998;&#36776;&#29575;&#19979;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#25913;&#21892;&#26657;&#20934;&#27169;&#22411;&#30340;&#38271;&#26399;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20248;&#21270;&#26041;&#26696;&#65292;&#21033;&#29992;&#20102;&#22522;&#20110;&#26799;&#24230;&#21644;&#26080;&#23548;&#25968;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#30701;&#26399;&#26102;&#38388;&#24207;&#21015;&#21644;&#38271;&#26399;&#32479;&#35745;&#25968;&#25454;&#19978;&#39640;&#25928;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#25968;&#20540;&#23454;&#20363;&#65292;&#21253;&#25324;&#31896;&#24615;Burgers&#26041;&#31243;&#12289;Navier-Stokes&#26041;&#31243;&#21644;Kuramoto-Sivashinsky&#26041;&#31243;&#65292;&#26469;&#30740;&#31350;&#31354;&#38388;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial differential equations are often used in the spatial-temporal modeling of complex dynamical systems in many engineering applications. In this work, we build on the recent progress of operator learning and present a data-driven modeling framework that is continuous in both space and time. A key feature of the proposed model is the resolution-invariance with respect to both spatial and temporal discretizations, without demanding abundant training data in different temporal resolutions. To improve the long-term performance of the calibrated model, we further propose a hybrid optimization scheme that leverages both gradient-based and derivative-free optimization methods and efficiently trains on both short-term time series and long-term statistics. We investigate the performance of the spatial-temporal continuous learning framework with three numerical examples, including the viscous Burgers' equation, the Navier-Stokes equations, and the Kuramoto-Sivashinsky equation. The results 
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#34987;&#24212;&#29992;&#20110;&#33258;&#20027;&#32593;&#32476;&#23433;&#20840;&#34892;&#21160;&#26377;&#24456;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36824;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07745</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33258;&#20027;&#32593;&#32476;&#23433;&#20840;&#34892;&#21160;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey. (arXiv:2310.07745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07745
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#34987;&#24212;&#29992;&#20110;&#33258;&#20027;&#32593;&#32476;&#23433;&#20840;&#34892;&#21160;&#26377;&#24456;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36824;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#32593;&#32476;&#25915;&#20987;&#25968;&#37327;&#30340;&#24555;&#36895;&#22686;&#21152;&#24341;&#21457;&#20102;&#23545;&#38024;&#23545;&#24694;&#24847;&#34892;&#20026;&#30340;&#32593;&#32476;&#38450;&#24481;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#24050;&#32463;&#25104;&#20026;&#32531;&#35299;&#36825;&#20123;&#25915;&#20987;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;DRL&#22312;&#32593;&#32476;&#38450;&#24481;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#23558;DRL&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#33258;&#20027;&#32593;&#32476;&#23433;&#20840;&#34892;&#21160;&#65288;ACO&#65289;&#20043;&#21069;&#65292;&#36824;&#38656;&#35201;&#20811;&#26381;&#35768;&#22810;&#25361;&#25112;&#12290;&#38656;&#35201;&#20026;&#19982;&#23398;&#20064;&#32773;&#38754;&#23545;&#38750;&#24120;&#39640;&#32500;&#24230;&#29366;&#24577;&#31354;&#38388;&#12289;&#22823;&#35268;&#27169;&#22810;&#31163;&#25955;&#25805;&#20316;&#31354;&#38388;&#21644;&#23545;&#25239;&#23398;&#20064;&#30456;&#36935;&#30340;&#29615;&#22659;&#25552;&#20379;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#38024;&#23545;&#23454;&#26102;&#31574;&#30053;&#28216;&#25103;&#20063;&#36827;&#34892;&#20102;&#21360;&#35937;&#28145;&#21051;&#30340;&#24037;&#31243;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;DRL&#24212;&#29992;&#20110;&#23436;&#25972;&#30340;ACO&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;&#30456;&#20851;&#30340;DRL&#25991;&#29486;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#26500;&#24819;&#20102;&#19968;&#20010;&#29702;&#24819;&#21270;&#30340;ACO-DRL&#20195;&#29702;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#65306;i.) &#39046;&#22495;&#29305;&#24615;&#30340;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
The rapid increase in the number of cyber-attacks in recent years raises the need for principled methods for defending networks against malicious actors. Deep reinforcement learning (DRL) has emerged as a promising approach for mitigating these attacks. However, while DRL has shown much potential for cyber-defence, numerous challenges must be overcome before DRL can be applied to autonomous cyber-operations (ACO) at scale. Principled methods are required for environments that confront learners with very high-dimensional state spaces, large multi-discrete action spaces, and adversarial learning. Recent works have reported success in solving these problems individually. There have also been impressive engineering efforts towards solving all three for real-time strategy games. However, applying DRL to the full ACO problem remains an open challenge. Here, we survey the relevant DRL literature and conceptualize an idealised ACO-DRL agent. We provide: i.) A summary of the domain properties t
&lt;/p&gt;</description></item><item><title>&#20851;&#31995;&#21367;&#31215;&#32593;&#32476;&#26159;&#19968;&#20010;&#23398;&#20064;&#26174;&#24335;&#23618;&#27425;&#20851;&#31995;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#32500;&#20869;&#31215;&#20851;&#31995;&#27169;&#22359;&#21644;&#20851;&#31995;&#21367;&#31215;&#23618;&#65292;&#20197;&#21450;&#22522;&#20110;&#22270;&#20803;&#28388;&#27874;&#22120;&#30340;&#32676;&#32452;&#27604;&#36739;&#65292;&#33021;&#22815;&#34920;&#36798;&#26356;&#39640;&#38454;&#12289;&#23618;&#27425;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.03240</link><description>&lt;p&gt;
&#20851;&#31995;&#21367;&#31215;&#32593;&#32476;&#65306;&#23398;&#20064;&#23618;&#27425;&#20851;&#31995;&#34920;&#31034;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Relational Convolutional Networks: A framework for learning representations of hierarchical relations. (arXiv:2310.03240v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03240
&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#21367;&#31215;&#32593;&#32476;&#26159;&#19968;&#20010;&#23398;&#20064;&#26174;&#24335;&#23618;&#27425;&#20851;&#31995;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#32500;&#20869;&#31215;&#20851;&#31995;&#27169;&#22359;&#21644;&#20851;&#31995;&#21367;&#31215;&#23618;&#65292;&#20197;&#21450;&#22522;&#20110;&#22270;&#20803;&#28388;&#27874;&#22120;&#30340;&#32676;&#32452;&#27604;&#36739;&#65292;&#33021;&#22815;&#34920;&#36798;&#26356;&#39640;&#38454;&#12289;&#23618;&#27425;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#20010;&#25104;&#29087;&#30340;&#30740;&#31350;&#39046;&#22495;&#26159;&#24320;&#21457;&#33021;&#22815;&#23398;&#20064;&#26174;&#24335;&#20851;&#31995;&#29305;&#24449;&#34920;&#31034;&#30340;&#26550;&#26500;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#23398;&#20064;&#23618;&#27425;&#20851;&#31995;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20851;&#31995;&#21367;&#31215;&#32593;&#32476;&#8221;&#30340;&#26550;&#26500;&#26694;&#26550;&#12290;&#32473;&#23450;&#19968;&#31995;&#21015;&#23545;&#35937;&#65292;&#19968;&#20010;&#8220;&#22810;&#32500;&#20869;&#31215;&#20851;&#31995;&#8221;&#27169;&#22359;&#29983;&#25104;&#19968;&#20010;&#25551;&#36848;&#25152;&#26377;&#25104;&#23545;&#20851;&#31995;&#30340;&#20851;&#31995;&#24352;&#37327;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#8220;&#20851;&#31995;&#21367;&#31215;&#8221;&#23618;&#23558;&#20851;&#31995;&#24352;&#37327;&#36716;&#21270;&#20026;&#19968;&#20010;&#26032;&#23545;&#35937;&#24207;&#21015;&#65292;&#27599;&#20010;&#23545;&#35937;&#25551;&#36848;&#21069;&#19968;&#23618;&#26576;&#32676;&#23545;&#35937;&#20869;&#30340;&#20851;&#31995;&#12290;&#31867;&#20284;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28388;&#27874;&#22120;&#65292;&#22270;&#20803;&#28388;&#27874;&#22120;&#20195;&#34920;&#35201;&#19982;&#20851;&#31995;&#24352;&#37327;&#22312;&#27599;&#20010;&#20998;&#32452;&#20013;&#36827;&#34892;&#27604;&#36739;&#30340;&#20851;&#31995;&#27169;&#26495;&#12290;&#36890;&#36807;&#37325;&#22797;&#36825;&#20010;&#36807;&#31243;&#65292;&#24471;&#21040;&#26356;&#39640;&#38454;&#12289;&#23618;&#27425;&#30340;&#20851;&#31995;&#34920;&#31034;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26550;&#26500;&#30340;&#21160;&#26426;&#21644;&#32454;&#33410;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#35777;&#26126;&#8230;
&lt;/p&gt;
&lt;p&gt;
A maturing area of research in deep learning is the development of architectures that can learn explicit representations of relational features. In this paper, we focus on the problem of learning representations of hierarchical relations, proposing an architectural framework we call "relational convolutional networks". Given a sequence of objects, a "multi-dimensional inner product relation" module produces a relation tensor describing all pairwise relations. A "relational convolution" layer then transforms the relation tensor into a sequence of new objects, each describing the relations within some group of objects at the previous layer. Graphlet filters, analogous to filters in convolutional neural networks, represent a template of relations against which the relation tensor is compared at each grouping. Repeating this yields representations of higher-order, hierarchical relations. We present the motivation and details of the architecture, together with a set of experiments to demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#24863;&#30693;&#23884;&#20837;&#28436;&#21270;(SEvo)&#26426;&#21046;&#65292;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#27880;&#20837;&#21040;&#23884;&#20837;&#20013;&#65292;&#20174;&#32780;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03032</link><description>&lt;p&gt;
&#22270;&#22686;&#24378;&#20248;&#21270;&#22120;&#29992;&#20110;&#32467;&#26500;&#24863;&#30693;&#25512;&#33616;&#23884;&#20837;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution. (arXiv:2310.03032v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#24863;&#30693;&#23884;&#20837;&#28436;&#21270;(SEvo)&#26426;&#21046;&#65292;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#27880;&#20837;&#21040;&#23884;&#20837;&#20013;&#65292;&#20174;&#32780;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#22312;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#34394;&#25311;&#34920;&#31034;&#65292;&#24182;&#19988;&#26159;&#21518;&#32493;&#20915;&#31574;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23884;&#20837;&#26356;&#26032;&#26426;&#21046;&#65292;&#31216;&#20026;&#32467;&#26500;&#24863;&#30693;&#23884;&#20837;&#28436;&#21270;(SEvo)&#65292;&#20197;&#40723;&#21169;&#30456;&#20851;&#33410;&#28857;&#22312;&#27599;&#19968;&#27493;&#20013;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#28436;&#21270;&#12290;&#19982;&#36890;&#24120;&#20316;&#20026;&#20013;&#38388;&#37096;&#20998;&#30340;GNN&#65288;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#19981;&#21516;&#65292;SEvo&#33021;&#22815;&#30452;&#25509;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#27880;&#20837;&#21040;&#23884;&#20837;&#20013;&#65292;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35745;&#31639;&#24320;&#38144;&#21487;&#24573;&#30053;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#39564;&#35777;&#20102;SEvo&#30340;&#25910;&#25947;&#24615;&#36136;&#21450;&#20854;&#21487;&#33021;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#20197;&#35777;&#26126;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;SEvo&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#20248;&#21270;&#22120;&#20013;&#65292;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#30697;&#20272;&#35745;&#26657;&#27491;&#30340;SEvo&#22686;&#24378;AdamW&#20013;&#65292;&#35777;&#26126;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#25928;&#26524;&#22312;&#22810;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#65292;&#20026;&#26377;&#25928;&#25512;&#33616;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#36335;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding plays a critical role in modern recommender systems because they are virtual representations of real-world entities and the foundation for subsequent decision models. In this paper, we propose a novel embedding update mechanism, Structure-aware Embedding Evolution (SEvo for short), to encourage related nodes to evolve similarly at each step. Unlike GNN (Graph Neural Network) that typically serves as an intermediate part, SEvo is able to directly inject the graph structure information into embedding with negligible computational overhead in training. The convergence properties of SEvo as well as its possible variants are theoretically analyzed to justify the validity of the designs. Moreover, SEvo can be seamlessly integrated into existing optimizers for state-of-the-art performance. In particular, SEvo-enhanced AdamW with moment estimate correction demonstrates consistent improvements across a spectrum of models and datasets, suggesting a novel technical route to effectively 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992; EHR &#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21482;&#38656;&#21033;&#29992;&#21382;&#21490;&#30340;&#21307;&#30103;&#26381;&#21153;&#20195;&#30721;&#21644;&#35786;&#26029;&#20449;&#24687;&#26469;&#23454;&#29616;&#26368;&#23567;&#21270;&#30340;&#25968;&#25454;&#38656;&#27714;&#65292;&#36890;&#36807;&#23558;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#23454;&#29616;&#23545;&#24739;&#32773;&#30284;&#30151;&#39118;&#38505;&#30340;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.15039</link><description>&lt;p&gt;
&#32467;&#21512;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#36827;&#34892;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Combining Survival Analysis and Machine Learning for Mass Cancer Risk Prediction using EHR data. (arXiv:2309.15039v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15039
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992; EHR &#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21482;&#38656;&#21033;&#29992;&#21382;&#21490;&#30340;&#21307;&#30103;&#26381;&#21153;&#20195;&#30721;&#21644;&#35786;&#26029;&#20449;&#24687;&#26469;&#23454;&#29616;&#26368;&#23567;&#21270;&#30340;&#25968;&#25454;&#38656;&#27714;&#65292;&#36890;&#36807;&#23558;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#23454;&#29616;&#23545;&#24739;&#32773;&#30284;&#30151;&#39118;&#38505;&#30340;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32431;&#31929;&#30340;&#21307;&#23398;&#32959;&#30244;&#31579;&#26597;&#26041;&#27861;&#36890;&#24120;&#36153;&#29992;&#39640;&#26114;&#12289;&#32791;&#26102;&#38271;&#65292;&#24182;&#19988;&#20165;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#12290;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#22312;&#30284;&#30151;&#26816;&#27979;&#26041;&#38754;&#21457;&#25381;&#20102;&#24040;&#22823;&#20316;&#29992;&#65292;&#20294;&#38656;&#35201;&#29305;&#23450;&#25110;&#28145;&#20837;&#30340;&#21307;&#23398;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#38754;&#24433;&#21709;&#20102;&#30284;&#30151;&#31579;&#26597;&#26041;&#27861;&#30340;&#22823;&#35268;&#27169;&#23454;&#26045;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#24050;&#26377;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#23545;&#24739;&#32773;&#36827;&#34892;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#30284;&#30151;&#39118;&#38505;&#35780;&#20272;&#24212;&#29992;AI&#26041;&#27861;&#26159;&#19968;&#31181;&#39072;&#35206;&#24615;&#30340;&#25913;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;EHR&#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#30340;&#25968;&#25454;&#36138;&#23146;&#31574;&#30053;&#33073;&#39062;&#32780;&#20986;&#65292;&#20165;&#38656;&#35201;&#26469;&#33258;EHR&#30340;&#21307;&#30103;&#26381;&#21153;&#20195;&#30721;&#21644;&#35786;&#26029;&#21382;&#21490;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20108;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;175441&#21517;&#19981;&#35760;&#21517;&#30340;&#24739;&#32773;&#65288;&#20854;&#20013;2861&#21517;&#34987;&#35786;&#26029;&#20026;&#30284;&#30151;&#65289;&#12290;&#20316;&#20026;&#22522;&#20934;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;
&lt;/p&gt;
&lt;p&gt;
Purely medical cancer screening methods are often costly, time-consuming, and weakly applicable on a large scale. Advanced Artificial Intelligence (AI) methods greatly help cancer detection but require specific or deep medical data. These aspects affect the mass implementation of cancer screening methods. For these reasons, it is a disruptive change for healthcare to apply AI methods for mass personalized assessment of the cancer risk among patients based on the existing Electronic Health Records (EHR) volume.  This paper presents a novel method for mass cancer risk prediction using EHR data. Among other methods, our one stands out by the minimum data greedy policy, requiring only a history of medical service codes and diagnoses from EHR. We formulate the problem as a binary classification. This dataset contains 175 441 de-identified patients (2 861 diagnosed with cancer). As a baseline, we implement a solution based on a recurrent neural network (RNN). We propose a method that combine
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#23884;&#20837;&#65292;&#20197;&#25581;&#31034;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#32858;&#31867;&#32467;&#26500;&#12290;&#36890;&#36807;&#32447;&#24615;&#32452;&#21512;&#23545;&#27604;PCA&#21644;&#23792;&#24230;&#25237;&#24433;&#36861;&#36394;&#20004;&#20010;&#30446;&#26631;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25490;&#38500;&#20808;&#39564;&#20449;&#24687;&#30456;&#20851;&#30340;&#32467;&#26500;&#24182;&#23454;&#29616;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2309.14857</link><description>&lt;p&gt;
&#20351;&#29992;&#20449;&#24687;&#27969;&#24418;&#25237;&#24433;&#36827;&#34892;&#32858;&#31867;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Cluster Exploration using Informative Manifold Projections. (arXiv:2309.14857v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#23884;&#20837;&#65292;&#20197;&#25581;&#31034;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#32858;&#31867;&#32467;&#26500;&#12290;&#36890;&#36807;&#32447;&#24615;&#32452;&#21512;&#23545;&#27604;PCA&#21644;&#23792;&#24230;&#25237;&#24433;&#36861;&#36394;&#20004;&#20010;&#30446;&#26631;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25490;&#38500;&#20808;&#39564;&#20449;&#24687;&#30456;&#20851;&#30340;&#32467;&#26500;&#24182;&#23454;&#29616;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#32500;&#26159;&#21487;&#35270;&#21270;&#25506;&#32034;&#39640;&#32500;&#25968;&#25454;&#21644;&#21457;&#29616;&#20854;&#22312;&#20108;&#32500;&#25110;&#19977;&#32500;&#31354;&#38388;&#20013;&#30340;&#32858;&#31867;&#32467;&#26500;&#30340;&#20851;&#38190;&#24037;&#20855;&#20043;&#19968;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#22823;&#37096;&#20998;&#38477;&#32500;&#26041;&#27861;&#24182;&#26410;&#32771;&#34385;&#23454;&#36341;&#32773;&#21487;&#33021;&#23545;&#25152;&#32771;&#34385;&#25968;&#25454;&#38598;&#30340;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#23884;&#20837;&#65292;&#19981;&#20165;&#25490;&#38500;&#19982;&#20808;&#39564;&#30693;&#35782;&#30456;&#20851;&#30340;&#32467;&#26500;&#65292;&#32780;&#19988;&#26088;&#22312;&#25581;&#31034;&#20219;&#20309;&#21097;&#20313;&#30340;&#28508;&#22312;&#32467;&#26500;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#20010;&#30446;&#26631;&#30340;&#32447;&#24615;&#32452;&#21512;&#65306;&#39318;&#20808;&#26159;&#23545;&#27604;PCA&#65292;&#33021;&#22815;&#28040;&#38500;&#19982;&#20808;&#39564;&#20449;&#24687;&#30456;&#20851;&#30340;&#32467;&#26500;&#65292;&#20854;&#27425;&#26159;&#23792;&#24230;&#25237;&#24433;&#36861;&#36394;&#65292;&#21487;&#20197;&#30830;&#20445;&#22312;&#24471;&#21040;&#30340;&#23884;&#20837;&#20013;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#20998;&#31163;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#23450;&#20041;&#20026;&#27969;&#24418;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#32771;&#34385;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dimensionality reduction (DR) is one of the key tools for the visual exploration of high-dimensional data and uncovering its cluster structure in two- or three-dimensional spaces. The vast majority of DR methods in the literature do not take into account any prior knowledge a practitioner may have regarding the dataset under consideration. We propose a novel method to generate informative embeddings which not only factor out the structure associated with different kinds of prior knowledge but also aim to reveal any remaining underlying structure. To achieve this, we employ a linear combination of two objectives: firstly, contrastive PCA that discounts the structure associated with the prior information, and secondly, kurtosis projection pursuit which ensures meaningful data separation in the obtained embeddings. We formulate this task as a manifold optimization problem and validate it empirically across a variety of datasets considering three distinct types of prior knowledge. Lastly, 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#27745;&#26579;&#25968;&#25454;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21435;&#27745;&#21644;&#21464;&#37327;&#20381;&#36182;&#24314;&#27169;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#23545;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.12563</link><description>&lt;p&gt;
&#38024;&#23545;&#24102;&#26377;&#27745;&#26579;&#25968;&#25454;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65306;&#23545;&#29983;&#29702;&#20449;&#21495;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals. (arXiv:2308.12563v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12563
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#27745;&#26579;&#25968;&#25454;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21435;&#27745;&#21644;&#21464;&#37327;&#20381;&#36182;&#24314;&#27169;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#23545;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#27969;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#36890;&#24120;&#22312;&#23398;&#26415;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#21463;&#21040;&#25511;&#21046;&#23454;&#39564;&#26465;&#20214;&#19979;&#30340;&#28165;&#27905;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#21253;&#21547;&#22122;&#22768;&#30340;&#25361;&#25112;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#26412;&#30740;&#31350;&#22312;&#24863;&#30693;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65288;TSAD&#65289;&#20013;&#28145;&#20837;&#30740;&#31350;&#20102;&#26631;&#31614;&#32423;&#22122;&#22768;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#23454;&#29992;&#30340;&#31471;&#21040;&#31471;&#26080;&#30417;&#30563;TSAD&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#20013;&#21253;&#21547;&#24322;&#24120;&#30340;&#24773;&#20917;&#19979;&#12290;&#35813;&#26041;&#27861;&#31216;&#20026;TSAD-C&#65292;&#20854;&#22312;&#35757;&#32451;&#38454;&#27573;&#19981;&#38656;&#35201;&#35775;&#38382;&#24322;&#24120;&#26631;&#31614;&#12290;TSAD-C&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#19968;&#20010;&#21435;&#27745;&#22120;&#29992;&#20110;&#32416;&#27491;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#24322;&#24120;&#65288;&#20063;&#31216;&#20026;&#22122;&#22768;&#65289;&#65292;&#19968;&#20010;&#21464;&#37327;&#20381;&#36182;&#24314;&#27169;&#27169;&#22359;&#29992;&#20110;&#25429;&#25417;&#21435;&#27745;&#21518;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#20869;&#37096;&#21644;&#36328;&#21464;&#37327;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#35270;&#20026;&#26367;&#20195;&#24615;&#30340;&#24322;&#24120;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mainstream unsupervised anomaly detection algorithms often excel in academic datasets, yet their real-world performance is restricted due to the controlled experimental conditions involving clean training data. Addressing the challenge of training with noise, a prevalent issue in practical anomaly detection, is frequently overlooked. In a pioneering endeavor, this study delves into the realm of label-level noise within sensory time-series anomaly detection (TSAD). This paper presents a novel and practical end-to-end unsupervised TSAD when the training data are contaminated with anomalies. The introduced approach, called TSAD-C, is devoid of access to abnormality labels during the training phase. TSAD-C encompasses three modules: a Decontaminator to rectify the abnormalities (aka noise) present in the training data, a Variable Dependency Modeling module to capture both long-term intra- and inter-variable dependencies within the decontaminated data that can be considered as a surrogate o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#21152;&#26435;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20445;&#25252;&#38544;&#31169;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#26435;&#37325;ERM&#20013;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#30340;&#31639;&#27861;&#65292;&#24182;&#19988;&#22312;&#19968;&#23450;&#30340;&#26465;&#20214;&#19979;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;DP&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.13127</link><description>&lt;p&gt;
&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#21152;&#26435;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#21450;&#20854;&#22312;&#32467;&#26524;&#21152;&#26435;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning. (arXiv:2307.13127v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#21152;&#26435;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20445;&#25252;&#38544;&#31169;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#26435;&#37325;ERM&#20013;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#30340;&#31639;&#27861;&#65292;&#24182;&#19988;&#22312;&#19968;&#23450;&#30340;&#26465;&#20214;&#19979;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;DP&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#26694;&#26550;&#20013;&#65292;&#20351;&#29992;&#21253;&#21547;&#20010;&#20154;&#20449;&#24687;&#30340;&#25968;&#25454;&#26469;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#26159;&#24120;&#35265;&#30340;&#20570;&#27861;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#19978;&#21487;&#20197;&#38750;&#24120;&#20934;&#30830;&#65292;&#20294;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#24471;&#21040;&#30340;&#32467;&#26524;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#38544;&#31169;&#25915;&#20987;&#12290;&#24046;&#20998;&#38544;&#31169;(DP)&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#25968;&#23398;&#19978;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#25439;&#22833;&#30028;&#38480;&#26469;&#35299;&#20915;&#36825;&#20123;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;DP&#24212;&#29992;&#20110;&#26080;&#26435;&#37325;&#30340;ERM&#20013;&#12290;&#25105;&#20204;&#32771;&#34385;&#21040;&#20102;&#26435;&#37325;ERM(wERM)&#30340;&#37325;&#35201;&#25512;&#24191;&#12290;&#22312;wERM&#20013;&#65292;&#21487;&#20197;&#20026;&#27599;&#20010;&#20010;&#20307;&#30340;&#30446;&#26631;&#20989;&#25968;&#36129;&#29486;&#20998;&#37197;&#19981;&#21516;&#30340;&#26435;&#37325;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#38556;&#30340;wERM&#31639;&#27861;&#65292;&#24182;&#22312;&#19968;&#23450;&#30340;&#27491;&#21017;&#26465;&#20214;&#19979;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#23558;&#29616;&#26377;&#30340;DP-ERM&#31243;&#24207;&#25193;&#23637;&#21040;wERM&#20026;&#32467;&#26524;&#21152;&#26435;&#23398;&#20064;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is commonplace to use data containing personal information to build predictive models in the framework of empirical risk minimization (ERM). While these models can be highly accurate in prediction, results obtained from these models with the use of sensitive data may be susceptible to privacy attacks. Differential privacy (DP) is an appealing framework for addressing such data privacy issues by providing mathematically provable bounds on the privacy loss incurred when releasing information from sensitive data. Previous work has primarily concentrated on applying DP to unweighted ERM. We consider an important generalization to weighted ERM (wERM). In wERM, each individual's contribution to the objective function can be assigned varying weights. In this context, we propose the first differentially private wERM algorithm, backed by a rigorous theoretical proof of its DP guarantees under mild regularity conditions. Extending the existing DP-ERM procedures to wERM paves a path to derivin
&lt;/p&gt;</description></item><item><title>MaskedKD&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36974;&#34109;&#22270;&#20687;&#22359;&#26469;&#26174;&#33879;&#38477;&#20302;Vision Transformer (ViT)&#33976;&#39311;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#24433;&#21709;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10494</link><description>&lt;p&gt;
MaskedKD&#65306;&#20351;&#29992;&#36974;&#34109;&#22270;&#20687;&#30340;&#39640;&#25928;Vision Transformer&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MaskedKD: Efficient Distillation of Vision Transformers with Masked Images. (arXiv:2302.10494v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10494
&lt;/p&gt;
&lt;p&gt;
MaskedKD&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36974;&#34109;&#22270;&#20687;&#22359;&#26469;&#26174;&#33879;&#38477;&#20302;Vision Transformer (ViT)&#33976;&#39311;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#24433;&#21709;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#23545;&#20110;&#35757;&#32451;&#36731;&#37327;&#32423;&#27169;&#22411;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20250;&#22312;&#35757;&#32451;&#25104;&#26412;&#20013;&#24341;&#20837;&#22823;&#37327;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#22240;&#20026;&#35813;&#26041;&#27861;&#38656;&#35201;&#22312;&#35757;&#32451;&#26679;&#26412;&#19978;&#33719;&#21462;&#25945;&#24072;&#30417;&#30563;&#12290;&#24403;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;Vision Transformer&#65288;ViTs&#65289;&#31561;&#25945;&#24072;&#27169;&#22411;&#26102;&#65292;&#36825;&#31181;&#38468;&#21152;&#25104;&#26412;&#8212;&#8212;&#33976;&#39311;&#25104;&#26412;&#8212;&#8212;&#26368;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MaskedKD&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#30528;&#38477;&#20302;&#33976;&#39311;ViTs&#30340;&#25104;&#26412;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MaskedKD&#36890;&#36807;&#36974;&#34109;&#19968;&#37096;&#20998;&#36755;&#20837;&#21040;&#25945;&#24072;&#27169;&#22411;&#30340;&#22270;&#20687;&#22359;&#20196;&#25945;&#24072;&#27169;&#22411;&#30340;&#25512;&#29702;&#25104;&#26412;&#20943;&#23569;&#65292;&#22240;&#27492;&#21487;&#20197;&#36339;&#36807;&#22788;&#29702;&#36825;&#20123;&#22359;&#25152;&#38656;&#30340;&#35745;&#31639;&#12290;&#25152;&#36873;&#30340;&#36974;&#32617;&#20301;&#32622;&#26088;&#22312;&#38450;&#27490;&#23631;&#34109;&#23398;&#29983;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#30340;&#22270;&#20687;&#30340;&#26680;&#24515;&#29305;&#24449;&#12290;&#35813;&#36974;&#32617;&#36873;&#25321;&#26426;&#21046;&#22522;&#20110;&#23398;&#29983;&#27169;&#22411;&#30340;&#26576;&#20123;&#27880;&#24847;&#21147;&#20998;&#25968;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is an effective method for training lightweight models, but it introduces a significant amount of computational overhead to the training cost, as the method requires acquiring teacher supervisions on training samples. This additional cost -- called distillation cost -- is most pronounced when we employ large-scale teacher models such as vision transformers (ViTs). We present MaskedKD, a simple yet effective strategy that can significantly reduce the cost of distilling ViTs without sacrificing the prediction accuracy of the student model. Specifically, MaskedKD diminishes the cost of running teacher at inference by masking a fraction of image patch tokens fed to the teacher, and therefore skipping the computations required to process those patches. The mask locations are selected to prevent masking away the core features of an image that the student model uses for prediction. This mask selection mechanism operates based on some attention score of the student model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#32479;&#19968;&#20132;&#21472;&#20551;&#35774;&#65292;&#32780;&#26159;&#21033;&#29992;&#20215;&#20540;&#30340;&#19979;&#38480;&#32622;&#20449;&#21306;&#38388;&#65288;LCBs&#65289;&#20248;&#21270;&#31574;&#30053;&#65292;&#22240;&#27492;&#33021;&#22815;&#36866;&#24212;&#20801;&#35768;&#34892;&#20026;&#31574;&#30053;&#28436;&#21464;&#21644;&#20542;&#21521;&#24615;&#20943;&#24369;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2212.09900</link><description>&lt;p&gt;
&#26080;&#20132;&#21472;&#31574;&#30053;&#23398;&#20064;&#65306;&#24754;&#35266;&#21644;&#24191;&#20041;&#32463;&#39564;Bernstein&#19981;&#31561;&#24335;
&lt;/p&gt;
&lt;p&gt;
Policy learning "without'' overlap: Pessimism and generalized empirical Bernstein's inequality. (arXiv:2212.09900v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#32479;&#19968;&#20132;&#21472;&#20551;&#35774;&#65292;&#32780;&#26159;&#21033;&#29992;&#20215;&#20540;&#30340;&#19979;&#38480;&#32622;&#20449;&#21306;&#38388;&#65288;LCBs&#65289;&#20248;&#21270;&#31574;&#30053;&#65292;&#22240;&#27492;&#33021;&#22815;&#36866;&#24212;&#20801;&#35768;&#34892;&#20026;&#31574;&#30053;&#28436;&#21464;&#21644;&#20542;&#21521;&#24615;&#20943;&#24369;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#65292;&#26088;&#22312;&#21033;&#29992;&#20808;&#21069;&#25910;&#38598;&#21040;&#30340;&#35266;&#27979;&#65288;&#26469;&#33258;&#20110;&#22266;&#23450;&#30340;&#25110;&#26159;&#36866;&#24212;&#28436;&#21464;&#30340;&#34892;&#20026;&#31574;&#30053;&#65289;&#26469;&#23398;&#20064;&#32473;&#23450;&#31867;&#21035;&#20013;&#30340;&#26368;&#20248;&#20010;&#24615;&#21270;&#20915;&#31574;&#35268;&#21017;&#12290;&#29616;&#26377;&#30340;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#32479;&#19968;&#20132;&#21472;&#20551;&#35774;&#65292;&#21363;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#25506;&#32034;&#25152;&#26377;&#20010;&#24615;&#21270;&#29305;&#24449;&#30340;&#25152;&#26377;&#21160;&#20316;&#30340;&#20542;&#21521;&#24615;&#19979;&#30028;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#26368;&#22351;&#30340;&#20542;&#21521;&#24615;&#12290;&#30001;&#20110;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#19981;&#21463;&#25511;&#21046;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#19981;&#22826;&#29616;&#23454;&#65292;&#29305;&#21035;&#26159;&#24403;&#20801;&#35768;&#34892;&#20026;&#31574;&#30053;&#38543;&#26102;&#38388;&#28436;&#21464;&#24182;&#19988;&#20542;&#21521;&#24615;&#20943;&#24369;&#26102;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23427;&#20248;&#21270;&#31574;&#30053;&#20215;&#20540;&#30340;&#19979;&#38480;&#32622;&#20449;&#21306;&#38388;&#65288;LCBs&#65289;&#8212;&#8212;&#32780;&#19981;&#26159;&#28857;&#20272;&#35745;&#12290;LCBs&#36890;&#36807;&#37327;&#21270;&#22686;&#24378;&#20498;&#25968;&#20542;&#21521;&#26435;&#37325;&#30340;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26469;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies offline policy learning, which aims at utilizing observations collected a priori (from either fixed or adaptively evolving behavior policies) to learn the optimal individualized decision rule in a given class. Existing policy learning methods rely on a uniform overlap assumption, i.e., the propensities of exploring all actions for all individual characteristics are lower bounded in the offline dataset. In other words, the performance of these methods depends on the worst-case propensity in the offline dataset. As one has no control over the data collection process, this assumption can be unrealistic in many situations, especially when the behavior policies are allowed to evolve over time with diminishing propensities.  In this paper, we propose a new algorithm that optimizes lower confidence bounds (LCBs) -- instead of point estimates -- of the policy values. The LCBs are constructed by quantifying the estimation uncertainty of the augmented inverse propensity weight
&lt;/p&gt;</description></item><item><title>DICTDIS&#26159;&#19968;&#31181;&#26032;&#39062;&#26377;&#35789;&#20856;&#32422;&#26463;&#30340;NMT&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#22810;&#20010;&#23383;&#20856;&#20505;&#36873;&#39033;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#20174;&#22810;&#20041;&#35789;&#20013;&#28040;&#38500;&#32763;&#35793;&#27495;&#20041;&#30340;&#30446;&#30340;&#65292;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2210.06996</link><description>&lt;p&gt;
DICTDIS&#65306;&#22522;&#20110;&#35789;&#20856;&#32422;&#26463;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#28040;&#27495;&#26041;&#27861;&#23545; NMT &#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
DICTDIS: Dictionary Constrained Disambiguation for Improved NMT. (arXiv:2210.06996v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06996
&lt;/p&gt;
&lt;p&gt;
DICTDIS&#26159;&#19968;&#31181;&#26032;&#39062;&#26377;&#35789;&#20856;&#32422;&#26463;&#30340;NMT&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#22810;&#20010;&#23383;&#20856;&#20505;&#36873;&#39033;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#20174;&#22810;&#20041;&#35789;&#20013;&#28040;&#38500;&#32763;&#35793;&#27495;&#20041;&#30340;&#30446;&#30340;&#65292;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#29305;&#23450;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65288;&#20363;&#22914;&#25945;&#32946;&#24212;&#29992;&#31243;&#24207;&#65289;&#22312;&#22810;&#35821;&#35328;&#31038;&#20250;&#20013;&#24110;&#21161;&#20351;&#20449;&#24687;&#23545;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#21487;&#35775;&#38382;&#26159;&#20855;&#26377;&#31038;&#20250;&#24847;&#20041;&#30340;&#12290;&#36825;&#31181; NMT &#31995;&#32479;&#24212;&#35813;&#20855;&#26377;&#35789;&#27719;&#32422;&#26463;&#24182;&#20174;&#39046;&#22495;&#29305;&#23450;&#30340;&#35789;&#20856;&#20013;&#27762;&#21462;&#12290;&#30001;&#20110;&#21333;&#35789;&#30340;&#22810;&#20041;&#24615;&#65292;&#35789;&#20856;&#20013;&#21487;&#33021;&#20250;&#20026;&#28304;&#21333;&#35789;&#25110;&#30701;&#35821;&#21576;&#29616;&#22810;&#20010;&#20505;&#36873;&#32763;&#35793;&#12290;&#36825;&#26102;&#65292;NMT &#27169;&#22411;&#38656;&#35201;&#36873;&#25321;&#19982;&#35821;&#22659;&#26368;&#30456;&#20851;&#30340;&#20505;&#36873;&#32763;&#35793;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#24573;&#30053;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#32780;&#20391;&#37325;&#20110;&#21333;&#20010;&#20505;&#36873;&#32422;&#26463;&#35774;&#32622;&#65292;&#20854;&#20013;&#30446;&#26631;&#35789;&#25110;&#30701;&#35821;&#34987;&#21333;&#20010;&#32422;&#26463;&#26367;&#25442;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DICTDIS&#30340;&#35789;&#20856;&#32422;&#26463; NMT &#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#28040;&#38500;&#20102;&#20174;&#23383;&#20856;&#20013;&#24471;&#20986;&#30340;&#22810;&#20010;&#20505;&#36873;&#32763;&#35793;&#30340;&#27495;&#20041;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35757;&#32451;&#25968;&#25454;&#19982;&#22810;&#20010;&#23383;&#20856;&#20505;&#36873;&#39033;&#36827;&#34892;&#22686;&#37327;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#31215;&#26497;&#40723;&#21169;&#28040;&#38500;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain-specific neural machine translation (NMT) systems (\eg, in educational applications) are socially significant with the potential to help make information accessible to a diverse set of users in multilingual societies. It is desirable that such NMT systems be lexically constrained and draw from domain-specific dictionaries. Dictionaries could present multiple candidate translations for a source word/phrase due to the polysemous nature of words. The onus is then on the NMT model to choose the contextually most appropriate candidate. Prior work has largely ignored this problem and focused on the single candidate constraint setting wherein the target word or phrase is replaced by a single constraint. In this work we present \dictdis, a lexically constrained NMT system that disambiguates between multiple candidate translations derived from dictionaries. We achieve this by augmenting training data with multiple dictionary candidates to actively encourage disambiguation during training
&lt;/p&gt;</description></item></channel></rss>