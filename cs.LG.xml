<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#26041;&#27861;&#21010;&#20998;&#20026;&#20803;&#23398;&#20064;&#12289;&#39044;&#35757;&#32451;&#21644;&#28151;&#21512;&#26041;&#27861;&#19977;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01440</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#65306;&#20174;&#20803;&#23398;&#20064;&#21040;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Learning on Graphs: from Meta-learning to Pre-training and Prompting
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#26041;&#27861;&#21010;&#20998;&#20026;&#20803;&#23398;&#20064;&#12289;&#39044;&#35757;&#32451;&#21644;&#28151;&#21512;&#26041;&#27861;&#19977;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26159;&#22270;&#20013;&#24515;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#22312;&#36825;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26089;&#26399;&#30340;&#25216;&#26415;&#36890;&#24120;&#22312;&#31471;&#21040;&#31471;&#30340;&#35774;&#32622;&#20013;&#36816;&#34892;&#65292;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20805;&#36275;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#36825;&#20010;&#38480;&#21046;&#24341;&#21457;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#20854;&#20013;&#27599;&#20010;&#20219;&#21153;&#21482;&#26377;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#26631;&#31614;&#21487;&#29992;&#12290;&#37492;&#20110;&#36825;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#25991;&#29486;&#65292;&#26412;&#32508;&#36848;&#35797;&#22270;&#32508;&#21512;&#26368;&#36817;&#30340;&#21457;&#23637;&#65292;&#25552;&#20379;&#27604;&#36739;&#24615;&#30340;&#35265;&#35299;&#65292;&#24182;&#30830;&#23450;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#30740;&#31350;&#31995;&#32479;&#22320;&#20998;&#20026;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20803;&#23398;&#20064;&#26041;&#27861;&#12289;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#65292;&#20197;&#24110;&#21161;&#35835;&#32773;&#36827;&#34892;&#26041;&#27861;&#36873;&#25321;&#12290;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#36825;&#20123;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#27604;&#36739;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#22270;&#19978;&#30340;&#23567;&#26679;&#26412;&#23398;&#20064;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning, a critical step in graph-centric tasks, has seen significant advancements. Earlier techniques often operate in an end-to-end setting, where performance heavily relies on the availability of ample labeled data. This constraint has spurred the emergence of few-shot learning on graphs, where only a few task-specific labels are available for each task. Given the extensive literature in this field, this survey endeavors to synthesize recent developments, provide comparative insights, and identify future directions. We systematically categorize existing studies into three major families: meta-learning approaches, pre-training approaches, and hybrid approaches, with a finer-grained classification in each family to aid readers in their method selection process. Within each category, we analyze the relationships among these methods and compare their strengths and limitations. Finally, we outline prospective future directions for few-shot learning on graphs to cata
&lt;/p&gt;</description></item><item><title>Latxa&#26159;&#19968;&#31181;&#29992;&#20110;&#24052;&#26031;&#20811;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29087;&#32451;&#24230;&#21644;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#25152;&#26377;&#20197;&#21069;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#24052;&#26031;&#20811;&#35821;&#39640;&#36136;&#37327;&#22522;&#20934;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.20266</link><description>&lt;p&gt;
Latxa: &#19968;&#31181;&#29992;&#20110;&#24052;&#26031;&#20811;&#35821;&#30340;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#21644;&#35780;&#20272;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
Latxa: An Open Language Model and Evaluation Suite for Basque
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20266
&lt;/p&gt;
&lt;p&gt;
Latxa&#26159;&#19968;&#31181;&#29992;&#20110;&#24052;&#26031;&#20811;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29087;&#32451;&#24230;&#21644;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#25152;&#26377;&#20197;&#21069;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#24052;&#26031;&#20811;&#35821;&#39640;&#36136;&#37327;&#22522;&#20934;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Latxa&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Llama 2&#30340;&#22823;&#22411;&#24052;&#26031;&#20811;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;7&#21040;700&#20159;&#12290;Latxa&#22522;&#20110;&#26032;&#30340;&#24052;&#26031;&#20811;&#35821;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;430&#19975;&#20010;&#25991;&#26723;&#21644;42&#20159;&#20010;&#26631;&#35760;&#12290;&#38024;&#23545;&#24052;&#26031;&#20811;&#35821;&#39640;&#36136;&#37327;&#22522;&#20934;&#30340;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;4&#20010;&#22810;&#39033;&#36873;&#25321;&#35780;&#20272;&#25968;&#25454;&#38598;&#65306;EusProficiency&#65292;&#21253;&#25324;&#26469;&#33258;&#23448;&#26041;&#35821;&#35328;&#33021;&#21147;&#32771;&#35797;&#30340;5169&#20010;&#38382;&#39064;&#65307;EusReading&#65292;&#21253;&#25324;352&#20010;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#65307;EusTrivia&#65292;&#21253;&#25324;&#26469;&#33258;5&#20010;&#30693;&#35782;&#39046;&#22495;&#30340;1715&#20010;&#29712;&#20107;&#38382;&#39064;&#65307;&#20197;&#21450;EusExams&#65292;&#21253;&#25324;&#26469;&#33258;&#20844;&#20849;&#32771;&#35797;&#30340;16774&#20010;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#24191;&#27867;&#35780;&#20272;&#20013;&#65292;Latxa&#22312;&#19982;&#25105;&#20204;&#27604;&#36739;&#30340;&#25152;&#26377;&#20808;&#21069;&#24320;&#25918;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22312;&#38405;&#35835;&#29702;&#35299;&#21644;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#26041;&#38754;&#33853;&#21518;&#65292;&#20294;&#22312;&#35821;&#35328;&#29087;&#32451;&#24230;&#21644;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#65292;&#23427;&#19982;GPT-4 Turbo&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;Latxa&#27169;&#22411;&#31995;&#21015;&#65292;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20266v1 Announce Type: cross  Abstract: We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#25509;&#21463;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#27169;&#24577;&#20998;&#31163;&#21644;&#37325;&#26500;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20058</link><description>&lt;p&gt;
&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#21644;&#28145;&#24230;&#25972;&#21512;&#30340;&#33041;&#20195;&#35874;&#12289;&#34880;&#28082;&#21160;&#21147;&#23398;&#21644;&#28748;&#27880;&#32593;&#32476;&#24443;&#24213;&#25913;&#21464;&#30142;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Disease Diagnosis with simultaneous functional PET/MR and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20058
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#25509;&#21463;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#27169;&#24577;&#20998;&#31163;&#21644;&#37325;&#26500;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#21151;&#33021;PET/MR&#65288;sf-PET/MR&#65289;&#26159;&#19968;&#31181;&#23574;&#31471;&#30340;&#22810;&#27169;&#24335;&#31070;&#32463;&#24433;&#20687;&#25216;&#26415;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#65292;&#21487;&#20197;&#21516;&#26102;&#30417;&#27979;&#21644;&#25972;&#21512;&#30001;&#26102;&#31354;&#21327;&#21464;&#20195;&#35874;&#27963;&#21160;&#12289;&#31070;&#32463;&#27963;&#21160;&#21644;&#33041;&#34880;&#27969;&#65288;&#28748;&#27880;&#65289;&#26500;&#24314;&#30340;&#22810;&#26041;&#38754;&#22823;&#33041;&#32593;&#32476;&#12290;&#34429;&#28982;&#22312;&#31185;&#23398;/&#20020;&#24202;&#20215;&#20540;&#19978;&#24456;&#39640;&#65292;&#20294;PET/MR&#30828;&#20214;&#30340;&#21487;&#21450;&#24615;&#19981;&#36275;&#38459;&#30861;&#20102;&#20854;&#24212;&#29992;&#65292;&#26356;&#19981;&#29992;&#35828;&#29616;&#20195;&#22522;&#20110;AI&#30340;PET/MR&#34701;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;AI&#30340;&#20020;&#24202;&#21487;&#34892;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#20840;&#38754;&#30340;sf-PET/MR&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20855;&#26377;&#20801;&#35768;&#21333;&#27169;&#24577;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#20165;PET&#65289;&#20197;&#21450;&#24378;&#21046;&#22810;&#27169;&#24577;&#20934;&#30830;&#24615;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#23545;&#40784;&#21644;&#37325;&#26500;&#27169;&#22411;&#12290;&#23427;&#26159;&#27169;&#24577;&#21487;&#20998;&#31163;&#21644;&#21487;&#20132;&#25442;&#30340;&#65292;&#21160;&#24577;&#20998;&#37197;&#19981;&#21516;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;"&#28151;&#21512;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20058v1 Announce Type: cross  Abstract: Simultaneous functional PET/MR (sf-PET/MR) presents a cutting-edge multimodal neuroimaging technique. It provides an unprecedented opportunity for concurrently monitoring and integrating multifaceted brain networks built by spatiotemporally covaried metabolic activity, neural activity, and cerebral blood flow (perfusion). Albeit high scientific/clinical values, short in hardware accessibility of PET/MR hinders its applications, let alone modern AI-based PET/MR fusion models. Our objective is to develop a clinically feasible AI-based disease diagnosis model trained on comprehensive sf-PET/MR data with the power of, during inferencing, allowing single modality input (e.g., PET only) as well as enforcing multimodal-based accuracy. To this end, we propose MX-ARM, a multimodal MiXture-of-experts Alignment and Reconstruction Model. It is modality detachable and exchangeable, allocating different multi-layer perceptrons dynamically ("mixture 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#25955;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#35299;&#20915;&#20132;&#36890;&#36335;&#21475;&#31359;&#36234;&#21644;&#33258;&#20027;&#36187;&#36710;&#31561;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.10996</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#25193;&#23637;&#19988;&#21487;&#24182;&#34892;&#21270;&#30340;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#21487;&#25345;&#32493;Sim2Real&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10996
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#25955;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#35299;&#20915;&#20132;&#36890;&#36335;&#21475;&#31359;&#36234;&#21644;&#33258;&#20027;&#36187;&#36710;&#31561;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#25353;&#38656;&#25193;&#23637;&#24182;&#34892;&#21270;&#35757;&#32451;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#21033;&#29992;&#26368;&#23569;&#30340;&#30828;&#20214;&#36164;&#28304;&#23558;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#20174;&#27169;&#25311;&#29615;&#22659;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20316;&#20026;&#19968;&#20010;&#21551;&#21160;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#12289;&#37096;&#32626;&#21644;&#36716;&#31227;&#21512;&#20316;&#21644;&#31454;&#20105;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20174;&#27169;&#25311;&#29615;&#22659;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#31350;&#20102;4&#21488;&#21512;&#20316;&#36710;&#36742;(Nigel)&#22312;&#21333;&#26234;&#33021;&#20307;&#21644;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#29615;&#22659;&#20013;&#20849;&#20139;&#26377;&#38480;&#29366;&#24577;&#20449;&#24687;&#30340;&#20132;&#21449;&#36941;&#21382;&#38382;&#39064;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#36890;&#29992;&#31574;&#30053;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20010;&#20307;&#31574;&#30053;&#26041;&#27861;&#30740;&#31350;&#20102;2&#36742;&#36710;(F1TENTH)&#30340;&#23545;&#25239;&#24615;&#33258;&#20027;&#36187;&#36710;&#38382;&#39064;&#12290;&#22312;&#20219;&#20309;&#19968;&#32452;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#26550;&#26500;&#65292;&#36825;&#20801;&#35768;&#23545;&#31574;&#30053;&#36827;&#34892;&#26377;&#21147;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10996v1 Announce Type: cross  Abstract: This work presents a sustainable multi-agent deep reinforcement learning framework capable of selectively scaling parallelized training workloads on-demand, and transferring the trained policies from simulation to reality using minimal hardware resources. We introduce AutoDRIVE Ecosystem as an enabling digital twin framework to train, deploy, and transfer cooperative as well as competitive multi-agent reinforcement learning policies from simulation to reality. Particularly, we first investigate an intersection traversal problem of 4 cooperative vehicles (Nigel) that share limited state information in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial autonomous racing problem of 2 vehicles (F1TENTH) using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted, which allowed robust training and testing of the policies 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#24211;&#26222;&#26364;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#35782;&#21035;&#8220;&#20004;&#20307;&#38382;&#39064;&#8221;&#21644;&#8220;&#22278;&#38480;&#21046;&#19977;&#20307;&#38382;&#39064;&#8221;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#23558;&#20854;&#20840;&#23616;&#32447;&#24615;&#21270;&#25104;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2403.08965</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#24211;&#26222;&#26364;&#29702;&#35770;&#30340;&#36712;&#36947;&#38382;&#39064;&#21160;&#21147;&#23398;&#35782;&#21035;&#19982;&#32447;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Dynamics Identification and Linearization of Orbital Problems using Koopman Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08965
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#24211;&#26222;&#26364;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#35782;&#21035;&#8220;&#20004;&#20307;&#38382;&#39064;&#8221;&#21644;&#8220;&#22278;&#38480;&#21046;&#19977;&#20307;&#38382;&#39064;&#8221;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#23558;&#20854;&#20840;&#23616;&#32447;&#24615;&#21270;&#25104;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33322;&#31354;&#33322;&#22825;&#24037;&#31243;&#21644;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;&#8220;&#20004;&#20307;&#38382;&#39064;&#8221;&#21644;&#8220;&#22278;&#38480;&#21046;&#19977;&#20307;&#38382;&#39064;&#8221;&#30340;&#30740;&#31350;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#21161;&#20110;&#25551;&#36848;&#22825;&#20307;&#21644;&#20154;&#36896;&#21355;&#26143;&#30340;&#36816;&#21160;&#12290;&#38543;&#30528;&#23545;&#21355;&#26143;&#21644;&#21355;&#26143;&#32534;&#38431;&#39134;&#34892;&#30340;&#38656;&#27714;&#26085;&#30410;&#22686;&#38271;&#65292;&#23545;&#36825;&#20123;&#31995;&#32479;&#36827;&#34892;&#24555;&#36895;&#26377;&#25928;&#30340;&#25511;&#21046;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24211;&#26222;&#26364;&#29702;&#35770;&#23454;&#29616;&#8220;&#20004;&#20307;&#38382;&#39064;&#8221;&#21644;&#8220;&#22278;&#38480;&#21046;&#19977;&#20307;&#38382;&#39064;&#8221;&#30340;&#21516;&#26102;&#31995;&#32479;&#35782;&#21035;&#21644;&#20840;&#23616;&#32447;&#24615;&#21270;&#65292;&#21363;&#36890;&#36807;&#32431;&#25968;&#25454;&#39537;&#21160;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#21457;&#29616;&#32447;&#24615;&#24211;&#26222;&#26364;&#31639;&#23376;&#65292;&#24182;&#23558;&#20854;&#20840;&#23616;&#32447;&#24615;&#21270;&#20026;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#65288;LTI&#65289;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08965v1 Announce Type: cross  Abstract: The study of the Two-Body and Circular Restricted Three-Body Problems in the field of aerospace engineering and sciences is deeply important because they help describe the motion of both celestial and artificial satellites. With the growing demand for satellites and satellite formation flying, fast and efficient control of these systems is becoming ever more important. Global linearization of these systems allows engineers to employ methods of control in order to achieve these desired results. We propose a data-driven framework for simultaneous system identification and global linearization of both the Two-Body Problem and Circular Restricted Three-Body Problem via deep learning-based Koopman Theory, i.e., a framework that can identify the underlying dynamics and globally linearize it into a linear time-invariant (LTI) system. The linear Koopman operator is discovered through purely data-driven training of a Deep Neural Network with a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#36798;&#21040;&#36739;&#20302;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.08955</link><description>&lt;p&gt;
&#26397;&#21521;&#39640;&#25928;&#30340;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#65306;&#19968;&#20010;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#36798;&#21040;&#36739;&#20302;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20351;&#24471;&#33258;&#20027;&#26234;&#33021;&#20307;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20114;&#21160;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#36845;&#20195;&#22797;&#26434;&#24230;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#24179;&#34913;&#20102;&#26399;&#26395;&#22238;&#25253;&#21644;&#39118;&#38505;&#65292;&#20855;&#26377;&#20135;&#29983;&#27010;&#29575;&#40065;&#26834;&#31574;&#30053;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;REINFORCE&#31639;&#27861;&#24182;&#37319;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;$\mathcal{O}(\epsilon^{-2})$&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#20197;&#36798;&#21040;$\epsilon$-&#36817;&#20284;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#65288;FOSP&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#39118;&#38505;&#25935;&#24863;&#31639;&#27861;&#26159;&#21542;&#21487;&#20197;&#27604;&#39118;&#38505;&#20013;&#24615;&#31639;&#27861;&#23454;&#29616;&#26356;&#22909;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08955v1 Announce Type: cross  Abstract: Reinforcement Learning (RL) has shown exceptional performance across various applications, enabling autonomous agents to learn optimal policies through interaction with their environments. However, traditional RL frameworks often face challenges in terms of iteration complexity and robustness. Risk-sensitive RL, which balances expected return and risk, has been explored for its potential to yield probabilistically robust policies, yet its iteration complexity analysis remains underexplored. In this study, we conduct a thorough iteration complexity analysis for the risk-sensitive policy gradient method, focusing on the REINFORCE algorithm and employing the exponential utility function. We obtain an iteration complexity of $\mathcal{O}(\epsilon^{-2})$ to reach an $\epsilon$-approximate first-order stationary point (FOSP). We investigate whether risk-sensitive algorithms can achieve better iteration complexity compared to their risk-neutr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32676;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20004;&#32423;&#26377;&#38480;&#21644;&#20985;&#20984;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#32467;&#26500;&#21644;&#38543;&#26426;&#26041;&#24046;&#20943;&#23567;&#38236;&#20687;Prox&#31639;&#27861;&#65292;&#23454;&#29616;&#23545;&#25152;&#26377;&#32452;&#30340;&#26041;&#24046;&#20943;&#23569;&#65292;&#24182;&#25903;&#25345;&#38750;&#24658;&#23450;&#23398;&#20064;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.03562</link><description>&lt;p&gt;
&#39640;&#25928;&#31639;&#27861;&#29992;&#20110;&#32463;&#39564;&#32676;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#21450;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
Efficient Algorithms for Empirical Group Distributional Robust Optimization and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03562
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32676;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20004;&#32423;&#26377;&#38480;&#21644;&#20985;&#20984;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#32467;&#26500;&#21644;&#38543;&#26426;&#26041;&#24046;&#20943;&#23567;&#38236;&#20687;Prox&#31639;&#27861;&#65292;&#23454;&#29616;&#23545;&#25152;&#26377;&#32452;&#30340;&#26041;&#24046;&#20943;&#23569;&#65292;&#24182;&#25903;&#25345;&#38750;&#24658;&#23450;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32676;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;GDRO&#65289;&#30340;&#32463;&#39564;&#23545;&#24212;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;$m$&#20010;&#19981;&#21516;&#32452;&#20013;&#30340;&#26368;&#22823;&#32463;&#39564;&#39118;&#38505;&#12290;&#25105;&#20204;&#23558;&#32463;&#39564;GDRO&#34920;&#36848;&#20026;$\textit{&#20004;&#32423;}$&#26377;&#38480;&#21644;&#20985;&#20984;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#38543;&#26426;&#26041;&#24046;&#20943;&#23567;&#38236;&#20687;Prox&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#36880;&#32452;&#25277;&#26679;&#25216;&#26415;&#26500;&#24314;&#20102;&#38543;&#26426;&#26799;&#24230;&#65292;&#24182;&#20026;&#25152;&#26377;&#32452;&#25191;&#34892;&#26041;&#24046;&#20943;&#23569;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#32463;&#39564;GDRO&#30340;$\textit{&#20004;&#32423;}$&#26377;&#38480;&#21644;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#19968;&#32034;&#24341;&#20559;&#31227;&#21152;&#26435;&#24179;&#22343;&#26469;&#35745;&#31639;&#24555;&#29031;&#21644;&#38236;&#20687;&#24555;&#29031;&#28857;&#65292;&#36825;&#20351;&#25105;&#20204;&#19982;&#26420;&#32032;&#30340;&#36941;&#21382;&#24179;&#22343;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36824;&#25903;&#25345;&#38750;&#24658;&#23450;&#23398;&#20064;&#29575;&#65292;&#36825;&#19982;&#29616;&#26377;&#25991;&#29486;&#19981;&#21516;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#26399;&#26395;&#21644;&#39640;&#27010;&#29575;&#25910;&#25947;&#20445;&#35777;&#65292;&#23637;&#31034;&#20986;$\m
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03562v1 Announce Type: new  Abstract: We investigate the empirical counterpart of group distributionally robust optimization (GDRO), which aims to minimize the maximal empirical risk across $m$ distinct groups. We formulate empirical GDRO as a $\textit{two-level}$ finite-sum convex-concave minimax optimization problem and develop a stochastic variance reduced mirror prox algorithm. Unlike existing methods, we construct the stochastic gradient by per-group sampling technique and perform variance reduction for all groups, which fully exploits the $\textit{two-level}$ finite-sum structure of empirical GDRO. Furthermore, we compute the snapshot and mirror snapshot point by a one-index-shifted weighted average, which distinguishes us from the naive ergodic average. Our algorithm also supports non-constant learning rates, which is different from existing literature. We establish convergence guarantees both in expectation and with high probability, demonstrating a complexity of $\m
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#32676;&#20307;&#27169;&#22411;&#65292;&#26377;&#25928;&#35299;&#24320;&#24739;&#32773;&#23646;&#24615;&#19982;&#30456;&#26426;&#25928;&#26524;&#65292;&#23454;&#29616;&#21487;&#25511;&#19988;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.19186</link><description>&lt;p&gt;
&#29992;&#29983;&#25104;&#27169;&#22411;&#35299;&#24320;&#35270;&#32593;&#33180;&#22270;&#20687;&#30340;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Disentangling representations of retinal images with generative models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19186
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#32676;&#20307;&#27169;&#22411;&#65292;&#26377;&#25928;&#35299;&#24320;&#24739;&#32773;&#23646;&#24615;&#19982;&#30456;&#26426;&#25928;&#26524;&#65292;&#23454;&#29616;&#21487;&#25511;&#19988;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#22312;&#26089;&#26399;&#26816;&#27979;&#30524;&#37096;&#30142;&#30149;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#29978;&#33267;&#34920;&#26126;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#20123;&#22270;&#20687;&#36824;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;&#24515;&#34880;&#31649;&#39118;&#38505;&#22240;&#32032;&#21644;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22270;&#20687;&#21463;&#25216;&#26415;&#22240;&#32032;&#30340;&#24433;&#21709;&#21487;&#33021;&#23545;&#30524;&#31185;&#39046;&#22495;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26500;&#25104;&#25361;&#25112;&#12290;&#20363;&#22914;&#65292;&#22823;&#22411;&#24213;&#22270;&#38431;&#21015;&#24448;&#24448;&#21463;&#21040;&#30456;&#26426;&#31867;&#22411;&#12289;&#22270;&#20687;&#36136;&#37327;&#25110;&#29031;&#26126;&#27700;&#24179;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#23384;&#22312;&#23398;&#20064;&#24555;&#25463;&#26041;&#24335;&#32780;&#19981;&#26159;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#32972;&#21518;&#22240;&#26524;&#20851;&#31995;&#30340;&#39118;&#38505;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#32676;&#20307;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35299;&#24320;&#20102;&#24739;&#32773;&#23646;&#24615;&#19982;&#30456;&#26426;&#25928;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21487;&#25511;&#19988;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36317;&#31163;&#30456;&#20851;&#24615;&#30340;&#26032;&#39062;&#35299;&#24320;&#25439;&#22833;&#12290;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19186v1 Announce Type: cross  Abstract: Retinal fundus images play a crucial role in the early detection of eye diseases and, using deep learning approaches, recent studies have even demonstrated their potential for detecting cardiovascular risk factors and neurological disorders. However, the impact of technical factors on these images can pose challenges for reliable AI applications in ophthalmology. For example, large fundus cohorts are often confounded by factors like camera type, image quality or illumination level, bearing the risk of learning shortcuts rather than the causal relationships behind the image generation process. Here, we introduce a novel population model for retinal fundus images that effectively disentangles patient attributes from camera effects, thus enabling controllable and highly realistic image generation. To achieve this, we propose a novel disentanglement loss based on distance correlation. Through qualitative and quantitative analyses, we demon
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#21644;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#20174;&#8220;&#36830;&#25509;&#24615;&#8221;&#35270;&#35282;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21892;&#26412;&#22320;&#27169;&#22411;&#38388;&#30340;&#36830;&#25509;&#24615;&#20197;&#29983;&#25104;&#26356;&#20855;&#27867;&#21270;&#33021;&#21147;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18949</link><description>&lt;p&gt;
&#25552;&#39640;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#30340;&#32676;&#32452;&#36830;&#25509;&#24615;&#20197;&#23454;&#29616;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Group Connectivity for Generalization of Federated Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18949
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#21644;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#20174;&#8220;&#36830;&#25509;&#24615;&#8221;&#35270;&#35282;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21892;&#26412;&#22320;&#27169;&#22411;&#38388;&#30340;&#36830;&#25509;&#24615;&#20197;&#29983;&#25104;&#26356;&#20855;&#27867;&#21270;&#33021;&#21147;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#28041;&#21450;&#22810;&#20010;&#24322;&#26500;&#23458;&#25143;&#31471;&#36890;&#36807;&#36845;&#20195;&#26412;&#22320;&#26356;&#26032;&#21644;&#27169;&#22411;&#34701;&#21512;&#20849;&#21516;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#30456;&#27604;&#65292;FL&#30340;&#20840;&#23616;&#27169;&#22411;&#30340;&#27867;&#21270;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#65292;&#36825;&#26159;&#20854;&#22312;&#26356;&#24191;&#27867;&#24212;&#29992;&#20013;&#30340;&#29942;&#39048;&#12290;&#26412;&#25991;&#36890;&#36807;&#22522;&#26412;&#30340;&#8220;&#36830;&#25509;&#24615;&#8221;&#35270;&#35282;&#30740;&#31350;&#21644;&#25913;&#36827;FL&#30340;&#27867;&#21270;&#65292;&#21363;&#26412;&#22320;&#27169;&#22411;&#22312;&#21442;&#25968;&#21306;&#22495;&#20013;&#22914;&#20309;&#36830;&#25509;&#24182;&#34701;&#21512;&#20026;&#27867;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#26415;&#35821;&#8220;&#36830;&#25509;&#24615;&#8221;&#28304;&#33258;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#65288;LMC&#65289;&#65292;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#31181;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#65288;&#20363;&#22914;&#27169;&#24335;&#65289;&#30340;&#20869;&#25554;&#25439;&#22833;&#26223;&#35266;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22266;&#23450;&#30340;&#38170;&#23450;&#27169;&#22411;&#26469;&#30740;&#31350;&#36830;&#25509;&#24615;&#30340;&#20256;&#36882;&#24615;&#36136;&#65292;&#20174;&#20004;&#20010;&#27169;&#22411;&#65288;LMC&#65289;&#21040;&#19968;&#32452;&#27169;&#22411;&#65288;FL&#20013;&#30340;&#27169;&#22411;&#34701;&#21512;&#65289;&#12290;&#26681;&#25454;&#25152;&#21457;&#29616;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18949v1 Announce Type: new  Abstract: Federated learning (FL) involves multiple heterogeneous clients collaboratively training a global model via iterative local updates and model fusion. The generalization of FL's global model has a large gap compared with centralized training, which is its bottleneck for broader applications. In this paper, we study and improve FL's generalization through a fundamental ``connectivity'' perspective, which means how the local models are connected in the parameter region and fused into a generalized global model. The term ``connectivity'' is derived from linear mode connectivity (LMC), studying the interpolated loss landscape of two different solutions (e.g., modes) of neural networks. Bridging the gap between LMC and FL, in this paper, we leverage fixed anchor models to empirically and theoretically study the transitivity property of connectivity from two models (LMC) to a group of models (model fusion in FL). Based on the findings, we propo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#23558;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#36864;&#21270;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#20013;&#12290;&#36890;&#36807;&#32771;&#34385;&#20855;&#26377;&#38646;&#24130;&#25351;&#25968;&#20026;2&#30340;&#38646;&#24130;&#22522;&#21521;&#37327;&#65292;&#21487;&#20197;&#27867;&#21270;&#22522;&#20110;&#20108;&#27425;&#25968;&#30340;&#26041;&#27861;&#24182;&#25429;&#25417;&#23454;&#20307;&#23884;&#20837;&#20013;&#32570;&#20047;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#27169;&#24335;&#12290;&#30740;&#31350;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#27169;&#22411;&#26469;&#21457;&#29616;&#20195;&#25968;&#30340;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#38646;&#24130;&#21521;&#37327;&#26377;&#21161;&#20110;&#25429;&#25417;&#23454;&#20307;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.04870</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#36864;&#21270;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#20013;
&lt;/p&gt;
&lt;p&gt;
Embedding Knowledge Graphs in Degenerate Clifford Algebras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04870
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#23558;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#36864;&#21270;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#20013;&#12290;&#36890;&#36807;&#32771;&#34385;&#20855;&#26377;&#38646;&#24130;&#25351;&#25968;&#20026;2&#30340;&#38646;&#24130;&#22522;&#21521;&#37327;&#65292;&#21487;&#20197;&#27867;&#21270;&#22522;&#20110;&#20108;&#27425;&#25968;&#30340;&#26041;&#27861;&#24182;&#25429;&#25417;&#23454;&#20307;&#23884;&#20837;&#20013;&#32570;&#20047;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#27169;&#24335;&#12290;&#30740;&#31350;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#27169;&#22411;&#26469;&#21457;&#29616;&#20195;&#25968;&#30340;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#38646;&#24130;&#21521;&#37327;&#26377;&#21161;&#20110;&#25429;&#25417;&#23454;&#20307;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#26159;&#23454;&#25968;&#12289;&#22797;&#25968;&#21644;&#22235;&#20803;&#25968;&#30340;&#33258;&#28982;&#25512;&#24191;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#32972;&#26223;&#19979;&#65292;&#21482;&#26377;&#24418;&#24335;&#20026;$Cl_{p,q}$&#65288;&#21363;&#27809;&#26377;&#38646;&#24130;&#22522;&#21521;&#37327;&#30340;&#20195;&#25968;&#65289;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#21463;&#21040;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#38646;&#24130;&#22522;&#21521;&#37327;&#65292;&#20854;&#24130;&#25351;&#25968;&#20026;2&#12290;&#22312;&#36825;&#20123;&#31354;&#38388;&#20013;&#65292;&#34987;&#31216;&#20026;$Cl_{p,q,r}$&#65292;&#21487;&#20197;&#27867;&#21270;&#22522;&#20110;&#20108;&#27425;&#25968;&#30340;&#26041;&#27861;&#65288;&#26080;&#27861;&#20351;&#29992;$Cl_{p,q}$&#36827;&#34892;&#24314;&#27169;&#65289;&#24182;&#25429;&#25417;&#28304;&#20110;&#23454;&#25968;&#21644;&#22797;&#25968;&#37096;&#20998;&#38388;&#32570;&#20047;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#23454;&#20307;&#23884;&#20837;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#27169;&#22411;&#26469;&#21457;&#29616;&#21442;&#25968;$p$&#65292;$q$&#21644;$r$&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#20351;&#29992;&#36138;&#23146;&#25628;&#32034;&#20248;&#21270;$p$&#65292;$q$&#21644;$r$&#12290;&#31532;&#20108;&#20010;&#27169;&#22411;&#22522;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#30340;&#36755;&#20837;&#30693;&#35782;&#22270;&#35889;&#30340;&#23884;&#20837;&#26469;&#39044;&#27979;$(p, q, r)$&#12290;&#25105;&#20204;&#22312;&#19971;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#38646;&#24130;&#21521;&#37327;&#26377;&#21161;&#20110;&#25429;&#25417;&#23454;&#20307;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clifford algebras are a natural generalization of the real numbers, the complex numbers, and the quaternions. So far, solely Clifford algebras of the form $Cl_{p,q}$ (i.e., algebras without nilpotent base vectors) have been studied in the context of knowledge graph embeddings. We propose to consider nilpotent base vectors with a nilpotency index of two. In these spaces, denoted $Cl_{p,q,r}$, allows generalizing over approaches based on dual numbers (which cannot be modelled using $Cl_{p,q}$) and capturing patterns that emanate from the absence of higher-order interactions between real and complex parts of entity embeddings. We design two new models for the discovery of the parameters $p$, $q$, and $r$. The first model uses a greedy search to optimize $p$, $q$, and $r$. The second predicts $(p, q,r)$ based on an embedding of the input knowledge graph computed using neural networks. The results of our evaluation on seven benchmark datasets suggest that nilpotent vectors can help capture 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#21457;&#29616;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#23384;&#22312;&#30340;&#8220;Z-&#24726;&#35770;&#8221;&#38480;&#21046;&#20102;&#20854;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MQuinE&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#65292;MQuinE&#25104;&#21151;&#35299;&#20915;&#20102;Z-&#24726;&#35770;&#65292;&#24182;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03583</link><description>&lt;p&gt;
MQuinE:&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#20013;&#8220;Z-&#24726;&#35770;&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
MQuinE: a cure for "Z-paradox'' in knowledge graph embedding models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03583
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#21457;&#29616;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#23384;&#22312;&#30340;&#8220;Z-&#24726;&#35770;&#8221;&#38480;&#21046;&#20102;&#20854;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MQuinE&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#65292;MQuinE&#25104;&#21151;&#35299;&#20915;&#20102;Z-&#24726;&#35770;&#65292;&#24182;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#27169;&#22411;&#22312;&#35768;&#22810;&#30693;&#35782;&#22270;&#35889;&#20219;&#21153;&#65292;&#21253;&#25324;&#38142;&#25509;&#39044;&#27979;&#21644;&#20449;&#24687;&#26816;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;KGE&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#27969;&#34892;&#30340;&#29616;&#26377;KGE&#27169;&#22411;&#23384;&#22312;&#34920;&#36798;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#31216;&#20026;&#8220;Z-&#24726;&#35770;&#8221;&#12290;&#21463;&#21040;Z-&#24726;&#35770;&#30340;&#23384;&#22312;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KGE&#27169;&#22411;&#65292;&#31216;&#20026;MQuinE&#65292;&#22312;&#19981;&#21463;Z-&#24726;&#35770;&#30340;&#22256;&#25200;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#26469;&#27169;&#25311;&#21508;&#31181;&#20851;&#31995;&#27169;&#24335;&#65292;&#21253;&#25324;&#23545;&#31216;/&#38750;&#23545;&#31216;&#65292;&#36870;&#21521;&#65292;1-N/N-1/N-N&#21644;&#32452;&#21512;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#12290;&#23545;&#23454;&#38469;&#30693;&#35782;&#24211;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Z-&#24726;&#35770;&#30830;&#23454;&#38477;&#20302;&#20102;&#29616;&#26377;KGE&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#33021;&#23548;&#33268;&#26576;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#36229;&#36807;20&#65285;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;MQuinE&#21487;&#20197;&#20943;&#36731;Z-&#24726;&#35770;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#22312;&#38142;&#25509;&#39044;&#27979;&#26041;&#38754;&#20197;&#26126;&#26174;&#20248;&#21183;&#36229;&#36234;&#29616;&#26377;&#30340;KGE&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) models achieved state-of-the-art results on many knowledge graph tasks including link prediction and information retrieval. Despite the superior performance of KGE models in practice, we discover a deficiency in the expressiveness of some popular existing KGE models called \emph{Z-paradox}. Motivated by the existence of Z-paradox, we propose a new KGE model called \emph{MQuinE} that does not suffer from Z-paradox while preserves strong expressiveness to model various relation patterns including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with theoretical justification. Experiments on real-world knowledge bases indicate that Z-paradox indeed degrades the performance of existing KGE models, and can cause more than 20\% accuracy drop on some challenging test samples. Our experiments further demonstrate that MQuinE can mitigate the negative impact of Z-paradox and outperform existing KGE models by a visible margin on link prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#32852;&#21512;&#28304;-&#20449;&#36947;&#32534;&#30721;&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#22122;&#22768;&#26080;&#32447;&#20449;&#36947;&#19978;&#36827;&#34892;&#22270;&#20687;&#20256;&#36755;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#33539;&#22260;-&#38646;&#31354;&#38388;&#20998;&#35299;&#21644;&#36880;&#27493;&#20248;&#21270;&#38646;&#31354;&#38388;&#20869;&#23481;&#65292;&#23454;&#29616;&#20102;&#22312;&#22833;&#30495;&#21644;&#24863;&#30693;&#36136;&#37327;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.15889</link><description>&lt;p&gt;
&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#39640;&#24863;&#30693;&#36136;&#37327;&#30340;&#26080;&#32447;&#22270;&#20687;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
High Perceptual Quality Wireless Image Delivery with Denoising Diffusion Models. (arXiv:2309.15889v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#32852;&#21512;&#28304;-&#20449;&#36947;&#32534;&#30721;&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#22122;&#22768;&#26080;&#32447;&#20449;&#36947;&#19978;&#36827;&#34892;&#22270;&#20687;&#20256;&#36755;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#33539;&#22260;-&#38646;&#31354;&#38388;&#20998;&#35299;&#21644;&#36880;&#27493;&#20248;&#21270;&#38646;&#31354;&#38388;&#20869;&#23481;&#65292;&#23454;&#29616;&#20102;&#22312;&#22833;&#30495;&#21644;&#24863;&#30693;&#36136;&#37327;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32852;&#21512;&#28304;-&#20449;&#36947;&#32534;&#30721;&#65288;DeepJSCC&#65289;&#20197;&#21450;&#25509;&#25910;&#31471;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#22312;&#22122;&#22768;&#26080;&#32447;&#20449;&#36947;&#19978;&#36827;&#34892;&#22270;&#20687;&#20256;&#36755;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#22312;&#23454;&#38469;&#26377;&#38480;&#22359;&#38271;&#24230;&#30340;&#24773;&#20917;&#19979;&#30340;&#24863;&#30693;&#22833;&#30495;&#26435;&#34913;&#38382;&#39064;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20998;&#31163;&#30340;&#28304;&#32534;&#30721;&#21644;&#20449;&#36947;&#32534;&#30721;&#21487;&#33021;&#20250;&#39640;&#24230;&#19981;&#29702;&#24819;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#30446;&#26631;&#22270;&#20687;&#30340;&#33539;&#22260;-&#38646;&#31354;&#38388;&#20998;&#35299;&#30340;&#26032;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#32534;&#30721;&#21518;&#20256;&#36755;&#22270;&#20687;&#30340;&#33539;&#22260;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;DDPM&#36880;&#27493;&#20248;&#21270;&#20854;&#38646;&#31354;&#38388;&#20869;&#23481;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#26631;&#20934;&#30340;DeepJSCC&#21644;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#24335;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#37325;&#26500;&#22270;&#20687;&#30340;&#22833;&#30495;&#21644;&#24863;&#30693;&#36136;&#37327;&#26041;&#38754;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#20026;&#20102;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#21487;&#37325;&#29616;&#24615;&#65292;&#25105;&#20204;&#23558;&#20844;&#24320;&#20998;&#20139;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the image transmission problem over a noisy wireless channel via deep learning-based joint source-channel coding (DeepJSCC) along with a denoising diffusion probabilistic model (DDPM) at the receiver. Specifically, we are interested in the perception-distortion trade-off in the practical finite block length regime, in which separate source and channel coding can be highly suboptimal. We introduce a novel scheme that utilizes the range-null space decomposition of the target image. We transmit the range-space of the image after encoding and employ DDPM to progressively refine its null space contents. Through extensive experiments, we demonstrate significant improvements in distortion and perceptual quality of reconstructed images compared to standard DeepJSCC and the state-of-the-art generative learning-based method. We will publicly share our source code to facilitate further research and reproducibility.
&lt;/p&gt;</description></item><item><title>&#19968;&#31867;&#36793;&#30028;&#21093;&#31163;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#20351;&#29992;&#20102;&#24179;&#22343;&#26377;&#31526;&#21495;&#36317;&#31163;&#21644;&#28789;&#27963;&#30340;&#36793;&#30028;&#29983;&#25104;&#26041;&#27861;&#12290;&#22312;&#26080;&#24322;&#24120;&#20540;&#21644;&#26377;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#31867;&#36793;&#30028;&#21093;&#31163;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05630</link><description>&lt;p&gt;
&#36793;&#30028;&#21093;&#31163;&#65306;&#20351;&#29992;&#19968;&#31867;&#21093;&#31163;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Boundary Peeling: Outlier Detection Method Using One-Class Peeling. (arXiv:2309.05630v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05630
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31867;&#36793;&#30028;&#21093;&#31163;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#20351;&#29992;&#20102;&#24179;&#22343;&#26377;&#31526;&#21495;&#36317;&#31163;&#21644;&#28789;&#27963;&#30340;&#36793;&#30028;&#29983;&#25104;&#26041;&#27861;&#12290;&#22312;&#26080;&#24322;&#24120;&#20540;&#21644;&#26377;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#31867;&#36793;&#30028;&#21093;&#31163;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#22312;&#25968;&#25454;&#20998;&#26512;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#38454;&#27573;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#20805;&#28385;&#27963;&#21147;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#19968;&#20010;&#22909;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#24212;&#35813;&#20855;&#22791;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#23545;&#35843;&#21442;&#36873;&#25321;&#40065;&#26834;&#12289;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#34920;&#29616;&#31283;&#23450;&#31561;&#29305;&#28857;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31867;&#36793;&#30028;&#21093;&#31163;&#65292;&#19968;&#31181;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;&#19968;&#31867;&#36793;&#30028;&#21093;&#31163;&#20351;&#29992;&#20102;&#19968;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#19981;&#26029;&#21093;&#31163;&#30340;&#12289;&#28789;&#27963;&#30340;&#36793;&#30028;&#29983;&#25104;&#30340;&#24179;&#22343;&#26377;&#31526;&#21495;&#36317;&#31163;&#12290;&#19968;&#31867;&#36793;&#30028;&#21093;&#31163;&#20855;&#26377;&#40065;&#26834;&#30340;&#36229;&#21442;&#25968;&#35774;&#32622;&#65292;&#24182;&#19988;&#20026;&#20102;&#22686;&#21152;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#38598;&#25104;&#26041;&#27861;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#27169;&#25311;&#20013;&#65292;&#19968;&#31867;&#36793;&#30028;&#21093;&#31163;&#22312;&#27809;&#26377;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#25152;&#26377;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#26377;&#24322;&#24120;&#20540;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#27491;&#30830;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised outlier detection constitutes a crucial phase within data analysis and remains a dynamic realm of research. A good outlier detection algorithm should be computationally efficient, robust to tuning parameter selection, and perform consistently well across diverse underlying data distributions. We introduce One-Class Boundary Peeling, an unsupervised outlier detection algorithm. One-class Boundary Peeling uses the average signed distance from iteratively-peeled, flexible boundaries generated by one-class support vector machines. One-class Boundary Peeling has robust hyperparameter settings and, for increased flexibility, can be cast as an ensemble method. In synthetic data simulations One-Class Boundary Peeling outperforms all state of the art methods when no outliers are present while maintaining comparable or superior performance in the presence of outliers, as compared to benchmark methods. One-Class Boundary Peeling performs competitively in terms of correct classificati
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#24452;&#21521;&#22522;&#20989;&#25968;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#22238;&#24402;&#38382;&#39064;&#12290;&#35813;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#22320;&#32452;&#21512;&#22810;&#20010;&#20551;&#35774;&#39044;&#27979;&#22120;&#65292;&#24182;&#36890;&#36807;&#25554;&#20540;&#36924;&#36817;&#22810;&#20010;&#20551;&#35774;&#30446;&#26631;&#20998;&#24067;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00781</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#24452;&#21521;&#22522;&#20989;&#25968;&#32593;&#32476;&#65306;&#29992;&#20110;&#22810;&#20551;&#35774;&#39044;&#27979;&#30340;&#22810;&#26679;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Structured Radial Basis Function Network: Modelling Diversity for Multiple Hypotheses Prediction. (arXiv:2309.00781v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00781
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#24452;&#21521;&#22522;&#20989;&#25968;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#22238;&#24402;&#38382;&#39064;&#12290;&#35813;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#22320;&#32452;&#21512;&#22810;&#20010;&#20551;&#35774;&#39044;&#27979;&#22120;&#65292;&#24182;&#36890;&#36807;&#25554;&#20540;&#36924;&#36817;&#22810;&#20010;&#20551;&#35774;&#30446;&#26631;&#20998;&#24067;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22238;&#24402;&#23545;&#20110;&#39044;&#27979;&#38750;&#24179;&#31283;&#36807;&#31243;&#25110;&#20855;&#26377;&#22797;&#26434;&#20998;&#24067;&#30340;&#38382;&#39064;&#38750;&#24120;&#37325;&#35201;&#12290;&#21487;&#20197;&#36890;&#36807;&#22810;&#20551;&#35774;&#26694;&#26550;&#26469;&#22788;&#29702;&#65292;&#20294;&#22312;&#23398;&#20064;&#27169;&#22411;&#20013;&#26377;&#25928;&#22320;&#32452;&#21512;&#23427;&#20204;&#26159;&#26377;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#24452;&#21521;&#22522;&#20989;&#25968;&#32593;&#32476;&#65292;&#20316;&#20026;&#22810;&#20551;&#35774;&#39044;&#27979;&#22120;&#30340;&#38598;&#21512;&#65292;&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#12290;&#36825;&#20123;&#39044;&#27979;&#22120;&#26159;&#20219;&#20309;&#31867;&#22411;&#30340;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#20197;&#24418;&#25104;&#20197;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25439;&#22833;&#20026;&#20989;&#25968;&#30340;&#37325;&#24515;&#32500;&#35834;&#22270;&#20998;&#21106;&#12290;&#35777;&#26126;&#20102;&#36825;&#20010;&#32467;&#26500;&#21270;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25554;&#20540;&#36825;&#20010;&#20998;&#21106;&#65292;&#24182;&#19988;&#36924;&#36817;&#22810;&#20010;&#20551;&#35774;&#30446;&#26631;&#20998;&#24067;&#65292;&#24182;&#19988;&#31561;&#20215;&#20110;&#25554;&#20540;&#39044;&#27979;&#22120;&#30340;&#20803;&#25439;&#22833;&#65292;&#25439;&#22833;&#26159;&#25554;&#20540;&#35823;&#24046;&#30340;&#38646;&#38598;&#12290;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#22120;&#21644;&#22522;&#20989;&#25968;&#20013;&#24515;&#20043;&#38388;&#20855;&#26377;&#22266;&#23450;&#28857;&#36845;&#20195;&#31639;&#27861;&#12290;&#21487;&#20197;&#36890;&#36807;&#25130;&#26029;&#20998;&#21106;&#26684;&#24335;&#26469;&#21442;&#25968;&#21270;&#22320;&#25511;&#21046;&#23398;&#20064;&#20013;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal regression is important in forecasting nonstationary processes or with a complex mixture of distributions. It can be tackled with multiple hypotheses frameworks but with the difficulty of combining them efficiently in a learning model. A Structured Radial Basis Function Network is presented as an ensemble of multiple hypotheses predictors for regression problems. The predictors are regression models of any type that can form centroidal Voronoi tessellations which are a function of their losses during training. It is proved that this structured model can efficiently interpolate this tessellation and approximate the multiple hypotheses target distribution and is equivalent to interpolating the meta-loss of the predictors, the loss being a zero set of the interpolation error. This model has a fixed-point iteration algorithm between the predictors and the centers of the basis functions. Diversity in learning can be controlled parametrically by truncating the tessellation format
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#21512;&#24182;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#26356;&#20026;&#26174;&#33879;&#65292;&#22240;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#20351;&#24471;&#21512;&#24182;&#26356;&#21152;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25968;&#25454;&#38598;&#21387;&#32553;&#21019;&#24314;&#30340;&#21387;&#32553;&#25968;&#25454;&#38598;&#21487;&#20197;&#20316;&#20026;&#21407;&#25968;&#25454;&#38598;&#30340;&#26367;&#20195;&#21697;&#12290;</title><link>http://arxiv.org/abs/2306.05641</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#32622;&#25442;&#23545;&#19981;&#21516;&#25968;&#25454;&#38598;&#38388;&#27169;&#22411;&#21512;&#24182;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting Permutation Symmetry for Merging Models between Different Datasets. (arXiv:2306.05641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#21512;&#24182;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#26356;&#20026;&#26174;&#33879;&#65292;&#22240;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#20351;&#24471;&#21512;&#24182;&#26356;&#21152;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25968;&#25454;&#38598;&#21387;&#32553;&#21019;&#24314;&#30340;&#21387;&#32553;&#25968;&#25454;&#38598;&#21487;&#20197;&#20316;&#20026;&#21407;&#25968;&#25454;&#38598;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21512;&#24182;&#26159;&#19968;&#31181;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#35757;&#32451;&#27169;&#22411;&#30340;&#26435;&#37325;&#26469;&#21019;&#24314;&#26032;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#27169;&#22411;&#21512;&#24182;&#23545;&#20110;&#19981;&#21516;&#38543;&#26426;&#25968;&#35757;&#32451;&#27169;&#22411;&#30340;&#21333;&#19968;&#25968;&#25454;&#38598;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26159;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#21512;&#24182;&#21364;&#24456;&#22256;&#38590;&#12290;&#23558;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#21512;&#24182;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#65292;&#20294;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#25506;&#35752;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#38388;&#21512;&#24182;&#27169;&#22411;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#38543;&#30528;&#25968;&#25454;&#38598;&#30340;&#24046;&#24322;&#36234;&#22823;&#65292;&#21512;&#24182;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#24471;&#26356;&#20026;&#26174;&#33879;&#65292;&#32780;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#20351;&#24471;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#27169;&#22411;&#21512;&#24182;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#21512;&#24182;&#30340;&#27169;&#22411;&#38656;&#35201;&#25968;&#25454;&#38598;&#25165;&#33021;&#23454;&#29616;&#39640;&#31934;&#24230;&#21512;&#24182;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#24403;&#21512;&#24182;&#27169;&#22411;&#26102;&#65292;&#36890;&#36807;&#25968;&#25454;&#38598;&#21387;&#32553;&#21019;&#24314;&#30340;&#21387;&#32553;&#25968;&#25454;&#38598;&#21487;&#20197;&#20316;&#20026;&#21407;&#25968;&#25454;&#38598;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model merging is a new approach to creating a new model by combining the weights of different trained models. Previous studies report that model merging works well for models trained on a single dataset with different random seeds, while model merging between different datasets is difficult. Merging knowledge from different datasets has practical significance, but it has not been well investigated. In this paper, we investigate the properties of merging models between different datasets. Through theoretical and empirical analyses, we find that the accuracy of the merged model decreases more significantly as the datasets diverge more and that the different loss landscapes for each dataset make model merging between different datasets difficult. We also show that merged models require datasets for merging in order to achieve a high accuracy. Furthermore, we show that condensed datasets created by dataset condensation can be used as substitutes for the original datasets when merging model
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38646;&#26679;&#26412;&#20219;&#21153;&#20559;&#22909;&#30340;&#19981;&#31934;&#30830;&#36125;&#21494;&#26031;&#32487;&#32493;&#23398;&#20064;&#65288;IBCL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#20984;&#22771;&#24418;&#24335;&#30340;&#30693;&#35782;&#24211;&#65292;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#33719;&#21462;&#27169;&#22411;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#20559;&#22909;&#65292;&#20351;&#24471;&#22312;&#20855;&#26377;&#22823;&#37327;&#20219;&#21153;&#20559;&#22909;&#30340;&#24773;&#20917;&#19979;&#26356;&#21152;&#21487;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.14782</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#20219;&#21153;&#20559;&#22909;&#30340;&#19981;&#31934;&#30830;&#36125;&#21494;&#26031;&#32487;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Task Preference Addressing Enabled by Imprecise Bayesian Continual Learning. (arXiv:2305.14782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14782
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38646;&#26679;&#26412;&#20219;&#21153;&#20559;&#22909;&#30340;&#19981;&#31934;&#30830;&#36125;&#21494;&#26031;&#32487;&#32493;&#23398;&#20064;&#65288;IBCL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#20984;&#22771;&#24418;&#24335;&#30340;&#30693;&#35782;&#24211;&#65292;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#33719;&#21462;&#27169;&#22411;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#20559;&#22909;&#65292;&#20351;&#24471;&#22312;&#20855;&#26377;&#22823;&#37327;&#20219;&#21153;&#20559;&#22909;&#30340;&#24773;&#20917;&#19979;&#26356;&#21152;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#20110;&#36890;&#29992;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#32487;&#32493;&#23398;&#20064;&#20063;&#20855;&#26377;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#29305;&#24615;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#19981;&#21516;&#20219;&#21153;&#30340;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#24179;&#34913;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#20026;&#20102;&#20248;&#21270;&#24403;&#21069;&#20219;&#21153;&#20998;&#24067;&#65292;&#21487;&#33021;&#38656;&#35201;&#22312;&#19968;&#20123;&#20219;&#21153;&#19978;&#29306;&#29298;&#24615;&#33021;&#20197;&#25552;&#39640;&#20854;&#20182;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#36825;&#24847;&#21619;&#30528;&#23384;&#22312;&#22810;&#20010;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#37117;&#26159;&#26368;&#20248;&#30340;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#33021;&#22815;&#35299;&#20915;&#19981;&#21516;&#30340;&#20219;&#21153;-&#24615;&#33021;&#26435;&#34913;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#35752;&#35770;&#22914;&#20309;&#35757;&#32451;&#29305;&#23450;&#30340;&#27169;&#22411;&#20197;&#28385;&#36275;&#20132;&#26131;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31639;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#37319;&#26679;&#24320;&#38144;-&#22312;&#23384;&#22312;&#22810;&#20010;&#65292;&#21487;&#33021;&#26159;&#26080;&#38480;&#25968;&#37327;&#30340;&#20559;&#22909;&#26102;&#20250;&#20135;&#29983;&#24456;&#22823;&#30340;&#36127;&#25285;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#36125;&#21494;&#26031;&#32487;&#32493;&#23398;&#20064;&#65288;IBCL&#65289;&#12290;&#19968;&#26086;&#26377;&#26032;&#20219;&#21153;&#65292;IBCL&#20250;&#65288;1&#65289;&#26356;&#26032;&#19968;&#20010;&#20197;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#20984;&#22771;&#24418;&#24335;&#23384;&#22312;&#30340;&#30693;&#35782;&#24211;&#65292;&#65288;2&#65289;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#33719;&#21462;&#29305;&#23450;&#27169;&#22411;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;IBCL&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#25968;&#25454;&#23601;&#33021;&#20026;&#19968;&#20010;&#29305;&#23450;&#30340;&#20219;&#21153;&#20559;&#22909;&#29983;&#25104;&#26032;&#30340;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;&#20855;&#26377;&#22823;&#37327;&#20219;&#21153;&#20559;&#22909;&#30340;&#24773;&#20917;&#19979;&#26356;&#21152;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like generic multi-task learning, continual learning has the nature of multi-objective optimization, and therefore faces a trade-off between the performance of different tasks. That is, to optimize for the current task distribution, it may need to compromise performance on some tasks to improve on others. This means there exist multiple models that are each optimal at different times, each addressing a distinct task-performance trade-off. Researchers have discussed how to train particular models to address specific preferences on these trade-offs. However, existing algorithms require additional sample overheads -- a large burden when there are multiple, possibly infinitely many, preferences. As a response, we propose Imprecise Bayesian Continual Learning (IBCL). Upon a new task, IBCL (1) updates a knowledge base in the form of a convex hull of model parameter distributions and (2) obtains particular models to address preferences with zero-shot. That is, IBCL does not require any additi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Shortcut Fine-Tuning&#65288;SFT&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#30452;&#25509;&#26368;&#23567;&#21270;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#65288;IPM&#65289;&#26469;&#23545;DDPM&#37319;&#26679;&#22120;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;DDPM&#37319;&#26679;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2301.13362</link><description>&lt;p&gt;
&#20351;&#29992;Shortcut Fine-Tuning&#20248;&#21270;DDPM&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Optimizing DDPM Sampling with Shortcut Fine-Tuning. (arXiv:2301.13362v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Shortcut Fine-Tuning&#65288;SFT&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#30452;&#25509;&#26368;&#23567;&#21270;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#65288;IPM&#65289;&#26469;&#23545;DDPM&#37319;&#26679;&#22120;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;DDPM&#37319;&#26679;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Shortcut Fine-Tuning&#65288;SFT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39044;&#35757;&#32451;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#30340;&#24555;&#36895;&#37319;&#26679;&#25361;&#25112;&#12290;SFT&#25552;&#20513;&#36890;&#36807;&#30452;&#25509;&#26368;&#23567;&#21270;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#65288;IPM&#65289;&#26469;&#23545;DDPM&#37319;&#26679;&#22120;&#36827;&#34892;&#24494;&#35843;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#21521;&#21518;&#25193;&#25955;&#36807;&#31243;&#12290;&#36825;&#20351;&#37319;&#26679;&#22120;&#33021;&#22815;&#21457;&#29616;&#19968;&#26465;&#26367;&#20195;&#30340;&#26356;&#39640;&#25928;&#30340;&#37319;&#26679;&#25463;&#24452;&#65292;&#20559;&#31163;&#21521;&#21518;&#25193;&#25955;&#36807;&#31243;&#12290;&#36890;&#36807;&#25511;&#21046;&#35282;&#24230;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;SFT-PG&#65306;&#20351;&#29992;Policy Gradient&#36827;&#34892;&#30340;Shortcut Fine-Tuning&#65292;&#24182;&#35777;&#26126;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#25193;&#25955;&#27169;&#22411;&#30456;&#23545;&#20110;IPM&#30340;&#26799;&#24230;&#19979;&#38477;&#31561;&#20215;&#20110;&#25191;&#34892;Policy Gradient&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23581;&#35797;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#26469;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#29616;&#26377;&#30340;&#24555;&#36895;DDPM&#37319;&#26679;&#22120;&#65292;&#20174;&#32780;&#23548;&#33268;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#36136;&#37327;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose Shortcut Fine-Tuning (SFT), a new approach for addressing the challenge of fast sampling of pretrained Denoising Diffusion Probabilistic Models (DDPMs). SFT advocates for the fine-tuning of DDPM samplers through the direct minimization of Integral Probability Metrics (IPM), instead of learning the backward diffusion process. This enables samplers to discover an alternative and more efficient sampling shortcut, deviating from the backward diffusion process. Inspired by a control perspective, we propose a new algorithm SFT-PG: Shortcut Fine-Tuning with Policy Gradient, and prove that under certain assumptions, gradient descent of diffusion models with respect to IPM is equivalent to performing policy gradient. To our best knowledge, this is the first attempt to utilize reinforcement learning (RL) methods to train diffusion models. Through empirical evaluation, we demonstrate that our fine-tuning method can further enhance existing fast DDPM samplers, resulting i
&lt;/p&gt;</description></item><item><title>FlexFringe&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#27010;&#29575;&#26377;&#38480;&#33258;&#21160;&#26426;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#36719;&#20214;&#34892;&#20026;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#36890;&#36807;&#23454;&#29616;&#25913;&#36827;&#30340;&#29366;&#24577;&#21512;&#24182;&#31574;&#30053;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#36719;&#20214;&#26085;&#24535;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#23398;&#20064;&#26356;&#23567;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#33021;&#22815;&#25552;&#39640;FlexFringe&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.16331</link><description>&lt;p&gt;
FlexFringe:&#36890;&#36807;&#23398;&#20064;&#27010;&#29575;&#26377;&#38480;&#33258;&#21160;&#26426;&#26469;&#24314;&#27169;&#36719;&#20214;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
FlexFringe: Modeling Software Behavior by Learning Probabilistic Automata. (arXiv:2203.16331v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16331
&lt;/p&gt;
&lt;p&gt;
FlexFringe&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#27010;&#29575;&#26377;&#38480;&#33258;&#21160;&#26426;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#36719;&#20214;&#34892;&#20026;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#36890;&#36807;&#23454;&#29616;&#25913;&#36827;&#30340;&#29366;&#24577;&#21512;&#24182;&#31574;&#30053;&#23454;&#29616;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#36719;&#20214;&#26085;&#24535;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#23398;&#20064;&#26356;&#23567;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#33021;&#22815;&#25552;&#39640;FlexFringe&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;FlexFringe&#20013;&#21487;&#29992;&#30340;&#27010;&#29575;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#23398;&#20064;&#26041;&#27861;&#30340;&#39640;&#25928;&#23454;&#29616;&#12290;&#36825;&#20123;&#23454;&#29616;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#29366;&#24577;&#21512;&#24182;&#31574;&#30053;&#65292;&#21253;&#25324;&#20960;&#31181;&#20462;&#25913;&#20197;&#25552;&#39640;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#33719;&#24471;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#40664;&#35748;&#23454;&#29616;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;FlexFringe&#20174;&#36719;&#20214;&#26085;&#24535;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#36739;&#38590;&#35299;&#37322;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#26356;&#23567;&#12289;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#22914;&#20309;&#25552;&#39640;FlexFringe&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the efficient implementations of probabilistic deterministic finite automaton learning methods available in FlexFringe. These implement well-known strategies for state-merging including several modifications to improve their performance in practice. We show experimentally that these algorithms obtain competitive results and significant improvements over a default implementation. We also demonstrate how to use FlexFringe to learn interpretable models from software logs and use these for anomaly detection. Although less interpretable, we show that learning smaller more convoluted models improves the performance of FlexFringe on anomaly detection, outperforming an existing solution based on neural nets.
&lt;/p&gt;</description></item></channel></rss>