<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#20984;&#38598;&#30340;&#27010;&#29575;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#65292;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#20449;&#20219;&#38598;&#65292;&#24182;&#25512;&#23548;&#20986;bounds&#12290;</title><link>https://rss.arxiv.org/abs/2402.00957</link><description>&lt;p&gt;
&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Credal Learning Theory
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#20984;&#38598;&#30340;&#27010;&#29575;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#65292;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#20449;&#20219;&#38598;&#65292;&#24182;&#25512;&#23548;&#20986;bounds&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#20026;&#20174;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#20013;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#30340;&#39118;&#38505;&#25552;&#20379;&#29702;&#35770;&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#20250;&#21464;&#21270;&#65292;&#23548;&#33268;&#39046;&#22495;&#36866;&#24212;/&#27867;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#8220;&#20449;&#20219;&#8221;&#23398;&#20064;&#29702;&#35770;&#30340;&#22522;&#30784;&#65292;&#20351;&#29992;&#27010;&#29575;&#30340;&#20984;&#38598;&#65288;&#20449;&#20219;&#38598;&#65289;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26679;&#30340;&#20449;&#20219;&#38598;&#21487;&#20197;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#23545;&#20110;&#26377;&#38480;&#20551;&#35774;&#31354;&#38388;&#65288;&#26080;&#35770;&#26159;&#21542;&#21487;&#23454;&#29616;&#65289;&#21644;&#26080;&#38480;&#27169;&#22411;&#31354;&#38388;&#65292;&#25512;&#23548;&#20986;&#30028;&#38480;&#65292;&#36825;&#30452;&#25509;&#25512;&#24191;&#20102;&#32463;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical learning theory is the foundation of machine learning, providing theoretical bounds for the risk of models learnt from a (single) training set, assumed to issue from an unknown probability distribution. In actual deployment, however, the data distribution may (and often does) vary, causing domain adaptation/generalization issues. In this paper we lay the foundations for a `credal' theory of learning, using convex sets of probabilities (credal sets) to model the variability in the data-generating distribution. Such credal sets, we argue, may be inferred from a finite sample of training sets. Bounds are derived for the case of finite hypotheses spaces (both assuming realizability or not) as well as infinite model spaces, which directly generalize classical results.
&lt;/p&gt;</description></item><item><title>fastprop&#26159;&#19968;&#31181;DeepQSPR&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23376;&#32423;&#25551;&#36848;&#31526;&#65292;&#22312;&#26497;&#22823;&#32553;&#30701;&#26102;&#38388;&#20869;&#65292;&#22312;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#24182;&#36229;&#36234;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02058</link><description>&lt;p&gt;
&#20855;&#26377;&#24555;&#36895;prop&#30340;&#21487;&#25512;&#24191;&#12289;&#24555;&#36895;&#21644;&#20934;&#30830;&#30340;DeepQSPR Part 1: &#26694;&#26550;&#21644;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Generalizable, Fast, and Accurate DeepQSPR with fastprop Part 1: Framework and Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02058
&lt;/p&gt;
&lt;p&gt;
fastprop&#26159;&#19968;&#31181;DeepQSPR&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23376;&#32423;&#25551;&#36848;&#31526;&#65292;&#22312;&#26497;&#22823;&#32553;&#30701;&#26102;&#38388;&#20869;&#65292;&#22312;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#24182;&#36229;&#36234;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#32467;&#26500;-&#24615;&#36136;&#20851;&#31995;&#30740;&#31350;&#26088;&#22312;&#23450;&#20041;&#20998;&#23376;&#32467;&#26500;&#19982;&#20219;&#24847;&#24863;&#20852;&#36259;&#30340;&#25968;&#37327;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#21382;&#21490;&#19978;&#65292;&#36825;&#26159;&#36890;&#36807;&#24320;&#21457;&#25551;&#36848;&#31526;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#38656;&#35201;&#26174;&#33879;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#38590;&#20197;&#27867;&#21270;&#12290;&#22240;&#27492;&#65292;&#35813;&#39046;&#22495;&#24050;&#32463;&#28436;&#21464;&#20026;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#65292;&#24182;&#36716;&#20026;&#20351;&#29992;&#39640;&#24230;&#21487;&#25512;&#24191;&#30340;&#23398;&#20064;&#34920;&#31034;&#12290;&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;fastprop&#65292;&#19968;&#31181;DeepQSPR&#26694;&#26550;&#65292;&#20351;&#29992;&#19968;&#32452;&#26126;&#26234;&#30340;&#20998;&#23376;&#32423;&#25551;&#36848;&#31526;&#65292;&#22312;&#26497;&#22823;&#32553;&#30701;&#30340;&#26102;&#38388;&#20869;&#28385;&#36275;&#24182;&#36229;&#36234;&#20102;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;fastprop&#21487;&#20197;&#22312;github&#19978;&#20813;&#36153;&#33719;&#21462;&#65292;&#32593;&#22336;&#20026;github.com/JacksonBurns/fastprop&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02058v1 Announce Type: new  Abstract: Quantitative Structure Property Relationship studies aim to define a mapping between molecular structure and arbitrary quantities of interest. This was historically accomplished via the development of descriptors which requires significant domain expertise and struggles to generalize. Thus the field has morphed into Molecular Property Prediction and been given over to learned representations which are highly generalizable. The paper introduces fastprop, a DeepQSPR framework which uses a cogent set of molecular level descriptors to meet and exceed the performance of learned representations on diverse datasets in dramatically less time. fastprop is freely available on github at github.com/JacksonBurns/fastprop.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23545;&#26469;&#33258;&#21333;&#20010;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#27425;&#36816;&#34892;&#36827;&#34892;&#36731;&#24494;&#24494;&#35843;&#65292;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26399;&#38388;&#30340;&#38543;&#26426;&#24615;&#36873;&#25321;&#26469;&#26174;&#33879;&#24433;&#21709;&#26368;&#32456;&#38598;&#21512;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01542</link><description>&lt;p&gt;
&#36890;&#36807;&#32447;&#19978;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Predicting the Performance of Foundation Models via Agreement-on-the-Line
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23545;&#26469;&#33258;&#21333;&#20010;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#27425;&#36816;&#34892;&#36827;&#34892;&#36731;&#24494;&#24494;&#35843;&#65292;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26399;&#38388;&#30340;&#38543;&#26426;&#24615;&#36873;&#25321;&#26469;&#26174;&#33879;&#24433;&#21709;&#26368;&#32456;&#38598;&#21512;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#26631;&#31614;&#31232;&#32570;&#24773;&#20917;&#19979;&#30340;&#22806;&#37096;&#20998;&#24067;&#24615;&#33021;&#23545;&#20110;&#23433;&#20840;&#37096;&#32626;&#22522;&#30784;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#35266;&#23519;&#21040;&#8220;&#32447;&#19978;&#19968;&#33268;&#24615;&#8221;&#29616;&#35937;&#65292;&#21487;&#20197;&#21033;&#29992;&#23427;&#21487;&#38752;&#22320;&#39044;&#27979;&#26080;&#26631;&#31614;&#30340;&#22806;&#37096;&#20998;&#24067;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#22312;&#20998;&#24067;&#25968;&#25454;&#19978;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22810;&#27425;&#36718;&#25968;&#30340;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#22522;&#30784;&#27169;&#22411;&#32463;&#21382;&#20102;&#20174;&#39044;&#35757;&#32451;&#26435;&#37325;&#20013;&#36827;&#34892;&#26368;&#23567;&#24494;&#35843;&#65292;&#36825;&#21487;&#33021;&#20250;&#20943;&#23569;&#35266;&#23519;&#21040;&#32447;&#19978;&#19968;&#33268;&#24615;&#25152;&#38656;&#30340;&#38598;&#21512;&#22810;&#26679;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20986;&#24403;&#36731;&#24494;&#24494;&#35843;&#25972;&#26469;&#33258;$\textit{&#21333;&#20010;}$&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#27425;&#36816;&#34892;&#26102;&#65292;&#35757;&#32451;&#26399;&#38388;&#30340;&#38543;&#26426;&#24615;&#36873;&#25321;&#65288;&#32447;&#24615;&#22836;&#21021;&#22987;&#21270;&#12289;&#25968;&#25454;&#25490;&#24207;&#21644;&#25968;&#25454;&#23376;&#38598;&#65289;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#31243;&#24230;&#30340;&#32447;&#19978;&#19968;&#33268;&#24615;&#30340;&#26368;&#32456;&#38598;&#21512;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21482;&#26377;&#38543;&#26426;&#22836;&#21021;&#22987;&#21270;&#23601;&#33021;&#26497;&#22823;&#31243;&#24230;&#22320;&#24433;&#21709;&#38598;&#21512;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;&#234;n&#21021;&#21270;&#23601;&#33021;&#20351;&#20135;&#29983;&#30340;&#38598;&#21512;&#20013;&#30340;&#19968;&#33268;&#24615;&#27700;&#24179;&#20135;&#29983;&#24040;&#22823;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01542v1 Announce Type: new  Abstract: Estimating the out-of-distribution performance in regimes where labels are scarce is critical to safely deploy foundation models. Recently, it was shown that ensembles of neural networks observe the phenomena ``agreement-on-the-line'', which can be leveraged to reliably predict OOD performance without labels. However, in contrast to classical neural networks that are trained on in-distribution data from scratch for numerous epochs, foundation models undergo minimal finetuning from heavily pretrained weights, which may reduce the ensemble diversity needed to observe agreement-on-the-line. In our work, we demonstrate that when lightly finetuning multiple runs from a $\textit{single}$ foundation model, the choice of randomness during training (linear head initialization, data ordering, and data subsetting) can lead to drastically different levels of agreement-on-the-line in the resulting ensemble. Surprisingly, only random head initializati
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#21487;&#34892;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20854;&#22312;&#28216;&#25103;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02107</link><description>&lt;p&gt;
&#36845;&#20195;$Q$-&#32593;&#32476;&#65306;&#36229;&#36234;&#21333;&#27493;&#36125;&#23572;&#26364;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Iterated $Q$-Network: Beyond the One-Step Bellman Operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02107
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#21487;&#34892;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20854;&#22312;&#28216;&#25103;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#20381;&#36182;&#20110;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#24212;&#29992;&#65292;&#35813;&#31639;&#23376;&#38656;&#35201;&#20174;&#26679;&#26412;&#20013;&#36827;&#34892;&#36817;&#20284;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#21253;&#25324;&#20132;&#26367;&#24212;&#29992;&#36125;&#23572;&#26364;&#31639;&#23376;&#21644;&#38543;&#21518;&#25237;&#24433;&#27493;&#39588;&#21040;&#32771;&#34385;&#30340;&#20989;&#25968;&#31354;&#38388;&#30340;&#36845;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#19968;&#31995;&#21015;$Q$&#20989;&#25968;&#36924;&#36817;&#65292;&#20854;&#20013;&#27599;&#20010;$Q$&#20989;&#25968;&#37117;&#20316;&#20026;&#19979;&#19968;&#20010;&#20989;&#25968;&#38142;&#20013;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;iQN&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#21487;&#20197;&#26080;&#32541;&#22320;&#29992;&#20110;&#20540;&#22522;&#21644;&#28436;&#21592;-&#35780;&#35770;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;Atari$2600$&#28216;&#25103;&#21644;&#36830;&#32493;&#25511;&#21046;MuJoCo&#29615;&#22659;&#20013;&#22312;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02107v1 Announce Type: cross  Abstract: Value-based Reinforcement Learning (RL) methods rely on the application of the Bellman operator, which needs to be approximated from samples. Most approaches consist of an iterative scheme alternating the application of the Bellman operator and a subsequent projection step onto a considered function space. However, we observe that these algorithms can be improved by considering multiple iterations of the Bellman operator at once. Thus, we introduce iterated $Q$-Networks (iQN), a novel approach that learns a sequence of $Q$-function approximations where each $Q$-function serves as the target for the next one in a chain of consecutive Bellman iterations. We demonstrate that iQN is theoretically sound and show how it can be seamlessly used in value-based and actor-critic methods. We empirically demonstrate its advantages on Atari $2600$ games and in continuous-control MuJoCo environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#21464;&#21387;&#22120;&#23545;&#20869;&#29983;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#24357;&#34917;&#20102;&#20197;&#24448;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#20013;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.19072</link><description>&lt;p&gt;
TimeXer&#65306;&#21033;&#29992;&#22806;&#29983;&#21464;&#37327;&#22686;&#24378;&#21464;&#21387;&#22120;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#21464;&#21387;&#22120;&#23545;&#20869;&#29983;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#24357;&#34917;&#20102;&#20197;&#24448;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#20013;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#23454;&#24212;&#29992;&#30340;&#37096;&#20998;&#35266;&#27979;&#24615;&#36136;&#65292;&#20165;&#19987;&#27880;&#20110;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#65292;&#20063;&#23601;&#26159;&#25152;&#35859;&#30340;&#20869;&#29983;&#21464;&#37327;&#65292;&#36890;&#24120;&#26159;&#19981;&#36275;&#20197;&#20445;&#35777;&#20934;&#30830;&#39044;&#27979;&#30340;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#31995;&#32479;&#36890;&#24120;&#35760;&#24405;&#20026;&#22810;&#20010;&#21464;&#37327;&#65292;&#20854;&#20013;&#22806;&#29983;&#24207;&#21015;&#21487;&#20197;&#20026;&#20869;&#29983;&#21464;&#37327;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#22806;&#37096;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#19982;&#20808;&#21069;&#30830;&#31435;&#30340;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#19981;&#21516;&#65292;&#23427;&#20204;&#35201;&#20040;&#23558;&#25152;&#26377;&#21464;&#37327;&#31561;&#21516;&#23545;&#24453;&#65292;&#35201;&#20040;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#65292;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#19968;&#31181;&#23454;&#38469;&#35774;&#32622;&#65292;&#21363;&#20855;&#26377;&#22806;&#29983;&#21464;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#20869;&#29983;&#21464;&#37327;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#23884;&#20837;&#23618;&#65292;TimeXer&#20351;&#20256;&#32479;&#30340;Transformer&#26550;&#26500;&#20855;&#26377;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19072v1 Announce Type: cross  Abstract: Recent studies have demonstrated remarkable performance in time series forecasting. However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting. Notably, a system is often recorded into multiple variables, where the exogenous series can provide valuable external information for endogenous variables. Thus, unlike prior well-established multivariate or univariate forecasting that either treats all the variables equally or overlooks exogenous information, this paper focuses on a practical setting, which is time series forecasting with exogenous variables. We propose a novel framework, TimeXer, to utilize external information to enhance the forecasting of endogenous variables. With a deftly designed embedding layer, TimeXer empowers the canonical Transformer architecture with the ability to reco
&lt;/p&gt;</description></item><item><title>&#22312;&#23454;&#38469;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#23454;&#38469;PIM&#31995;&#32479;&#30340;&#26234;&#33021;&#24182;&#34892;&#21270;&#25216;&#26415;&#21644;&#28151;&#21512;&#24335;&#25191;&#34892;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16731</link><description>&lt;p&gt;
&#22312;&#23454;&#38469;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Accelerating Graph Neural Networks on Real Processing-In-Memory Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16731
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#23454;&#38469;PIM&#31995;&#32479;&#30340;&#26234;&#33021;&#24182;&#34892;&#21270;&#25216;&#26415;&#21644;&#28151;&#21512;&#24335;&#25191;&#34892;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25191;&#34892;&#28041;&#21450;&#35745;&#31639;&#23494;&#38598;&#22411;&#21644;&#20869;&#23384;&#23494;&#38598;&#22411;&#26680;&#24515;&#65292;&#21518;&#32773;&#22312;&#24635;&#26102;&#38388;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#21463;&#25968;&#25454;&#22312;&#20869;&#23384;&#21644;&#22788;&#29702;&#22120;&#20043;&#38388;&#31227;&#21160;&#30340;&#20005;&#37325;&#29942;&#39048;&#25152;&#38480;&#21046;&#12290;&#22788;&#29702;&#20869;&#23384;&#65288;PIM&#65289;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#22312;&#20869;&#23384;&#38453;&#21015;&#38468;&#36817;&#25110;&#20869;&#37096;&#25918;&#32622;&#31616;&#21333;&#22788;&#29702;&#22120;&#26469;&#32531;&#35299;&#36825;&#31181;&#25968;&#25454;&#31227;&#21160;&#29942;&#39048;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PyGim&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;PIM&#31995;&#32479;&#19978;&#21152;&#36895;GNNs&#12290;&#25105;&#20204;&#20026;&#38024;&#23545;&#23454;&#38469;PIM&#31995;&#32479;&#23450;&#21046;&#30340;GNN&#20869;&#23384;&#23494;&#38598;&#22411;&#26680;&#24515;&#25552;&#20986;&#26234;&#33021;&#24182;&#34892;&#21270;&#25216;&#26415;&#65292;&#24182;&#20026;&#23427;&#20204;&#24320;&#21457;&#20102;&#26041;&#20415;&#30340;Python API&#12290;&#25105;&#20204;&#25552;&#20379;&#28151;&#21512;&#24335;GNN&#25191;&#34892;&#65292;&#20854;&#20013;&#35745;&#31639;&#23494;&#38598;&#22411;&#21644;&#20869;&#23384;&#23494;&#38598;&#22411;&#26680;&#24515;&#20998;&#21035;&#22312;&#20197;&#22788;&#29702;&#22120;&#20026;&#20013;&#24515;&#21644;&#20197;&#20869;&#23384;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#31995;&#32479;&#20013;&#25191;&#34892;&#65292;&#20197;&#21305;&#37197;&#23427;&#20204;&#30340;&#31639;&#27861;&#29305;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16731v2 Announce Type: replace-cross  Abstract: Graph Neural Networks (GNNs) are emerging ML models to analyze graph-structure data. Graph Neural Network (GNN) execution involves both compute-intensive and memory-intensive kernels, the latter dominates the total time, being significantly bottlenecked by data movement between memory and processors. Processing-In-Memory (PIM) systems can alleviate this data movement bottleneck by placing simple processors near or inside to memory arrays. In this work, we introduce PyGim, an efficient ML framework that accelerates GNNs on real PIM systems. We propose intelligent parallelization techniques for memory-intensive kernels of GNNs tailored for real PIM systems, and develop handy Python API for them. We provide hybrid GNN execution, in which the compute-intensive and memory-intensive kernels are executed in processor-centric and memory-centric computing systems, respectively, to match their algorithmic nature. We extensively evaluate 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#19982;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30456;&#32467;&#21512;&#30340;VerSAILLE&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#22312;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#19978;&#30340;&#23433;&#20840;&#24615;&#35777;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.10998</link><description>&lt;p&gt;
&#36890;&#36807;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Provably Safe Neural Network Controllers via Differential Dynamic Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10998
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#19982;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30456;&#32467;&#21512;&#30340;VerSAILLE&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#22312;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#19978;&#30340;&#23433;&#20840;&#24615;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#20316;&#20026;&#38754;&#21521;&#30446;&#26631;&#30340;&#25511;&#21046;&#22120;&#22312;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#39564;&#35777;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25511;&#21046;&#31995;&#32479;&#65288;NNCS&#65289;&#30340;&#23433;&#20840;&#24615;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;NN&#26469;&#35828;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24403;&#38656;&#35201;&#23545;&#26080;&#30028;&#26102;&#38388;&#33539;&#22260;&#36827;&#34892;&#23433;&#20840;&#24615;&#39564;&#35777;&#26102;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;VerSAILLE&#65288;&#36890;&#36807;&#36923;&#36753;&#38142;&#25509;&#21253;&#39564;&#35777;&#30340;&#21487;&#39564;&#35777;&#23433;&#20840;&#20154;&#24037;&#26234;&#33021;&#65289;&#65306;&#36825;&#26159;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#65288;dL&#65289;&#21644;NN&#39564;&#35777;&#32452;&#21512;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#12290;&#36890;&#36807;&#21512;&#20316;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;NN&#39564;&#35777;&#24037;&#20855;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#30041;dL&#30340;&#20005;&#35880;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25511;&#21046;&#22120;&#20449;&#23553;&#30340;&#23433;&#20840;&#24615;&#35777;&#26126;&#65292;&#20197;&#35777;&#26126;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#19978;&#20855;&#20307;NNCS&#30340;&#23433;&#20840;&#24615;&#12290;VerSAILLE&#23548;&#33268;&#30340;NN&#39564;&#35777;&#23646;&#24615;&#36890;&#24120;&#38656;&#35201;&#38750;&#32447;&#24615;&#31639;&#26415;&#65292;&#32780;&#39640;&#25928;&#30340;NN&#39564;&#35777;&#24037;&#20855;&#20165;&#25903;&#25345;&#32447;&#24615;&#31639;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10998v1 Announce Type: cross  Abstract: While neural networks (NNs) have a large potential as goal-oriented controllers for Cyber-Physical Systems, verifying the safety of neural network based control systems (NNCSs) poses significant challenges for the practical use of NNs -- especially when safety is needed for unbounded time horizons. One reason for this is the intractability of NN and hybrid system analysis. We introduce VerSAILLE (Verifiably Safe AI via Logically Linked Envelopes): The first approach for the combination of differential dynamic logic (dL) and NN verification. By joining forces, we can exploit the efficiency of NN verification tools while retaining the rigor of dL. We reflect a safety proof for a controller envelope in an NN to prove the safety of concrete NNCS on an infinite-time horizon. The NN verification properties resulting from VerSAILLE typically require nonlinear arithmetic while efficient NN verification tools merely support linear arithmetic. T
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#36924;&#36817;&#30340;&#19968;&#33324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31574;&#30053;&#23545;&#27604;&#25915;&#20987;&#8221;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;&#36890;&#36807;&#20351;&#20302;&#24615;&#33021;&#31574;&#30053;&#30475;&#36215;&#26469;&#20687;&#26159;&#39640;&#24615;&#33021;&#30340;&#65292;&#21516;&#26102;&#20351;&#39640;&#24615;&#33021;&#31574;&#30053;&#30475;&#36215;&#26469;&#20687;&#26159;&#20302;&#24615;&#33021;&#30340;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#25915;&#20987;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09695</link><description>&lt;p&gt;
&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reward Poisoning Attack Against Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09695
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#36924;&#36817;&#30340;&#19968;&#33324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31574;&#30053;&#23545;&#27604;&#25915;&#20987;&#8221;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;&#36890;&#36807;&#20351;&#20302;&#24615;&#33021;&#31574;&#30053;&#30475;&#36215;&#26469;&#20687;&#26159;&#39640;&#24615;&#33021;&#30340;&#65292;&#21516;&#26102;&#20351;&#39640;&#24615;&#33021;&#31574;&#30053;&#30475;&#36215;&#26469;&#20687;&#26159;&#20302;&#24615;&#33021;&#30340;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#25915;&#20987;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#36924;&#36817;&#30340;&#19968;&#33324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#40657;&#30418;&#23041;&#32961;&#27169;&#22411;&#65292;&#25915;&#20987;&#32773;&#23545;&#23398;&#20064;&#31639;&#27861;&#23436;&#20840;&#19981;&#20102;&#35299;&#65292;&#24182;&#19988;&#20854;&#39044;&#31639;&#21463;&#21040;&#38480;&#21046;&#65292;&#38480;&#21046;&#20102;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#27745;&#26579;&#37327;&#20197;&#21450;&#24635;&#25200;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31574;&#30053;&#23545;&#27604;&#25915;&#20987;&#8221;&#30340;&#25915;&#20987;&#31574;&#30053;&#12290;&#20854;&#39640;&#23618;&#24605;&#24819;&#26159;&#20351;&#19968;&#20123;&#20302;&#24615;&#33021;&#31574;&#30053;&#30475;&#36215;&#26469;&#20687;&#26159;&#39640;&#24615;&#33021;&#30340;&#65292;&#21516;&#26102;&#20351;&#39640;&#24615;&#33021;&#31574;&#30053;&#30475;&#36215;&#26469;&#20687;&#26159;&#20302;&#24615;&#33021;&#30340;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#33324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22330;&#26223;&#30340;&#40657;&#30418;&#22870;&#21169;&#27745;&#26579;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25915;&#20987;&#35774;&#35745;&#30340;&#29702;&#35770;&#27934;&#23519;&#65292;&#24182;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#32463;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#25915;&#20987;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09695v1 Announce Type: cross  Abstract: We study the problem of reward poisoning attacks against general offline reinforcement learning with deep neural networks for function approximation. We consider a black-box threat model where the attacker is completely oblivious to the learning algorithm and its budget is limited by constraining both the amount of corruption at each data point, and the total perturbation. We propose an attack strategy called `policy contrast attack'. The high-level idea is to make some low-performing policies appear as high-performing while making high-performing policies appear as low-performing. To the best of our knowledge, we propose the first black-box reward poisoning attack in the general offline RL setting. We provide theoretical insights on the attack design and empirically show that our attack is efficient against current state-of-the-art offline RL algorithms in different kinds of learning datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#28176;&#21464;&#33609;&#22270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#21644;&#25439;&#22833;&#22320;&#35980;&#30740;&#31350;&#12290;&#20316;&#32773;&#22312;&#19977;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03994</link><description>&lt;p&gt;
&#20351;&#29992;&#28176;&#21464;&#33609;&#22270;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#21644;&#25439;&#22833;&#22320;&#35980;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Gradient Sketches for Training Data Attribution and Studying the Loss Landscape
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#28176;&#21464;&#33609;&#22270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#21644;&#25439;&#22833;&#22320;&#35980;&#30740;&#31350;&#12290;&#20316;&#32773;&#22312;&#19977;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#25237;&#24433;&#25110;&#28176;&#21464;&#21644;Hessian&#21521;&#37327;&#20056;&#31215;&#30340;&#33609;&#22270;&#22312;&#38656;&#35201;&#23384;&#20648;&#35768;&#22810;&#36825;&#20123;&#21521;&#37327;&#24182;&#20445;&#30041;&#20851;&#20110;&#23427;&#20204;&#30340;&#30456;&#23545;&#20960;&#20309;&#20449;&#24687;&#30340;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20004;&#20010;&#37325;&#35201;&#22330;&#26223;&#26159;&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#65288;&#36319;&#36394;&#27169;&#22411;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#34892;&#20026;&#65289;&#65292;&#20854;&#20013;&#38656;&#35201;&#23384;&#20648;&#27599;&#20010;&#35757;&#32451;&#31034;&#20363;&#30340;&#28176;&#21464;&#65292;&#20197;&#21450;Hessian&#30340;&#39057;&#35889;&#30740;&#31350;&#65288;&#20998;&#26512;&#35757;&#32451;&#21160;&#24577;&#65289;&#65292;&#20854;&#20013;&#38656;&#35201;&#23384;&#20648;&#22810;&#20010;Hessian&#21521;&#37327;&#20056;&#31215;&#12290;&#34429;&#28982;&#20351;&#29992;&#23494;&#38598;&#30697;&#38453;&#30340;&#33609;&#22270;&#26131;&#20110;&#23454;&#29616;&#65292;&#20294;&#23427;&#20204;&#21463;&#23384;&#20648;&#38480;&#21046;&#65292;&#19981;&#33021;&#25193;&#23637;&#21040;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#31070;&#32463;&#32593;&#32476;&#20869;&#22312;&#32500;&#24230;&#30340;&#30740;&#31350;&#24037;&#20316;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#21487;&#20280;&#32553;&#33609;&#22270;&#31639;&#27861;&#30340;&#35774;&#35745;&#31354;&#38388;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#65292;Hessian&#35889;&#20998;&#26512;&#21644;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#26102;&#30340;&#20869;&#22312;&#32500;&#24230;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random projections or sketches of gradients and Hessian vector products play an essential role in applications where one needs to store many such vectors while retaining accurate information about their relative geometry. Two important scenarios are training data attribution (tracing a model's behavior to the training data), where one needs to store a gradient for each training example, and the study of the spectrum of the Hessian (to analyze the training dynamics), where one needs to store multiple Hessian vector products. While sketches that use dense matrices are easy to implement, they are memory bound and cannot be scaled to modern neural networks. Motivated by work on the intrinsic dimension of neural networks, we propose and study a design space for scalable sketching algorithms. We demonstrate the efficacy of our approach in three applications: training data attribution, the analysis of the Hessian spectrum and the computation of the intrinsic dimension when fine-tuning pre-tra
&lt;/p&gt;</description></item><item><title>&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#19982;f-&#20998;&#24067;&#26063;&#30340;&#27491;&#21017;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#26159;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;f-&#20998;&#24067;&#27491;&#21017;&#21270;&#31561;&#25928;&#22320;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.00501</link><description>&lt;p&gt;
&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#19982;f-&#20998;&#24067;&#26063;&#27491;&#21017;&#21270;&#30340;&#31561;&#20215;&#24615;
&lt;/p&gt;
&lt;p&gt;
Equivalence of the Empirical Risk Minimization to Regularization on the Family of f-Divergences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00501
&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#19982;f-&#20998;&#24067;&#26063;&#30340;&#27491;&#21017;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#26159;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;f-&#20998;&#24067;&#27491;&#21017;&#21270;&#31561;&#25928;&#22320;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;f&#20013;&#30340;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#32473;&#20986;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#19982;f-&#20998;&#24067;&#30340;&#27491;&#21017;&#21270;&#65288;ERM-$f$DR&#65289;&#30340;&#35299;&#27861;&#12290;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#26368;&#20248;&#27979;&#24230;&#34987;&#35777;&#26126;&#26159;&#21807;&#19968;&#30340;&#12290;&#24182;&#32473;&#20986;&#20102;&#29305;&#23450;&#36873;&#25321;&#20989;&#25968;f&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#21033;&#29992;f-&#20998;&#24067;&#26063;&#30340;&#28789;&#27963;&#24615;&#65292;&#33719;&#24471;&#20102;&#20808;&#21069;&#23545;&#24120;&#35265;&#27491;&#21017;&#21270;&#36873;&#25321;&#30340;&#24050;&#30693;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#30340;&#21807;&#19968;&#35299;&#65288;Type-I&#21644;Type-II&#65289;&#12290;&#23545;&#35299;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22312;ERM-$f$DR&#38382;&#39064;&#20013;&#20351;&#29992;f-&#20998;&#24067;&#26102;&#30340;&#20197;&#19979;&#23646;&#24615;&#65306;$i)$ f-&#20998;&#24067;&#27491;&#21017;&#21270;&#24378;&#21046;&#23558;&#35299;&#30340;&#25903;&#25345;&#19982;&#21442;&#32771;&#27979;&#24230;&#30340;&#25903;&#25345;&#37325;&#21512;&#65292;&#24341;&#20837;&#20102;&#22312;&#35757;&#32451;&#25968;&#25454;&#25552;&#20379;&#30340;&#35777;&#25454;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#24378;&#24402;&#32435;&#20559;&#24046;&#65307;$ii)$ &#20219;&#20309;f-&#20998;&#24067;&#30340;&#27491;&#21017;&#21270;&#37117;&#31561;&#20215;&#20110;&#21478;&#19968;&#31181;f-&#20998;&#24067;&#30340;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The solution to empirical risk minimization with $f$-divergence regularization (ERM-$f$DR) is presented under mild conditions on $f$. Under such conditions, the optimal measure is shown to be unique. Examples of the solution for particular choices of the function $f$ are presented. Previously known solutions to common regularization choices are obtained by leveraging the flexibility of the family of $f$-divergences. These include the unique solutions to empirical risk minimization with relative entropy regularization (Type-I and Type-II). The analysis of the solution unveils the following properties of $f$-divergences when used in the ERM-$f$DR problem: $i\bigl)$ $f$-divergence regularization forces the support of the solution to coincide with the support of the reference measure, which introduces a strong inductive bias that dominates the evidence provided by the training data; and $ii\bigl)$ any $f$-divergence regularization is equivalent to a different $f$-divergence regularization 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#38750;&#32467;&#26500;&#32593;&#26684;&#21270;&#27969;&#20307;&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;GNNs&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#21487;&#35299;&#37322;&#30340;&#29289;&#29702;&#31354;&#38388;&#21306;&#22495;&#21450;&#20854;&#23545;&#24212;&#30340;&#23376;&#22270;&#65292;&#24110;&#21161;&#29702;&#35299;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#24050;&#30693;&#29289;&#29702;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2311.07548</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Interpretable Fine-Tuning for Graph Neural Network Surrogate Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#38750;&#32467;&#26500;&#32593;&#26684;&#21270;&#27969;&#20307;&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;GNNs&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#21487;&#35299;&#37322;&#30340;&#29289;&#29702;&#31354;&#38388;&#21306;&#22495;&#21450;&#20854;&#23545;&#24212;&#30340;&#23376;&#22270;&#65292;&#24110;&#21161;&#29702;&#35299;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#24050;&#30693;&#29289;&#29702;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#26367;&#20195;&#24314;&#27169;&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#20986;&#29616;&#22312;&#26368;&#36817;&#20960;&#24180;&#20869;&#34028;&#21187;&#21457;&#23637;&#65292;GNNs&#21487;&#20197;&#30452;&#25509;&#22312;&#22522;&#20110;&#32593;&#26684;&#30340;&#25968;&#25454;&#34920;&#31034;&#19978;&#36816;&#34892;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#20026;GNN&#24341;&#20837;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#24212;&#29992;&#20110;&#38750;&#32467;&#26500;&#32593;&#26684;&#21270;&#27969;&#20307;&#21160;&#21147;&#23398;&#24314;&#27169;&#12290;&#26368;&#32456;&#32467;&#26524;&#26159;&#19968;&#20010;&#22686;&#24378;&#30340;&#24494;&#35843;&#27169;&#22411;&#65292;&#23427;&#38548;&#31163;&#20102;&#19982;&#39044;&#27979;&#20219;&#21153;&#23494;&#20999;&#30456;&#20851;&#30340;&#29289;&#29702;&#31354;&#38388;&#21306;&#22495;&#65292;&#30456;&#24212;&#20110;&#23376;&#22270;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#22522;&#32447;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#20123;&#30001;&#24494;&#35843;&#30340;GNN&#35782;&#21035;&#20986;&#30340;&#32467;&#26500;&#22312;&#21069;&#21521;&#20256;&#36882;&#20013;&#26159;&#33258;&#36866;&#24212;&#29983;&#25104;&#30340;&#65292;&#24182;&#20316;&#20026;&#21487;&#35299;&#37322;&#30340;&#38142;&#25509;&#23384;&#22312;&#20110;&#22522;&#32447;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#24050;&#30693;&#38382;&#39064;&#29305;&#23450;&#29289;&#29702;&#20043;&#38388;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#31243;&#24207;&#65292;&#24494;&#35843;&#30340;GNNs&#36824;&#21487;&#20197;&#22312;&#25512;&#26029;&#26399;&#38388;&#29992;&#20110;&#35782;&#21035;&#23545;&#24212;&#30340;&#22270;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07548v2 Announce Type: replace  Abstract: Data-driven surrogate modeling has surged in capability in recent years with the emergence of graph neural networks (GNNs), which can operate directly on mesh-based representations of data. The goal of this work is to introduce an interpretable fine-tuning strategy for GNNs, with application to unstructured mesh-based fluid dynamics modeling. The end result is an enhanced fine-tuned model that isolates regions in physical space, corresponding to sub-graphs, that are intrinsically linked to the forecasting task while retaining the predictive capability of the baseline. These structures, identified by the fine-tuned GNNs, are adaptively produced in the forward pass and serve as explainable links between the baseline model architecture, the optimization goal, and known problem-specific physics. Additionally, through a regularization procedure, the fine-tuned GNNs can also be used to identify, during inference, graph nodes that correspon
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Pointer Q-Network (PQN)&#26041;&#27861;&#65292;&#23558;Ptr-Nets&#21644;Q-learning&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#20854;&#25209;&#35780;&#32773;&#24615;&#36136;&#65292;&#20986;&#33394;&#22320;&#25429;&#33719;&#20102;&#23884;&#20837;&#22270;&#20013;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;OP&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#20855;&#20307;&#25361;&#25112;</title><link>https://arxiv.org/abs/2311.02629</link><description>&lt;p&gt;
&#24102;&#26377;Q-Learning&#30340;&#25351;&#38024;&#32593;&#32476;&#29992;&#20110;OP&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Pointer Networks with Q-Learning for OP Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02629
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Pointer Q-Network (PQN)&#26041;&#27861;&#65292;&#23558;Ptr-Nets&#21644;Q-learning&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#20854;&#25209;&#35780;&#32773;&#24615;&#36136;&#65292;&#20986;&#33394;&#22320;&#25429;&#33719;&#20102;&#23884;&#20837;&#22270;&#20013;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;OP&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#20855;&#20307;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Orienteering Problem&#65288;OP&#65289;&#22312;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#20013;&#25552;&#20986;&#20102;&#29420;&#29305;&#25361;&#25112;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#29289;&#27969;&#12289;&#20132;&#20184;&#21644;&#36816;&#36755;&#35268;&#21010;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#30001;&#20110;OP&#30340;NP-hard&#24615;&#36136;&#65292;&#33719;&#24471;&#26368;&#20248;&#35299;&#26412;&#36136;&#19978;&#26159;&#22797;&#26434;&#30340;&#12290;&#23613;&#31649;&#25351;&#38024;&#32593;&#32476;&#65288;Ptr-Nets&#65289;&#22312;&#21508;&#31181;&#32452;&#21512;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;OP&#19978;&#30340;&#34920;&#29616;&#20197;&#21450;&#38656;&#35201;&#19987;&#27880;&#20110;&#26410;&#26469;&#22238;&#25253;&#25110;&#25506;&#32034;&#30340;&#20219;&#21153;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;&#35748;&#35782;&#21040;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#19982;&#24207;&#21015;-&#24207;&#21015;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#28508;&#33021;&#65292;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#25351;&#38024;Q&#32593;&#32476;&#65288;PQN&#65289;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;Ptr-Nets&#21644;Q-learning&#65292;&#30001;&#20110;&#20854;&#20165;&#20855;&#25209;&#35780;&#32773;&#24615;&#36136;&#65292;&#23427;&#22312;&#25429;&#33719;&#23884;&#20837;&#22270;&#20013;&#30340;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#26159;&#26377;&#25928;&#24212;&#23545;OP&#25552;&#20986;&#30340;&#20855;&#20307;&#25361;&#25112;&#30340;&#22522;&#26412;&#35201;&#27714;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#26550;&#26500;&#21644;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02629v2 Announce Type: replace  Abstract: The Orienteering Problem (OP) presents a unique challenge in Combinatorial Optimization (CO), emphasized by its widespread use in logistics, delivery, and transportation planning. Given the NP-hard nature of OP, obtaining optimal solutions is inherently complex. While Pointer Networks (Ptr-Nets) have exhibited prowess in various combinatorial tasks, their performance in the context of OP, and duties requiring focus on future return or exploration, leaves room for improvement. Recognizing the potency combining Reinforcement Learning (RL) methods with sequence-to-sequence models, this research unveils the Pointer Q-Network (PQN). This method combines Ptr-Nets and Q-learning, which, thanks to its critic only nature, outstands in its capability of capturing relationships within an embedded graph, a fundamental requirement in order to effectively address the specific challenges presented by OP. We explore the architecture and functionalit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24178;&#25200;&#19988;&#36731;&#37327;&#32423;&#30340;&#32819;&#25140;&#31995;&#32479;EarSD&#65292;&#36890;&#36807;&#27979;&#37327;&#29992;&#25143;&#32819;&#21518;&#37096;&#20301;&#30340;&#29983;&#29702;&#20449;&#21495;&#65292;&#23454;&#29616;&#20102;&#23545;&#30315;&#30187;&#21457;&#20316;&#30340;&#36830;&#32493;&#26816;&#27979;&#12290;&#36825;&#31181;&#31995;&#32479;&#30456;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#22836;&#30382;&#30340;&#33041;&#30005;&#22270;&#27979;&#35797;&#20855;&#26377;&#25104;&#26412;&#20302;&#12289;&#20415;&#25658;&#24615;&#22909;&#12289;&#20351;&#29992;&#33298;&#36866;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.05425</link><description>&lt;p&gt;
&#19968;&#31181;&#26080;&#24178;&#25200;&#19988;&#36731;&#37327;&#32423;&#30340;&#32819;&#25140;&#31995;&#32479;&#29992;&#20110;&#36830;&#32493;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
An Unobtrusive and Lightweight Ear-worn System for Continuous Epileptic Seizure Detection. (arXiv:2401.05425v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24178;&#25200;&#19988;&#36731;&#37327;&#32423;&#30340;&#32819;&#25140;&#31995;&#32479;EarSD&#65292;&#36890;&#36807;&#27979;&#37327;&#29992;&#25143;&#32819;&#21518;&#37096;&#20301;&#30340;&#29983;&#29702;&#20449;&#21495;&#65292;&#23454;&#29616;&#20102;&#23545;&#30315;&#30187;&#21457;&#20316;&#30340;&#36830;&#32493;&#26816;&#27979;&#12290;&#36825;&#31181;&#31995;&#32479;&#30456;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#22836;&#30382;&#30340;&#33041;&#30005;&#22270;&#27979;&#35797;&#20855;&#26377;&#25104;&#26412;&#20302;&#12289;&#20415;&#25658;&#24615;&#22909;&#12289;&#20351;&#29992;&#33298;&#36866;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#20840;&#29699;&#26368;&#24120;&#35265;&#30340;&#31070;&#32463;&#30142;&#30149;&#20043;&#19968;&#65292;&#24433;&#21709;&#30528;&#20840;&#29699;&#32422;5000&#19975;&#20154;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22914;&#26524;&#33021;&#24471;&#21040;&#27491;&#30830;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#65292;&#39640;&#36798;70%&#30340;&#30315;&#30187;&#24739;&#32773;&#21487;&#20197;&#26080;&#30315;&#30187;&#21457;&#20316;&#22320;&#29983;&#27963;&#65292;&#32780;&#19968;&#31181;&#21487;&#38752;&#30340;&#30417;&#27979;&#30315;&#30187;&#21457;&#20316;&#30340;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#37027;&#20123;&#19981;&#26029;&#38754;&#20020;&#38543;&#26426;&#21457;&#20316;&#24656;&#24807;&#30340;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#23613;&#31649;&#22522;&#20110;&#22836;&#30382;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#27979;&#35797;&#26159;&#35786;&#26029;&#30315;&#30187;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#20294;&#35813;&#26041;&#27861;&#25104;&#26412;&#39640;&#12289;&#38656;&#35201;&#20303;&#38498;&#27835;&#30103;&#12289;&#38656;&#35201;&#29087;&#32451;&#30340;&#25805;&#20316;&#20154;&#21592;&#65292;&#24182;&#19988;&#23545;&#29992;&#25143;&#26469;&#35828;&#19981;&#33298;&#36866;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EarSD&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36731;&#37327;&#32423;&#12289;&#26080;&#24178;&#25200;&#21644;&#31038;&#20250;&#25509;&#21463;&#24230;&#39640;&#30340;&#32819;&#25140;&#31995;&#32479;&#65292;&#36890;&#36807;&#27979;&#37327;&#29992;&#25143;&#32819;&#21518;&#37096;&#20301;&#30340;&#29983;&#29702;&#20449;&#21495;&#26469;&#26816;&#27979;&#30315;&#30187;&#21457;&#20316;&#30340;&#24320;&#22987;&#12290;EarSD&#21253;&#25324;&#19968;&#20010;&#38598;&#25104;&#30340;&#33258;&#23450;&#20041;&#20256;&#24863;&#12289;&#35745;&#31639;&#21644;&#36890;&#20449;&#30005;&#36335;&#26495;&#65292;&#29992;&#20110;&#25910;&#38598;&#21644;&#25918;&#22823;&#24863;&#20852;&#36259;&#30340;&#20449;&#21495;&#65292;&#21435;&#38500;&#36816;&#21160;&#20266;&#24433;&#21644;&#29615;&#22659;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epilepsy is one of the most common neurological diseases globally, affecting around 50 million people worldwide. Fortunately, up to 70 percent of people with epilepsy could live seizure-free if properly diagnosed and treated, and a reliable technique to monitor the onset of seizures could improve the quality of life of patients who are constantly facing the fear of random seizure attacks. The scalp-based EEG test, despite being the gold standard for diagnosing epilepsy, is costly, necessitates hospitalization, demands skilled professionals for operation, and is discomforting for users. In this paper, we propose EarSD, a novel lightweight, unobtrusive, and socially acceptable ear-worn system to detect epileptic seizure onsets by measuring the physiological signals from behind the user's ears. EarSD includes an integrated custom-built sensing, computing, and communication PCB to collect and amplify the signals of interest, remove the noises caused by motion artifacts and environmental im
&lt;/p&gt;</description></item><item><title>FairWASP&#26159;&#19968;&#31181;&#24555;&#36895;&#21644;&#26368;&#20248;&#30340;&#20844;&#24179;Wasserstein&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#25968;&#25454;&#38598;&#26469;&#20943;&#23569;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#31561;&#24615;&#65292;&#21516;&#26102;&#28385;&#36275;&#20154;&#21475;&#24179;&#31561;&#24615;&#20934;&#21017;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#21487;&#20197;&#36755;&#20837;&#20219;&#20309;&#20998;&#31867;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2311.00109</link><description>&lt;p&gt;
FairWASP&#65306;&#24555;&#36895;&#21644;&#26368;&#20248;&#30340;&#20844;&#24179;Wasserstein&#39044;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
FairWASP: Fast and Optimal Fair Wasserstein Pre-processing. (arXiv:2311.00109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00109
&lt;/p&gt;
&lt;p&gt;
FairWASP&#26159;&#19968;&#31181;&#24555;&#36895;&#21644;&#26368;&#20248;&#30340;&#20844;&#24179;Wasserstein&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#25968;&#25454;&#38598;&#26469;&#20943;&#23569;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#31561;&#24615;&#65292;&#21516;&#26102;&#28385;&#36275;&#20154;&#21475;&#24179;&#31561;&#24615;&#20934;&#21017;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#21487;&#20197;&#36755;&#20837;&#20219;&#20309;&#20998;&#31867;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24555;&#36895;&#21457;&#23637;&#26088;&#22312;&#20943;&#23569;&#19981;&#21516;&#23376;&#32676;&#20307;&#20043;&#38388;&#27169;&#22411;&#36755;&#20986;&#30340;&#19981;&#24179;&#31561;&#24615;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#20250;&#34987;&#19981;&#21516;&#29992;&#25143;&#22312;&#22810;&#20010;&#19979;&#28216;&#24212;&#29992;&#20013;&#20351;&#29992;&#65292;&#36825;&#24847;&#21619;&#30528;&#23545;&#35757;&#32451;&#25968;&#25454;&#26412;&#36523;&#36827;&#34892;&#24178;&#39044;&#21487;&#33021;&#26159;&#26368;&#26377;&#25928;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;FairWASP&#65292;&#26088;&#22312;&#20943;&#23569;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#31561;&#24615;&#65292;&#32780;&#19981;&#20250;&#20462;&#25913;&#21407;&#22987;&#25968;&#25454;&#12290;FairWASP&#36820;&#22238;&#26679;&#26412;&#32423;&#26435;&#37325;&#65292;&#20351;&#37325;&#26032;&#21152;&#26435;&#30340;&#25968;&#25454;&#38598;&#26368;&#23567;&#21270;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;Wasserstein&#36317;&#31163;&#65292;&#21516;&#26102;&#28385;&#36275;&#65288;&#32463;&#39564;&#29256;&#26412;&#30340;&#65289;&#20154;&#21475;&#24179;&#31561;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#20844;&#24179;&#24615;&#20934;&#21017;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25972;&#25968;&#26435;&#37325;&#30340;&#26368;&#20248;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#31561;&#21516;&#22320;&#29702;&#35299;&#20026;&#22797;&#21046;&#25110;&#21024;&#38500;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;FairWASP&#21487;&#29992;&#20110;&#26500;&#24314;&#21487;&#20197;&#36755;&#20837;&#20219;&#20309;&#20998;&#31867;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#25509;&#21463;&#26679;&#26412;&#26435;&#37325;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen a surge of machine learning approaches aimed at reducing disparities in model outputs across different subgroups. In many settings, training data may be used in multiple downstream applications by different users, which means it may be most effective to intervene on the training data itself. In this work, we present FairWASP, a novel pre-processing approach designed to reduce disparities in classification datasets without modifying the original data. FairWASP returns sample-level weights such that the reweighted dataset minimizes the Wasserstein distance to the original dataset while satisfying (an empirical version of) demographic parity, a popular fairness criterion. We show theoretically that integer weights are optimal, which means our method can be equivalently understood as duplicating or eliminating samples. FairWASP can therefore be used to construct datasets which can be fed into any classification method, not just methods which accept sample weights. Ou
&lt;/p&gt;</description></item><item><title>RRCNN$^{+}$&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;&#27531;&#24046;&#36882;&#24402;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#38750;&#24179;&#31283;&#20449;&#21495;&#20998;&#35299;&#12290;&#23427;&#33021;&#26356;&#31283;&#23450;&#22320;&#20998;&#35299;&#20449;&#21495;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#28145;&#24230;&#23398;&#20064;&#20026;&#38750;&#24179;&#31283;&#20449;&#21495;&#20998;&#35299;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2309.04782</link><description>&lt;p&gt;
RRCNN$^{+}$&#65306;&#19968;&#31181;&#22686;&#24378;&#30340;&#29992;&#20110;&#38750;&#24179;&#31283;&#20449;&#21495;&#20998;&#35299;&#30340;&#27531;&#24046;&#36882;&#24402;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
RRCNN$^{+}$: An Enhanced Residual Recursive Convolutional Neural Network for Non-stationary Signal Decomposition. (arXiv:2309.04782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04782
&lt;/p&gt;
&lt;p&gt;
RRCNN$^{+}$&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;&#27531;&#24046;&#36882;&#24402;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#38750;&#24179;&#31283;&#20449;&#21495;&#20998;&#35299;&#12290;&#23427;&#33021;&#26356;&#31283;&#23450;&#22320;&#20998;&#35299;&#20449;&#21495;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#28145;&#24230;&#23398;&#20064;&#20026;&#38750;&#24179;&#31283;&#20449;&#21495;&#20998;&#35299;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#26159;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#23567;&#27874;&#20998;&#26512;&#26159;&#20004;&#31181;&#32463;&#20856;&#26041;&#27861;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#21644;&#38750;&#24179;&#31283;&#20449;&#21495;&#26102;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#38750;&#32447;&#24615;&#21644;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20197;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#26041;&#27861;&#20026;&#20808;&#39537;&#12290;&#23427;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#38750;&#24179;&#31283;&#20449;&#21495;&#20998;&#35299;&#20026;&#20934;&#24179;&#31283;&#20998;&#37327;&#65292;&#20197;&#22312;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#20013;&#25581;&#31034;&#26356;&#22909;&#30340;&#29305;&#24449;&#12290;&#26368;&#36817;&#65292;&#21463;&#28145;&#24230;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#27531;&#24046;&#36882;&#24402;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RRCNN&#65289;&#12290;RRCNN&#19981;&#20165;&#21487;&#20197;&#22312;&#23545;&#22823;&#35268;&#27169;&#20449;&#21495;&#36827;&#34892;&#25209;&#22788;&#29702;&#26102;&#23454;&#29616;&#26356;&#31283;&#23450;&#30340;&#20998;&#35299;&#65292;&#32780;&#19988;&#36824;&#20026;&#38750;&#24179;&#31283;&#20449;&#21495;&#20998;&#35299;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20511;&#21161;&#26381;&#21153;&#22120;&#30340;&#24110;&#21161;&#36827;&#19968;&#27493;&#25913;&#36827;RRCNN&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-frequency analysis is an important and challenging task in many applications. Fourier and wavelet analysis are two classic methods that have achieved remarkable success in many fields. They also exhibit limitations when applied to nonlinear and non-stationary signals. To address this challenge, a series of nonlinear and adaptive methods, pioneered by the empirical mode decomposition method have been proposed. Their aim is to decompose a non-stationary signal into quasi-stationary components which reveal better features in the time-frequency analysis. Recently, inspired by deep learning, we proposed a novel method called residual recursive convolutional neural network (RRCNN). Not only RRCNN can achieve more stable decomposition than existing methods while batch processing large-scale signals with low computational cost, but also deep learning provides a unique perspective for non-stationary signal decomposition. In this study, we aim to further improve RRCNN with the help of sever
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32479;&#19968;&#27010;&#24565;&#32534;&#36753;&#65288;UCE&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#38381;&#21512;&#35299;&#20915;&#26041;&#26696;&#23545;&#27169;&#22411;&#36827;&#34892;&#32534;&#36753;&#65292;&#21516;&#26102;&#35299;&#20915;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12289;&#29256;&#26435;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#31561;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14761</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#32479;&#19968;&#27010;&#24565;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Unified Concept Editing in Diffusion Models. (arXiv:2308.14761v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32479;&#19968;&#27010;&#24565;&#32534;&#36753;&#65288;UCE&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#38381;&#21512;&#35299;&#20915;&#26041;&#26696;&#23545;&#27169;&#22411;&#36827;&#34892;&#32534;&#36753;&#65292;&#21516;&#26102;&#35299;&#20915;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12289;&#29256;&#26435;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#31561;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23384;&#22312;&#21508;&#31181;&#23433;&#20840;&#38382;&#39064;&#65292;&#21487;&#33021;&#38480;&#21046;&#20854;&#36866;&#29992;&#24615;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12289;&#29256;&#26435;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#31561;&#21508;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#65292;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#37117;&#21516;&#26102;&#20986;&#29616;&#22312;&#21516;&#19968;&#20010;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;&#26041;&#27861;&#35299;&#20915;&#25152;&#26377;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#32479;&#19968;&#27010;&#24565;&#32534;&#36753;&#65288;UCE&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#32463;&#36807;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#38381;&#21512;&#35299;&#20915;&#26041;&#26696;&#23545;&#27169;&#22411;&#36827;&#34892;&#32534;&#36753;&#65292;&#24182;&#21487;&#26080;&#32541;&#22320;&#25193;&#23637;&#21040;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#25193;&#25955;&#27169;&#22411;&#19978;&#36827;&#34892;&#24182;&#34892;&#32534;&#36753;&#12290;&#25105;&#20204;&#36890;&#36807;&#32534;&#36753;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25237;&#24433;&#26469;&#23637;&#31034;&#21487;&#25193;&#23637;&#30340;&#21516;&#26102;&#21435;&#20559;&#35265;&#12289;&#28040;&#38500;&#39118;&#26684;&#21644;&#20869;&#23481;&#35843;&#33410;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://unified.baulab.info&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image models suffer from various safety issues that may limit their suitability for deployment. Previous methods have separately addressed individual issues of bias, copyright, and offensive content in text-to-image models. However, in the real world, all of these issues appear simultaneously in the same model. We present a method that tackles all issues with a single approach. Our method, Unified Concept Editing (UCE), edits the model without training using a closed-form solution, and scales seamlessly to concurrent edits on text-conditional diffusion models. We demonstrate scalable simultaneous debiasing, style erasure, and content moderation by editing text-to-image projections, and we present extensive experiments demonstrating improved efficacy and scalability over prior work. Our code is available at https://unified.baulab.info
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#26041;&#27861;&#65288;XAIG-R&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#21644;&#28151;&#21512;&#26694;&#26550;&#26469;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#22788;&#29702;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2307.07840</link><description>&lt;p&gt;
RegExplainer: &#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#29983;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task. (arXiv:2307.07840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#26041;&#27861;&#65288;XAIG-R&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#21644;&#28151;&#21512;&#26694;&#26550;&#26469;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#22788;&#29702;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22238;&#24402;&#26159;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#65292;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25512;&#29702;&#36807;&#31243;&#36890;&#24120;&#26159;&#19981;&#21487;&#35299;&#37322;&#30340;&#12290;&#29616;&#26377;&#30340;&#35299;&#37322;&#25216;&#26415;&#22823;&#22810;&#38480;&#20110;&#29702;&#35299;&#20998;&#31867;&#20219;&#21153;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23547;&#27714;&#35299;&#37322;&#26469;&#35299;&#37322;&#22270;&#22238;&#24402;&#27169;&#22411;&#65288;XAIG-R&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#20998;&#24067;&#20559;&#31227;&#21644;&#36830;&#32493;&#26377;&#24207;&#30340;&#20915;&#31574;&#36793;&#30028;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#26032;&#30446;&#26631;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#24335;&#25903;&#25345;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#24212;&#23545;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#36830;&#32493;&#26377;&#24207;&#26631;&#31614;&#12290;&#20026;&#20102;&#20174;&#32463;&#39564;&#19978;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph regression is a fundamental task and has received increasing attention in a wide range of graph learning tasks. However, the inference process is often not interpretable. Most existing explanation techniques are limited to understanding GNN behaviors in classification tasks. In this work, we seek an explanation to interpret the graph regression models (XAIG-R). We show that existing methods overlook the distribution shifting and continuously ordered decision boundary, which hinders them away from being applied in the regression tasks. To address these challenges, we propose a novel objective based on the information bottleneck theory and introduce a new mix-up framework, which could support various GNNs in a model-agnostic manner. We further present a contrastive learning strategy to tackle the continuously ordered labels in regression task. To empirically verify the effectiveness of the proposed method, we introduce three benchmark datasets and a real-life dataset for evaluation
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#24352;&#37327;&#32593;&#32476;&#29702;&#35299;&#21644;&#28436;&#21270;&#21367;&#31215;&#30340;&#26032;&#35270;&#35282;&#65292;&#21487;&#20197;&#36890;&#36807;&#32472;&#21046;&#21644;&#25805;&#20316;&#24352;&#37327;&#32593;&#32476;&#26469;&#36827;&#34892;&#20989;&#25968;&#36716;&#25442;&#12289;&#23376;&#24352;&#37327;&#35775;&#38382;&#21644;&#34701;&#21512;&#12290;&#30740;&#31350;&#20154;&#21592;&#36824;&#28436;&#31034;&#20102;&#21367;&#31215;&#22270;&#34920;&#30340;&#23548;&#20986;&#20197;&#21450;&#21508;&#31181;&#33258;&#21160;&#24494;&#20998;&#25805;&#20316;&#21644;&#20108;&#38454;&#20449;&#24687;&#36924;&#36817;&#22270;&#34920;&#30340;&#29983;&#25104;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#29305;&#23450;&#20110;&#21367;&#31215;&#30340;&#22270;&#34920;&#36716;&#25442;&#65292;&#20197;&#20248;&#21270;&#35745;&#31639;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02275</link><description>&lt;p&gt;
&#36879;&#36807;&#24352;&#37327;&#32593;&#32476;&#30340;&#35270;&#35282;&#35299;&#26512;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Convolutions Through the Lens of Tensor Networks. (arXiv:2307.02275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#24352;&#37327;&#32593;&#32476;&#29702;&#35299;&#21644;&#28436;&#21270;&#21367;&#31215;&#30340;&#26032;&#35270;&#35282;&#65292;&#21487;&#20197;&#36890;&#36807;&#32472;&#21046;&#21644;&#25805;&#20316;&#24352;&#37327;&#32593;&#32476;&#26469;&#36827;&#34892;&#20989;&#25968;&#36716;&#25442;&#12289;&#23376;&#24352;&#37327;&#35775;&#38382;&#21644;&#34701;&#21512;&#12290;&#30740;&#31350;&#20154;&#21592;&#36824;&#28436;&#31034;&#20102;&#21367;&#31215;&#22270;&#34920;&#30340;&#23548;&#20986;&#20197;&#21450;&#21508;&#31181;&#33258;&#21160;&#24494;&#20998;&#25805;&#20316;&#21644;&#20108;&#38454;&#20449;&#24687;&#36924;&#36817;&#22270;&#34920;&#30340;&#29983;&#25104;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#29305;&#23450;&#20110;&#21367;&#31215;&#30340;&#22270;&#34920;&#36716;&#25442;&#65292;&#20197;&#20248;&#21270;&#35745;&#31639;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21367;&#31215;&#30340;&#30452;&#35266;&#27010;&#24565;&#31616;&#21333;&#65292;&#20294;&#20854;&#20998;&#26512;&#27604;&#31264;&#23494;&#23618;&#26356;&#21152;&#22797;&#26434;&#65292;&#36825;&#20351;&#24471;&#29702;&#35770;&#21644;&#31639;&#27861;&#30340;&#25512;&#24191;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#36890;&#36807;&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;&#25552;&#20379;&#20102;&#23545;&#21367;&#31215;&#30340;&#26032;&#35270;&#35282;&#65292;&#36890;&#36807;&#32472;&#21046;&#22270;&#34920;&#12289;&#25805;&#20316;&#22270;&#34920;&#36827;&#34892;&#20989;&#25968;&#36716;&#25442;&#12289;&#23376;&#24352;&#37327;&#35775;&#38382;&#21644;&#34701;&#21512;&#26469;&#25512;&#29702;&#24213;&#23618;&#24352;&#37327;&#20056;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#21508;&#31181;&#33258;&#21160;&#24494;&#20998;&#25805;&#20316;&#30340;&#22270;&#34920;&#20197;&#21450;&#20855;&#26377;&#23436;&#25972;&#36229;&#21442;&#25968;&#25903;&#25345;&#12289;&#25209;&#22788;&#29702;&#12289;&#36890;&#36947;&#32452;&#21644;&#20219;&#24847;&#21367;&#31215;&#32500;&#24230;&#27867;&#21270;&#30340;&#27969;&#34892;&#30340;&#20108;&#38454;&#20449;&#24687;&#36924;&#36817;&#30340;&#22270;&#34920;&#26469;&#23637;&#31034;&#36825;&#31181;&#34920;&#36798;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#36830;&#25509;&#27169;&#24335;&#25552;&#20379;&#20102;&#29305;&#23450;&#20110;&#21367;&#31215;&#30340;&#36716;&#25442;&#65292;&#20801;&#35768;&#22312;&#35780;&#20272;&#20043;&#21069;&#37325;&#26032;&#36830;&#25509;&#21644;&#31616;&#21270;&#22270;&#34920;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20381;&#36182;&#20110;&#39640;&#25928;TN&#32553;&#24182;&#30340;&#24050;&#24314;&#31435;&#26426;&#21046;&#26469;&#25506;&#31350;&#35745;&#31639;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;TN&#23454;&#29616;&#21152;&#36895;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;
&lt;/p&gt;
&lt;p&gt;
Despite their simple intuition, convolutions are more tedious to analyze than dense layers, which complicates the generalization of theoretical and algorithmic ideas. We provide a new perspective onto convolutions through tensor networks (TNs) which allow reasoning about the underlying tensor multiplications by drawing diagrams, and manipulating them to perform function transformations, sub-tensor access, and fusion. We demonstrate this expressive power by deriving the diagrams of various autodiff operations and popular approximations of second-order information with full hyper-parameter support, batching, channel groups, and generalization to arbitrary convolution dimensions. Further, we provide convolution-specific transformations based on the connectivity pattern which allow to re-wire and simplify diagrams before evaluation. Finally, we probe computational performance, relying on established machinery for efficient TN contraction. Our TN implementation speeds up a recently-proposed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#21464;&#21387;&#22120;&#20551;&#35774;&#31354;&#38388;&#30340;&#26222;&#36941;&#36924;&#36817;&#23450;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#24459;&#27010;&#24565;&#29992;&#20110;&#31934;&#30830;&#36924;&#36817;&#36895;&#29575;&#20272;&#35745;&#65292;&#25581;&#31034;&#20102;&#21464;&#21387;&#22120;&#36866;&#29992;&#20110;&#36924;&#36817;&#21738;&#20123;&#31867;&#22411;&#30340;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#19982;&#20256;&#32479;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#20043;&#38388;&#30340;&#32467;&#26500;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.18475</link><description>&lt;p&gt;
&#24207;&#21015;&#24314;&#27169;&#30340;&#21464;&#21387;&#22120;&#32593;&#32476;&#30340;&#36924;&#36817;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Approximation theory of transformer networks for sequence modeling. (arXiv:2305.18475v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#21464;&#21387;&#22120;&#20551;&#35774;&#31354;&#38388;&#30340;&#26222;&#36941;&#36924;&#36817;&#23450;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#24459;&#27010;&#24565;&#29992;&#20110;&#31934;&#30830;&#36924;&#36817;&#36895;&#29575;&#20272;&#35745;&#65292;&#25581;&#31034;&#20102;&#21464;&#21387;&#22120;&#36866;&#29992;&#20110;&#36924;&#36817;&#21738;&#20123;&#31867;&#22411;&#30340;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#19982;&#20256;&#32479;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#20043;&#38388;&#30340;&#32467;&#26500;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#26159;&#24207;&#21015;&#24314;&#27169;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#26550;&#26500;&#65292;&#20294;&#20854;&#24037;&#20316;&#21407;&#29702;&#30340;&#29702;&#35770;&#29702;&#35299;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21464;&#21387;&#22120;&#36924;&#36817;&#24207;&#21015;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#21464;&#21387;&#22120;&#20551;&#35774;&#31354;&#38388;&#30340;&#26222;&#36941;&#36924;&#36817;&#23450;&#29702;&#12290;&#36890;&#36807;&#25512;&#23548;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#24459;&#27010;&#24565;&#65292;&#22312;&#27492;&#27010;&#24565;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#19968;&#20010;&#26126;&#30830;&#30340;&#36924;&#36817;&#36895;&#29575;&#20272;&#35745;&#12290;&#36825;&#20010;&#20272;&#35745;&#25581;&#31034;&#20102;&#21464;&#21387;&#22120;&#30340;&#20851;&#38190;&#32467;&#26500;&#29305;&#24615;&#65292;&#24182;&#26263;&#31034;&#20102;&#21464;&#21387;&#22120;&#36866;&#29992;&#20110;&#36924;&#36817;&#21738;&#20123;&#31867;&#22411;&#30340;&#24207;&#21015;&#20851;&#31995;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#20855;&#20307;&#22320;&#35752;&#35770;&#21464;&#21387;&#22120;&#19982;&#20256;&#32479;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65288;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#20043;&#38388;&#30340;&#32467;&#26500;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24471;&#21040;&#20102;&#25968;&#23383;&#23454;&#39564;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer is a widely applied architecture in sequence modeling applications, but the theoretical understanding of its working principles is limited. In this work, we investigate the ability of transformers to approximate sequential relationships. We first prove a universal approximation theorem for the transformer hypothesis space. From its derivation, we identify a novel notion of regularity under which we can prove an explicit approximation rate estimate. This estimate reveals key structural properties of the transformer and suggests the types of sequence relationships that the transformer is adapted to approximating. In particular, it allows us to concretely discuss the structural bias between the transformer and classical sequence modeling methods, such as recurrent neural networks. Our findings are supported by numerical experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#27010;&#25324;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31867;&#22411;&#21644;&#24212;&#29992;&#65292;&#27604;&#36739;&#20998;&#26512;&#20102;&#21508;&#20010;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#26377;&#21161;&#20110;&#36873;&#25321;&#21644;&#35774;&#35745;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.17473</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27010;&#36848;&#19982;&#27604;&#36739;&#20998;&#26512;&#65306;CNN&#12289;RNN&#12289;LSTM&#12289;GRU&#12290;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Overview and Comparative Analysis on Deep Learning Models: CNN, RNN, LSTM, GRU. (arXiv:2305.17473v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#27010;&#25324;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31867;&#22411;&#21644;&#24212;&#29992;&#65292;&#27604;&#36739;&#20998;&#26512;&#20102;&#21508;&#20010;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#26377;&#21161;&#20110;&#36873;&#25321;&#21644;&#35774;&#35745;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24378;&#22823;&#23376;&#38598;&#65292;&#29305;&#21035;&#22312;&#22788;&#29702;&#38750;&#32467;&#26500;&#21270;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;ML&#26041;&#27861;&#12290;&#20854;&#24433;&#21709;&#36328;&#36234;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#35821;&#38899;&#35782;&#21035;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12289;&#32593;&#32476;&#23433;&#20840;&#12289;&#39044;&#27979;&#20998;&#26512;&#31561;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21644;&#21160;&#24577;&#24615;&#32473;&#35774;&#35745;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#24320;&#21457;&#20986;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#19981;&#21516;&#30340;&#38382;&#39064;&#21644;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#12289;&#29983;&#25104;&#27169;&#22411;&#12289;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21644;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#32771;&#23519;&#20102;&#27599;&#20010;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#24212;&#29992;&#12289;&#22909;&#22788;&#21644;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has emerged as a powerful subset of machine learning (ML) and artificial intelligence (AI), outperforming traditional ML methods, especially in handling unstructured and large datasets. Its impact spans across various domains, including speech recognition, healthcare, autonomous vehicles, cybersecurity, predictive analytics, and more. However, the complexity and dynamic nature of real-world problems present challenges in designing effective deep learning models. Consequently, several deep learning models have been developed to address different problems and applications. In this article, we conduct a comprehensive survey of various deep learning models, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Models, Deep Reinforcement Learning (DRL), and Deep Transfer Learning. We examine the structure, applications, benefits, and limitations of each model. Furthermore, we perform an analysis using three publicly available dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26032;&#22411;&#25955;&#24230;&#8212;&#8212;&#34920;&#31034;Jensen-Shannon&#25955;&#24230;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#20998;&#24067;&#23884;&#20837;&#21040;RKHS&#20013;&#65292;&#24182;&#21033;&#29992;&#34920;&#31034;&#30340;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#39057;&#35889;&#65292;&#23454;&#29616;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#20272;&#35745;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#25193;&#23637;&#24615;&#65292;&#21487;&#24494;&#20998;&#24615;&#30340;&#32463;&#39564;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#20989;&#25968;&#21644;&#22522;&#20110;&#26680;&#30697;&#38453;&#30340;&#20272;&#35745;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.16446</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#31034;&#30340;Jensen-Shannon&#25955;&#24230;
&lt;/p&gt;
&lt;p&gt;
The Representation Jensen-Shannon Divergence. (arXiv:2305.16446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26032;&#22411;&#25955;&#24230;&#8212;&#8212;&#34920;&#31034;Jensen-Shannon&#25955;&#24230;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#20998;&#24067;&#23884;&#20837;&#21040;RKHS&#20013;&#65292;&#24182;&#21033;&#29992;&#34920;&#31034;&#30340;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#39057;&#35889;&#65292;&#23454;&#29616;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#20272;&#35745;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#25193;&#23637;&#24615;&#65292;&#21487;&#24494;&#20998;&#24615;&#30340;&#32463;&#39564;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#20989;&#25968;&#21644;&#22522;&#20110;&#26680;&#30697;&#38453;&#30340;&#20272;&#35745;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#25955;&#24230;&#37327;&#21270;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#24213;&#23618;&#20998;&#24067;&#36890;&#24120;&#26410;&#30693;&#65292;&#20174;&#32463;&#39564;&#26679;&#26412;&#20013;&#20272;&#35745;&#25955;&#24230;&#26159;&#19968;&#20010;&#22522;&#26412;&#38590;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#20013;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#26032;&#22411;&#25955;&#24230;&#8212;&#8212;&#34920;&#31034;Jensen-Shannon&#25955;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25968;&#25454;&#20998;&#24067;&#23884;&#20837;&#21040;RKHS&#20013;&#65292;&#24182;&#21033;&#29992;&#34920;&#31034;&#30340;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#39057;&#35889;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#32463;&#39564;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#30340;&#20272;&#35745;&#20989;&#25968;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;Fourier&#29305;&#24449;&#23558;&#25968;&#25454;&#26144;&#23556;&#21040;RKHS&#20013;&#12290;&#27492;&#20272;&#35745;&#20989;&#25968;&#26159;&#28789;&#27963;&#12289;&#21487;&#25193;&#23637;&#12289;&#21487;&#24494;&#20998;&#30340;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#23567;&#25209;&#37327;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30697;&#38453;&#30340;&#20272;&#35745;&#20989;&#25968;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;RKHS&#36827;&#34892;&#26174;&#24335;&#26144;&#23556;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#37327;&#26159;Jensen-Shannon&#25955;&#24230;&#30340;&#19968;&#20010;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical divergences quantify the difference between probability distributions finding multiple uses in machine-learning. However, a fundamental challenge is to estimate divergence from empirical samples since the underlying distributions of the data are usually unknown. In this work, we propose the representation Jensen-Shannon Divergence, a novel divergence based on covariance operators in reproducing kernel Hilbert spaces (RKHS). Our approach embeds the data distributions in an RKHS and exploits the spectrum of the covariance operators of the representations. We provide an estimator from empirical covariance matrices by explicitly mapping the data to an RKHS using Fourier features. This estimator is flexible, scalable, differentiable, and suitable for minibatch-based optimization problems. Additionally, we provide an estimator based on kernel matrices without having an explicit mapping to the RKHS. We show that this quantity is a lower bound on the Jensen-Shannon divergence, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#22495;&#30340;&#26377;&#25928;&#39044;&#22788;&#29702;&#22120;&#12290;&#20854;&#36890;&#36807;&#26367;&#25442;&#20256;&#32479;&#25163;&#24037;&#39044;&#22788;&#29702;&#22120;&#26174;&#30528;&#25552;&#39640;&#20102;&#25910;&#25947;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#38382;&#39064;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#22343;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16368</link><description>&lt;p&gt;
&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#65306;&#23398;&#20064;&#20849;&#36717;&#26799;&#24230;&#27861;&#30340;&#39044;&#22788;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural incomplete factorization: learning preconditioners for the conjugate gradient method. (arXiv:2305.16368v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#22495;&#30340;&#26377;&#25928;&#39044;&#22788;&#29702;&#22120;&#12290;&#20854;&#36890;&#36807;&#26367;&#25442;&#20256;&#32479;&#25163;&#24037;&#39044;&#22788;&#29702;&#22120;&#26174;&#30528;&#25552;&#39640;&#20102;&#25910;&#25947;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#38382;&#39064;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#22343;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#31185;&#23398;&#35745;&#31639;&#21644;&#20248;&#21270;&#20013;&#36935;&#21040;&#30340;&#22823;&#35268;&#27169;&#32447;&#24615;&#26041;&#31243;&#32452;&#27714;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29983;&#25104;&#36866;&#29992;&#20110;&#29305;&#23450;&#38382;&#39064;&#22495;&#30340;&#26377;&#25928;&#39044;&#22788;&#29702;&#22120;&#12290;&#36890;&#36807;&#26367;&#25442;&#19982;&#20849;&#36717;&#26799;&#24230;&#27861;&#19968;&#36215;&#20351;&#29992;&#30340;&#20256;&#32479;&#25163;&#24037;&#39044;&#22788;&#29702;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;&#31216;&#20026;&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#65289;&#26174;&#30528;&#21152;&#36895;&#20102;&#25910;&#25947;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#21463;&#31232;&#30095;&#30697;&#38453;&#29702;&#35770;&#21551;&#21457;&#30340;&#26032;&#22411;&#28040;&#24687;&#20256;&#36882;&#22359;&#65292;&#23427;&#19982;&#23547;&#25214;&#30697;&#38453;&#30340;&#31232;&#30095;&#20998;&#35299;&#30340;&#30446;&#26631;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#38382;&#39064;&#21644;&#26469;&#33258;&#31185;&#23398;&#35745;&#31639;&#30340;&#30495;&#23454;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#19981;&#23436;&#20840;&#20998;&#35299;&#22987;&#32456;&#20248;&#20110;&#26368;&#24120;&#35265;&#30340;&#36890;&#29992;&#39044;&#22788;&#29702;&#22120;&#65292;&#21253;&#25324;&#19981;&#23436;&#20840;&#30340;Cholesky&#26041;&#27861;&#65292;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop a novel data-driven approach to accelerate solving large-scale linear equation systems encountered in scientific computing and optimization. Our method utilizes self-supervised training of a graph neural network to generate an effective preconditioner tailored to the specific problem domain. By replacing conventional hand-crafted preconditioners used with the conjugate gradient method, our approach, named neural incomplete factorization (NeuralIF), significantly speeds-up convergence and computational efficiency. At the core of our method is a novel message-passing block, inspired by sparse matrix theory, that aligns with the objective to find a sparse factorization of the matrix. We evaluate our proposed method on both a synthetic and a real-world problem arising from scientific computing. Our results demonstrate that NeuralIF consistently outperforms the most common general-purpose preconditioners, including the incomplete Cholesky method, achieving competit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#21512;&#25104;&#26679;&#26412;&#23454;&#29616;&#20840;&#23616;&#20248;&#21270;&#65292;&#21152;&#20837;&#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169;&#20197;&#20445;&#25252;&#38544;&#31169;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.01068</link><description>&lt;p&gt;
Fed-GLOSS-DP: &#21033;&#29992;&#20855;&#26377;&#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#21512;&#25104;&#38598;&#36827;&#34892;&#32852;&#37030;&#20840;&#23616;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fed-GLOSS-DP: Federated, Global Learning using Synthetic Sets with Record Level Differential Privacy. (arXiv:2302.01068v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#21512;&#25104;&#26679;&#26412;&#23454;&#29616;&#20840;&#23616;&#20248;&#21270;&#65292;&#21152;&#20837;&#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169;&#20197;&#20445;&#25252;&#38544;&#31169;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Fed-GLOSS-DP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#19982;&#20197;&#21069;&#30340;&#32447;&#24615;&#36880;&#28857;&#26799;&#24230;&#20998;&#20139;&#26041;&#26696;&#65288;&#22914;FedAvg&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#20844;&#24335;&#21033;&#29992;&#20174;&#23458;&#25143;&#31471;&#25509;&#25910;&#21040;&#30340;&#21512;&#25104;&#26679;&#26412;&#23454;&#29616;&#20102;&#19968;&#31181;&#20840;&#23616;&#20248;&#21270;&#12290;&#36825;&#20123;&#21512;&#25104;&#26679;&#26412;&#20316;&#20026;&#25439;&#22833;&#26367;&#20195;&#29289;&#65292;&#36890;&#36807;&#27169;&#25311;&#26412;&#22320;&#21306;&#22495;&#20869;&#30495;&#23454;&#22270;&#20687;&#30340;&#23454;&#29992;&#24615;&#26469;&#36817;&#20284;&#26412;&#22320;&#25439;&#22833;&#22320;&#24418;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#34913;&#37327;&#26377;&#25928;&#36924;&#36817;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#21453;&#26144;&#20102;&#36817;&#20284;&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#26381;&#21153;&#22120;&#21487;&#20197;&#24674;&#22797;&#20840;&#23616;&#25439;&#22833;&#22320;&#24418;&#24182;&#20840;&#38754;&#20248;&#21270;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#21463;&#26085;&#30410;&#20005;&#37325;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26080;&#32541;&#37197;&#21512;&#65292;&#20026;&#23458;&#25143;&#31471;&#19978;&#30340;&#27599;&#20010;&#25968;&#25454;&#35760;&#24405;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#24191;&#27867;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20844;&#24335;&#22312;&#20855;&#26377;&#39640;&#24230;&#20542;&#26012;&#20998;&#24067;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes Fed-GLOSS-DP, a novel privacy-preserving approach for federated learning. Unlike previous linear point-wise gradient-sharing schemes, such as FedAvg, our formulation enables a type of global optimization by leveraging synthetic samples received from clients. These synthetic samples, serving as loss surrogates, approximate local loss landscapes by simulating the utility of real images within a local region. We additionally introduce an approach to measure effective approximation regions reflecting the quality of the approximation. Therefore, the server can recover the global loss landscape and comprehensively optimize the model. Moreover, motivated by the emerging privacy concerns, we demonstrate that our approach seamlessly works with record-level differential privacy (DP), granting theoretical privacy guarantees for every data record on the clients. Extensive results validate the efficacy of our formulation on various datasets with highly skewed distributions. Our m
&lt;/p&gt;</description></item><item><title>&#38754;&#37096;&#27880;&#37322;&#26631;&#27880;&#32773;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#20010;&#20154;&#29305;&#24449;&#20250;&#24433;&#21709;&#20854;&#25968;&#25454;&#26631;&#27880;&#30340;&#20844;&#27491;&#24615;&#65292;&#24378;&#35843;&#38656;&#35201;&#23545;&#25972;&#20010;&#20154;&#24037;&#26234;&#33021;&#22521;&#35757;&#36807;&#31243;&#20445;&#25345;&#39640;&#24230;&#36879;&#26126;&#20197;&#23613;&#26089;&#35782;&#21035;&#21644;&#32416;&#27491;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2301.09902</link><description>&lt;p&gt;
&#25506;&#31350;&#38754;&#37096;&#27880;&#37322;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26631;&#27880;&#32773;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Investigating Labeler Bias in Face Annotation for Machine Learning. (arXiv:2301.09902v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09902
&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#27880;&#37322;&#26631;&#27880;&#32773;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#20010;&#20154;&#29305;&#24449;&#20250;&#24433;&#21709;&#20854;&#25968;&#25454;&#26631;&#27880;&#30340;&#20844;&#27491;&#24615;&#65292;&#24378;&#35843;&#38656;&#35201;&#23545;&#25972;&#20010;&#20154;&#24037;&#26234;&#33021;&#22521;&#35757;&#36807;&#31243;&#20445;&#25345;&#39640;&#24230;&#36879;&#26126;&#20197;&#23613;&#26089;&#35782;&#21035;&#21644;&#32416;&#27491;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#36234;&#26469;&#36234;&#20381;&#36182;&#20154;&#24037;&#26234;&#33021;&#30340;&#19990;&#30028;&#20013;&#65292;&#32771;&#34385;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#31867;&#30340;&#20262;&#29702;&#24433;&#21709;&#21464;&#24471;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#21152;&#37325;&#35201;&#12290;&#19968;&#20010;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#26631;&#27880;&#32773;&#20559;&#35265;&#65292;&#36825;&#21487;&#33021;&#20250;&#20026;&#35757;&#32451;&#21019;&#24314;&#26412;&#36136;&#19978;&#24102;&#26377;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#38543;&#21518;&#23548;&#33268;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#23601;&#19994;&#12289;&#25945;&#32946;&#21644;&#25191;&#27861;&#31561;&#39046;&#22495;&#20013;&#20986;&#29616;&#19981;&#20934;&#30830;&#25110;&#19981;&#20844;&#24179;&#30340;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#31181;&#26063;&#21644;&#24615;&#21035;&#30340;&#20154;&#30340;&#22270;&#20687;&#36827;&#34892;&#26631;&#35760;&#20219;&#21153;&#65292;&#20197;&#35843;&#26597;&#21644;&#34913;&#37327;&#26631;&#27880;&#32773;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21442;&#19982;&#32773;&#25317;&#26377;&#24433;&#21709;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#24182;&#19988;&#26631;&#27880;&#32773;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#23545;&#25152;&#20998;&#37197;&#30340;&#27880;&#37322;&#26631;&#31614;&#20135;&#29983;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26631;&#27880;&#32773;&#20559;&#35265;&#22914;&#20309;&#24433;&#21709;&#25968;&#25454;&#38598;&#65292;&#38543;&#21518;&#24433;&#21709;&#25152;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#22312;&#25972;&#20010;&#20154;&#24037;&#26234;&#33021;&#22521;&#35757;&#36807;&#31243;&#20013;&#24517;&#39035;&#20445;&#25345;&#39640;&#24230;&#36879;&#26126;&#65292;&#23613;&#26089;&#35782;&#21035;&#21644;&#32416;&#27491;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a world increasingly reliant on artificial intelligence, it is more important than ever to consider the ethical implications of artificial intelligence on humanity. One key under-explored challenge is labeler bias, which can create inherently biased datasets for training and subsequently lead to inaccurate or unfair decisions in healthcare, employment, education, and law enforcement. Hence, we conducted a study to investigate and measure the existence of labeler bias using images of people from different ethnicities and sexes in a labeling task. Our results show that participants possess stereotypes that influence their decision-making process and that labeler demographics impact assigned labels. We also discuss how labeler bias influences datasets and, subsequently, the models trained on them. Overall, a high degree of transparency must be maintained throughout the entire artificial intelligence training process to identify and correct biases in the data as early as possible.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#20250;&#21512;&#65292;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#23548;&#33322;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#38752;&#12289;&#20934;&#30830;&#21644;&#33258;&#20027;&#22320;&#36973;&#36935;&#24555;&#36895;&#31227;&#21160;&#30340;&#26143;&#38469;&#29289;&#20307;&#12290;&#23427;&#36890;&#36807;&#28857;&#26368;&#23567;&#33539;&#25968;&#36861;&#36394;&#25511;&#21046;&#21644;&#35889;&#24402;&#19968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24341;&#23548;&#31574;&#30053;&#26469;&#25552;&#20379;&#39640;&#27010;&#29575;&#25351;&#25968;&#19978;&#30028;&#30340;&#39134;&#34892;&#22120;&#20132;&#20184;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2208.04883</link><description>&lt;p&gt;
&#31070;&#32463;&#20250;&#21512;&#65306;&#38754;&#21521;&#26143;&#38469;&#29289;&#20307;&#30340;&#21487;&#38752;&#23548;&#33322;&#21644;&#25511;&#21046;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Neural-Rendezvous: Provably Robust Guidance and Control to Encounter Interstellar Objects. (arXiv:2208.04883v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#20250;&#21512;&#65292;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#23548;&#33322;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#38752;&#12289;&#20934;&#30830;&#21644;&#33258;&#20027;&#22320;&#36973;&#36935;&#24555;&#36895;&#31227;&#21160;&#30340;&#26143;&#38469;&#29289;&#20307;&#12290;&#23427;&#36890;&#36807;&#28857;&#26368;&#23567;&#33539;&#25968;&#36861;&#36394;&#25511;&#21046;&#21644;&#35889;&#24402;&#19968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24341;&#23548;&#31574;&#30053;&#26469;&#25552;&#20379;&#39640;&#27010;&#29575;&#25351;&#25968;&#19978;&#30028;&#30340;&#39134;&#34892;&#22120;&#20132;&#20184;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26143;&#38469;&#29289;&#20307;&#65288;ISOs&#65289;&#24456;&#21487;&#33021;&#26159;&#19981;&#21487;&#26367;&#20195;&#30340;&#21407;&#22987;&#26448;&#26009;&#65292;&#22312;&#29702;&#35299;&#31995;&#22806;&#34892;&#26143;&#26143;&#31995;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36816;&#34892;&#36712;&#36947;&#38590;&#20197;&#32422;&#26463;&#65292;&#36890;&#24120;&#20855;&#26377;&#36739;&#39640;&#30340;&#20542;&#35282;&#21644;&#30456;&#23545;&#36895;&#24230;&#65292;&#20351;&#29992;&#20256;&#32479;&#30340;&#20154;&#22312;&#29615;&#36335;&#26041;&#27861;&#25506;&#32034;ISOs&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#20250;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#23548;&#33322;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23454;&#26102;&#20013;&#20197;&#21487;&#38752;&#12289;&#20934;&#30830;&#21644;&#33258;&#20027;&#30340;&#26041;&#24335;&#36973;&#36935;&#24555;&#36895;&#31227;&#21160;&#30340;&#29289;&#20307;&#65292;&#21253;&#25324;ISOs&#12290;&#23427;&#22312;&#22522;&#20110;&#35889;&#24402;&#19968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24341;&#23548;&#31574;&#30053;&#20043;&#19978;&#20351;&#29992;&#28857;&#26368;&#23567;&#33539;&#25968;&#36861;&#36394;&#25511;&#21046;&#65292;&#20854;&#20013;&#21442;&#25968;&#36890;&#36807;&#30452;&#25509;&#24809;&#32602;MPC&#29366;&#24577;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35843;&#20248;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#20250;&#21512;&#22312;&#39044;&#26399;&#30340;&#39134;&#34892;&#22120;&#20132;&#20184;&#35823;&#24046;&#19978;&#25552;&#20379;&#20102;&#39640;&#27010;&#29575;&#25351;&#25968;&#19978;&#30028;&#65292;&#20854;&#35777;&#26126;&#21033;&#29992;&#20102;&#38543;&#26426;&#36882;&#22686;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interstellar objects (ISOs) are likely representatives of primitive materials invaluable in understanding exoplanetary star systems. Due to their poorly constrained orbits with generally high inclinations and relative velocities, however, exploring ISOs with conventional human-in-the-loop approaches is significantly challenging. This paper presents Neural-Rendezvous, a deep learning-based guidance and control framework for encountering fast-moving objects, including ISOs, robustly, accurately, and autonomously in real time. It uses pointwise minimum norm tracking control on top of a guidance policy modeled by a spectrally-normalized deep neural network, where its hyperparameters are tuned with a loss function directly penalizing the MPC state trajectory tracking error. We show that Neural-Rendezvous provides a high probability exponential bound on the expected spacecraft delivery error, the proof of which leverages stochastic incremental stability analysis. In particular, it is used to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21015;&#20030;&#38382;&#39064;&#23454;&#20363;&#24635;&#25439;&#22833;&#20989;&#25968;&#30340;&#37096;&#20998;&#26469;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20986;&#24863;&#30693;ERM&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#22810;&#21442;&#25968;&#32452;&#21512;&#31639;&#27861;&#26063;&#30340;&#35745;&#31639;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2204.03569</link><description>&lt;p&gt;
&#22522;&#20110;&#36755;&#20986;&#24863;&#30693;ERM&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Output-sensitive ERM-based techniques for data-driven algorithm design. (arXiv:2204.03569v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21015;&#20030;&#38382;&#39064;&#23454;&#20363;&#24635;&#25439;&#22833;&#20989;&#25968;&#30340;&#37096;&#20998;&#26469;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20986;&#24863;&#30693;ERM&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#22810;&#21442;&#25968;&#32452;&#21512;&#31639;&#27861;&#26063;&#30340;&#35745;&#31639;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#26159;&#19968;&#31181;&#26377;&#28508;&#21147;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36229;&#20986;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#20855;&#26377;&#21487;&#35843;&#21442;&#25968;&#30340;&#31639;&#27861;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#26159;&#20026;&#20855;&#26377;&#22810;&#20010;&#21442;&#25968;&#30340;&#32452;&#21512;&#31639;&#27861;&#26063;&#35774;&#35745;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#12290;&#24403;&#22266;&#23450;&#38382;&#39064;&#23454;&#20363;&#24182;&#21464;&#21270;&#21442;&#25968;&#26102;&#65292;"&#23545;&#20598;"&#25439;&#22833;&#20989;&#25968;&#36890;&#24120;&#20855;&#26377;&#20998;&#27573;&#21487;&#20998;&#35299;&#30340;&#32467;&#26500;&#65292;&#21363;&#38500;&#20102;&#26576;&#20123;&#23574;&#38160;&#30340;&#36716;&#25442;&#36793;&#30028;&#22806;&#37117;&#34920;&#29616;&#33391;&#22909;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21015;&#20030;&#19968;&#32452;&#38382;&#39064;&#23454;&#20363;&#30340;&#24635;&#25439;&#22833;&#20989;&#25968;&#30340;&#37096;&#20998;&#26469;&#24320;&#23637;&#25216;&#26415;&#30740;&#31350;&#65292;&#20197;&#24320;&#21457;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#30340;&#39640;&#25928;ERM&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#19982;&#23454;&#38469;&#20986;&#29616;&#30340;&#37096;&#20998;&#25968;&#30446;&#25104;&#27604;&#20363;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#37096;&#20998;&#25968;&#30446;&#30340;&#26368;&#22351;&#24773;&#20917;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20004;&#20010;&#26032;&#39062;&#30340;&#35201;&#32032; - &#19968;&#31181;&#29992;&#20110;&#26522;&#20030;&#30001;&#19968;&#32452;&#36229;&#24179;&#38754;&#35825;&#23548;&#30340;&#22810;&#38754;&#20307;&#30340;&#36755;&#20986;&#24863;&#30693;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven algorithm design is a promising, learning-based approach for beyond worst-case analysis of algorithms with tunable parameters. An important open problem is the design of computationally efficient data-driven algorithms for combinatorial algorithm families with multiple parameters. As one fixes the problem instance and varies the parameters, the "dual" loss function typically has a piecewise-decomposable structure, i.e. is well-behaved except at certain sharp transition boundaries. In this work we initiate the study of techniques to develop efficient ERM learning algorithms for data-driven algorithm design by enumerating the pieces of the sum dual loss functions for a collection of problem instances. The running time of our approach scales with the actual number of pieces that appear as opposed to worst case upper bounds on the number of pieces. Our approach involves two novel ingredients -- an output-sensitive algorithm for enumerating polytopes induced by a set of hyperpla
&lt;/p&gt;</description></item></channel></rss>