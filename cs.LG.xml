<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26816;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;&#31192;&#23494;&#23494;&#38053;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#21516;&#26102;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01245</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#26694;&#26550;: &#26530;&#36724;&#12289;&#26816;&#27979;&#25928;&#29575;&#21644;&#26368;&#20248;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26816;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;&#31192;&#23494;&#23494;&#38053;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#21516;&#26102;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;ChatGPT&#20110;2022&#24180;11&#26376;&#25512;&#20986;&#20197;&#26469;&#65292;&#23558;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#32479;&#35745;&#20449;&#21495;&#23884;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#20063;&#34987;&#31216;&#20026;&#27700;&#21360;&#65292;&#24050;&#34987;&#29992;&#20316;&#20174;&#20854;&#20154;&#31867;&#25776;&#20889;&#23545;&#24212;&#29289;&#19978;&#21487;&#35777;&#26816;&#27979;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#24182;&#35774;&#35745;&#24378;&#22823;&#30340;&#26816;&#27979;&#35268;&#21017;&#12290;&#21463;&#27700;&#21360;&#26816;&#27979;&#30340;&#20551;&#35774;&#26816;&#39564;&#20844;&#24335;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#39318;&#20808;&#36873;&#25321;&#25991;&#26412;&#30340;&#26530;&#36724;&#32479;&#35745;&#37327;&#21644;&#30001;LLM&#25552;&#20379;&#32473;&#39564;&#35777;&#22120;&#30340;&#31192;&#23494;&#23494;&#38053;&#65292;&#20197;&#23454;&#29616;&#25511;&#21046;&#35823;&#25253;&#29575;&#65288;&#23558;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#38169;&#35823;&#22320;&#26816;&#27979;&#20026;LLM&#29983;&#25104;&#30340;&#38169;&#35823;&#65289;&#12290; &#25509;&#19979;&#26469;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#36890;&#36807;&#33719;&#21462;&#28176;&#36817;&#38169;&#35823;&#36127;&#29575;&#65288;&#23558;LLM&#29983;&#25104;&#25991;&#26412;&#38169;&#35823;&#22320;&#26816;&#27979;&#20026;&#20154;&#31867;&#25776;&#20889;&#30340;&#38169;&#35823;&#65289;&#30340;&#23553;&#38381;&#24418;&#24335;&#34920;&#36798;&#24335;&#26469;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01245v1 Announce Type: cross  Abstract: Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DeepMIF&#65292;&#36890;&#36807;&#35774;&#35745;&#23398;&#20064;&#31995;&#32479;&#38598;&#25104;&#21333;&#35843;&#24615;&#25439;&#22833;&#65292;&#22312;&#22823;&#35268;&#27169;3D&#22320;&#22270;&#32472;&#21046;&#20013;&#20248;&#21270;&#31070;&#32463;&#21333;&#35843;&#22330;&#65292;&#36991;&#20813;&#20102;LiDAR&#27979;&#37327;&#30340;&#22024;&#26434;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.17550</link><description>&lt;p&gt;
DeepMIF: &#29992;&#20110;&#22823;&#35268;&#27169;LiDAR 3D&#22320;&#22270;&#32472;&#21046;&#30340;&#28145;&#24230;&#21333;&#35843;&#38544;&#24335;&#22330;
&lt;/p&gt;
&lt;p&gt;
DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17550
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DeepMIF&#65292;&#36890;&#36807;&#35774;&#35745;&#23398;&#20064;&#31995;&#32479;&#38598;&#25104;&#21333;&#35843;&#24615;&#25439;&#22833;&#65292;&#22312;&#22823;&#35268;&#27169;3D&#22320;&#22270;&#32472;&#21046;&#20013;&#20248;&#21270;&#31070;&#32463;&#21333;&#35843;&#22330;&#65292;&#36991;&#20813;&#20102;LiDAR&#27979;&#37327;&#30340;&#22024;&#26434;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;&#29616;&#20195;&#33719;&#21462;&#35774;&#22791;&#22914;LiDAR&#20256;&#24863;&#22120;&#65292;&#22312;&#24863;&#30693;&#30495;&#23454;&#22823;&#35268;&#27169;&#23460;&#22806;3D&#29615;&#22659;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#25104;&#31264;&#23494;&#12289;&#23436;&#25972;&#30340;3D&#22330;&#26223;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#38598;&#25104;&#20102;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#21644;&#21487;&#20248;&#21270;&#29305;&#24449;&#32593;&#26684;&#65292;&#20197;&#36924;&#36817;3D&#22330;&#26223;&#30340;&#34920;&#38754;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#27839;&#21407;&#22987;LiDAR&#20809;&#32447;&#25311;&#21512;&#26679;&#26412;&#20250;&#23548;&#33268;&#30001;&#20110;&#31232;&#30095;&#12289;&#20114;&#30456;&#30683;&#30462;&#30340;LiDAR&#27979;&#37327;&#30340;&#29305;&#24615;&#32780;&#20135;&#29983;&#22024;&#26434;&#30340;3D&#32472;&#22270;&#32467;&#26524;&#12290;&#30456;&#21453;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19981;&#20877;&#31934;&#30830;&#25311;&#21512;LiDAR&#25968;&#25454;&#65292;&#32780;&#26159;&#35753;&#32593;&#32476;&#20248;&#21270;&#22312;3D&#31354;&#38388;&#20013;&#23450;&#20041;&#30340;&#38750;&#24230;&#37327;&#21333;&#35843;&#38544;&#24335;&#22330;&#12290;&#20026;&#36866;&#24212;&#25105;&#20204;&#30340;&#22330;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23398;&#20064;&#31995;&#32479;&#65292;&#38598;&#25104;&#20102;&#19968;&#20010;&#21333;&#35843;&#24615;&#25439;&#22833;&#65292;&#20351;&#24471;&#33021;&#22815;&#20248;&#21270;&#31070;&#32463;&#21333;&#35843;&#22330;&#24182;&#21033;&#29992;&#20102;&#22823;&#35268;&#27169;3D&#22320;&#22270;&#32472;&#21046;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17550v1 Announce Type: cross  Abstract: Recently, significant progress has been achieved in sensing real large-scale outdoor 3D environments, particularly by using modern acquisition equipment such as LiDAR sensors. Unfortunately, they are fundamentally limited in their ability to produce dense, complete 3D scenes. To address this issue, recent learning-based methods integrate neural implicit representations and optimizable feature grids to approximate surfaces of 3D scenes. However, naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results due to the nature of sparse, conflicting LiDAR measurements. Instead, in this work we depart from fitting LiDAR data exactly, instead letting the network optimize a non-metric monotonic implicit field defined in 3D space. To fit our field, we design a learning system integrating a monotonicity loss that enables optimizing neural monotonic fields and leverages recent progress in large-scale 3D mapping. Our algorithm ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#21453;&#39304;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#23545;&#20219;&#21153;&#36827;&#34892;&#24418;&#24335;&#21270;&#34920;&#36848;&#65292;&#23454;&#29616;&#23545;&#29305;&#23450;&#20219;&#21153;&#30446;&#26631;&#30340;&#23450;&#37327;&#28385;&#36275;&#35821;&#20041;&#65292;&#24182;&#21033;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.15826</link><description>&lt;p&gt;
&#36890;&#36807;Dropout&#23545;&#26102;&#38388;&#20219;&#21153;&#36827;&#34892;&#27604;&#20363;&#23398;&#20064;&#30340;&#31574;&#30053;&#20248;&#21270;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Scaling Learning based Policy Optimization for Temporal Tasks via Dropout
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#21453;&#39304;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#23545;&#20219;&#21153;&#36827;&#34892;&#24418;&#24335;&#21270;&#34920;&#36848;&#65292;&#23454;&#29616;&#23545;&#29305;&#23450;&#20219;&#21153;&#30446;&#26631;&#30340;&#23450;&#37327;&#28385;&#36275;&#35821;&#20041;&#65292;&#24182;&#21033;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#24076;&#26395;&#32463;&#36807;&#35757;&#32451;&#30340;&#31574;&#30053;&#33021;&#22815;&#30830;&#20445;&#35813;&#26234;&#33021;&#20307;&#28385;&#36275;&#29305;&#23450;&#30340;&#20219;&#21153;&#30446;&#26631;&#65292;&#36825;&#20123;&#30446;&#26631;&#20197;&#31163;&#25955;&#26102;&#38388;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;DT-STL&#65289;&#34920;&#31034;&#12290;&#36890;&#36807;&#23558;&#20219;&#21153;&#37325;&#26032;&#34920;&#36848;&#20026;&#24418;&#24335;&#21270;&#26694;&#26550;&#65288;&#22914;DT-STL&#65289;&#65292;&#19968;&#20010;&#20248;&#21183;&#26159;&#20801;&#35768;&#23450;&#37327;&#28385;&#36275;&#35821;&#20041;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#32473;&#23450;&#19968;&#20010;&#36712;&#36857;&#21644;&#19968;&#20010;DT-STL&#20844;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#35745;&#31639;&#40065;&#26834;&#24615;&#65292;&#36825;&#21487;&#20197;&#35299;&#37322;&#20026;&#36712;&#36857;&#19982;&#28385;&#36275;&#35813;&#20844;&#24335;&#30340;&#36712;&#36857;&#38598;&#20043;&#38388;&#30340;&#36817;&#20284;&#26377;&#31526;&#21495;&#36317;&#31163;&#12290;&#25105;&#20204;&#21033;&#29992;&#21453;&#39304;&#25511;&#21046;&#22120;&#65292;&#24182;&#20551;&#35774;&#20351;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#36825;&#20123;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#19982;&#35757;&#32451;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#31867;&#20284;&#30340;&#22320;&#26041;&#65292;&#20854;&#20013;&#36882;&#24402;&#21333;&#20803;&#30340;&#25968;&#37327;&#19982;&#26234;&#33021;&#20307;&#30340;&#26102;&#38388;&#35270;&#37326;&#25104;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15826v1 Announce Type: cross  Abstract: This paper introduces a model-based approach for training feedback controllers for an autonomous agent operating in a highly nonlinear environment. We desire the trained policy to ensure that the agent satisfies specific task objectives, expressed in discrete-time Signal Temporal Logic (DT-STL). One advantage for reformulation of a task via formal frameworks, like DT-STL, is that it permits quantitative satisfaction semantics. In other words, given a trajectory and a DT-STL formula, we can compute the robustness, which can be interpreted as an approximate signed distance between the trajectory and the set of trajectories satisfying the formula. We utilize feedback controllers, and we assume a feed forward neural network for learning these feedback controllers. We show how this learning problem is similar to training recurrent neural networks (RNNs), where the number of recurrent units is proportional to the temporal horizon of the agen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#21160;&#24577;&#31995;&#32479;&#27010;&#29575;&#39044;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#38543;&#26426;&#25554;&#20540;&#22120;&#26500;&#24314;&#34394;&#26500;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26080;&#20559;&#35265;&#22320;&#29983;&#25104;&#26410;&#26469;&#31995;&#32479;&#29366;&#24577;&#30340;&#27010;&#29575;&#24615;&#39044;&#27979;&#38598;&#21512;</title><link>https://arxiv.org/abs/2403.13724</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#25554;&#20540;&#22120;&#21644;F\"ollmer&#36807;&#31243;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Forecasting with Stochastic Interpolants and F\"ollmer Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13724
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#21160;&#24577;&#31995;&#32479;&#27010;&#29575;&#39044;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#38543;&#26426;&#25554;&#20540;&#22120;&#26500;&#24314;&#34394;&#26500;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26080;&#20559;&#35265;&#22320;&#29983;&#25104;&#26410;&#26469;&#31995;&#32479;&#29366;&#24577;&#30340;&#27010;&#29575;&#24615;&#39044;&#27979;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#24314;&#27169;&#30340;&#21160;&#24577;&#31995;&#32479;&#27010;&#29575;&#39044;&#27979;&#26694;&#26550;&#12290;&#22312;&#32473;&#23450;&#31995;&#32479;&#38543;&#26102;&#38388;&#30340;&#29366;&#24577;&#35266;&#27979;&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#39044;&#27979;&#38382;&#39064;&#26500;&#24314;&#20026;&#20174;&#32473;&#23450;&#24403;&#21069;&#29366;&#24577;&#30340;&#26465;&#20214;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#24471;&#21040;&#26410;&#26469;&#31995;&#32479;&#29366;&#24577;&#30340;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#38543;&#26426;&#25554;&#20540;&#22120;&#30340;&#26694;&#26550;&#65292;&#36825;&#26377;&#21161;&#20110;&#26500;&#24314;&#22312;&#20219;&#24847;&#22522;&#30784;&#20998;&#24067;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#34394;&#26500;&#30340;&#12289;&#38750;&#29289;&#29702;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#20854;&#20197;&#24403;&#21069;&#31995;&#32479;&#29366;&#24577;&#20316;&#20026;&#21021;&#22987;&#26465;&#20214;&#65292;&#24182;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26080;&#20559;&#35265;&#22320;&#29983;&#25104;&#19968;&#20010;&#26469;&#33258;&#30446;&#26631;&#26465;&#20214;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#36825;&#20010;&#36807;&#31243;&#23558;&#20197;&#24403;&#21069;&#29366;&#24577;&#20026;&#20013;&#24515;&#30340;&#28857;&#29366;&#36136;&#37327;&#26144;&#23556;&#21040;&#19968;&#20010;&#27010;&#29575;&#24615;&#30340;&#39044;&#27979;&#38598;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23454;&#29616;&#36825;&#19968;&#20219;&#21153;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(SDE)&#20013;&#30340;&#28418;&#31227;&#31995;&#25968;&#26159;&#38750;&#22855;&#24322;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13724v1 Announce Type: new  Abstract: We propose a framework for probabilistic forecasting of dynamical systems based on generative modeling. Given observations of the system state over time, we formulate the forecasting problem as sampling from the conditional distribution of the future system state given its current state. To this end, we leverage the framework of stochastic interpolants, which facilitates the construction of a generative model between an arbitrary base distribution and the target. We design a fictitious, non-physical stochastic dynamics that takes as initial condition the current system state and produces as output a sample from the target conditional distribution in finite time and without bias. This process therefore maps a point mass centered at the current state onto a probabilistic ensemble of forecasts. We prove that the drift coefficient entering the stochastic differential equation (SDE) achieving this task is non-singular, and that it can be lear
&lt;/p&gt;</description></item><item><title>CASPER&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#20851;&#31995;&#24863;&#30693;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25554;&#34917;&#38382;&#39064;&#65292;&#36991;&#20813;&#36807;&#24230;&#21033;&#29992;&#38750;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#39640;&#25968;&#25454;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11960</link><description>&lt;p&gt;
CASPER&#65306;&#22240;&#26524;&#20851;&#31995;&#24863;&#30693;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;
&lt;/p&gt;
&lt;p&gt;
CASPER: Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11960
&lt;/p&gt;
&lt;p&gt;
CASPER&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#20851;&#31995;&#24863;&#30693;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25554;&#34917;&#38382;&#39064;&#65292;&#36991;&#20813;&#36807;&#24230;&#21033;&#29992;&#38750;&#22240;&#26524;&#20851;&#31995;&#65292;&#25552;&#39640;&#25968;&#25454;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11960v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25552;&#35201;&#65306;&#26102;&#31354;&#26102;&#38388;&#24207;&#21015;&#26159;&#29702;&#35299;&#20154;&#31867;&#27963;&#21160;&#21450;&#20854;&#24433;&#21709;&#30340;&#22522;&#30784;&#65292;&#36890;&#24120;&#36890;&#36807;&#25918;&#32622;&#22312;&#19981;&#21516;&#20301;&#32622;&#30340;&#30417;&#27979;&#20256;&#24863;&#22120;&#25910;&#38598;&#12290;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#30001;&#20110;&#21508;&#31181;&#25925;&#38556;&#32780;&#23548;&#33268;&#30340;&#32570;&#22833;&#20540;&#65292;&#36825;&#23545;&#25968;&#25454;&#20998;&#26512;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#20102;&#22635;&#34917;&#32570;&#22833;&#20540;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#12290;&#22312;&#24674;&#22797;&#29305;&#23450;&#25968;&#25454;&#28857;&#26102;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20542;&#21521;&#20110;&#32771;&#34385;&#19982;&#35813;&#28857;&#30456;&#20851;&#30340;&#25152;&#26377;&#20449;&#24687;&#65292;&#26080;&#35770;&#23427;&#20204;&#26159;&#21542;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#12290;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#65292;&#21253;&#25324;&#19968;&#20123;&#26410;&#30693;&#28151;&#26434;&#22240;&#32032;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#20363;&#22914;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32972;&#26223;&#22122;&#22768;&#21644;&#26500;&#24314;&#30340;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#38750;&#22240;&#26524;&#24555;&#25463;&#36793;&#12290;&#36825;&#20123;&#28151;&#26434;&#22240;&#32032;&#21487;&#33021;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#24320;&#36767;&#21453;&#21521;&#36335;&#24452;&#65292;&#25442;&#21477;&#35805;&#35828;&#65292;&#23427;&#20204;&#24314;&#31435;&#20102;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#38750;&#22240;&#26524;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11960v1 Announce Type: new  Abstract: Spatiotemporal time series is the foundation of understanding human activities and their impacts, which is usually collected via monitoring sensors placed at different locations. The collected data usually contains missing values due to various failures, which have significant impact on data analysis. To impute the missing values, a lot of methods have been introduced. When recovering a specific data point, most existing methods tend to take into consideration all the information relevant to that point regardless of whether they have a cause-and-effect relationship. During data collection, it is inevitable that some unknown confounders are included, e.g., background noise in time series and non-causal shortcut edges in the constructed sensor network. These confounders could open backdoor paths between the input and output, in other words, they establish non-causal correlations between the input and output. Over-exploiting these non-causa
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#30456;&#31354;&#38388;&#30340;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;BCI&#35299;&#30721;&#65292;&#25552;&#20379;&#20102;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#20013;&#21487;&#38752;&#31639;&#27861;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#29992;&#25143;&#33298;&#36866;&#24230;&#24182;&#20419;&#36827;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.05645</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#31354;&#38388;&#30340;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;BCI&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Geometric Neural Network based on Phase Space for BCI decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05645
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30456;&#31354;&#38388;&#30340;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;BCI&#35299;&#30721;&#65292;&#25552;&#20379;&#20102;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#20013;&#21487;&#38752;&#31639;&#27861;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#29992;&#25143;&#33298;&#36866;&#24230;&#24182;&#20419;&#36827;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Deep Learning(DL)&#31639;&#27861;&#19982;&#33041;&#20449;&#21495;&#20998;&#26512;&#30340;&#25972;&#21512;&#20173;&#22788;&#20110;&#33804;&#33469;&#38454;&#27573;&#65292;&#30456;&#27604;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#30340;&#25104;&#21151;&#65292;&#22312;&#33041;&#26426;&#25509;&#21475;(BCI)&#39046;&#22495;&#23588;&#20026;&#31361;&#20986;&#65292;BCI&#36890;&#36807;&#35299;&#30721;&#22823;&#33041;&#27963;&#21160;&#25511;&#21046;&#22806;&#37096;&#35774;&#22791;&#32780;&#26080;&#38656;&#32908;&#32905;&#25511;&#21046;&#12290;&#33041;&#30005;&#22270;(EEG)&#26159;&#35774;&#35745;BCI&#31995;&#32479;&#30340;&#24191;&#27867;&#36873;&#25321;&#65292;&#22240;&#20854;&#26080;&#21019;&#24615;&#12289;&#25104;&#26412;&#25928;&#30410;&#21644;&#20986;&#33394;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#20294;&#32570;&#23569;&#35757;&#32451;&#25968;&#25454;&#12289;&#20449;&#22122;&#27604;&#20302;&#12289;&#20197;&#21450;&#22312;&#20010;&#20307;&#38388;&#21644;&#20869;&#37096;&#30340;&#22823;&#37327;&#21464;&#21270;&#12290; &#26368;&#21518;&#65292;&#20351;&#29992;&#22810;&#20010;&#30005;&#26497;&#35774;&#32622;BCI&#31995;&#32479;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#65292;&#38459;&#30861;&#21487;&#38752;DL&#26550;&#26500;&#22312;&#30740;&#31350;&#23454;&#39564;&#23460;&#20043;&#22806;&#30340;BCI&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290; &#20026;&#20102;&#25552;&#39640;&#37319;&#32435;&#29575;&#65292;&#25105;&#20204;&#38656;&#35201;&#25913;&#21892;&#29992;&#25143;&#33298;&#36866;&#24230;&#65292;&#20363;&#22914;&#20351;&#29992;&#23569;&#37327;&#30005;&#26497;&#25805;&#20316;&#30340;&#21487;&#38752;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05645v1 Announce Type: cross  Abstract: The integration of Deep Learning (DL) algorithms on brain signal analysis is still in its nascent stages compared to their success in fields like Computer Vision, especially in Brain-Computer Interface (BCI), where the brain activity is decoded to control external devices without requiring muscle control. Electroencephalography (EEG) is a widely adopted choice for designing BCI systems due to its non-invasive and cost-effective nature and excellent temporal resolution. Still, it comes at the expense of limited training data, poor signal-to-noise, and a large variability across and within-subject recordings. Finally, setting up a BCI system with many electrodes takes a long time, hindering the widespread adoption of reliable DL architectures in BCIs outside research laboratories. To improve adoption, we need to improve user comfort using, for instance, reliable algorithms that operate with few electrodes. \textbf{Approach:} Our research
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24418;&#24335;&#36923;&#36753;&#20026;&#22522;&#30784;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;LLM&#20869;&#23481;&#29983;&#25104;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#27969;&#36923;&#36753;&#65288;TSL&#65289;&#23545;&#29983;&#25104;&#24335;&#20195;&#29702;&#26045;&#21152;&#26102;&#38388;&#32422;&#26463;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20195;&#29702;&#34892;&#20026;&#30340;&#20445;&#35777;&#27700;&#24179;&#12289;&#31995;&#32479;&#30340;&#35299;&#37322;&#24615;&#21644;&#20195;&#29702;&#30340;&#27169;&#22359;&#21270;&#26500;&#24314;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16905</link><description>&lt;p&gt;
&#21033;&#29992;&#21453;&#24212;&#21512;&#25104;&#23545;&#29983;&#25104;&#24335;&#20195;&#29702;&#34892;&#20026;&#26045;&#21152;&#26102;&#38388;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Enforcing Temporal Constraints on Generative Agent Behavior with Reactive Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16905
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24418;&#24335;&#36923;&#36753;&#20026;&#22522;&#30784;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;LLM&#20869;&#23481;&#29983;&#25104;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#27969;&#36923;&#36753;&#65288;TSL&#65289;&#23545;&#29983;&#25104;&#24335;&#20195;&#29702;&#26045;&#21152;&#26102;&#38388;&#32422;&#26463;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20195;&#29702;&#34892;&#20026;&#30340;&#20445;&#35777;&#27700;&#24179;&#12289;&#31995;&#32479;&#30340;&#35299;&#37322;&#24615;&#21644;&#20195;&#29702;&#30340;&#27169;&#22359;&#21270;&#26500;&#24314;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27969;&#34892;&#24341;&#21457;&#20102;&#23545;&#21019;&#24314;&#20132;&#20114;&#20195;&#29702;&#26032;&#26041;&#27861;&#30340;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#20114;&#21160;&#36807;&#31243;&#20013;&#31649;&#29702;&#36825;&#20123;&#20195;&#29702;&#30340;&#26102;&#38388;&#34892;&#20026;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24418;&#24335;&#36923;&#36753;&#20026;&#22522;&#30784;&#30340;&#31243;&#24207;&#21512;&#25104;&#19982;LLM&#20869;&#23481;&#29983;&#25104;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#21019;&#24314;&#36981;&#23432;&#26102;&#38388;&#32422;&#26463;&#30340;&#29983;&#25104;&#24335;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#26102;&#38388;&#27969;&#36923;&#36753;&#65288;Temporal Stream Logic&#65292;TSL&#65289;&#29983;&#25104;&#19968;&#20010;&#33258;&#21160;&#26426;&#65292;&#23545;&#20195;&#29702;&#26045;&#21152;&#26102;&#38388;&#32467;&#26500;&#65292;&#24182;&#23558;&#27599;&#20010;&#21160;&#20316;&#30340;&#32454;&#33410;&#30041;&#32473;LLM&#12290;&#36890;&#36807;&#20351;&#29992;TSL&#65292;&#25105;&#20204;&#33021;&#22815;&#22686;&#24378;&#29983;&#25104;&#20195;&#29702;&#65292;&#20351;&#29992;&#25143;&#22312;&#34892;&#20026;&#19978;&#26377;&#26356;&#39640;&#30340;&#20445;&#35777;&#27700;&#24179;&#65292;&#31995;&#32479;&#26356;&#26131;&#35299;&#37322;&#65292;&#24182;&#19988;&#26356;&#33021;&#20197;&#27169;&#22359;&#21270;&#26041;&#24335;&#26500;&#24314;&#20195;&#29702;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16905v1 Announce Type: new  Abstract: The surge in popularity of Large Language Models (LLMs) has opened doors for new approaches to the creation of interactive agents. However, managing the temporal behavior of such agents over the course of an interaction remains challenging. The stateful, long-term horizon and quantitative reasoning required for coherent agent behavior does not fit well into the LLM paradigm. We propose a combination of formal logic-based program synthesis and LLM content generation to create generative agents that adhere to temporal constraints. Our approach uses Temporal Stream Logic (TSL) to generate an automaton that enforces a temporal structure on an agent and leaves the details of each action for a moment in time to an LLM. By using TSL, we are able to augment the generative agent where users have a higher level of guarantees on behavior, better interpretability of the system, and more ability to build agents in a modular way. We evaluate our appro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#20215;&#20540;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#34920;&#36798;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#23545;&#35805;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;19&#20010;LLMs&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.14846</link><description>&lt;p&gt;
&#22362;&#25345;&#20320;&#30340;&#35282;&#33394;&#65281;&#20010;&#20154;&#20215;&#20540;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stick to your Role! Stability of Personal Values Expressed in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20010;&#20154;&#20215;&#20540;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#34920;&#36798;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#27169;&#25311;&#23545;&#35805;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;19&#20010;LLMs&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#25110;&#24515;&#29702;&#38382;&#21367;&#30340;&#26631;&#20934;&#26041;&#24335;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#25552;&#20379;&#35768;&#22810;&#26469;&#28304;&#20110;&#31867;&#20284;&#26368;&#23567;&#32972;&#26223;&#30340;&#19981;&#21516;&#26597;&#35810;&#65288;&#20363;&#22914;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#39640;&#24230;&#20381;&#36182;&#20110;&#32972;&#26223;&#65292;&#22240;&#27492;&#20174;&#36825;&#31181;&#26368;&#23567;&#32972;&#26223;&#35780;&#20272;&#20013;&#24471;&#20986;&#30340;&#32467;&#35770;&#21487;&#33021;&#23545;&#27169;&#22411;&#22312;&#37096;&#32626;&#20013;&#30340;&#34892;&#20026;&#65288;&#22312;&#37027;&#37324;&#23427;&#23558;&#26292;&#38706;&#20110;&#35768;&#22810;&#26032;&#32972;&#26223;&#65289;&#30340;&#35828;&#26126;&#24456;&#23569;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20381;&#36182;&#20110;&#32972;&#26223;&#30340;&#29305;&#24615;&#24212;&#35813;&#20316;&#20026;LLM&#27604;&#36739;&#30340;&#21478;&#19968;&#20010;&#32500;&#24230;&#26469;&#30740;&#31350;&#65292;&#32780;&#19981;&#26159;&#20854;&#20182;&#32500;&#24230;&#65292;&#22914;&#35748;&#30693;&#33021;&#21147;&#12289;&#30693;&#35782;&#25110;&#27169;&#22411;&#22823;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#65288;&#27169;&#25311;&#23545;&#19981;&#21516;&#35805;&#39064;&#30340;&#23545;&#35805;&#65289;&#20215;&#20540;&#34920;&#36798;&#31283;&#23450;&#24615;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#24515;&#29702;&#23398;&#38382;&#21367;&#65288;PVQ&#65289;&#21644;&#34892;&#20026;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#27979;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26469;&#33258;&#20116;&#20010;&#23478;&#26063;&#30340;19&#20010;&#24320;&#28304;LLM&#12290;&#20511;&#37492;&#24515;&#29702;&#23398;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31561;&#32423;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14846v1 Announce Type: cross  Abstract: The standard way to study Large Language Models (LLMs) through benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLM's highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model's behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence should be studied as another dimension of LLM comparison alongside others such as cognitive abilities, knowledge, or model size. In this paper, we present a case-study about the stability of value expression over different contexts (simulated conversations on different topics), and as measured using a standard psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19 open-sourced LLMs from five families. Reusing methods from psychology, we study Rank-order stabilit
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;StyleGAN3&#27169;&#22411;&#20013;&#21028;&#21035;&#22120;&#30340;&#30149;&#24577;&#20559;&#35265;&#65292;&#23427;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#30340;&#24471;&#20998;&#20998;&#23618;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2402.09786</link><description>&lt;p&gt;
&#26816;&#26597;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21028;&#21035;&#22120;&#20013;&#30340;&#30149;&#24577;&#20559;&#35265;&#65306;&#20197;StyleGAN3&#27169;&#22411;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09786
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;StyleGAN3&#27169;&#22411;&#20013;&#21028;&#21035;&#22120;&#30340;&#30149;&#24577;&#20559;&#35265;&#65292;&#23427;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#30340;&#24471;&#20998;&#20998;&#23618;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#65292;&#24448;&#24448;&#38590;&#20197;&#34987;&#20154;&#31867;&#21306;&#20998;&#20986;&#26469;&#12290;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;StyleGAN3&#27169;&#22411;&#20013;&#30340;&#21028;&#21035;&#22120;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#31995;&#32479;&#22320;&#23545;&#24471;&#20998;&#36827;&#34892;&#20998;&#23618;&#65292;&#24182;&#19988;&#36825;&#19981;&#25104;&#27604;&#20363;&#22320;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#21028;&#21035;&#22120;&#22312;&#33394;&#24425;&#21644;&#20142;&#24230;&#26041;&#38754;&#23545;&#24863;&#30693;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#30340;&#20559;&#35265;&#65292;&#28982;&#21518;&#26816;&#26597;&#20102;&#31038;&#20250;&#24515;&#29702;&#23398;&#20013;&#20851;&#20110;&#21051;&#26495;&#21360;&#35937;&#30740;&#31350;&#20013;&#24120;&#35265;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09786v1 Announce Type: cross  Abstract: Generative adversarial networks generate photorealistic faces that are often indistinguishable by humans from real faces. We find that the discriminator in the pre-trained StyleGAN3 model, a popular GAN network, systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine the discriminator's bias for color and luminance across axes perceived race and gender; we then examine axes common in research on stereotyping in social psychology.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22266;&#20307;&#24223;&#29289;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#21355;&#26143;&#25552;&#20379;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#65292;&#36890;&#36807;&#36965;&#24863;&#22270;&#20687;&#23454;&#29616;&#20102;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#30340;&#35782;&#21035;&#12289;&#30417;&#27979;&#21644;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.09066</link><description>&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#22266;&#20307;&#24223;&#29289;&#26816;&#27979;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Solid Waste Detection in Remote Sensing Images: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22266;&#20307;&#24223;&#29289;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#21355;&#26143;&#25552;&#20379;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#65292;&#36890;&#36807;&#36965;&#24863;&#22270;&#20687;&#23454;&#29616;&#20102;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#30340;&#35782;&#21035;&#12289;&#30417;&#27979;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#34920;&#24449;&#38750;&#27861;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#23545;&#29615;&#22659;&#20445;&#25252;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#24212;&#23545;&#27745;&#26579;&#21644;&#20581;&#24247;&#21361;&#23475;&#12290;&#19981;&#24403;&#31649;&#29702;&#30340;&#22403;&#22334;&#22635;&#22475;&#22330;&#36890;&#36807;&#38632;&#27700;&#28183;&#36879;&#27745;&#26579;&#22303;&#22756;&#21644;&#22320;&#19979;&#27700;&#65292;&#23545;&#21160;&#29289;&#21644;&#20154;&#31867;&#26500;&#25104;&#23041;&#32961;&#12290;&#20256;&#32479;&#30340;&#22635;&#22475;&#22330;&#36776;&#35782;&#26041;&#27861;&#65292;&#22914;&#29616;&#22330;&#26816;&#26597;&#65292;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#36965;&#24863;&#25216;&#26415;&#26159;&#29992;&#20110;&#35782;&#21035;&#21644;&#30417;&#27979;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#30340;&#19968;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#24191;&#27867;&#35206;&#30422;&#21644;&#22810;&#27425;&#33719;&#21462;&#12290;&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#21355;&#26143;&#37197;&#22791;&#20102;&#19968;&#31995;&#21015;&#20256;&#24863;&#22120;&#21644;&#25104;&#20687;&#33021;&#21147;&#65292;&#20960;&#21313;&#24180;&#26469;&#19968;&#30452;&#25552;&#20379;&#39640;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19987;&#38376;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#36965;&#24863;&#22270;&#20687;&#25191;&#34892;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#22914;&#24223;&#29289;&#22330;&#22320;&#26816;&#27979;&#12289;&#20542;&#20498;&#22330;&#30417;&#27979;&#21644;&#36866;&#23452;&#20301;&#32622;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09066v1 Announce Type: cross Abstract: The detection and characterization of illegal solid waste disposal sites are essential for environmental protection, particularly for mitigating pollution and health hazards. Improperly managed landfills contaminate soil and groundwater via rainwater infiltration, posing threats to both animals and humans. Traditional landfill identification approaches, such as on-site inspections, are time-consuming and expensive. Remote sensing is a cost-effective solution for the identification and monitoring of solid waste disposal sites that enables broad coverage and repeated acquisitions over time. Earth Observation (EO) satellites, equipped with an array of sensors and imaging capabilities, have been providing high-resolution data for several decades. Researchers proposed specialized techniques that leverage remote sensing imagery to perform a range of tasks such as waste site detection, dumping site monitoring, and assessment of suitable locati
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#29992;&#25143;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#22810;&#20010;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#36825;&#20123;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#21306;&#20998;&#22270;&#20687;&#23545;&#24182;&#33258;&#21160;&#26631;&#27880;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2312.03187</link><description>&lt;p&gt;
FERGI&#65306;&#26469;&#33258;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#29992;&#25143;&#20559;&#22909;&#30340;&#33258;&#21160;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
FERGI: Automatic Annotation of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03187
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#29992;&#25143;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#22810;&#20010;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#36825;&#20123;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#21306;&#20998;&#22270;&#20687;&#23545;&#24182;&#33258;&#21160;&#26631;&#27880;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20351;&#29992;&#20154;&#31867;&#20559;&#22909;&#21453;&#39304;&#25968;&#25454;&#26469;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#65292;&#20154;&#31867;&#21453;&#39304;&#25910;&#38598;&#30340;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20174;&#29992;&#25143;&#30340;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#20854;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#21040;&#29983;&#25104;&#22270;&#20687;&#65288;FERGI&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#20010;&#38754;&#37096;&#36816;&#21160;&#21333;&#20803;&#65288;AUs&#65289;&#30340;&#28608;&#27963;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AU4&#65288;&#30473;&#27611;&#19979;&#22402;&#32773;&#65289;&#21453;&#26144;&#20102;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#36127;&#38754;&#35780;&#20215;&#65292;&#32780;AU12&#65288;&#22068;&#35282;&#25289;&#21160;&#32773;&#65289;&#21453;&#26144;&#20102;&#27491;&#38754;&#35780;&#20215;&#12290;&#36825;&#20004;&#32773;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#24456;&#26377;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#22320;&#20351;&#29992;&#36825;&#20123;AU&#21709;&#24212;&#23384;&#22312;&#23454;&#36136;&#24046;&#24322;&#30340;&#22270;&#20687;&#23545;&#20043;&#38388;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03187v2 Announce Type: replace-cross  Abstract: Researchers have proposed to use data of human preference feedback to fine-tune text-to-image generative models. However, the scalability of human feedback collection has been limited by its reliance on manual annotation. Therefore, we develop and test a method to automatically annotate user preferences from their spontaneous facial expression reaction to the generated images. We collect a dataset of Facial Expression Reaction to Generated Images (FERGI) and show that the activations of multiple facial action units (AUs) are highly correlated with user evaluations of the generated images. Specifically, AU4 (brow lowerer) is reflective of negative evaluations of the generated image whereas AU12 (lip corner puller) is reflective of positive evaluations. These can be useful in two ways. Firstly, we can automatically annotate user preferences between image pairs with substantial difference in these AU responses with an accuracy sig
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#39118;&#30005;&#22330;&#20013;&#39118;&#21147;&#21457;&#30005;&#26426;&#30340;&#27178;&#25130;&#38754;&#21644;&#26102;&#38388;&#23618;&#27425;&#32467;&#26500;&#65292;&#26500;&#24314;&#36328;&#26102;&#23618;&#27425;&#32467;&#26500;&#65292;&#20174;&#32780;&#25552;&#39640;&#39118;&#30005;&#22330;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2308.03472</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#27425;&#32467;&#26500;&#25552;&#39640;&#39118;&#21147;&#21457;&#30005;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the forecast accuracy of wind power by leveraging multiple hierarchical structure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.03472
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#39118;&#30005;&#22330;&#20013;&#39118;&#21147;&#21457;&#30005;&#26426;&#30340;&#27178;&#25130;&#38754;&#21644;&#26102;&#38388;&#23618;&#27425;&#32467;&#26500;&#65292;&#26500;&#24314;&#36328;&#26102;&#23618;&#27425;&#32467;&#26500;&#65292;&#20174;&#32780;&#25552;&#39640;&#39118;&#30005;&#22330;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#23545;&#20840;&#29699;&#20943;&#30899;&#33267;&#20851;&#37325;&#35201;&#12290;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#65292;&#29305;&#21035;&#26159;&#39118;&#33021;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#39118;&#33021;&#21457;&#30005;&#21463;&#27668;&#20505;&#26465;&#20214;&#30340;&#19981;&#30830;&#23450;&#24615;&#24433;&#21709;&#12290;&#26368;&#36817;&#36890;&#36807;&#21327;&#35843;&#23454;&#29616;&#30340;&#23618;&#27425;&#39044;&#27979;&#22312;&#30701;&#26399;&#20869;&#26174;&#33879;&#25552;&#39640;&#20102;&#39118;&#33021;&#39044;&#27979;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#21033;&#29992;&#39118;&#30005;&#22330;&#20013;&#39118;&#21147;&#21457;&#30005;&#26426;&#30340;&#27178;&#25130;&#38754;&#21644;&#26102;&#38388;&#23618;&#27425;&#32467;&#26500;&#65292;&#26500;&#24314;&#27178;&#26102;&#23618;&#27425;&#32467;&#26500;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#36328;&#27178;&#25130;&#38754;&#21644;&#26102;&#38388;&#32500;&#24230;&#30340;&#25972;&#21512;&#22914;&#20309;&#22686;&#21152;&#39118;&#30005;&#22330;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36328;&#26102;&#38388;&#21327;&#35843;&#22312;&#22810;&#20010;&#26102;&#38388;&#27719;&#24635;&#20013;&#20248;&#20110;&#21333;&#29420;&#36328;&#27178;&#25130;&#38754;&#21327;&#35843;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36328;&#26102;&#21327;&#35843;&#39044;&#27979;&#34920;&#29616;&#20986;&#23545;&#36739;&#31895;&#26102;&#38388;&#32858;&#21512;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.03472v2 Announce Type: replace  Abstract: Renewable energy generation is of utmost importance for global decarbonization. Forecasting renewable energies, particularly wind energy, is challenging due to the inherent uncertainty in wind energy generation, which depends on weather conditions. Recent advances in hierarchical forecasting through reconciliation have demonstrated a significant increase in the quality of wind energy forecasts for short-term periods. We leverage the cross-sectional and temporal hierarchical structure of turbines in wind farms and build cross-temporal hierarchies to further investigate how integrated cross-sectional and temporal dimensions can add value to forecast accuracy in wind farms. We found that cross-temporal reconciliation was superior to individual cross-sectional reconciliation at multiple temporal aggregations. Additionally, machine learning based forecasts that were cross-temporally reconciled demonstrated high accuracy at coarser tempora
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;Conformal Prediction&#30340;Gaussian Process Regression&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27169;&#22411;&#23436;&#20840;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#20445;&#35777;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#35206;&#30422;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.15641</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#20445;&#35777;&#35206;&#30422;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Guaranteed Coverage Prediction Intervals with Gaussian Process Regression. (arXiv:2310.15641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;Conformal Prediction&#30340;Gaussian Process Regression&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27169;&#22411;&#23436;&#20840;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#20445;&#35777;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#35206;&#30422;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#22238;&#24402;&#26041;&#27861;&#65292;&#19982;&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#19981;&#21516;&#65292;&#23427;&#25552;&#20379;&#20102;&#20854;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26159;&#22522;&#20110;&#27169;&#22411;&#20551;&#35774;&#27491;&#30830;&#30340;&#21069;&#25552;&#19979;&#36827;&#34892;&#30340;&#65292;&#32780;&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25152;&#38656;&#30340;&#30693;&#35782;&#24456;&#23569;&#21487;&#29992;&#65292;&#36825;&#23548;&#33268;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21487;&#33021;&#38750;&#24120;&#35823;&#23548;&#20154;&#65292;&#20363;&#22914;&#23545;&#20110;95&#65285;&#32622;&#20449;&#27700;&#24179;&#20135;&#29983;&#30340;&#39044;&#27979;&#21306;&#38388;&#21487;&#33021;&#21482;&#35206;&#30422;&#20102;&#23569;&#20110;95&#65285;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;Conformal Prediction&#65288;CP&#65289;&#30340;GPR&#25193;&#23637;&#12290;&#36825;&#31181;&#25193;&#23637;&#21487;&#20197;&#22312;&#27169;&#22411;&#23436;&#20840;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#20445;&#35777;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#35206;&#30422;&#24230;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;GPR&#30340;&#20248;&#21183;&#21644;CP&#30340;&#26377;&#25928;&#35206;&#30422;&#20445;&#35777;&#65292;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian Process Regression (GPR) is a popular regression method, which unlike most Machine Learning techniques, provides estimates of uncertainty for its predictions. These uncertainty estimates however, are based on the assumption that the model is well-specified, an assumption that is violated in most practical applications, since the required knowledge is rarely available. As a result, the produced uncertainty estimates can become very misleading; for example the prediction intervals (PIs) produced for the 95\% confidence level may cover much less than 95\% of the true labels. To address this issue, this paper introduces an extension of GPR based on a Machine Learning framework called, Conformal Prediction (CP). This extension guarantees the production of PIs with the required coverage even when the model is completely misspecified. The proposed approach combines the advantages of GPR with the valid coverage guarantee of CP, while the performed experimental results demonstrate its 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#20808;&#39564;&#30340;&#25554;&#20837;&#24335;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.10835</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#20808;&#39564;&#30340;&#21487;&#35777;&#26126;&#30340;&#27010;&#29575;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Provable Probabilistic Imaging using Score-Based Generative Priors. (arXiv:2310.10835v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#20808;&#39564;&#30340;&#25554;&#20837;&#24335;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#21453;&#38382;&#39064;&#26102;&#65292;&#20272;&#35745;&#39640;&#36136;&#37327;&#22270;&#20687;&#24182;&#37327;&#21270;&#20854;&#19981;&#30830;&#23450;&#24615;&#26159;&#22270;&#20687;&#37325;&#24314;&#31639;&#27861;&#20013;&#30340;&#20004;&#20010;&#29702;&#24819;&#29305;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25554;&#20837;&#24335;&#33945;&#29305;&#21345;&#27931;&#65288;PMC&#65289;&#20316;&#20026;&#19968;&#31181;&#23545;&#19968;&#33324;&#21453;&#38382;&#39064;&#21487;&#33021;&#35299;&#31354;&#38388;&#36827;&#34892;&#24314;&#27169;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;PMC&#33021;&#22815;&#36890;&#36807;&#21518;&#39564;&#37319;&#26679;&#26469;&#32467;&#21512;&#20016;&#23500;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#20808;&#39564;&#36827;&#34892;&#39640;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#65292;&#24182;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;PMC&#31639;&#27861;&#65292;&#21487;&#20197;&#35270;&#20026;&#20256;&#32479;&#25554;&#20837;&#24335;&#20808;&#39564;&#65288;PnP&#65289;&#21644;&#21435;&#22122;&#27491;&#21017;&#21270;&#65288;RED&#65289;&#31639;&#27861;&#30340;&#37319;&#26679;&#27169;&#25311;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#23545;PMC&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20026;&#20004;&#31181;&#31639;&#27861;&#25552;&#20379;&#20102;&#38750;&#28176;&#36817;&#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#21363;&#20351;&#22312;&#38750;&#23545;&#25968;&#20985;&#20284;&#28982;&#21644;&#19981;&#23436;&#32654;&#24471;&#20998;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating high-quality images while also quantifying their uncertainty are two desired features in an image reconstruction algorithm for solving ill-posed inverse problems. In this paper, we propose plug-and-play Monte Carlo (PMC) as a principled framework for characterizing the space of possible solutions to a general inverse problem. PMC is able to incorporate expressive score-based generative priors for high-quality image reconstruction while also performing uncertainty quantification via posterior sampling. In particular, we introduce two PMC algorithms which can be viewed as the sampling analogues of the traditional plug-and-play priors (PnP) and regularization by denoising (RED) algorithms. We also establish a theoretical analysis for characterizing the convergence of the PMC algorithms. Our analysis provides non-asymptotic stationarity guarantees for both algorithms, even in the presence of non-log-concave likelihoods and imperfect score networks. We demonstrate the performance
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24230;&#37327;&#24544;&#23454;&#24615;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;&#23631;&#34109;&#20196;&#29260;&#20316;&#20026;&#35774;&#35745;&#20351;&#20854;&#25104;&#20026;&#20998;&#24067;&#20869;&#65292;&#20197;&#35299;&#20915;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26102;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07819</link><description>&lt;p&gt;
&#21487;&#24230;&#37327;&#24544;&#23454;&#24615;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Faithfulness Measurable Masked Language Models. (arXiv:2310.07819v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24230;&#37327;&#24544;&#23454;&#24615;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;&#23631;&#34109;&#20196;&#29260;&#20316;&#20026;&#35774;&#35745;&#20351;&#20854;&#25104;&#20026;&#20998;&#24067;&#20869;&#65292;&#20197;&#35299;&#20915;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26102;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#37325;&#35201;&#24615;&#24230;&#37327;&#26469;&#34920;&#36798;&#21738;&#20123;&#20196;&#29260;&#23545;&#20110;&#39044;&#27979;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#35299;&#37322;&#20855;&#26377;&#35828;&#26381;&#21147;&#65292;&#20294;&#24448;&#24448;&#26159;&#38169;&#35823;&#30340;&#12290;&#22240;&#27492;&#65292;&#27979;&#37327;&#23427;&#20204;&#30340;&#24544;&#23454;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#31181;&#24230;&#37327;&#26631;&#20934;&#26159;&#22914;&#26524;&#20196;&#29260;&#30830;&#23454;&#24456;&#37325;&#35201;&#65292;&#37027;&#20040;&#23631;&#34109;&#23427;&#20204;&#24212;&#35813;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#21464;&#24046;&#12290;&#28982;&#32780;&#65292;&#20196;&#29260;&#23631;&#34109;&#20250;&#24341;&#20837;&#21306;&#22495;&#22806;&#38382;&#39064;&#65292;&#32780;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#35745;&#31639;&#19978;&#24456;&#26114;&#36149;&#24182;&#19988;&#20351;&#29992;&#20195;&#29702;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20854;&#20182;&#25351;&#26631;&#30340;&#36866;&#29992;&#33539;&#22260;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22266;&#26377;&#30340;&#24544;&#23454;&#24615;&#21487;&#24230;&#37327;&#27169;&#22411;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#35813;&#26041;&#27861;&#23558;&#23631;&#34109;&#20196;&#29260;&#20316;&#20026;&#35774;&#35745;&#20351;&#20854;&#25104;&#20026;&#20998;&#24067;&#20869;&#12290;&#36825;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#29616;&#26377;&#26041;&#27861;&#23436;&#20840;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#19981;&#36866;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common approach to explain NLP models, is to use importance measures that express which tokens are important for a prediction. Unfortunately, such explanations are often wrong despite being persuasive. Therefore, it is essential to measure their faithfulness. One such metric is if tokens are truly important, then masking them should result in worse model performance. However, token masking introduces out-of-distribution issues and existing solutions are computationally expensive and employ proxy-models. Furthermore, other metrics are very limited in scope. In this work, we propose an inherently faithfulness measurable model that addresses these challenges. This is achieved by using a novel fine-tuning method that incorporates masking, such that masking tokens become in-distribution by design. This differs from existing approaches, which are completely model-agnostic but are inapplicable in practice. We demonstrate the generality of our approach by applying it to various tasks and val
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34880;&#28165;&#20998;&#31867;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22810;&#32500;&#21644;&#26377;&#26434;&#36136;&#30340;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#30340;&#20998;&#31867;&#21644;&#20272;&#35745;&#24739;&#30149;&#29575;&#26469;&#20943;&#23569;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#30452;&#25509;&#35775;&#38382;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#32780;&#26159;&#23558;&#25968;&#25454;&#23884;&#20837;&#21442;&#25968;&#21270;&#30340;&#26354;&#32447;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#32463;&#39564;&#35823;&#24046;&#26469;&#20248;&#21270;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.00645</link><description>&lt;p&gt;
&#23545;&#20110;&#22810;&#32500;&#21644;&#26434;&#36136;&#35757;&#32451;&#25968;&#25454;&#30340;&#26368;&#20248;&#34880;&#28165;&#20998;&#31867;&#30340;&#26368;&#23567;&#20551;&#35774;&#65306;&#29702;&#35770;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Minimal Assumptions for Optimal Serology Classification: Theory and Implications for Multidimensional Settings and Impure Training Data. (arXiv:2309.00645v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34880;&#28165;&#20998;&#31867;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22810;&#32500;&#21644;&#26377;&#26434;&#36136;&#30340;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#30340;&#20998;&#31867;&#21644;&#20272;&#35745;&#24739;&#30149;&#29575;&#26469;&#20943;&#23569;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#30452;&#25509;&#35775;&#38382;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#32780;&#26159;&#23558;&#25968;&#25454;&#23884;&#20837;&#21442;&#25968;&#21270;&#30340;&#26354;&#32447;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#32463;&#39564;&#35823;&#24046;&#26469;&#20248;&#21270;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34880;&#28165;&#23398;&#20013;&#65292;&#20943;&#23569;&#20559;&#24046;&#20272;&#35745;&#21644;&#35786;&#26029;&#20998;&#31867;&#22120;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29702;&#35770;&#19978;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36716;&#21270;&#20026;&#24314;&#27169;&#27979;&#37327;&#32467;&#26524;&#30340;&#31867;&#21035;-&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65288;PDFs&#65289;&#65292;&#23427;&#20204;&#25511;&#21046;&#25152;&#26377;&#21518;&#32493;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#20165;&#20855;&#26377;&#23569;&#25968;&#32500;&#24230;&#65288;&#20363;&#22914;&#30446;&#26631;&#25239;&#21407;&#65289;&#30340;&#27979;&#37327;&#36755;&#20986;&#65292;&#36825;&#20010;&#20219;&#21153;&#20063;&#24456;&#24555;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#21033;&#29992;&#32463;&#39564;&#35757;&#32451;&#25968;&#25454;&#22312;&#20219;&#24847;&#32500;&#24230;&#19978;&#20998;&#31867;&#26679;&#26412;&#21644;&#20272;&#35745;&#24739;&#30149;&#29575;&#65292;&#32780;&#19981;&#38656;&#35201;&#30452;&#25509;&#35775;&#38382;&#26465;&#20214;PDFs&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#24341;&#29702;&#26469;&#35299;&#37322;&#36825;&#20010;&#26041;&#27861;&#65292;&#35813;&#24341;&#29702;&#23558;&#30456;&#23545;&#26465;&#20214;&#27010;&#29575;&#19982;&#26368;&#23567;&#35823;&#24046;&#20998;&#31867;&#36793;&#30028;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21046;&#23450;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65306;&#65288;i&#65289;&#23558;&#25968;&#25454;&#23884;&#20837;&#21442;&#25968;&#21270;&#30340;&#26354;&#32447;&#31354;&#38388;&#65307;&#65288;ii&#65289;&#26681;&#25454;&#26679;&#26412;&#30456;&#23545;&#20110;&#22352;&#26631;&#36724;&#30340;&#20301;&#32622;&#23545;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#65307;&#65288;iii&#65289;&#36890;&#36807;&#26368;&#23567;&#21270;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
Minimizing error in prevalence estimates and diagnostic classifiers remains a challenging task in serology. In theory, these problems can be reduced to modeling class-conditional probability densities (PDFs) of measurement outcomes, which control all downstream analyses. However, this task quickly succumbs to the curse of dimensionality, even for assay outputs with only a few dimensions (e.g. target antigens). To address this problem, we propose a technique that uses empirical training data to classify samples and estimate prevalence in arbitrary dimension without direct access to the conditional PDFs. We motivate this method via a lemma that relates relative conditional probabilities to minimum-error classification boundaries. This leads us to formulate an optimization problem that: (i) embeds the data in a parameterized, curved space; (ii) classifies samples based on their position relative to a coordinate axis; and (iii) subsequently optimizes the space by minimizing the empirical c
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35752;&#35770;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22914;&#20309;&#22312;&#37096;&#20998;&#30417;&#30563;&#35774;&#32622;&#19979;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22797;&#26434;&#30340;&#20248;&#21270;&#26041;&#26696;&#21644;&#39640;&#26631;&#31614;&#38656;&#27714;&#32780;&#24341;&#20837;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.14382</link><description>&lt;p&gt;
&#24403;&#22810;&#20219;&#21153;&#23398;&#20064;&#36935;&#21040;&#37096;&#20998;&#30417;&#30563;&#65306;&#35745;&#31639;&#26426;&#35270;&#35273;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
When Multi-Task Learning Meets Partial Supervision: A Computer Vision Review. (arXiv:2307.14382v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35752;&#35770;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22914;&#20309;&#22312;&#37096;&#20998;&#30417;&#30563;&#35774;&#32622;&#19979;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22797;&#26434;&#30340;&#20248;&#21270;&#26041;&#26696;&#21644;&#39640;&#26631;&#31614;&#38656;&#27714;&#32780;&#24341;&#20837;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;(MTL)&#26088;&#22312;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#36164;&#28304;&#21516;&#26102;&#35745;&#31639;&#22810;&#20010;&#36755;&#20986;&#65292;&#36825;&#31181;&#23398;&#20064;&#33539;&#24335;&#26377;&#28508;&#21147;&#27604;&#20256;&#32479;&#26041;&#27861;&#22312;&#20869;&#23384;&#38656;&#27714;&#21644;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#26356;&#20302;&#12290;&#20197;&#24448;&#30340;MTL&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#19978;&#65292;&#22240;&#20026;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#21487;&#20197;&#38477;&#20302;&#36825;&#20123;&#26041;&#27861;&#23545;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;MTL&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#20248;&#21270;&#26041;&#26696;&#21644;&#26356;&#39640;&#30340;&#26631;&#31614;&#38656;&#27714;&#12290;&#26412;&#32508;&#36848;&#30528;&#37325;&#20110;MTL&#22914;&#20309;&#22312;&#19981;&#21516;&#30340;&#37096;&#20998;&#30417;&#30563;&#35774;&#32622;&#19979;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#26412;&#32508;&#36848;&#20998;&#26512;&#20102;MTL&#20256;&#32479;&#19978;&#22914;&#20309;&#20351;&#29992;&#19981;&#21516;&#30340;&#21442;&#25968;&#20849;&#20139;&#25216;&#26415;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#30693;&#35782;&#36716;&#31227;&#12290;&#20854;&#27425;&#65292;&#23427;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while exploiting their mutual relationships. By using shared resources to simultaneously calculate multiple outputs, this learning paradigm has the potential to have lower memory requirements and inference times compared to the traditional approach of using separate methods for each task. Previous work in MTL has mainly focused on fully-supervised methods, as task relationships can not only be leveraged to lower the level of data-dependency of those methods but they can also improve performance. However, MTL introduces a set of challenges due to a complex optimisation scheme and a higher labeling requirement. This review focuses on how MTL could be utilised under different partial supervision settings to address these challenges. First, this review analyses how MTL traditionally uses different parameter sharing techniques to transfer knowledge in between tasks. Second, it presents the different challenges arising fro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#65292;&#21517;&#20026;MIMIC-Diff-VQA&#65292;&#20026;&#33258;&#21160;&#21270;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#22238;&#31572;&#20851;&#20110;&#30142;&#30149;&#21644;&#22270;&#20687;&#24046;&#24322;&#30340;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20102;&#19987;&#23478;&#30693;&#35782;&#24863;&#30693;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.11986</link><description>&lt;p&gt;
Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering.&#65288;&#19987;&#23478;&#30693;&#35782;&#24863;&#30693;&#30340;&#22270;&#20687;&#21464;&#21270;&#22270;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#20851;&#27880;&#24046;&#24322;&#30340;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#65289;
&lt;/p&gt;
&lt;p&gt;
Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering. (arXiv:2307.11986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#65292;&#21517;&#20026;MIMIC-Diff-VQA&#65292;&#20026;&#33258;&#21160;&#21270;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#22238;&#31572;&#20851;&#20110;&#30142;&#30149;&#21644;&#22270;&#20687;&#24046;&#24322;&#30340;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20102;&#19987;&#23478;&#30693;&#35782;&#24863;&#30693;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20026;&#33258;&#21160;&#21270;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33016;&#37096;X&#20809;&#22270;&#20687;&#24046;&#24322;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#22238;&#31572;&#20960;&#20010;&#20851;&#20110;&#30142;&#30149;&#20197;&#21450;&#26356;&#37325;&#35201;&#30340;&#26159;&#23427;&#20204;&#20043;&#38388;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#36825;&#19982;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#35786;&#26029;&#23454;&#36341;&#30456;&#19968;&#33268;&#65292;&#25918;&#23556;&#31185;&#21307;&#29983;&#22312;&#24471;&#20986;&#25253;&#21578;&#20043;&#21069;&#20250;&#23545;&#24403;&#21069;&#22270;&#20687;&#19982;&#21442;&#32771;&#22270;&#20687;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;MIMIC-Diff-VQA&#65292;&#21253;&#25324;&#26469;&#33258;164,324&#23545;&#20027;&#22270;&#20687;&#21644;&#21442;&#32771;&#22270;&#20687;&#30340;700,703&#20010;&#38382;&#39064;-&#31572;&#26696;&#37197;&#23545;&#12290;&#19982;&#29616;&#26377;&#30340;&#21307;&#23398;VQA&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#38382;&#39064;&#38024;&#23545;&#20102;&#20020;&#24202;&#19987;&#19994;&#20154;&#21592;&#20351;&#29992;&#30340;&#35780;&#20272;-&#35786;&#26029;-&#24178;&#39044;-&#35780;&#20272;&#27835;&#30103;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#30693;&#35782;&#24863;&#30693;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
To contribute to automating the medical vision-language model, we propose a novel Chest-Xray Difference Visual Question Answering (VQA) task. Given a pair of main and reference images, this task attempts to answer several questions on both diseases and, more importantly, the differences between them. This is consistent with the radiologist's diagnosis practice that compares the current image with the reference before concluding the report. We collect a new dataset, namely MIMIC-Diff-VQA, including 700,703 QA pairs from 164,324 pairs of main and reference images. Compared to existing medical VQA datasets, our questions are tailored to the Assessment-Diagnosis-Intervention-Evaluation treatment procedure used by clinical professionals. Meanwhile, we also propose a novel expert knowledge-aware graph representation learning model to address this task. The proposed baseline model leverages expert knowledge such as anatomical structure prior, semantic, and spatial knowledge to construct a mul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#20809;&#20239;&#30005;&#27744;&#32452;&#28151;&#21512;&#29289;&#29702;&#27010;&#29575;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25968;&#20540;&#22825;&#27668;&#39044;&#27979;&#32467;&#26524;&#20316;&#20026;&#21327;&#21464;&#37327;&#65292;&#25913;&#21892;&#20102;&#20809;&#20239;&#31995;&#32479;&#21151;&#29575;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#21487;&#20197;&#36798;&#21040;7.54&#65285;&#30340;&#25216;&#33021;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2303.08459</link><description>&lt;p&gt;
&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#20809;&#20239;&#30005;&#27744;&#32452;&#30340;&#28151;&#21512;&#29289;&#29702;&#27010;&#29575;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hybrid-Physical Probabilistic Forecasting for a Set of Photovoltaic Systems using Recurrent Neural Networks. (arXiv:2303.08459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#20809;&#20239;&#30005;&#27744;&#32452;&#28151;&#21512;&#29289;&#29702;&#27010;&#29575;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25968;&#20540;&#22825;&#27668;&#39044;&#27979;&#32467;&#26524;&#20316;&#20026;&#21327;&#21464;&#37327;&#65292;&#25913;&#21892;&#20102;&#20809;&#20239;&#31995;&#32479;&#21151;&#29575;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#21487;&#20197;&#36798;&#21040;7.54&#65285;&#30340;&#25216;&#33021;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#20809;&#20239;&#31995;&#32479;&#30340;&#21151;&#29575;&#36755;&#20986;&#23545;&#20110;&#25913;&#21892;&#33021;&#28304;&#20998;&#24067;&#32593;&#32476;&#30340;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;-&#29289;&#29702;&#27169;&#22411;&#65292;&#22312;&#25968;&#20540;&#22825;&#27668;&#39044;&#27979;&#30340;&#24110;&#21161;&#19979;&#65292;&#36890;&#36807;&#20351;&#29992;&#20854;&#20316;&#20026;&#21327;&#21464;&#37327;&#30340;PV&#24615;&#33021;&#27169;&#22411;&#21644;&#33258;&#22238;&#24402;&#36882;&#24402;&#31070;&#32463;&#27169;&#22411;&#26469;&#25913;&#36827;&#30830;&#23450;&#24615;&#30340;&#30701;&#26102;&#39044;&#27979;&#12290;&#25105;&#20204;&#37325;&#26032;&#35774;&#35745;&#20102;&#26368;&#21021;&#29992;&#20110;&#38646;&#21806;&#39046;&#22495;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#25130;&#26029;&#39640;&#26031;&#36755;&#20986;&#20998;&#24067;&#12290;&#25105;&#20204;&#23558;&#35768;&#22810;&#27169;&#22411;&#21464;&#37327;&#19982;&#25991;&#29486;&#20013;&#30340;&#26367;&#20195;&#26041;&#26696;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#65292;&#24182;&#19988;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#26368;&#20339;&#24615;&#33021;&#21464;&#20307;&#20013;&#30340;&#32452;&#20214;&#21327;&#21516;&#24037;&#20316;&#20197;&#36798;&#21040;&#19982;NWP&#39537;&#21160;&#30340;PV&#24615;&#33021;&#27169;&#22411;&#22522;&#32447;&#30456;&#27604;&#30340;&#25216;&#33021;&#35780;&#20998;&#20026;7.54&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate intra-day forecasts of the power output by PhotoVoltaic (PV) systems are critical to improve the operation of energy distribution grids. We describe a hybrid-physical model, which aims at improving deterministic intra-day forecasts, issued by a PV performance model fed by Numerical Weather Predictions (NWP), by using them as covariates in the context of an autoregressive recurrent neural model. Our proposal repurposes a neural model initially used in the retail sector, and discloses a novel truncated Gaussian output distribution. We experimentally compare many model variants to alternatives from the literature, and an ablation study shows that the components in the best performing variant work synergistically to reach a skill score of 7.54% with respect to the NWP-driven PV performance model baseline.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#32452;&#21512;&#29983;&#25104;&#20013;&#30340;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2302.11552</link><description>&lt;p&gt;
&#20943;&#23569;&#12289;&#37325;&#22797;&#21033;&#29992;&#12289;&#22238;&#25910;&#65306;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC. (arXiv:2302.11552v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11552
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#25193;&#25955;&#27169;&#22411;&#21644;MCMC&#30340;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#22312;&#32452;&#21512;&#29983;&#25104;&#20013;&#30340;&#22833;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#25193;&#25955;&#27169;&#22411;&#38382;&#19990;&#20197;&#26469;&#65292;&#23427;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24050;&#32463;&#36805;&#36895;&#25104;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#23427;&#20204;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#23398;&#20064;&#19968;&#31995;&#21015;&#26102;&#21464;&#30340;&#23545;&#25968;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#36825;&#31181;&#35299;&#37322;&#24050;&#32463;&#28608;&#21457;&#20102;&#22522;&#20110;&#20998;&#31867;&#22120;&#21644;&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#24605;&#24819;&#25104;&#20026;&#21518;&#32493;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#22312;&#36825;&#20123;&#24819;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#25968;-based&#35299;&#37322;&#65292;&#25506;&#32034;&#20102;&#29992;&#20110;&#28041;&#21450;&#32452;&#21512;&#29983;&#25104;&#21644;&#25351;&#23548;&#30340;&#26465;&#20214;&#12289;&#20462;&#25913;&#21644;&#37325;&#22797;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20026;&#20160;&#20040;&#26576;&#20123;&#31867;&#22411;&#30340;&#32452;&#21512;&#20351;&#29992;&#24403;&#21069;&#25216;&#26415;&#22833;&#36133;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#37319;&#26679;&#32773;(&#32780;&#19981;&#26159;&#27169;&#22411;)&#23545;&#27492;&#22833;&#36133;&#36127;&#26377;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#37319;&#26679;&#22120;&#65292;&#21463;MCMC&#30340;&#21551;&#21457;&#65292;&#20351;&#32452;&#21512;&#29983;&#25104;&#25104;&#21151;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#26356;&#21152;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since their introduction, diffusion models have quickly become the prevailing approach to generative modeling in many domains. They can be interpreted as learning the gradients of a time-varying sequence of log-probability density functions. This interpretation has motivated classifier-based and classifier-free guidance as methods for post-hoc control of diffusion models. In this work, we build upon these ideas using the score-based interpretation of diffusion models, and explore alternative ways to condition, modify, and reuse diffusion models for tasks involving compositional generation and guidance. In particular, we investigate why certain types of composition fail using current techniques and present a number of solutions. We conclude that the sampler (not the model) is responsible for this failure and propose new samplers, inspired by MCMC, which enable successful compositional generation. Further, we propose an energy-based parameterization of diffusion models which enables the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36328;&#27169;&#24577;&#36866;&#24212;&#26041;&#27861;&#65292;&#22312;&#22810;&#27169;&#24577;&#27169;&#22411;&#19979;&#21033;&#29992;&#23569;&#26679;&#26412;&#31034;&#20363;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22768;&#38899;&#65289;&#36827;&#34892;&#29399;&#30340;&#35270;&#35273;&#20998;&#31867;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.06267</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#26377;&#21161;&#20110;&#21333;&#27169;&#24577;&#65306;&#22810;&#27169;&#24577;&#27169;&#22411;&#19979;&#30340;&#20132;&#21449;&#27169;&#24577;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models. (arXiv:2301.06267v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06267
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#24577;&#36866;&#24212;&#26041;&#27861;&#65292;&#22312;&#22810;&#27169;&#24577;&#27169;&#22411;&#19979;&#21033;&#29992;&#23569;&#26679;&#26412;&#31034;&#20363;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22768;&#38899;&#65289;&#36827;&#34892;&#29399;&#30340;&#35270;&#35273;&#20998;&#31867;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#26159;&#26234;&#33021;&#20195;&#29702;&#30340;&#26680;&#24515;&#35201;&#32032;&#65292;&#20063;&#34987;&#31216;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20256;&#32479;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#20351;&#29992;&#26469;&#33258;&#21333;&#27169;&#24577;&#30340;&#23569;&#26679;&#26412;&#26679;&#26412;&#65292;&#20294;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#19981;&#36275;&#20197;&#25551;&#36848;&#25972;&#20010;&#27010;&#24565;&#31867;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#20351;&#29992;&#36328;&#27169;&#24577;&#20449;&#24687;&#39640;&#25928;&#22320;&#23398;&#20064;&#26032;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#38405;&#35835;&#20851;&#20110;&#29399;&#24182;&#21548;&#23427;&#20204;&#21536;&#21483;&#30340;&#22768;&#38899;&#26469;&#26500;&#24314;&#26356;&#22909;&#30340;&#35270;&#35273;&#29399;&#20998;&#31867;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#26159;&#22266;&#26377;&#30340;&#36328;&#27169;&#24577;&#30340;&#29305;&#24615;&#65292;&#23558;&#19981;&#21516;&#30340;&#27169;&#24577;&#26144;&#23556;&#21040;&#30456;&#21516;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36328;&#27169;&#24577;&#36866;&#24212;&#26041;&#27861;&#65292;&#20174;&#36328;&#36234;&#19981;&#21516;&#27169;&#24577;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#36890;&#36807;&#23558;&#31867;&#21517;&#37325;&#26032;&#29992;&#20316;&#39069;&#22806;&#30340;&#19968;&#27425;&#24615;&#35757;&#32451;&#26679;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26497;&#20854;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to quickly learn a new task with minimal instruction - known as few-shot learning - is a central aspect of intelligent agents. Classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufficient to characterize an entire concept class. In contrast, humans use cross-modal information to learn new concepts efficiently. In this work, we demonstrate that one can indeed build a better ${\bf visual}$ dog classifier by ${\bf read}$ing about dogs and ${\bf listen}$ing to them bark. To do so, we exploit the fact that recent multimodal foundation models such as CLIP are inherently cross-modal, mapping different modalities to the same representation space. Specifically, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning different modalities. By repurposing class names as additional one-shot training samples, we achieve SOTA results with an embarrassingly simple linear classifier for visi
&lt;/p&gt;</description></item><item><title>&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#31639;&#27861;&#31867;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#31232;&#32570;&#39640;&#32500;&#25968;&#25454;&#30340;&#20219;&#24847;&#30446;&#26631;&#20998;&#24067;&#24182;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25968;&#25454;&#25972;&#21512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.17230</link><description>&lt;p&gt;
Lipschitz&#27491;&#21017;&#21270;&#26799;&#24230;&#27969;&#21644;&#39640;&#32500;&#31232;&#32570;&#25968;&#25454;&#30340;&#29983;&#25104;&#31890;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lipschitz-regularized gradient flows and generative particle algorithms for high-dimensional scarce data. (arXiv:2210.17230v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17230
&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#31639;&#27861;&#31867;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#31232;&#32570;&#39640;&#32500;&#25968;&#25454;&#30340;&#20219;&#24847;&#30446;&#26631;&#20998;&#24067;&#24182;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#25968;&#25454;&#25972;&#21512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#31639;&#27861;&#31867;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#21487;&#33021;&#31232;&#32570;&#12289;&#39640;&#32500;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#20219;&#24847;&#30446;&#26631;&#20998;&#24067;&#65292;&#24182;&#29983;&#25104;&#26032;&#30340;&#26679;&#26412;&#12290;&#36825;&#20123;&#29983;&#25104;&#31639;&#27861;&#26159;&#22522;&#20110;&#31890;&#23376;&#30340;&#65292;&#24182;&#19988;&#26159;&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;Kullback-Leibler&#25110;&#20854;&#20182;f-&#25955;&#24230;&#30340;&#26799;&#24230;&#27969;&#26469;&#26500;&#36896;&#30340;&#65292;&#20854;&#20013;&#26469;&#33258;&#28304;&#20998;&#24067;&#30340;&#25968;&#25454;&#21487;&#20197;&#31283;&#23450;&#22320;&#20316;&#20026;&#31890;&#23376;&#20256;&#36755;&#21040;&#30446;&#26631;&#20998;&#24067;&#30340;&#38468;&#36817;&#12290;&#20316;&#20026;&#25968;&#25454;&#25972;&#21512;&#30340;&#19968;&#20010;&#31361;&#20986;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#27491;&#30830;&#20256;&#36755;&#32500;&#25968;&#36229;&#36807;54K&#30340;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#28857;&#65292;&#32780;&#26679;&#26412;&#37327;&#36890;&#24120;&#21482;&#26377;&#20960;&#30334;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
We build a new class of generative algorithms capable of efficiently learning an arbitrary target distribution from possibly scarce, high-dimensional data and subsequently generate new samples. These generative algorithms are particle-based and are constructed as gradient flows of Lipschitz-regularized Kullback-Leibler or other $f$-divergences, where data from a source distribution can be stably transported as particles, towards the vicinity of the target distribution. As a highlighted result in data integration, we demonstrate that the proposed algorithms correctly transport gene expression data points with dimension exceeding 54K, while the sample size is typically only in the hundreds.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#26377;&#38480;&#32500;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#21387;&#32553;&#32463;&#39564;&#27979;&#24230;&#30340;&#26041;&#27861;&#65292;&#23548;&#20986;&#20102;&#20851;&#20110;&#36825;&#26679;&#19968;&#20010;&#36817;&#20284;&#30340;&#26680;&#24515;&#38598;&#24517;&#39035;&#26377;&#30340;&#22823;&#23567;&#30340;&#39640;&#27010;&#29575;&#19979;&#38480;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20123;&#25216;&#26415;&#20197;&#23558;&#21387;&#32553;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#20307;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2204.08847</link><description>&lt;p&gt;
&#26377;&#38480;&#32500;&#19979;&#30340;&#21387;&#32553;&#32463;&#39564;&#27979;&#24230;
&lt;/p&gt;
&lt;p&gt;
Compressed Empirical Measures (in finite dimensions). (arXiv:2204.08847v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#26377;&#38480;&#32500;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#21387;&#32553;&#32463;&#39564;&#27979;&#24230;&#30340;&#26041;&#27861;&#65292;&#23548;&#20986;&#20102;&#20851;&#20110;&#36825;&#26679;&#19968;&#20010;&#36817;&#20284;&#30340;&#26680;&#24515;&#38598;&#24517;&#39035;&#26377;&#30340;&#22823;&#23567;&#30340;&#39640;&#27010;&#29575;&#19979;&#38480;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20123;&#25216;&#26415;&#20197;&#23558;&#21387;&#32553;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#20307;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26377;&#38480;&#32500;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHSs&#65289;&#20013;&#21387;&#32553;&#32463;&#39564;&#27979;&#24230;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32463;&#39564;&#27979;&#24230;&#21253;&#21547;&#22312;&#19968;&#20010;&#33258;&#28982;&#30340;&#20984;&#38598;&#20013;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#20984;&#20248;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#12290;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#36825;&#31181;&#36817;&#20284;&#20250;&#23548;&#33268;&#25968;&#25454;&#28857;&#30340;coreset&#12290;&#25511;&#21046;&#36825;&#26679;&#19968;&#20010;coreset&#24517;&#39035;&#26377;&#22810;&#22823;&#30340;&#19968;&#20010;&#20851;&#38190;&#25968;&#37327;&#26159;&#21253;&#21547;&#22312;&#32463;&#39564;&#20984;&#38598;&#20013;&#30340;&#32463;&#39564;&#27979;&#37327;&#21608;&#22260;&#30340;&#26368;&#22823;&#29699;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#26159;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#23548;&#20986;&#20851;&#20110;&#36825;&#26679;&#19968;&#20010;&#29699;&#30340;&#22823;&#23567;&#30340;&#39640;&#27010;&#29575;&#19979;&#38480;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#25216;&#26415;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23558;&#21387;&#32553;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#20307;&#30340;&#25512;&#26029;&#38382;&#39064;&#65292;&#22914;&#26680;&#23725;&#22238;&#24402;&#65292;&#26469;&#34917;&#20805;&#36825;&#31181;&#19979;&#38480;&#30340;&#27966;&#29983;&#12290;&#25105;&#20204;&#26368;&#21518;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38480;&#32500;RKHS&#30340;&#26500;&#36896;&#65292;&#20854;&#20013;&#21387;&#32553;&#24456;&#24046;&#65292;&#31361;&#20986;&#20102;&#25105;&#20204;&#38754;&#20020;&#30340;&#19968;&#20123;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study approaches for compressing the empirical measure in the context of finite dimensional reproducing kernel Hilbert spaces (RKHSs).In this context, the empirical measure is contained within a natural convex set and can be approximated using convex optimization methods.Such an approximation gives under certain conditions rise to a coreset of data points. A key quantity that controls how large such a coreset has to be is the size of the largest ball around the empirical measure that is contained within the empirical convex set. The bulk of our work is concerned with deriving high probability lower bounds on the size of such a ball under various conditions. We complement this derivation of the lower bound by developing techniques that allow us to apply the compression approach to concrete inference problems such as kernel ridge regression. We conclude with a construction of an infinite dimensional RKHS for which the compression is poor, highlighting some of the difficulties one face
&lt;/p&gt;</description></item></channel></rss>