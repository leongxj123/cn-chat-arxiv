<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#30740;&#31350;&#20102;&#32431;&#22270;&#24418;&#12289;&#25991;&#26412;&#23646;&#24615;&#22270;&#24418;&#21644;&#25991;&#26412;&#37197;&#23545;&#22270;&#24418;&#19977;&#20010;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#25506;&#35752;&#20102;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;</title><link>https://rss.arxiv.org/abs/2312.02783</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models on Graphs: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.02783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#30740;&#31350;&#20102;&#32431;&#22270;&#24418;&#12289;&#25991;&#26412;&#23646;&#24615;&#22270;&#24418;&#21644;&#25991;&#26412;&#37197;&#23545;&#22270;&#24418;&#19977;&#20010;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#25506;&#35752;&#20102;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT4&#21644;LLaMA&#65292;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#25991;&#26412;&#32534;&#30721;/&#35299;&#30721;&#33021;&#21147;&#21644;&#26032;&#21457;&#29616;&#30340;&#32039;&#24613;&#33021;&#21147;&#65288;&#20363;&#22914;&#25512;&#29702;&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#34429;&#28982;LLMs&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#32431;&#25991;&#26412;&#65292;&#20294;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#25991;&#26412;&#25968;&#25454;&#19982;&#22270;&#24418;&#24418;&#24335;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#30456;&#20851;&#32852;&#65288;&#20363;&#22914;&#23398;&#26415;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#65289;&#65292;&#25110;&#32773;&#22270;&#24418;&#25968;&#25454;&#19982;&#20016;&#23500;&#30340;&#25991;&#26412;&#20449;&#24687;&#37197;&#23545;&#65288;&#20363;&#22914;&#24102;&#26377;&#25551;&#36848;&#30340;&#20998;&#23376;&#65289;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;LLMs&#24050;&#32463;&#23637;&#31034;&#20102;&#20854;&#22522;&#20110;&#32431;&#25991;&#26412;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#27492;&#31867;&#33021;&#21147;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#21040;&#22270;&#24418;&#19978;&#65288;&#21363;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29702;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#22330;&#26223;&#21644;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#12290;&#25105;&#20204;&#39318;&#20808;&#24635;&#32467;&#20102;&#37319;&#29992;LLMs&#22312;&#22270;&#24418;&#19978;&#30340;&#28508;&#22312;&#22330;&#26223;&#65292;&#20998;&#20026;&#32431;&#22270;&#24418;&#12289;&#25991;&#26412;&#23646;&#24615;&#22270;&#24418;&#21644;&#25991;&#26412;&#37197;&#23545;&#22270;&#24418;&#19977;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-pa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Winner-Take-All&#65288;WTA&#65289;&#36873;&#25321;&#24615;&#21644;&#29983;&#29289;&#21551;&#21457;&#30340;&#31283;&#24577;&#26426;&#21046;&#30456;&#32467;&#21512;&#30340;&#8220;&#33258;&#23450;&#20041;&#30446;&#26631;&#8221;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#36793;&#32536;AI&#30828;&#20214;&#19978;&#30340;&#35745;&#31639;&#36164;&#28304;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12116</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#23450;&#20041;&#29983;&#29289;&#21551;&#21457;&#30446;&#26631;&#30340;&#26080;&#30417;&#30563;&#31471;&#21040;&#31471;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Unsupervised End-to-End Training with a Self-Defined Bio-Inspired Target
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Winner-Take-All&#65288;WTA&#65289;&#36873;&#25321;&#24615;&#21644;&#29983;&#29289;&#21551;&#21457;&#30340;&#31283;&#24577;&#26426;&#21046;&#30456;&#32467;&#21512;&#30340;&#8220;&#33258;&#23450;&#20041;&#30446;&#26631;&#8221;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#36793;&#32536;AI&#30828;&#20214;&#19978;&#30340;&#35745;&#31639;&#36164;&#28304;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#33258;&#30417;&#30563;&#23398;&#20064;&#65289;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#25110;&#32773;&#37319;&#29992;&#36890;&#36807;&#31867;&#20284;Hebbian&#23398;&#20064;&#30340;&#29983;&#29289;&#21551;&#21457;&#26041;&#27861;&#36880;&#23618;&#35757;&#32451;&#65292;&#20351;&#29992;&#19982;&#30417;&#30563;&#23398;&#20064;&#19981;&#20860;&#23481;&#30340;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#32593;&#32476;&#26368;&#32456;&#23618;&#30340;&#32988;&#32773;&#36890;&#21507;&#65288;WTA&#65289;&#36873;&#25321;&#24615;&#30340;&#8220;&#33258;&#23450;&#20041;&#30446;&#26631;&#8221;&#65292;&#24182;&#36890;&#36807;&#29983;&#29289;&#21551;&#21457;&#30340;&#31283;&#24577;&#26426;&#21046;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12116v1 Announce Type: cross  Abstract: Current unsupervised learning methods depend on end-to-end training via deep learning techniques such as self-supervised learning, with high computational requirements, or employ layer-by-layer training using bio-inspired approaches like Hebbian learning, using local learning rules incompatible with supervised learning. Both approaches are problematic for edge AI hardware that relies on sparse computational resources and would strongly benefit from alternating between unsupervised and supervised learning phases - thus leveraging widely available unlabeled data from the environment as well as labeled training datasets. To solve this challenge, in this work, we introduce a 'self-defined target' that uses Winner-Take-All (WTA) selectivity at the network's final layer, complemented by regularization through biologically inspired homeostasis mechanism. This approach, framework-agnostic and compatible with both global (Backpropagation) and l
&lt;/p&gt;</description></item><item><title>Linguacodus&#26159;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#21160;&#24577;&#27969;&#27700;&#32447;&#21644;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20195;&#30721;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.11585</link><description>&lt;p&gt;
Linguacodus&#65306;&#19968;&#31181;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#20013;&#36827;&#34892;&#21464;&#38761;&#24615;&#20195;&#30721;&#29983;&#25104;&#30340;&#21327;&#21516;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11585
&lt;/p&gt;
&lt;p&gt;
Linguacodus&#26159;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#21160;&#24577;&#27969;&#27700;&#32447;&#21644;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20195;&#30721;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26080;&#32541;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#20195;&#30721;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Linguacodus&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#37096;&#32626;&#19968;&#20010;&#21160;&#24577;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#39640;&#32423;&#25968;&#25454;&#22609;&#24418;&#25351;&#20196;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36845;&#20195;&#22320;&#36716;&#25442;&#20026;&#20195;&#30721;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;Linguacodus&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33021;&#22815;&#35780;&#20272;&#21508;&#31181;&#38382;&#39064;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20026;&#29305;&#23450;&#20219;&#21153;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#31934;&#32454;&#35843;&#25972;&#36807;&#31243;&#65292;&#24182;&#38416;&#26126;&#20102;&#22914;&#20309;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#21270;&#20026;&#21151;&#33021;&#24615;&#20195;&#30721;&#12290;Linguacodus&#20195;&#34920;&#20102;&#33258;&#21160;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#37325;&#22823;&#39134;&#36291;&#65292;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#20219;&#21153;&#25551;&#36848;&#21644;&#21487;&#25191;&#34892;&#20195;&#30721;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#23545;&#25512;&#36827;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11585v1 Announce Type: cross  Abstract: In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions. The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code. Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. It holds great promise for advancing machine learning applications across div
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#22312;&#26412;&#25991;&#20013;&#24314;&#31435;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#31639;&#26415;&#30005;&#36335;&#20043;&#38388;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;&#24212;&#20851;&#31995;&#65292;&#32467;&#26524;&#34920;&#26126;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#31561;&#20215;&#20110;&#23454;&#25968;&#19978;&#30340;&#31639;&#26415;&#30005;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.17805</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#31639;&#26415;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks and Arithmetic Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17805
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#22312;&#26412;&#25991;&#20013;&#24314;&#31435;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#31639;&#26415;&#30005;&#36335;&#20043;&#38388;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;&#24212;&#20851;&#31995;&#65292;&#32467;&#26524;&#34920;&#26126;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#31561;&#20215;&#20110;&#23454;&#25968;&#19978;&#30340;&#31639;&#26415;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#34920;&#24449;&#20102;&#36981;&#24490;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#19981;&#38480;&#20110;&#32858;&#21512;-&#32452;&#21512;GNN&#25110;&#20854;&#20182;&#29305;&#23450;&#31867;&#22411;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20351;&#29992;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#23454;&#25968;&#19978;&#30340;&#31639;&#26415;&#30005;&#36335;&#20043;&#38388;&#30340;&#20934;&#30830;&#23545;&#24212;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;&#32467;&#26524;&#20013;&#65292;&#32593;&#32476;&#30340;&#28608;&#27963;&#20989;&#25968;&#25104;&#20026;&#30005;&#36335;&#20013;&#30340;&#38376;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;&#24120;&#25968;&#28145;&#24230;&#30005;&#36335;&#21644;&#32593;&#32476;&#23478;&#26063;&#22343;&#25104;&#31435;&#65292;&#26080;&#35770;&#26159;&#22312;&#19968;&#33268;&#36824;&#26159;&#38750;&#19968;&#33268;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#25152;&#26377;&#24120;&#35265;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17805v1 Announce Type: cross  Abstract: We characterize the computational power of neural networks that follow the graph neural network (GNN) architecture, not restricted to aggregate-combine GNNs or other particular types. We establish an exact correspondence between the expressivity of GNNs using diverse activation functions and arithmetic circuits over real numbers. In our results the activation function of the network becomes a gate type in the circuit. Our result holds for families of constant depth circuits and networks, both uniformly and non-uniformly, for all common activation functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#26377;&#38480;&#20803;&#20912;&#30422;&#27169;&#25311;&#30340;&#24555;&#36895;&#39640;&#20445;&#30495;&#24230;&#30340;&#20223;&#30495;&#22120;&#65292;&#24182;&#22312;Pine Island Glacier&#30340;&#30636;&#24577;&#27169;&#25311;&#20013;&#23637;&#31034;&#20102;&#19982;&#20256;&#32479;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#30456;&#27604;&#26356;&#20934;&#30830;&#30340;&#37325;&#29616;&#20912;&#30422;&#21402;&#24230;&#21644;&#36895;&#24230;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;GNN&#25104;&#21151;&#25429;&#25417;&#21040;&#20102;&#26356;&#39640;&#24213;&#37096;&#29076;&#21270;&#36895;&#29575;&#24341;&#36215;&#30340;&#20912;&#36136;&#37327;&#25439;&#22833;&#21644;&#21152;&#36895;&#36807;&#31243;&#12290;&#22312;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#19978;&#23454;&#29616;&#30340;GNN&#20223;&#30495;&#22120;&#26174;&#31034;&#20986;&#39640;&#36798;50&#20493;&#30340;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2402.05291</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#26377;&#38480;&#20803;&#20912;&#30422;&#27169;&#25311;&#30340;&#24555;&#36895;&#39640;&#20445;&#30495;&#24230;&#30340;&#20223;&#30495;&#22120;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks as Fast and High-fidelity Emulators for Finite-Element Ice Sheet Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#26377;&#38480;&#20803;&#20912;&#30422;&#27169;&#25311;&#30340;&#24555;&#36895;&#39640;&#20445;&#30495;&#24230;&#30340;&#20223;&#30495;&#22120;&#65292;&#24182;&#22312;Pine Island Glacier&#30340;&#30636;&#24577;&#27169;&#25311;&#20013;&#23637;&#31034;&#20102;&#19982;&#20256;&#32479;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#30456;&#27604;&#26356;&#20934;&#30830;&#30340;&#37325;&#29616;&#20912;&#30422;&#21402;&#24230;&#21644;&#36895;&#24230;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;GNN&#25104;&#21151;&#25429;&#25417;&#21040;&#20102;&#26356;&#39640;&#24213;&#37096;&#29076;&#21270;&#36895;&#29575;&#24341;&#36215;&#30340;&#20912;&#36136;&#37327;&#25439;&#22833;&#21644;&#21152;&#36895;&#36807;&#31243;&#12290;&#22312;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#19978;&#23454;&#29616;&#30340;GNN&#20223;&#30495;&#22120;&#26174;&#31034;&#20986;&#39640;&#36798;50&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20912;&#30422;&#21644;&#28023;&#24179;&#38754;&#31995;&#32479;&#27169;&#22411;&#65288;ISSM&#65289;&#30340;&#26377;&#38480;&#20803;&#26041;&#27861;&#21487;&#20197;&#24555;&#36895;&#20934;&#30830;&#22320;&#35299;&#20915;&#30001;Stokes&#26041;&#31243;&#25551;&#36848;&#30340;&#20912;&#21160;&#21147;&#23398;&#38382;&#39064;&#65292;&#20294;&#36825;&#31181;&#25968;&#20540;&#24314;&#27169;&#38656;&#35201;&#22312;&#20013;&#22830;&#22788;&#29702;&#21333;&#20803;&#65288;CPU&#65289;&#19978;&#36827;&#34892;&#23494;&#38598;&#30340;&#35745;&#31639;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#24555;&#36895;&#20195;&#29702;&#27169;&#22411;&#26469;&#20445;&#25345;ISSM&#30340;&#26377;&#38480;&#20803;&#32467;&#26500;&#12290;&#21033;&#29992;Pine Island Glacier&#65288;PIG&#65289;&#30340;20&#24180;&#30636;&#24577;&#27169;&#25311;&#65292;&#25105;&#20204;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#19977;&#20010;GNN&#65306;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#65292;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#21644;&#31561;&#21464;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;EGCN&#65289;&#12290;&#36825;&#20123;GNN&#19982;&#32463;&#20856;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#30456;&#27604;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#37325;&#29616;&#20912;&#21402;&#24230;&#21644;&#36895;&#24230;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;PIG&#20013;&#65292;GNN&#25104;&#21151;&#25429;&#25417;&#21040;&#20102;&#30001;&#26356;&#39640;&#24213;&#37096;&#29076;&#21270;&#36895;&#29575;&#24341;&#36215;&#30340;&#20912;&#36136;&#37327;&#25439;&#22833;&#21644;&#21152;&#36895;&#12290;&#24403;&#25105;&#20204;&#30340;GNN&#20223;&#30495;&#22120;&#22312;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#65288;GPU&#65289;&#19978;&#23454;&#29616;&#26102;&#65292;&#23427;&#20204;&#26174;&#31034;&#20986;&#39640;&#36798;50&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the finite element approach of the Ice-sheet and Sea-level System Model (ISSM) solves ice dynamics problems governed by Stokes equations quickly and accurately, such numerical modeling requires intensive computation on central processing units (CPU). In this study, we develop graph neural networks (GNN) as fast surrogate models to preserve the finite element structure of ISSM. Using the 20-year transient simulations in the Pine Island Glacier (PIG), we train and test three GNNs: graph convolutional network (GCN), graph attention network (GAT), and equivariant graph convolutional network (EGCN). These GNNs reproduce ice thickness and velocity with better accuracy than the classic convolutional neural network (CNN) and multi-layer perception (MLP). In particular, GNNs successfully capture the ice mass loss and acceleration induced by higher basal melting rates in the PIG. When our GNN emulators are implemented on graphic processing units (GPUs), they show up to 50 times faster c
&lt;/p&gt;</description></item><item><title>&#27169;&#25311;&#36807;&#24230;&#21442;&#25968;&#21270;&#65288;SOP&#65289;&#26159;&#19968;&#31181;&#23558;&#32039;&#20945;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#19982;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#39640;&#32423;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#30340;&#26032;&#33539;&#24335;&#12290;&#36890;&#36807;&#20351;&#29992;&#27169;&#25311;&#35757;&#32451;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#20027;&#35201;&#26550;&#26500;&#26080;&#20851;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;"majority kernels"&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#26550;&#26500;&#21644;&#20219;&#21153;&#20013;&#23454;&#29616;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.05033</link><description>&lt;p&gt;
&#27169;&#25311;&#36807;&#24230;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simulated Overparameterization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05033
&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#36807;&#24230;&#21442;&#25968;&#21270;&#65288;SOP&#65289;&#26159;&#19968;&#31181;&#23558;&#32039;&#20945;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#19982;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#39640;&#32423;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#30340;&#26032;&#33539;&#24335;&#12290;&#36890;&#36807;&#20351;&#29992;&#27169;&#25311;&#35757;&#32451;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#20027;&#35201;&#26550;&#26500;&#26080;&#20851;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;"majority kernels"&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#26550;&#26500;&#21644;&#20219;&#21153;&#20013;&#23454;&#29616;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#31216;&#20026;&#27169;&#25311;&#36807;&#24230;&#21442;&#25968;&#21270;&#65288;SOP&#65289;&#12290;SOP&#23558;&#32039;&#20945;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#19982;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#39640;&#32423;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;SOP&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#26029;&#26041;&#27861;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#26174;&#33879;&#26356;&#22810;&#21442;&#25968;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#21482;&#20351;&#29992;&#20854;&#20013;&#36739;&#23567;&#12289;&#39640;&#25928;&#30340;&#23376;&#38598;&#36827;&#34892;&#23454;&#38469;&#35745;&#31639;&#12290;&#22312;&#27492;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#20027;&#35201;&#26550;&#26500;&#65288;&#21253;&#25324;Transformer&#27169;&#22411;&#65289;&#26080;&#20851;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;"majority kernels"&#12290; majority kernels&#20351;&#24471;&#27169;&#25311;&#35757;&#32451;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#26550;&#26500;&#21644;&#20219;&#21153;&#20013;&#21462;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#26102;&#23545;&#20110;&#35745;&#31639;&#25104;&#26412;&#65288;&#22681;&#19978;&#25346;&#38047;&#26102;&#38388;&#65289;&#30340;&#22686;&#21152;&#38750;&#24120;&#23567;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a novel paradigm called Simulated Overparametrization (SOP). SOP merges the computational efficiency of compact models with the advanced learning proficiencies of overparameterized models. SOP proposes a unique approach to model training and inference, where a model with a significantly larger number of parameters is trained in such a way that a smaller, efficient subset of these parameters is used for the actual computation during inference. Building upon this framework, we present a novel, architecture agnostic algorithm called "majority kernels", which seamlessly integrates with predominant architectures, including Transformer models. Majority kernels enables the simulated training of overparameterized models, resulting in performance gains across architectures and tasks. Furthermore, our approach adds minimal overhead to the cost incurred (wall clock time) at training time. The proposed approach shows strong performance on a wide variety of datasets and m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;dSGP4&#65292;&#19968;&#31181;&#20351;&#29992;PyTorch&#23454;&#29616;&#30340;&#21487;&#24494;&#29256;&#26412;&#30340;SGP4&#12290;&#36890;&#36807;&#21487;&#24494;&#21270;&#65292;dSGP4&#23454;&#29616;&#20102;&#36712;&#36947;&#20256;&#25773;&#30340;&#39640;&#31934;&#24230;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#19982;&#22826;&#31354;&#30456;&#20851;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#21355;&#26143;&#36712;&#36947;&#30830;&#23450;&#12289;&#29366;&#24577;&#36716;&#25442;&#12289;&#21327;&#26041;&#24046;&#20256;&#25773;&#31561;&#12290;</title><link>https://arxiv.org/abs/2402.04830</link><description>&lt;p&gt;
&#32553;&#23567;SGP4&#21644;&#39640;&#31934;&#24230;&#20256;&#25773;&#20043;&#38388;&#30340;&#24046;&#36317;&#65306;&#36890;&#36807;&#21487;&#24494;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Closing the Gap Between SGP4 and High-Precision Propagation via Differentiable Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;dSGP4&#65292;&#19968;&#31181;&#20351;&#29992;PyTorch&#23454;&#29616;&#30340;&#21487;&#24494;&#29256;&#26412;&#30340;SGP4&#12290;&#36890;&#36807;&#21487;&#24494;&#21270;&#65292;dSGP4&#23454;&#29616;&#20102;&#36712;&#36947;&#20256;&#25773;&#30340;&#39640;&#31934;&#24230;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#19982;&#22826;&#31354;&#30456;&#20851;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#21355;&#26143;&#36712;&#36947;&#30830;&#23450;&#12289;&#29366;&#24577;&#36716;&#25442;&#12289;&#21327;&#26041;&#24046;&#20256;&#25773;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21270;&#30340;&#31532;&#22235;&#32423;&#25668;&#21160;(SGP4)&#36712;&#36947;&#20256;&#25773;&#26041;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#24555;&#36895;&#21487;&#38752;&#22320;&#39044;&#27979;&#22320;&#29699;&#36712;&#36947;&#29289;&#20307;&#30340;&#20301;&#32622;&#21644;&#36895;&#24230;&#12290;&#23613;&#31649;&#19981;&#26029;&#25913;&#36827;&#65292;SGP&#27169;&#22411;&#20173;&#28982;&#32570;&#20047;&#25968;&#20540;&#20256;&#25773;&#22120;&#30340;&#31934;&#24230;&#65292;&#21518;&#32773;&#30340;&#35823;&#24046;&#26174;&#33879;&#36739;&#23567;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;dSGP4&#65292;&#19968;&#31181;&#20351;&#29992;PyTorch&#23454;&#29616;&#30340;&#26032;&#22411;&#21487;&#24494;&#29256;&#26412;&#30340;SGP4&#12290;&#36890;&#36807;&#20351;SGP4&#21487;&#24494;&#21270;&#65292;dSGP4&#20415;&#20110;&#36827;&#34892;&#21508;&#31181;&#19982;&#22826;&#31354;&#30456;&#20851;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#33322;&#22825;&#22120;&#36712;&#36947;&#30830;&#23450;&#12289;&#29366;&#24577;&#36716;&#25442;&#12289;&#21327;&#26041;&#24046;&#36716;&#25442;&#12289;&#29366;&#24577;&#36716;&#31227;&#30697;&#38453;&#35745;&#31639;&#21644;&#21327;&#26041;&#24046;&#20256;&#25773;&#12290;&#27492;&#22806;&#65292;dSGP4&#30340;PyTorch&#23454;&#29616;&#20801;&#35768;&#22312;&#25209;&#37327;&#30340;TLE&#65288;&#20004;&#34892;&#21442;&#25968;&#65289;&#38598;&#19978;&#36827;&#34892;&#23604;&#23596;&#30340;&#24182;&#34892;&#36712;&#36947;&#20256;&#25773;&#65292;&#21033;&#29992;CPU&#12289;GPU&#21644;&#20998;&#24067;&#24335;&#39044;&#27979;&#21355;&#26143;&#20301;&#32622;&#30340;&#39640;&#32423;&#30828;&#20214;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;dSGP4&#30340;&#21487;&#24494;&#24615;&#20351;&#20854;&#33021;&#19982;&#27169;&#24335;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Simplified General Perturbations 4 (SGP4) orbital propagation method is widely used for predicting the positions and velocities of Earth-orbiting objects rapidly and reliably. Despite continuous refinement, SGP models still lack the precision of numerical propagators, which offer significantly smaller errors. This study presents dSGP4, a novel differentiable version of SGP4 implemented using PyTorch. By making SGP4 differentiable, dSGP4 facilitates various space-related applications, including spacecraft orbit determination, state conversion, covariance transformation, state transition matrix computation, and covariance propagation. Additionally, dSGP4's PyTorch implementation allows for embarrassingly parallel orbital propagation across batches of Two-Line Element Sets (TLEs), leveraging the computational power of CPUs, GPUs, and advanced hardware for distributed prediction of satellite positions at future times. Furthermore, dSGP4's differentiability enables integration with mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21333;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#39640;&#32500;&#26497;&#38480;&#19979;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#39044;&#27979;&#65292;&#24182;&#25512;&#24191;&#20102;&#23545;&#22810;&#31181;&#25968;&#25454;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#23613;&#31649;GCN&#22312;&#25910;&#25947;&#36895;&#24230;&#19978;&#26159;&#19968;&#33268;&#30340;&#65292;&#20294;&#22312;&#20219;&#20309;&#24773;&#20917;&#19979;&#37117;&#19981;&#33021;&#36798;&#21040;&#36125;&#21494;&#26031;&#26368;&#20248;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.03818</link><description>&lt;p&gt;
&#21333;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#28176;&#36827;&#27867;&#21270;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Asymptotic generalization error of a single-layer graph convolutional network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21333;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#39640;&#32500;&#26497;&#38480;&#19979;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#39044;&#27979;&#65292;&#24182;&#25512;&#24191;&#20102;&#23545;&#22810;&#31181;&#25968;&#25454;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#23613;&#31649;GCN&#22312;&#25910;&#25947;&#36895;&#24230;&#19978;&#26159;&#19968;&#33268;&#30340;&#65292;&#20294;&#22312;&#20219;&#20309;&#24773;&#20917;&#19979;&#37117;&#19981;&#33021;&#36798;&#21040;&#36125;&#21494;&#26031;&#26368;&#20248;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#23637;&#29616;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#30456;&#23545;&#20110;&#24191;&#27867;&#30740;&#31350;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#20851;&#20110;&#20854;&#27867;&#21270;&#29305;&#24615;&#19982;&#26679;&#26412;&#25968;&#37327;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39044;&#27979;&#20102;&#22312;&#39640;&#32500;&#26497;&#38480;&#19979;&#65292;&#22522;&#20110;&#23646;&#24615;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;SBM&#65289;&#29983;&#25104;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#21333;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#24615;&#33021;&#12290;&#20043;&#21069;&#65292;&#20165;&#22312;Shi&#31561;&#20154;&#30340;&#25991;&#31456;&#20013;&#32771;&#34385;&#20102;&#19978;&#19979;&#25991;-SBM&#65288;CSBM&#65289;&#19978;&#30340;&#23725;&#22238;&#24402;&#20998;&#26512;&#65307;&#25105;&#20204;&#23558;&#20998;&#26512;&#25512;&#24191;&#21040;CSBM&#30340;&#20219;&#24847;&#20984;&#25439;&#22833;&#21644;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#24182;&#28155;&#21152;&#20102;&#23545;&#21478;&#19968;&#20010;&#25968;&#25454;&#27169;&#22411;&#8212;&#8212;&#31070;&#32463;&#20248;&#20808;SBM&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#39640;&#20449;&#22122;&#27604;&#26497;&#38480;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;GCN&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#23613;&#31649;&#19968;&#33268;&#65292;&#20294;&#23545;&#20110;&#20219;&#20309;&#32771;&#34385;&#30340;&#24773;&#20917;&#37117;&#19981;&#33021;&#36798;&#21040;&#36125;&#21494;&#26031;&#26368;&#20248;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While graph convolutional networks show great practical promises, the theoretical understanding of their generalization properties as a function of the number of samples is still in its infancy compared to the more broadly studied case of supervised fully connected neural networks. In this article, we predict the performances of a single-layer graph convolutional network (GCN) trained on data produced by attributed stochastic block models (SBMs) in the high-dimensional limit. Previously, only ridge regression on contextual-SBM (CSBM) has been considered in Shi et al. 2022; we generalize the analysis to arbitrary convex loss and regularization for the CSBM and add the analysis for another data model, the neural-prior SBM. We also study the high signal-to-noise ratio limit, detail the convergence rates of the GCN and show that, while consistent, it does not reach the Bayes-optimal rate for any of the considered cases.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#30830;&#23450;&#24615;&#20316;&#20026;&#32622;&#20449;&#24230;&#20272;&#35745;&#21644;&#23884;&#20837;&#31354;&#38388;&#30340;&#27491;&#21017;&#39033;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2401.13721</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in Regression. (arXiv:2401.13721v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13721
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#30830;&#23450;&#24615;&#20316;&#20026;&#32622;&#20449;&#24230;&#20272;&#35745;&#21644;&#23884;&#20837;&#31354;&#38388;&#30340;&#27491;&#21017;&#39033;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#65288;UDAR&#65289;&#26088;&#22312;&#23558;&#26469;&#33258;&#26377;&#26631;&#31614;&#28304;&#39046;&#22495;&#30340;&#27169;&#22411;&#35843;&#25972;&#21040;&#26080;&#26631;&#31614;&#30446;&#26631;&#39046;&#22495;&#65292;&#20197;&#23436;&#25104;&#22238;&#24402;&#20219;&#21153;&#12290;&#26368;&#36817;&#22312;UDAR&#39046;&#22495;&#21462;&#24471;&#30340;&#25104;&#21151;&#20027;&#35201;&#38598;&#20013;&#22312;&#23376;&#31354;&#38388;&#23545;&#40784;&#19978;&#65292;&#28041;&#21450;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#25152;&#36873;&#25321;&#23376;&#31354;&#38388;&#30340;&#23545;&#40784;&#12290;&#36825;&#19982;&#29992;&#20110;&#20998;&#31867;&#30340;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#26088;&#22312;&#23545;&#40784;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#25928;&#26524;&#36739;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20998;&#31867;&#20219;&#21153;&#26088;&#22312;&#22312;&#25972;&#20010;&#23884;&#20837;&#31354;&#38388;&#30340;&#32500;&#24230;&#19978;&#35782;&#21035;&#29420;&#31435;&#30340;&#31751;&#65292;&#32780;&#22238;&#24402;&#20219;&#21153;&#23545;&#25968;&#25454;&#34920;&#31034;&#30340;&#32467;&#26500;&#24615;&#35201;&#27714;&#36739;&#20302;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#25351;&#23548;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26377;&#25928;UDAR&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#21452;&#37325;&#20316;&#29992;&#65306;&#25552;&#20379;&#20102;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#32622;&#20449;&#24230;&#34913;&#37327;&#65292;&#24182;&#20316;&#20026;&#23884;&#20837;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#35777;&#25454;&#27169;&#22411;&#26469;&#25552;&#20379;&#23545;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#23884;&#20837;&#31354;&#38388;&#30340;&#27491;&#21017;&#39033;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation for Regression (UDAR) aims to adapt a model from a labeled source domain to an unlabeled target domain for regression tasks. Recent successful works in UDAR mostly focus on subspace alignment, involving the alignment of a selected subspace within the entire feature space. This contrasts with the feature alignment methods used for classification, which aim at aligning the entire feature space and have proven effective but are less so in regression settings. Specifically, while classification aims to identify separate clusters across the entire embedding dimension, regression induces less structure in the data representation, necessitating additional guidance for efficient alignment. In this paper, we propose an effective method for UDAR by incorporating guidance from uncertainty. Our approach serves a dual purpose: providing a measure of confidence in predictions and acting as a regularization of the embedding space. Specifically, we leverage the Deep Evid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#20851;&#38190;&#24615;&#30340;&#39640;&#25928;SNN&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#21152;&#24378;&#29305;&#24449;&#25552;&#21462;&#21644;&#21152;&#36895;&#20462;&#21098;&#36807;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.16141</link><description>&lt;p&gt;
SNNs&#20013;&#22522;&#20110;&#20851;&#38190;&#24615;&#30340;&#39640;&#25928;&#20462;&#21098;&#26041;&#27861;&#65292;&#21463;&#21040;&#20851;&#38190;&#24615;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;
&lt;/p&gt;
&lt;p&gt;
Criticality-Guided Efficient Pruning in Spiking Neural Networks Inspired by Critical Brain Hypothesis. (arXiv:2311.16141v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#20851;&#38190;&#24615;&#30340;&#39640;&#25928;SNN&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#21152;&#24378;&#29305;&#24449;&#25552;&#21462;&#21644;&#21152;&#36895;&#20462;&#21098;&#36807;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#33410;&#33021;&#21644;&#26080;&#20056;&#27861;&#29305;&#24615;&#65292;SNNs&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#28145;&#24230;SNNs&#35268;&#27169;&#30340;&#19981;&#26029;&#22686;&#38271;&#32473;&#27169;&#22411;&#37096;&#32626;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#32593;&#32476;&#20462;&#21098;&#36890;&#36807;&#21387;&#32553;&#32593;&#32476;&#35268;&#27169;&#26469;&#20943;&#23569;&#27169;&#22411;&#37096;&#32626;&#30340;&#30828;&#20214;&#36164;&#28304;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;SNN&#20462;&#21098;&#26041;&#27861;&#30001;&#20110;&#20462;&#21098;&#36845;&#20195;&#22686;&#21152;&#20102;SNNs&#30340;&#35757;&#32451;&#38590;&#24230;&#65292;&#23548;&#33268;&#20462;&#21098;&#25104;&#26412;&#39640;&#26114;&#19988;&#24615;&#33021;&#25439;&#22833;&#20005;&#37325;&#12290;&#26412;&#25991;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#20851;&#38190;&#24615;&#30340;&#29992;&#20110;SNN&#20462;&#21098;&#30340;&#20877;&#29983;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#25552;&#21462;&#24182;&#21152;&#36895;&#20462;&#21098;&#36807;&#31243;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;SNN&#20013;&#29992;&#20110;&#20851;&#38190;&#24615;&#30340;&#20302;&#25104;&#26412;&#24230;&#37327;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#22312;&#20462;&#21098;&#21518;&#23545;&#25152;&#20462;&#21098;&#32467;&#26500;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#24182;&#20877;&#29983;&#37027;&#20123;&#20855;&#26377;&#36739;&#39640;&#20851;&#38190;&#24615;&#30340;&#32467;&#26500;&#65292;&#20197;&#33719;&#21462;&#20851;&#38190;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have gained considerable attention due to the energy-efficient and multiplication-free characteristics. The continuous growth in scale of deep SNNs poses challenges for model deployment. Network pruning reduces hardware resource requirements of model deployment by compressing the network scale. However, existing SNN pruning methods cause high pruning costs and performance loss because the pruning iterations amplify the training difficulty of SNNs. In this paper, inspired by the critical brain hypothesis in neuroscience, we propose a regeneration mechanism based on the neuron criticality for SNN pruning to enhance feature extraction and accelerate the pruning process. Firstly, we propose a low-cost metric for the criticality in SNNs. Then, we re-rank the pruned structures after pruning and regenerate those with higher criticality to obtain the critical network. Our method achieves higher performance than the current state-of-the-art (SOTA) method with up t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HIS-Unet&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21152;&#26435;&#27880;&#24847;&#21147;&#27169;&#22359;&#23454;&#29616;&#28023;&#20912;&#27987;&#24230;&#21644;&#28418;&#31227;&#30340;&#39044;&#27979;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;HIS-Unet&#22312;&#28023;&#20912;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2311.00167</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#39044;&#27979;&#21271;&#20912;&#27915;&#28023;&#20912;&#27987;&#24230;&#21644;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Multi-task Deep Convolutional Network to Predict Sea Ice Concentration and Drift in the Arctic Ocean. (arXiv:2311.00167v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00167
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HIS-Unet&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21152;&#26435;&#27880;&#24847;&#21147;&#27169;&#22359;&#23454;&#29616;&#28023;&#20912;&#27987;&#24230;&#21644;&#28418;&#31227;&#30340;&#39044;&#27979;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;HIS-Unet&#22312;&#28023;&#20912;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21271;&#20912;&#27915;&#22320;&#21306;&#65292;&#39044;&#27979;&#28023;&#20912;&#27987;&#24230;(SIC)&#21644;&#28023;&#20912;&#28418;&#31227;(SID)&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#22240;&#20026;&#26368;&#36817;&#30340;&#27668;&#20505;&#21464;&#26262;&#24050;&#32463;&#25913;&#21464;&#20102;&#36825;&#20010;&#29615;&#22659;&#12290;&#30001;&#20110;&#29289;&#29702;&#28023;&#20912;&#27169;&#22411;&#38656;&#35201;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#22797;&#26434;&#30340;&#21442;&#25968;&#21270;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#26377;&#25928;&#26367;&#20195;&#29289;&#29702;&#27169;&#22411;&#65292;&#24182;&#25552;&#39640;&#28023;&#20912;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#20840;&#21367;&#31215;&#32593;&#32476;&#26550;&#26500;&#65292;&#21517;&#20026;Hierarchical Information-Sharing U-Net (HIS-Unet)&#65292;&#29992;&#20110;&#39044;&#27979;&#27599;&#26085;&#30340;SIC&#21644;SID&#12290;&#25105;&#20204;&#36890;&#36807;&#21152;&#26435;&#27880;&#24847;&#21147;&#27169;&#22359;(WAMs)&#20801;&#35768;SIC&#21644;SID&#23618;&#20849;&#20139;&#20449;&#24687;&#65292;&#24182;&#20114;&#30456;&#36741;&#21161;&#39044;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#32479;&#35745;&#26041;&#27861;&#12289;&#28023;&#20912;&#29289;&#29702;&#27169;&#22411;&#21644;&#27809;&#26377;&#20449;&#24687;&#20849;&#20139;&#21333;&#20803;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#30340;HIS-Unet&#22312;SIC&#21644;SID&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#12290;&#22312;&#39044;&#27979;&#21271;&#20912;&#27915;&#28023;&#20912;&#27987;&#24230;&#21644;&#28418;&#31227;&#26041;&#38754;&#65292;HIS-Unet&#30340;&#25913;&#36827;&#37117;&#26159;&#26174;&#33879;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting sea ice concentration (SIC) and sea ice drift (SID) in the Arctic Ocean is of great significance as the Arctic environment has been changed by the recent warming climate. Given that physical sea ice models require high computational costs with complex parameterization, deep learning techniques can effectively replace the physical model and improve the performance of sea ice prediction. This study proposes a novel multi-task fully conventional network architecture named hierarchical information-sharing U-net (HIS-Unet) to predict daily SIC and SID. Instead of learning SIC and SID separately at each branch, we allow the SIC and SID layers to share their information and assist each other's prediction through the weighting attention modules (WAMs). Consequently, our HIS-Unet outperforms other statistical approaches, sea ice physical models, and neural networks without such information-sharing units. The improvement of HIS-Unet is obvious both for SIC and SID prediction when and
&lt;/p&gt;</description></item><item><title>CAMELL&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#24207;&#21015;&#22810;&#36755;&#20986;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#38656;&#19987;&#23478;&#26631;&#27880;&#24207;&#21015;&#30340;&#19968;&#23567;&#37096;&#20998;&#12289;&#33258;&#30417;&#30563;&#21644;&#26631;&#31614;&#39564;&#35777;&#26426;&#21046;&#26469;&#35299;&#20915;&#30417;&#30563;&#31070;&#32463;&#26041;&#27861;&#23545;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.08944</link><description>&lt;p&gt;
CAMELL&#65306;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#39640;&#25928;&#33258;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#19982;&#26631;&#31614;&#39564;&#35777;&#33719;&#21462;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CAMELL: Confidence-based Acquisition Model for Efficient Self-supervised Active Learning with Label Validation. (arXiv:2310.08944v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08944
&lt;/p&gt;
&lt;p&gt;
CAMELL&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#24207;&#21015;&#22810;&#36755;&#20986;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#38656;&#19987;&#23478;&#26631;&#27880;&#24207;&#21015;&#30340;&#19968;&#23567;&#37096;&#20998;&#12289;&#33258;&#30417;&#30563;&#21644;&#26631;&#31614;&#39564;&#35777;&#26426;&#21046;&#26469;&#35299;&#20915;&#30417;&#30563;&#31070;&#32463;&#26041;&#27861;&#23545;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24207;&#21015;&#20219;&#21153;&#20013;&#65292;&#21463;&#22823;&#35268;&#27169;&#19988;&#31934;&#30830;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#38480;&#21046;&#65292;&#30417;&#30563;&#31070;&#32463;&#26041;&#27861;&#21463;&#21040;&#38459;&#30861;&#12290;&#26631;&#27880;&#36136;&#37327;&#38543;&#30528;&#20174;&#19987;&#23478;&#26631;&#27880;&#21521;&#20247;&#21253;&#26631;&#27880;&#30340;&#36716;&#21464;&#32780;&#36880;&#28176;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAMELL&#65288;Confidence-based Acquisition Model for Efficient self-supervised active Learning with Label validation&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#24207;&#21015;&#22810;&#36755;&#20986;&#38382;&#39064;&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#27744;&#21270;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#12290;CAMELL&#20855;&#26377;&#19977;&#20010;&#26680;&#24515;&#29305;&#28857;&#65306;(1)&#20165;&#35201;&#27714;&#19987;&#23478;&#26631;&#27880;&#25152;&#36873;&#24207;&#21015;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;(2)&#20026;&#20854;&#20313;&#24207;&#21015;&#25552;&#20379;&#33258;&#30417;&#30563;&#65292;(3)&#37319;&#29992;&#26631;&#31614;&#39564;&#35777;&#26426;&#21046;&#65292;&#38450;&#27490;&#38169;&#35823;&#26631;&#31614;&#27745;&#26579;&#25968;&#25454;&#38598;&#24182;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#24207;&#21015;&#20219;&#21153;&#20013;&#23545;CAMELL&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#29305;&#21035;&#24378;&#35843;&#23545;&#35805;&#20449;&#24565;&#36319;&#36394;&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#38480;&#21046;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised neural approaches are hindered by their dependence on large, meticulously annotated datasets, a requirement that is particularly cumbersome for sequential tasks. The quality of annotations tends to deteriorate with the transition from expert-based to crowd-sourced labelling. To address these challenges, we present \textbf{CAMELL} (Confidence-based Acquisition Model for Efficient self-supervised active Learning with Label validation), a pool-based active learning framework tailored for sequential multi-output problems. CAMELL possesses three core features: (1) it requires expert annotators to label only a fraction of a chosen sequence, (2) it facilitates self-supervision for the remainder of the sequence, and (3) it employs a label validation mechanism to prevent erroneous labels from contaminating the dataset and harming model performance. We evaluate CAMELL on sequential tasks, with a special emphasis on dialogue belief tracking, a task plagued by the constraints of limited
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20174;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#35282;&#24230;&#20840;&#38754;&#35780;&#20272;&#20102;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22238;&#31572;&#20102;&#20309;&#26102;&#20462;&#25913;&#22270;&#25968;&#25454;&#12289;&#22270;&#25968;&#25454;&#30340;&#21738;&#19968;&#37096;&#20998;&#38656;&#35201;&#20462;&#25913;&#20197;&#21450;&#22914;&#20309;&#20445;&#25252;&#22270;&#27169;&#22411;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04987</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#21270;&#22270;&#23398;&#20064;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Data-centric Graph Learning: A Survey. (arXiv:2310.04987v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20174;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#35282;&#24230;&#20840;&#38754;&#35780;&#20272;&#20102;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22238;&#31572;&#20102;&#20309;&#26102;&#20462;&#25913;&#22270;&#25968;&#25454;&#12289;&#22270;&#25968;&#25454;&#30340;&#21738;&#19968;&#37096;&#20998;&#38656;&#35201;&#20462;&#25913;&#20197;&#21450;&#22914;&#20309;&#20445;&#25252;&#22270;&#27169;&#22411;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#21382;&#21490;&#35265;&#35777;&#20102;&#39640;&#36136;&#37327;&#25968;&#25454;&#23545;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#22823;&#24433;&#21709;&#65292;&#20363;&#22914;AlexNet&#21644;ResNet&#30340;ImageNet&#12290;&#26368;&#36817;&#65292;&#19982;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#35774;&#35745;&#26356;&#22797;&#26434;&#30340;&#31070;&#32463;&#32467;&#26500;&#19981;&#21516;&#65292;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#30340;&#20851;&#27880;&#37325;&#28857;&#36716;&#21521;&#20102;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20391;&#37325;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#25968;&#25454;&#20197;&#22686;&#24378;&#31070;&#32463;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#32780;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#65292;&#25805;&#20316;&#26222;&#36941;&#23384;&#22312;&#30340;&#25299;&#25169;&#25968;&#25454;&#30340;&#22270;&#23398;&#20064;&#20063;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20174;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#35282;&#24230;&#20840;&#38754;&#35780;&#20272;&#20102;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#26088;&#22312;&#22238;&#31572;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#20309;&#26102;&#20462;&#25913;&#22270;&#25968;&#25454;&#65292;&#65288;2&#65289;&#22270;&#25968;&#25454;&#30340;&#21738;&#19968;&#37096;&#20998;&#38656;&#35201;&#20462;&#25913;&#20197;&#37322;&#25918;&#21508;&#31181;&#22270;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22914;&#20309;&#20445;&#25252;&#22270;&#27169;&#22411;&#20813;&#21463;&#26377;&#38382;&#39064;&#30340;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#23398;&#20064;&#27969;&#31243;&#38454;&#27573;&#30340;&#21019;&#26032;&#20998;&#31867;&#27861;&#65292;&#24182;&#31361;&#20986;&#20102;&#20851;&#38190;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The history of artificial intelligence (AI) has witnessed the significant impact of high-quality data on various deep learning models, such as ImageNet for AlexNet and ResNet. Recently, instead of designing more complex neural architectures as model-centric approaches, the attention of AI community has shifted to data-centric ones, which focuses on better processing data to strengthen the ability of neural models. Graph learning, which operates on ubiquitous topological data, also plays an important role in the era of deep learning. In this survey, we comprehensively review graph learning approaches from the data-centric perspective, and aim to answer three crucial questions: (1) when to modify graph data, (2) what part of the graph data needs modification to unlock the potential of various graph models, and (3) how to safeguard graph models from problematic data influence. Accordingly, we propose a novel taxonomy based on the stages in the graph learning pipeline, and highlight the pr
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;CDML&#35774;&#35745;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#25351;&#23548;&#24320;&#21457;&#32773;&#35774;&#35745;&#28385;&#36275;&#29992;&#20363;&#35201;&#27714;&#30340;&#21327;&#20316;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.16584</link><description>&lt;p&gt;
&#29992;&#20110;&#24320;&#21457;&#21327;&#20316;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#35774;&#35745;&#24037;&#20855;&#31665;
&lt;/p&gt;
&lt;p&gt;
A Design Toolbox for the Development of Collaborative Distributed Machine Learning Systems. (arXiv:2309.16584v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16584
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;CDML&#35774;&#35745;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#25351;&#23548;&#24320;&#21457;&#32773;&#35774;&#35745;&#28385;&#36275;&#29992;&#20363;&#35201;&#27714;&#30340;&#21327;&#20316;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26426;&#23494;&#24615;&#30340;&#21516;&#26102;&#21033;&#29992;&#26469;&#33258;&#22810;&#26041;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#20805;&#20998;&#35757;&#32451;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#21508;&#31181;&#21327;&#20316;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;CDML&#65289;&#31995;&#32479;&#35774;&#35745;&#65292;&#20363;&#22914;&#36741;&#21161;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;&#20998;&#35010;&#23398;&#20064;&#12290;CDML&#31995;&#32479;&#35774;&#35745;&#23637;&#31034;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#39640;&#24230;&#30340;&#20195;&#29702;&#20154;&#33258;&#27835;&#24615;&#12289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26426;&#23494;&#24615;&#21644;&#23481;&#38169;&#24615;&#12290;&#38754;&#23545;&#19981;&#21516;&#29305;&#24449;&#30340;&#21508;&#31181;CDML&#31995;&#32479;&#35774;&#35745;&#65292;&#24320;&#21457;&#32773;&#24456;&#38590;&#26377;&#38024;&#23545;&#24615;&#22320;&#35774;&#35745;&#28385;&#36275;&#29992;&#20363;&#35201;&#27714;&#30340;CDML&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#19981;&#21512;&#36866;&#30340;CDML&#31995;&#32479;&#35774;&#35745;&#21487;&#33021;&#23548;&#33268;CDML&#31995;&#32479;&#26080;&#27861;&#23454;&#29616;&#20854;&#39044;&#26399;&#30446;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;CDML&#35774;&#35745;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#25351;&#23548;CDML&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;&#22522;&#20110;CDML&#35774;&#35745;&#24037;&#20855;&#31665;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#19981;&#21516;&#20851;&#38190;&#29305;&#24449;&#30340;CDML&#31995;&#32479;&#20856;&#22411;&#65292;&#21487;&#20197;&#25903;&#25345;&#35774;&#35745;&#28385;&#36275;&#29992;&#20363;&#35201;&#27714;&#30340;CDML&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
To leverage training data for the sufficient training of ML models from multiple parties in a confidentiality-preserving way, various collaborative distributed machine learning (CDML) system designs have been developed, for example, to perform assisted learning, federated learning, and split learning. CDML system designs show different traits, for example, high agent autonomy, machine learning (ML) model confidentiality, and fault tolerance. Facing a wide variety of CDML system designs with different traits, it is difficult for developers to design CDML systems with traits that match use case requirements in a targeted way. However, inappropriate CDML system designs may result in CDML systems failing their envisioned purposes. We developed a CDML design toolbox that can guide the development of CDML systems. Based on the CDML design toolbox, we present CDML system archetypes with distinct key traits that can support the design of CDML systems to meet use case requirements.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#21457;&#29616;&#22312;&#19968;&#23450;&#30340;&#31070;&#32463;&#26550;&#26500;&#31867;&#21035;&#20013;&#65292;&#35745;&#31639;&#21644;&#39564;&#35777;&#29702;&#24819;&#30340;&#31283;&#23450;&#21644;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#29978;&#33267;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.07072</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#21487;&#39564;&#35777;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#30340;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
The Boundaries of Verifiable Accuracy, Robustness, and Generalisation in Deep Learning. (arXiv:2309.07072v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#21457;&#29616;&#22312;&#19968;&#23450;&#30340;&#31070;&#32463;&#26550;&#26500;&#31867;&#21035;&#20013;&#65292;&#35745;&#31639;&#21644;&#39564;&#35777;&#29702;&#24819;&#30340;&#31283;&#23450;&#21644;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#29978;&#33267;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#29702;&#35770;&#38480;&#21046;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#32463;&#20856;&#30340;&#20998;&#24067;&#26080;&#20851;&#26694;&#26550;&#21644;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#30340;&#31639;&#27861;&#65292;&#21487;&#33021;&#36824;&#21463;&#21040;&#19968;&#20123;&#26435;&#37325;&#27491;&#21017;&#21270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#24456;&#22810;&#20219;&#21153;&#26469;&#35828;&#65292;&#22312;&#19978;&#36848;&#35774;&#32622;&#19979;&#35745;&#31639;&#21644;&#39564;&#35777;&#29702;&#24819;&#30340;&#31283;&#23450;&#21644;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#65292;&#29978;&#33267;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#21363;&#20351;&#22312;&#32473;&#23450;&#30340;&#31070;&#32463;&#26550;&#26500;&#31867;&#21035;&#20013;&#23384;&#22312;&#36825;&#26679;&#30340;&#29702;&#24819;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we assess the theoretical limitations of determining guaranteed stability and accuracy of neural networks in classification tasks. We consider classical distribution-agnostic framework and algorithms minimising empirical risks and potentially subjected to some weights regularisation. We show that there is a large family of tasks for which computing and verifying ideal stable and accurate neural networks in the above settings is extremely challenging, if at all possible, even when such ideal solutions exist within the given class of neural architectures.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#19977;&#32500;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#20808;&#39564;&#21644;&#26174;&#33879;&#24615;&#22270;&#37325;&#26032;&#25773;&#25918;&#65292;&#23454;&#29616;&#23545;&#20043;&#21069;&#24050;&#35265;&#31867;&#21035;&#30340;&#21512;&#29702;&#37325;&#24314;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08812</link><description>&lt;p&gt;
&#21464;&#20998;&#20998;&#24067;&#20808;&#39564;&#19982;&#26174;&#33879;&#24615;&#22270;&#37325;&#26032;&#25773;&#25918;&#30340;&#34701;&#21512;&#65292;&#29992;&#20110;&#36830;&#32493;&#19977;&#32500;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
A Fusion of Variational Distribution Priors and Saliency Map Replay for Continual 3D Reconstruction. (arXiv:2308.08812v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#19977;&#32500;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#20808;&#39564;&#21644;&#26174;&#33879;&#24615;&#22270;&#37325;&#26032;&#25773;&#25918;&#65292;&#23454;&#29616;&#23545;&#20043;&#21069;&#24050;&#35265;&#31867;&#21035;&#30340;&#21512;&#29702;&#37325;&#24314;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#22270;&#20687;&#19977;&#32500;&#37325;&#24314;&#26159;&#30740;&#31350;&#22914;&#20309;&#26681;&#25454;&#21333;&#35270;&#35282;&#22270;&#20687;&#39044;&#27979;&#19977;&#32500;&#29289;&#20307;&#24418;&#29366;&#30340;&#19968;&#20010;&#25361;&#25112;&#12290;&#36825;&#20010;&#20219;&#21153;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#33719;&#21462;&#26469;&#39044;&#27979;&#24418;&#29366;&#30340;&#21487;&#35265;&#21644;&#36974;&#25377;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#38754;&#20020;&#21019;&#24314;&#38024;&#23545;&#25152;&#26377;&#21487;&#33021;&#31867;&#21035;&#30340;&#20840;&#38754;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#19977;&#32500;&#37325;&#24314;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#20351;&#29992;&#21464;&#20998;&#20808;&#39564;&#30340;&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#26032;&#31867;&#21035;&#21518;&#20173;&#21487;&#20197;&#21512;&#29702;&#37325;&#24314;&#20197;&#21069;&#35265;&#36807;&#30340;&#31867;&#21035;&#12290;&#21464;&#20998;&#20808;&#39564;&#20195;&#34920;&#25277;&#35937;&#24418;&#29366;&#24182;&#36991;&#20813;&#36951;&#24536;&#65292;&#32780;&#26174;&#33879;&#24615;&#22270;&#20197;&#36739;&#23569;&#30340;&#20869;&#23384;&#20351;&#29992;&#20445;&#30041;&#23545;&#35937;&#23646;&#24615;&#12290;&#36825;&#23545;&#20110;&#23384;&#20648;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#36164;&#28304;&#38480;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#26174;&#33879;&#24615;&#22270;&#30340;&#32463;&#39564;&#37325;&#25918;&#65292;&#20197;&#25429;&#25417;&#20840;&#23616;&#21644;&#29420;&#29305;&#30340;&#23545;&#35937;&#29305;&#24449;&#12290;&#35814;&#32454;&#30340;&#23454;&#39564;&#26174;&#31034;&#19982;&#24050;&#24314;&#31435;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single-image 3D reconstruction is a research challenge focused on predicting 3D object shapes from single-view images. This task requires significant data acquisition to predict both visible and occluded portions of the shape. Furthermore, learning-based methods face the difficulty of creating a comprehensive training dataset for all possible classes. To this end, we propose a continual learning-based 3D reconstruction method where our goal is to design a model using Variational Priors that can still reconstruct the previously seen classes reasonably even after training on new classes. Variational Priors represent abstract shapes and combat forgetting, whereas saliency maps preserve object attributes with less memory usage. This is vital due to resource constraints in storing extensive training data. Additionally, we introduce saliency map-based experience replay to capture global and distinct object features. Thorough experiments show competitive results compared to established method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#65288;RDF&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#38750;&#35782;&#21035;&#24615;&#30340;DF&#35299;&#65292;&#23398;&#20064;&#21516;&#26102;&#26368;&#22823;&#21270;&#26399;&#26395;&#22238;&#25253;&#21644;&#25269;&#24481;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;DF&#23545;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24635;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2304.03365</link><description>&lt;p&gt;
&#22870;&#21169;&#36716;&#31227;&#30340;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Decision-Focused Learning for Reward Transfer. (arXiv:2304.03365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#65288;RDF&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#38750;&#35782;&#21035;&#24615;&#30340;DF&#35299;&#65292;&#23398;&#20064;&#21516;&#26102;&#26368;&#22823;&#21270;&#26399;&#26395;&#22238;&#25253;&#21644;&#25269;&#24481;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;DF&#23545;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24635;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20915;&#31574;&#37325;&#28857;&#65288;Decision-focused&#65292;DF&#65289;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#34987;&#20171;&#32461;&#20026;&#19968;&#31181;&#24378;&#26377;&#21147;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#19987;&#27880;&#20110;&#23398;&#20064;&#26368;&#26377;&#21033;&#20110;&#33719;&#24471;&#39640;&#25253;&#37228;&#30340;MDP&#21160;&#24577;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#19987;&#27880;&#20110;&#30452;&#25509;&#20248;&#21270;&#25253;&#37228;&#26469;&#25552;&#39640;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#65292;&#20294;&#20174;MLE&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#23427;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#19981;&#22815;&#20934;&#30830;&#65292;&#22240;&#27492;&#21487;&#33021;&#23545;&#22870;&#21169;&#20989;&#25968;&#30340;&#21464;&#21270;&#24456;&#33030;&#24369;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#65288;RDF&#65289;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;DF&#35299;&#30340;&#38750;&#35782;&#21035;&#24615;&#65292;&#23398;&#20064;&#21516;&#26102;&#26368;&#22823;&#21270;&#26399;&#26395;&#22238;&#25253;&#21644;&#25269;&#24481;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#29609;&#20855;&#31034;&#20363;&#21644;&#21307;&#30103;&#27169;&#25311;&#22120;&#19978;&#23637;&#31034;&#20102;RDF&#26174;&#30528;&#22686;&#21152;&#20102;DF&#23545;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24635;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-focused (DF) model-based reinforcement learning has recently been introduced as a powerful algorithm which can focus on learning the MDP dynamics which are most relevant for obtaining high rewards. While this approach increases the performance of agents by focusing the learning towards optimizing for the reward directly, it does so by learning less accurate dynamics (from a MLE standpoint), and may thus be brittle to changes in the reward function. In this work, we develop the robust decision-focused (RDF) algorithm which leverages the non-identifiability of DF solutions to learn models which maximize expected returns while simultaneously learning models which are robust to changes in the reward function. We demonstrate on a variety of toy example and healthcare simulators that RDF significantly increases the robustness of DF to changes in the reward function, without decreasing the overall return the agent obtains.
&lt;/p&gt;</description></item><item><title>BODEGA&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#30340;&#20869;&#23481;&#31649;&#29702;&#22330;&#26223;&#65292;&#22312;&#22235;&#20010;&#35823;&#20256;&#26816;&#27979;&#20219;&#21153;&#19978;&#27979;&#35797;&#21463;&#23475;&#27169;&#22411;&#21644;&#25915;&#20987;&#26041;&#27861;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#36827;&#34892;&#24494;&#23567;&#30340;&#25991;&#26412;&#20462;&#25913;&#65292;&#20063;&#21487;&#20197;&#27450;&#39575;&#26368;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2303.08032</link><description>&lt;p&gt;
BODEGA: &#38024;&#23545;&#21487;&#20449;&#24230;&#35780;&#20272;&#20013;&#23545;&#25239;&#24615;&#26679;&#26412;&#29983;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BODEGA: Benchmark for Adversarial Example Generation in Credibility Assessment. (arXiv:2303.08032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08032
&lt;/p&gt;
&lt;p&gt;
BODEGA&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#30340;&#20869;&#23481;&#31649;&#29702;&#22330;&#26223;&#65292;&#22312;&#22235;&#20010;&#35823;&#20256;&#26816;&#27979;&#20219;&#21153;&#19978;&#27979;&#35797;&#21463;&#23475;&#27169;&#22411;&#21644;&#25915;&#20987;&#26041;&#27861;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#36827;&#34892;&#24494;&#23567;&#30340;&#25991;&#26412;&#20462;&#25913;&#65292;&#20063;&#21487;&#20197;&#27450;&#39575;&#26368;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26816;&#27979;&#19981;&#21487;&#20449;&#20869;&#23481;&#65292;&#22914;&#20551;&#26032;&#38395;&#12289;&#31038;&#20132;&#23186;&#20307;&#26426;&#22120;&#20154;&#12289;&#23459;&#20256;&#31561;&#12290;&#36739;&#20026;&#20934;&#30830;&#30340;&#27169;&#22411;&#65288;&#21487;&#33021;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#26377;&#21161;&#20110;&#31649;&#29702;&#20844;&#20849;&#30005;&#23376;&#24179;&#21488;&#65292;&#24182;&#32463;&#24120;&#23548;&#33268;&#20869;&#23481;&#21019;&#24314;&#32773;&#38754;&#20020;&#25552;&#20132;&#25298;&#32477;&#25110;&#24050;&#21457;&#24067;&#25991;&#26412;&#30340;&#25764;&#19979;&#12290;&#20026;&#20102;&#36991;&#20813;&#36827;&#19968;&#27493;&#34987;&#26816;&#27979;&#65292;&#20869;&#23481;&#21019;&#24314;&#32773;&#23581;&#35797;&#20135;&#29983;&#19968;&#20010;&#31245;&#24494;&#20462;&#25913;&#36807;&#30340;&#25991;&#26412;&#29256;&#26412;&#65288;&#21363;&#25915;&#20987;&#23545;&#25239;&#24615;&#26679;&#26412;&#65289;&#65292;&#21033;&#29992;&#20998;&#31867;&#22120;&#30340;&#24369;&#28857;&#23548;&#33268;&#19981;&#21516;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BODEGA&#65306;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#22312;&#27169;&#25311;&#20869;&#23481;&#31649;&#29702;&#30340;&#30495;&#23454;&#29992;&#20363;&#20013;&#27979;&#35797;&#21463;&#23475;&#27169;&#22411;&#21644;&#25915;&#20987;&#26041;&#27861;&#22312;&#22235;&#20010;&#35823;&#20256;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#21463;&#27426;&#36814;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#23545;&#21487;&#29992;&#25915;&#20987;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#22312;&#25991;&#26412;&#20013;&#36827;&#34892;&#24494;&#23567;&#30340;&#20462;&#25913;&#20063;&#21487;&#20197;&#27450;&#39575;&#26368;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification methods have been widely investigated as a way to detect content of low credibility: fake news, social media bots, propaganda, etc. Quite accurate models (likely based on deep neural networks) help in moderating public electronic platforms and often cause content creators to face rejection of their submissions or removal of already published texts. Having the incentive to evade further detection, content creators try to come up with a slightly modified version of the text (known as an attack with an adversarial example) that exploit the weaknesses of classifiers and result in a different output. Here we introduce BODEGA: a benchmark for testing both victim models and attack methods on four misinformation detection tasks in an evaluation framework designed to simulate real use-cases of content moderation. We also systematically test the robustness of popular text classifiers against available attacking techniques and discover that, indeed, in some cases barely signif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#25351;&#25968;&#21028;&#25454;&#26469;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#20316;&#32773;&#36827;&#34892;&#20102;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25191;&#34892;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.09010</link><description>&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65306;&#25351;&#25968;&#26631;&#20934;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Risk-Sensitive Reinforcement Learning with Exponential Criteria. (arXiv:2212.09010v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#25351;&#25968;&#21028;&#25454;&#26469;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#20316;&#32773;&#36827;&#34892;&#20102;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25191;&#34892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39118;&#38505;&#20013;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#24456;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#23454;&#39564;&#25104;&#21151;&#65292;&#20294;&#26159;&#36825;&#31181;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#21644;&#31995;&#32479;&#21442;&#25968;&#25200;&#21160;&#30340;&#24433;&#21709;&#32780;&#19981;&#22815;&#31283;&#20581;&#12290;&#22240;&#27492;,&#23545;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#65292;&#26679;&#26412;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26080;&#27169;&#22411;&#39118;&#38505;&#25935;&#24863;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24191;&#27867;&#20351;&#29992;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#21464;&#20307;&#65292;&#20854;&#23454;&#29616;&#36807;&#31243;&#31867;&#20284;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#25351;&#25968;&#26631;&#20934;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#31574;&#30053;&#39118;&#38505;&#25935;&#24863;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#20102;&#33945;&#29305;&#21345;&#32599;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21644;&#22312;&#32447;(&#26102;&#38388;&#24046;&#20998;)&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#25968;&#26631;&#20934;&#30340;&#20351;&#29992;&#33021;&#22815;&#25512;&#24191;&#24120;&#29992;&#30340;&#29305;&#23450;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#22312;&#25670;&#21160;&#26438;&#21644;&#25670;&#25670;&#26438;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#23454;&#29616;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While risk-neutral reinforcement learning has shown experimental success in a number of applications, it is well-known to be non-robust with respect to noise and perturbations in the parameters of the system. For this reason, risk-sensitive reinforcement learning algorithms have been studied to introduce robustness and sample efficiency, and lead to better real-life performance. In this work, we introduce new model-free risk-sensitive reinforcement learning algorithms as variations of widely-used Policy Gradient algorithms with similar implementation properties. In particular, we study the effect of exponential criteria on the risk-sensitivity of the policy of a reinforcement learning agent, and develop variants of the Monte Carlo Policy Gradient algorithm and the online (temporal-difference) Actor-Critic algorithm. Analytical results showcase that the use of exponential criteria generalize commonly used ad-hoc regularization approaches. The implementation, performance, and robustness 
&lt;/p&gt;</description></item></channel></rss>