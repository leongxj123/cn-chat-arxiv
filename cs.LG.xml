<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#37492;&#21035;&#24615;&#33258;&#30417;&#30563;&#31639;&#27861;&#22312;&#34920;&#31034;&#20013;&#36817;&#20284;&#35825;&#23548;&#28508;&#21464;&#37327;&#32467;&#26500;&#30340;&#32479;&#19968;&#29702;&#35770;&#26694;&#26550;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01399</link><description>&lt;p&gt;
&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Model to explain Self-Supervised Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#37322;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#37492;&#21035;&#24615;&#33258;&#30417;&#30563;&#31639;&#27861;&#22312;&#34920;&#31034;&#20013;&#36817;&#20284;&#35825;&#23548;&#28508;&#21464;&#37327;&#32467;&#26500;&#30340;&#32479;&#19968;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#30340;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#20363;&#22914;&#23545;&#35821;&#20041;&#30456;&#20851;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#22914;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24378;&#25110;&#27169;&#24577;&#26469;&#23398;&#20064;&#34920;&#31034;&#12290;&#22312;&#20247;&#22810;SSL&#26041;&#27861;&#20013;&#65292;&#23545;&#27604;&#26041;&#27861;&#65288;&#20363;&#22914;SimCLR&#65292;CLIP&#21644;VicREG&#65289;&#22240;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#22312;&#19979;&#28216;&#24615;&#33021;&#19978;&#25509;&#36817;&#26377;&#30417;&#30563;&#23398;&#20064;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32972;&#21518;&#30340;&#26426;&#21046;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#34920;&#31034;&#25968;&#25454;&#65292;&#24182;&#23637;&#31034;&#20102;&#20960;&#31867;&#20855;&#26377;&#37492;&#21035;&#24615;&#30340;&#33258;&#30417;&#30563;&#31639;&#27861;&#65288;&#21253;&#25324;&#23545;&#27604;&#26041;&#27861;&#65289;&#36817;&#20284;&#35825;&#23548;&#20854;&#34920;&#31034;&#20013;&#30340;&#28508;&#21464;&#37327;&#32467;&#26500;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19982;&#20114;&#20449;&#24687;&#21644;&#25237;&#24433;&#22836;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#29983;&#25104;&#24335;&#22320;&#25311;&#21512;&#25105;&#20204;&#30340;&#27169;&#22411;&#65288;&#22914;SimVE&#65289;&#65292;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#65288;&#20363;&#22914;FashionMNIST&#65292;CIFAR10&#65292;CelebA&#65289;&#65292;&#24615;&#33021;&#20248;&#20110;&#20043;&#21069;&#30340;VAE&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) learns representations by leveraging an auxiliary unsupervised task, such as classifying semantically related samples, e.g. different data augmentations or modalities. Of the many approaches to SSL, contrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for learning representations that achieve downstream performance close to that of supervised learning. However, a theoretical understanding of the mechanism behind these methods eludes. We propose a generative latent variable model for the data and show that several families of discriminative self-supervised algorithms, including contrastive methods, approximately induce its latent structure over representations, providing a unifying theoretical framework. We also justify links to mutual information and the use of a projection head. Fitting our model generatively, as SimVE, improves performance over previous VAE methods on common benchmarks (e.g. FashionMNIST, CIFAR10, CelebA), narrows th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SinkhornDRL&#26041;&#27861;&#65292;&#20351;&#29992;Sinkhorn&#25955;&#24230;&#26469;&#20943;&#23567;&#24403;&#21069;&#21644;&#30446;&#26631;Bellman&#22238;&#25253;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#23454;&#35777;&#23454;&#39564;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2202.00769</link><description>&lt;p&gt;
&#20351;&#29992;Sinkhorn&#25955;&#24230;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributional Reinforcement Learning by Sinkhorn Divergence
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2202.00769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SinkhornDRL&#26041;&#27861;&#65292;&#20351;&#29992;Sinkhorn&#25955;&#24230;&#26469;&#20943;&#23567;&#24403;&#21069;&#21644;&#30446;&#26631;Bellman&#22238;&#25253;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#23454;&#35777;&#23454;&#39564;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#35777;&#25104;&#21151;&#39640;&#24230;&#20381;&#36182;&#20110;&#20998;&#24067;&#34920;&#31034;&#21644;&#20998;&#24067;&#25955;&#24230;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Sinkhorn&#20998;&#24067;&#24378;&#21270;&#23398;&#20064; (SinkhornDRL)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#22238;&#25253;&#20998;&#24067;&#20013;&#23398;&#20064;&#26080;&#38480;&#21046;&#30340;&#32479;&#35745;&#37327;&#65292;&#24182;&#21033;&#29992;Sinkhorn&#25955;&#24230;&#26469;&#20943;&#23567;&#24403;&#21069;&#21644;&#30446;&#26631;Bellman&#22238;&#25253;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20174;&#29702;&#35770;&#19978;&#26469;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SinkhornDRL&#30340;&#25910;&#32553;&#24615;&#36136;&#65292;&#19982;Sinkhorn&#25955;&#24230;&#22312;Wasserstein&#36317;&#31163;&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322; (MMD)&#20043;&#38388;&#30340;&#25554;&#20540;&#24615;&#36136;&#19968;&#33268;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;Sinkhorn&#25955;&#24230;&#19982;&#24102;&#26377;&#27491;&#21017;&#21270;Moment Matching&#34892;&#20026;&#30340;&#27491;&#21017;&#21270;MMD&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#65292;&#20174;&#32780;&#35299;&#37322;&#20102;SinkhornDRL&#30340;&#20248;&#36234;&#24615;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SinkhornDRL&#22312;Atari&#28216;&#25103;&#22871;&#20214;&#19978;&#22987;&#32456;&#34920;&#29616;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#25110;&#21487;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The empirical success of distributional reinforcement learning~(RL) highly depends on the distribution representation and the choice of distribution divergence. In this paper, we propose \textit{Sinkhorn distributional RL~(SinkhornDRL)} that learns unrestricted statistics from return distributions and leverages Sinkhorn divergence to minimize the difference between current and target Bellman return distributions. Theoretically, we prove the contraction properties of SinkhornDRL, consistent with the interpolation nature of Sinkhorn divergence between Wasserstein distance and Maximum Mean Discrepancy~(MMD). We also establish the equivalence between Sinkhorn divergence and a regularized MMD with a regularized Moment Matching behavior, contributing to explaining the superiority of SinkhornDRL. Empirically, we show that SinkhornDRL is consistently better or comparable to existing algorithms on the Atari games suite.
&lt;/p&gt;</description></item><item><title>CtRL-Sim&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Nocturne&#27169;&#25311;&#22120;&#20013;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.19918</link><description>&lt;p&gt;
CtRL-Sim&#65306;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#24212;&#24615;&#21487;&#25511;&#39550;&#39542;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19918
&lt;/p&gt;
&lt;p&gt;
CtRL-Sim&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Nocturne&#27169;&#25311;&#22120;&#20013;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CtRL-Sim&#65292;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#22686;&#24378;&#30340;Nocturne&#27169;&#25311;&#22120;&#20013;&#30340;&#22238;&#25253;&#26465;&#20214;&#21270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#39640;&#25928;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;Nocturne&#27169;&#25311;&#22120;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#65292;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19918v1 Announce Type: cross  Abstract: Evaluating autonomous vehicle stacks (AVs) in simulation typically involves replaying driving logs from real-world recorded traffic. However, agents replayed from offline data do not react to the actions of the AV, and their behaviour cannot be easily controlled to simulate counterfactual scenarios. Existing approaches have attempted to address these shortcomings by proposing methods that rely on heuristics or learned generative models of real-world data but these approaches either lack realism or necessitate costly iterative sampling procedures to control the generated behaviours. In this work, we take an alternative approach and propose CtRL-Sim, a method that leverages return-conditioned offline reinforcement learning within a physics-enhanced Nocturne simulator to efficiently generate reactive and controllable traffic agents. Specifically, we process real-world driving data through the Nocturne simulator to generate a diverse offli
&lt;/p&gt;</description></item><item><title>Nellie&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#12289;&#26080;&#20559;&#35265;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#22312;2D/3D&#27963;&#32454;&#32990;&#26174;&#24494;&#38236;&#19979;&#20998;&#21106;&#12289;&#36319;&#36394;&#21644;&#25552;&#21462;&#22810;&#26679;&#32454;&#32990;&#20869;&#32467;&#26500;&#29305;&#24449;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#20998;&#23618;&#20998;&#21106;&#21644;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#20998;&#26512;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13214</link><description>&lt;p&gt;
Nellie&#65306;&#33258;&#21160;&#30340;2D/3D&#27963;&#32454;&#32990;&#26174;&#24494;&#38236;&#19979;&#22120;&#23448;&#20998;&#21106;&#12289;&#36319;&#36394;&#21644;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Nellie: Automated organelle segmentation, tracking, and hierarchical feature extraction in 2D/3D live-cell microscopy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13214
&lt;/p&gt;
&lt;p&gt;
Nellie&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#12289;&#26080;&#20559;&#35265;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#22312;2D/3D&#27963;&#32454;&#32990;&#26174;&#24494;&#38236;&#19979;&#20998;&#21106;&#12289;&#36319;&#36394;&#21644;&#25552;&#21462;&#22810;&#26679;&#32454;&#32990;&#20869;&#32467;&#26500;&#29305;&#24449;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#20998;&#23618;&#20998;&#21106;&#21644;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#32454;&#32990;&#22120;&#30340;&#20998;&#26512;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#65292;&#20294;&#23545;&#20110;&#29702;&#35299;&#29983;&#29289;&#23398;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Nellie&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#19988;&#26080;&#20559;&#35265;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#20998;&#21106;&#12289;&#36319;&#36394;&#21644;&#25552;&#21462;&#22810;&#26679;&#30340;&#32454;&#32990;&#20869;&#32467;&#26500;&#29305;&#24449;&#12290;Nellie&#33021;&#22815;&#36866;&#24212;&#22270;&#20687;&#30340;&#20803;&#25968;&#25454;&#65292;&#28040;&#38500;&#20102;&#29992;&#25143;&#30340;&#36755;&#20837;&#12290;Nellie&#30340;&#39044;&#22788;&#29702;&#31649;&#36947;&#22312;&#22810;&#20010;&#32454;&#32990;&#20869;&#23610;&#24230;&#19978;&#22686;&#24378;&#20102;&#32467;&#26500;&#23545;&#27604;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20122;&#22120;&#23448;&#21306;&#22495;&#30340;&#24378;&#22823;&#20998;&#23618;&#20998;&#21106;&#12290;&#36890;&#36807;&#21322;&#24452;&#33258;&#36866;&#24212;&#30340;&#27169;&#24335;&#21305;&#37197;&#26041;&#26696;&#29983;&#25104;&#21644;&#36319;&#36394;&#20869;&#37096;&#36816;&#21160;&#25429;&#25417;&#26631;&#35760;&#65292;&#24182;&#29992;&#20316;&#20122;&#20307;&#31215;&#27969;&#25554;&#20540;&#30340;&#25351;&#21335;&#12290;Nellie&#22312;&#22810;&#20010;&#20998;&#23618;&#27700;&#24179;&#25552;&#21462;&#22823;&#37327;&#29305;&#24449;&#65292;&#29992;&#20110;&#28145;&#24230;&#21644;&#21487;&#23450;&#21046;&#30340;&#20998;&#26512;&#12290;Nellie&#20855;&#26377;&#22522;&#20110;Napari&#30340;GUI&#65292;&#23454;&#29616;&#26080;&#20195;&#30721;&#25805;&#20316;&#21644;&#21487;&#35270;&#21270;&#65292;&#21516;&#26102;&#20854;&#27169;&#22359;&#21270;&#30340;&#24320;&#28304;&#20195;&#30721;&#24211;&#25552;&#20379;&#20102;&#32463;&#39564;&#20016;&#23500;&#29992;&#25143;&#30340;&#33258;&#23450;&#20041;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13214v1 Announce Type: cross  Abstract: The analysis of dynamic organelles remains a formidable challenge, though key to understanding biological processes. We introduce Nellie, an automated and unbiased pipeline for segmentation, tracking, and feature extraction of diverse intracellular structures. Nellie adapts to image metadata, eliminating user input. Nellie's preprocessing pipeline enhances structural contrast on multiple intracellular scales allowing for robust hierarchical segmentation of sub-organellar regions. Internal motion capture markers are generated and tracked via a radius-adaptive pattern matching scheme, and used as guides for sub-voxel flow interpolation. Nellie extracts a plethora of features at multiple hierarchical levels for deep and customizable analysis. Nellie features a Napari-based GUI that allows for code-free operation and visualization, while its modular open-source codebase invites customization by experienced users. We demonstrate Nellie's wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#23433;&#20840;&#24615;&#30340;&#35774;&#35745;&#33539;&#24335;&#65292;&#22312;&#32447;&#24615;&#32422;&#26463;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;$\tilde{\mathcal{O}}(\sqrt{T})$&#30340;&#21518;&#24724;&#20540;&#65292;&#21516;&#26102;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#22312;&#26102;&#21464;&#38543;&#26426;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;OCO&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#31639;&#27861;&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05786</link><description>&lt;p&gt;
&#32447;&#24615;&#32422;&#26463;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#20048;&#35266;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Optimistic Safety for Linearly-Constrained Online Convex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#23433;&#20840;&#24615;&#30340;&#35774;&#35745;&#33539;&#24335;&#65292;&#22312;&#32447;&#24615;&#32422;&#26463;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;$\tilde{\mathcal{O}}(\sqrt{T})$&#30340;&#21518;&#24724;&#20540;&#65292;&#21516;&#26102;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#22312;&#26102;&#21464;&#38543;&#26426;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;OCO&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#31639;&#27861;&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#26410;&#30693;&#32422;&#26463;&#19979;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#38382;&#39064;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#20010;&#20855;&#26377;&#38745;&#24577;&#32447;&#24615;&#32422;&#26463;&#30340;&#38382;&#39064;&#29256;&#26412;&#65292;&#29609;&#23478;&#20250;&#25910;&#21040;&#22024;&#26434;&#21453;&#39304;&#24182;&#19988;&#24517;&#39035;&#22987;&#32456;&#28385;&#36275;&#36825;&#20123;&#32422;&#26463;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#21019;&#26032;&#30340;&#20048;&#35266;&#23433;&#20840;&#24615;&#35774;&#35745;&#33539;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#30340;&#21518;&#24724;&#20540;&#20026;$\tilde{\mathcal{O}}(\sqrt{T})$&#12290;&#36825;&#19968;&#25913;&#36827;&#20102;&#20197;&#24448;$\tilde{\mathcal{O}}(T^{2/3})$&#30340;&#26368;&#20339;&#21518;&#24724;&#20540;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#20102;&#30053;&#24378;&#30340;&#29420;&#31435;&#22122;&#22768;&#21644;&#26080;&#24847;&#35782;&#23545;&#25163;&#30340;&#20551;&#35774;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#36825;&#20010;&#38382;&#39064;&#37325;&#26032;&#26500;&#24314;&#20026;&#22312;&#26102;&#21464;&#38543;&#26426;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;OCO&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#36825;&#26679;&#30340;&#35774;&#32622;&#19979;&#20855;&#26377;&#30456;&#21516;&#30340;&#21518;&#24724;&#20540;&#20445;&#35777;&#65292;&#24182;&#19988;&#20174;&#26399;&#26395;&#19978;&#27704;&#36828;&#19981;&#20250;&#36829;&#21453;&#32422;&#26463;&#12290;&#36825;&#20026;OCO&#22312;&#26102;&#21464;&#38543;&#26426;&#32422;&#26463;&#19979;&#30340;&#25991;&#29486;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#20854;&#20013;&#29616;&#26377;&#30340;&#20808;&#36827;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05786v1 Announce Type: new  Abstract: The setting of online convex optimization (OCO) under unknown constraints has garnered significant attention in recent years. In this work, we consider a version of this problem with static linear constraints that the player receives noisy feedback of and must always satisfy. By leveraging our novel design paradigm of optimistic safety, we give an algorithm for this problem that enjoys $\tilde{\mathcal{O}}(\sqrt{T})$ regret. This improves on the previous best regret bound of $\tilde{\mathcal{O}}(T^{2/3})$ while using only slightly stronger assumptions of independent noise and an oblivious adversary. Then, by recasting this problem as OCO under time-varying stochastic linear constraints, we show that our algorithm enjoys the same regret guarantees in such a setting and never violates the constraints in expectation. This contributes to the literature on OCO under time-varying stochastic constraints, where the state-of-the-art algorithms en
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21160;&#24577;&#35268;&#21010;&#12289;&#24037;&#20855;&#38598;&#25104;&#21644;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18679</link><description>&lt;p&gt;
&#25968;&#25454;&#35299;&#37322;&#22120;&#65306;&#29992;&#20110;&#25968;&#25454;&#31185;&#23398;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Data Interpreter: An LLM Agent For Data Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#37319;&#29992;&#21160;&#24577;&#35268;&#21010;&#12289;&#24037;&#20855;&#38598;&#25104;&#21644;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#24050;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#23454;&#26102;&#25968;&#25454;&#35843;&#25972;&#12289;&#20248;&#21270;&#19987;&#19994;&#30693;&#35782;&#20197;&#24212;&#23545;&#21508;&#31181;&#20219;&#21153;&#38388;&#22797;&#26434;&#20381;&#36182;&#24615;&#20197;&#21450;&#31934;&#30830;&#25512;&#29702;&#30340;&#36923;&#36753;&#38169;&#35823;&#35782;&#21035;&#30340;&#25968;&#25454;&#31185;&#23398;&#22330;&#26223;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#24378;&#35843;&#19977;&#31181;&#20851;&#38190;&#25216;&#26415;&#20197;&#22686;&#24378;&#25968;&#25454;&#31185;&#23398;&#20013;&#38382;&#39064;&#35299;&#20915;&#30340;&#26041;&#26696;&#30340;&#20195;&#30721;&#65306;1&#65289;&#20855;&#26377;&#20998;&#23618;&#22270;&#32467;&#26500;&#30340;&#21160;&#24577;&#35268;&#21010;&#65292;&#29992;&#20110;&#23454;&#26102;&#25968;&#25454;&#36866;&#24212;&#24615;&#65307;2&#65289;&#24037;&#20855;&#38598;&#25104;&#21160;&#24577;&#21270;&#65292;&#20197;&#22686;&#24378;&#20195;&#30721;&#25191;&#34892;&#36807;&#31243;&#20013;&#30340;&#29087;&#32451;&#24230;&#65292;&#20016;&#23500;&#24517;&#35201;&#30340;&#19987;&#19994;&#30693;&#35782;&#65307;3&#65289;&#22312;&#21453;&#39304;&#20013;&#35782;&#21035;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#36890;&#36807;&#32463;&#39564;&#35760;&#24405;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25968;&#25454;&#35299;&#37322;&#22120;&#22312;&#21508;&#31181;&#25968;&#25454;&#31185;&#23398;&#21644;&#29616;&#23454;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#19982;&#24320;&#28304;&#22522;&#32447;&#30456;&#27604;&#65292;&#23427;&#23637;&#29616;&#20102;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18679v1 Announce Type: new  Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21033;&#29992;&#37096;&#20998;&#25513;&#30721;&#34701;&#21512;&#30340;Gromov-Wasserstein&#21305;&#37197;&#36827;&#34892;&#20219;&#24847;&#22823;&#23567;&#22270;&#30340;&#31471;&#23545;&#31471;&#30417;&#30563;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30456;&#27604;&#31454;&#20105;&#32773;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12269</link><description>&lt;p&gt;
&#21033;&#29992;&#37096;&#20998;&#25513;&#30721;&#34701;&#21512;&#30340;Gromov-Wasserstein&#21305;&#37197;&#36827;&#34892;&#20219;&#24847;&#22823;&#23567;&#22270;&#30340;&#31471;&#23545;&#31471;&#30417;&#30563;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
End-to-end Supervised Prediction of Arbitrary-size Graphs with Partially-Masked Fused Gromov-Wasserstein Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12269
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21033;&#29992;&#37096;&#20998;&#25513;&#30721;&#34701;&#21512;&#30340;Gromov-Wasserstein&#21305;&#37197;&#36827;&#34892;&#20219;&#24847;&#22823;&#23567;&#22270;&#30340;&#31471;&#23545;&#31471;&#30417;&#30563;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30456;&#27604;&#31454;&#20105;&#32773;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#30340;&#30417;&#30563;&#22270;&#39044;&#27979;&#65288;SGP&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21407;&#22987;&#30340;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#30340;&#25439;&#22833;&#65292;&#37096;&#20998;&#25513;&#30721;&#34701;&#21512;&#30340;Gromov-Wasserstein&#25439;&#22833;&#65288;PM-FGW&#65289;&#65292;&#21487;&#20197;&#30452;&#25509;&#21033;&#29992;&#22270;&#34920;&#31034;&#65292;&#27604;&#22914;&#37051;&#25509;&#21644;&#29305;&#24449;&#30697;&#38453;&#12290;PM-FGW&#20855;&#26377;SGP&#30340;&#25152;&#26377;&#29702;&#24819;&#23646;&#24615;&#65306;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#24615;&#65292;&#21487;&#24494;&#20998;&#24615;&#65292;&#36890;&#36807;&#27604;&#36739;&#23427;&#20204;&#30340;&#22635;&#20805;&#34920;&#31034;&#20197;&#21450;&#23427;&#20204;&#30340;&#25513;&#30721;&#21521;&#37327;&#22788;&#29702;&#19981;&#21516;&#22823;&#23567;&#30340;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#22312;&#23454;&#39564;&#37096;&#20998;&#65292;&#19977;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#19968;&#20010;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65288;image2graph&#65289;&#21644;&#20004;&#20010;&#30495;&#23454;&#20219;&#21153;&#65292;&#22270;&#20687;&#21040;&#22320;&#22270;&#21644;&#25351;&#32441;&#21040;&#20998;&#23376; - &#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30456;&#27604;&#31454;&#20105;&#32773;&#30340;&#25928;&#29575;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12269v1 Announce Type: new  Abstract: We present a novel end-to-end deep learning-based approach for Supervised Graph Prediction (SGP). We introduce an original Optimal Transport (OT)-based loss, the Partially-Masked Fused Gromov-Wasserstein loss (PM-FGW), that allows to directly leverage graph representations such as adjacency and feature matrices. PM-FGW exhibits all the desirable properties for SGP: it is node permutation invariant, sub-differentiable and handles graphs of different sizes by comparing their padded representations as well as their masking vectors. Moreover, we present a flexible transformer-based architecture that easily adapts to different types of input data. In the experimental section, three different tasks, a novel and challenging synthetic dataset (image2graph) and two real-world tasks, image2map and fingerprint2molecule - showcase the efficiency and versatility of the approach compared to competitors.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#25311;&#24322;&#24120;&#33410;&#28857;&#26469;&#35757;&#32451;&#21028;&#21035;&#24615;&#21333;&#31867;&#20998;&#31867;&#22120;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#22270;&#20013;&#30340;&#24050;&#30693;&#27491;&#24120;&#33410;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.11887</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Generative Semi-supervised Graph Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11887
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#25311;&#24322;&#24120;&#33410;&#28857;&#26469;&#35757;&#32451;&#21028;&#21035;&#24615;&#21333;&#31867;&#20998;&#31867;&#22120;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#22270;&#20013;&#30340;&#24050;&#30693;&#27491;&#24120;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#32771;&#34385;&#20102;&#19968;&#20010;&#23454;&#38469;&#24773;&#22659;&#19979;&#30340;&#21322;&#30417;&#30563;&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#65292;&#22312;&#36825;&#20010;&#24773;&#22659;&#20013;&#65292;&#22270;&#20013;&#30340;&#37096;&#20998;&#33410;&#28857;&#34987;&#30693;&#26195;&#26159;&#27491;&#24120;&#30340;&#65292;&#19982;&#22823;&#22810;&#25968;GAD&#30740;&#31350;&#20013;&#20351;&#29992;&#23436;&#20840;&#26410;&#26631;&#35760;&#22270;&#30340;&#26080;&#30417;&#30563;&#24773;&#20917;&#24418;&#25104;&#23545;&#27604;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#27491;&#24120;&#33410;&#28857;&#26377;&#21161;&#20110;&#25552;&#21319;&#29616;&#26377;&#26080;&#30417;&#30563;GAD&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#24773;&#22659;&#19979;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#36825;&#20123;&#27491;&#24120;&#33410;&#28857;&#30340;&#21033;&#29992;&#26159;&#26377;&#38480;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#21322;&#30417;&#30563;&#24773;&#22659;&#30340;&#29983;&#25104;&#24335;GAD&#26041;&#27861;&#65288;GGAD&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#27491;&#24120;&#33410;&#28857;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#29983;&#25104;&#27169;&#25311;&#24322;&#24120;&#33410;&#28857;&#30340;&#24322;&#24120;&#33410;&#28857;&#65292;&#23427;&#20204;&#34701;&#21512;&#20102;&#26412;&#22320;&#32467;&#26500;&#21644;&#33410;&#28857;&#34920;&#31034;&#65292;&#20026;&#35757;&#32451;&#21028;&#21035;&#22411;&#21333;&#31867;&#20998;&#31867;&#22120;&#25552;&#20379;&#26377;&#25928;&#30340;&#36127;&#38754;&#33410;&#28857;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11887v1 Announce Type: new  Abstract: This work considers a practical semi-supervised graph anomaly detection (GAD) scenario, where part of the nodes in a graph are known to be normal, contrasting to the unsupervised setting in most GAD studies with a fully unlabeled graph. As expected, we find that having access to these normal nodes helps enhance the detection performance of existing unsupervised GAD methods when they are adapted to the semi-supervised setting. However, their utilization of these normal nodes is limited. In this paper, we propose a novel Generative GAD approach (GGAD) for the semi-supervised scenario to better exploit the normal nodes. The key idea is to generate outlier nodes that assimilate anomaly nodes in both local structure and node representations for providing effective negative node samples in training a discriminative one-class classifier. There have been many generative anomaly detection approaches, but they are designed for non-graph data, and 
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#23398;&#20064;&#23545;&#31034;&#20363;&#35838;&#31243;&#21644;&#20219;&#21153;&#32467;&#26500;&#26377;&#25935;&#24863;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#33719;&#24471;&#20998;&#32452;&#21644;&#20132;&#38169;&#35757;&#32451;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.08674</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#20986;&#29616;&#20154;&#31867;&#35838;&#31243;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Human Curriculum Effects Emerge with In-Context Learning in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08674
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23398;&#20064;&#23545;&#31034;&#20363;&#35838;&#31243;&#21644;&#20219;&#21153;&#32467;&#26500;&#26377;&#25935;&#24863;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#33719;&#24471;&#20998;&#32452;&#21644;&#20132;&#38169;&#35757;&#32451;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23398;&#20064;&#23545;&#35268;&#21017;&#32467;&#26500;&#21644;&#35757;&#32451;&#20013;&#25152;&#20351;&#29992;&#30340;&#31034;&#20363;&#35838;&#31243;&#38750;&#24120;&#25935;&#24863;&#12290;&#22312;&#30001;&#31616;&#27905;&#35268;&#21017;&#25511;&#21046;&#30340;&#20219;&#21153;&#20013;&#65292;&#24403;&#30456;&#20851;&#31034;&#20363;&#22312;&#22810;&#27425;&#35797;&#39564;&#20013;&#34987;&#20998;&#32452;&#26102;&#65292;&#23398;&#20064;&#26356;&#21152;&#31283;&#20581;&#65307;&#20294;&#22312;&#32570;&#20047;&#36825;&#26679;&#30340;&#35268;&#21017;&#30340;&#24773;&#20917;&#19979;&#65292;&#20132;&#38169;&#35757;&#32451;&#26356;&#21152;&#26377;&#25928;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#27809;&#26377;&#31070;&#32463;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;&#21040;&#36825;&#20123;&#30475;&#20284;&#30683;&#30462;&#30340;&#25928;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#19978;&#19979;&#25991;&#23398;&#20064;&#8221;&#65288;ICL&#65289;&#22312;&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#33258;&#21457;&#20135;&#29983;&#20102;&#21516;&#26679;&#30340;&#26435;&#34913;&#12290;ICL&#26159;&#36890;&#36807;&#20869;&#23618;&#24490;&#29615;&#31639;&#27861;&#22312;&#28608;&#27963;&#21160;&#21147;&#23398;&#20013;&#23454;&#29616;&#30340;&#19968;&#31181;&#8220;&#19978;&#19979;&#25991;&#20869;&#23398;&#20064;&#8221;&#65288;in-context learning&#65289;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26435;&#37325;&#26356;&#25913;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#23545;&#39044;&#35757;&#32451;&#30340;LLMs&#21644;&#20803;&#23398;&#20064;&#21464;&#21387;&#22120;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ICL&#22312;&#28041;&#21450;&#35268;&#21017;&#32467;&#26500;&#30340;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20154;&#31867;&#25152;&#31034;&#30340;&#20998;&#32452;&#20248;&#21183;&#65292;&#32780;&#21516;&#26102;&#36827;&#34892;&#26435;&#37325;&#23398;&#20064;&#21017;&#22797;&#21046;&#20102;&#20154;&#31867;&#22312;&#32570;&#23569;&#36825;&#26679;&#32467;&#26500;&#30340;&#20219;&#21153;&#19978;&#25152;&#35266;&#23519;&#21040;&#30340;&#20132;&#38169;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human learning is sensitive to rule-like structure and the curriculum of examples used for training. In tasks governed by succinct rules, learning is more robust when related examples are blocked across trials, but in the absence of such rules, interleaving is more effective. To date, no neural model has simultaneously captured these seemingly contradictory effects. Here we show that this same tradeoff spontaneously emerges with "in-context learning" (ICL) both in neural networks trained with metalearning and in large language models (LLMs). ICL is the ability to learn new tasks "in context" - without weight changes - via an inner-loop algorithm implemented in activation dynamics. Experiments with pretrained LLMs and metalearning transformers show that ICL exhibits the blocking advantage demonstrated in humans on a task involving rule-like structure, and conversely, that concurrent in-weight learning reproduces the interleaving advantage observed in humans on tasks lacking such structu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#23450;&#29702;&#65292;&#25193;&#23637;&#20102;&#32463;&#20856;&#23450;&#29702;&#23545;&#20110;&#29420;&#31435;&#20294;&#38750;&#24658;&#23450;&#20998;&#24067;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#35813;&#23450;&#29702;&#22312;&#39640;&#32500;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#38750;&#20809;&#28369;&#20248;&#21270;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.07356</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#30340;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#23450;&#29702;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Novel Gaussian Min-Max Theorem and its Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#23450;&#29702;&#65292;&#25193;&#23637;&#20102;&#32463;&#20856;&#23450;&#29702;&#23545;&#20110;&#29420;&#31435;&#20294;&#38750;&#24658;&#23450;&#20998;&#24067;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#35813;&#23450;&#29702;&#22312;&#39640;&#32500;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#38750;&#20809;&#28369;&#20248;&#21270;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gordon&#30340;&#19968;&#20010;&#33879;&#21517;&#32467;&#26524;&#20801;&#35768;&#27604;&#36739;&#20004;&#20010;&#39640;&#26031;&#36807;&#31243;&#30340;&#26368;&#23567;&#26368;&#22823;&#34892;&#20026;&#65292;&#22914;&#26524;&#28385;&#36275;&#26576;&#20123;&#19981;&#31561;&#24335;&#26465;&#20214;&#12290;&#36825;&#20010;&#32467;&#26524;&#30340;&#32467;&#26524;&#21253;&#25324;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#65288;GMT&#65289;&#21644;&#20984;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#65288;CGMT&#65289;&#23450;&#29702;&#65292;&#36825;&#20123;&#23450;&#29702;&#22312;&#39640;&#32500;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#38750;&#20809;&#28369;&#20248;&#21270;&#21644;&#20449;&#21495;&#22788;&#29702;&#26041;&#38754;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#30446;&#21069;&#20026;&#27490;&#65292;&#27809;&#26377;&#21457;&#29616;&#28385;&#36275;&#36825;&#20123;&#19981;&#31561;&#24335;&#30340;&#20854;&#20182;&#19968;&#23545;&#39640;&#26031;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#26679;&#19968;&#23545;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#23450;&#29702;&#23558;&#32463;&#20856;&#30340;GMT&#23450;&#29702;&#21644;CGMT&#23450;&#29702;&#20174;&#22522;&#26412;&#36807;&#31243;&#20013;&#30340;&#24213;&#23618;&#39640;&#26031;&#30697;&#38453;&#20855;&#26377;iid&#34892;&#30340;&#24773;&#20917;&#25193;&#23637;&#21040;&#20855;&#26377;&#29420;&#31435;&#20294;&#38750;&#24658;&#23450;&#20998;&#24067;&#30340;&#24773;&#20917;&#12290;&#26032;&#30340;CGMT&#23450;&#29702;&#24212;&#29992;&#20110;&#22810;&#28304;&#39640;&#26031;&#22238;&#24402;&#38382;&#39064;&#65292;&#20197;&#21450;&#23646;&#20110;&#30340;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A celebrated result by Gordon allows one to compare the min-max behavior of two Gaussian processes if certain inequality conditions are met. The consequences of this result include the Gaussian min-max (GMT) and convex Gaussian min-max (CGMT) theorems which have had far-reaching implications in high-dimensional statistics, machine learning, non-smooth optimization, and signal processing. Both theorems rely on a pair of Gaussian processes, first identified by Slepian, that satisfy Gordon's comparison inequalities. To date, no other pair of Gaussian processes satisfying these inequalities has been discovered. In this paper, we identify such a new pair. The resulting theorems extend the classical GMT and CGMT Theorems from the case where the underlying Gaussian matrix in the primary process has iid rows to where it has independent but non-identically-distributed ones. The new CGMT is applied to the problems of multi-source Gaussian regression, as well as to binary classification of genera
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#30340;&#22870;&#21169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#31526;&#21512;&#36136;&#37327;&#26631;&#20934;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#33021;&#25552;&#39640;&#20854;&#39044;&#27979;&#19982;&#22870;&#21169;&#20989;&#25968;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04856</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Explaining Learned Reward Functions with Counterfactual Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04856
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#30340;&#22870;&#21169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#31526;&#21512;&#36136;&#37327;&#26631;&#20934;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#33021;&#25552;&#39640;&#20854;&#39044;&#27979;&#19982;&#22870;&#21169;&#20989;&#25968;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#34892;&#20026;&#25110;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#26159;&#23558;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20294;&#26080;&#27861;&#22987;&#32456;&#25552;&#21462;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21644;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#20197;&#21450;&#23427;&#20204;&#21508;&#33258;&#25509;&#25910;&#30340;&#22870;&#21169;&#26469;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#20026;CTEs&#21046;&#23450;&#20102;&#20845;&#20010;&#36136;&#37327;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Monte-Carlo&#30340;&#26032;&#31639;&#27861;&#26469;&#29983;&#25104;&#20248;&#21270;&#36825;&#20123;&#36136;&#37327;&#26631;&#20934;&#30340;CTEs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#20154;&#27169;&#22411;&#26469;&#34913;&#37327;&#29983;&#25104;&#30340;&#35299;&#37322;&#23545;&#20854;&#30340;&#20449;&#24687;&#24615;&#12290;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#22686;&#21152;&#20102;&#20854;&#39044;&#27979;&#19982;&#26410;&#35265;&#36712;&#36857;&#19978;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#23398;&#20250;&#20102;&#20934;&#30830;&#21028;&#26029;&#36712;&#36857;&#20043;&#38388;&#30340;&#22870;&#21169;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning rewards from human behaviour or feedback is a promising approach to aligning AI systems with human values but fails to consistently extract correct reward functions. Interpretability tools could enable users to understand and evaluate possible flaws in learned reward functions. We propose Counterfactual Trajectory Explanations (CTEs) to interpret reward functions in reinforcement learning by contrasting an original with a counterfactual partial trajectory and the rewards they each receive. We derive six quality criteria for CTEs and propose a novel Monte-Carlo-based algorithm for generating CTEs that optimises these quality criteria. Finally, we measure how informative the generated explanations are to a proxy-human model by training it on CTEs. CTEs are demonstrably informative for the proxy-human model, increasing the similarity between its predictions and the reward function on unseen trajectories. Further, it learns to accurately judge differences in rewards between trajec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#65288;MiPS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#25512;&#29702;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#23436;&#25104;&#26469;&#33258;&#21160;&#36827;&#34892;&#25968;&#25454;&#25972;&#29702;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#25105;&#20204;&#24212;&#20248;&#20808;&#36873;&#25321;&#39564;&#35777;&#22120;&#39044;&#27979;&#24471;&#20998;&#39640;&#30340;&#39564;&#35777;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;PaLM 2&#22312;&#25968;&#23398;&#21644;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02658</link><description>&lt;p&gt;
&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#30340;&#39564;&#35777;&#22120;&#65306;&#20851;&#20110;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#65288;MiPS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#25512;&#29702;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#23436;&#25104;&#26469;&#33258;&#21160;&#36827;&#34892;&#25968;&#25454;&#25972;&#29702;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#25105;&#20204;&#24212;&#20248;&#20808;&#36873;&#25321;&#39564;&#35777;&#22120;&#39044;&#27979;&#24471;&#20998;&#39640;&#30340;&#39564;&#35777;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25913;&#36827;&#20102;PaLM 2&#22312;&#25968;&#23398;&#21644;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#30417;&#30563;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#39564;&#35777;&#22120;&#26469;&#35780;&#20272;&#25512;&#29702;&#22120;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#65292;&#24050;&#32463;&#22312;&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#36991;&#20813;&#22312;&#39564;&#35777;&#22120;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#24341;&#23548;&#30340;&#36807;&#31243;&#30417;&#30563;&#65288;MiPS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#25968;&#25454;&#25972;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;MiPS&#36890;&#36807;&#23545;&#25512;&#29702;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#23436;&#25104;&#65292;&#24182;&#33719;&#24471;&#19968;&#20010;&#20934;&#30830;&#29575;&#65292;&#20854;&#20013;&#20934;&#30830;&#23436;&#25104;&#30340;&#27604;&#20363;&#23450;&#20041;&#20026;&#20934;&#30830;&#29575;&#12290;&#25512;&#29702;&#22120;&#20013;&#30340;&#38169;&#35823;&#20250;&#23548;&#33268;MiPS&#20302;&#20272;&#20013;&#38388;&#27493;&#39588;&#30340;&#20934;&#30830;&#29575;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#24212;&#20248;&#20808;&#36873;&#25321;&#39564;&#35777;&#22120;&#39044;&#27979;&#24471;&#20998;&#39640;&#30340;&#39564;&#35777;&#65292;&#32780;&#19981;&#26159;&#20302;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;PaLM 2&#22312;&#25968;&#23398;&#21644;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65288;GSM8K&#19978;&#30340;&#20934;&#30830;&#29575;+0.67&#65285;&#65292;&#25968;&#23398;&#19978;&#30340;&#20934;&#30830;&#29575;+4.16&#65285;&#65292;MBPP&#19978;&#30340;&#20934;&#30830;&#29575;+0.92&#65285;&#19982;&#36755;&#20986;s&#30456;&#27604;&#12290;&#65289;
&lt;/p&gt;
&lt;p&gt;
Process supervision, using a trained verifier to evaluate the intermediate steps generated by reasoner, has demonstrated significant improvements in multi-step problem solving. In this paper, to avoid expensive human annotation effort on the verifier training data, we introduce Model-induced Process Supervision (MiPS), a novel method for automating data curation. MiPS annotates an intermediate step by sampling completions of this solution through the reasoning model, and obtaining an accuracy defined as the proportion of correct completions. Errors in the reasoner would cause MiPS to underestimate the accuracy of intermediate steps, therefore, we suggest and empirically show that verification focusing on high predicted scores of the verifier shall be preferred over that of low predicted scores, contrary to prior work. Our approach significantly improves the performance of PaLM 2 on math and coding tasks (accuracy +0.67% on GSM8K, +4.16% on MATH, +0.92% on MBPP compared with an output s
&lt;/p&gt;</description></item><item><title>AnimateLCM&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31163;&#30340;&#19968;&#33268;&#24615;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#29983;&#25104;&#20248;&#20808;&#32423;&#21644;&#21160;&#20316;&#29983;&#25104;&#20248;&#20808;&#32423;&#30340;&#33976;&#39311;&#20998;&#31163;&#24320;&#26469;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#24182;&#22686;&#24378;&#20102;&#29983;&#25104;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.00769</link><description>&lt;p&gt;
AnimateLCM: &#20351;&#29992;&#20998;&#31163;&#30340;&#19968;&#33268;&#24615;&#23398;&#20064;&#21152;&#36895;&#20010;&#24615;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#36866;&#37197;&#22120;&#30340;&#21160;&#30011;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00769
&lt;/p&gt;
&lt;p&gt;
AnimateLCM&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31163;&#30340;&#19968;&#33268;&#24615;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#29983;&#25104;&#20248;&#20808;&#32423;&#21644;&#21160;&#20316;&#29983;&#25104;&#20248;&#20808;&#32423;&#30340;&#33976;&#39311;&#20998;&#31163;&#24320;&#26469;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#24182;&#22686;&#24378;&#20102;&#29983;&#25104;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#33021;&#22815;&#20135;&#29983;&#36830;&#36143;&#19988;&#39640;&#20445;&#30495;&#24230;&#30340;&#35270;&#39057;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36845;&#20195;&#30340;&#21435;&#22122;&#36807;&#31243;&#20351;&#20854;&#35745;&#31639;&#23494;&#38598;&#19988;&#32791;&#26102;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;&#21463;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CM&#65289;&#30340;&#21551;&#21457;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#26368;&#23567;&#30340;&#27493;&#39588;&#33976;&#39311;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20197;&#21152;&#36895;&#37319;&#26679;&#65292;&#20197;&#21450;&#20854;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#19978;&#30340;&#25104;&#21151;&#25193;&#23637;&#8212;&#8212;&#28508;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;LCM&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AnimateLCM&#65292;&#20801;&#35768;&#22312;&#26368;&#23567;&#30340;&#27493;&#39588;&#20869;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31163;&#30340;&#19968;&#33268;&#24615;&#23398;&#20064;&#31574;&#30053;&#65292;&#23558;&#22270;&#20687;&#29983;&#25104;&#20248;&#20808;&#32423;&#21644;&#21160;&#20316;&#29983;&#25104;&#20248;&#20808;&#32423;&#30340;&#33976;&#39311;&#20998;&#31163;&#24320;&#26469;&#65292;&#36825;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#24182;&#22686;&#24378;&#20102;&#29983;&#25104;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#31283;&#23450;&#30340;&#25193;&#25955;&#31038;&#21306;&#20013;&#30340;&#21363;&#25554;&#21363;&#29992;&#36866;&#37197;&#22120;&#30340;&#32452;&#21512;&#20197;&#23454;&#29616;&#21508;&#31181;&#20462;&#25913;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#36866;&#37197;&#22120;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video diffusion models has been gaining increasing attention for its ability to produce videos that are both coherent and of high fidelity. However, the iterative denoising process makes it computationally intensive and time-consuming, thus limiting its applications. Inspired by the Consistency Model (CM) that distills pretrained image diffusion models to accelerate the sampling with minimal steps and its successful extension Latent Consistency Model (LCM) on conditional image generation, we propose AnimateLCM, allowing for high-fidelity video generation within minimal steps. Instead of directly conducting consistency learning on the raw video dataset, we propose a decoupled consistency learning strategy that decouples the distillation of image generation priors and motion generation priors, which improves the training efficiency and enhance the generation visual quality. Additionally, to enable the combination of plug-and-play adapters in stable diffusion community to achieve various 
&lt;/p&gt;</description></item><item><title>ComplexityNet&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#25552;&#39640;&#20102;LLM&#25512;&#29702;&#25928;&#29575;&#65292;&#36890;&#36807;&#39044;&#27979;&#20219;&#21153;&#30340;&#20934;&#30830;&#36755;&#20986;&#27010;&#29575;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;90%&#30340;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#65292;&#24182;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#30830;&#23450;&#26041;&#38754;&#21462;&#24471;&#20102;79%&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.11511</link><description>&lt;p&gt;
ComplexityNet: &#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#22797;&#26434;&#24615;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
ComplexityNet: Increasing LLM Inference Efficiency by Learning Task Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11511
&lt;/p&gt;
&lt;p&gt;
ComplexityNet&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#25552;&#39640;&#20102;LLM&#25512;&#29702;&#25928;&#29575;&#65292;&#36890;&#36807;&#39044;&#27979;&#20219;&#21153;&#30340;&#20934;&#30830;&#36755;&#20986;&#27010;&#29575;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;90%&#30340;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#65292;&#24182;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#30830;&#23450;&#26041;&#38754;&#21462;&#24471;&#20102;79%&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ComplexityNet&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#35780;&#20272;&#20219;&#21153;&#22797;&#26434;&#24615;&#32780;&#35774;&#35745;&#30340;&#31616;&#21270;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#19981;&#21516;&#33021;&#21147;&#30340;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#26469;&#39044;&#27979;&#20934;&#30830;&#36755;&#20986;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#39318;&#27425;&#22312;Mostly Basic Python Problems&#65288;MBPP&#65289;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#20102;ComplexityNet&#12290;&#25105;&#20204;&#24320;&#21019;&#24615;&#22320;&#21019;&#24314;&#20102;&#31532;&#19968;&#32452;&#26631;&#31614;&#26469;&#23450;&#20041;&#20219;&#21153;&#22797;&#26434;&#24615;&#12290;ComplexityNet&#22312;&#30830;&#23450;&#20219;&#21153;&#22797;&#26434;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;79%&#20934;&#30830;&#29575;&#65292;&#36739;&#21407;&#22987;&#12289;&#38750;&#24494;&#35843;&#27169;&#22411;&#30340;34%&#20934;&#30830;&#29575;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#19982;&#20351;&#29992;&#26368;&#39640;&#22797;&#26434;&#24615;&#27169;&#22411;&#30456;&#27604;&#65292;ComplexityNet&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;90%&#30340;&#35745;&#31639;&#36164;&#28304;&#20351;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;86.7%&#30340;&#39640;&#20195;&#30721;&#29983;&#25104;&#20934;&#30830;&#29575;&#12290;&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#24494;&#35843;&#36739;&#23567;&#30340;&#27169;&#22411;&#26469;&#23545;&#20219;&#21153;&#36827;&#34892;&#20998;&#31867;&#65292;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#26356;&#24179;&#34913;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11511v2 Announce Type: replace-cross  Abstract: We present ComplexityNet, a streamlined language model designed for assessing task complexity. This model predicts the likelihood of accurate output by various language models, each with different capabilities. Our initial application of ComplexityNet involves the Mostly Basic Python Problems (MBPP) dataset. We pioneered the creation of the first set of labels to define task complexity. ComplexityNet achieved a notable 79% accuracy in determining task complexity, a significant improvement over the 34% accuracy of the original, non fine-tuned model. Furthermore, ComplexityNet effectively reduces computational resource usage by 90% compared to using the highest complexity model, while maintaining a high code generation accuracy of 86.7%. This study demonstrates that fine-tuning smaller models to categorize tasks based on their complexity can lead to a more balanced trade-off between accuracy and efficiency in the use of Large Lan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#32447;&#24615;&#21464;&#25442;&#22120;&#20316;&#20026;transformer&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;transformers&#22312;&#22788;&#29702;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#21644;&#25512;&#26029;&#25104;&#26412;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15719</link><description>&lt;p&gt;
&#24490;&#29615;&#32447;&#24615;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Recurrent Linear Transformers. (arXiv:2310.15719v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#32447;&#24615;&#21464;&#25442;&#22120;&#20316;&#20026;transformer&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;transformers&#22312;&#22788;&#29702;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#21644;&#25512;&#26029;&#25104;&#26412;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
transformer&#26550;&#26500;&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#33021;&#22815;&#25429;&#25417;&#38271;&#36317;&#31163;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#20063;&#26159;&#20854;&#22312;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#26102;&#26377;&#25928;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#25104;&#21151;&#65292;transformers&#20173;&#28982;&#26377;&#20004;&#20010;&#37325;&#22823;&#32570;&#28857;&#65292;&#38480;&#21046;&#20102;&#20854;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65306;(1)&#20026;&#20102;&#35760;&#20303;&#36807;&#21435;&#30340;&#20449;&#24687;&#65292;&#33258;&#27880;&#24847;&#26426;&#21046;&#38656;&#35201;&#35775;&#38382;&#25972;&#20010;&#21382;&#21490;&#20449;&#24687;&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;(2)transformers&#30340;&#25512;&#26029;&#25104;&#26412;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;transformer&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#24490;&#29615;&#26367;&#20195;&#26041;&#26696;&#65292;&#20854;&#20855;&#26377;&#29420;&#31435;&#20110;&#19978;&#19979;&#25991;&#30340;&#25512;&#26029;&#25104;&#26412;&#24182;&#26377;&#25928;&#22320;&#21033;&#29992;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#19978;&#36848;&#35745;&#31639;&#38480;&#21046;&#20960;&#20046;&#20351;&#24471;transformers&#30340;&#24212;&#29992;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#35786;&#26029;&#29615;&#22659;&#20013;&#37327;&#21270;&#20102;&#25105;&#20204;&#26550;&#26500;&#20013;&#19981;&#21516;&#37096;&#20998;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The self-attention mechanism in the transformer architecture is capable of capturing long-range dependencies and it is the main reason behind its effectiveness in processing sequential data. Nevertheless, despite their success, transformers have two significant drawbacks that still limit their broader applicability: (1) In order to remember past information, the self-attention mechanism requires access to the whole history to be provided as context. (2) The inference cost in transformers is expensive. In this paper we introduce recurrent alternatives to the transformer self-attention mechanism that offer a context-independent inference cost, leverage long-range dependencies effectively, and perform well in practice. We evaluate our approaches in reinforcement learning problems where the aforementioned computational limitations make the application of transformers nearly infeasible. We quantify the impact of the different components of our architecture in a diagnostic environment and as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#38543;&#26426;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#36845;&#20195;&#22797;&#26434;&#24230;&#36798;&#21040;&#20102;&#26368;&#20339;&#24050;&#30693;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.15448</link><description>&lt;p&gt;
&#19968;&#31181;&#21152;&#36895;&#30340;&#19968;&#38454;&#27491;&#21017;&#21160;&#37327;&#19979;&#38477;&#31639;&#27861;&#29992;&#20110;&#38543;&#26426;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
An accelerated first-order regularized momentum descent ascent algorithm for stochastic nonconvex-concave minimax problems. (arXiv:2310.15448v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#38543;&#26426;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#36845;&#20195;&#22797;&#26434;&#24230;&#36798;&#21040;&#20102;&#26368;&#20339;&#24050;&#30693;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#36817;&#24180;&#26469;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#30340;&#19968;&#38454;&#27491;&#21017;&#21160;&#37327;&#19979;&#38477;&#31639;&#27861;&#65288;FORMDA&#65289;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#12290;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$\tilde{\mathcal{O}}(\varepsilon ^{-6.5})$&#20197;&#36798;&#21040;$\varepsilon$-&#31283;&#23450;&#28857;&#65292;&#36825;&#22312;&#30446;&#26631;&#20989;&#25968;&#31283;&#23450;&#24615;&#19979;&#23454;&#29616;&#20102;&#35299;&#20915;&#38543;&#26426;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#26368;&#20339;&#24050;&#30693;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic nonconvex minimax problems have attracted wide attention in machine learning, signal processing and many other fields in recent years. In this paper, we propose an accelerated first-order regularized momentum descent ascent algorithm (FORMDA) for solving stochastic nonconvex-concave minimax problems. The iteration complexity of the algorithm is proved to be $\tilde{\mathcal{O}}(\varepsilon ^{-6.5})$ to obtain an $\varepsilon$-stationary point, which achieves the best-known complexity bound for single-loop algorithms to solve the stochastic nonconvex-concave minimax problems under the stationarity of the objective function.
&lt;/p&gt;</description></item><item><title>GraphMaker&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.13833</link><description>&lt;p&gt;
GraphMaker: &#25193;&#25955;&#27169;&#22411;&#33021;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?. (arXiv:2310.13833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13833
&lt;/p&gt;
&lt;p&gt;
GraphMaker&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20855;&#26377;&#33410;&#28857;&#23646;&#24615;&#30340;&#22823;&#35268;&#27169;&#22270;&#21464;&#24471;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#21019;&#24314;&#19982;&#30495;&#23454;&#19990;&#30028;&#31034;&#20363;&#31867;&#20284;&#30340;&#21512;&#25104;&#12289;&#23500;&#23646;&#24615;&#22270;&#23545;&#20110;&#20849;&#20139;&#22270;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#21644;&#24320;&#21457;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#24403;&#21407;&#22987;&#25968;&#25454;&#38480;&#21046;&#34987;&#20849;&#20139;&#26102;&#12290;&#20256;&#32479;&#30340;&#22270;&#29983;&#25104;&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#20123;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26368;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#27809;&#26377;&#23646;&#24615;&#21644;&#36739;&#23567;&#30340;&#20998;&#23376;&#22270;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#26041;&#38754;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#22797;&#26434;&#30340;&#23646;&#24615;-&#32467;&#26500;&#30456;&#20851;&#24615;&#21644;&#22270;&#30340;&#22823;&#35268;&#27169;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#65306;GraphMaker&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#33410;&#28857;&#23646;&#24615;&#21644;&#22270;&#32467;&#26500;&#29983;&#25104;&#36807;&#31243;&#30340;&#32452;&#21512;&#65292;&#21457;&#29616;&#24322;&#27493;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#20869;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale graphs with node attributes are increasingly common in various real-world applications. Creating synthetic, attribute-rich graphs that mirror real-world examples is crucial, especially for sharing graph data for analysis and developing learning models when original data is restricted to be shared. Traditional graph generation methods are limited in their capacity to handle these complex structures. Recent advances in diffusion models have shown potential in generating graph structures without attributes and smaller molecular graphs. However, these models face challenges in generating large attributed graphs due to the complex attribute-structure correlations and the large size of these graphs. This paper introduces a novel diffusion model, GraphMaker, specifically designed for generating large attributed graphs. We explore various combinations of node attribute and graph structure generation processes, finding that an asynchronous approach more effectively captures the intr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#30340;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#23548;&#21040;&#20010;&#21035;&#32676;&#20307;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#22312;&#22522;&#26412;LLM&#19978;&#21152;&#20837;&#29420;&#31435;&#30340;transformer&#27169;&#22359;&#26469;&#39044;&#27979;&#32676;&#20307;&#20559;&#22909;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;GPO&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11523</link><description>&lt;p&gt;
&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Group Preference Optimization: Few-Shot Alignment of Large Language Models. (arXiv:2310.11523v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11523
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#30340;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#23548;&#21040;&#20010;&#21035;&#32676;&#20307;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#22312;&#22522;&#26412;LLM&#19978;&#21152;&#20837;&#29420;&#31435;&#30340;transformer&#27169;&#22359;&#26469;&#39044;&#27979;&#32676;&#20307;&#20559;&#22909;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;GPO&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35768;&#22810;&#24212;&#29992;&#65292;&#20174;&#32842;&#22825;&#26426;&#22120;&#20154;&#21040;&#21019;&#24847;&#20889;&#20316;&#65292;&#37117;&#38656;&#35201;&#32454;&#33268;&#20837;&#24494;&#30340;&#20027;&#35266;&#21028;&#26029;&#65292;&#36825;&#20123;&#21028;&#26029;&#22312;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#31639;&#27861;&#22312;&#27599;&#20010;&#32676;&#20307;&#19978;&#23545;&#40784;&#30340;&#25104;&#26412;&#24456;&#39640;&#65292;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#32780;&#35328;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#32676;&#20307;&#29305;&#23450;&#20559;&#22909;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23558;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#21040;&#20010;&#21035;&#32676;&#20307;&#30340;&#20559;&#22909;&#12290;&#22312;GPO&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29420;&#31435;&#30340;transformer&#27169;&#22359;&#26469;&#25193;&#20805;&#22522;&#26412;LLM&#65292;&#29992;&#20110;&#39044;&#27979;&#32676;&#20307;&#23545;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20559;&#22909;&#12290;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#27169;&#22359;&#21442;&#25968;&#21270;&#20026;&#19968;&#20010;&#19978;&#19979;&#25991;&#33258;&#22238;&#24402;&#30340;transformer&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#22810;&#20010;&#32676;&#20307;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#20005;&#26684;&#30340;&#35780;&#20272;&#65292;&#20351;&#29992;&#19981;&#21516;&#35268;&#27169;&#30340;LLM&#22312;&#19977;&#20010;&#20154;&#31867;&#24847;&#35265;&#36866;&#24212;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;GPO&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups. Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases. We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. In GPO, we augment the base LLM with an independent transformer module trained to predict the preferences of a group for the LLM generations. For few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups. We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks. These tasks involve adapting to the preferences
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#21508;&#31181;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;&#65292;&#21363;&#36127;&#36131;&#32534;&#30721;&#29305;&#23450;&#30693;&#35782;&#30340;&#31232;&#30095;&#35745;&#31639;&#23376;&#22270;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#21487;&#24494;&#20998;&#26435;&#37325;&#23631;&#34109;&#26041;&#26696;&#65292;&#25105;&#20204;&#21487;&#20197;&#31934;&#30830;&#22320;&#21024;&#38500;&#29305;&#23450;&#30693;&#35782;&#65292;&#21448;&#26368;&#23567;&#21270;&#23545;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.03084</link><description>&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Discovering Knowledge-Critical Subnetworks in Pretrained Language Models. (arXiv:2310.03084v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#21508;&#31181;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;&#65292;&#21363;&#36127;&#36131;&#32534;&#30721;&#29305;&#23450;&#30693;&#35782;&#30340;&#31232;&#30095;&#35745;&#31639;&#23376;&#22270;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#21487;&#24494;&#20998;&#26435;&#37325;&#23631;&#34109;&#26041;&#26696;&#65292;&#25105;&#20204;&#21487;&#20197;&#31934;&#30830;&#22320;&#21024;&#38500;&#29305;&#23450;&#30693;&#35782;&#65292;&#21448;&#26368;&#23567;&#21270;&#23545;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#21442;&#25968;&#20013;&#32534;&#30721;&#20102;&#38544;&#21547;&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#28982;&#32780;&#65292;&#23450;&#20301;&#36825;&#20123;&#34920;&#31034;&#24182;&#23558;&#20854;&#35299;&#31163;&#20986;&#26469;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21253;&#21547;&#20102;&#21508;&#31181;&#20851;&#38190;&#30693;&#35782;&#23376;&#32593;&#32476;&#65306;&#36127;&#36131;&#32534;&#30721;&#27169;&#22411;&#25152;&#35760;&#24518;&#30340;&#29305;&#23450;&#30693;&#35782;&#30340;&#29305;&#23450;&#31232;&#30095;&#35745;&#31639;&#23376;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#21487;&#24494;&#20998;&#26435;&#37325;&#23631;&#34109;&#26041;&#26696;&#26469;&#21457;&#29616;&#36825;&#20123;&#23376;&#32593;&#32476;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#23427;&#20204;&#26469;&#31934;&#30830;&#22320;&#20174;&#27169;&#22411;&#20013;&#21024;&#38500;&#29305;&#23450;&#30693;&#35782;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#21407;&#22987;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;GPT2&#21464;&#20307;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#39640;&#24230;&#31232;&#30095;&#23376;&#32593;&#32476;&#65288;98%+&#65289;&#65292;&#23427;&#20204;&#20165;&#36127;&#36131;&#29305;&#23450;&#30340;&#20851;&#31995;&#30693;&#35782;&#38598;&#21512;&#12290;&#24403;&#21024;&#38500;&#36825;&#20123;&#23376;&#32593;&#32476;&#26102;&#65292;&#21097;&#20313;&#30340;&#32593;&#32476;&#20173;&#20445;&#25345;&#20102;&#22823;&#37096;&#20998;&#20854;&#21021;&#22987;&#23481;&#37327;&#65288;&#23545;&#35821;&#35328;&#21644;&#20854;&#20182;&#35760;&#24518;&#20851;&#31995;&#30340;&#24314;&#27169;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (LMs) encode implicit representations of knowledge in their parameters. However, localizing these representations and disentangling them from each other remains an open problem. In this work, we investigate whether pretrained language models contain various knowledge-critical subnetworks: particular sparse computational subgraphs responsible for encoding specific knowledge the model has memorized. We propose a multi-objective differentiable weight masking scheme to discover these subnetworks and show that we can use them to precisely remove specific knowledge from models while minimizing adverse effects on the behavior of the original language model. We demonstrate our method on multiple GPT2 variants, uncovering highly sparse subnetworks (98%+) that are solely responsible for specific collections of relational knowledge. When these subnetworks are removed, the remaining network maintains most of its initial capacity (modeling language and other memorized rel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23545;&#20915;&#20105;&#22842;&#20013;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#31639;&#27861;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21644;&#26041;&#24046;&#24863;&#30693;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.00968</link><description>&lt;p&gt;
&#38543;&#26426;&#24773;&#22659;&#23545;&#20915;&#20105;&#22842;&#20915;&#31574;&#30340;&#26041;&#24046;&#24863;&#30693;&#36951;&#25022;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Variance-Aware Regret Bounds for Stochastic Contextual Dueling Bandits. (arXiv:2310.00968v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23545;&#20915;&#20105;&#22842;&#20013;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#31639;&#27861;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21644;&#26041;&#24046;&#24863;&#30693;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20915;&#20105;&#22842;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#28041;&#21450;&#21040;&#20559;&#22909;&#21453;&#39304;&#30340;&#20915;&#31574;&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#20154;&#26426;&#20132;&#20114;&#30340;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#30340;&#26377;&#20215;&#20540;&#29305;&#24615;&#65292;&#20363;&#22914;&#25490;&#21517;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#25512;&#33616;&#31995;&#32479;&#12290;&#34429;&#28982;&#22312;&#23545;&#20915;&#20105;&#22842;&#20013;&#24050;&#32463;&#20570;&#20986;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#26469;&#26368;&#23567;&#21270;&#32047;&#35745;&#36951;&#25022;&#65292;&#20294;&#30446;&#21069;&#30740;&#31350;&#20013;&#23384;&#22312;&#19968;&#20010;&#26126;&#26174;&#30340;&#31354;&#30333;&#65292;&#21363;&#36951;&#25022;&#30028;&#38480;&#26410;&#32771;&#34385;&#21040;&#23545;&#20915;&#25163;&#34920;&#38388;&#25104;&#23545;&#27604;&#36739;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#26356;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#24847;&#21619;&#30528;&#38382;&#39064;&#30340;&#38590;&#24230;&#26356;&#39640;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#24773;&#22659;&#23545;&#20915;&#20105;&#22842;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#20915;&#31574;&#25163;&#34920;&#30340;&#20108;&#20803;&#23545;&#27604;&#26159;&#30001;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SupLinUCB&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#36825;&#20010;&#31639;&#27861;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21644;&#19968;&#20010;&#24863;&#30693;&#26041;&#24046;&#36951;&#25022;&#30028;&#38480;$\tilde O\big(d\sqrt{\sum_{t=1}^T\sigma_t^2} + d\big)$&#65292;&#20854;&#20013;$\sigma_t$&#26159;&#27599;&#36718;&#25104;&#23545;&#27604;&#36739;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dueling bandits is a prominent framework for decision-making involving preferential feedback, a valuable feature that fits various applications involving human interaction, such as ranking, information retrieval, and recommendation systems. While substantial efforts have been made to minimize the cumulative regret in dueling bandits, a notable gap in the current research is the absence of regret bounds that account for the inherent uncertainty in pairwise comparisons between the dueling arms. Intuitively, greater uncertainty suggests a higher level of difficulty in the problem. To bridge this gap, this paper studies the problem of contextual dueling bandits, where the binary comparison of dueling arms is generated from a generalized linear model (GLM). We propose a new SupLinUCB-type algorithm that enjoys computational efficiency and a variance-aware regret bound $\tilde O\big(d\sqrt{\sum_{t=1}^T\sigma_t^2} + d\big)$, where $\sigma_t$ is the variance of the pairwise comparison in round
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.07601</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#20449;&#21495;&#21644;&#24369;&#30417;&#30563;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision. (arXiv:2309.07601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#24230;&#20449;&#21495;&#20195;&#34920;&#20102;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#21592;&#36890;&#24120;&#29992;&#26469;&#35780;&#20272;&#22312;&#32447;&#20869;&#23481;&#30495;&#23454;&#24615;&#30340;&#19968;&#31995;&#21015;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#21270;&#21487;&#20449;&#24230;&#20449;&#21495;&#25552;&#21462;&#30340;&#20219;&#21153;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#35757;&#32451;&#39640;&#20934;&#30830;&#29575;&#30340;&#29305;&#23450;&#20449;&#21495;&#25552;&#21462;&#22120;&#65292;&#32780;&#30446;&#21069;&#27809;&#26377;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#23545;&#25152;&#26377;&#21487;&#20449;&#24230;&#20449;&#21495;&#36827;&#34892;&#27880;&#37322;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#19968;&#32452;18&#20010;&#21487;&#20449;&#24230;&#20449;&#21495;&#26469;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20197;&#20135;&#29983;&#27599;&#20010;&#20449;&#21495;&#30340;&#24369;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#23545;&#36825;&#20123;&#28508;&#22312;&#30340;&#22122;&#22768;&#26631;&#31614;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#39044;&#27979;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#32467;&#21512;&#20102;&#38646;-shot LLM&#21487;&#20449;&#24230;&#20449;&#21495;&#26631;&#27880;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#65292;&#32780;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#35757;&#32451;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Credibility signals represent a wide range of heuristics that are typically used by journalists and fact-checkers to assess the veracity of online content. Automating the task of credibility signal extraction, however, is very challenging as it requires high-accuracy signal-specific extractors to be trained, while there are currently no sufficiently large datasets annotated with all credibility signals. This paper investigates whether large language models (LLMs) can be prompted effectively with a set of 18 credibility signals to produce weak labels for each signal. We then aggregate these potentially noisy labels using weak supervision in order to predict content veracity. We demonstrate that our approach, which combines zero-shot LLM credibility signal labeling and weak supervision, outperforms state-of-the-art classifiers on two misinformation datasets without using any ground-truth labels for training. We also analyse the contribution of the individual credibility signals towards p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#27169;&#22411;&#20013;&#30340;mesa-optimization&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20869;&#37096;&#23398;&#20064;&#30446;&#26631;&#21644;&#30456;&#24212;&#30340;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#39537;&#21160;&#39044;&#27979;&#29983;&#25104;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36825;&#31181;&#23398;&#20064;&#30340;&#20248;&#21270;&#31639;&#27861;&#21487;&#20197;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#30417;&#30563;&#24335;&#23569;&#26679;&#26412;&#20219;&#21153;&#65292;&#26263;&#31034;&#20102;mesa-optimization&#21487;&#33021;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2309.05858</link><description>&lt;p&gt;
&#25581;&#31034;Transformer&#20013;&#30340;mesa-optimization&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Uncovering mesa-optimization algorithms in Transformers. (arXiv:2309.05858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#27169;&#22411;&#20013;&#30340;mesa-optimization&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20869;&#37096;&#23398;&#20064;&#30446;&#26631;&#21644;&#30456;&#24212;&#30340;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#39537;&#21160;&#39044;&#27979;&#29983;&#25104;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36825;&#31181;&#23398;&#20064;&#30340;&#20248;&#21270;&#31639;&#27861;&#21487;&#20197;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#30417;&#30563;&#24335;&#23569;&#26679;&#26412;&#20219;&#21153;&#65292;&#26263;&#31034;&#20102;mesa-optimization&#21487;&#33021;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#20013;&#20027;&#23548;&#30340;&#27169;&#22411;&#65292;&#20294;&#20854;&#21331;&#36234;&#24615;&#33021;&#30340;&#21407;&#22240;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#20551;&#35774;Transformer&#30340;&#24378;&#22823;&#24615;&#33021;&#28304;&#20110;&#20854;&#26550;&#26500;&#20013;&#23545;mesa-optimization&#30340;&#20559;&#22909;&#65292;&#21363;&#19968;&#20010;&#23398;&#20064;&#36807;&#31243;&#22312;&#27169;&#22411;&#30340;&#21069;&#21521;&#20256;&#36882;&#20013;&#36816;&#34892;&#65292;&#30001;&#20197;&#19979;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#65306;&#65288;i&#65289;&#26500;&#24314;&#20869;&#37096;&#23398;&#20064;&#30446;&#26631;&#65292;&#21644;&#65288;ii&#65289;&#36890;&#36807;&#20248;&#21270;&#25214;&#21040;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#22312;&#31616;&#21333;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;Transformer&#36827;&#34892;&#20102;&#36870;&#21521;&#24037;&#31243;&#65292;&#25581;&#31034;&#20102;&#39537;&#21160;&#39044;&#27979;&#29983;&#25104;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24213;&#23618;mesa-optimization&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#21069;&#21521;&#20256;&#36882;&#20248;&#21270;&#31639;&#27861;&#21487;&#20197;&#31435;&#21363;&#34987;&#37325;&#26032;&#24212;&#29992;&#20110;&#35299;&#20915;&#30417;&#30563;&#24335;&#23569;&#26679;&#26412;&#20219;&#21153;&#65292;&#36825;&#34920;&#26126;mesa-optimization&#21487;&#33021;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#30340;&#22522;&#30784;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have become the dominant model in deep learning, but the reason for their superior performance is poorly understood. Here, we hypothesize that the strong performance of Transformers stems from an architectural bias towards mesa-optimization, a learned process running within the forward pass of a model consisting of the following two steps: (i) the construction of an internal learning objective, and (ii) its corresponding solution found through optimization. To test this hypothesis, we reverse-engineer a series of autoregressive Transformers trained on simple sequence modeling tasks, uncovering underlying gradient-based mesa-optimization algorithms driving the generation of predictions. Moreover, we show that the learned forward-pass optimization algorithm can be immediately repurposed to solve supervised few-shot tasks, suggesting that mesa-optimization might underlie the in-context learning capabilities of large language models. Finally, we propose a novel self-attention 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CASSLE&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20462;&#25913;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26377;&#21521;&#25237;&#24433;&#32593;&#32476;&#65292;&#21033;&#29992;&#22686;&#24378;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06082</link><description>&lt;p&gt;
&#22686;&#24378;&#24863;&#30693;&#30340;&#26377;&#21521;&#25237;&#24433;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Augmentation-aware Self-supervised Learning with Guided Projector. (arXiv:2306.06082v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CASSLE&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20462;&#25913;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26377;&#21521;&#25237;&#24433;&#32593;&#32476;&#65292;&#21033;&#29992;&#22686;&#24378;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#20581;&#22766;&#34920;&#31034;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;SimCLR&#21644;MoCo&#31561;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#23545;&#24212;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#20445;&#25345;&#19981;&#21464;&#65292;&#33021;&#22815;&#36798;&#21040;&#19982;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#19981;&#21464;&#24615;&#21487;&#33021;&#23545;&#35299;&#20915;&#26576;&#20123;&#19979;&#28216;&#20219;&#21153;&#26377;&#23475;&#65292;&#36825;&#20123;&#20219;&#21153;&#20381;&#36182;&#20110;&#21463;&#21040;&#39044;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30340;&#22686;&#24378;&#24433;&#21709;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#39068;&#33394;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20462;&#25913;&#33258;&#30417;&#30563;&#26550;&#26500;&#30340;&#24120;&#35265;&#32452;&#20214;&#20043;&#19968;&#30340;&#26377;&#21521;&#25237;&#24433;&#32593;&#32476;&#65292;&#26469;&#20419;&#36827;&#34920;&#31034;&#31354;&#38388;&#23545;&#36825;&#20123;&#29305;&#24449;&#30340;&#25935;&#24863;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;&#25237;&#24433;&#22120;&#34917;&#20805;&#26377;&#20851;&#24212;&#29992;&#20110;&#22270;&#20687;&#30340;&#22686;&#24378;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35753;&#25237;&#24433;&#22120;&#22312;&#35299;&#20915;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#26102;&#21033;&#29992;&#36825;&#31181;&#36741;&#21161;&#25351;&#23548;&#65292;&#29305;&#24449;&#25552;&#21462;&#22120;&#23398;&#20064;&#22312;&#20854;&#34920;&#31034;&#20013;&#20445;&#30041;&#22686;&#24378;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#26377;&#21521;&#25237;&#24433;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;CASSLE&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) is a powerful technique for learning robust representations from unlabeled data. By learning to remain invariant to applied data augmentations, methods such as SimCLR and MoCo are able to reach quality on par with supervised approaches. However, this invariance may be harmful to solving some downstream tasks which depend on traits affected by augmentations used during pretraining, such as color. In this paper, we propose to foster sensitivity to such characteristics in the representation space by modifying the projector network, a common component of self-supervised architectures. Specifically, we supplement the projector with information about augmentations applied to images. In order for the projector to take advantage of this auxiliary guidance when solving the SSL task, the feature extractor learns to preserve the augmentation information in its representations. Our approach, coined Conditional Augmentation-aware Selfsupervised Learning (CASSLE), is d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;(RFD)&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#35745;&#31639;&#20986;&#27493;&#38271;&#24182;&#19988;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30456;&#21516;&#12290;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;RFD&#31639;&#27861;&#27604;&#26410;&#35843;&#25972;&#30340;Adam&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#25552;&#20986;&#30340;heuristic&#25193;&#23637;&#21487;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2305.01377</link><description>&lt;p&gt;
&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;&#27861;
&lt;/p&gt;
&lt;p&gt;
Random Function Descent. (arXiv:2305.01377v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;(RFD)&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#35745;&#31639;&#20986;&#27493;&#38271;&#24182;&#19988;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30456;&#21516;&#12290;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;RFD&#31639;&#27861;&#27604;&#26410;&#35843;&#25972;&#30340;Adam&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#25552;&#20986;&#30340;heuristic&#25193;&#23637;&#21487;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21313;&#20998;&#24120;&#35265;&#65292;&#20294;&#26159;&#36873;&#25321;&#27491;&#30830;&#30340;&#27493;&#38271;&#32463;&#24120;&#38656;&#35201;&#36827;&#34892;&#8220;&#36229;&#21442;&#25968;&#35843;&#25972;&#8221;&#12290;&#36825;&#26159;&#22240;&#20026;&#22238;&#28335;&#31243;&#24207;&#22914;Armijo's&#20934;&#21017;&#20381;&#36182;&#20110;&#27599;&#20010;&#27493;&#39588;&#20013;&#30340;&#36136;&#37327;&#35780;&#20272;&#65292;&#32780;&#36825;&#20123;&#35780;&#20272;&#22312;&#38543;&#26426;&#24773;&#20917;&#19979;&#19981;&#21487;&#29992;&#12290;&#30001;&#20110;&#20248;&#21270;&#26041;&#26696;&#21487;&#20197;&#29992;Taylor&#36924;&#36817;&#26469;&#35299;&#37322;&#65292;&#25105;&#20204;&#23558;Taylor&#36924;&#36817;&#26367;&#25442;&#20026;&#26465;&#20214;&#26399;&#26395;&#65288;&#26368;&#20339;&#30340;$L^2$&#20272;&#35745;&#65289;&#65292;&#25552;&#20986;&#20102;&#8220;&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;&#8221;&#65288;RFD&#65289;&#12290; &#22312;Bayesian&#20248;&#21270;&#20013;&#24120;&#35265;&#30340;&#19968;&#20123;&#36731;&#24494;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RFD&#19982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26159;&#30456;&#21516;&#30340;&#65292;&#20294;&#26159;&#22312;&#38543;&#26426;&#24773;&#20917;&#19979;&#20855;&#26377;&#21487;&#35745;&#31639;&#30340;&#27493;&#38271;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#27604;&#26410;&#35843;&#25972;&#30340;Adam&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#20026;&#20102;&#32553;&#23567;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#31639;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#25193;&#23637;&#65292;&#21487;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
While gradient based methods are ubiquitous in machine learning, selecting the right step size often requires "hyperparameter tuning". This is because backtracking procedures like Armijo's rule depend on quality evaluations in every step, which are not available in a stochastic context. Since optimization schemes can be motivated using Taylor approximations, we replace the Taylor approximation with the conditional expectation (the best $L^2$ estimator) and propose "Random Function Descent" (RFD). Under light assumptions common in Bayesian optimization, we prove that RFD is identical to gradient descent, but with calculable step sizes, even in a stochastic context. We beat untuned Adam in synthetic benchmarks. To close the performance gap to tuned Adam, we propose a heuristic extension competitive with tuned Adam.
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26045;&#21152;&#28966;&#34385;&#33021;&#24433;&#21709;&#23427;&#20204;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;&#65292;&#36825;&#38656;&#35201;&#26356;&#22810;&#36947;&#24503;&#32771;&#34385;&#21644;&#30417;&#31649;&#12290;</title><link>http://arxiv.org/abs/2304.11111</link><description>&lt;p&gt;
&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28966;&#34385;&#20250;&#22686;&#21152;&#23427;&#20204;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Inducing anxiety in large language models increases exploration and bias. (arXiv:2304.11111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11111
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26045;&#21152;&#28966;&#34385;&#33021;&#24433;&#21709;&#23427;&#20204;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;&#65292;&#36825;&#38656;&#35201;&#26356;&#22810;&#36947;&#24503;&#32771;&#34385;&#21644;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#25913;&#21464;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#65292;&#24341;&#21457;&#20844;&#20247;&#30340;&#36777;&#35770;&#12290;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#19981;&#20165;&#20309;&#26102;&#33021;&#22815;&#27491;&#24120;&#24037;&#20316;&#21644;&#25104;&#21151;&#65292;&#20063;&#20026;&#20160;&#20040;&#20250;&#22833;&#36133;&#21644;&#34892;&#20026;&#22833;&#24120;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#31038;&#20250;&#24847;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#35745;&#31639;&#31934;&#31070;&#30149;&#23398;&#30340;&#35270;&#35282;&#36716;&#21521;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;Generative Pre-Trained Transformer 3.5&#65292;&#24182;&#23558;&#20854;&#32622;&#20110;&#31934;&#31070;&#30149;&#23398;&#20013;&#24120;&#35265;&#30340;&#20219;&#21153;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-3.5&#23545;&#24120;&#35265;&#30340;&#28966;&#34385;&#38382;&#21367;&#20570;&#20986;&#26377;&#21147;&#30340;&#21453;&#24212;&#65292;&#20135;&#29983;&#27604;&#20154;&#31867;&#20027;&#20307;&#26356;&#39640;&#30340;&#28966;&#34385;&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#24773;&#32490;&#24863;&#24212;&#25552;&#31034;&#21487;&#20197;&#21487;&#39044;&#27979;&#22320;&#25913;&#21464;GPT-3.5&#30340;&#21453;&#24212;&#12290;&#24773;&#24863;&#24863;&#24212;&#19981;&#20165;&#24433;&#21709;GPT-3.5&#22312;&#34913;&#37327;&#25506;&#32034;&#20915;&#31574;-making&#30340;&#35748;&#30693;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65292;&#36824;&#24433;&#21709;&#20854;&#22312;&#20043;&#21069;&#24314;&#31435;&#30340;&#34913;&#37327;&#31181;&#26063;&#20027;&#20041;&#21644;&#22833;&#33021;&#20027;&#20041;&#31561;&#20559;&#35265;&#30340;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;GPT-3.5&#22312;&#21463;&#21040;&#28966;&#34385;&#35825;&#23548;&#26102;&#21576;&#29616;&#20986;&#26126;&#26174;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;&#22686;&#21152;&#65292;&#34920;&#26126;&#20854;&#36755;&#20986;&#23481;&#26131;&#21463;&#21040;&#24773;&#24863;&#25805;&#32437;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#20351;&#29992;&#36807;&#31243;&#20013;&#38656;&#35201;&#26356;&#22810;&#30340;&#36947;&#24503;&#32771;&#34385;&#21644;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are transforming research on machine learning while galvanizing public debates. Understanding not only when these models work well and succeed but also why they fail and misbehave is of great societal relevance. We propose to turn the lens of computational psychiatry, a framework used to computationally describe and modify aberrant behavior, to the outputs produced by these models. We focus on the Generative Pre-Trained Transformer 3.5 and subject it to tasks commonly studied in psychiatry. Our results show that GPT-3.5 responds robustly to a common anxiety questionnaire, producing higher anxiety scores than human subjects. Moreover, GPT-3.5's responses can be predictably changed by using emotion-inducing prompts. Emotion-induction not only influences GPT-3.5's behavior in a cognitive task measuring exploratory decision-making but also influences its behavior in a previously-established task measuring biases such as racism and ableism. Crucially, GPT-3.5 shows a s
&lt;/p&gt;</description></item></channel></rss>