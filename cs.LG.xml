<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25913;&#36827;&#20102;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#38750;&#23618;&#27425;Mamba&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#36830;&#32493;2D&#25195;&#25551;&#36807;&#31243;&#21644;&#26041;&#21521;&#24863;&#30693;&#26356;&#26032;&#65292;&#25552;&#39640;&#20102;&#20174;&#20108;&#32500;&#22270;&#20687;&#20013;&#23398;&#20064;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.17695</link><description>&lt;p&gt;
PlainMamba&#65306;&#25913;&#36827;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#38750;&#23618;&#27425;Mamba
&lt;/p&gt;
&lt;p&gt;
PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17695
&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#20102;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#38750;&#23618;&#27425;Mamba&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#36830;&#32493;2D&#25195;&#25551;&#36807;&#31243;&#21644;&#26041;&#21521;&#24863;&#30693;&#26356;&#26032;&#65292;&#25552;&#39640;&#20102;&#20174;&#20108;&#32500;&#22270;&#20687;&#20013;&#23398;&#20064;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;PlainMamba&#65306;&#19968;&#31181;&#31616;&#21333;&#30340;&#38750;&#23618;&#27425;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#65292;&#26088;&#22312;&#29992;&#20110;&#19968;&#33324;&#30340;&#35270;&#35273;&#35782;&#21035;&#12290;&#26368;&#36817;&#30340;Mamba&#27169;&#22411;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#39034;&#24207;&#25968;&#25454;&#19978;SSM&#21487;&#20197;&#19982;&#20854;&#20182;&#26550;&#26500;&#31454;&#20105;&#28608;&#28872;&#65292;&#24182;&#24050;&#21021;&#27493;&#23581;&#35797;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#20687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;Mamba&#30340;&#36873;&#25321;&#24615;&#25195;&#25551;&#36807;&#31243;&#20197;&#36866;&#24212;&#35270;&#35273;&#39046;&#22495;&#65292;&#36890;&#36807;&#65288;i&#65289;&#36890;&#36807;&#30830;&#20445;&#22312;&#25195;&#25551;&#24207;&#21015;&#20013;&#20196;&#29260;&#30456;&#37051;&#26469;&#25913;&#21892;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#36830;&#32493;2D&#25195;&#25551;&#36807;&#31243;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#21551;&#29992;&#27169;&#22411;&#21306;&#20998;&#20196;&#29260;&#30340;&#31354;&#38388;&#20851;&#31995;&#30340;&#26041;&#21521;&#24863;&#30693;&#26356;&#26032;&#65292;&#36890;&#36807;&#32534;&#30721;&#26041;&#21521;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#35774;&#35745;&#26131;&#20110;&#20351;&#29992;&#21644;&#26131;&#20110;&#25193;&#23637;&#65292;&#30001;&#22534;&#21472;&#30456;&#21516;&#30340;PlainMamba&#22359;&#24418;&#25104;&#65292;&#32467;&#26524;&#26159;&#22987;&#32456;&#20855;&#26377;&#24658;&#23450;&#23485;&#24230;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17695v1 Announce Type: cross  Abstract: We present PlainMamba: a simple non-hierarchical state space model (SSM) designed for general visual recognition. The recent Mamba model has shown how SSMs can be highly competitive with other architectures on sequential data and initial attempts have been made to apply it to images. In this paper, we further adapt the selective scanning process of Mamba to the visual domain, enhancing its ability to learn features from two-dimensional images by (i) a continuous 2D scanning process that improves spatial continuity by ensuring adjacency of tokens in the scanning sequence, and (ii) direction-aware updating which enables the model to discern the spatial relations of tokens by encoding directional information. Our architecture is designed to be easy to use and easy to scale, formed by stacking identical PlainMamba blocks, resulting in a model with constant width throughout all layers. The architecture is further simplified by removing the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#25490;&#21517;&#20998;&#31867;&#31995;&#32479;&#65292;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#23436;&#25972;&#24615;&#12289;&#24320;&#25918;&#24615;&#20197;&#21450;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#21407;&#21017;&#65292;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#35782;&#21035;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13784</link><description>&lt;p&gt;
&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;: &#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#21487;&#37325;&#29616;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#29992;&#24615;&#30340;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13784
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#25490;&#21517;&#20998;&#31867;&#31995;&#32479;&#65292;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#23436;&#25972;&#24615;&#12289;&#24320;&#25918;&#24615;&#20197;&#21450;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#21407;&#21017;&#65292;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#35782;&#21035;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#20854;&#21830;&#19994;&#21270;&#24341;&#21457;&#20102;&#20851;&#20110;&#36879;&#26126;&#24230;&#12289;&#21487;&#37325;&#29616;&#24615;&#12289;&#20559;&#35265;&#21644;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#12290;&#35768;&#22810;"&#24320;&#28304;"&#30340;GAI&#27169;&#22411;&#32570;&#20047;&#23436;&#25972;&#29702;&#35299;&#21644;&#20877;&#29616;&#25152;&#24517;&#38656;&#30340;&#32452;&#20214;&#65292;&#19968;&#20123;&#37319;&#29992;&#38480;&#21046;&#24615;&#35768;&#21487;&#35777;&#65292;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;"&#24320;&#28304;&#27927;&#30333;"&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;&#20998;&#31867;&#30340;&#31995;&#32479;&#65292;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#25968;&#25454;&#21644;&#24320;&#25918;&#33719;&#21462;&#30340;&#21407;&#21017;&#12290;MOF&#35201;&#27714;&#27169;&#22411;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#29305;&#23450;&#32452;&#20214;&#34987;&#21253;&#21547;&#24182;&#26681;&#25454;&#36866;&#24403;&#30340;&#24320;&#25918;&#35768;&#21487;&#35777;&#21457;&#24067;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#38450;&#27490;&#23459;&#31216;&#33258;&#24049;&#26159;&#24320;&#25918;&#30340;&#27169;&#22411;&#34987;&#35823;&#35299;&#65292;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#20197;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#21457;&#24067;&#25152;&#26377;&#27169;&#22411;&#32452;&#20214;&#65292;&#24182;&#24110;&#21161;&#20844;&#21496;&#12289;&#23398;&#26415;&#30028;&#21644;&#29233;&#22909;&#32773;&#35782;&#21035;&#21487;&#20197;&#23433;&#20840;&#37319;&#29992;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13784v1 Announce Type: new  Abstract: Generative AI (GAI) offers unprecedented possibilities but its commercialization has raised concerns about transparency, reproducibility, bias, and safety. Many "open-source" GAI models lack the necessary components for full understanding and reproduction, and some use restrictive licenses, a practice known as "openwashing." We propose the Model Openness Framework (MOF), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access. The MOF requires specific components of the model development lifecycle to be included and released under appropriate open licenses. This framework aims to prevent misrepresentation of models claiming to be open, guide researchers and developers in providing all model components under permissive licenses, and help companies, academia, and hobbyists identify models that can be safely adop
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#35760;&#24518;&#22686;&#24378;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;STRIPE&#65289;&#29992;&#20110;&#21160;&#24577;&#22270;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#38376;&#25511;&#26102;&#38388;&#21367;&#31215;&#23618;&#26469;&#25552;&#21462;&#31354;&#38388;&#29305;&#24449;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.09039</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#35760;&#24518;&#22686;&#24378;&#22270;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#21160;&#24577;&#22270;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly Detection in Dynamic Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09039
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#35760;&#24518;&#22686;&#24378;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;STRIPE&#65289;&#29992;&#20110;&#21160;&#24577;&#22270;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#38376;&#25511;&#26102;&#38388;&#21367;&#31215;&#23618;&#26469;&#25552;&#21462;&#31354;&#38388;&#29305;&#24449;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#38754;&#20020;&#36739;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#22270;&#32467;&#26500;&#21644;&#23646;&#24615;&#30340;&#26102;&#38388;&#28436;&#21464;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;-&#26102;&#38388;&#35760;&#24518;&#22686;&#24378;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;STRIPE&#65289;&#12290;STRIPE&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#38376;&#25511;&#26102;&#38388;&#21367;&#31215;&#23618;&#20998;&#21035;&#25552;&#21462;&#31354;&#38388;&#29305;&#24449;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09039v1 Announce Type: cross  Abstract: Anomaly detection in dynamic graphs presents a significant challenge due to the temporal evolution of graph structures and attributes. The conventional approaches that tackle this problem typically employ an unsupervised learning framework, capturing normality patterns with exclusive normal data during training and identifying deviations as anomalies during testing. However, these methods face critical drawbacks: they either only depend on proxy tasks for general representation without directly pinpointing normal patterns, or they neglect to differentiate between spatial and temporal normality patterns, leading to diminished efficacy in anomaly detection. To address these challenges, we introduce a novel Spatial-Temporal memories-enhanced graph autoencoder (STRIPE). Initially, STRIPE employs Graph Neural Networks (GNNs) and gated temporal convolution layers to extract spatial features and temporal features, respectively. Then STRIPE in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;&#65292;&#22312;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#26102;&#37319;&#29992;&#25913;&#36827;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.07818</link><description>&lt;p&gt;
&#26631;&#31614;&#20002;&#22833;&#29575;&#65306;&#21033;&#29992;&#20855;&#26377;&#22495;&#36716;&#31227;&#21644;&#37096;&#20998;&#26631;&#35760;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Label Dropout: Improved Deep Learning Echocardiography Segmentation Using Multiple Datasets With Domain Shift and Partial Labelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;&#65292;&#22312;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#26102;&#37319;&#29992;&#25913;&#36827;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;&#24515;&#21160;&#22270;&#65288;&#36229;&#22768;&#65289;&#26159;&#35780;&#20272;&#24515;&#33039;&#21151;&#33021;&#26102;&#20351;&#29992;&#30340;&#31532;&#19968;&#31181;&#25104;&#20687;&#26041;&#24335;&#12290;&#20174;&#36229;&#22768;&#20013;&#27979;&#37327;&#21151;&#33021;&#29983;&#29289;&#26631;&#24535;&#29289;&#20381;&#36182;&#20110;&#23545;&#24515;&#33039;&#32467;&#26500;&#36827;&#34892;&#20998;&#21106;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#25552;&#20986;&#26469;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23558;&#36825;&#20123;&#24037;&#20855;&#36716;&#21270;&#20026;&#24191;&#27867;&#30340;&#20020;&#24202;&#24212;&#29992;&#65292;&#37325;&#35201;&#30340;&#26159;&#20998;&#21106;&#27169;&#22411;&#23545;&#21508;&#31181;&#22270;&#20687;&#20855;&#26377;&#40065;&#26834;&#24615;&#65288;&#20363;&#22914;&#65292;&#30001;&#19981;&#21516;&#25195;&#25551;&#20202;&#33719;&#24471;&#65292;&#30001;&#19981;&#21516;&#32423;&#21035;&#30340;&#19987;&#23478;&#25805;&#20316;&#21592;&#33719;&#24471;&#31561;&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#40065;&#26834;&#24615;&#27700;&#24179;&#65292;&#26377;&#24517;&#35201;&#20351;&#29992;&#22810;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#26102;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#26631;&#31614;&#23384;&#22312;&#30340;&#21464;&#21270;&#65292;&#21363;&#21512;&#24182;&#25968;&#25454;&#36890;&#24120;&#26159;&#37096;&#20998;&#26631;&#35760;&#30340;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#30340;&#25913;&#36827;&#26469;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35757;&#32451;&#30340;naively
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07818v1 Announce Type: cross  Abstract: Echocardiography (echo) is the first imaging modality used when assessing cardiac function. The measurement of functional biomarkers from echo relies upon the segmentation of cardiac structures and deep learning models have been proposed to automate the segmentation process. However, in order to translate these tools to widespread clinical use it is important that the segmentation models are robust to a wide variety of images (e.g. acquired from different scanners, by operators with different levels of expertise etc.). To achieve this level of robustness it is necessary that the models are trained with multiple diverse datasets. A significant challenge faced when training with multiple diverse datasets is the variation in label presence, i.e. the combined data are often partially-labelled. Adaptations of the cross entropy loss function have been proposed to deal with partially labelled data. In this paper we show that training naively 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22823;&#25968;&#25454;&#20013;&#23398;&#20064;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#19982;&#36923;&#36753;&#22238;&#24402;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#35745;&#31639;&#30340;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00680</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#27169;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scalable Learning of Item Response Theory Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00680
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22823;&#25968;&#25454;&#20013;&#23398;&#20064;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#19982;&#36923;&#36753;&#22238;&#24402;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#35745;&#31639;&#30340;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#65288;IRT&#65289;&#27169;&#22411;&#26088;&#22312;&#35780;&#20272; $n$ &#21517;&#32771;&#29983;&#30340;&#28508;&#22312;&#33021;&#21147;&#20197;&#21450; $m$ &#20010;&#27979;&#39564;&#39033;&#30446;&#30340;&#38544;&#21547;&#38590;&#24230;&#29305;&#24449;&#65292;&#36825;&#20123;&#39033;&#30446;&#26159;&#20174;&#34920;&#26126;&#20854;&#23545;&#24212;&#31572;&#26696;&#36136;&#37327;&#30340;&#20998;&#31867;&#25968;&#25454;&#20013;&#24471;&#20986;&#30340;&#12290;&#20256;&#32479;&#30340;&#24515;&#29702;&#27979;&#37327;&#35780;&#20272;&#22522;&#20110;&#30456;&#23545;&#36739;&#23569;&#30340;&#32771;&#29983;&#21644;&#39033;&#30446;&#65292;&#20363;&#22914;&#19968;&#20010;&#30001; $200$ &#21517;&#23398;&#29983;&#35299;&#20915;&#21253;&#21547; $10$ &#36947;&#39064;&#30446;&#30340;&#32771;&#35797;&#30340;&#29677;&#32423;&#12290;&#32780;&#36817;&#24180;&#26469;&#30340;&#20840;&#29699;&#22823;&#35268;&#27169;&#35780;&#20272;&#65292;&#22914;PISA&#65292;&#25110;&#20114;&#32852;&#32593;&#30740;&#31350;&#65292;&#21487;&#33021;&#23548;&#33268;&#21442;&#19982;&#32773;&#25968;&#37327;&#26174;&#33879;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#31639;&#27861;&#25198;&#28436;&#32771;&#29983;&#35282;&#33394;&#65292;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#25198;&#28436;&#39033;&#30446;&#35282;&#33394;&#65292;$n$ &#21644; $m$ &#37117;&#21487;&#33021;&#21464;&#24471;&#38750;&#24120;&#22823;&#65292;&#25361;&#25112;&#35745;&#31639;&#30340;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#20026;&#20102;&#20174;&#22823;&#25968;&#25454;&#20013;&#23398;&#20064;IRT&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#19982;&#36923;&#36753;&#22238;&#24402;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21518;&#32773;&#21487;&#20197;&#20351;&#29992;s&#20934;&#30830;&#22320;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00680v1 Announce Type: new  Abstract: Item Response Theory (IRT) models aim to assess latent abilities of $n$ examinees along with latent difficulty characteristics of $m$ test items from categorical data that indicates the quality of their corresponding answers. Classical psychometric assessments are based on a relatively small number of examinees and items, say a class of $200$ students solving an exam comprising $10$ problems. More recent global large scale assessments such as PISA, or internet studies, may lead to significantly increased numbers of participants. Additionally, in the context of Machine Learning where algorithms take the role of examinees and data analysis problems take the role of items, both $n$ and $m$ may become very large, challenging the efficiency and scalability of computations. To learn the latent variables in IRT models from large data, we leverage the similarity of these models to logistic regression, which can be approximated accurately using s
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#26862;&#26519;&#35757;&#32451;&#20013;&#27809;&#26377;&#37319;&#29992;&#33258;&#20030;&#32858;&#21512;&#20294;&#20855;&#26377;&#29305;&#24449;&#38543;&#26426;&#21270;&#30340;&#27169;&#22411;&#23481;&#26131;&#34987;&#23436;&#20840;&#37325;&#24314;&#65292;&#21363;&#20351;&#37319;&#29992;&#33258;&#20030;&#32858;&#21512;&#65292;&#22823;&#37096;&#20998;&#25968;&#25454;&#20063;&#21487;&#20197;&#34987;&#37325;&#24314;&#12290;</title><link>https://arxiv.org/abs/2402.19232</link><description>&lt;p&gt;
&#35757;&#32451;&#30340;&#38543;&#26426;&#26862;&#26519;&#23436;&#20840;&#25581;&#31034;&#24744;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Trained Random Forests Completely Reveal your Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19232
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#35757;&#32451;&#20013;&#27809;&#26377;&#37319;&#29992;&#33258;&#20030;&#32858;&#21512;&#20294;&#20855;&#26377;&#29305;&#24449;&#38543;&#26426;&#21270;&#30340;&#27169;&#22411;&#23481;&#26131;&#34987;&#23436;&#20840;&#37325;&#24314;&#65292;&#21363;&#20351;&#37319;&#29992;&#33258;&#20030;&#32858;&#21512;&#65292;&#22823;&#37096;&#20998;&#25968;&#25454;&#20063;&#21487;&#20197;&#34987;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#37325;&#24314;&#25915;&#20987;&#65292;&#33021;&#22815;&#23436;&#20840;&#25110;&#20960;&#20046;&#23436;&#20840;&#37325;&#24314;&#29992;&#20110;&#35757;&#32451;&#38543;&#26426;&#26862;&#26519;&#30340;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#24120;&#29992;&#24211;&#65288;&#22914;scikit-learn&#65289;&#20013;&#38543;&#22788;&#21487;&#24471;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23558;&#37325;&#24314;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#32452;&#21512;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#20284;&#28982;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#38382;&#39064;&#26159;NP&#38590;&#38382;&#39064;&#65292;&#20294;&#21487;&#20197;&#21033;&#29992;&#32422;&#26463;&#32534;&#31243;&#22312;&#35268;&#27169;&#19978;&#35299;&#20915; &#8212;&#8212; &#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#20256;&#25773;&#21644;&#35299;&#22495;&#32553;&#20943;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35745;&#31639;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#27809;&#26377;&#37319;&#29992;&#33258;&#20030;&#32858;&#21512;&#20294;&#20855;&#26377;&#29305;&#24449;&#38543;&#26426;&#21270;&#30340;&#38543;&#26426;&#26862;&#26519;&#23481;&#26131;&#34987;&#23436;&#20840;&#37325;&#24314;&#12290;&#21363;&#20351;&#20351;&#29992;&#23569;&#37327;&#26641;&#65292;&#36825;&#20173;&#28982;&#25104;&#31435;&#12290;&#21363;&#20351;&#36890;&#36807;&#33258;&#20030;&#32858;&#21512;&#65292;&#22823;&#37096;&#20998;&#25968;&#25454;&#20063;&#21487;&#20197;&#34987;&#37325;&#24314;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#19968;&#31181;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19232v1 Announce Type: new  Abstract: We introduce an optimization-based reconstruction attack capable of completely or near-completely reconstructing a dataset utilized for training a random forest. Notably, our approach relies solely on information readily available in commonly used libraries such as scikit-learn. To achieve this, we formulate the reconstruction problem as a combinatorial problem under a maximum likelihood objective. We demonstrate that this problem is NP-hard, though solvable at scale using constraint programming -- an approach rooted in constraint propagation and solution-domain reduction. Through an extensive computational investigation, we demonstrate that random forests trained without bootstrap aggregation but with feature randomization are susceptible to a complete reconstruction. This holds true even with a small number of trees. Even with bootstrap aggregation, the majority of the data can also be reconstructed. These findings underscore a critica
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#24378;&#22823;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#22768;&#38899;&#29983;&#29289;&#26631;&#24535;&#29289;&#26469;&#39044;&#27979;&#20303;&#38498;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13812</link><description>&lt;p&gt;
&#22522;&#20110;&#22768;&#38899;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22686;&#24378;&#30340;&#20303;&#38498;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#27515;&#20129;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Voice-Driven Mortality Prediction in Hospitalized Heart Failure Patients: A Machine Learning Approach Enhanced with Diagnostic Biomarkers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13812
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#24378;&#22823;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#22768;&#38899;&#29983;&#29289;&#26631;&#24535;&#29289;&#26469;&#39044;&#27979;&#20303;&#38498;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#23545;&#24515;&#21147;&#34928;&#31469;&#20316;&#20026;&#19968;&#31181;&#26222;&#36941;&#23384;&#22312;&#30340;&#20840;&#29699;&#20581;&#24247;&#38382;&#39064;&#65292;&#23454;&#26045;&#21019;&#26032;&#26041;&#27861;&#20197;&#22686;&#24378;&#24739;&#32773;&#25252;&#29702;&#23384;&#22312;&#22256;&#38590;&#12290;&#29305;&#21035;&#26159;&#22312;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#20013;&#39044;&#27979;&#27515;&#20129;&#29575;&#26082;&#22256;&#38590;&#21448;&#20851;&#38190;&#65292;&#38656;&#35201;&#20010;&#24615;&#21270;&#25252;&#29702;&#12289;&#31215;&#26497;&#31649;&#29702;&#65292;&#24182;&#25903;&#25345;&#30693;&#24773;&#20915;&#31574;&#20197;&#22686;&#24378;&#32467;&#26524;&#12290;&#26368;&#36817;&#65292;&#22768;&#38899;&#29983;&#29289;&#26631;&#24535;&#29289;&#19982;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30456;&#32467;&#21512;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#31361;&#20986;&#65292;&#29305;&#21035;&#22312;&#39044;&#27979;&#24515;&#21147;&#34928;&#31469;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#26377;&#25928;&#24615;&#12290;&#22768;&#38899;&#20998;&#26512;&#19982;ML&#31639;&#27861;&#30340;&#21327;&#21516;&#20316;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#19988;&#26131;&#20110;&#33719;&#21462;&#30340;&#35780;&#20272;&#24739;&#32773;&#20581;&#24247;&#29366;&#20917;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#31526;&#21512;&#26631;&#20934;&#21270;&#35821;&#38899;&#21327;&#35758;&#30340;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#39044;&#27979;&#27515;&#20129;&#29575;&#32570;&#20047;&#22768;&#38899;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#24378;&#22823;&#26377;&#25928;&#30340;ML&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22768;&#38899;&#29983;&#29289;&#26631;&#24535;&#29289;&#26469;&#39044;&#27979;&#20303;&#38498;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13812v1 Announce Type: new  Abstract: Addressing heart failure (HF) as a prevalent global health concern poses difficulties in implementing innovative approaches for enhanced patient care. Predicting mortality rates in HF patients, in particular, is difficult yet critical, necessitating individualized care, proactive management, and enabling educated decision-making to enhance outcomes. Recently, the significance of voice biomarkers coupled with Machine Learning (ML) has surged, demonstrating remarkable efficacy, particularly in predicting heart failure. The synergy of voice analysis and ML algorithms provides a non-invasive and easily accessible means to evaluate patients' health. However, there is a lack of voice biomarkers for predicting mortality rates among heart failure patients with standardized speech protocols. Here, we demonstrate a powerful and effective ML model for predicting mortality rates in hospitalized HF patients through the utilization of voice biomarkers
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#24687;&#35770;&#26694;&#26550;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#23545;&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#27700;&#24179;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20013;&#38544;&#21547;&#30340;&#39118;&#38505;</title><link>https://arxiv.org/abs/2402.10686</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#21644;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65306;&#20449;&#24687;&#35770;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Uncertainty, Calibration, and Membership Inference Attacks: An Information-Theoretic Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10686
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#26694;&#26550;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#23545;&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#27700;&#24179;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20013;&#38544;&#21547;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#20013;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#20856;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#36807;&#24230;&#33258;&#20449;&#26469;&#30830;&#23450;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#30446;&#26631;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#20869;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#65288;LiRA&#65289;&#30340;&#24615;&#33021;&#65292;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#20801;&#35768;&#30740;&#31350;&#30495;&#23454;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#30001;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#38598;&#24341;&#36215;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#30446;&#26631;&#27169;&#22411;&#30340;&#26657;&#20934;&#27700;&#24179;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#20174;&#30446;&#26631;&#27169;&#22411;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#36880;&#28176;&#20943;&#23569;&#65306;&#32622;&#20449;&#21521;&#37327;&#65288;CV&#65289;&#25259;&#38706;&#65292;&#20854;&#20013;&#36755;&#20986;&#27010;&#29575;&#21521;&#37327;&#34987;&#21457;&#24067;&#65307;&#30495;&#23454;&#26631;&#31614;&#32622;&#20449;&#24230;&#65288;TLC&#65289;&#25259;&#38706;&#65292;&#20854;&#20013;&#21482;&#26377;&#27169;&#22411;&#20998;&#37197;&#32473;&#30495;&#23454;&#26631;&#31614;&#30340;&#27010;&#29575;&#26159;&#21487;&#29992;&#30340;&#65307;&#20197;&#21450;&#20915;&#31574;&#38598;&#65288;DS&#65289;&#25259;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10686v1 Announce Type: cross  Abstract: In a membership inference attack (MIA), an attacker exploits the overconfidence exhibited by typical machine learning models to determine whether a specific data point was used to train a target model. In this paper, we analyze the performance of the state-of-the-art likelihood ratio attack (LiRA) within an information-theoretical framework that allows the investigation of the impact of the aleatoric uncertainty in the true data generation process, of the epistemic uncertainty caused by a limited training data set, and of the calibration level of the target model. We compare three different settings, in which the attacker receives decreasingly informative feedback from the target model: confidence vector (CV) disclosure, in which the output probability vector is released; true label confidence (TLC) disclosure, in which only the probability assigned to the true label is made available by the model; and decision set (DS) disclosure, in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#36827;&#34892;&#31934;&#30830;&#24352;&#37327;&#34917;&#20840;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;-&#24352;&#37327;&#20056;&#31215;&#21644;&#24352;&#37327;&#26680;&#33539;&#25968;&#23450;&#20041;&#65292;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#24182;&#24471;&#21040;&#29702;&#35770;&#30028;&#38480;&#12290;&#36825;&#20123;&#24037;&#20316;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#24352;&#37327;&#34917;&#20840;&#30340;&#28789;&#27963;&#24615;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03468</link><description>&lt;p&gt;
&#30001;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#39537;&#21160;&#30340;&#31934;&#30830;&#24352;&#37327;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Exact Tensor Completion Powered by Arbitrary Linear Transforms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#36827;&#34892;&#31934;&#30830;&#24352;&#37327;&#34917;&#20840;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;-&#24352;&#37327;&#20056;&#31215;&#21644;&#24352;&#37327;&#26680;&#33539;&#25968;&#23450;&#20041;&#65292;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#24182;&#24471;&#21040;&#29702;&#35770;&#30028;&#38480;&#12290;&#36825;&#20123;&#24037;&#20316;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#24352;&#37327;&#34917;&#20840;&#30340;&#28789;&#27963;&#24615;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#65292;&#26088;&#22312;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#23436;&#32654;&#24674;&#22797;&#24352;&#37327;&#12290;&#29616;&#26377;&#30340;&#29702;&#35770;&#20445;&#35777;&#35201;&#27714;&#28041;&#21450;&#30340;&#21464;&#25442;&#26159;&#27491;&#20132;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36339;&#20986;&#20102;&#21508;&#21521;&#21516;&#24615;&#25110;&#33258;&#20276;&#30340;&#32422;&#26463;&#65292;&#24314;&#31435;&#20102;&#20351;&#29992;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#36827;&#34892;&#31934;&#30830;&#24352;&#37327;&#34917;&#20840;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;-&#24352;&#37327;&#20056;&#31215;&#65292;&#23548;&#33268;&#20102;&#24352;&#37327;&#26680;&#33539;&#25968;&#30340;&#26032;&#23450;&#20041;&#12290;&#37197;&#22791;&#20102;&#36825;&#20123;&#24037;&#20855;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#25968;&#27861;&#30340;&#39640;&#25928;&#31639;&#27861;&#26469;&#35299;&#20915;&#21464;&#25442;&#21518;&#30340;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#65292;&#24182;&#24471;&#21040;&#29702;&#35770;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#35777;&#26126;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#24352;&#37327;&#34917;&#20840;&#30340;&#28789;&#27963;&#24615;&#65292;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, a tensor completion problem is studied, which aims to perfectly recover the tensor from partial observations. Existing theoretical guarantee requires the involved transform to be orthogonal, which hinders its applications. In this paper, jumping out of the constraints of isotropy or self-adjointness, the theoretical guarantee of exact tensor completion with arbitrary linear transforms is established. To that end, we define a new tensor-tensor product, which leads us to a new definition of the tensor nuclear norm. Equipped with these tools, an efficient algorithm based on alternating direction of multipliers is designed to solve the transformed tensor completion program and the theoretical bound is obtained. Our model and proof greatly enhance the flexibility of tensor completion and extensive experiments validate the superiority of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36827;&#34892;&#25506;&#32034;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#21487;&#20197;&#26377;&#25928;&#35745;&#31639;&#29366;&#24577;&#25968;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#32500;&#29615;&#22659;&#20013;&#39044;&#35757;&#32451;&#34920;&#31034;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.03138</link><description>&lt;p&gt;
Just Cluster It: &#19968;&#31181;&#20351;&#29992;&#32858;&#31867;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#36827;&#34892;&#39640;&#32500;&#31354;&#38388;&#25506;&#32034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36827;&#34892;&#25506;&#32034;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#21487;&#20197;&#26377;&#25928;&#35745;&#31639;&#29366;&#24577;&#25968;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#32500;&#29615;&#22659;&#20013;&#39044;&#35757;&#32451;&#34920;&#31034;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#34920;&#24449;&#20026;&#20013;&#24515;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#38382;&#39064;&#65292;&#23558;&#25506;&#32034;&#35270;&#20026;&#23494;&#24230;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#20351;&#29992;&#32858;&#31867;&#26469;&#36827;&#34892;&#25506;&#32034;&#30340;&#26377;&#25928;&#24615;&#65292;&#22522;&#20110;&#19968;&#20010;&#35266;&#23519;&#65306;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#65292;&#19982;&#20108;&#32500;&#29615;&#22659;&#30456;&#27604;&#65292;&#29366;&#24577;&#36716;&#25442;&#20013;&#30340;&#20687;&#32032;&#21464;&#21270;&#30340;&#37325;&#35201;&#24615;&#19981;&#37027;&#20040;&#26126;&#26174;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#34920;&#31034;&#21644;&#39044;&#35757;&#32451;DINO&#34920;&#31034;&#36827;&#34892;&#21608;&#26399;&#24615;&#21644;&#20840;&#23616;&#32858;&#31867;&#26469;&#35745;&#31639;&#29366;&#24577;&#25968;&#65292;&#21363;&#20272;&#35745;&#20266;&#35745;&#25968;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#26159;&#38543;&#26426;&#29305;&#24449;&#20063;&#21487;&#20197;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#32858;&#31867;&#20197;&#35745;&#31639;&#29366;&#24577;&#25968;&#65292;&#28982;&#32780;&#24403;&#36825;&#20123;&#29305;&#24449;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#26102;&#65292;&#39044;&#35757;&#32451;DINO&#34920;&#31034;&#30001;&#20110;&#20854;&#39044;&#35757;&#32451;&#30340;&#24402;&#32435;&#20559;&#24046;&#22312;&#34920;&#31034;&#20013;&#26356;&#21152;&#26377;&#25928;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#20026;&#38598;&#25104;&#39044;&#35757;&#32451;&#34920;&#31034;&#25552;&#20379;&#20102;&#19968;&#26465;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we adopt a representation-centric perspective on exploration in reinforcement learning, viewing exploration fundamentally as a density estimation problem. We investigate the effectiveness of clustering representations for exploration in 3-D environments, based on the observation that the importance of pixel changes between transitions is less pronounced in 3-D environments compared to 2-D environments, where pixel changes between transitions are typically distinct and significant. We propose a method that performs episodic and global clustering on random representations and on pre-trained DINO representations to count states, i.e, estimate pseudo-counts. Surprisingly, even random features can be clustered effectively to count states in 3-D environments, however when these become visually more complex, pre-trained DINO representations are more effective thanks to the pre-trained inductive biases in the representations. Overall, this presents a pathway for integrating pre-t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17435</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#21462;&#20195;&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#23454;&#39564;&#23460;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Replace Economic Choice Prediction Labs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17435
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#24448;&#24448;&#21463;&#38480;&#20110;&#33719;&#21462;&#20154;&#31867;&#36873;&#25321;&#25968;&#25454;&#30340;&#22256;&#38590;&#12290;&#23454;&#39564;&#32463;&#27982;&#23398;&#30740;&#31350;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19987;&#27880;&#20110;&#31616;&#21333;&#30340;&#36873;&#25321;&#29615;&#22659;&#12290;&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#30028;&#20197;&#20004;&#31181;&#26041;&#24335;&#20026;&#35813;&#21162;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#65306;&#32771;&#34385;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#20195;&#26367;&#20154;&#31867;&#22312;&#19978;&#36848;&#31616;&#21333;&#36873;&#25321;&#39044;&#27979;&#29615;&#22659;&#20013;&#65292;&#20197;&#21450;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#30740;&#31350;&#26356;&#22797;&#26434;&#20294;&#20173;&#20005;&#26684;&#30340;&#23454;&#39564;&#32463;&#27982;&#23398;&#29615;&#22659;&#65292;&#21253;&#25324;&#19981;&#23436;&#20840;&#20449;&#24687;&#12289;&#37325;&#22797;&#21338;&#24328;&#21644;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#30340;&#35828;&#26381;&#28216;&#25103;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28789;&#24863;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#20840;&#27169;&#25311;&#32463;&#27982;&#29615;&#22659;&#65292;&#24182;&#29983;&#25104;&#29992;&#20110;&#39640;&#25928;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#26367;&#20195;&#22797;&#26434;&#30340;&#32463;&#27982;&#23454;&#39564;&#23460;&#30740;&#31350;&#65311;&#25105;&#20204;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#24320;&#21019;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#20165;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25361;&#25112;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#33539;&#24335;&#65292;&#36890;&#36807;&#30740;&#31350;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#22120;&#36873;&#25321;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#29702;&#35770;&#25512;&#23548;&#20986;&#20102;&#26799;&#24230;&#20914;&#31361;&#30340;&#35282;&#33394;&#12290;</title><link>https://arxiv.org/abs/2311.04698</link><description>&lt;p&gt;
&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#25361;&#25112;&#24120;&#35265;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Challenging Common Paradigms in Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04698
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25361;&#25112;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#33539;&#24335;&#65292;&#36890;&#36807;&#30740;&#31350;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#22120;&#36873;&#25321;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#29702;&#35770;&#25512;&#23548;&#20986;&#20102;&#26799;&#24230;&#20914;&#31361;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#20294;&#20854;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#24182;&#26410;&#24102;&#26469;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#30456;&#27604;&#21333;&#20219;&#21153;&#23398;&#20064;&#65288;STL&#65289;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#26356;&#28145;&#20837;&#20102;&#35299;MTL&#29305;&#23450;&#25361;&#25112;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;MTL&#20013;&#30340;&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#20960;&#28857;&#20851;&#20110;STL&#30340;&#37325;&#35201;&#24433;&#21709;&#65306;&#39318;&#20808;&#65292;&#20248;&#21270;&#22120;&#30340;&#36873;&#25321;&#23545;MTL&#30340;&#24433;&#21709;&#21482;&#21463;&#21040;&#20102;&#36731;&#24494;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#30340;&#23454;&#35777;&#26041;&#27861;&#23637;&#31034;&#20102;&#24120;&#35265;STL&#24037;&#20855;&#65288;&#20363;&#22914;Adam&#20248;&#21270;&#22120;&#65289;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;Adam&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#19968;&#23450;&#30340;&#20551;&#35774;&#19979;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#37096;&#20998;&#25439;&#22833;&#23610;&#24230;&#19981;&#21464;&#24615;&#12290;&#20854;&#27425;&#65292;&#26799;&#24230;&#20914;&#31361;&#30340;&#27010;&#24565;&#32463;&#24120;&#34987;&#25551;&#36848;&#20026;MTL&#20013;&#30340;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#26799;&#24230;&#20914;&#31361;&#22312;MTL&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#23558;&#20854;&#19982;STL&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#35282;&#24230;&#26799;&#24230;&#23545;&#40784;&#26041;&#38754;&#65292;&#25105;&#20204;&#27809;&#26377;&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04698v3 Announce Type: replace-cross  Abstract: While multi-task learning (MTL) has gained significant attention in recent years, its underlying mechanisms remain poorly understood. Recent methods did not yield consistent performance improvements over single task learning (STL) baselines, underscoring the importance of gaining more profound insights about challenges specific to MTL. In our study, we challenge paradigms in MTL in the context of STL: First, the impact of the choice of optimizer has only been mildly investigated in MTL. We show the pivotal role of common STL tools such as the Adam optimizer in MTL empirically in various experiments. To further investigate Adam's effectiveness, we theoretical derive a partial loss-scale invariance under mild assumptions. Second, the notion of gradient conflicts has often been phrased as a specific problem in MTL. We delve into the role of gradient conflicts in MTL and compare it to STL. For angular gradient alignment we find no 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#22240;&#26524;&#35780;&#20998;&#20316;&#20026;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#65292;&#25903;&#25345;&#20915;&#31574;&#21046;&#23450;&#65292;&#25552;&#20379;&#27934;&#23519;&#21147;&#65292;&#24182;&#21487;&#29992;&#20110;&#25928;&#24212;&#20272;&#35745;&#12289;&#25928;&#24212;&#25490;&#24207;&#21644;&#25928;&#24212;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2206.12532</link><description>&lt;p&gt;
&#22240;&#26524;&#35780;&#20998;&#65306;&#25928;&#24212;&#20272;&#35745;&#12289;&#25928;&#24212;&#25490;&#24207;&#21644;&#25928;&#24212;&#20998;&#31867;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Causal Scoring: A Framework for Effect Estimation, Effect Ordering, and Effect Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.12532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#22240;&#26524;&#35780;&#20998;&#20316;&#20026;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#65292;&#25903;&#25345;&#20915;&#31574;&#21046;&#23450;&#65292;&#25552;&#20379;&#27934;&#23519;&#21147;&#65292;&#24182;&#21487;&#29992;&#20110;&#25928;&#24212;&#20272;&#35745;&#12289;&#25928;&#24212;&#25490;&#24207;&#21644;&#25928;&#24212;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22240;&#26524;&#35780;&#20998;&#24341;&#20837;&#21040;&#20915;&#31574;&#21046;&#23450;&#30340;&#32972;&#26223;&#20013;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#28041;&#21450;&#20272;&#35745;&#25903;&#25345;&#20915;&#31574;&#21046;&#23450;&#30340;&#24471;&#20998;&#65292;&#20174;&#32780;&#25552;&#20379;&#22240;&#26524;&#25928;&#24212;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#20123;&#35780;&#20998;&#30340;&#19977;&#31181;&#26377;&#20215;&#20540;&#30340;&#22240;&#26524;&#35299;&#37322;&#65306;&#25928;&#24212;&#20272;&#35745;&#65288;EE&#65289;&#12289;&#25928;&#24212;&#25490;&#24207;&#65288;EO&#65289;&#21644;&#25928;&#24212;&#20998;&#31867;&#65288;EC&#65289;&#12290;&#22312;EE&#35299;&#37322;&#20013;&#65292;&#22240;&#26524;&#35780;&#20998;&#20195;&#34920;&#20102;&#25928;&#24212;&#26412;&#36523;&#12290;EO&#35299;&#37322;&#26263;&#31034;&#35780;&#20998;&#21487;&#20197;&#20316;&#20026;&#25928;&#24212;&#22823;&#23567;&#30340;&#20195;&#29702;&#65292;&#21487;&#20197;&#26681;&#25454;&#20854;&#22240;&#26524;&#25928;&#24212;&#23545;&#20010;&#20307;&#36827;&#34892;&#25490;&#24207;&#12290;EC&#35299;&#37322;&#36890;&#36807;&#39044;&#23450;&#20041;&#30340;&#38408;&#20540;&#65292;&#20351;&#20010;&#20307;&#20998;&#20026;&#39640;&#25928;&#24212;&#21644;&#20302;&#25928;&#24212;&#31867;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#26367;&#20195;&#22240;&#26524;&#35299;&#37322;&#65288;EO&#21644;EC&#65289;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.12532v4 Announce Type: replace-cross  Abstract: This paper introduces causal scoring as a novel approach to frame causal estimation in the context of decision making. Causal scoring entails the estimation of scores that support decision making by providing insights into causal effects. We present three valuable causal interpretations of these scores: effect estimation (EE), effect ordering (EO), and effect classification (EC). In the EE interpretation, the causal score represents the effect itself. The EO interpretation implies that the score can serve as a proxy for the magnitude of the effect, enabling the sorting of individuals based on their causal effects. The EC interpretation enables the classification of individuals into high- and low-effect categories using a predefined threshold. We demonstrate the value of these alternative causal interpretations (EO and EC) through two key results. First, we show that aligning the statistical modeling with the desired causal inte
&lt;/p&gt;</description></item><item><title>CoTFormer&#26159;&#19968;&#31181;transformer&#21464;&#20307;&#65292;&#36890;&#36807;&#20351;&#29992;&#38544;&#21547;&#30340;&#38142;&#24605;&#32771;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#19982;&#26356;&#28145;&#27169;&#22411;&#30456;&#24403;&#30340;&#23481;&#37327;&#65292;&#24182;&#19988;&#22312;&#23454;&#35777;&#20013;&#26174;&#33879;&#20248;&#20110;&#26356;&#22823;&#30340;&#26631;&#20934;transformers&#12290;</title><link>http://arxiv.org/abs/2310.10845</link><description>&lt;p&gt;
CoTFormer&#65306;&#26356;&#22810;&#30340;&#20851;&#27880;&#20196;&#29260;&#24357;&#34917;&#20102;&#26356;&#23569;&#30340;&#28145;&#24230;
&lt;/p&gt;
&lt;p&gt;
CoTFormer: More Tokens With Attention Make Up For Less Depth. (arXiv:2310.10845v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10845
&lt;/p&gt;
&lt;p&gt;
CoTFormer&#26159;&#19968;&#31181;transformer&#21464;&#20307;&#65292;&#36890;&#36807;&#20351;&#29992;&#38544;&#21547;&#30340;&#38142;&#24605;&#32771;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#19982;&#26356;&#28145;&#27169;&#22411;&#30456;&#24403;&#30340;&#23481;&#37327;&#65292;&#24182;&#19988;&#22312;&#23454;&#35777;&#20013;&#26174;&#33879;&#20248;&#20110;&#26356;&#22823;&#30340;&#26631;&#20934;transformers&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#21457;&#23637;&#36234;&#26469;&#36234;&#22823;&#21644;&#26356;&#28145;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#31454;&#36187;&#27491;&#22312;&#36827;&#34892;&#20013;&#12290;&#28982;&#32780;&#65292;&#20687;&#38142;&#24605;&#32771;&#65288;CoT&#65289;&#26041;&#27861;&#36825;&#26679;&#30340;&#25216;&#26415;&#22312;&#23454;&#29616;&#26368;&#20339;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#20173;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20351;&#29992;&#38142;&#24605;&#32771;&#21644;&#20351;&#29992;&#26356;&#28145;&#30340;transformer&#20043;&#38388;&#30340;&#36817;&#20284;&#24179;&#34892;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CoTFormer&#65292;&#19968;&#31181;&#20351;&#29992;&#38544;&#21547;&#38142;&#24605;&#32771;&#26426;&#21046;&#26469;&#23454;&#29616;&#19982;&#26356;&#28145;&#27169;&#22411;&#30456;&#24403;&#23481;&#37327;&#30340;transformer&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#35777;&#26126;&#20102;CoTFormer&#30340;&#26377;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26126;&#26174;&#20248;&#20110;&#26356;&#22823;&#30340;&#26631;&#20934;transformers&#12290;
&lt;/p&gt;
&lt;p&gt;
The race to continually develop ever larger and deeper foundational models is underway. However, techniques like the Chain-of-Thought (CoT) method continue to play a pivotal role in achieving optimal downstream performance. In this work, we establish an approximate parallel between using chain-of-thought and employing a deeper transformer. Building on this insight, we introduce CoTFormer, a transformer variant that employs an implicit CoT-like mechanism to achieve capacity comparable to a deeper model. Our empirical findings demonstrate the effectiveness of CoTFormers, as they significantly outperform larger standard transformers.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#23427;&#36890;&#36807;&#23884;&#20837;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#26469;&#35299;&#20915;&#39640;&#32500;&#21704;&#23494;&#39039;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.01069</link><description>&lt;p&gt;
&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Separable Hamiltonian Neural Networks. (arXiv:2309.01069v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01069
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#23427;&#36890;&#36807;&#23884;&#20837;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#26469;&#35299;&#20915;&#39640;&#32500;&#21704;&#23494;&#39039;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31163;&#25955;&#35266;&#27979;&#25968;&#25454;&#24314;&#27169;&#21160;&#21147;&#31995;&#32479;&#26159;&#29616;&#20195;&#31185;&#23398;&#21644;&#24037;&#31243;&#25968;&#25454;&#31995;&#32479;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290; &#21704;&#23494;&#39039;&#31995;&#32479;&#26159;&#19968;&#31867;&#22522;&#26412;&#19988;&#24191;&#27867;&#23384;&#22312;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290; &#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#26159;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#27721;&#23494;&#23572;&#39039;&#26041;&#31243;&#30340;&#23398;&#20064;&#20559;&#24046;&#19979;&#65292;&#20174;&#31163;&#25955;&#35266;&#27979;&#30340;&#21521;&#37327;&#22330;&#20013;&#26080;&#30417;&#30563;&#22320;&#22238;&#24402;&#21160;&#21147;&#31995;&#32479;&#30340;&#21704;&#23494;&#39039;&#37327;&#12290;&#28982;&#32780;&#65292;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#36890;&#24120;&#24456;&#22797;&#26434;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#20854;&#20013;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#29366;&#24577;&#31354;&#38388;&#30456;&#23545;&#20110;&#26679;&#26412;&#25968;&#37327;&#26159;&#24456;&#22823;&#30340;&#12290; &#26368;&#36817;&#21457;&#29616;&#30340;&#19968;&#31181;&#32531;&#35299;&#29366;&#24577;&#21464;&#37327;&#20043;&#38388;&#22797;&#26434;&#24615;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#65292;&#24182;&#23558;&#35813;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#23884;&#20837;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#26681;&#25454;&#29289;&#29702;&#23398;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#26415;&#35821;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#21487;&#20998;&#31163;&#30340;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20123;&#27169;&#22411;&#23884;&#20837;&#20102;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The modelling of dynamical systems from discrete observations is a challenge faced by modern scientific and engineering data systems. Hamiltonian systems are one such fundamental and ubiquitous class of dynamical systems. Hamiltonian neural networks are state-of-the-art models that unsupervised-ly regress the Hamiltonian of a dynamical system from discrete observations of its vector field under the learning bias of Hamilton's equations. Yet Hamiltonian dynamics are often complicated, especially in higher dimensions where the state space of the Hamiltonian system is large relative to the number of samples. A recently discovered remedy to alleviate the complexity between state variables in the state space is to leverage the additive separability of the Hamiltonian system and embed that additive separability into the Hamiltonian neural network. Following the nomenclature of physics-informed machine learning, we propose three separable Hamiltonian neural networks. These models embed additi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#29992;&#20110;&#20122;&#32447;&#24615;&#36229;&#20307;&#31215;&#36951;&#25022;&#24230;&#37327;&#30340;&#26368;&#20248;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#30340;&#36229;&#20307;&#31215;&#26631;&#37327;&#21270;&#26041;&#27861;&#22312;&#26368;&#23567;&#21270;&#36229;&#20307;&#31215;&#36951;&#25022;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#22312;&#22810;&#30446;&#26631;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.03288</link><description>&lt;p&gt;
&#29992;&#20110;&#20122;&#32447;&#24615;&#36229;&#20307;&#31215;&#36951;&#25022;&#24230;&#37327;&#30340;&#26368;&#20248;&#26631;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Scalarizations for Sublinear Hypervolume Regret. (arXiv:2307.03288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03288
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#29992;&#20110;&#20122;&#32447;&#24615;&#36229;&#20307;&#31215;&#36951;&#25022;&#24230;&#37327;&#30340;&#26368;&#20248;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#30340;&#36229;&#20307;&#31215;&#26631;&#37327;&#21270;&#26041;&#27861;&#22312;&#26368;&#23567;&#21270;&#36229;&#20307;&#31215;&#36951;&#25022;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#22312;&#22810;&#30446;&#26631;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#37327;&#21270;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22810;&#30446;&#26631;&#35774;&#32622;&#20013;&#65292;&#23558;&#22810;&#20010;&#30446;&#26631;&#20943;&#23569;&#20026;&#19968;&#20010;&#65292;&#20363;&#22914;&#26368;&#36817;&#22312;RLHF&#20013;&#29992;&#20110;&#35757;&#32451;&#26657;&#20934;&#20154;&#31867;&#20559;&#22909;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#20154;&#23545;&#36825;&#31181;&#32463;&#20856;&#26041;&#27861;&#25345;&#21542;&#23450;&#24577;&#24230;&#65292;&#22240;&#20026;&#24050;&#30693;&#32447;&#24615;&#26631;&#37327;&#21270;&#20250;&#24573;&#30053;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#20985;&#21306;&#22495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#25214;&#21040;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#36890;&#36807;&#34987;&#25903;&#37197;&#30340;&#36229;&#20307;&#31215;&#26469;&#25506;&#32034;&#24085;&#32047;&#25176;&#21069;&#27839;&#19978;&#30340;&#22810;&#26679;&#21270;&#30446;&#26631;&#38598;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20855;&#26377;&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#30340;&#36229;&#20307;&#31215;&#26631;&#37327;&#21270;&#20196;&#20154;&#24778;&#35766;&#22320;&#26159;&#20026;&#20102;&#35777;&#26126;&#26368;&#23567;&#21270;&#36229;&#20307;&#31215;&#36951;&#25022;&#32780;&#26368;&#20248;&#30340;&#65292;&#23454;&#29616;&#20102; $O(T^{-1/k})$ &#30340;&#26368;&#20248;&#20122;&#32447;&#24615;&#36951;&#25022;&#30028;&#65292;&#21516;&#26102;&#21305;&#37197;&#30340;&#19979;&#30028;&#34920;&#26126;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#27809;&#26377;&#20219;&#20309;&#31639;&#27861;&#33021;&#20570;&#24471;&#26356;&#22909;&#12290;&#20316;&#20026;&#19968;&#20010;&#29702;&#35770;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#30446;&#26631;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;&#36229;&#32447;&#24615;&#36951;&#25022;&#30028;&#30340;&#36229;&#20307;&#31215;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
Scalarization is a general technique that can be deployed in any multiobjective setting to reduce multiple objectives into one, such as recently in RLHF for training reward models that align human preferences. Yet some have dismissed this classical approach because linear scalarizations are known to miss concave regions of the Pareto frontier. To that end, we aim to find simple non-linear scalarizations that can explore a diverse set of $k$ objectives on the Pareto frontier, as measured by the dominated hypervolume. We show that hypervolume scalarizations with uniformly random weights are surprisingly optimal for provably minimizing the hypervolume regret, achieving an optimal sublinear regret bound of $O(T^{-1/k})$, with matching lower bounds that preclude any algorithm from doing better asymptotically. As a theoretical case study, we consider the multiobjective stochastic linear bandits problem and demonstrate that by exploiting the sublinear regret bounds of the hypervolume scalariz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#31639;&#27861;DPM&#65292;&#36890;&#36807;&#25628;&#32034;&#20934;&#30830;&#30340;&#25968;&#25454;&#28857;&#20998;&#31163;&#22120;&#26469;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#30340;&#32858;&#31867;&#12290;&#20851;&#38190;&#36129;&#29486;&#26159;&#35782;&#21035;&#22823;&#38388;&#38548;&#20998;&#31163;&#22120;&#24182;&#21512;&#29702;&#20998;&#37197;&#38544;&#31169;&#39044;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.02969</link><description>&lt;p&gt;
DPM: &#36890;&#36807;&#20998;&#31163;&#32858;&#31867;&#25935;&#24863;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
DPM: Clustering Sensitive Data through Separation. (arXiv:2307.02969v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#31639;&#27861;DPM&#65292;&#36890;&#36807;&#25628;&#32034;&#20934;&#30830;&#30340;&#25968;&#25454;&#28857;&#20998;&#31163;&#22120;&#26469;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#30340;&#32858;&#31867;&#12290;&#20851;&#38190;&#36129;&#29486;&#26159;&#35782;&#21035;&#22823;&#38388;&#38548;&#20998;&#31163;&#22120;&#24182;&#21512;&#29702;&#20998;&#37197;&#38544;&#31169;&#39044;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#32858;&#31867;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#20998;&#32452;&#65292;&#21516;&#26102;&#30830;&#20445;&#25935;&#24863;&#20449;&#24687;&#24471;&#20197;&#20445;&#25252;&#12290;&#20808;&#21069;&#30340;&#38544;&#31169;&#20445;&#25252;&#32858;&#31867;&#20851;&#27880;&#28857;&#22312;&#20110;&#35782;&#21035;&#28857;&#20113;&#30340;&#32858;&#38598;&#12290;&#26412;&#25991;&#21017;&#37319;&#21462;&#21478;&#19968;&#31181;&#26041;&#27861;&#65292;&#20851;&#27880;&#20110;&#35782;&#21035;&#36866;&#24403;&#30340;&#20998;&#31163;&#22120;&#20197;&#20998;&#31163;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#31639;&#27861;DPM&#65292;&#20197;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#24335;&#25628;&#32034;&#20934;&#30830;&#30340;&#25968;&#25454;&#28857;&#20998;&#31163;&#22120;&#12290;DPM&#35299;&#20915;&#20102;&#23547;&#25214;&#20934;&#30830;&#20998;&#31163;&#22120;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#35782;&#21035;&#32858;&#31867;&#38388;&#30340;&#22823;&#38388;&#38548;&#20998;&#31163;&#22120;&#32780;&#19981;&#26159;&#32858;&#31867;&#20869;&#30340;&#23567;&#38388;&#38548;&#20998;&#31163;&#22120;&#65292;&#20197;&#21450;&#22312;&#24320;&#38144;&#38544;&#31169;&#39044;&#31639;&#26102;&#65292;&#20248;&#20808;&#32771;&#34385;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#36739;&#22823;&#23376;&#37096;&#20998;&#30340;&#20998;&#31163;&#22120;&#12290;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#25351;&#25968;&#26426;&#21046;&#65292;DPM&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#20855;&#26377;&#39640;&#25928;&#29992;&#24615;&#30340;&#32858;&#31867;&#20998;&#31163;&#22120;&#65306;&#23545;&#20110;&#25968;&#25454;&#38598;D&#65292;&#22914;&#26524;&#20013;&#24515;&#30340;60%&#20998;&#20301;&#25968;&#20013;&#23384;&#22312;&#23485;&#30340;&#20302;&#23494;&#24230;&#20998;&#31163;&#22120;&#65292;DPM&#20250;&#21457;&#29616;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving clustering groups data points in an unsupervised manner whilst ensuring that sensitive information remains protected. Previous privacy-preserving clustering focused on identifying concentration of point clouds. In this paper, we take another path and focus on identifying appropriate separators that split a data set. We introduce the novel differentially private clustering algorithm DPM that searches for accurate data point separators in a differentially private manner. DPM addresses two key challenges for finding accurate separators: identifying separators that are large gaps between clusters instead of small gaps within a cluster and, to efficiently spend the privacy budget, prioritising separators that split the data into large subparts. Using the differentially private Exponential Mechanism, DPM randomly chooses cluster separators with provably high utility: For a data set $D$, if there is a wide low-density separator in the central $60\%$ quantile, DPM finds that
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#12290;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#19978;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16927</link><description>&lt;p&gt;
&#32447;&#26463;&#33258;&#21160;&#39550;&#39542;&#65306;&#25361;&#25112;&#19982;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
End-to-end Autonomous Driving: Challenges and Frontiers. (arXiv:2306.16927v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16927
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#12290;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#19978;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26041;&#27861;&#37319;&#29992;&#31471;&#21040;&#31471;&#31639;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22987;&#20256;&#24863;&#22120;&#36755;&#20837;&#29983;&#25104;&#36710;&#36742;&#36816;&#21160;&#35745;&#21010;&#65292;&#32780;&#19981;&#26159;&#19987;&#27880;&#20110;&#35832;&#22914;&#26816;&#27979;&#21644;&#36816;&#21160;&#39044;&#27979;&#31561;&#21333;&#20010;&#20219;&#21153;&#12290;&#19982;&#27169;&#22359;&#21270;&#27969;&#27700;&#32447;&#30456;&#27604;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#26469;&#33719;&#30410;&#12290;&#36825;&#19968;&#39046;&#22495;&#22240;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12289;&#38381;&#29615;&#35780;&#20272;&#20197;&#21450;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#22312;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#25191;&#34892;&#25152;&#38656;&#30340;&#38656;&#27714;&#32780;&#34028;&#21187;&#21457;&#23637;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;250&#22810;&#31687;&#35770;&#25991;&#65292;&#28085;&#30422;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#21160;&#26426;&#12289;&#36335;&#32447;&#22270;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22522;&#30784;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 250 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#36335;&#23398;&#20064;&#36755;&#20837;&#20302;&#32500;&#24230;&#34920;&#31034;&#21644;&#26368;&#23567;&#21270;&#29305;&#24449;&#26144;&#23556;&#20013;&#30340;&#22797;&#26434;&#24615;/&#19981;&#35268;&#21017;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25511;&#21046;&#20102;&#35268;&#24459;&#24615;&#65292;&#24182;&#21033;&#29992;&#29702;&#35770;&#24037;&#20855;&#35777;&#26126;&#20102;&#29942;&#39048;&#32467;&#26500;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2305.19008</link><description>&lt;p&gt;
&#23398;&#20064;&#29305;&#24449;&#20013;&#30340;&#29942;&#39048;&#32467;&#26500;&#65306;&#20302;&#32500;&#24230;&#19982;&#35268;&#24459;&#24615;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Bottleneck Structure in Learned Features: Low-Dimension vs Regularity Tradeoff. (arXiv:2305.19008v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#36335;&#23398;&#20064;&#36755;&#20837;&#20302;&#32500;&#24230;&#34920;&#31034;&#21644;&#26368;&#23567;&#21270;&#29305;&#24449;&#26144;&#23556;&#20013;&#30340;&#22797;&#26434;&#24615;/&#19981;&#35268;&#21017;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25511;&#21046;&#20102;&#35268;&#24459;&#24615;&#65292;&#24182;&#21033;&#29992;&#29702;&#35770;&#24037;&#20855;&#35777;&#26126;&#20102;&#29942;&#39048;&#32467;&#26500;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;&#22823;&#28145;&#24230;$L$&#21644;$L_{2}$&#27491;&#21017;&#21270;&#30340;DNN&#20559;&#21521;&#20110;&#23398;&#20064;&#36755;&#20837;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#26368;&#23567;&#21270;&#23398;&#20064;&#20989;&#25968;$f$&#30340;&#31209;$R^{(0)}(f)$&#30340;&#27010;&#24565;&#65292;&#20854;&#34987;&#25512;&#27979;&#20026;&#29942;&#39048;&#31209;&#12290;&#25105;&#20204;&#35745;&#31639;&#20102;&#36825;&#20010;&#32467;&#26524;&#30340;&#26377;&#38480;&#28145;&#24230;&#20462;&#27491;&#65292;&#25581;&#31034;&#20102;&#19968;&#20010;&#24230;&#37327;$R^{(1)}$&#30340;&#35268;&#24459;&#24615;&#65292;&#23427;&#25511;&#21046;&#20102;&#38597;&#21487;&#27604;&#30697;&#38453;$\left|Jf(x)\right|_{+}$&#30340;&#20266;&#34892;&#21015;&#24335;&#24182;&#22312;&#32452;&#21512;&#21644;&#21152;&#27861;&#19979;&#26159;&#27425;&#21487;&#21152;&#30340;&#12290;&#36825;&#20351;&#24471;&#32593;&#32476;&#21487;&#20197;&#22312;&#23398;&#20064;&#20302;&#32500;&#34920;&#31034;&#21644;&#26368;&#23567;&#21270;&#29305;&#24449;&#26144;&#23556;&#20013;&#30340;&#22797;&#26434;&#24615;/&#19981;&#35268;&#21017;&#24615;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#65292;&#20174;&#32780;&#23398;&#20064;&#8220;&#27491;&#30830;&#8221;&#30340;&#20869;&#37096;&#23610;&#23544;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22823;&#23398;&#20064;&#36895;&#29575;&#22914;&#20309;&#25511;&#21046;&#23398;&#20064;&#20989;&#25968;&#30340;&#35268;&#24459;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#29702;&#35770;&#24037;&#20855;&#35777;&#26126;&#20102;&#29942;&#39048;&#32467;&#26500;&#22312;$L\to\infty$&#26102;&#22312;&#23398;&#20064;&#29305;&#24449;&#20013;&#30340;&#29468;&#24819;&#65306;&#23545;&#20110;&#22823;&#28145;&#24230;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;&#38544;&#34255;&#34920;&#31034;&#37117;&#38598;&#20013;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Previous work has shown that DNNs with large depth $L$ and $L_{2}$-regularization are biased towards learning low-dimensional representations of the inputs, which can be interpreted as minimizing a notion of rank $R^{(0)}(f)$ of the learned function $f$, conjectured to be the Bottleneck rank. We compute finite depth corrections to this result, revealing a measure $R^{(1)}$ of regularity which bounds the pseudo-determinant of the Jacobian $\left|Jf(x)\right|_{+}$ and is subadditive under composition and addition. This formalizes a balance between learning low-dimensional representations and minimizing complexity/irregularity in the feature maps, allowing the network to learn the `right' inner dimension. We also show how large learning rates also control the regularity of the learned function. Finally, we use these theoretical tools to prove the conjectured bottleneck structure in the learned features as $L\to\infty$: for large depths, almost all hidden representations concentrates aroun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#21644;&#31639;&#27861;&#65292;&#20174;&#38750;&#20984;&#20248;&#21270;&#30340;&#35282;&#24230;&#26469;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#12290;&#31639;&#27861;&#35299;&#20915;&#20102;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#65292;&#24182;&#20445;&#35777;&#20102;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06815</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#30340;&#26694;&#26550;&#12289;&#31639;&#27861;&#21644;&#25910;&#25947;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
On Model Compression for Neural Networks: Framework, Algorithm, and Convergence Guarantee. (arXiv:2303.06815v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#21644;&#31639;&#27861;&#65292;&#20174;&#38750;&#20984;&#20248;&#21270;&#30340;&#35282;&#24230;&#26469;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#12290;&#31639;&#27861;&#35299;&#20915;&#20102;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#65292;&#24182;&#20445;&#35777;&#20102;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21387;&#32553;&#23545;&#20110;&#37096;&#32626;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#35745;&#31639;&#35774;&#22791;&#30340;&#20869;&#23384;&#21644;&#23384;&#20648;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#20851;&#27880;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65306;&#20302;&#31209;&#36924;&#36817;&#21644;&#26435;&#37325;&#35009;&#21098;&#65292;&#36825;&#20123;&#25216;&#26415;&#30446;&#21069;&#38750;&#24120;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#21644;&#26435;&#37325;&#35009;&#21098;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#24635;&#26159;&#20250;&#36973;&#21463;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#21644;&#25910;&#25947;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#20174;&#38750;&#20984;&#20248;&#21270;&#30340;&#26032;&#35270;&#35282;&#35774;&#35745;&#20102;&#36866;&#24403;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22359;&#22352;&#26631;&#19979;&#38477;&#65288;BCD&#65289;&#31639;&#27861;NN-BCD&#26469;&#35299;&#20915;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#19968;&#20010;&#20248;&#28857;&#26159;&#21487;&#20197;&#33719;&#24471;&#20855;&#26377;&#38381;&#24335;&#24418;&#24335;&#30340;&#39640;&#25928;&#36845;&#20195;&#26041;&#26696;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#20102;Kurdyka-{\L}ojasiewicz (K{\L})&#24615;&#36136;&#65292;&#20445;&#35777;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model compression is a crucial part of deploying neural networks (NNs), especially when the memory and storage of computing devices are limited in many applications. This paper focuses on two model compression techniques: low-rank approximation and weight pruning in neural networks, which are very popular nowadays. However, training NN with low-rank approximation and weight pruning always suffers significant accuracy loss and convergence issues. In this paper, a holistic framework is proposed for model compression from a novel perspective of nonconvex optimization by designing an appropriate objective function. Then, we introduce NN-BCD, a block coordinate descent (BCD) algorithm to solve the nonconvex optimization. One advantage of our algorithm is that an efficient iteration scheme can be derived with closed-form, which is gradient-free. Therefore, our algorithm will not suffer from vanishing/exploding gradient problems. Furthermore, with the Kurdyka-{\L}ojasiewicz (K{\L}) property o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#36864;&#21270;&#29616;&#35937;&#65292;&#22312;&#20840;&#36830;&#25509;ReLU&#32593;&#32476;&#21021;&#22987;&#21270;&#26102;&#65292;&#20004;&#20010;&#36755;&#20837;&#20043;&#38388;&#30340;&#35282;&#24230;&#20250;&#36235;&#36817;&#20110;0&#12290;&#36890;&#36807;&#20351;&#29992;&#32452;&#21512;&#23637;&#24320;&#65292;&#24471;&#21040;&#20102;&#20854;&#36235;&#21521;&#20110;0&#30340;&#36895;&#24230;&#30340;&#31934;&#30830;&#20844;&#24335;&#65292;&#24182;&#39564;&#35777;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.09712</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#36864;&#21270;&#65306;&#20840;&#36830;&#25509;ReLU&#32593;&#32476;&#21021;&#22987;&#21270;&#26102;&#65292;&#28040;&#22833;&#35282;&#24230;&#30340;&#29616;&#35937; (arXiv:2302.09712v2 [stat.ML] &#26356;&#26032;&#29256;)
&lt;/p&gt;
&lt;p&gt;
Depth Degeneracy in Neural Networks: Vanishing Angles in Fully Connected ReLU Networks on Initialization. (arXiv:2302.09712v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#36864;&#21270;&#29616;&#35937;&#65292;&#22312;&#20840;&#36830;&#25509;ReLU&#32593;&#32476;&#21021;&#22987;&#21270;&#26102;&#65292;&#20004;&#20010;&#36755;&#20837;&#20043;&#38388;&#30340;&#35282;&#24230;&#20250;&#36235;&#36817;&#20110;0&#12290;&#36890;&#36807;&#20351;&#29992;&#32452;&#21512;&#23637;&#24320;&#65292;&#24471;&#21040;&#20102;&#20854;&#36235;&#21521;&#20110;0&#30340;&#36895;&#24230;&#30340;&#31934;&#30830;&#20844;&#24335;&#65292;&#24182;&#39564;&#35777;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#35768;&#22810;&#20854;&#24615;&#36136;&#20173;&#26410;&#34987;&#29702;&#35770;&#19978;&#29702;&#35299;&#65292;&#20854;&#20013;&#19968;&#20010;&#35868;&#22242;&#26159;&#28145;&#24230;&#36864;&#21270;&#29616;&#35937;&#65306;&#32593;&#32476;&#23618;&#25968;&#36234;&#28145;&#65292;&#21021;&#22987;&#21270;&#26102;&#32593;&#32476;&#36234;&#25509;&#36817;&#20110;&#24120;&#25968;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#20004;&#20010;&#36755;&#20837;&#20043;&#38388;&#38543;&#30528;&#23618;&#25968;&#21464;&#21270;&#30340;&#35282;&#24230;&#28436;&#21464;&#24773;&#20917;&#12290;&#36890;&#36807;&#20351;&#29992;&#32452;&#21512;&#23637;&#24320;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#23427;&#38543;&#28145;&#24230;&#22686;&#21152;&#36235;&#21521;&#20110;0&#30340;&#36895;&#24230;&#30340;&#31934;&#30830;&#20844;&#24335;&#65292;&#36825;&#20123;&#20844;&#24335;&#25429;&#25417;&#20102;&#24494;&#35266;&#27874;&#21160;&#12290;&#25105;&#20204;&#29992;Monte Carlo&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#32467;&#26524;&#20934;&#30830;&#22320;&#36817;&#20284;&#20102;&#26377;&#38480;&#32593;&#32476;&#30340;&#34892;&#20026;&#12290;&#36825;&#20123;&#20844;&#24335;&#20197;&#36890;&#36807;ReLU&#20989;&#25968;&#30340;&#30456;&#20851;&#39640;&#26031;&#21464;&#37327;&#30340;&#28151;&#21512;&#30697;&#24418;&#24335;&#32473;&#20986;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32452;&#21512;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite remarkable performance on a variety of tasks, many properties of deep neural networks are not yet theoretically understood. One such mystery is the depth degeneracy phenomenon: the deeper you make your network, the closer your network is to a constant function on initialization. In this paper, we examine the evolution of the angle between two inputs to a ReLU neural network as a function of the number of layers. By using combinatorial expansions, we find precise formulas for how fast this angle goes to zero as depth increases. These formulas capture microscopic fluctuations that are not visible in the popular framework of infinite width limits, and leads to qualitatively different predictions. We validate our theoretical results with Monte Carlo experiments and show that our results accurately approximate finite network behaviour. The formulas are given in terms of the mixed moments of correlated Gaussians passed through the ReLU function. We also find a surprising combinatoria
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#21644;&#39044;&#27979;&#23433;&#20840;&#36807;&#28388;&#22120;&#36827;&#34892;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(NMPC)&#30340;&#23433;&#20840;&#36817;&#20284;&#30340;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#23545;&#26580;&#24615;&#26426;&#22120;&#20154;&#30340;&#24555;&#36895;&#25511;&#21046;&#12290;&#19982;NMPC&#30456;&#27604;&#65292;&#22312;&#20445;&#35777;&#23433;&#20840;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35745;&#31639;&#26102;&#38388;&#19978;&#25913;&#21892;&#20102;8&#20493;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2212.02941</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22312;&#26580;&#24615;&#26426;&#22120;&#20154;&#20013;&#30340;&#23433;&#20840;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe Imitation Learning of Nonlinear Model Predictive Control for Flexible Robots. (arXiv:2212.02941v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#21644;&#39044;&#27979;&#23433;&#20840;&#36807;&#28388;&#22120;&#36827;&#34892;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(NMPC)&#30340;&#23433;&#20840;&#36817;&#20284;&#30340;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#23545;&#26580;&#24615;&#26426;&#22120;&#20154;&#30340;&#24555;&#36895;&#25511;&#21046;&#12290;&#19982;NMPC&#30456;&#27604;&#65292;&#22312;&#20445;&#35777;&#23433;&#20840;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35745;&#31639;&#26102;&#38388;&#19978;&#25913;&#21892;&#20102;8&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26580;&#24615;&#26426;&#22120;&#20154;&#21487;&#20197;&#35299;&#20915;&#19968;&#20123;&#24037;&#19994;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#22914;&#23454;&#29616;&#22266;&#26377;&#23433;&#20840;&#30340;&#20154;&#26426;&#21327;&#20316;&#21644;&#23454;&#29616;&#26356;&#39640;&#30340;&#36127;&#36733;&#37325;&#37327;&#27604;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#21253;&#25324;&#25391;&#33633;&#34892;&#20026;&#21644;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#65292;&#25511;&#21046;&#26580;&#24615;&#26426;&#22120;&#20154;&#38750;&#24120;&#22797;&#26434;&#12290;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(NMPC)&#33021;&#22815;&#26377;&#25928;&#25511;&#21046;&#27492;&#31867;&#26426;&#22120;&#20154;&#65292;&#20294;&#20854;&#35745;&#31639;&#38656;&#27714;&#36739;&#39640;&#24120;&#24120;&#38480;&#21046;&#20854;&#22312;&#23454;&#26102;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#26580;&#24615;&#26426;&#22120;&#20154;&#30340;&#24555;&#36895;&#25511;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#21644;&#39044;&#27979;&#23433;&#20840;&#36807;&#28388;&#22120;&#36827;&#34892;NMPC&#30340;&#23433;&#20840;&#36817;&#20284;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#26102;&#38388;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#19978;&#30053;&#26377;&#25439;&#22833;&#12290;&#19982;NMPC&#30456;&#27604;&#65292;&#22312;&#27169;&#25311;&#20013;&#25511;&#21046;&#19968;&#20010;&#19977;&#32500;&#26580;&#24615;&#26426;&#26800;&#33218;&#26102;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35745;&#31639;&#26102;&#38388;&#19978;&#25913;&#21892;&#20102;8&#20493;&#20197;&#19978;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#23433;&#20840;&#32422;&#26463;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible robots may overcome some of the industry's major challenges, such as enabling intrinsically safe human-robot collaboration and achieving a higher load-to-mass ratio. However, controlling flexible robots is complicated due to their complex dynamics, which include oscillatory behavior and a high-dimensional state space. NMPC offers an effective means to control such robots, but its extensive computational demands often limit its application in real-time scenarios. To enable fast control of flexible robots, we propose a framework for a safe approximation of NMPC using imitation learning and a predictive safety filter. Our framework significantly reduces computation time while incurring a slight loss in performance. Compared to NMPC, our framework shows more than a eightfold improvement in computation time when controlling a three-dimensional flexible robot arm in simulation, all while guaranteeing safety constraints. Notably, our approach outperforms conventional reinforcement le
&lt;/p&gt;</description></item></channel></rss>