<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#30740;&#31350;&#26159;&#31532;&#19968;&#20221;&#33268;&#21147;&#20110;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#28145;&#20837;&#20171;&#32461;&#20102;HNN&#26550;&#26500;&#12289;&#35757;&#32451;&#31574;&#30053;&#21644;&#24212;&#29992;&#65292;&#20026;&#20174;&#20107;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2404.01039</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35843;&#26597;&#65306;&#28145;&#20837;&#21644;&#20998;&#27493;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
A Survey on Hypergraph Neural Networks: An In-Depth and Step-By-Step Guide
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01039
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26159;&#31532;&#19968;&#20221;&#33268;&#21147;&#20110;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#28145;&#20837;&#20171;&#32461;&#20102;HNN&#26550;&#26500;&#12289;&#35757;&#32451;&#31574;&#30053;&#21644;&#24212;&#29992;&#65292;&#20026;&#20174;&#20107;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22797;&#26434;&#31995;&#32479;&#21644;&#24212;&#29992;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#22240;&#27492;&#65292;&#23545;&#20110;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#20026;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#19968;&#39033;&#37325;&#35201;&#35758;&#31243;&#12290;&#30001;&#20110;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#22312;&#25968;&#23398;&#19978;&#34987;&#34920;&#36798;&#20026;&#36229;&#22270;&#65292;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HNNs&#65289;&#24050;&#25104;&#20026;&#22312;&#36229;&#22270;&#19978;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#37492;&#20110;&#36825;&#19968;&#26032;&#36235;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20221;&#33268;&#21147;&#20110;HNNs&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#20854;&#20013;&#21253;&#21547;&#28145;&#20837;&#21644;&#20998;&#27493;&#25351;&#21335;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#26412;&#35843;&#26597;&#27010;&#36848;&#20102;HNN&#26550;&#26500;&#12289;&#35757;&#32451;&#31574;&#30053;&#21644;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;HNNs&#20998;&#35299;&#20026;&#22235;&#20010;&#35774;&#35745;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;i&#65289;&#36755;&#20837;&#29305;&#24449;&#65292;&#65288;ii&#65289;&#36755;&#20837;&#32467;&#26500;&#65292;&#65288;iii&#65289;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#65292;&#21644;&#65288;iv&#65289;&#35757;&#32451;&#31574;&#30053;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#32771;&#23519;HNNs&#22914;&#20309;&#36890;&#36807;&#21508;&#33258;&#30340;&#32452;&#25104;&#37096;&#20998;&#22788;&#29702;&#21644;&#23398;&#20064;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;HNNs&#22312;&#25512;&#33616;&#12289;&#29983;&#29289;&#21644;&#21307;&#23398;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01039v1 Announce Type: new  Abstract: Higher-order interactions (HOIs) are ubiquitous in real-world complex systems and applications, and thus investigation of deep learning for HOIs has become a valuable agenda for the data mining and machine learning communities. As networks of HOIs are expressed mathematically as hypergraphs, hypergraph neural networks (HNNs) have emerged as a powerful tool for representation learning on hypergraphs. Given the emerging trend, we present the first survey dedicated to HNNs, with an in-depth and step-by-step guide. Broadly, the present survey overviews HNN architectures, training strategies, and applications. First, we break existing HNNs down into four design components: (i) input features, (ii) input structures, (iii) message-passing schemes, and (iv) training strategies. Second, we examine how HNNs address and learn HOIs with each of their components. Third, we overview the recent applications of HNNs in recommendation, biological and med
&lt;/p&gt;</description></item><item><title>MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.19888</link><description>&lt;p&gt;
MambaMixer&#65306;&#20855;&#26377;&#21452;&#37325;&#26631;&#35760;&#21644;&#36890;&#36947;&#36873;&#25321;&#30340;&#39640;&#25928;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19888
&lt;/p&gt;
&lt;p&gt;
MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;Transformers&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#24615;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#22823;&#35268;&#27169;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26550;&#26500;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#23637;&#29616;&#20986;&#36755;&#20837;&#22823;&#23567;&#30340;&#20108;&#27425;&#26102;&#38388;&#21644;&#31354;&#38388;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#23581;&#35797;&#20026;&#22810;&#32500;&#25968;&#25454;&#35774;&#35745;&#39640;&#25928;&#26377;&#25928;&#30340;&#26550;&#26500;&#20027;&#24178;&#65292;&#20363;&#22914;&#22270;&#20687;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#35201;&#20040;&#26159;&#25968;&#25454;&#29420;&#31435;&#30340;&#65292;&#35201;&#20040;&#26080;&#27861;&#20801;&#35768;&#36328;&#32500;&#24230;&#21644;&#20869;&#37096;&#32500;&#24230;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#26368;&#36817;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#39640;&#25928;&#30828;&#20214;&#24863;&#30693;&#23454;&#29616;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#23637;&#29616;&#20986;&#20102;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#21463;&#21040;SSMs&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MambaMixer&#65292;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#36328;&#26631;&#35760;&#21644;&#36890;&#36947;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19888v1 Announce Type: cross  Abstract: Recent advances in deep learning have mainly relied on Transformers due to their data dependency and ability to learn at scale. The attention module in these architectures, however, exhibits quadratic time and space in input size, limiting their scalability for long-sequence modeling. Despite recent attempts to design efficient and effective architecture backbone for multi-dimensional data, such as images and multivariate time series, existing models are either data independent, or fail to allow inter- and intra-dimension communication. Recently, State Space Models (SSMs), and more specifically Selective State Space Models, with efficient hardware-aware implementation, have shown promising potential for long sequence modeling. Motivated by the success of SSMs, we present MambaMixer, a new architecture with data-dependent weights that uses a dual selection mechanism across tokens and channels, called Selective Token and Channel Mixer. M
&lt;/p&gt;</description></item><item><title>&#21487;&#24494;&#20998;&#32534;&#31243;&#26159;&#19968;&#20010;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#65292;&#20351;&#24471;&#22797;&#26434;&#31243;&#24207;&#33021;&#22815;&#31471;&#23545;&#31471;&#22320;&#36827;&#34892;&#24494;&#20998;&#65292;&#23454;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#21442;&#25968;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.14606</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#32534;&#31243;&#30340;&#35201;&#32032;
&lt;/p&gt;
&lt;p&gt;
The Elements of Differentiable Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14606
&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#32534;&#31243;&#26159;&#19968;&#20010;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#65292;&#20351;&#24471;&#22797;&#26434;&#31243;&#24207;&#33021;&#22815;&#31471;&#23545;&#31471;&#22320;&#36827;&#34892;&#24494;&#20998;&#65292;&#23454;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#21442;&#25968;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36825;&#24471;&#30410;&#20110;&#22823;&#22411;&#27169;&#22411;&#12289;&#24222;&#22823;&#25968;&#25454;&#38598;&#12289;&#21152;&#36895;&#30828;&#20214;&#65292;&#20197;&#21450;&#21487;&#24494;&#20998;&#32534;&#31243;&#30340;&#21464;&#38761;&#24615;&#21147;&#37327;&#12290;&#36825;&#31181;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#20351;&#22797;&#26434;&#35745;&#31639;&#26426;&#31243;&#24207;&#65288;&#21253;&#25324;&#20855;&#26377;&#25511;&#21046;&#27969;&#21644;&#25968;&#25454;&#32467;&#26500;&#30340;&#31243;&#24207;&#65289;&#33021;&#22815;&#36827;&#34892;&#31471;&#23545;&#31471;&#30340;&#24494;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#31243;&#24207;&#21442;&#25968;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#12290;&#19981;&#20165;&#20165;&#26159;&#31243;&#24207;&#30340;&#24494;&#20998;&#65292;&#21487;&#24494;&#20998;&#32534;&#31243;&#20063;&#21253;&#25324;&#20102;&#31243;&#24207;&#20248;&#21270;&#12289;&#27010;&#29575;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#27010;&#24565;&#12290;&#26412;&#20070;&#20171;&#32461;&#20102;&#21487;&#24494;&#20998;&#32534;&#31243;&#25152;&#38656;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#24182;&#37319;&#29992;&#20102;&#20248;&#21270;&#21644;&#27010;&#29575;&#20004;&#20010;&#20027;&#35201;&#35270;&#35282;&#36827;&#34892;&#38416;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14606v1 Announce Type: new  Abstract: Artificial intelligence has recently experienced remarkable advances, fueled by large models, vast datasets, accelerated hardware, and, last but not least, the transformative power of differentiable programming. This new programming paradigm enables end-to-end differentiation of complex computer programs (including those with control flows and data structures), making gradient-based optimization of program parameters possible. As an emerging paradigm, differentiable programming builds upon several areas of computer science and applied mathematics, including automatic differentiation, graphical models, optimization and statistics. This book presents a comprehensive review of the fundamental concepts useful for differentiable programming. We adopt two main perspectives, that of optimization and that of probability, with clear analogies between the two. Differentiable programming is not merely the differentiation of programs, but also the t
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.14236</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#26356;&#26032;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;ROME&#21644;MEMIT&#20316;&#20026;&#20027;&#35201;&#30340;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#33073;&#39062;&#32780;&#20986;&#12290;&#32780;MEMIT&#21487;&#20197;&#25209;&#37327;&#32534;&#36753;&#35760;&#24518;&#65292;ROME&#21017;&#19968;&#27425;&#21482;&#33021;&#25913;&#21464;&#19968;&#20010;&#20107;&#23454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;ROME&#21644;MEMIT&#32435;&#20837;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#20248;&#21270;&#21516;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20445;&#23384;-&#35760;&#24518;&#8221;&#30446;&#26631;&#12290;&#35813;&#30446;&#26631;&#26088;&#22312;&#22312;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#30340;&#21516;&#26102;&#20445;&#30041;&#26576;&#20123;&#36873;&#23450;&#21521;&#37327;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ROME&#20351;&#29992;&#31561;&#24335;&#32422;&#26463;&#20248;&#21270;&#27492;&#30446;&#26631;&#65292;&#32780;MEMIT&#37319;&#29992;&#26356;&#28789;&#27963;&#30340;&#26368;&#23567;&#20108;&#20056;&#32422;&#26463;&#12290;&#38500;&#20102;&#25209;&#37327;&#32534;&#36753;&#22806;&#65292;MEMIT&#36824;&#21487;&#20197;&#22312;&#22810;&#20010;&#23618;&#38754;&#32534;&#36753;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32534;&#36753;&#30340;&#20998;&#24067;&#20174;&#22810;&#20010;&#23618;&#38754;&#20998;&#24320;&#65292;&#21306;&#21035;&#20110;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14236v1 Announce Type: cross  Abstract: Model editing is a growing area focused on updating the knowledge embedded within models. Among the various methodologies, ROME and MEMIT stand out as leading "locate-and-edit" model editing techniques. While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time. This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the "preservation-memorization" objective. This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information. Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint. In addition to making batched edits, MEMIT also edits the model at multiple layers. We disentangle the distribution of edits to multiple layers from the optimization objectiv
&lt;/p&gt;</description></item><item><title>RigorLLM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#26377;&#25928;&#22320;&#35843;&#33410;LLMs&#30340;&#26377;&#23475;&#21644;&#19981;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#21253;&#25324;&#33021;&#37327;&#25968;&#25454;&#22686;&#24378;&#12289;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#23433;&#20840;&#36755;&#20837;&#21518;&#32512;&#65292;&#20197;&#21450;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#40065;&#26834;KNN&#19982;LLMs&#34701;&#21512;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.13031</link><description>&lt;p&gt;
RigorLLM&#65306;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25269;&#24481;&#19981;&#33391;&#20869;&#23481;&#30340;&#40065;&#26834;&#38450;&#25252;&#26639;
&lt;/p&gt;
&lt;p&gt;
RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13031
&lt;/p&gt;
&lt;p&gt;
RigorLLM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#26377;&#25928;&#22320;&#35843;&#33410;LLMs&#30340;&#26377;&#23475;&#21644;&#19981;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#21253;&#25324;&#33021;&#37327;&#25968;&#25454;&#22686;&#24378;&#12289;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#23433;&#20840;&#36755;&#20837;&#21518;&#32512;&#65292;&#20197;&#21450;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#40065;&#26834;KNN&#19982;LLMs&#34701;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#20013;&#20986;&#29616;&#30340;&#20559;&#35265;&#20197;&#21450;&#22312;&#24694;&#24847;&#36755;&#20837;&#19979;&#20135;&#29983;&#26377;&#23475;&#20869;&#23481;&#30340;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#65292;&#37117;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#38450;&#25252;&#26639;&#65288;RigorLLM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#26377;&#25928;&#22320;&#35843;&#33410;LLMs&#30340;&#26377;&#23475;&#21644;&#19981;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#36890;&#36807;&#37319;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#36827;&#34892;&#22522;&#20110;&#33021;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#12289;&#36890;&#36807;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38024;&#23545;&#36755;&#20837;&#20248;&#21270;&#23433;&#20840;&#21518;&#32512;&#65292;&#20197;&#21450;&#22522;&#20110;&#25105;&#20204;&#30340;&#25968;&#25454;&#22686;&#24378;&#23558;&#40065;&#26834;KNN&#19982;LLMs&#34701;&#21512;&#30340;&#22522;&#20110;&#34701;&#21512;&#30340;&#27169;&#22411;&#65292;RigorLLM&#20026;&#26377;&#23475;&#20869;&#23481;&#30340;&#35843;&#33410;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13031v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs) have showcased remarkable capabilities across various tasks in different domains. However, the emergence of biases and the potential for generating harmful content in LLMs, particularly under malicious inputs, pose significant challenges. Current mitigation strategies, while effective, are not resilient under adversarial attacks. This paper introduces Resilient Guardrails for Large Language Models (RigorLLM), a novel framework designed to efficiently and effectively moderate harmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted approach that includes energy-based training data augmentation through Langevin dynamics, optimizing a safe suffix for inputs via minimax optimization, and integrating a fusion-based model combining robust KNN with LLMs based on our data augmentation, RigorLLM offers a robust solution to harmful content moderation. Our experimental evalua
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#31227;&#21160;&#35774;&#22791;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25191;&#34892;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#24182;&#21019;&#24314;&#20102;&#33258;&#21160;&#21270;&#22522;&#30784;&#26550;&#26500;MELT&#26469;&#25903;&#25345;&#20854;&#35780;&#20272;&#21644;&#24615;&#33021;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2403.12844</link><description>&lt;p&gt;
MELTing point: &#31227;&#21160;&#35821;&#35328;&#36716;&#25442;&#22120;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MELTing point: Mobile Evaluation of Language Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12844
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#31227;&#21160;&#35774;&#22791;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25191;&#34892;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#24182;&#21019;&#24314;&#20102;&#33258;&#21160;&#21270;&#22522;&#30784;&#26550;&#26500;MELT&#26469;&#25903;&#25345;&#20854;&#35780;&#20272;&#21644;&#24615;&#33021;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#36880;&#28176;&#24212;&#29992;&#20110;&#26085;&#24120;&#20219;&#21153;&#65292;&#36171;&#20104;&#25105;&#20204;&#30340;&#35745;&#31639;&#26426;&#8220;&#26234;&#33021;&#30340;&#28779;&#33457;&#8221;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#36816;&#34892;&#26102;&#38656;&#27714;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#24191;&#27867;&#37096;&#32626;&#12290;&#22312;&#20010;&#20154;&#35774;&#22791;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#20197;&#21450;&#36805;&#36895;&#38544;&#31169;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#32039;&#36843;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#25191;&#34892;&#30340;&#29616;&#29366;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#33258;&#24049;&#30340;&#33258;&#21160;&#21270;&#22522;&#30784;&#26550;&#26500;MELT&#65292;&#25903;&#25345;&#22312;&#35774;&#22791;&#19978;&#26080;&#30028;&#38754;&#25191;&#34892;&#21644;&#35780;&#20272;LLMs&#65292;&#24182;&#25903;&#25345;&#19981;&#21516;&#30340;&#27169;&#22411;&#12289;&#35774;&#22791;&#21644;&#26694;&#26550;&#65292;&#21253;&#25324;Android&#12289;iOS&#21644;Nvidia Jetson&#35774;&#22791;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#27969;&#34892;&#30340;&#25351;&#20196;&#24494;&#35843;&#30340;LLMs&#65292;&#24182;&#21033;&#29992;&#19981;&#21516;&#30340;&#26694;&#26550;&#26469;&#27979;&#37327;&#23427;&#20204;&#30340;&#31471;&#21040;&#31471;&#21644;&#32454;&#31890;&#24230;&#24615;&#33021;&#65292;&#36319;&#36394;&#23427;&#20204;&#30340;&#20869;&#23384;&#21644;&#33021;&#32791;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12844v1 Announce Type: new  Abstract: Transformers have revolutionized the machine learning landscape, gradually making their way into everyday tasks and equipping our computers with ``sparks of intelligence''. However, their runtime requirements have prevented them from being broadly deployed on mobile. As personal devices become increasingly powerful and prompt privacy becomes an ever more pressing issue, we explore the current state of mobile execution of Large Language Models (LLMs). To achieve this, we have created our own automation infrastructure, MELT, which supports the headless execution and benchmarking of LLMs on device, supporting different models, devices and frameworks, including Android, iOS and Nvidia Jetson devices. We evaluate popular instruction fine-tuned LLMs and leverage different frameworks to measure their end-to-end and granular performance, tracing their memory and energy requirements along the way.   Our analysis is the first systematic study of o
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#20462;&#21098;&#31070;&#32463;&#20803;&#26469;&#23454;&#29616;&#21435;&#23398;&#20064;&#65292;&#21457;&#29616;LLMs&#20013;&#30340;&#31070;&#32463;&#20803;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01267</link><description>&lt;p&gt;
&#35299;&#21078;&#35821;&#35328;&#27169;&#22411;&#65306;&#36890;&#36807;&#36873;&#25321;&#24615;&#20462;&#21098;&#23454;&#29616;&#26426;&#22120;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dissecting Language Models: Machine Unlearning via Selective Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01267
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#20462;&#21098;&#31070;&#32463;&#20803;&#26469;&#23454;&#29616;&#21435;&#23398;&#20064;&#65292;&#21457;&#29616;LLMs&#20013;&#30340;&#31070;&#32463;&#20803;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#20855;&#26377;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35774;&#35745;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LLMs&#30340;&#36873;&#25321;&#24615;&#20462;&#21098;&#26041;&#27861;&#65292;&#26681;&#25454;&#31070;&#32463;&#20803;&#23545;&#29305;&#23450;&#33021;&#21147;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#26469;&#31227;&#38500;&#31070;&#32463;&#20803;&#65292;&#32780;&#38750;&#25972;&#20307;&#32593;&#32476;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#21024;&#38500;&#33021;&#22815;&#23454;&#29616;&#29305;&#23450;&#34892;&#20026;&#30340;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#20013;&#30340;&#21069;&#39304;&#31070;&#32463;&#20803;&#21644;&#27880;&#24847;&#21147;&#31070;&#32463;&#20803;&#26159;&#19987;&#38376;&#21270;&#30340;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;&#23545;&#20110;&#29305;&#23450;&#20219;&#21153;&#65292;&#26576;&#20123;&#31070;&#32463;&#20803;&#27604;&#20854;&#20182;&#31070;&#32463;&#20803;&#26356;&#20026;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01267v1 Announce Type: cross  Abstract: Understanding and shaping the behaviour of Large Language Models (LLMs) is increasingly important as applications become more powerful and more frequently adopted. This paper introduces a machine unlearning method specifically designed for LLMs. We introduce a selective pruning method for LLMs that removes neurons based on their relative importance on a targeted capability compared to overall network performance. This approach is a compute- and data-efficient method for identifying and removing neurons that enable specific behaviours. Our findings reveal that both feed-forward and attention neurons in LLMs are specialized; that is, for specific tasks, certain neurons are more crucial than others.
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#65292;&#20026;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2403.01046</link><description>&lt;p&gt;
&#19968;&#20010;&#38236;&#23376;&#30340;&#24211;&#65306;&#20302;&#32500;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#21453;&#23556;&#29305;&#24449;&#30340;&#20984;Lasso&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01046
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#65292;&#20026;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#24102;&#26377;&#22266;&#23450;&#12289;&#26126;&#30830;&#23450;&#20041;&#30340;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#12290;&#20855;&#20307;&#30340;&#23383;&#20856;&#21462;&#20915;&#20110;&#28608;&#27963;&#20989;&#25968;&#21644;&#28145;&#24230;&#12290;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#20998;&#27573;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#32593;&#32476;&#65292;&#28145;&#31364;&#30340;ReLU&#32593;&#32476;&#26368;&#22810;&#26377;4&#23618;&#65292;&#20197;&#21450;&#20855;&#26377;&#31526;&#21495;&#28608;&#27963;&#21644;&#20219;&#24847;&#28145;&#24230;&#30340;&#30697;&#24418;&#21644;&#26641;&#32593;&#32476;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;ReLU&#32593;&#32476;&#20013;&#65292;&#31532;&#22235;&#23618;&#21019;&#24314;&#20195;&#34920;&#35757;&#32451;&#25968;&#25454;&#20851;&#20110;&#33258;&#36523;&#30340;&#21453;&#23556;&#30340;&#29305;&#24449;&#12290;Lasso&#34920;&#31034;&#27861;&#25581;&#31034;&#20102;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01046v1 Announce Type: cross  Abstract: We prove that training neural networks on 1-D data is equivalent to solving a convex Lasso problem with a fixed, explicitly defined dictionary matrix of features. The specific dictionary depends on the activation and depth. We consider 2-layer networks with piecewise linear activations, deep narrow ReLU networks with up to 4 layers, and rectangular and tree networks with sign activation and arbitrary depth. Interestingly in ReLU networks, a fourth layer creates features that represent reflections of training data about themselves. The Lasso representation sheds insight to globally optimal networks and the solution landscape.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#26399;&#21002;&#25991;&#31456;&#65292;&#24635;&#32467;&#20102;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#22312;&#24494;&#29983;&#29289;&#32452;&#23398;&#20013;&#30340;&#29616;&#26377;&#23454;&#36341;&#65292;&#25506;&#35752;&#20102;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36991;&#20813;&#24120;&#35265;&#23454;&#39564;&#35774;&#35745;&#32570;&#38519;&#30340;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2402.17621</link><description>&lt;p&gt;
&#29992;&#20110;&#24494;&#29983;&#29289;&#32452;&#23398;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#65306;&#24357;&#21512;&#24403;&#21069;&#21644;&#26368;&#20339;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Supervised machine learning for microbiomics: bridging the gap between current and best practices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17621
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#26399;&#21002;&#25991;&#31456;&#65292;&#24635;&#32467;&#20102;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#22312;&#24494;&#29983;&#29289;&#32452;&#23398;&#20013;&#30340;&#29616;&#26377;&#23454;&#36341;&#65292;&#25506;&#35752;&#20102;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36991;&#20813;&#24120;&#35265;&#23454;&#39564;&#35774;&#35745;&#32570;&#38519;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#23558;&#21152;&#36895;&#20020;&#24202;&#24494;&#29983;&#29289;&#32452;&#23398;&#21019;&#26032;&#65292;&#22914;&#30142;&#30149;&#35786;&#26029;&#21644;&#39044;&#21518;&#12290;&#36825;&#23558;&#38656;&#35201;&#39640;&#36136;&#37327;&#12289;&#21487;&#37325;&#29616;&#12289;&#21487;&#35299;&#37322;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#20854;&#39044;&#27979;&#33021;&#21147;&#36798;&#21040;&#25110;&#36229;&#36807;&#30417;&#31649;&#26426;&#26500;&#23545;&#20020;&#24202;&#24037;&#20855;&#35774;&#23450;&#30340;&#39640;&#38376;&#27099;&#12290;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;2021-2022&#24180;&#21457;&#34920;&#30340;100&#31687;&#21516;&#34892;&#35780;&#35758;&#30340;&#26399;&#21002;&#25991;&#31456;&#65292;&#25429;&#25417;&#20102;&#24403;&#21069;&#23558;&#30417;&#30563;ML&#24212;&#29992;&#20110;&#24494;&#29983;&#29289;&#32452;&#23398;&#25968;&#25454;&#30340;&#23454;&#36341;&#30340;&#19968;&#20010;&#24555;&#29031;&#12290;&#25105;&#20204;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#24341;&#23548;&#35752;&#35770;&#21508;&#31181;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#21253;&#25324;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#22914;&#20309;&#20943;&#36731;&#23567;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#21516;&#26102;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20851;&#20110;&#22914;&#20309;&#36991;&#20813;&#21487;&#33021;&#25439;&#23475;&#27169;&#22411;&#24615;&#33021;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#30340;&#24120;&#35265;&#23454;&#39564;&#35774;&#35745;&#32570;&#38519;&#30340;&#25351;&#21335;&#12290;&#35752;&#35770;&#38468;&#26377;&#19968;&#20010;&#20114;&#21160;&#22312;&#32447;&#25945;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17621v1 Announce Type: cross  Abstract: Machine learning (ML) is set to accelerate innovations in clinical microbiomics, such as in disease diagnostics and prognostics. This will require high-quality, reproducible, interpretable workflows whose predictive capabilities meet or exceed the high thresholds set for clinical tools by regulatory agencies. Here, we capture a snapshot of current practices in the application of supervised ML to microbiomics data, through an in-depth analysis of 100 peer-reviewed journal articles published in 2021-2022. We apply a data-driven approach to steer discussion of the merits of varied approaches to experimental design, including key considerations such as how to mitigate the effects of small dataset size while avoiding data leakage. We further provide guidance on how to avoid common experimental design pitfalls that can hurt model performance, trustworthiness, and reproducibility. Discussion is accompanied by an interactive online tutorial th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#20110;&#25490;&#21015;&#19981;&#21464;&#25110;&#32773;&#21487;&#21152;&#21487;&#20998;&#30340;&#20998;&#35010;&#20989;&#25968;&#65292;&#39640;&#25928;&#30340;&#26080;&#20559;&#31232;&#30095;&#21270;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.14925</link><description>&lt;p&gt;
&#39640;&#25928;&#26080;&#20559;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Unbiased Sparsification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#23545;&#20110;&#25490;&#21015;&#19981;&#21464;&#25110;&#32773;&#21487;&#21152;&#21487;&#20998;&#30340;&#20998;&#35010;&#20989;&#25968;&#65292;&#39640;&#25928;&#30340;&#26080;&#20559;&#31232;&#30095;&#21270;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#21521;&#37327;$p\in \mathbb{R}^n$&#30340;&#26080;&#20559;$m$-&#31232;&#30095;&#21270;&#26159;&#19968;&#20010;&#20855;&#26377;&#24179;&#22343;&#20540;&#20026;$p$&#65292;&#26368;&#22810;&#26377;$m&lt;n$&#20010;&#38750;&#38646;&#22352;&#26631;&#30340;&#38543;&#26426;&#21521;&#37327;$Q\in \mathbb{R}^n&#12290; &#26080;&#20559;&#31232;&#30095;&#21270;&#21487;&#20197;&#21387;&#32553;&#21407;&#22987;&#21521;&#37327;&#32780;&#19981;&#24341;&#20837;&#20559;&#24046;&#65307;&#23427;&#20986;&#29616;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#65292;&#27604;&#22914;&#32852;&#37030;&#23398;&#20064;&#21644;&#37319;&#26679;&#31232;&#30095;&#27010;&#29575;&#20998;&#24067;&#12290; &#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#26080;&#20559;&#31232;&#30095;&#21270;&#36824;&#24212;&#35813;&#26368;&#23567;&#21270;&#19968;&#20010;&#24230;&#37327;$Q$&#19982;&#21407;&#22987;$p$&#20043;&#38388;&#36317;&#31163;&#26377;&#22810;&#36828;&#30340;&#20998;&#35010;&#20989;&#25968;$\mathsf{Div}(Q,p)$&#30340;&#26399;&#26395;&#20540;&#12290; &#22914;&#26524;$Q$&#22312;&#36825;&#20010;&#24847;&#20041;&#19978;&#26159;&#26368;&#20248;&#30340;&#65292;&#37027;&#20040;&#25105;&#20204;&#31216;&#20043;&#20026;&#39640;&#25928;&#12290; &#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#25551;&#36848;&#20102;&#23545;&#20110;&#26082;&#26159;&#25490;&#21015;&#19981;&#21464;&#21448;&#26159;&#21487;&#21152;&#21487;&#20998;&#30340;&#20998;&#35010;&#20989;&#25968;&#30340;&#39640;&#25928;&#26080;&#20559;&#31232;&#30095;&#21270;&#12290; &#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25490;&#21015;&#19981;&#21464;&#20998;&#35010;&#20989;&#25968;&#30340;&#34920;&#24449;&#23545;&#20110;&#20998;&#35010;&#20989;&#25968;&#30340;&#36873;&#25321;&#26159;&#20581;&#22766;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#38024;&#23545;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#30340;&#26368;&#20248;$Q$&#30340;&#31867;&#19982;&#25105;&#20204;&#30340;&#31867;&#37325;&#21512;&#20102;op
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14925v1 Announce Type: cross  Abstract: An unbiased $m$-sparsification of a vector $p\in \mathbb{R}^n$ is a random vector $Q\in \mathbb{R}^n$ with mean $p$ that has at most $m&lt;n$ nonzero coordinates. Unbiased sparsification compresses the original vector without introducing bias; it arises in various contexts, such as in federated learning and sampling sparse probability distributions. Ideally, unbiased sparsification should also minimize the expected value of a divergence function $\mathsf{Div}(Q,p)$ that measures how far away $Q$ is from the original $p$. If $Q$ is optimal in this sense, then we call it efficient. Our main results describe efficient unbiased sparsifications for divergences that are either permutation-invariant or additively separable. Surprisingly, the characterization for permutation-invariant divergences is robust to the choice of divergence function, in the sense that our class of optimal $Q$ for squared Euclidean distance coincides with our class of op
&lt;/p&gt;</description></item><item><title>SLADE&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#36793;&#32536;&#27969;&#20013;&#36805;&#36895;&#26816;&#27979;&#21160;&#24577;&#24322;&#24120;&#65292;&#26080;&#38656;&#20381;&#36182;&#26631;&#31614;&#65292;&#20027;&#35201;&#36890;&#36807;&#35266;&#23519;&#33410;&#28857;&#20132;&#20114;&#27169;&#24335;&#30340;&#20559;&#24046;&#26469;&#26816;&#27979;&#33410;&#28857;&#29366;&#24577;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2402.11933</link><description>&lt;p&gt;
SLADE&#65306;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#36793;&#32536;&#27969;&#20013;&#26816;&#27979;&#21160;&#24577;&#24322;&#24120;
&lt;/p&gt;
&lt;p&gt;
SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11933
&lt;/p&gt;
&lt;p&gt;
SLADE&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#36793;&#32536;&#27969;&#20013;&#36805;&#36895;&#26816;&#27979;&#21160;&#24577;&#24322;&#24120;&#65292;&#26080;&#38656;&#20381;&#36182;&#26631;&#31614;&#65292;&#20027;&#35201;&#36890;&#36807;&#35266;&#23519;&#33410;&#28857;&#20132;&#20114;&#27169;&#24335;&#30340;&#20559;&#24046;&#26469;&#26816;&#27979;&#33410;&#28857;&#29366;&#24577;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26816;&#27979;&#30495;&#23454;&#19990;&#30028;&#22270;&#20013;&#30340;&#24322;&#24120;&#65292;&#22914;&#31038;&#20132;&#12289;&#30005;&#23376;&#37038;&#20214;&#21644;&#37329;&#34701;&#32593;&#32476;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26041;&#27861;&#12290;&#22312;&#22823;&#22810;&#25968;&#30495;&#23454;&#19990;&#30028;&#22270;&#38543;&#26102;&#38388;&#22686;&#38271;&#65292;&#33258;&#28982;&#22320;&#34920;&#31034;&#20026;&#36793;&#32536;&#27969;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#65306;(a)&#22312;&#24322;&#24120;&#21457;&#29983;&#26102;&#21363;&#26102;&#26816;&#27979;&#24322;&#24120;&#65292;(b)&#36866;&#24212;&#21160;&#24577;&#21464;&#21270;&#30340;&#29366;&#24577;&#65292;(c)&#22788;&#29702;&#21160;&#24577;&#24322;&#24120;&#26631;&#31614;&#30340;&#31232;&#32570;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SLADE&#65288;&#36793;&#32536;&#27969;&#24322;&#24120;&#26816;&#27979;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65289;&#65292;&#29992;&#20110;&#22312;&#36793;&#32536;&#27969;&#20013;&#24555;&#36895;&#26816;&#27979;&#21160;&#24577;&#24322;&#24120;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#26631;&#31614;&#12290;SLADE&#36890;&#36807;&#35266;&#23519;&#33410;&#28857;&#22312;&#26102;&#38388;&#19978;&#30456;&#20114;&#20316;&#29992;&#27169;&#24335;&#30340;&#20559;&#24046;&#26469;&#26816;&#27979;&#33410;&#28857;&#36827;&#20837;&#24322;&#24120;&#29366;&#24577;&#30340;&#36716;&#21464;&#12290;&#20026;&#27492;&#65292;&#23427;&#35757;&#32451;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25191;&#34892;&#20004;&#20010;&#33258;&#30417;&#30563;&#20219;&#21153;&#65306;(a)&#26368;&#23567;&#21270;&#33410;&#28857;&#34920;&#31034;&#20013;&#30340;&#28418;&#31227;&#65292;(b)&#20174;&#30701;&#26399;&#29983;&#25104;&#38271;&#26399;&#20132;&#20114;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11933v1 Announce Type: new  Abstract: To detect anomalies in real-world graphs, such as social, email, and financial networks, various approaches have been developed. While they typically assume static input graphs, most real-world graphs grow over time, naturally represented as edge streams. In this context, we aim to achieve three goals: (a) instantly detecting anomalies as they occur, (b) adapting to dynamically changing states, and (c) handling the scarcity of dynamic anomaly labels. In this paper, we propose SLADE (Self-supervised Learning for Anomaly Detection in Edge Streams) for rapid detection of dynamic anomalies in edge streams, without relying on labels. SLADE detects the shifts of nodes into abnormal states by observing deviations in their interaction patterns over time. To this end, it trains a deep neural network to perform two self-supervised tasks: (a) minimizing drift in node representations and (b) generating long-term interaction patterns from short-term 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#30334;&#19975;&#38271;&#24230;&#30340;&#35270;&#39057;&#21644;&#35821;&#35328;&#24207;&#21015;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#19990;&#30028;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#35270;&#39057;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#21644;&#35821;&#35328;&#30340;&#25991;&#26412;&#30693;&#35782;&#20197;&#21450;&#36880;&#28176;&#22686;&#21152;&#19978;&#19979;&#25991;&#22823;&#23567;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;AI&#36741;&#21161;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08268</link><description>&lt;p&gt;
&#30334;&#19975;&#38271;&#24230;&#35270;&#39057;&#21644;&#35821;&#35328;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
World Model on Million-Length Video And Language With RingAttention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08268
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#30334;&#19975;&#38271;&#24230;&#30340;&#35270;&#39057;&#21644;&#35821;&#35328;&#24207;&#21015;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#19990;&#30028;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#35270;&#39057;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#21644;&#35821;&#35328;&#30340;&#25991;&#26412;&#30693;&#35782;&#20197;&#21450;&#36880;&#28176;&#22686;&#21152;&#19978;&#19979;&#25991;&#22823;&#23567;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;AI&#36741;&#21161;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#38590;&#20197;&#29992;&#25991;&#23383;&#25551;&#36848;&#30340;&#19990;&#30028;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#38271;&#31687;&#20219;&#21153;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#35270;&#39057;&#24207;&#21015;&#25552;&#20379;&#20102;&#21482;&#26377;&#35821;&#35328;&#21644;&#38745;&#24577;&#22270;&#20687;&#25152;&#19981;&#20855;&#22791;&#30340;&#23453;&#36149;&#26102;&#38388;&#20449;&#24687;&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#19982;&#35821;&#35328;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#26102;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#36825;&#31181;&#27169;&#22411;&#21487;&#20197;&#23545;&#20154;&#31867;&#30340;&#25991;&#26412;&#30693;&#35782;&#21644;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#29702;&#35299;&#65292;&#20026;&#36741;&#21161;&#20154;&#31867;&#25552;&#20379;&#26356;&#24191;&#27867;&#30340;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20174;&#30334;&#19975;&#20010;&#26631;&#35760;&#30340;&#35270;&#39057;&#21644;&#35821;&#35328;&#24207;&#21015;&#20013;&#23398;&#20064;&#38754;&#20020;&#30528;&#35760;&#24518;&#32422;&#26463;&#12289;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#26377;&#38480;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#26679;&#21270;&#35270;&#39057;&#21644;&#20070;&#31821;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#29615;&#24418;&#27880;&#24847;&#21147;&#25216;&#26415;&#23545;&#38271;&#24207;&#21015;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#35757;&#32451;&#65292;&#36880;&#28176;&#22686;&#21152;&#19978;&#19979;&#25991;&#22823;&#23567;&#20174;4K&#21040;1M&#20010;&#26631;&#35760;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22914;&#19979;&#65306;
&lt;/p&gt;
&lt;p&gt;
Current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens of video and language sequences poses challenges due to memory constraints, computational complexity, and limited datasets. To address these challenges, we curate a large dataset of diverse videos and books, utilize the RingAttention technique to scalably train on long sequences, and gradually increase context size from 4K to 1M tokens. This paper makes the following contributions: (a) Largest context size neural network: We train one of the largest context size transformers on long video and language seq
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#36827;&#21270;&#31574;&#30053;(ES)&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#20197;&#36890;&#36807;&#30452;&#25509;&#31574;&#30053;&#25628;&#32034;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22522;&#20934;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;ES&#21487;&#20197;&#22312;&#35768;&#22810;&#22522;&#20934;&#20219;&#21153;&#20013;&#25214;&#21040;&#26377;&#25928;&#30340;&#32447;&#24615;&#31574;&#30053;&#65292;&#19982;&#24403;&#21069;&#20351;&#29992;&#26356;&#22823;&#32593;&#32476;&#30340;DRL&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#22522;&#20934;&#38382;&#39064;&#27604;&#20197;&#24448;&#35748;&#20026;&#30340;&#26356;&#23481;&#26131;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.06912</link><description>&lt;p&gt;
&#29992;&#32447;&#24615;&#31574;&#30053;&#32593;&#32476;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Deep Reinforcement Learning Benchmarks with Linear Policy Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#36827;&#21270;&#31574;&#30053;(ES)&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#20197;&#36890;&#36807;&#30452;&#25509;&#31574;&#30053;&#25628;&#32034;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22522;&#20934;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;ES&#21487;&#20197;&#22312;&#35768;&#22810;&#22522;&#20934;&#20219;&#21153;&#20013;&#25214;&#21040;&#26377;&#25928;&#30340;&#32447;&#24615;&#31574;&#30053;&#65292;&#19982;&#24403;&#21069;&#20351;&#29992;&#26356;&#22823;&#32593;&#32476;&#30340;DRL&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#22522;&#20934;&#38382;&#39064;&#27604;&#20197;&#24448;&#35748;&#20026;&#30340;&#26356;&#23481;&#26131;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#20687;Atari&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#20219;&#21153;&#36825;&#26679;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#20294;&#31639;&#27861;&#22797;&#26434;&#65292;&#35757;&#32451;&#26102;&#38388;&#24448;&#24448;&#36739;&#38271;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36827;&#21270;&#31574;&#30053;(ES)&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;&#25105;&#20204;&#20351;&#29992;ES&#36890;&#36807;&#31070;&#32463;&#36827;&#21270;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#36890;&#36807;&#30452;&#25509;&#31574;&#30053;&#25628;&#32034;&#26469;&#23436;&#25104;&#12290;&#25105;&#20204;&#23545;&#24120;&#35268;&#32593;&#32476;&#21644;&#30001;&#19968;&#20010;&#20174;&#35266;&#27979;&#21040;&#21160;&#20316;&#30340;&#21333;&#19968;&#32447;&#24615;&#23618;&#32452;&#25104;&#30340;&#31574;&#30053;&#32593;&#32476;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65307;&#23545;&#20110;&#19977;&#31181;&#32463;&#20856;&#30340;ES&#26041;&#27861;&#21644;&#19977;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#22914;PPO&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ES&#21487;&#20197;&#22312;&#35768;&#22810;RL&#22522;&#20934;&#20219;&#21153;&#20013;&#25214;&#21040;&#26377;&#25928;&#30340;&#32447;&#24615;&#31574;&#30053;&#65292;&#32780;DRL&#26041;&#27861;&#21482;&#33021;&#20351;&#29992;&#26356;&#22823;&#30340;&#32593;&#32476;&#25214;&#21040;&#25104;&#21151;&#30340;&#31574;&#30053;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#22522;&#20934;&#38382;&#39064;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#26356;&#23481;&#26131;&#35299;&#20915;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#21363;&#20351;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;ES&#30340;&#32467;&#26524;&#20063;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Deep Reinforcement Learning (DRL) methods can learn effective policies for challenging problems such as Atari games and robotics tasks, algorithms are complex and training times are often long. This study investigates how evolution strategies (ES) perform compared to gradient-based deep reinforcement learning methods. We use ES to optimize the weights of a neural network via neuroevolution, performing direct policy search. We benchmark both regular networks and policy networks consisting of a single linear layer from observations to actions; for three classical ES methods and for three gradient-based methods such as PPO. Our results reveal that ES can find effective linear policies for many RL benchmark tasks, in contrast to DRL methods that can only find successful policies using much larger networks, suggesting that current benchmarks are easier to solve than previously assumed. Interestingly, also for higher complexity tasks, ES achieves results comparable to gradient-based
&lt;/p&gt;</description></item><item><title>SAE&#26159;&#19968;&#31181;&#21333;&#19968;&#26550;&#26500;&#38598;&#21512;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#38598;&#21512;&#36755;&#20837;&#30340;&#26368;&#20339;&#36864;&#20986;&#25968;&#37327;&#21644;&#28145;&#24230;&#65292;&#22312;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;&#23427;&#33021;&#22815;&#26681;&#25454;&#29305;&#23450;&#26550;&#26500;&#25110;&#24212;&#29992;&#31243;&#24207;&#28789;&#27963;&#22320;&#23450;&#21046;&#20854;&#37197;&#32622;&#12290;</title><link>https://arxiv.org/abs/2402.06580</link><description>&lt;p&gt;
SAE: &#21333;&#19968;&#26550;&#26500;&#38598;&#21512;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SAE: Single Architecture Ensemble Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06580
&lt;/p&gt;
&lt;p&gt;
SAE&#26159;&#19968;&#31181;&#21333;&#19968;&#26550;&#26500;&#38598;&#21512;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#38598;&#21512;&#36755;&#20837;&#30340;&#26368;&#20339;&#36864;&#20986;&#25968;&#37327;&#21644;&#28145;&#24230;&#65292;&#22312;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;&#23427;&#33021;&#22815;&#26681;&#25454;&#29305;&#23450;&#26550;&#26500;&#25110;&#24212;&#29992;&#31243;&#24207;&#28789;&#27963;&#22320;&#23450;&#21046;&#20854;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#19968;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#38598;&#21512;&#33021;&#22815;&#22312;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#25552;&#21069;&#36864;&#20986;&#25110;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#26694;&#26550;&#23558;&#38598;&#21512;&#21387;&#32553;&#21040;&#21333;&#19968;&#32593;&#32476;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#26223;&#35266;&#36804;&#20170;&#20026;&#27490;&#26159;&#38646;&#25955;&#30340;&#65292;&#22240;&#27492;&#24456;&#38590;&#36873;&#25321;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#31639;&#27861;&#24615;&#33021;&#33853;&#21518;&#20110;&#29420;&#31435;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#21512;&#65292;&#24182;&#38656;&#35201;&#24191;&#27867;&#30340;&#26550;&#26500;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#26041;&#27861;&#32479;&#19968;&#21040;&#21333;&#19968;&#26550;&#26500;&#38598;&#21512;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21333;&#19968;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#38598;&#21512;&#36755;&#20837;&#30340;&#26368;&#20339;&#36864;&#20986;&#25968;&#37327;&#21644;&#28145;&#24230;&#12290;&#36825;&#20351;&#24471;SAE&#26694;&#26550;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#26550;&#26500;&#25110;&#24212;&#29992;&#31243;&#24207;&#28789;&#27963;&#22320;&#23450;&#21046;&#20854;&#37197;&#32622;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#21508;&#31181;&#32593;&#32476;&#26550;&#26500;&#31867;&#22411;&#21644;&#22823;&#23567;&#19978;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;SAE&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#22522;&#32447;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#25110;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensembles of separate neural networks (NNs) have shown superior accuracy and confidence calibration over single NN across tasks. Recent methods compress ensembles within a single network via early exits or multi-input multi-output frameworks. However, the landscape of these methods is fragmented thus far, making it difficult to choose the right approach for a given task. Furthermore, the algorithmic performance of these methods is behind the ensemble of separate NNs and requires extensive architecture tuning. We propose a novel methodology unifying these approaches into a Single Architecture Ensemble (SAE). Our method learns the optimal number and depth of exits per ensemble input in a single NN. This enables the SAE framework to flexibly tailor its configuration for a given architecture or application. We evaluate SAEs on image classification and regression across various network architecture types and sizes. We demonstrate competitive accuracy or confidence calibration to baselines w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#20844;&#24179;&#25512;&#33616;&#31995;&#32479;&#65292;&#21517;&#20026;HetroFair&#65292;&#26088;&#22312;&#25552;&#39640;&#39033;&#30446;&#20391;&#30340;&#20844;&#24179;&#24615;&#12290;HetroFair&#20351;&#29992;&#20844;&#24179;&#27880;&#24847;&#21147;&#21644;&#24322;&#36136;&#24615;&#29305;&#24449;&#21152;&#26435;&#20004;&#20010;&#32452;&#20214;&#26469;&#29983;&#25104;&#20855;&#26377;&#20844;&#24179;&#24615;&#24847;&#35782;&#30340;&#23884;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.03365</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#30064;&#36136;&#21451;&#21892;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Heterophily-Aware Fair Recommendation using Graph Convolutional Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#20844;&#24179;&#25512;&#33616;&#31995;&#32479;&#65292;&#21517;&#20026;HetroFair&#65292;&#26088;&#22312;&#25552;&#39640;&#39033;&#30446;&#20391;&#30340;&#20844;&#24179;&#24615;&#12290;HetroFair&#20351;&#29992;&#20844;&#24179;&#27880;&#24847;&#21147;&#21644;&#24322;&#36136;&#24615;&#29305;&#24449;&#21152;&#26435;&#20004;&#20010;&#32452;&#20214;&#26469;&#29983;&#25104;&#20855;&#26377;&#20844;&#24179;&#24615;&#24847;&#35782;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#19981;&#20165;&#35774;&#35745;&#20026;&#20026;&#26368;&#32456;&#29992;&#25143;&#26381;&#21153;&#65292;&#36824;&#35201;&#35753;&#20854;&#20182;&#21442;&#19982;&#32773;&#65288;&#22914;&#39033;&#30446;&#21644;&#39033;&#30446;&#20379;&#24212;&#21830;&#65289;&#20174;&#20013;&#21463;&#30410;&#12290;&#36825;&#20123;&#21442;&#19982;&#32773;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#25110;&#20914;&#31361;&#30340;&#30446;&#26631;&#21644;&#21033;&#30410;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#20844;&#24179;&#24615;&#21644;&#27969;&#34892;&#24230;&#20559;&#24046;&#32771;&#34385;&#30340;&#38656;&#27714;&#12290;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#26041;&#27861;&#20063;&#38754;&#20020;&#19981;&#20844;&#24179;&#24615;&#21644;&#27969;&#34892;&#24230;&#20559;&#24046;&#30340;&#25361;&#25112;&#65292;&#20854;&#24402;&#19968;&#21270;&#21644;&#32858;&#21512;&#36807;&#31243;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#31216;&#20026;HetroFair&#65292;&#26088;&#22312;&#25552;&#39640;&#39033;&#30446;&#20391;&#30340;&#20844;&#24179;&#24615;&#12290;HetroFair&#20351;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;&#32452;&#20214;&#29983;&#25104;&#20855;&#26377;&#20844;&#24179;&#24615;&#24847;&#35782;&#30340;&#23884;&#20837;&#65306;i&#65289;&#20844;&#24179;&#27880;&#24847;&#21147;&#65292;&#23427;&#22312;GNN&#30340;&#24402;&#19968;&#21270;&#36807;&#31243;&#20013;&#32467;&#21512;&#20102;&#28857;&#31215;&#65292;&#20197;&#20943;&#23569;&#33410;&#28857;&#24230;&#25968;&#30340;&#24433;&#21709;&#65307;ii&#65289;&#24322;&#36136;&#24615;&#29305;&#24449;&#21152;&#26435;&#65292;&#20026;&#19981;&#21516;&#30340;&#29305;&#24449;&#20998;&#37197;&#19981;&#21516;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, graph neural networks (GNNs) have become a popular tool to improve the accuracy and performance of recommender systems. Modern recommender systems are not only designed to serve the end users, but also to benefit other participants, such as items and items providers. These participants may have different or conflicting goals and interests, which raise the need for fairness and popularity bias considerations. GNN-based recommendation methods also face the challenges of unfairness and popularity bias and their normalization and aggregation processes suffer from these challenges. In this paper, we propose a fair GNN-based recommender system, called HetroFair, to improve items' side fairness. HetroFair uses two separate components to generate fairness-aware embeddings: i) fairness-aware attention which incorporates dot product in the normalization process of GNNs, to decrease the effect of nodes' degrees, and ii) heterophily feature weighting to assign distinct weights to 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#24863;&#30693;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#65292;&#29992;&#20110;predict-then-optimize&#26694;&#26550;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#35777;&#25454;&#35777;&#23454;&#20102;&#20854;&#22312;&#35823;&#35774;&#32622;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03256</link><description>&lt;p&gt;
&#23398;&#20064;Predict-then-Optimize&#26694;&#26550;&#20013;&#30340;&#26368;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Best-in-Class Policies for the Predict-then-Optimize Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03256
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#24863;&#30693;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#65292;&#29992;&#20110;predict-then-optimize&#26694;&#26550;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#35777;&#25454;&#35777;&#23454;&#20102;&#20854;&#22312;&#35823;&#35774;&#32622;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#24863;&#30693;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#65292;&#31216;&#20026;Perturbation Gradient&#65288;PG&#65289;&#25439;&#22833;&#65292;&#29992;&#20110;predict-then-optimize&#26694;&#26550;&#12290;&#36825;&#20123;&#25439;&#22833;&#30452;&#25509;&#36817;&#20284;&#20102;&#19979;&#28216;&#20915;&#31574;&#25439;&#22833;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#19982;&#29616;&#26377;&#30340;&#26367;&#20195;&#25439;&#22833;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;PG&#25439;&#22833;&#30340;&#36817;&#20284;&#35823;&#24046;&#38543;&#30528;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#28040;&#22833;&#12290;&#36825;&#24847;&#21619;&#30528;&#20248;&#21270;&#25105;&#20204;&#30340;&#26367;&#20195;&#25439;&#22833;&#21487;&#20197;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#24471;&#21040;&#26368;&#20339;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#35823;&#35774;&#32622;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#35823;&#35774;&#32622;&#19979;&#30340;&#36825;&#26679;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#35777;&#25454;&#35777;&#23454;&#20102;&#24403;&#22522;&#30784;&#27169;&#22411;&#35823;&#35774;&#32622;&#19988;&#22122;&#22768;&#19981;&#26159;&#20013;&#24515;&#23545;&#31216;&#26102;&#65292;&#25105;&#20204;&#30340;PG&#25439;&#22833;&#22312;&#23454;&#36341;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#25552;&#26696;&#12290;&#37492;&#20110;&#22312;&#23454;&#36341;&#20013;&#35823;&#35774;&#32622;&#24456;&#24120;&#35265;--&#29305;&#21035;&#26159;&#24403;&#25105;&#20204;&#21487;&#33021;&#26356;&#21916;&#27426;&#19968;&#20010;&#26356;&#31616;&#21333;&#12289;&#26356;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26102;--PG&#25439;&#22833;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#29702;&#35770;&#19978;&#26377;&#20381;&#25454;&#30340;&#12289;&#21487;&#35745;&#31639;&#30340;&#20915;&#31574;&#24863;&#30693;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel family of decision-aware surrogate losses, called Perturbation Gradient (PG) losses, for the predict-then-optimize framework. These losses directly approximate the downstream decision loss and can be optimized using off-the-shelf gradient-based methods. Importantly, unlike existing surrogate losses, the approximation error of our PG losses vanishes as the number of samples grows. This implies that optimizing our surrogate loss yields a best-in-class policy asymptotically, even in misspecified settings. This is the first such result in misspecified settings and we provide numerical evidence confirming our PG losses substantively outperform existing proposals when the underlying model is misspecified and the noise is not centrally symmetric. Insofar as misspecification is commonplace in practice -- especially when we might prefer a simpler, more interpretable model -- PG losses offer a novel, theoretically justified, method for computationally tractable decision-aware 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#25216;&#26415;&#35282;&#24230;&#20840;&#38754;&#27010;&#36848;&#20102;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#29256;&#26435;&#20445;&#25252;&#38382;&#39064;&#65292;&#21253;&#25324;&#25968;&#25454;&#29256;&#26435;&#21644;&#27169;&#22411;&#29256;&#26435;&#20004;&#20010;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21019;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.02333</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#29256;&#26435;&#20445;&#25252;&#65306;&#25216;&#26415;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Copyright Protection in Generative AI: A Technical Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#25216;&#26415;&#35282;&#24230;&#20840;&#38754;&#27010;&#36848;&#20102;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#29256;&#26435;&#20445;&#25252;&#38382;&#39064;&#65292;&#21253;&#25324;&#25968;&#25454;&#29256;&#26435;&#21644;&#27169;&#22411;&#29256;&#26435;&#20004;&#20010;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21019;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;Generative AI&#65289;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#25193;&#23637;&#20102;&#20854;&#21019;&#24314;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#20195;&#30721;&#31561;&#21512;&#25104;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;Deep Generative Models&#65292;DGMs&#65289;&#29983;&#25104;&#30340;&#20869;&#23481;&#39640;&#20445;&#30495;&#24230;&#21644;&#30495;&#23454;&#24615;&#24341;&#21457;&#20102;&#37325;&#22823;&#30340;&#29256;&#26435;&#38382;&#39064;&#12290;&#20851;&#20110;&#22914;&#20309;&#26377;&#25928;&#20445;&#25252;DGMs&#20013;&#30340;&#29256;&#26435;&#38382;&#39064;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#21508;&#31181;&#27861;&#24459;&#36777;&#35770;&#12290;&#26412;&#25991;&#20174;&#25216;&#26415;&#35282;&#24230;&#25552;&#20379;&#20102;&#29256;&#26435;&#20445;&#25252;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#20174;&#20004;&#20010;&#19981;&#21516;&#30340;&#35270;&#35282;&#26469;&#36827;&#34892;&#30740;&#31350;&#65306;&#19968;&#26159;&#19982;&#25968;&#25454;&#25152;&#26377;&#32773;&#25152;&#25345;&#26377;&#30340;&#28304;&#25968;&#25454;&#30456;&#20851;&#30340;&#29256;&#26435;&#65292;&#20108;&#26159;&#19982;&#27169;&#22411;&#26500;&#24314;&#32773;&#25152;&#32500;&#25252;&#30340;&#29983;&#25104;&#27169;&#22411;&#30456;&#20851;&#30340;&#29256;&#26435;&#12290;&#23545;&#20110;&#25968;&#25454;&#29256;&#26435;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#25152;&#26377;&#32773;&#22914;&#20309;&#20445;&#25252;&#20854;&#20869;&#23481;&#65292;&#24182;&#22312;&#19981;&#20405;&#29359;&#36825;&#20123;&#26435;&#21033;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;DGMs&#12290;&#23545;&#20110;&#27169;&#22411;&#29256;&#26435;&#65292;&#25105;&#20204;&#30340;&#35752;&#35770;&#24310;&#20280;&#21040;&#38450;&#27490;&#27169;&#22411;&#30423;&#31363;&#21644;&#35782;&#21035;&#29305;&#23450;&#27169;&#22411;&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#20123;&#21019;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#26469;&#22788;&#29702;&#36825;&#20123;&#29256;&#26435;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2401.17505</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#31661;&#22836;
&lt;/p&gt;
&lt;p&gt;
Arrows of Time for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#26041;&#21521;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#21457;&#29616;&#36825;&#31867;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#65306;&#39044;&#27979;&#19979;&#19968;&#20010;&#35760;&#21495;&#21644;&#39044;&#27979;&#21069;&#19968;&#20010;&#35760;&#21495;&#26102;&#30340;&#24179;&#22343;&#23545;&#25968;&#22256;&#24785;&#24230;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#31181;&#24046;&#24322;&#26082;&#24494;&#22937;&#21448;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#35821;&#35328;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#26102;&#38388;&#31561;&#65289;&#19979;&#38750;&#24120;&#19968;&#33268;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#22312;&#29702;&#35770;&#19978;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#65292;&#19981;&#24212;&#35813;&#23384;&#22312;&#36825;&#26679;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#22914;&#20309;&#20986;&#29616;&#22312;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#32771;&#34385;&#20013;&#65292;&#24182;&#27010;&#36848;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#24102;&#26469;&#30340;&#19968;&#20123;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the probabilistic modeling performed by Autoregressive Large Language Models through the angle of time directionality. We empirically find a time asymmetry exhibited by such models in their ability to model natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#32508;&#21512;&#27010;&#36848;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#31561;&#26041;&#38754;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.09769</link><description>&lt;p&gt;
&#36208;&#21521;&#24322;&#36136;&#22270;&#23398;&#20064;&#65306;&#36827;&#23637;&#19982;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Towards Learning from Graphs with Heterophily: Progress and Future. (arXiv:2401.09769v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32508;&#21512;&#27010;&#36848;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#31561;&#26041;&#38754;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26159;&#29992;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#23454;&#20307;&#20043;&#38388;&#22797;&#26434;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#24322;&#36136;&#22270;&#65292;&#20854;&#20013;&#36830;&#25509;&#30340;&#33410;&#28857;&#24448;&#24448;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#25110;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#25214;&#21040;&#20102;&#35768;&#22810;&#24212;&#29992;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20154;&#20204;&#20063;&#22312;&#19981;&#26029;&#21162;&#21147;&#25512;&#36827;&#20174;&#24322;&#36136;&#22270;&#20013;&#23398;&#20064;&#12290;&#34429;&#28982;&#26377;&#20851;&#35813;&#20027;&#39064;&#30340;&#35843;&#26597;&#23384;&#22312;&#65292;&#20294;&#23427;&#20204;&#21482;&#20851;&#27880;&#20110;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#32780;&#24573;&#30053;&#20102;&#24322;&#36136;&#22270;&#23398;&#20064;&#30340;&#20854;&#20182;&#23376;&#20027;&#39064;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;180&#22810;&#31687;&#35770;&#25991;&#65292;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#23618;&#27425;&#20998;&#31867;&#27861;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#31867;&#65292;&#21253;&#25324;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs are structured data that models complex relations between real-world entities. Heterophilous graphs, where linked nodes are prone to be with different labels or dissimilar features, have recently attracted significant attention and found many applications. Meanwhile, increasing efforts have been made to advance learning from heterophilous graphs. Although there exist surveys on the relevant topic, they focus on heterophilous GNNs, which are only sub-topics of heterophilous graph learning. In this survey, we comprehensively overview existing works on learning from graphs with heterophily.First, we collect over 180 publications and introduce the development of this field. Then, we systematically categorize existing methods based on a hierarchical taxonomy including learning strategies, model architectures and practical applications. Finally, we discuss the primary challenges of existing studies and highlight promising avenues for future research.More publication details and corres
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22797;&#21512;&#24335;&#21644;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#30340;&#26032;&#30340;&#38543;&#26426;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#35009;&#26799;&#24230;&#24046;&#20540;&#23454;&#29616;&#20102;&#32039;&#33268;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.01860</link><description>&lt;p&gt;
&#39640;&#27010;&#29575;&#19979;&#20855;&#26377;&#37325;&#23614;&#22122;&#22768;&#30340;&#22797;&#21512;&#24335;&#21644;&#20998;&#24067;&#24335;&#38543;&#26426;&#26368;&#23567;&#21270;&#21644;&#21464;&#20998;&#19981;&#31561;&#24335;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise. (arXiv:2310.01860v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01860
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22797;&#21512;&#24335;&#21644;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#30340;&#26032;&#30340;&#38543;&#26426;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#35009;&#26799;&#24230;&#24046;&#20540;&#23454;&#29616;&#20102;&#32039;&#33268;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#20855;&#26377;&#36731;&#24494;&#22122;&#22768;&#20551;&#35774;&#30340;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#30340;&#39640;&#27010;&#29575;&#20998;&#26512;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#24403;&#22122;&#22768;&#26159;&#37325;&#23614;&#30340;&#26102;&#20505;&#65292;&#26799;&#24230;&#21098;&#35009;&#26159;&#25512;&#23548;&#20986;&#33391;&#22909;&#30340;&#39640;&#27010;&#29575;&#20445;&#35777;&#30340;&#20851;&#38190;&#31639;&#27861;&#35201;&#32032;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#21152;&#20197;&#22788;&#29702;&#22320;&#23454;&#29616;&#65292;&#21098;&#35009;&#25805;&#20316;&#20250;&#30772;&#22351;&#24120;&#29992;&#30340;&#22797;&#21512;&#24335;&#21644;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#65288;&#22914;Prox-SGD/Parallel SGD&#65289;&#30340;&#25910;&#25947;&#24615;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20219;&#20309;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#39640;&#27010;&#29575;&#20998;&#26512;&#30340;&#24037;&#20316;&#20165;&#32771;&#34385;&#26080;&#32422;&#26463;&#30340;&#38750;&#20998;&#24067;&#24335;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#22797;&#21512;&#24335;/&#20998;&#24067;&#24335;&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#32467;&#26524;&#24182;&#19981;&#21253;&#25324;&#19968;&#20123;&#37325;&#35201;&#30340;&#29305;&#27530;&#24773;&#20917;&#65288;&#22914;&#24378;&#20984;&#38382;&#39064;&#65289;&#65292;&#20063;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#24046;&#20540;&#21098;&#35009;&#30340;&#22797;&#21512;&#24335;&#21644;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#26032;&#30340;&#38543;&#26426;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#32039;&#33268;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#24615;&#32467;&#26524;&#65288;&#21253;&#25324;&#20960;&#20046;&#25152;&#26377;&#30340;&#22330;&#26223;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-probability analysis of stochastic first-order optimization methods under mild assumptions on the noise has been gaining a lot of attention in recent years. Typically, gradient clipping is one of the key algorithmic ingredients to derive good high-probability guarantees when the noise is heavy-tailed. However, if implemented na\"ively, clipping can spoil the convergence of the popular methods for composite and distributed optimization (Prox-SGD/Parallel SGD) even in the absence of any noise. Due to this reason, many works on high-probability analysis consider only unconstrained non-distributed problems, and the existing results for composite/distributed problems do not include some important special cases (like strongly convex problems) and are not optimal. To address this issue, we propose new stochastic methods for composite and distributed optimization based on the clipping of stochastic gradient differences and prove tight high-probability convergence results (including nearly
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#40065;&#26834;&#27169;&#22411;&#23545;&#21487;&#25805;&#20316;&#35299;&#37322;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#27604;&#36739;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#21644;&#38750;&#40065;&#26834;&#27169;&#22411;&#29983;&#25104;&#30340;&#36861;&#32034;&#32467;&#26524;&#30340;&#25104;&#26412;&#21644;&#26377;&#25928;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.16452</link><description>&lt;p&gt;
&#20851;&#20110;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#21487;&#25805;&#20316;&#35299;&#37322;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
On the Trade-offs between Adversarial Robustness and Actionable Explanations. (arXiv:2309.16452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#40065;&#26834;&#27169;&#22411;&#23545;&#21487;&#25805;&#20316;&#35299;&#37322;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#27604;&#36739;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#21644;&#38750;&#40065;&#26834;&#27169;&#22411;&#29983;&#25104;&#30340;&#36861;&#32034;&#32467;&#26524;&#30340;&#25104;&#26412;&#21644;&#26377;&#25928;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#20165;&#20855;&#26377;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#36824;&#33021;&#21521;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#26159;&#21542;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#36825;&#20004;&#20010;&#27010;&#24565;&#65292;&#25110;&#32773;&#23427;&#20204;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#30740;&#31350;&#23545;&#25239;&#24615;&#40065;&#26834;&#27169;&#22411;&#23545;&#21487;&#25805;&#20316;&#35299;&#37322;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#35299;&#37322;&#20026;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#20102;&#36861;&#32034;&#26435;&#21033;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#23618;&#38754;&#19978;&#20998;&#26512;&#20102;&#20808;&#36827;&#31639;&#27861;&#29983;&#25104;&#30340;&#36861;&#32034;&#32467;&#26524;&#30340;&#25104;&#26412;&#65288;&#23454;&#26045;&#30340;&#23481;&#26131;&#31243;&#24230;&#65289;&#21644;&#26377;&#25928;&#24615;&#65288;&#33719;&#24471;&#27491;&#21521;&#27169;&#22411;&#39044;&#27979;&#30340;&#27010;&#29575;&#65289;&#65292;&#24182;&#27604;&#36739;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#21644;&#38750;&#40065;&#26834;&#27169;&#22411;&#29983;&#25104;&#30340;&#36861;&#32034;&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#23545;&#25239;&#24615;&#40065;&#26834;&#21644;&#38750;&#40065;&#26834;&#27169;&#22411;&#29983;&#25104;&#30340;&#36861;&#32034;&#32467;&#26524;&#30340;&#25104;&#26412;&#21644;&#26377;&#25928;&#24615;&#20043;&#38388;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning models are increasingly being employed in various high-stakes settings, it becomes important to ensure that predictions of these models are not only adversarially robust, but also readily explainable to relevant stakeholders. However, it is unclear if these two notions can be simultaneously achieved or if there exist trade-offs between them. In this work, we make one of the first attempts at studying the impact of adversarially robust models on actionable explanations which provide end users with a means for recourse. We theoretically and empirically analyze the cost (ease of implementation) and validity (probability of obtaining a positive model prediction) of recourses output by state-of-the-art algorithms when the underlying models are adversarially robust vs. non-robust. More specifically, we derive theoretical bounds on the differences between the cost and the validity of the recourses generated by state-of-the-art algorithms for adversarially robust vs. non-ro
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36125;&#21494;&#26031;&#33258;&#36866;&#24212;&#26102;&#21051;&#27491;&#21017;&#21270;&#30340;&#40065;&#26834;&#24615;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#26377;&#25928;&#22320;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#36731;&#37327;&#32423;&#21644;&#20219;&#21153;&#23454;&#39564;&#23460;&#31561;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.08546</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#33258;&#36866;&#24212;&#26102;&#21051;&#27491;&#21017;&#21270;&#30340;&#40065;&#26834;&#24615;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Continual Learning with Bayesian Adaptive Moment Regularization. (arXiv:2309.08546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08546
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#33258;&#36866;&#24212;&#26102;&#21051;&#27491;&#21017;&#21270;&#30340;&#40065;&#26834;&#24615;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#26377;&#25928;&#22320;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#36731;&#37327;&#32423;&#21644;&#20219;&#21153;&#23454;&#39564;&#23460;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36861;&#27714;&#38271;&#26399;&#33258;&#20027;&#24615;&#65292;&#26426;&#22120;&#20154;&#20195;&#29702;&#24517;&#39035;&#19981;&#26029;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#24182;&#23398;&#20064;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;&#25345;&#32493;&#23398;&#20064;&#35797;&#22270;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#25361;&#25112;&#65292;&#21363;&#23398;&#20064;&#35299;&#20915;&#26032;&#20219;&#21153;&#23548;&#33268;&#27169;&#22411;&#24536;&#35760;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#22522;&#20110;&#20808;&#39564;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#23545;&#20110;&#26426;&#22120;&#20154;&#24212;&#29992;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#31354;&#38388;&#25928;&#29575;&#19978;&#24456;&#39640;&#65292;&#24182;&#19988;&#36890;&#24120;&#19981;&#20250;&#38543;&#30528;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#23613;&#31649;&#20855;&#26377;&#36825;&#20123;&#29702;&#24819;&#30340;&#29305;&#24615;&#65292;&#20294;&#22522;&#20110;&#20808;&#39564;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#37325;&#35201;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#22833;&#36133;&#65292;&#22240;&#27492;&#19982;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#28508;&#22312;&#24212;&#29992;&#26041;&#38754;&#26377;&#38480;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#33258;&#36866;&#24212;&#26102;&#21051;&#27491;&#21017;&#21270;&#65288;BAdam&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20808;&#39564;&#30340;&#26041;&#27861;&#65292;&#23427;&#26356;&#22909;&#22320;&#32422;&#26463;&#21442;&#25968;&#22686;&#38271;&#65292;&#38477;&#20302;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#20855;&#26377;&#19968;&#31995;&#21015;&#29702;&#24819;&#30340;&#29305;&#24615;&#65292;&#20363;&#22914;&#36731;&#37327;&#32423;&#21644;&#20219;&#21153;&#23454;&#39564;&#23460;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pursuit of long-term autonomy mandates that robotic agents must continuously adapt to their changing environments and learn to solve new tasks. Continual learning seeks to overcome the challenge of catastrophic forgetting, where learning to solve new tasks causes a model to forget previously learnt information. Prior-based continual learning methods are appealing for robotic applications as they are space efficient and typically do not increase in computational complexity as the number of tasks grows. Despite these desirable properties, prior-based approaches typically fail on important benchmarks and consequently are limited in their potential applications compared to their memory-based counterparts. We introduce Bayesian adaptive moment regularization (BAdam), a novel prior-based method that better constrains parameter growth, leading to lower catastrophic forgetting. Our method boasts a range of desirable properties for robotic applications such as being lightweight and task lab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SAH-GNN&#65292;&#19968;&#31181;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24212;&#29992;&#32467;&#26500;&#24863;&#30693;&#30340;&#36763;&#31995;&#32479;&#21704;&#23494;&#39039;&#23884;&#20837;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;SAH-GNN&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#23398;&#20064;&#36763;&#32467;&#26500;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#20934;&#36763;&#32467;&#26500;&#24418;&#24335;&#30340;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#22270;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25345;&#29289;&#29702;&#24847;&#20041;&#19978;&#30340;&#33021;&#37327;&#23432;&#24658;&#12290;</title><link>http://arxiv.org/abs/2309.04885</link><description>&lt;p&gt;
&#32467;&#26500;&#24863;&#30693;&#30340;&#36763;&#31995;&#32479;&#21704;&#23494;&#39039;&#65288;&#22270;&#65289;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Symplectic Structure-Aware Hamiltonian (Graph) Embeddings. (arXiv:2309.04885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SAH-GNN&#65292;&#19968;&#31181;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24212;&#29992;&#32467;&#26500;&#24863;&#30693;&#30340;&#36763;&#31995;&#32479;&#21704;&#23494;&#39039;&#23884;&#20837;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;SAH-GNN&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#23398;&#20064;&#36763;&#32467;&#26500;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#20934;&#36763;&#32467;&#26500;&#24418;&#24335;&#30340;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#22270;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25345;&#29289;&#29702;&#24847;&#20041;&#19978;&#30340;&#33021;&#37327;&#23432;&#24658;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#65292;&#22266;&#23450;&#23884;&#20837;&#27969;&#24418;&#30340;&#20551;&#35774;&#24120;&#24120;&#38480;&#21046;&#20102;&#20854;&#23545;&#19981;&#21516;&#22270;&#20960;&#20309;&#32467;&#26500;&#30340;&#36866;&#24212;&#24615;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;GNNs&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#23450;&#24459;&#32435;&#20837;&#33410;&#28857;&#29305;&#24449;&#26356;&#26032;&#20013;&#65292;&#26469;&#35299;&#20915;&#36825;&#31867;&#23884;&#20837;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SAH-GNN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#25512;&#24191;&#21040;&#26356;&#28789;&#27963;&#30340;&#33410;&#28857;&#29305;&#24449;&#26356;&#26032;&#20013;&#12290;&#19982;&#29616;&#26377;&#30340;&#21463;&#21704;&#23494;&#39039;&#21551;&#21457;&#30340;GNNs&#19981;&#21516;&#65292;SAH-GNN&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#36763;&#26031;&#33922;&#36153;&#23572;&#27969;&#24418;&#19978;&#30340;&#40654;&#26364;&#20248;&#21270;&#65292;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#28508;&#22312;&#30340;&#36763;&#32467;&#26500;&#65292;&#20174;&#32780;&#35268;&#36991;&#20102;&#29616;&#26377;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#20934;&#36763;&#32467;&#26500;&#24418;&#24335;&#30340;&#21704;&#23494;&#39039;GNNs&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#19968;&#21019;&#26032;&#20351;&#24471;SAH-GNN&#33021;&#22815;&#22312;&#27809;&#26377;&#22823;&#37327;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#36866;&#24212;&#21508;&#31181;&#22270;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#33021;&#37327;&#23432;&#24658;&#65292;&#20351;&#24471;&#38544;&#24335;&#21704;&#23494;&#39039;&#31995;&#32479;&#20855;&#26377;&#29289;&#29702;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traditional Graph Neural Networks (GNNs), the assumption of a fixed embedding manifold often limits their adaptability to diverse graph geometries. Recently, Hamiltonian system-inspired GNNs are proposed to address the dynamic nature of such embeddings by incorporating physical laws into node feature updates. In this work, we present SAH-GNN, a novel approach that generalizes Hamiltonian dynamics for more flexible node feature updates. Unlike existing Hamiltonian-inspired GNNs, SAH-GNN employs Riemannian optimization on the symplectic Stiefel manifold to adaptively learn the underlying symplectic structure during training, circumventing the limitations of existing Hamiltonian GNNs that rely on a pre-defined form of standard symplectic structure. This innovation allows SAH-GNN to automatically adapt to various graph datasets without extensive hyperparameter tuning. Moreover, it conserves energy during training such that the implicit Hamiltonian system is physically meaningful. To thi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33647;&#29289;&#19987;&#21033;&#21644;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#20855;&#26377;&#25216;&#26415;&#28508;&#21147;&#21644;&#31185;&#23398;&#35777;&#25454;&#30340;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#12290;&#36890;&#36807;&#26500;&#24314;&#31185;&#23398;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#21644;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#21487;&#20197;&#32508;&#21512;&#20998;&#26512;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#20026;&#33647;&#29289;&#20877;&#23450;&#20301;&#30740;&#31350;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2309.03227</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#25581;&#31034;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#30340;&#25216;&#26415;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates. (arXiv:2309.03227v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33647;&#29289;&#19987;&#21033;&#21644;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#20855;&#26377;&#25216;&#26415;&#28508;&#21147;&#21644;&#31185;&#23398;&#35777;&#25454;&#30340;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#12290;&#36890;&#36807;&#26500;&#24314;&#31185;&#23398;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#21644;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#21487;&#20197;&#32508;&#21512;&#20998;&#26512;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#20026;&#33647;&#29289;&#20877;&#23450;&#20301;&#30740;&#31350;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#20877;&#23450;&#20301;&#26159;&#19968;&#31181;&#21457;&#29616;&#29616;&#26377;&#33647;&#29289;&#26032;&#27835;&#30103;&#29992;&#36884;&#30340;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#65292;&#36817;&#24180;&#26469;&#22312;&#35745;&#31639;&#31185;&#23398;&#25991;&#29486;&#20013;&#20351;&#29992;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#30340;&#25216;&#26415;&#28508;&#21147;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32508;&#21512;&#20998;&#26512;&#33647;&#29289;&#19987;&#21033;&#21644;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#31561;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#35782;&#21035;&#20855;&#26377;&#25216;&#26415;&#28508;&#21147;&#21644;&#31185;&#23398;&#35777;&#25454;&#30340;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31185;&#23398;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65288;s-BKG&#65289;&#65292;&#21253;&#25324;&#26469;&#33258;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#30340;&#33647;&#29289;&#12289;&#30142;&#30149;&#21644;&#22522;&#22240;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#35782;&#21035;&#22312;s-BKG&#20013;&#19982;&#30446;&#26631;&#30142;&#30149;&#20851;&#32852;&#26377;&#38480;&#20294;&#22312;&#31354;&#38388;&#19978;&#32039;&#23494;&#30456;&#37051;&#30340;&#33647;&#29289;&#20316;&#20026;&#28508;&#22312;&#30340;&#33647;&#29289;&#20505;&#36873;&#29289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#33647;&#29289;&#19987;&#21033;&#20449;&#24687;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65288;p-BKG&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug repositioning-a promising strategy for discovering new therapeutic uses for existing drugs-has been increasingly explored in the computational science literature using biomedical databases. However, the technological potential of drug repositioning candidates has often been overlooked. This study presents a novel protocol to comprehensively analyse various sources such as pharmaceutical patents and biomedical databases, and identify drug repositioning candidates with both technological potential and scientific evidence. To this end, first, we constructed a scientific biomedical knowledge graph (s-BKG) comprising relationships between drugs, diseases, and genes derived from biomedical databases. Our protocol involves identifying drugs that exhibit limited association with the target disease but are closely located in the s-BKG, as potential drug candidates. We constructed a patent-informed biomedical knowledge graph (p-BKG) by adding pharmaceutical patent information. Finally, we d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20174;&#35821;&#38899;&#35760;&#24405;&#20013;&#26816;&#27979;&#21897;&#30284;&#30340;&#25991;&#29486;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#21457;&#29616;&#20102;22&#31687;&#30456;&#20851;&#35770;&#25991;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#25552;&#21462;&#38899;&#39057;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#20102;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.09230</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#35821;&#38899;&#20449;&#21495;&#20013;&#26816;&#27979;&#21897;&#30284;&#65306;&#21487;&#22797;&#29616;&#30340;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Detecting Throat Cancer from Speech Signals Using Machine Learning: A Reproducible Literature Review. (arXiv:2307.09230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20174;&#35821;&#38899;&#35760;&#24405;&#20013;&#26816;&#27979;&#21897;&#30284;&#30340;&#25991;&#29486;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#21457;&#29616;&#20102;22&#31687;&#30456;&#20851;&#35770;&#25991;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#25552;&#21462;&#38899;&#39057;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#20102;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20174;&#35821;&#38899;&#35760;&#24405;&#20013;&#26816;&#27979;&#21897;&#30284;&#30340;&#24403;&#21069;&#25991;&#29486;&#36827;&#34892;&#20102;&#33539;&#22260;&#35780;&#20272;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;22&#31687;&#30456;&#20851;&#35770;&#25991;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#35770;&#25991;&#20998;&#20026;&#20004;&#32452; - &#20061;&#31687;&#36827;&#34892;&#20108;&#20998;&#31867;&#65292;13&#31687;&#36827;&#34892;&#22810;&#31867;&#21035;&#20998;&#31867;&#12290;&#36825;&#20123;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#20854;&#20013;&#26368;&#24120;&#35265;&#30340;&#26159;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#20998;&#31867;&#20043;&#21069;&#36824;&#20174;&#38899;&#39057;&#20013;&#25552;&#21462;&#20102;&#35768;&#22810;&#29305;&#24449;&#65292;&#20854;&#20013;&#26368;&#24120;&#35265;&#30340;&#26159;&#26757;&#23572;&#39057;&#29575;&#20498;&#35889;&#31995;&#25968;&#12290;&#22312;&#36825;&#27425;&#25628;&#32034;&#20013;&#26410;&#25214;&#21040;&#20219;&#20309;&#24102;&#26377;&#20195;&#30721;&#24211;&#30340;&#35770;&#25991;&#65292;&#22240;&#27492;&#26080;&#27861;&#22797;&#29616;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20195;&#30721;&#24211;&#26469;&#35757;&#32451;&#33258;&#24049;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22810;&#31867;&#21035;&#38382;&#39064;&#19978;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#65292;&#23558;&#19977;&#31181;&#30149;&#29702;&#21644;&#20581;&#24247;&#23545;&#29031;&#36827;&#34892;&#20998;&#31867;&#12290;&#20351;&#29992;&#36825;&#31181;&#25216;&#26415;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;53.54%&#30340;&#21152;&#26435;&#24179;&#22343;&#21484;&#22238;&#29575;&#12289;83.14%&#30340;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we perform a scoping review of the current literature on the detection of throat cancer from speech recordings using machine learning and artificial intelligence. We find 22 papers within this area and discuss their methods and results. We split these papers into two groups - nine performing binary classification, and 13 performing multi-class classification. The papers present a range of methods with neural networks being most commonly implemented. Many features are also extracted from the audio before classification, with the most common bring mel-frequency cepstral coefficients. None of the papers found in this search have associated code repositories and as such are not reproducible. Therefore, we create a publicly available code repository of our own classifiers. We use transfer learning on a multi-class problem, classifying three pathologies and healthy controls. Using this technique we achieve an unweighted average recall of 53.54%, sensitivity of 83.14%, and specif
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#20248;&#21270;&#22120;&#30340;&#20449;&#24687;&#20934;&#21017;(OIC)&#30340;&#36890;&#29992;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#24110;&#21161;&#35299;&#20915;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#20013;&#30340;&#20048;&#35266;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30452;&#25509;&#36817;&#20284;&#19968;&#38454;&#20559;&#24046;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#35299;&#20915;&#39069;&#22806;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#26159;&#22312;&#20915;&#31574;&#36873;&#25321;&#26041;&#38754;&#30340;&#19968;&#20010;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2306.10081</link><description>&lt;p&gt;
&#20248;&#21270;&#22120;&#30340;&#20449;&#24687;&#20934;&#21017;&#65306;&#21078;&#26512;&#21644;&#32416;&#27491;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#20013;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Optimizer's Information Criterion: Dissecting and Correcting Bias in Data-Driven Optimization. (arXiv:2306.10081v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10081
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#20248;&#21270;&#22120;&#30340;&#20449;&#24687;&#20934;&#21017;(OIC)&#30340;&#36890;&#29992;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#24110;&#21161;&#35299;&#20915;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#20013;&#30340;&#20048;&#35266;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30452;&#25509;&#36817;&#20284;&#19968;&#38454;&#20559;&#24046;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#35299;&#20915;&#39069;&#22806;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#26159;&#22312;&#20915;&#31574;&#36873;&#25321;&#26041;&#38754;&#30340;&#19968;&#20010;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#20248;&#21270;&#20013;&#65292;&#25152;&#24471;&#20915;&#31574;&#30340;&#26679;&#26412;&#34920;&#29616;&#36890;&#24120;&#23384;&#22312;&#30528;&#23545;&#30495;&#23454;&#34920;&#29616;&#30340;&#20048;&#35266;&#20559;&#24046;&#65292;&#36825;&#31181;&#29616;&#35937;&#36890;&#24120;&#34987;&#31216;&#20026;&#20248;&#21270;&#22120;&#30340;&#35781;&#21650;&#65292;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#23494;&#20999;&#30456;&#20851;&#12290;&#20256;&#32479;&#30340;&#32416;&#27491;&#36825;&#31181;&#20559;&#24046;&#30340;&#25216;&#26415;&#65292;&#22914;&#20132;&#21449;&#39564;&#35777;&#65292;&#38656;&#35201;&#21453;&#22797;&#35299;&#20915;&#39069;&#22806;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#27492;&#35745;&#31639;&#20195;&#20215;&#24456;&#39640;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#24314;&#31435;&#22312;&#25105;&#20204;&#31216;&#20043;&#20026;&#20248;&#21270;&#22120;&#30340;&#20449;&#24687;&#20934;&#21017;&#65288;OIC&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#30452;&#25509;&#36817;&#20284;&#19968;&#38454;&#20559;&#24046;&#65292;&#19981;&#38656;&#35201;&#35299;&#20915;&#20219;&#20309;&#39069;&#22806;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;OIC&#23558;&#33879;&#21517;&#30340;&#36196;&#27744;&#20449;&#24687;&#20934;&#21017;&#25512;&#24191;&#21040;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#20013;&#65292;&#20851;&#38190;&#26159;&#35780;&#20272;&#23458;&#35266;&#34920;&#29616;&#65292;&#19981;&#20165;&#28041;&#21450;&#27169;&#22411;&#25311;&#21512;&#65292;&#36824;&#28041;&#21450;&#20854;&#19982;&#19979;&#28216;&#20248;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#20915;&#31574;&#36873;&#25321;&#32780;&#19981;&#20165;&#20165;&#26159;&#27169;&#22411;&#36873;&#25321;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#38382;&#39064;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In data-driven optimization, the sample performance of the obtained decision typically incurs an optimistic bias against the true performance, a phenomenon commonly known as the Optimizer's Curse and intimately related to overfitting in machine learning. Common techniques to correct this bias, such as cross-validation, require repeatedly solving additional optimization problems and are therefore computationally expensive. We develop a general bias correction approach, building on what we call Optimizer's Information Criterion (OIC), that directly approximates the first-order bias and does not require solving any additional optimization problems. Our OIC generalizes the celebrated Akaike Information Criterion to evaluate the objective performance in data-driven optimization, which crucially involves not only model fitting but also its interplay with the downstream optimization. As such it can be used for decision selection instead of only model selection. We apply our approach to a rang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33258;&#19979;&#32780;&#19978;&#31639;&#27861;&#24674;&#22797;Hierarchical Stochastic Block Model&#30340;&#26641;&#24418;&#32467;&#26500;&#21644;&#31038;&#21306;&#32467;&#26500;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#30830;&#23450;&#20102;&#20854;&#22312;&#20013;&#38388;&#23618;&#27425;&#19978;&#36798;&#21040;&#20102;&#30830;&#20999;&#24674;&#22797;&#20449;&#24687;&#29702;&#35770;&#38408;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.00833</link><description>&lt;p&gt;
&#33258;&#19979;&#32780;&#19978;&#20309;&#26102;&#20987;&#36133;&#33258;&#19978;&#32780;&#19979;&#36827;&#34892;&#20998;&#23618;&#31038;&#21306;&#26816;&#27979;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Does Bottom-up Beat Top-down in Hierarchical Community Detection?. (arXiv:2306.00833v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33258;&#19979;&#32780;&#19978;&#31639;&#27861;&#24674;&#22797;Hierarchical Stochastic Block Model&#30340;&#26641;&#24418;&#32467;&#26500;&#21644;&#31038;&#21306;&#32467;&#26500;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#30830;&#23450;&#20102;&#20854;&#22312;&#20013;&#38388;&#23618;&#27425;&#19978;&#36798;&#21040;&#20102;&#30830;&#20999;&#24674;&#22797;&#20449;&#24687;&#29702;&#35770;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#30340;&#20998;&#23618;&#32858;&#31867;&#26159;&#25351;&#26597;&#25214;&#19968;&#32452;&#31038;&#21306;&#30340;&#26641;&#24418;&#32467;&#26500;&#65292;&#20854;&#20013;&#23618;&#27425;&#32467;&#26500;&#30340;&#36739;&#20302;&#32423;&#21035;&#26174;&#31034;&#26356;&#32454;&#31890;&#24230;&#30340;&#31038;&#21306;&#32467;&#26500;&#12290;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#31639;&#27861;&#26377;&#20004;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#33258;&#19978;&#32780;&#19979;&#30340;&#31639;&#27861;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33258;&#19979;&#32780;&#19978;&#31639;&#27861;&#24674;&#22797;&#20998;&#23618;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#26641;&#24418;&#32467;&#26500;&#21644;&#31038;&#21306;&#32467;&#26500;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#36825;&#31181;&#33258;&#19979;&#32780;&#19978;&#31639;&#27861;&#22312;&#23618;&#27425;&#32467;&#26500;&#30340;&#20013;&#38388;&#23618;&#27425;&#19978;&#36798;&#21040;&#20102;&#30830;&#20999;&#24674;&#22797;&#20449;&#24687;&#29702;&#35770;&#38408;&#20540;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#24674;&#22797;&#26465;&#20214;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#33258;&#19978;&#32780;&#19979;&#31639;&#27861;&#30340;&#26465;&#20214;&#26469;&#35828;&#65292;&#38480;&#21046;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical clustering of networks consists in finding a tree of communities, such that lower levels of the hierarchy reveal finer-grained community structures. There are two main classes of algorithms tackling this problem. Divisive ($\textit{top-down}$) algorithms recursively partition the nodes into two communities, until a stopping rule indicates that no further split is needed. In contrast, agglomerative ($\textit{bottom-up}$) algorithms first identify the smallest community structure and then repeatedly merge the communities using a $\textit{linkage}$ method. In this article, we establish theoretical guarantees for the recovery of the hierarchical tree and community structure of a Hierarchical Stochastic Block Model by a bottom-up algorithm. We also establish that this bottom-up algorithm attains the information-theoretic threshold for exact recovery at intermediate levels of the hierarchy. Notably, these recovery conditions are less restrictive compared to those existing for to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#36890;&#36807;&#21152;&#26435;&#34928;&#20943;&#35757;&#32451;&#30340;&#22810;&#36755;&#20986;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20989;&#25968;&#31867;&#22411;&#21644;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#26032;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.16534</link><description>&lt;p&gt;
&#21521;&#37327;&#20540;&#21464;&#20998;&#31354;&#38388;&#21644;DNN&#30340;&#23485;&#24230;&#30028;&#65306;&#20851;&#20110;&#26435;&#37325;&#34928;&#20943;&#27491;&#21017;&#21270;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vector-Valued Variation Spaces and Width Bounds for DNNs: Insights on Weight Decay Regularization. (arXiv:2305.16534v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16534
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#36890;&#36807;&#21152;&#26435;&#34928;&#20943;&#35757;&#32451;&#30340;&#22810;&#36755;&#20986;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20989;&#25968;&#31867;&#22411;&#21644;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#26368;&#23567;&#21270;&#25439;&#22833;&#39033;&#21644;&#24179;&#26041;&#26435;&#37325;&#21644;&#30456;&#24212;&#65292;&#23545;&#24212;&#20110;&#35757;&#32451;&#21152;&#26435;&#34928;&#20943;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#36825;&#31181;&#24120;&#35265;&#23398;&#20064;&#26694;&#26550;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#35757;&#32451;&#21152;&#26435;&#34928;&#20943;&#20197;&#33719;&#24471;&#22810;&#36755;&#20986;(&#21521;&#37327;&#20540;)ReLU&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#20989;&#25968;&#31867;&#22411;&#12290;&#36825;&#25193;&#23637;&#20102;&#20808;&#21069;&#38480;&#20110;&#21333;&#36755;&#20986;(&#26631;&#37327;&#20540;)&#32593;&#32476;&#30340;&#34920;&#24449;&#12290;&#36825;&#31181;&#34920;&#24449;&#38656;&#35201;&#23450;&#20041;&#25105;&#20204;&#31216;&#20043;&#20026;&#21521;&#37327;&#20540;&#21464;&#20998;(VV)&#31354;&#38388;&#30340;&#26032;&#31867;&#31070;&#32463;&#20989;&#25968;&#31354;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#34920;&#24449;&#23450;&#29702;&#35777;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;(NNs)&#26159;&#36890;&#36807;VV&#31354;&#38388;&#20013;&#25552;&#20986;&#23398;&#20064;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#12290;&#36825;&#20010;&#26032;&#30340;&#34920;&#24449;&#23450;&#29702;&#34920;&#26126;&#65292;&#36825;&#20123;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#23384;&#22312;&#20110;&#23485;&#24230;&#21463;&#35757;&#32451;&#25968;&#25454;&#25968;&#38480;&#21046;&#30340;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#25509;&#19979;&#26469;&#65292;&#36890;&#36807;&#19982;&#22810;&#20219;&#21153;lasso&#38382;&#39064;&#30340;&#26032;&#32852;&#31995;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) trained to minimize a loss term plus the sum of squared weights via gradient descent corresponds to the common approach of training with weight decay. This paper provides new insights into this common learning framework. We characterize the kinds of functions learned by training with weight decay for multi-output (vector-valued) ReLU neural networks. This extends previous characterizations that were limited to single-output (scalar-valued) networks. This characterization requires the definition of a new class of neural function spaces that we call vector-valued variation (VV) spaces. We prove that neural networks (NNs) are optimal solutions to learning problems posed over VV spaces via a novel representer theorem. This new representer theorem shows that solutions to these learning problems exist as vector-valued neural networks with widths bounded in terms of the number of training data. Next, via a novel connection to the multi-task lasso problem, we derive
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#20041;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#24403;&#21069;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.12517</link><description>&lt;p&gt;
&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Retrieving Texts based on Abstract Descriptions. (arXiv:2305.12517v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#20041;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#24403;&#21069;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38024;&#23545;&#25991;&#26412;&#30340;&#20449;&#24687;&#25552;&#21462;&#65292;&#25351;&#20196;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23545;&#20110;&#22312;&#22823;&#35268;&#27169;&#25991;&#26723;&#38598;&#21512;&#20013;&#23450;&#20301;&#31526;&#21512;&#32473;&#23450;&#25551;&#36848;&#30340;&#25991;&#26412;&#65288;&#35821;&#20041;&#26816;&#32034;&#65289;&#24182;&#19981;&#36866;&#29992;&#12290;&#22522;&#20110;&#23884;&#20837;&#21521;&#37327;&#30340;&#30456;&#20284;&#24230;&#25628;&#32034;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;&#25191;&#34892;&#26816;&#32034;&#65292;&#20294;&#23884;&#20837;&#20013;&#30340;&#30456;&#20284;&#24230;&#23450;&#20041;&#19981;&#26126;&#30830;&#19988;&#19981;&#19968;&#33268;&#65292;&#24182;&#19988;&#23545;&#20110;&#35768;&#22810;&#29992;&#20363;&#26469;&#35828;&#37117;&#26159;&#27425;&#20248;&#30340;&#12290;&#37027;&#20040;&#65292;&#20160;&#20040;&#26159;&#26377;&#25928;&#26816;&#32034;&#30340;&#22909;&#30340;&#26597;&#35810;&#34920;&#31034;&#65311;&#25105;&#20204;&#30830;&#23450;&#20102;&#26681;&#25454;&#20869;&#23481;&#30340;&#25688;&#35201;&#25551;&#36848;&#26816;&#32034;&#21477;&#23376;&#30340;&#26126;&#30830;&#23450;&#20041;&#19988;&#19968;&#33268;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#25991;&#26412;&#23884;&#20837;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#27169;&#22411;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#30340;&#34920;&#29616;&#26174;&#33879;&#25552;&#21319;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#36890;&#36807;&#25552;&#31034;LLM&#33719;&#24471;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#34429;&#28982;&#24456;&#23481;&#26131;&#20174;LLM&#20013;&#33719;&#24471;&#35757;&#32451;&#26448;&#26009;&#65292;&#20294;LLM&#26080;&#27861;&#30452;&#25509;&#25191;&#34892;&#26816;&#32034;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While instruction-tuned Large Language Models (LLMs) excel at extracting information from text, they are not suitable for locating texts conforming to a given description in a large document collection (semantic retrieval). Similarity search over embedding vectors does allow to perform retrieval by query, but the similarity reflected in the embedding is ill-defined and non-consistent, and is sub-optimal for many use cases. What, then, is a good query representation for effective retrieval?  We identify the well defined and consistent task of retrieving sentences based on abstract descriptions of their content. We demonstrate the inadequacy of current text embeddings and propose an alternative model that significantly improves when used in standard nearest neighbor search. The model is trained using positive and negative pairs sourced through prompting a LLM. While it is easy to source the training material from an LLM, the retrieval task cannot be performed by the LLM directly. This de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27431;&#25289;&#29305;&#24449;&#25216;&#26415;&#22312;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#28857;&#36816;&#31639;&#27431;&#25289;&#29305;&#24449;&#24471;&#21040;&#27431;&#25289;&#29305;&#24449;&#36718;&#24275;&#65292;&#22312;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#27431;&#25289;&#36718;&#24275;&#21450;&#20854;&#28151;&#21512;&#21464;&#25442;&#25429;&#25417;&#20449;&#24687;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.14040</link><description>&lt;p&gt;
&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#27431;&#25289;&#29305;&#24449;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Euler Characteristic Tools For Topological Data Analysis. (arXiv:2303.14040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27431;&#25289;&#29305;&#24449;&#25216;&#26415;&#22312;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#28857;&#36816;&#31639;&#27431;&#25289;&#29305;&#24449;&#24471;&#21040;&#27431;&#25289;&#29305;&#24449;&#36718;&#24275;&#65292;&#22312;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#27431;&#25289;&#36718;&#24275;&#21450;&#20854;&#28151;&#21512;&#21464;&#25442;&#25429;&#25417;&#20449;&#24687;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#27431;&#25289;&#29305;&#24449;&#25216;&#26415;&#12290;&#20174;&#25968;&#25454;&#26500;&#24314;&#30340;&#19968;&#26063;&#21333;&#32431;&#22797;&#21512;&#20307;&#30340;&#28857;&#36816;&#31639;&#27431;&#25289;&#29305;&#24449;&#65292;&#24471;&#21040;&#25152;&#35859;&#30340;&#27431;&#25289;&#29305;&#24449;&#36718;&#24275;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#31616;&#21333;&#25551;&#36848;&#31526;&#20197;&#26497;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#22312;&#30417;&#30563;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#21463;&#20449;&#21495;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35745;&#31639;&#27431;&#25289;&#29305;&#24449;&#36718;&#24275;&#30340;&#28151;&#21512;&#21464;&#25442;&#12290;&#36825;&#20123;&#31215;&#20998;&#21464;&#25442;&#23558;&#27431;&#25289;&#29305;&#24449;&#25216;&#26415;&#19982;&#21202;&#36125;&#26684;&#31215;&#20998;&#28151;&#21512;&#65292;&#25552;&#20379;&#39640;&#25928;&#21387;&#32553;&#25299;&#25169;&#20449;&#21495;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#22312;&#23450;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27431;&#25289;&#36718;&#24275;&#21450;&#20854;&#28151;&#21512;&#21464;&#25442;&#25152;&#25429;&#25417;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#30340;&#20247;&#22810;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#25551;&#36848;&#31526;&#30340;&#31283;&#23450;&#24615;&#32467;&#26524;&#20197;&#21450;&#22312;&#38543;&#26426;&#35774;&#32622;&#19979;&#30340;&#28176;&#36817;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we study Euler characteristic techniques in topological data analysis. Pointwise computing the Euler characteristic of a family of simplicial complexes built from data gives rise to the so-called Euler characteristic profile. We show that this simple descriptor achieve state-of-the-art performance in supervised tasks at a very low computational cost. Inspired by signal analysis, we compute hybrid transforms of Euler characteristic profiles. These integral transforms mix Euler characteristic techniques with Lebesgue integration to provide highly efficient compressors of topological signals. As a consequence, they show remarkable performances in unsupervised settings. On the qualitative side, we provide numerous heuristics on the topological and geometric information captured by Euler profiles and their hybrid transforms. Finally, we prove stability results for these descriptors as well as asymptotic guarantees in random settings.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#31867;&#31995;&#32479;&#26469;&#24402;&#32435;&#36825;&#20123;&#26041;&#27861;&#12290;&#35813;&#32508;&#36848;&#24635;&#32467;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.06433</link><description>&lt;p&gt;
&#26631;&#31614;&#25928;&#29575;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Label-efficient Time Series Representation Learning: A Review. (arXiv:2302.06433v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06433
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#31867;&#31995;&#32479;&#26469;&#24402;&#32435;&#36825;&#20123;&#26041;&#27861;&#12290;&#35813;&#32508;&#36848;&#24635;&#32467;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#26159;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26102;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#20363;&#22914;&#36801;&#31227;&#23398;&#20064;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#20197;&#20419;&#36827;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#26377;&#38480;&#30340;&#26102;&#38388;&#24207;&#21015;&#26631;&#31614;&#20013;&#33719;&#21462;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#29992;&#26469;&#26681;&#25454;&#23427;&#20204;&#23545;&#22806;&#37096;&#25968;&#25454;&#28304;&#30340;&#20381;&#36182;&#65292;&#23545;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#30340;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27599;&#31181;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#23457;&#26597;&#65292;&#24182;&#24635;&#32467;&#20102;&#24403;&#21069;&#24037;&#20316;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#33021;&#22312;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#26356;&#22909;&#36827;&#23637;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scarcity of labeled data is one of the main challenges of applying deep learning models on time series data in the real world. Therefore, several approaches, e.g., transfer learning, self-supervised learning, and semi-supervised learning, have been recently developed to promote the learning capability of deep learning models from the limited time series labels. In this survey, for the first time, we provide a novel taxonomy to categorize existing approaches that address the scarcity of labeled data problem in time series data based on their dependency on external data sources. Moreover, we present a review of the recent advances in each approach and conclude the limitations of the current works and provide future directions that could yield better progress in the field.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#20449;&#24687;&#22330;&#29702;&#35770;(IFT)&#21040;&#29289;&#29702;&#20449;&#24687;&#22330;&#29702;&#35770;(PIFT)&#65292;&#23558;&#25551;&#36848;&#22330;&#30340;&#29289;&#29702;&#23450;&#24459;&#30340;&#20449;&#24687;&#32534;&#30721;&#20026;&#20989;&#25968;&#20808;&#39564;&#12290;&#20174;&#36825;&#20010;PIFT&#24471;&#20986;&#30340;&#21518;&#39564;&#19982;&#20219;&#20309;&#25968;&#20540;&#26041;&#26696;&#26080;&#20851;&#65292;&#24182;&#19988;&#21487;&#20197;&#25429;&#25417;&#22810;&#31181;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2301.07609</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#30693;&#35782;&#20316;&#20026;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22411;&#30340;&#20449;&#24687;&#22330;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Physics-informed Information Field Theory for Modeling Physical Systems with Uncertainty Quantification. (arXiv:2301.07609v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07609
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#20449;&#24687;&#22330;&#29702;&#35770;(IFT)&#21040;&#29289;&#29702;&#20449;&#24687;&#22330;&#29702;&#35770;(PIFT)&#65292;&#23558;&#25551;&#36848;&#22330;&#30340;&#29289;&#29702;&#23450;&#24459;&#30340;&#20449;&#24687;&#32534;&#30721;&#20026;&#20989;&#25968;&#20808;&#39564;&#12290;&#20174;&#36825;&#20010;PIFT&#24471;&#20986;&#30340;&#21518;&#39564;&#19982;&#20219;&#20309;&#25968;&#20540;&#26041;&#26696;&#26080;&#20851;&#65292;&#24182;&#19988;&#21487;&#20197;&#25429;&#25417;&#22810;&#31181;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#32467;&#21512;&#29289;&#29702;&#23398;&#30693;&#35782;&#26159;&#24314;&#27169;&#31995;&#32479;&#30340;&#24378;&#26377;&#21147;&#25216;&#26415;&#12290;&#27492;&#31867;&#27169;&#22411;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#23558;&#27979;&#37327;&#32467;&#26524;&#19982;&#24050;&#30693;&#29289;&#29702;&#23450;&#24459;&#30456;&#32467;&#21512;&#65292;&#39640;&#25928;&#22320;&#27714;&#35299;&#22522;&#26412;&#22330;&#12290;&#30001;&#20110;&#35768;&#22810;&#31995;&#32479;&#21253;&#21547;&#26410;&#30693;&#20803;&#32032;&#65292;&#22914;&#32570;&#22833;&#21442;&#25968;&#12289;&#22024;&#26434;&#25968;&#25454;&#25110;&#19981;&#23436;&#25972;&#30340;&#29289;&#29702;&#23450;&#24459;&#65292;&#22240;&#27492;&#36825;&#36890;&#24120;&#34987;&#35270;&#20026;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#12290;&#22788;&#29702;&#25152;&#26377;&#21464;&#37327;&#30340;&#24120;&#35265;&#25216;&#26415;&#36890;&#24120;&#21462;&#20915;&#20110;&#29992;&#20110;&#36817;&#20284;&#21518;&#39564;&#30340;&#25968;&#20540;&#26041;&#26696;&#65292;&#24182;&#19988;&#24076;&#26395;&#26377;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#31163;&#25955;&#21270;&#30340;&#26041;&#27861;&#12290;&#20449;&#24687;&#22330;&#29702;&#35770;&#65288;IFT&#65289;&#25552;&#20379;&#20102;&#23545;&#19981;&#19968;&#23450;&#26159;&#39640;&#26031;&#22330;&#30340;&#22330;&#36827;&#34892;&#32479;&#35745;&#23398;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25551;&#36848;&#22330;&#30340;&#29289;&#29702;&#23450;&#24459;&#30340;&#20449;&#24687;&#32534;&#30721;&#20026;&#20989;&#25968;&#20808;&#39564;&#26469;&#25193;&#23637;IFT&#21040;&#29289;&#29702;&#20449;&#24687;&#22330;&#29702;&#35770;&#65288;PIFT&#65289;&#12290;&#20174;&#36825;&#20010;PIFT&#24471;&#20986;&#30340;&#21518;&#39564;&#19982;&#20219;&#20309;&#25968;&#20540;&#26041;&#26696;&#26080;&#20851;&#65292;&#24182;&#19988;&#21487;&#20197;&#25429;&#25417;&#22810;&#31181;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven approaches coupled with physical knowledge are powerful techniques to model systems. The goal of such models is to efficiently solve for the underlying field by combining measurements with known physical laws. As many systems contain unknown elements, such as missing parameters, noisy data, or incomplete physical laws, this is widely approached as an uncertainty quantification problem. The common techniques to handle all the variables typically depend on the numerical scheme used to approximate the posterior, and it is desirable to have a method which is independent of any such discretization. Information field theory (IFT) provides the tools necessary to perform statistics over fields that are not necessarily Gaussian. We extend IFT to physics-informed IFT (PIFT) by encoding the functional priors with information about the physical laws which describe the field. The posteriors derived from this PIFT remain independent of any numerical scheme and can capture multiple modes,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#25351;&#23548;&#30340;&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120; ($\Phi$-DVAE) &#29992;&#20110;&#23558;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#21516;&#21270;&#21040;&#29289;&#29702;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#26410;&#30693;&#26144;&#23556;&#24773;&#20917;&#19979;&#26080;&#27861;&#23454;&#29616;&#19968;&#33268;&#27169;&#22411;&#19982;&#25968;&#25454;&#32508;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.15609</link><description>&lt;p&gt;
$\Phi$-DVAE: &#29289;&#29702;&#25351;&#23548;&#30340;&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#21516;&#21270;
&lt;/p&gt;
&lt;p&gt;
$\Phi$-DVAE: Physics-Informed Dynamical Variational Autoencoders for Unstructured Data Assimilation. (arXiv:2209.15609v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#25351;&#23548;&#30340;&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120; ($\Phi$-DVAE) &#29992;&#20110;&#23558;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#21516;&#21270;&#21040;&#29289;&#29702;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#26410;&#30693;&#26144;&#23556;&#24773;&#20917;&#19979;&#26080;&#27861;&#23454;&#29616;&#19968;&#33268;&#27169;&#22411;&#19982;&#25968;&#25454;&#32508;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#21516;&#21270;&#20013;&#65292;&#23558;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#32435;&#20837;&#29289;&#29702;&#27169;&#22411;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20851;&#27880;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#35266;&#27979;&#31639;&#23376;&#30340;&#24773;&#20917;&#65292;&#20854;&#20989;&#25968;&#24418;&#24335;&#36890;&#24120;&#34987;&#20551;&#23450;&#20026;&#24050;&#30693;&#12290;&#36825;&#38459;&#27490;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#20174;&#25968;&#25454;&#31354;&#38388;&#21040;&#27169;&#22411;&#31354;&#38388;&#30340;&#26144;&#23556;&#26410;&#30693;&#30340;&#37197;&#32622;&#20013;&#23454;&#29616;&#19968;&#33268;&#30340;&#27169;&#22411;&#19982;&#25968;&#25454;&#32508;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29289;&#29702;&#25351;&#23548;&#30340;&#21160;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;($\Phi$-DVAE)&#65292;&#23558;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#27969;&#23884;&#20837;&#21040;&#30001;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#30340;&#26102;&#21464;&#29289;&#29702;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#19968;&#20010;&#26631;&#20934;&#30340;&#12289;&#21487;&#33021;&#26159;&#38750;&#32447;&#24615;&#30340;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#28388;&#27874;&#22120;&#21644;&#19968;&#20010;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#21516;&#21270;&#21040;&#28508;&#22312;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#12290;&#22312;&#25105;&#20204;&#30340;&#31034;&#20363;&#31995;&#32479;&#20013;&#65292;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#37319;&#29992;&#35270;&#39057;&#25968;&#25454;&#21644;&#36895;&#24230;&#22330;&#27979;&#37327;&#30340;&#24418;&#24335;&#65292;&#20294;&#35813;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#36275;&#22815;&#36890;&#29992;&#65292;&#21487;&#20197;&#20801;&#35768;&#20219;&#24847;&#26410;&#30693;&#30340;&#35266;&#27979;&#31639;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating unstructured data into physical models is a challenging problem that is emerging in data assimilation. Traditional approaches focus on well-defined observation operators whose functional forms are typically assumed to be known. This prevents these methods from achieving a consistent model-data synthesis in configurations where the mapping from data-space to model-space is unknown. To address these shortcomings, in this paper we develop a physics-informed dynamical variational autoencoder ($\Phi$-DVAE) to embed diverse data streams into time-evolving physical systems described by differential equations. Our approach combines a standard, possibly nonlinear, filter for the latent state-space model and a VAE, to assimilate the unstructured data into the latent dynamical system. Unstructured data, in our example systems, comes in the form of video data and velocity field measurements, however the methodology is suitably generic to allow for arbitrary unknown observation operat
&lt;/p&gt;</description></item><item><title>TAD&#26159;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#33258;&#36866;&#24212;&#35774;&#35745;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#27169;&#22411;&#22312;&#25351;&#23450;&#20844;&#24046;&#20013;&#30830;&#23450;&#20135;&#29983;&#26399;&#26395;&#35774;&#35745;&#29305;&#24449;&#30340;&#26368;&#20339;&#25511;&#21046;&#35774;&#32622;&#65292;&#30456;&#27604;&#20854;&#20182;&#33258;&#36866;&#24212;&#35774;&#35745;&#31639;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2205.14208</link><description>&lt;p&gt;
&#30446;&#26631;&#33258;&#36866;&#24212;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Targeted Adaptive Design. (arXiv:2205.14208v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14208
&lt;/p&gt;
&lt;p&gt;
TAD&#26159;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#33258;&#36866;&#24212;&#35774;&#35745;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#27169;&#22411;&#22312;&#25351;&#23450;&#20844;&#24046;&#20013;&#30830;&#23450;&#20135;&#29983;&#26399;&#26395;&#35774;&#35745;&#29305;&#24449;&#30340;&#26368;&#20339;&#25511;&#21046;&#35774;&#32622;&#65292;&#30456;&#27604;&#20854;&#20182;&#33258;&#36866;&#24212;&#35774;&#35745;&#31639;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20808;&#36827;&#21046;&#36896;&#21644;&#39640;&#32423;&#26448;&#26009;&#35774;&#35745;&#24448;&#24448;&#38656;&#35201;&#22312;&#36739;&#39640;&#32500;&#24230;&#30340;&#36807;&#31243;&#25511;&#21046;&#21442;&#25968;&#31354;&#38388;&#20013;&#25628;&#32034;&#26368;&#20339;&#32467;&#26500;&#12289;&#24615;&#33021;&#21644;&#24615;&#33021;&#21442;&#25968;&#30340;&#35774;&#32622;&#12290;&#20174;&#21069;&#32773;&#21040;&#21518;&#32773;&#30340;&#26144;&#23556;&#24517;&#39035;&#36890;&#36807;&#22024;&#26434;&#30340;&#23454;&#39564;&#25110;&#26114;&#36149;&#30340;&#27169;&#25311;&#26469;&#30830;&#23450;&#12290;&#25105;&#20204;&#25226;&#36825;&#20010;&#38382;&#39064;&#25277;&#35937;&#25104;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#20854;&#20013;&#24517;&#39035;&#36890;&#36807;&#26114;&#36149;&#30340;&#22024;&#26434;&#27979;&#37327;&#26469;&#30830;&#23450;&#20174;&#25511;&#21046;&#31354;&#38388;&#21040;&#35774;&#35745;&#31354;&#38388;&#30340;&#26410;&#30693;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#22312;&#25351;&#23450;&#30340;&#20844;&#24046;&#33539;&#22260;&#20869;&#23450;&#20301;&#20135;&#29983;&#26399;&#26395;&#35774;&#35745;&#29305;&#24449;&#30340;&#26368;&#20339;&#25511;&#21046;&#35774;&#32622;&#65292;&#24182;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#30446;&#26631;&#33258;&#36866;&#24212;&#35774;&#35745; (TAD)&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#25191;&#34892;&#36825;&#20010;&#37319;&#26679;&#20219;&#21153;&#30340;&#26032;&#31639;&#27861;&#12290;TAD &#22312;&#27599;&#20010;&#36845;&#20195;&#38454;&#27573;&#21019;&#24314;&#19968;&#20010;&#26410;&#30693;&#26144;&#23556;&#30340;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#27169;&#22411;&#65292;&#24314;&#35758;&#19968;&#25209;&#26032;&#30340;&#25511;&#21046;&#35774;&#32622;&#36827;&#34892;&#23454;&#39564;&#37319;&#26679;&#65292;&#24182;&#20248;&#21270;&#26356;&#26032;&#30340;&#30446;&#26631;&#35774;&#35745;&#29305;&#24449;&#30340;&#23545;&#25968;&#39044;&#27979;&#20284;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern advanced manufacturing and advanced materials design often require searches of relatively high-dimensional process control parameter spaces for settings that result in optimal structure, property, and performance parameters. The mapping from the former to the latter must be determined from noisy experiments or from expensive simulations. We abstract this problem to a mathematical framework in which an unknown function from a control space to a design space must be ascertained by means of expensive noisy measurements, which locate optimal control settings generating desired design features within specified tolerances, with quantified uncertainty. We describe targeted adaptive design (TAD), a new algorithm that performs this sampling task efficiently. TAD creates a Gaussian process surrogate model of the unknown mapping at each iterative stage, proposing a new batch of control settings to sample experimentally and optimizing the updated log-predictive likelihood of the target desi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#20219;&#21153;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#24615;&#65292;&#20248;&#21270;&#26399;&#26395;&#20934;&#30830;&#29575;&#65292;&#21462;&#24471;&#20102;&#24378;&#26377;&#21147;&#30340;&#26367;&#20195;&#20998;&#31867;&#25439;&#22833;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.09615</link><description>&lt;p&gt;
EXACT: &#22914;&#20309;&#25552;&#39640;&#20934;&#30830;&#29575;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
EXACT: How to Train Your Accuracy. (arXiv:2205.09615v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#20219;&#21153;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#24615;&#65292;&#20248;&#21270;&#26399;&#26395;&#20934;&#30830;&#29575;&#65292;&#21462;&#24471;&#20102;&#24378;&#26377;&#21147;&#30340;&#26367;&#20195;&#20998;&#31867;&#25439;&#22833;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#20219;&#21153;&#36890;&#24120;&#20250;&#20197;&#20934;&#30830;&#29575;&#20316;&#20026;&#35780;&#20272;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#29575;&#26159;&#19981;&#36830;&#32493;&#30340;&#65292;&#26080;&#27861;&#30452;&#25509;&#20351;&#29992;&#26799;&#24230;&#19978;&#21319;&#36827;&#34892;&#20248;&#21270;&#12290;&#27969;&#34892;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#12289;&#38128;&#38142;&#25439;&#22833;&#25110;&#20854;&#20182;&#26367;&#20195;&#25439;&#22833;&#26469;&#20248;&#21270;&#65292;&#20294;&#36825;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27169;&#22411;&#30340;&#36755;&#20986;&#24341;&#20837;&#38543;&#26426;&#24615;&#24182;&#20248;&#21270;&#26399;&#26395;&#20934;&#30830;&#29575;&#65292;&#21363;&#38543;&#26426;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#12290;&#23545;&#32447;&#24615;&#27169;&#22411;&#21644;&#28145;&#24230;&#22270;&#20687;&#20998;&#31867;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#20248;&#21270;&#26041;&#27861;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#20998;&#31867;&#25439;&#22833;&#30340;&#24378;&#26377;&#21147;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification tasks are usually evaluated in terms of accuracy. However, accuracy is discontinuous and cannot be directly optimized using gradient ascent. Popular methods minimize cross-entropy, hinge loss, or other surrogate losses, which can lead to suboptimal results. In this paper, we propose a new optimization framework by introducing stochasticity to a model's output and optimizing expected accuracy, i.e. accuracy of the stochastic model. Extensive experiments on linear models and deep image classification show that the proposed optimization method is a powerful alternative to widely used classification losses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#30830;&#20445;&#40065;&#26834;&#24615;&#30340;&#26126;&#30830;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2202.12319</link><description>&lt;p&gt;
&#20445;&#25252;&#38544;&#31169;&#30340;&#24352;&#37327;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving machine learning with tensor networks. (arXiv:2202.12319v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#30830;&#20445;&#40065;&#26834;&#24615;&#30340;&#26126;&#30830;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#34987;&#24191;&#27867;&#29992;&#20110;&#25552;&#20379;&#20302;&#33021;&#37327;&#24577;&#30340;&#39640;&#25928;&#34920;&#31034;&#65292;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36825;&#23545;&#20110;&#22788;&#29702;&#21307;&#30103;&#35760;&#24405;&#31561;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#30340;&#26032;&#38544;&#31169;&#28431;&#27934;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30830;&#20445;&#23545;&#36825;&#31181;&#28431;&#27934;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26126;&#30830;&#26465;&#20214;&#65292;&#36825;&#28041;&#21450;&#21040;&#22312;&#35268;&#33539;&#23545;&#31216;&#24615;&#19979;&#31561;&#20215;&#30340;&#27169;&#22411;&#30340;&#21051;&#30011;&#12290;&#25105;&#20204;&#20005;&#26684;&#35777;&#26126;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#30697;&#38453;&#20056;&#31215;&#24577;&#30340;&#35268;&#33539;&#24418;&#24335;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#35268;&#24459;&#24615;&#24182;&#20462;&#27491;&#20102;&#27531;&#20313;&#35268;&#33539;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor networks, widely used for providing efficient representations of low-energy states of local quantum many-body systems, have been recently proposed as machine learning architectures which could present advantages with respect to traditional ones. In this work we show that tensor network architectures have especially prospective properties for privacy-preserving machine learning, which is important in tasks such as the processing of medical records. First, we describe a new privacy vulnerability that is present in feedforward neural networks, illustrating it in synthetic and real-world datasets. Then, we develop well-defined conditions to guarantee robustness to such vulnerability, which involve the characterization of models equivalent under gauge symmetry. We rigorously prove that such conditions are satisfied by tensor-network architectures. In doing so, we define a novel canonical form for matrix product states, which has a high degree of regularity and fixes the residual gaug
&lt;/p&gt;</description></item></channel></rss>