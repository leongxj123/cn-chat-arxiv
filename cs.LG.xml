<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21333;&#28436;&#31034;&#27169;&#20223;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;TDIL&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36716;&#25442;&#37492;&#21035;&#22120;&#30340;&#26367;&#20195;&#22870;&#21169;&#20989;&#25968;&#65292;&#40723;&#21169;&#20195;&#29702;&#21521;&#38752;&#36817;&#19987;&#23478;&#29366;&#24577;&#30340;&#29366;&#24577;&#23548;&#33322;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22870;&#21169;&#20449;&#21495;&#31232;&#30095;&#30340;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01057</link><description>&lt;p&gt;
&#19987;&#23478;&#25509;&#36817;&#24615;&#20316;&#20026;&#21333;&#28436;&#31034;&#27169;&#20223;&#23398;&#20064;&#30340;&#26367;&#20195;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Expert Proximity as Surrogate Rewards for Single Demonstration Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21333;&#28436;&#31034;&#27169;&#20223;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;TDIL&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36716;&#25442;&#37492;&#21035;&#22120;&#30340;&#26367;&#20195;&#22870;&#21169;&#20989;&#25968;&#65292;&#40723;&#21169;&#20195;&#29702;&#21521;&#38752;&#36817;&#19987;&#23478;&#29366;&#24577;&#30340;&#29366;&#24577;&#23548;&#33322;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22870;&#21169;&#20449;&#21495;&#31232;&#30095;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#21333;&#28436;&#31034;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#33719;&#21462;&#22823;&#37327;&#19987;&#23478;&#28436;&#31034;&#22256;&#38590;&#25110;&#19981;&#21487;&#34892;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#19982; typicIL &#35774;&#32622;&#20013;&#20855;&#26377;&#22810;&#20010;&#31034;&#33539;&#19981;&#21516;&#65292;&#21333;&#28436;&#31034;IL&#28041;&#21450;&#20195;&#29702;&#21482;&#26377;&#19968;&#26465;&#19987;&#23478;&#36712;&#36857;&#30340;&#35775;&#38382;&#12290;&#25105;&#20204;&#24378;&#35843;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#22870;&#21169;&#20449;&#21495;&#31232;&#30095;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#36716;&#25442;&#37492;&#21035;&#22120;&#30340;IL&#65288;TDIL&#65289;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;TDIL&#26159;&#19968;&#31181;&#22522;&#20110;IRL&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#32771;&#34385;&#29615;&#22659;&#21160;&#24577;&#30340;&#26356;&#23494;&#38598;&#30340;&#26367;&#20195;&#22870;&#21169;&#20989;&#25968;&#26469;&#35299;&#20915;&#22870;&#21169;&#31232;&#30095;&#24615;&#12290;&#36825;&#20010;&#26367;&#20195;&#22870;&#21169;&#20989;&#25968;&#40723;&#21169;&#20195;&#29702;&#21521;&#38752;&#36817;&#19987;&#23478;&#29366;&#24577;&#30340;&#29366;&#24577;&#23548;&#33322;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;TDIL&#35757;&#32451;&#19968;&#20010;&#36807;&#28193;&#37492;&#21035;&#22120;&#26469;&#21306;&#20998;&#32473;&#23450;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#21644;&#38750;&#26377;&#25928;&#36807;&#28193;&#20197;&#35745;&#31639;&#26367;&#20195;&#22870;&#21169;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;TDIL&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on single-demonstration imitation learning (IL), a practical approach for real-world applications where obtaining numerous expert demonstrations is costly or infeasible. In contrast to typical IL settings with multiple demonstrations, single-demonstration IL involves an agent having access to only one expert trajectory. We highlight the issue of sparse reward signals in this setting and propose to mitigate this issue through our proposed Transition Discriminator-based IL (TDIL) method. TDIL is an IRL method designed to address reward sparsity by introducing a denser surrogate reward function that considers environmental dynamics. This surrogate reward function encourages the agent to navigate towards states that are proximal to expert states. In practice, TDIL trains a transition discriminator to differentiate between valid and non-valid transitions in a given environment to compute the surrogate rewards. The experiments demonstrate that TDIL outperforms existin
&lt;/p&gt;</description></item><item><title>CAM-based&#26041;&#27861;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20915;&#31574;&#26102;&#65292;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#23558;&#27169;&#22411;&#26080;&#27861;&#30475;&#21040;&#30340;&#37096;&#20998;&#24402;&#22240;&#20026;&#37325;&#35201;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#35823;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2404.01964</link><description>&lt;p&gt;
&#22522;&#20110;CAM&#30340;&#26041;&#27861;&#21487;&#20197;&#31359;&#22681;&#32780;&#36807;
&lt;/p&gt;
&lt;p&gt;
CAM-Based Methods Can See through Walls
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01964
&lt;/p&gt;
&lt;p&gt;
CAM-based&#26041;&#27861;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20915;&#31574;&#26102;&#65292;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#23558;&#27169;&#22411;&#26080;&#27861;&#30475;&#21040;&#30340;&#37096;&#20998;&#24402;&#22240;&#20026;&#37325;&#35201;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#35823;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CAM-based&#26041;&#27861;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#20107;&#21518;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29983;&#25104;&#26174;&#33879;&#24615;&#22320;&#22270;&#26469;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#26174;&#33879;&#24615;&#22320;&#22270;&#31361;&#20986;&#26174;&#31034;&#19982;&#39044;&#27979;&#30456;&#20851;&#30340;&#22270;&#20687;&#37325;&#35201;&#21306;&#22495;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#38169;&#35823;&#22320;&#23558;&#22270;&#20687;&#30340;&#26576;&#20123;&#37096;&#20998;&#24402;&#22240;&#20026;&#27169;&#22411;&#26080;&#27861;&#30475;&#21040;&#30340;&#37325;&#35201;&#24471;&#20998;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#31181;&#29616;&#35937;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#20013;&#22343;&#23384;&#22312;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;GradCAM&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#25513;&#33180;CNN&#27169;&#22411;&#21021;&#22987;&#21270;&#26102;&#30340;&#34892;&#20026;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#31867;&#20284;VGG&#30340;&#27169;&#22411;&#65292;&#38480;&#21046;&#20854;&#19981;&#20351;&#29992;&#22270;&#20687;&#30340;&#19979;&#21322;&#37096;&#20998;&#65292;&#20173;&#28982;&#35266;&#23519;&#21040;&#26410;&#35265;&#37096;&#20998;&#30340;&#27491;&#20998;&#25968;&#12290;&#36825;&#31181;&#34892;&#20026;&#22312;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#38169;&#35823;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01964v1 Announce Type: cross  Abstract: CAM-based methods are widely-used post-hoc interpretability method that produce a saliency map to explain the decision of an image classification model. The saliency map highlights the important areas of the image relevant to the prediction. In this paper, we show that most of these methods can incorrectly attribute an important score to parts of the image that the model cannot see. We show that this phenomenon occurs both theoretically and experimentally. On the theory side, we analyze the behavior of GradCAM on a simple masked CNN model at initialization. Experimentally, we train a VGG-like model constrained to not use the lower part of the image and nevertheless observe positive scores in the unseen part of the image. This behavior is evaluated quantitatively on two new datasets. We believe that this is problematic, potentially leading to mis-interpretation of the model's behavior.
&lt;/p&gt;</description></item><item><title>YNetr&#27169;&#22411;&#22312;Plain Scan Liver Tumors&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;62.63%&#30340;Dice&#31995;&#25968;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#32925;&#32959;&#30244;&#26222;&#36890;&#25195;&#25551;&#20998;&#21106;&#25968;&#25454;&#38598;&#21644;&#31639;&#27861;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2404.00327</link><description>&lt;p&gt;
YNetr&#65306;&#22312;Plain Scan Liver Tumors (PSLT)&#19978;&#30340;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
YNetr: Dual-Encoder architecture on Plain Scan Liver Tumors (PSLT)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00327
&lt;/p&gt;
&lt;p&gt;
YNetr&#27169;&#22411;&#22312;Plain Scan Liver Tumors&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;62.63%&#30340;Dice&#31995;&#25968;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#32925;&#32959;&#30244;&#26222;&#36890;&#25195;&#25551;&#20998;&#21106;&#25968;&#25454;&#38598;&#21644;&#31639;&#27861;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32925;&#32959;&#30244;&#26159;&#32925;&#33039;&#20013;&#19981;&#27491;&#24120;&#30340;&#29983;&#38271;&#65292;&#21487;&#33021;&#26159;&#33391;&#24615;&#25110;&#24694;&#24615;&#65292;&#32925;&#30284;&#26159;&#20840;&#29699;&#37325;&#35201;&#30340;&#20581;&#24247;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#29992;&#20110;&#32925;&#32959;&#30244;&#26222;&#36890;&#25195;&#25551;&#20998;&#21106;&#30340;&#25968;&#25454;&#38598;&#65292;&#20063;&#27809;&#26377;&#30456;&#20851;&#31639;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Plain Scan Liver Tumors(PSLT)&#21644;YNetr&#12290;&#20351;&#29992;40&#20010;&#32925;&#32959;&#30244;&#26222;&#36890;&#25195;&#25551;&#20998;&#21106;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32452;&#35013;&#21644;&#27880;&#37322;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21033;&#29992;Dice&#31995;&#25968;&#20316;&#20026;&#35780;&#20272;YNetr&#20135;&#29983;&#30340;&#20998;&#21106;&#32467;&#26524;&#30340;&#25351;&#26631;&#65292;&#26377;&#21033;&#20110;&#25429;&#33719;&#19981;&#21516;&#39057;&#29575;&#20449;&#24687;&#12290;YNetr&#27169;&#22411;&#22312;PSLT&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;62.63%&#30340;Dice&#31995;&#25968;&#65292;&#36229;&#36807;&#20854;&#20182;&#20844;&#24320;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#33539;&#22260;1.22%&#12290;&#36827;&#34892;&#20102;&#19982;&#21253;&#25324; UNet 3+&#12289;XNet&#12289;UNetr&#12289;Swin UNetr&#12289;Trans-BTS&#12289;COTr&#12289;nnUNetv2 (2D)&#12289;nnUNetv2 (3D fullres)&#12289;MedNext &#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#27169;&#22411;&#30340;&#27604;&#36739;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00327v1 Announce Type: cross  Abstract: Background: Liver tumors are abnormal growths in the liver that can be either benign or malignant, with liver cancer being a significant health concern worldwide. However, there is no dataset for plain scan segmentation of liver tumors, nor any related algorithms. To fill this gap, we propose Plain Scan Liver Tumors(PSLT) and YNetr. Methods: A collection of 40 liver tumor plain scan segmentation datasets was assembled and annotated. Concurrently, we utilized Dice coefficient as the metric for assessing the segmentation outcomes produced by YNetr, having advantage of capturing different frequency information. Results: The YNetr model achieved a Dice coefficient of 62.63% on the PSLT dataset, surpassing the other publicly available model by an accuracy margin of 1.22%. Comparative evaluations were conducted against a range of models including UNet 3+, XNet, UNetr, Swin UNetr, Trans-BTS, COTr, nnUNetv2 (2D), nnUNetv2 (3D fullres), MedNext
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#65292;&#32467;&#21512;&#28145;&#24230;&#20449;&#24687;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#36523;&#38382;&#31572;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.15941</link><description>&lt;p&gt;
&#25506;&#32034;&#30452;&#21040;&#33258;&#20449;: &#38754;&#21521;&#20855;&#36523;&#38382;&#31572;&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Explore until Confident: Efficient Exploration for Embodied Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15941
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#65292;&#32467;&#21512;&#28145;&#24230;&#20449;&#24687;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#36523;&#38382;&#31572;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#36523;&#38382;&#31572;&#65288;EQA&#65289;&#30340;&#38382;&#39064;&#65292;&#36825;&#25351;&#30340;&#26159;&#22312;&#38656;&#35201;&#20027;&#21160;&#25506;&#32034;&#29615;&#22659;&#20197;&#25910;&#38598;&#20449;&#24687;&#30452;&#21040;&#23545;&#38382;&#39064;&#30340;&#31572;&#26696;&#26377;&#33258;&#20449;&#30340;&#20855;&#36523;&#20195;&#29702;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#24378;&#22823;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#26469;&#39640;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;EQA&#20013;&#20351;&#29992;VLMs&#26102;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#23427;&#20204;&#27809;&#26377;&#20869;&#37096;&#35760;&#24518;&#23558;&#22330;&#26223;&#26144;&#23556;&#20197;&#20415;&#35268;&#21010;&#22914;&#20309;&#38543;&#26102;&#38388;&#25506;&#32034;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#32622;&#20449;&#24230;&#21487;&#33021;&#34987;&#38169;&#35823;&#26657;&#20934;&#24182;&#21487;&#33021;&#23548;&#33268;&#26426;&#22120;&#20154;&#36807;&#26089;&#20572;&#27490;&#25506;&#32034;&#25110;&#36807;&#24230;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#39318;&#20808;&#22522;&#20110;&#28145;&#24230;&#20449;&#24687;&#21644;&#36890;&#36807;&#35270;&#35273;&#25552;&#31034;VLM&#26469;&#26500;&#24314;&#22330;&#26223;&#30340;&#35821;&#20041;&#22320;&#22270;-&#21033;&#29992;&#20854;&#23545;&#22330;&#26223;&#30456;&#20851;&#21306;&#22495;&#30340;&#24191;&#27867;&#30693;&#35782;&#26469;&#36827;&#34892;&#25506;&#32034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#26469;&#26657;&#20934;VLM&#30340;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15941v1 Announce Type: cross  Abstract: We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM - leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM's 
&lt;/p&gt;</description></item><item><title>Larimar&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#22686;&#24378;LLMs&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#30340;&#30693;&#35782;&#26356;&#26032;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#19988;&#22312;&#36895;&#24230;&#21644;&#28789;&#27963;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.11901</link><description>&lt;p&gt;
Larimar: &#20855;&#26377;&#24773;&#33410;&#35760;&#24518;&#25511;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Larimar: Large Language Models with Episodic Memory Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11901
&lt;/p&gt;
&lt;p&gt;
Larimar&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#22686;&#24378;LLMs&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#30340;&#30693;&#35782;&#26356;&#26032;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#19988;&#22312;&#36895;&#24230;&#21644;&#28789;&#27963;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Larimar - &#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#12290; Larimar&#30340;&#35760;&#24518;&#20801;&#35768;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#26356;&#26032;&#30693;&#35782;&#65292;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#20107;&#23454;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Larimar&#22312;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322; - &#26681;&#25454;&#22522;&#30784;LLM&#30340;&#19981;&#21516;&#65292;&#36895;&#24230;&#25552;&#21319;&#20026;4-10&#20493;&#65292;&#24182;&#19988;&#30001;&#20110;&#25552;&#20986;&#30340;&#26550;&#26500;&#31616;&#21333;&#12289;&#19981;&#20381;&#36182;&#20110;LLM&#65292;&#22240;&#27492;&#20855;&#26377;&#33391;&#22909;&#30340;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#36873;&#25321;&#24615;&#20107;&#23454;&#36951;&#24536;&#21644;&#36755;&#20837;&#19978;&#19979;&#25991;&#38271;&#24230;&#27010;&#25324;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11901v1 Announce Type: cross  Abstract: Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed - yielding speed-ups of 4-10x depending on the base LLM - as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting and input context length generalization with Larimar and show their effectiveness.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#21307;&#23398;&#25968;&#25454;&#20013;&#30340;&#38590;&#20197;&#23519;&#35273;&#22122;&#22768;&#26469;&#20445;&#25252;&#25968;&#25454;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#35757;&#32451;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2403.10573</link><description>&lt;p&gt;
&#21307;&#23398;&#19981;&#21487;&#23398;&#20064;&#30340;&#31034;&#20363;&#65306;&#36890;&#36807;&#31232;&#30095;&#24863;&#30693;&#26412;&#22320;&#33945;&#29256;&#20445;&#25252;&#21307;&#23398;&#25968;&#25454;&#20813;&#21463;&#26410;&#32463;&#25480;&#26435;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Medical Unlearnable Examples: Securing Medical Data from Unauthorized Traning via Sparsity-Aware Local Masking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10573
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#21307;&#23398;&#25968;&#25454;&#20013;&#30340;&#38590;&#20197;&#23519;&#35273;&#22122;&#22768;&#26469;&#20445;&#25252;&#25968;&#25454;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#35757;&#32451;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#25935;&#24863;&#21307;&#23398;&#25968;&#25454;&#30340;&#29983;&#25104;&#21644;&#23384;&#20648;&#26174;&#33879;&#22686;&#21152;&#12290;&#36825;&#31181;&#25968;&#25454;&#30340;&#20016;&#23500;&#37327;&#25512;&#21160;&#20102;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#21033;&#29992;&#65292;&#20363;&#22914;&#29992;&#20110;&#35757;&#32451;&#21830;&#19994;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#24120;&#24120;&#20351;&#30740;&#31350;&#20154;&#21592;&#26395;&#32780;&#21364;&#27493;&#65292;&#22240;&#20026;&#20182;&#20204;&#19981;&#24895;&#20844;&#24320;&#20854;&#23453;&#36149;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20445;&#25252;&#36825;&#20123;&#38590;&#20197;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#40723;&#21169;&#21307;&#30103;&#26426;&#26500;&#20998;&#20139;&#25968;&#25454;&#65292;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21521;&#25968;&#25454;&#20013;&#24341;&#20837;&#38590;&#20197;&#23519;&#35273;&#30340;&#22122;&#22768;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#22312;&#27169;&#22411;&#27867;&#21270;&#20013;&#24341;&#20837;&#36864;&#21270;&#26469;&#20445;&#25252;&#25968;&#25454;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#35757;&#32451;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#22312;&#19968;&#33324;&#39046;&#22495;&#26174;&#31034;&#20986;&#20196;&#20154;&#38054;&#20329;&#30340;&#25968;&#25454;&#20445;&#25252;&#33021;&#21147;&#65292;&#20294;&#24403;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#20204;&#26410;&#33021;&#32771;&#34385;&#21040;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10573v1 Announce Type: cross  Abstract: With the rapid growth of artificial intelligence (AI) in healthcare, there has been a significant increase in the generation and storage of sensitive medical data. This abundance of data, in turn, has propelled the advancement of medical AI technologies. However, concerns about unauthorized data exploitation, such as training commercial AI models, often deter researchers from making their invaluable datasets publicly available. In response to the need to protect this hard-to-collect data while still encouraging medical institutions to share it, one promising solution is to introduce imperceptible noise into the data. This method aims to safeguard the data against unauthorized training by inducing degradation in model generalization. Although existing methods have shown commendable data protection capabilities in general domains, they tend to fall short when applied to biomedical data, mainly due to their failure to account for the spar
&lt;/p&gt;</description></item><item><title>SMART&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#24494;&#35843;&#20013;&#37325;&#26032;&#20998;&#37197;&#39044;&#31639;&#65292;&#20174;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#20013;&#21462;&#24471;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.08370</link><description>&lt;p&gt;
SMART: &#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#30340;&#23376;&#27169;&#22359;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SMART: Submodular Data Mixture Strategy for Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08370
&lt;/p&gt;
&lt;p&gt;
SMART&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#24494;&#35843;&#20013;&#37325;&#26032;&#20998;&#37197;&#39044;&#31639;&#65292;&#20174;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#20013;&#21462;&#24471;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#28041;&#21450;&#22312;&#19968;&#32452;&#20197;&#25351;&#20196;&#26684;&#24335;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24179;&#34913;&#19981;&#21516;&#20219;&#21153;&#27604;&#20363;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#25214;&#21040;&#21512;&#36866;&#30340;&#24179;&#34913;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#38500;&#20102;&#25163;&#21160;&#35843;&#25972;&#25110;&#20381;&#36182;&#20174;&#19994;&#32773;&#30340;&#30452;&#35273;&#22806;&#65292;&#23578;&#26080;&#31995;&#32479;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SMART&#65288;Submodular data Mixture strAtegy for instRuction Tuning&#65289;- &#19968;&#31181;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#26032;&#39062;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#20998;&#25968;&#26469;&#30830;&#23450;&#28151;&#21512;&#26435;&#37325;&#12290;&#32473;&#23450;&#24494;&#35843;&#39044;&#31639;&#65292;SMART&#37325;&#26032;&#20998;&#37197;&#20219;&#21153;&#38388;&#30340;&#39044;&#31639;&#65292;&#24182;&#20174;&#27599;&#20010;&#20219;&#21153;&#20013;&#36873;&#25321;&#38750;&#20887;&#20313;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SMART&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#20363;&#23376;&#27604;&#20363;&#28151;&#21512;&#21644;&#22343;&#31561;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08370v1 Announce Type: cross  Abstract: Instruction Tuning involves finetuning a language model on a collection of instruction-formatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#23558;&#26465;&#20214;&#35745;&#31639;&#26041;&#27861;&#24212;&#29992;&#20110;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#21407;&#29702;&#21644;&#24605;&#24819;&#65292;&#24182;&#20171;&#32461;&#20102;&#19987;&#23478;&#28151;&#21512;&#32593;&#32476;&#12289;&#26631;&#35760;&#36873;&#25321;&#26426;&#21046;&#21644;&#25552;&#21069;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#31561;&#19977;&#31181;&#23454;&#29616;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.07965</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26465;&#20214;&#35745;&#31639;: &#21407;&#29702;&#19982;&#30740;&#31350;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Conditional computation in neural networks: principles and research trends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#23558;&#26465;&#20214;&#35745;&#31639;&#26041;&#27861;&#24212;&#29992;&#20110;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#21407;&#29702;&#21644;&#24605;&#24819;&#65292;&#24182;&#20171;&#32461;&#20102;&#19987;&#23478;&#28151;&#21512;&#32593;&#32476;&#12289;&#26631;&#35760;&#36873;&#25321;&#26426;&#21046;&#21644;&#25552;&#21069;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#31561;&#19977;&#31181;&#23454;&#29616;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#24635;&#32467;&#20102;&#23558;&#26465;&#20214;&#35745;&#31639;&#26041;&#27861;&#24212;&#29992;&#20110;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#21407;&#29702;&#21644;&#24605;&#24819;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#21487;&#20197;&#26681;&#25454;&#36755;&#20837;&#21160;&#24577;&#28608;&#27963;&#25110;&#21435;&#28608;&#27963;&#20854;&#35745;&#31639;&#22270;&#37096;&#20998;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20363;&#22914;&#65292;&#21160;&#24577;&#36873;&#25321;&#36755;&#20837;&#26631;&#35760;&#12289;&#23618;&#65288;&#25110;&#19968;&#32452;&#23618;&#65289;&#20197;&#21450;&#27599;&#20010;&#23618;&#20869;&#30340;&#23376;&#27169;&#22359;&#65288;&#20363;&#22914;&#65292;&#21367;&#31215;&#28388;&#27874;&#22120;&#20013;&#30340;&#36890;&#36947;&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#19968;&#20010;&#36890;&#29992;&#24418;&#24335;&#26469;&#32479;&#19968;&#25551;&#36848;&#36825;&#20123;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#20123;&#21407;&#21017;&#30340;&#19977;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#23454;&#29616;&#65306;&#19987;&#23478;&#28151;&#21512;&#65288;MoEs&#65289;&#32593;&#32476;&#12289;&#26631;&#35760;&#36873;&#25321;&#26426;&#21046;&#21644;&#25552;&#21069;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#26088;&#22312;&#21521;&#36825;&#19968;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#25552;&#20379;&#31867;&#20284;&#25945;&#31243;&#30340;&#20171;&#32461;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22359;&#21270;&#35774;&#35745;&#22312;&#25928;&#29575;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#38754;&#30340;&#22909;&#22788;&#65292;&#37325;&#28857;&#25918;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07965v1 Announce Type: cross  Abstract: This article summarizes principles and ideas from the emerging area of applying \textit{conditional computation} methods to the design of neural networks. In particular, we focus on neural networks that can dynamically activate or de-activate parts of their computational graph conditionally on their input. Examples include the dynamic selection of, e.g., input tokens, layers (or sets of layers), and sub-modules inside each layer (e.g., channels in a convolutional filter). We first provide a general formalism to describe these techniques in an uniform way. Then, we introduce three notable implementations of these principles: mixture-of-experts (MoEs) networks, token selection mechanisms, and early-exit neural networks. The paper aims to provide a tutorial-like introduction to this growing field. To this end, we analyze the benefits of these modular designs in terms of efficiency, explainability, and transfer learning, with a focus on em
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25581;&#31034;&#20102;&#22312;&#26576;&#20123;&#20219;&#21153;&#31867;&#21035;&#20013;&#65292;&#25945;&#24072;&#24378;&#21046;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22312;&#31532;&#19968;&#26102;&#38388;&#23398;&#20064;&#21040;&#20934;&#30830;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#36827;&#32780;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#19968;&#33324;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.06963</link><description>&lt;p&gt;
&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
The pitfalls of next-token prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06963
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25581;&#31034;&#20102;&#22312;&#26576;&#20123;&#20219;&#21153;&#31867;&#21035;&#20013;&#65292;&#25945;&#24072;&#24378;&#21046;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22312;&#31532;&#19968;&#26102;&#38388;&#23398;&#20064;&#21040;&#20934;&#30830;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#36827;&#32780;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#19968;&#33324;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31687;&#20851;&#20110;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#25285;&#24551;&#65306;&#19968;&#20010;&#20165;&#20165;&#22522;&#20110;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#24544;&#23454;&#22320;&#27169;&#25311;&#20154;&#31867;&#26234;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#32463;&#24120;&#28151;&#28102;&#30340;&#20004;&#20010;&#38454;&#27573; -- &#33258;&#22238;&#24402;&#25512;&#26029;&#21644;&#25945;&#24072;&#24378;&#21046;&#35757;&#32451; -- &#24517;&#39035;&#34987;&#21306;&#21035;&#23545;&#24453;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#19968;&#33324;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#25945;&#24072;&#24378;&#21046;&#22914;&#20309;&#22833;&#36133;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#35745;&#21010;&#20219;&#21153;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;Transformer&#21644;Mamba&#26550;&#26500;&#22312;&#23454;&#36341;&#20013;&#20197;&#36825;&#31181;&#26041;&#24335;&#22833;&#36133; -- &#23613;&#31649;&#20219;&#21153;&#26412;&#36523;&#24456;&#23481;&#26131;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06963v1 Announce Type: cross  Abstract: Can a mere next-token predictor faithfully model human intelligence? We crystallize this intuitive concern, which is fragmented in the literature. As a starting point, we argue that the two often-conflated phases of next-token prediction -- autoregressive inference and teacher-forced training -- must be treated distinctly. The popular criticism that errors can compound during autoregressive inference, crucially assumes that teacher-forcing has learned an accurate next-token predictor. This assumption sidesteps a more deep-rooted problem we expose: in certain classes of tasks, teacher-forcing can simply fail to learn an accurate next-token predictor in the first place. We describe a general mechanism of how teacher-forcing can fail, and design a minimal planning task where both the Transformer and the Mamba architecture empirically fail in that manner -- remarkably, despite the task being straightforward to learn. We provide preliminary
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;$\textbf{S}^2$IP-LLM&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#23558;&#35821;&#20041;&#31354;&#38388;&#19982;&#26102;&#38388;&#24207;&#21015;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.05798</link><description>&lt;p&gt;
$\textbf{S}^2$IP-LLM: &#20511;&#21161;LLM&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#35821;&#20041;&#31354;&#38388;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
$\textbf{S}^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05798
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;$\textbf{S}^2$IP-LLM&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#23558;&#35821;&#20041;&#31354;&#38388;&#19982;&#26102;&#38388;&#24207;&#21015;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#24314;&#31435;&#30340;LLM&#30340;&#35821;&#20041;&#31354;&#38388;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#20135;&#29983;&#26356;&#21152;&#29420;&#29305;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#20197;&#20419;&#36827;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20511;&#21161;LLM&#36827;&#34892;&#35821;&#20041;&#31354;&#38388;&#25552;&#31034;&#23398;&#20064;&#65288;$\textbf{S}^2$IP-LLM&#65289;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#31354;&#38388;&#19982;&#26102;&#38388;&#24207;&#21015;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#22522;&#20110;&#32852;&#21512;&#31354;&#38388;&#20013;&#23398;&#21040;&#30340;&#25552;&#31034;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#19987;&#20026;&#36328;&#27169;&#24577;&#23545;&#40784;&#23450;&#21046;&#30340;&#26631;&#35760;&#21270;&#27169;&#22359;&#65292;&#26174;&#24335;&#22320;&#20018;&#32852;&#20998;&#35299;&#30340;&#26102;&#38388;&#24207;&#21015;&#32452;&#20214;&#30340;&#34917;&#19969;&#65292;&#20197;&#21019;&#24314;&#33021;&#22815;&#26377;&#25928;&#32534;&#30721;&#26102;&#38388;&#21160;&#24577;&#30340;&#23884;&#20837;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#21333;&#35789;&#26631;&#35760;&#23884;&#20837;&#26469;&#23548;&#20986;&#35821;&#20041;&#38170;&#28857;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#23545;&#40784;&#25152;&#36873;&#38170;&#28857;&#19982;&#26102;&#38388;&#24207;&#21015;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05798v1 Announce Type: new  Abstract: Recently, there has been a growing interest in leveraging pre-trained large language models (LLMs) for various time series applications. However, the semantic space of LLMs, established through the pre-training, is still underexplored and may help yield more distinctive and informative representations to facilitate time series forecasting. To this end, we propose Semantic Space Informed Prompt learning with LLM ($S^2$IP-LLM) to align the pre-trained semantic space with time series embeddings space and perform time series forecasting based on learned prompts from the joint space. We first design a tokenization module tailored for cross-modality alignment, which explicitly concatenates patches of decomposed time series components to create embeddings that effectively encode the temporal dynamics. Next, we leverage the pre-trained word token embeddings to derive semantic anchors and align selected anchors with time series embeddings by maxi
&lt;/p&gt;</description></item><item><title>JAX-SPH&#26159;&#19968;&#20010;&#22312;JAX&#20013;&#23454;&#29616;&#30340;&#24179;&#28369;&#31890;&#23376;&#27969;&#20307;&#21160;&#21147;&#23398;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;LagrangeBench&#39033;&#30446;&#30340;&#20195;&#30721;&#65292;&#38598;&#25104;&#20102;&#20851;&#38190;&#30340;SPH&#31639;&#27861;&#65292;&#39564;&#35777;&#20102;&#26799;&#24230;&#30340;&#20934;&#30830;&#24615;&#65292;&#23637;&#31034;&#20102;&#26799;&#24230;&#22312;&#35299;&#20915;&#36870;&#38382;&#39064;&#21644;Solver-i&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.04750</link><description>&lt;p&gt;
JAX-SPH&#65306;&#19968;&#31181;&#21487;&#24494;&#24179;&#28369;&#31890;&#23376;&#27969;&#20307;&#21160;&#21147;&#23398;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
JAX-SPH: A Differentiable Smoothed Particle Hydrodynamics Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04750
&lt;/p&gt;
&lt;p&gt;
JAX-SPH&#26159;&#19968;&#20010;&#22312;JAX&#20013;&#23454;&#29616;&#30340;&#24179;&#28369;&#31890;&#23376;&#27969;&#20307;&#21160;&#21147;&#23398;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;LagrangeBench&#39033;&#30446;&#30340;&#20195;&#30721;&#65292;&#38598;&#25104;&#20102;&#20851;&#38190;&#30340;SPH&#31639;&#27861;&#65292;&#39564;&#35777;&#20102;&#26799;&#24230;&#30340;&#20934;&#30830;&#24615;&#65292;&#23637;&#31034;&#20102;&#26799;&#24230;&#22312;&#35299;&#20915;&#36870;&#38382;&#39064;&#21644;Solver-i&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31890;&#23376;&#30340;&#27969;&#20307;&#27169;&#25311;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;Navier-Stokes&#26041;&#31243;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#22797;&#26434;&#29289;&#29702;&#21644;&#33258;&#30001;&#34920;&#38754;&#30340;&#24773;&#20917;&#19979;&#12290;&#26368;&#36817;&#23558;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#28155;&#21152;&#21040;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#30340;&#24037;&#20855;&#31665;&#20013;&#27491;&#22312;&#25512;&#21160;&#36825;&#20123;&#25968;&#20540;&#27169;&#25311;&#30340;&#36136;&#37327;&#19982;&#36895;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#36793;&#30028;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#39046;&#20102;&#19982;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20860;&#23481;&#30340;&#25289;&#26684;&#26391;&#26085;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#26032;&#26041;&#21521;&#65292;&#24182;&#25552;&#20986;&#20102;JAX-SPH&#8212;&#8212;&#19968;&#20010;&#22312;JAX&#20013;&#23454;&#29616;&#30340;&#24179;&#28369;&#31890;&#23376;&#27969;&#20307;&#21160;&#21147;&#23398;&#65288;SPH&#65289;&#26694;&#26550;&#12290;JAX-SPH&#22522;&#20110;&#20174;LagrangeBench&#39033;&#30446;&#65288;Toshev&#31561;&#20154;&#65292;2023&#24180;&#65289;&#20013;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#20195;&#30721;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#25193;&#23637;&#20102;&#27492;&#20195;&#30721;&#65306;(a)&#38598;&#25104;&#20102;&#36827;&#19968;&#27493;&#30340;&#20851;&#38190;SPH&#31639;&#27861;&#65292;(b)&#23558;&#20195;&#30721;&#37325;&#32452;&#20026;Python&#24211;&#65292;(c)&#36890;&#36807;&#27714;&#35299;&#22120;&#39564;&#35777;&#26799;&#24230;&#65292;&#20197;&#21450;(d)&#28436;&#31034;&#20102;&#36825;&#20123;&#26799;&#24230;&#22312;&#35299;&#20915;&#36870;&#38382;&#39064;&#21644;Solver-i&#20013;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04750v1 Announce Type: cross  Abstract: Particle-based fluid simulations have emerged as a powerful tool for solving the Navier-Stokes equations, especially in cases that include intricate physics and free surfaces. The recent addition of machine learning methods to the toolbox for solving such problems is pushing the boundary of the quality vs. speed tradeoff of such numerical simulations. In this work, we lead the way to Lagrangian fluid simulators compatible with deep learning frameworks, and propose JAX-SPH - a Smoothed Particle Hydrodynamics (SPH) framework implemented in JAX. JAX-SPH builds on the code for dataset generation from the LagrangeBench project (Toshev et al., 2023) and extends this code in multiple ways: (a) integration of further key SPH algorithms, (b) restructuring the code toward a Python library, (c) verification of the gradients through the solver, and (d) demonstration of the utility of the gradients for solving inverse problems as well as a Solver-i
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20219;&#20309;&#25299;&#25169;&#29305;&#24449;&#19978;&#30340;&#25299;&#25169;&#24863;&#30693;&#36827;&#34892;&#34920;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.04482</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#24863;&#30693;&#21644;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
On the Topology Awareness and Generalization Performance of Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04482
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20219;&#20309;&#25299;&#25169;&#29305;&#24449;&#19978;&#30340;&#25299;&#25169;&#24863;&#30693;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#22312;&#22270;&#19978;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#34920;&#31034;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;GNNs&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#26159;&#23427;&#20204;&#21033;&#29992;&#22270;&#32467;&#26500;&#20316;&#20026;&#36755;&#20837;&#65292;&#20174;&#32780;&#33021;&#22815;&#21033;&#29992;&#22270;&#30340;&#22266;&#26377;&#25299;&#25169;&#23646;&#24615;&#65292;&#21363;GNNs&#30340;&#25299;&#25169;&#24863;&#30693;&#12290;&#23613;&#31649;GNNs&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25299;&#25169;&#24863;&#30693;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;&#20173;&#26410;&#34987;&#25506;&#35752;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#19982;&#25968;&#25454;&#29420;&#31435;&#21516;&#20998;&#24067;(I.I.D.)&#30340;&#20551;&#35774;&#32972;&#36947;&#32780;&#39536;&#30340;&#33410;&#28857;&#32423;&#20219;&#21153;&#12290;&#23545;&#20110;GNNs&#30340;&#25299;&#25169;&#24863;&#30693;&#30340;&#31934;&#30830;&#23450;&#20041;&#21644;&#34920;&#24449;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#19981;&#21516;&#25299;&#25169;&#29305;&#24449;&#30340;&#24773;&#20917;&#65292;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;GNNs&#22312;&#20219;&#20309;&#25299;&#25169;&#29305;&#24449;&#19978;&#30340;&#25299;&#25169;&#24863;&#30693;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04482v1 Announce Type: new  Abstract: Many computer vision and machine learning problems are modelled as learning tasks on graphs, where graph neural networks (GNNs) have emerged as a dominant tool for learning representations of graph-structured data. A key feature of GNNs is their use of graph structures as input, enabling them to exploit the graphs' inherent topological properties-known as the topology awareness of GNNs. Despite the empirical successes of GNNs, the influence of topology awareness on generalization performance remains unexplored, particularly for node-level tasks that diverge from the assumption of data being independent and identically distributed (I.I.D.). The precise definition and characterization of the topology awareness of GNNs, especially concerning different topological features, are still unclear. This paper introduces a comprehensive framework to characterize the topology awareness of GNNs across any topological feature. Using this framework, we
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#30446;&#26631;&#33829;&#38144;&#30340;&#24378;&#30423;&#21033;&#28070;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#25932;&#23545;&#24378;&#30423;&#24773;&#22659;&#19979;&#30340;&#36817;&#20046;&#26368;&#20248;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01361</link><description>&lt;p&gt;
&#38024;&#23545;&#30446;&#26631;&#33829;&#38144;&#30340;&#24378;&#30423;&#21033;&#28070;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bandit Profit-maximization for Targeted Marketing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01361
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#30446;&#26631;&#33829;&#38144;&#30340;&#24378;&#30423;&#21033;&#28070;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#25932;&#23545;&#24378;&#30423;&#24773;&#22659;&#19979;&#30340;&#36817;&#20046;&#26368;&#20248;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#39034;&#24207;&#21033;&#28070;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#20248;&#21270;&#20215;&#26684;&#21644;&#20687;&#33829;&#38144;&#25903;&#20986;&#36825;&#26679;&#30340;&#36741;&#21161;&#21464;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#19968;&#20010;&#20219;&#24847;&#24207;&#21015;&#30340;&#22810;&#20010;&#38656;&#27714;&#26354;&#32447;&#19978;&#26368;&#22823;&#21270;&#21033;&#28070;&#65292;&#27599;&#20010;&#26354;&#32447;&#20381;&#36182;&#20110;&#19968;&#20010;&#19981;&#21516;&#30340;&#36741;&#21161;&#21464;&#37327;&#65292;&#20294;&#20849;&#20139;&#30456;&#21516;&#30340;&#20215;&#26684;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#38024;&#23545;&#33829;&#38144;&#65292;&#20854;&#20013;&#19968;&#23478;&#20844;&#21496;&#65288;&#21334;&#26041;&#65289;&#24076;&#26395;&#22312;&#22810;&#20010;&#24066;&#22330;&#19978;&#38144;&#21806;&#20135;&#21697;&#12290;&#20844;&#21496;&#21487;&#20197;&#20026;&#19981;&#21516;&#24066;&#22330;&#25237;&#20837;&#19981;&#21516;&#30340;&#33829;&#38144;&#25903;&#20986;&#20197;&#20248;&#21270;&#23458;&#25143;&#33719;&#21462;&#65292;&#20294;&#24517;&#39035;&#22312;&#25152;&#26377;&#24066;&#22330;&#19978;&#20445;&#25345;&#30456;&#21516;&#30340;&#20215;&#26684;&#12290;&#27492;&#22806;&#65292;&#24066;&#22330;&#21487;&#33021;&#20855;&#26377;&#24322;&#36136;&#30340;&#38656;&#27714;&#26354;&#32447;&#65292;&#27599;&#20010;&#38656;&#27714;&#26354;&#32447;&#23545;&#20215;&#26684;&#21644;&#33829;&#38144;&#25903;&#20986;&#30340;&#21709;&#24212;&#26041;&#24335;&#19981;&#21516;&#12290;&#20844;&#21496;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#27611;&#21033;&#28070;&#65292;&#21363;&#24635;&#25910;&#20837;&#20943;&#21435;&#33829;&#38144;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01361v1 Announce Type: new  Abstract: We study a sequential profit-maximization problem, optimizing for both price and ancillary variables like marketing expenditures. Specifically, we aim to maximize profit over an arbitrary sequence of multiple demand curves, each dependent on a distinct ancillary variable, but sharing the same price. A prototypical example is targeted marketing, where a firm (seller) wishes to sell a product over multiple markets. The firm may invest different marketing expenditures for different markets to optimize customer acquisition, but must maintain the same price across all markets. Moreover, markets may have heterogeneous demand curves, each responding to prices and marketing expenditures differently. The firm's objective is to maximize its gross profit, the total revenue minus marketing costs.   Our results are near-optimal algorithms for this class of problems in an adversarial bandit setting, where demand curves are arbitrary non-adaptive seque
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34892;&#19994;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#23398;&#20064;&#36866;&#24403;&#34920;&#31034;&#30340;&#25361;&#25112;&#21644;&#23454;&#36341;&#65292;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#22788;&#29702;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#23884;&#20837;&#30340;&#32500;&#24230;&#22349;&#32553;&#21644;&#36328;&#20219;&#21153;&#25110;&#22330;&#26223;&#30340;&#20852;&#36259;&#32416;&#32544;&#12290;</title><link>https://arxiv.org/abs/2403.00793</link><description>&lt;p&gt;
&#22312;&#19968;&#20010;&#28151;&#20081;&#32780;&#32416;&#32544;&#30340;&#19990;&#30028;&#20013;&#30340;&#24191;&#21578;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Ad Recommendation in a Collapsed and Entangled World
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00793
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34892;&#19994;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#23398;&#20064;&#36866;&#24403;&#34920;&#31034;&#30340;&#25361;&#25112;&#21644;&#23454;&#36341;&#65292;&#37319;&#29992;&#22810;&#31181;&#26041;&#27861;&#22788;&#29702;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#23884;&#20837;&#30340;&#32500;&#24230;&#22349;&#32553;&#21644;&#36328;&#20219;&#21153;&#25110;&#22330;&#26223;&#30340;&#20852;&#36259;&#32416;&#32544;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34892;&#19994;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#65292;&#20851;&#27880;&#23398;&#20064;&#36866;&#24403;&#34920;&#31034;&#30340;&#25361;&#25112;&#21644;&#23454;&#36341;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#23637;&#31034;&#22914;&#20309;&#22312;&#23545;&#21508;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#36827;&#34892;&#23884;&#20837;&#34920;&#31034;&#26102;&#20445;&#30041;&#20808;&#39564;&#24320;&#22987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24207;&#21015;&#29305;&#24449;&#12289;&#25968;&#20540;&#29305;&#24449;&#12289;&#39044;&#35757;&#32451;&#23884;&#20837;&#29305;&#24449;&#20197;&#21450;&#31232;&#30095;ID&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#19982;&#29305;&#24449;&#34920;&#31034;&#30456;&#20851;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#23884;&#20837;&#30340;&#32500;&#24230;&#22349;&#32553;&#21644;&#36328;&#22810;&#20010;&#20219;&#21153;&#25110;&#22330;&#26223;&#30340;&#20852;&#36259;&#32416;&#32544;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#23454;&#29992;&#26041;&#27861;&#26469;&#26377;&#25928;&#24212;&#23545;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20960;&#31181;&#35757;&#32451;&#25216;&#26415;&#65292;&#20197;&#20419;&#36827;&#27169;&#22411;&#20248;&#21270;&#65292;&#20943;&#23569;&#20559;&#24046;&#24182;&#22686;&#24378;&#25506;&#32034;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#20998;&#26512;&#24037;&#20855;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20840;&#38754;&#30740;&#31350;&#29305;&#24449;&#30456;&#20851;&#24615;&#12289;&#32500;&#24230;&#22349;&#32553;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00793v1 Announce Type: cross  Abstract: In this paper, we present an industry ad recommendation system, paying attention to the challenges and practices of learning appropriate representations. Our study begins by showcasing our approaches to preserving priors when encoding features of diverse types into embedding representations. Specifically, we address sequence features, numeric features, pre-trained embedding features, as well as sparse ID features. Moreover, we delve into two pivotal challenges associated with feature representation: the dimensional collapse of embeddings and the interest entanglement across various tasks or scenarios. Subsequently, we propose several practical approaches to effectively tackle these two challenges. We then explore several training techniques to facilitate model optimization, reduce bias, and enhance exploration. Furthermore, we introduce three analysis tools that enable us to comprehensively study feature correlation, dimensional collap
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#22312;&#27169;&#22359;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#30340;&#36890;&#29992;&#27169;&#22359;&#21040;&#27169;&#22359;&#30693;&#35782;&#33976;&#39311;&#65288;m2mKD&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#27169;&#22359;&#21270;Transformer&#35757;&#32451;&#20013;&#30340;&#20248;&#21270;&#22256;&#38590;&#21644;&#21442;&#25968;&#25968;&#37327;&#24222;&#22823;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16918</link><description>&lt;p&gt;
m2mKD&#65306;&#27169;&#22359;&#38388;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#27169;&#22359;&#21270;Transformer
&lt;/p&gt;
&lt;p&gt;
m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16918
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#22312;&#27169;&#22359;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#30340;&#36890;&#29992;&#27169;&#22359;&#21040;&#27169;&#22359;&#30693;&#35782;&#33976;&#39311;&#65288;m2mKD&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#27169;&#22359;&#21270;Transformer&#35757;&#32451;&#20013;&#30340;&#20248;&#21270;&#22256;&#38590;&#21644;&#21442;&#25968;&#25968;&#37327;&#24222;&#22823;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22359;&#21270;&#31070;&#32463;&#32467;&#26500;&#22240;&#20854;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23545;&#26032;&#39046;&#22495;&#30340;&#39640;&#25928;&#36866;&#24212;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#27169;&#22359;&#21270;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#26089;&#26399;&#38454;&#27573;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#31232;&#30095;&#36830;&#25509;&#23548;&#33268;&#30340;&#20248;&#21270;&#22256;&#38590;&#65292;&#23384;&#22312;&#25361;&#25112;&#12290;&#21033;&#29992;&#26469;&#33258;&#25972;&#20307;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#22914;&#30693;&#35782;&#33976;&#39311;&#31561;&#25216;&#26415;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#35757;&#32451;&#27169;&#22359;&#21270;&#27169;&#22411;&#65292;&#24182;&#20351;&#23427;&#20204;&#33021;&#22815;&#25972;&#21512;&#26469;&#33258;&#22312;&#22810;&#20010;&#26469;&#28304;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#24182;&#19981;&#38024;&#23545;&#27169;&#22359;&#21270;&#27169;&#22411;&#35774;&#35745;&#65292;&#30452;&#25509;&#24212;&#29992;&#26102;&#21487;&#33021;&#22833;&#36133;&#65292;&#36825;&#26159;&#30001;&#20110;&#29420;&#29305;&#30340;&#26550;&#26500;&#21644;&#22823;&#37327;&#28041;&#21450;&#30340;&#21442;&#25968;&#12290;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#27169;&#22359;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#30340;&#36890;&#29992;&#27169;&#22359;&#21040;&#27169;&#22359;&#30693;&#35782;&#33976;&#39311;&#65288;m2mKD&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16918v1 Announce Type: new  Abstract: Modular neural architectures are gaining increasing attention due to their powerful capability for generalization and sample-efficient adaptation to new domains. However, training modular models, particularly in the early stages, poses challenges due to the optimization difficulties arising from their intrinsic sparse connectivity. Leveraging the knowledge from monolithic models, using techniques such as knowledge distillation, is likely to facilitate the training of modular models and enable them to integrate knowledge from multiple models pretrained on diverse sources. Nevertheless, conventional knowledge distillation approaches are not tailored to modular models and can fail when directly applied due to the unique architectures and the enormous number of parameters involved. Motivated by these challenges, we propose a general module-to-module knowledge distillation (m2mKD) method for transferring knowledge between modules. Our approac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#37319;&#29992;&#21512;&#20316;&#21338;&#24328;&#35770;&#35299;&#37322;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#20013;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#30340;&#26032;&#29702;&#35770;&#65292;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;</title><link>https://arxiv.org/abs/2402.15259</link><description>&lt;p&gt;
&#37319;&#29992;&#21512;&#20316;&#21338;&#24328;&#35770;&#30340;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Open Ad Hoc Teamwork with Cooperative Game Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15259
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#37319;&#29992;&#21512;&#20316;&#21338;&#24328;&#35770;&#35299;&#37322;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#20013;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#30340;&#26032;&#29702;&#35770;&#65292;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#38754;&#20020;&#30528;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#19982;&#38431;&#21451;&#21327;&#20316;&#20294;&#27809;&#26377;&#20808;&#21069;&#21327;&#35843;&#25110;&#32852;&#21512;&#35757;&#32451;&#30340;&#26234;&#33021;&#20307;&#12290;&#24320;&#25918;&#24335;&#21363;&#20852;&#22242;&#38431;&#21512;&#20316;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#32771;&#34385;&#20102;&#20855;&#26377;&#19981;&#26029;&#21464;&#21270;&#30340;&#38431;&#21451;&#25968;&#37327;&#30340;&#29615;&#22659;&#65292;&#21363;&#24320;&#25918;&#24335;&#22242;&#38431;&#12290;&#29616;&#26377;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26159;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31574;&#30053;&#23398;&#20064;&#65288;GPL&#65289;&#65292;&#21033;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#26469;&#22788;&#29702;&#26080;&#38480;&#25968;&#37327;&#30340;&#26234;&#33021;&#20307;&#65292;&#26377;&#25928;&#24212;&#23545;&#24320;&#25918;&#24335;&#22242;&#38431;&#12290;GPL&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20294;&#20854;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#23545;&#35299;&#37322;&#36896;&#25104;&#20102;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#21644;&#24212;&#29992;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#65292;&#20174;&#21512;&#20316;&#21338;&#24328;&#35770;&#30340;&#35282;&#24230;&#20026;GPL&#20013;&#37319;&#29992;&#30340;&#32852;&#21512;Q&#20540;&#34920;&#31034;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#37322;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15259v1 Announce Type: cross  Abstract: Ad hoc teamwork poses a challenging problem, requiring the design of an agent to collaborate with teammates without prior coordination or joint training. Open ad hoc teamwork further complicates this challenge by considering environments with a changing number of teammates, referred to as open teams. The state-of-the-art solution to this problem is graph-based policy learning (GPL), leveraging the generalizability of graph neural networks to handle an unrestricted number of agents and effectively address open teams. GPL's performance is superior to other methods, but its joint Q-value representation presents challenges for interpretation, hindering further development of this research line and applicability. In this paper, we establish a new theory to give an interpretation for the joint Q-value representation employed in GPL, from the perspective of cooperative game theory. Building on our theory, we propose a novel algorithm based on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#19981;&#30830;&#23450;&#24615;&#20027;&#21160;&#23398;&#20064;&#65288;BUAL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#38543;&#26426;&#26631;&#31614;&#36127;&#23398;&#20064;&#26041;&#27861;&#21644;&#21452;&#21521;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31574;&#30053;&#65292;&#26088;&#22312;&#21516;&#26102;&#31579;&#36873;&#26082;&#21487;&#33021;&#23646;&#20110;&#24050;&#30693;&#31867;&#21035;&#21448;&#39640;&#24230;&#20449;&#24687;&#20016;&#23500;&#30340;&#31034;&#20363;&#65292;&#35299;&#20915;&#20102;&#22312;&#24320;&#25918;&#38598;&#22330;&#26223;&#19979;&#30830;&#23450;&#26368;&#26377;&#20215;&#20540;&#31034;&#20363;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15198</link><description>&lt;p&gt;
&#24320;&#25918;&#38598;&#27880;&#37322;&#30340;&#21452;&#21521;&#19981;&#30830;&#23450;&#24615;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Uncertainty-Based Active Learning for Open Set Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#19981;&#30830;&#23450;&#24615;&#20027;&#21160;&#23398;&#20064;&#65288;BUAL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#38543;&#26426;&#26631;&#31614;&#36127;&#23398;&#20064;&#26041;&#27861;&#21644;&#21452;&#21521;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31574;&#30053;&#65292;&#26088;&#22312;&#21516;&#26102;&#31579;&#36873;&#26082;&#21487;&#33021;&#23646;&#20110;&#24050;&#30693;&#31867;&#21035;&#21448;&#39640;&#24230;&#20449;&#24687;&#20016;&#23500;&#30340;&#31034;&#20363;&#65292;&#35299;&#20915;&#20102;&#22312;&#24320;&#25918;&#38598;&#22330;&#26223;&#19979;&#30830;&#23450;&#26368;&#26377;&#20215;&#20540;&#31034;&#20363;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#38598;&#22330;&#26223;&#19979;&#30340;&#20027;&#21160;&#23398;&#20064;(AL)&#38754;&#20020;&#30528;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#21363;&#30830;&#23450;&#22312;&#19968;&#20010;&#21253;&#21547;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#25968;&#25454;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#27744;&#20013;&#35782;&#21035;&#26368;&#26377;&#20215;&#20540;&#30340;&#31034;&#20363;&#12290;&#20256;&#32479;&#26041;&#27861;&#20248;&#20808;&#36873;&#25321;&#32622;&#20449;&#24230;&#36739;&#20302;&#30340;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#31034;&#20363;&#65292;&#23384;&#22312;&#35823;&#36873;&#32622;&#20449;&#24230;&#21516;&#26679;&#36739;&#20302;&#30340;&#26410;&#30693;&#31867;&#21035;&#31034;&#20363;&#30340;&#39118;&#38505;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#26356;&#38738;&#30544;&#26368;&#26377;&#21487;&#33021;&#23646;&#20110;&#24050;&#30693;&#31867;&#21035;&#30340;&#31034;&#20363;&#65292;&#20294;&#23384;&#22312;&#36873;&#21462;&#24050;&#32463;&#25484;&#25569;&#30340;&#31616;&#21333;&#31034;&#20363;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#26597;&#35810;&#26082;&#26377;&#21487;&#33021;&#26469;&#33258;&#24050;&#30693;&#31867;&#21035;&#21448;&#39640;&#24230;&#20449;&#24687;&#20016;&#23500;&#30340;&#31034;&#20363;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;\textit{&#21452;&#21521;&#19981;&#30830;&#23450;&#24615;&#20027;&#21160;&#23398;&#20064;}&#65288;BUAL&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;\textit{&#38543;&#26426;&#26631;&#31614;&#36127;&#23398;&#20064;}&#26041;&#27861;&#23558;&#26410;&#30693;&#31867;&#21035;&#31034;&#20363;&#25512;&#21521;&#39640;&#32622;&#20449;&#24230;&#39044;&#27979;&#21306;&#22495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;\textit{&#21452;&#21521;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;}&#31574;&#30053;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15198v1 Announce Type: new  Abstract: Active learning (AL) in open set scenarios presents a novel challenge of identifying the most valuable examples in an unlabeled data pool that comprises data from both known and unknown classes. Traditional methods prioritize selecting informative examples with low confidence, with the risk of mistakenly selecting unknown-class examples with similarly low confidence. Recent methods favor the most probable known-class examples, with the risk of picking simple already mastered examples. In this paper, we attempt to query examples that are both likely from known classes and highly informative, and propose a \textit{Bidirectional Uncertainty-based Active Learning} (BUAL) framework. Specifically, we achieve this by first pushing the unknown class examples toward regions with high-confidence predictions with our proposed \textit{Random Label Negative Learning} method. Then, we propose a \textit{Bidirectional Uncertainty sampling} strategy by j
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CS-CNNs&#65289;&#65292;&#36890;&#36807;&#22312;&#20266;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#19978;&#22788;&#29702;&#22810;&#30690;&#22330;&#65292;&#21033;&#29992;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;$\mathrm{O}(p,q)$&#21487;&#23548;&#26680;&#36827;&#34892;&#38544;&#24335;&#21442;&#25968;&#21270;&#65292;&#26174;&#30528;&#19988;&#19968;&#33268;&#22320;&#20248;&#20110;&#27969;&#20307;&#21160;&#21147;&#23398;&#21644;&#30456;&#23545;&#35770;&#30005;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#30340;&#22522;&#20934;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.14730</link><description>&lt;p&gt;
Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Clifford-Steerable Convolutional Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14730
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CS-CNNs&#65289;&#65292;&#36890;&#36807;&#22312;&#20266;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#19978;&#22788;&#29702;&#22810;&#30690;&#22330;&#65292;&#21033;&#29992;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;$\mathrm{O}(p,q)$&#21487;&#23548;&#26680;&#36827;&#34892;&#38544;&#24335;&#21442;&#25968;&#21270;&#65292;&#26174;&#30528;&#19988;&#19968;&#33268;&#22320;&#20248;&#20110;&#27969;&#20307;&#21160;&#21147;&#23398;&#21644;&#30456;&#23545;&#35770;&#30005;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#30340;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CS-CNNs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;$\mathrm{E}(p, q)$&#31561;&#21464;CNN&#31867;&#12290; CS-CNNs&#22312;&#20266;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;$\mathbb{R}^{p,q}$&#19978;&#22788;&#29702;&#22810;&#30690;&#22330;&#12290; &#23427;&#20204;&#28085;&#30422;&#20102;&#20363;&#22914;$\mathrm{E}(3)$&#22312;$\mathbb{R}^3$&#19978;&#21644;Poincar\'e&#22312;&#38389;&#21487;&#22827;&#26031;&#22522;&#26102;&#31354;$\mathbb{R}^{1,3}$&#19978;&#30340;&#31561;&#21464;&#24615;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36890;&#36807;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;$\mathrm{O}(p,q)$&#21487;&#23548;&#26680;&#36827;&#34892;&#38544;&#24335;&#21442;&#25968;&#21270;&#12290; &#22312;&#27969;&#20307;&#21160;&#21147;&#23398;&#21644;&#30456;&#23545;&#35770;&#30005;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#26041;&#27861;&#19978;&#26174;&#30528;&#19988;&#19968;&#33268;&#22320;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14730v1 Announce Type: cross  Abstract: We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a novel class of $\mathrm{E}(p, q)$-equivariant CNNs. CS-CNNs process multivector fields on pseudo-Euclidean spaces $\mathbb{R}^{p,q}$. They cover, for instance, $\mathrm{E}(3)$-equivariance on $\mathbb{R}^3$ and Poincar\'e-equivariance on Minkowski spacetime $\mathbb{R}^{1,3}$. Our approach is based on an implicit parametrization of $\mathrm{O}(p,q)$-steerable kernels via Clifford group equivariant neural networks. We significantly and consistently outperform baseline methods on fluid dynamics as well as relativistic electrodynamics forecasting tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#33021;&#22815;&#25104;&#21151;&#22320;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.09132</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Adversarial Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#33021;&#22815;&#25104;&#21151;&#22320;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#24191;&#27867;&#21644;&#26222;&#36941;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24378;&#22823;&#30340;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#65292;&#20026;&#34892;&#19994;&#21644;&#30740;&#31350;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#33021;&#21542;&#34920;&#29616;&#20986;&#23545;&#25239;&#34892;&#20026;&#30340;&#31243;&#24230;&#20173;&#28982;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30740;&#31350;&#24120;&#35265;&#30340;&#20844;&#24320;&#21487;&#29992;LLMs&#26159;&#21542;&#20855;&#26377;&#33021;&#21147;&#25200;&#20081;&#25991;&#26412;&#26679;&#26412;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#21363;&#25152;&#35859;&#30340;&#23545;&#25239;&#31034;&#20363;&#25110;&#25915;&#20987;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35843;&#26597;LLMs&#26159;&#21542;&#26412;&#36136;&#19978;&#33021;&#22815;&#20174;&#33391;&#24615;&#26679;&#26412;&#20013;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#29616;&#26377;&#30340;&#23433;&#20840;&#38450;&#32447;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#37325;&#28857;&#20851;&#27880;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65292;&#21457;&#29616;LLMs&#25104;&#21151;&#22320;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#26377;&#25928;&#22320;&#30772;&#22351;&#20102;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#65288;&#21322;&#65289;&#33258;&#21160;&#21270;&#23433;&#20840;&#35780;&#20272;&#21644;&#38450;&#24481;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09132v1 Announce Type: new Abstract: The proliferation of large language models (LLMs) has sparked widespread and general interest due to their strong language generation capabilities, offering great potential for both industry and research. While previous research delved into the security and privacy issues of LLMs, the extent to which these models can exhibit adversarial behavior remains largely unexplored. Addressing this gap, we investigate whether common publicly available LLMs have inherent capabilities to perturb text samples to fool safety measures, so-called adversarial examples resp.~attacks. More specifically, we investigate whether LLMs are inherently able to craft adversarial examples out of benign samples to fool existing safe rails. Our experiments, which focus on hate speech detection, reveal that LLMs succeed in finding adversarial perturbations, effectively undermining hate speech detection systems. Our findings carry significant implications for (semi-)aut
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26102;&#31354;&#28151;&#21512;&#31574;&#30053;&#29983;&#25104;&#29420;&#31435;&#21516;&#20998;&#24067;&#21512;&#25104;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#38543;&#26426;&#36807;&#31243;&#23454;&#29616;&#26368;&#20339;&#36716;&#36816;&#65292;&#36827;&#19968;&#27493;&#32454;&#21270;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#25216;&#26415;&#35757;&#32451;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.08847</link><description>&lt;p&gt;
&#26102;&#31354;&#26725;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Space-Time Bridge-Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08847
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26102;&#31354;&#28151;&#21512;&#31574;&#30053;&#29983;&#25104;&#29420;&#31435;&#21516;&#20998;&#24067;&#21512;&#25104;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#38543;&#26426;&#36807;&#31243;&#23454;&#29616;&#26368;&#20339;&#36716;&#36816;&#65292;&#36827;&#19968;&#27493;&#32454;&#21270;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#25216;&#26415;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#30001;&#19968;&#32452;&#22320;&#38754;&#30495;&#23454;&#26679;&#26412;&#65288;GT&#26679;&#26412;&#65289;&#38544;&#24335;&#23450;&#20041;&#30340;&#39640;&#32500;&#23454;&#20540;&#27010;&#29575;&#20998;&#24067;&#20013;&#29983;&#25104;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#30340;&#26032;&#21512;&#25104;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#26102;&#31354;&#28151;&#21512;&#31574;&#30053;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#32500;&#24230;&#19978;&#36827;&#34892;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19977;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#26088;&#22312;&#23454;&#29616;&#20174;&#23481;&#26131;&#22788;&#29702;&#30340;&#21021;&#22987;&#27010;&#29575;&#20998;&#24067;&#21040;&#30001;GT&#26679;&#26412;&#34920;&#31034;&#30340;&#30446;&#26631;&#20998;&#24067;&#30340;&#26368;&#20339;&#36716;&#36816;&#65306;&#65288;a&#65289;&#21253;&#21547;&#26102;&#31354;&#28151;&#21512;&#30340;&#32447;&#24615;&#36807;&#31243;&#20135;&#29983;&#39640;&#26031;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#65292;&#65288;b&#65289;&#20854;&#26725;&#25193;&#25955;&#27169;&#25311;&#65292;&#26465;&#20214;&#20026;&#21021;&#22987;&#21644;&#26368;&#32456;&#29366;&#24577;&#21521;&#37327;&#65292;&#20197;&#21450;&#65288;c&#65289;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#25216;&#26415;&#36827;&#34892;&#32454;&#21270;&#30340;&#38750;&#32447;&#24615;&#38543;&#26426;&#36807;&#31243;&#12290;&#25105;&#20204;&#35757;&#32451;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#31934;&#35843;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08847v1 Announce Type: cross Abstract: In this study, we introduce a novel method for generating new synthetic samples that are independent and identically distributed (i.i.d.) from high-dimensional real-valued probability distributions, as defined implicitly by a set of Ground Truth (GT) samples. Central to our method is the integration of space-time mixing strategies that extend across temporal and spatial dimensions. Our methodology is underpinned by three interrelated stochastic processes designed to enable optimal transport from an easily tractable initial probability distribution to the target distribution represented by the GT samples: (a) linear processes incorporating space-time mixing that yield Gaussian conditional probability densities, (b) their bridge-diffusion analogs that are conditioned to the initial and final state vectors, and (c) nonlinear stochastic processes refined through score-matching techniques. The crux of our training regime involves fine-tuning
&lt;/p&gt;</description></item><item><title>&#26465;&#20214;&#31070;&#32463;&#19987;&#23478;&#36807;&#31243;&#65288;CNEP&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;&#20174;&#28436;&#31034;&#20013;&#33719;&#21462;&#25216;&#33021;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24335;&#30340;&#28436;&#31034;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#19987;&#23478;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20449;&#24687;&#23558;&#19987;&#23478;&#19982;&#32534;&#30721;&#34920;&#31034;&#21305;&#37197;&#65292;&#35299;&#20915;&#20102;&#30456;&#21516;&#25216;&#33021;&#28436;&#31034;&#30340;&#21464;&#21270;&#21644;&#22810;&#31181;&#26041;&#24335;&#33719;&#21462;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08424</link><description>&lt;p&gt;
&#26465;&#20214;&#31070;&#32463;&#19987;&#23478;&#36807;&#31243;&#29992;&#20110;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conditional Neural Expert Processes for Learning from Demonstration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08424
&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#31070;&#32463;&#19987;&#23478;&#36807;&#31243;&#65288;CNEP&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;&#20174;&#28436;&#31034;&#20013;&#33719;&#21462;&#25216;&#33021;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24335;&#30340;&#28436;&#31034;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#19987;&#23478;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20449;&#24687;&#23558;&#19987;&#23478;&#19982;&#32534;&#30721;&#34920;&#31034;&#21305;&#37197;&#65292;&#35299;&#20915;&#20102;&#30456;&#21516;&#25216;&#33021;&#28436;&#31034;&#30340;&#21464;&#21270;&#21644;&#22810;&#31181;&#26041;&#24335;&#33719;&#21462;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#65288;LfD&#65289;&#26159;&#26426;&#22120;&#20154;&#23398;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#25216;&#33021;&#33719;&#21462;&#12290;&#28982;&#32780;&#65292;&#30456;&#21516;&#25216;&#33021;&#30340;&#28436;&#31034;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#25110;&#32773;&#23398;&#20064;&#31995;&#32479;&#21487;&#33021;&#21516;&#26102;&#23581;&#35797;&#33719;&#21462;&#30456;&#21516;&#25216;&#33021;&#30340;&#19981;&#21516;&#26041;&#24335;&#65292;&#36825;&#20351;&#24471;&#23558;&#36825;&#20123;&#21160;&#20316;&#32534;&#30721;&#20026;&#36816;&#21160;&#21407;&#35821;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;LfD&#26694;&#26550;&#65292;&#21363;&#26465;&#20214;&#31070;&#32463;&#19987;&#23478;&#36807;&#31243;&#65288;CNEP&#65289;&#65292;&#23427;&#23398;&#20064;&#23558;&#26469;&#33258;&#19981;&#21516;&#27169;&#24335;&#30340;&#28436;&#31034;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#19987;&#23478;&#32593;&#32476;&#65292;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20869;&#22312;&#20449;&#24687;&#23558;&#19987;&#23478;&#19982;&#32534;&#30721;&#34920;&#31034;&#21305;&#37197;&#36215;&#26469;&#12290;CNEP&#19981;&#38656;&#35201;&#22312;&#21738;&#31181;&#27169;&#24335;&#19979;&#36712;&#36857;&#23646;&#20110;&#30340;&#30417;&#30563;&#12290;&#22312;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;CNEP&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;CNEP&#19982;&#21478;&#19968;&#20010;LfD&#26694;&#26550;&#8212;&#8212;&#26465;&#20214;&#31070;&#32463;&#36816;&#21160;&#21407;&#35821;&#65288;CNMP&#65289;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#23545;&#30495;&#23454;&#26426;&#22120;&#20154;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from Demonstration (LfD) is a widely used technique for skill acquisition in robotics. However, demonstrations of the same skill may exhibit significant variances, or learning systems may attempt to acquire different means of the same skill simultaneously, making it challenging to encode these motions into movement primitives. To address these challenges, we propose an LfD framework, namely the Conditional Neural Expert Processes (CNEP), that learns to assign demonstrations from different modes to distinct expert networks utilizing the inherent information within the latent space to match experts with the encoded representations. CNEP does not require supervision on which mode the trajectories belong to. Provided experiments on artificially generated datasets demonstrate the efficacy of CNEP. Furthermore, we compare the performance of CNEP with another LfD framework, namely Conditional Neural Movement Primitives (CNMP), on a range of tasks, including experiments on a real robo
&lt;/p&gt;</description></item><item><title>Pix2Code &#26159;&#19968;&#20010;&#23558;&#31070;&#32463;&#35270;&#35273;&#27010;&#24565;&#32452;&#21512;&#25104;&#31243;&#24207;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26174;&#24335;&#12289;&#32452;&#21512;&#30340;&#31526;&#21495;&#21644;&#38544;&#24335;&#30340;&#31070;&#32463;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#22270;&#20687;&#20013;&#26816;&#32034;&#23545;&#35937;&#34920;&#31034;&#24182;&#23558;&#20851;&#31995;&#27010;&#24565;&#21512;&#25104;&#20026;lambda&#28436;&#31639;&#31243;&#24207;&#65292;&#26469;&#35299;&#20915;&#36890;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;&#22312;&#25512;&#29702;&#39046;&#22495;Kandinsky Patterns&#21644;CURI&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Pix2Code &#33021;&#22815;&#35782;&#21035;&#32452;&#21512;&#35270;&#35273;&#27010;&#24565;&#24182;&#25512;&#24191;&#21040;&#26032;&#25968;&#25454;&#21644;&#25512;&#29702;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.08280</link><description>&lt;p&gt;
Pix2Code&#65306;&#23398;&#20064;&#23558;&#31070;&#32463;&#35270;&#35273;&#27010;&#24565;&#32452;&#21512;&#25104;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Pix2Code: Learning to Compose Neural Visual Concepts as Programs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08280
&lt;/p&gt;
&lt;p&gt;
Pix2Code &#26159;&#19968;&#20010;&#23558;&#31070;&#32463;&#35270;&#35273;&#27010;&#24565;&#32452;&#21512;&#25104;&#31243;&#24207;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26174;&#24335;&#12289;&#32452;&#21512;&#30340;&#31526;&#21495;&#21644;&#38544;&#24335;&#30340;&#31070;&#32463;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#22270;&#20687;&#20013;&#26816;&#32034;&#23545;&#35937;&#34920;&#31034;&#24182;&#23558;&#20851;&#31995;&#27010;&#24565;&#21512;&#25104;&#20026;lambda&#28436;&#31639;&#31243;&#24207;&#65292;&#26469;&#35299;&#20915;&#36890;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;&#22312;&#25512;&#29702;&#39046;&#22495;Kandinsky Patterns&#21644;CURI&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Pix2Code &#33021;&#22815;&#35782;&#21035;&#32452;&#21512;&#35270;&#35273;&#27010;&#24565;&#24182;&#25512;&#24191;&#21040;&#26032;&#25968;&#25454;&#21644;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#20174;&#22270;&#20687;&#20013;&#25277;&#35937;&#27010;&#24565;&#30340;&#25361;&#25112;&#22312;&#20110;&#38656;&#35201;&#23558;&#35270;&#35273;&#24863;&#30693;&#21644;&#36890;&#29992;&#20851;&#31995;&#25512;&#29702;&#36827;&#34892;&#25972;&#21512;&#12290;&#27492;&#22806;&#65292;&#35813;&#20219;&#21153;&#30340;&#26080;&#30417;&#30563;&#24615;&#36136;&#20351;&#24471;&#20154;&#31867;&#29992;&#25143;&#38656;&#35201;&#33021;&#22815;&#29702;&#35299;&#27169;&#22411;&#23398;&#21040;&#30340;&#27010;&#24565;&#65292;&#24182;&#21487;&#33021;&#20462;&#27491;&#38169;&#35823;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#35270;&#35273;&#27010;&#24565;&#23398;&#20064;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Pix2Code&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#31243;&#24207;&#21512;&#25104;&#25193;&#23637;&#21040;&#35270;&#35273;&#20851;&#31995;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#26126;&#30830;&#30340;&#12289;&#32452;&#21512;&#30340;&#31526;&#21495;&#21644;&#38544;&#24335;&#30340;&#31070;&#32463;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20174;&#22270;&#20687;&#20013;&#26816;&#32034;&#23545;&#35937;&#34920;&#31034;&#24182;&#23558;&#20851;&#31995;&#27010;&#24565;&#21512;&#25104;&#20026;lambda&#28436;&#31639;&#31243;&#24207;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#39046;&#22495;Kandinsky Patterns&#21644;CURI&#19978;&#35780;&#20272;&#20102;Pix2Code&#30340;&#22810;&#26679;&#29305;&#24615;&#65292;&#20174;&#32780;&#27979;&#35797;&#20854;&#35782;&#21035;&#32452;&#21512;&#35270;&#35273;&#27010;&#24565;&#24182;&#25512;&#24191;&#21040;&#26032;&#25968;&#25454;&#21644;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge in learning abstract concepts from images in an unsupervised fashion lies in the required integration of visual perception and generalizable relational reasoning. Moreover, the unsupervised nature of this task makes it necessary for human users to be able to understand a model's learnt concepts and potentially revise false behaviours. To tackle both the generalizability and interpretability constraints of visual concept learning, we propose Pix2Code, a framework that extends program synthesis to visual relational reasoning by utilizing the abilities of both explicit, compositional symbolic and implicit neural representations. This is achieved by retrieving object representations from images and synthesizing relational concepts as lambda-calculus programs. We evaluate the diverse properties of Pix2Code on the challenging reasoning domains, Kandinsky Patterns and CURI, thereby testing its ability to identify compositional visual concepts that generalize to novel data and co
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SMX&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#21019;&#24314;&#20102;&#26377;&#25928;&#30340;&#33258;&#25105;&#23398;&#20064;&#26426;&#21046;&#12290;&#23427;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#39640;&#24182;&#34892;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07963</link><description>&lt;p&gt;
SMX: &#19987;&#23478;&#36845;&#20195;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
SMX: Sequential Monte Carlo Planning for Expert Iteration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07963
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SMX&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#21019;&#24314;&#20102;&#26377;&#25928;&#30340;&#33258;&#25105;&#23398;&#20064;&#26426;&#21046;&#12290;&#23427;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#39640;&#24182;&#34892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#33021;&#22815;&#22312;&#20915;&#31574;&#21644;&#23398;&#20064;&#36807;&#31243;&#20013;&#21033;&#29992;&#35268;&#21010;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#26641;&#29366;&#25628;&#32034;&#26041;&#27861;&#21644;&#33258;&#25105;&#23545;&#24328;&#23398;&#20064;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25628;&#32034;&#36807;&#31243;&#30340;&#39034;&#24207;&#24615;&#36136;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38754;&#20020;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#34429;&#28982;&#23454;&#36341;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#37096;&#20998;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20173;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#21517;&#20026;SMX&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35745;&#21010;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21019;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33258;&#25105;&#23398;&#20064;&#26426;&#21046;&#12290;SMX&#22522;&#20110;&#25511;&#21046;&#20316;&#20026;&#25512;&#26029;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#21463;&#30410;&#20110;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#23427;&#22522;&#20110;&#37319;&#26679;&#30340;&#25628;&#32034;&#26041;&#27861;&#20351;&#20854;&#36866;&#24212;&#20855;&#26377;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#29615;&#22659;&#12290;&#27492;&#22806;&#65292;SMX&#20801;&#35768;&#39640;&#24230;&#24182;&#34892;&#21270;&#24182;&#21487;&#20197;&#36816;&#34892;&#20110;&#21508;&#31867;&#35745;&#31639;&#26426;&#35774;&#22791;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing agents that can leverage planning abilities during their decision and learning processes is critical to the advancement of Artificial Intelligence. Recent works have demonstrated the effectiveness of combining tree-based search methods and self-play learning mechanisms. Yet, these methods typically face scaling challenges due to the sequential nature of their search. While practical engineering solutions can partly overcome this, they still demand extensive computational resources, which hinders their applicability. In this paper, we introduce SMX, a model-based planning algorithm that utilises scalable Sequential Monte Carlo methods to create an effective self-learning mechanism. Grounded in the theoretical framework of control as inference, SMX benefits from robust theoretical underpinnings. Its sampling-based search approach makes it adaptable to environments with both discrete and continuous action spaces. Furthermore, SMX allows for high parallelisation and can run on h
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19968;&#20215;&#25293;&#21334;&#20013;&#36827;&#34892;&#31454;&#26631;&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#22312;&#25112;&#30053;&#32972;&#26223;&#19979;&#30340;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#38754;&#23545;&#23545;&#31574;&#24615;&#21334;&#23478;&#26102;&#34920;&#29616;&#33391;&#22909;&#65292;&#28608;&#21169;&#20080;&#23478;&#36827;&#34892;&#30495;&#23454;&#30340;&#20132;&#26131;&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#21518;&#24724;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07363</link><description>&lt;p&gt;
&#22522;&#20110;&#31574;&#30053;&#31283;&#23450;&#24615;&#30340;&#23398;&#20064;&#31639;&#27861;&#22312;&#19968;&#20215;&#25293;&#21334;&#20013;&#30340;&#31454;&#26631;
&lt;/p&gt;
&lt;p&gt;
Strategically-Robust Learning Algorithms for Bidding in First-Price Auctions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19968;&#20215;&#25293;&#21334;&#20013;&#36827;&#34892;&#31454;&#26631;&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#22312;&#25112;&#30053;&#32972;&#26223;&#19979;&#30340;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#38754;&#23545;&#23545;&#31574;&#24615;&#21334;&#23478;&#26102;&#34920;&#29616;&#33391;&#22909;&#65292;&#28608;&#21169;&#20080;&#23478;&#36827;&#34892;&#30495;&#23454;&#30340;&#20132;&#26131;&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#21518;&#24724;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28216;&#25103;&#29702;&#35770;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#30028;&#22788;&#65292;&#23398;&#20064;&#22312;&#37325;&#22797;&#30340;&#19968;&#20215;&#25293;&#21334;&#20013;&#36827;&#34892;&#31454;&#26631;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#30001;&#20110;&#26174;&#31034;&#24191;&#21578;&#36716;&#21521;&#19968;&#20215;&#25293;&#21334;&#65292;&#26368;&#36817;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20985;&#20989;&#25968;&#24418;&#24335;&#65292;&#29992;&#20110;&#19968;&#20215;&#25293;&#21334;&#20013;&#32431;&#31574;&#30053;&#30340;&#31454;&#26631;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20998;&#26512;&#36825;&#20010;&#38382;&#39064;&#30340;&#33258;&#28982;&#26799;&#24230;&#19978;&#21319;&#31639;&#27861;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36229;&#36234;&#20102;&#36807;&#21435;&#24037;&#20316;&#30340;&#24046;&#36317;&#65292;&#36824;&#32771;&#34385;&#20102;&#22312;&#32447;&#24191;&#21578;&#24066;&#22330;&#30340;&#25112;&#30053;&#32972;&#26223;&#65292;&#20854;&#20013;&#37096;&#32626;&#20102;&#31454;&#26631;&#31639;&#27861; - &#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#20250;&#34987;&#31574;&#30053;&#24615;&#21334;&#23478;&#21033;&#29992;&#65292;&#24182;&#19988;&#23427;&#20204;&#28608;&#21169;&#20080;&#23478;&#35802;&#23454;&#20132;&#26131;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#26368;&#39640;&#31454;&#20105;&#20986;&#20215;&#36890;&#36807;&#23545;&#25239;&#26041;&#24335;&#29983;&#25104;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;$O(\sqrt{T})$&#30340;&#21518;&#24724;&#65292;&#24182;&#34920;&#26126;&#27809;&#26377;&#26356;&#22909;&#30340;&#22312;&#32447;&#31639;&#27861;&#21487;&#20197;&#20570;&#24471;&#26356;&#22909;&#12290;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#24403;&#26368;&#39640;&#31454;&#20105;&#20986;&#20215;&#36890;&#36807;&#23545;&#25239;&#26041;&#24335;&#29983;&#25104;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;$O(\log T)$&#30340;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to bid in repeated first-price auctions is a fundamental problem at the interface of game theory and machine learning, which has seen a recent surge in interest due to the transition of display advertising to first-price auctions. In this work, we propose a novel concave formulation for pure-strategy bidding in first-price auctions, and use it to analyze natural Gradient-Ascent-based algorithms for this problem. Importantly, our analysis goes beyond regret, which was the typical focus of past work, and also accounts for the strategic backdrop of online-advertising markets where bidding algorithms are deployed -- we prove that our algorithms cannot be exploited by a strategic seller and that they incentivize truth-telling for the buyer.   Concretely, we show that our algorithms achieve $O(\sqrt{T})$ regret when the highest competing bids are generated adversarially, and show that no online algorithm can do better. We further prove that the regret improves to $O(\log T)$ when th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;&#19968;&#31181;&#21483;&#20570;&#31070;&#32463;SPH&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26631;&#20934;SPH&#27714;&#35299;&#22120;&#30340;&#32452;&#21512;&#26469;&#25913;&#36827;GNN&#27169;&#25311;&#22120;&#30340;&#24615;&#33021;&#65292;&#22312;&#20934;&#30830;&#24314;&#27169;&#29289;&#29702;&#29616;&#35937;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06275</link><description>&lt;p&gt;
&#31070;&#32463;SPH: &#25913;&#36827;&#30340;&#25289;&#26684;&#26391;&#26085;&#27969;&#20307;&#21160;&#21147;&#23398;&#31070;&#32463;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Neural SPH: Improved Neural Modeling of Lagrangian Fluid Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;&#19968;&#31181;&#21483;&#20570;&#31070;&#32463;SPH&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#26631;&#20934;SPH&#27714;&#35299;&#22120;&#30340;&#32452;&#21512;&#26469;&#25913;&#36827;GNN&#27169;&#25311;&#22120;&#30340;&#24615;&#33021;&#65292;&#22312;&#20934;&#30830;&#24314;&#27169;&#29289;&#29702;&#29616;&#35937;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#28369;&#31890;&#23376;&#27969;&#20307;&#21160;&#21147;&#23398;&#65288;SPH&#65289;&#22312;&#29616;&#20195;&#24037;&#31243;&#21644;&#31185;&#23398;&#39046;&#22495;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;SPH&#26159;&#19968;&#31867;&#36890;&#36807;&#26377;&#38480;&#26448;&#26009;&#28857;&#23545;&#27969;&#20307;&#21160;&#21147;&#23398;&#36827;&#34892;&#31163;&#25955;&#21270;&#22788;&#29702;&#30340;&#25289;&#26684;&#26391;&#26085;&#26041;&#26696;&#65292;&#36890;&#36807;&#36319;&#36394;&#36825;&#20123;&#26448;&#26009;&#28857;&#26469;&#36861;&#36394;&#20854;&#28436;&#21464;&#30340;&#36895;&#24230;&#22330;&#12290;&#30001;&#20110;&#20223;&#30495;&#30340;&#31890;&#23376;&#29305;&#24615;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20855;&#26377;&#21560;&#24341;&#21147;&#21644;&#25104;&#21151;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;GNN&#30340;&#27169;&#25311;&#22120;&#30340;&#23454;&#38469;&#23454;&#29992;&#24615;&#20381;&#36182;&#20110;&#20854;&#23545;&#29289;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24314;&#27169;&#33021;&#21147;&#65292;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25552;&#20379;&#20934;&#30830;&#19988;&#31283;&#23450;&#30340;&#39044;&#27979;&#65292;&#36825;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38590;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24352;&#21147;&#19981;&#31283;&#23450;&#24615;&#23548;&#33268;&#30340;&#31890;&#23376;&#32858;&#31867;&#29616;&#35937;&#26159;&#20027;&#35201;&#38382;&#39064;&#20043;&#19968;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#29992;&#26631;&#20934;SPH&#27714;&#35299;&#22120;&#30340;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#65288;&#21253;&#25324;&#21387;&#21147;&#12289;&#31896;&#24615;&#21644;&#22806;&#21147;&#37096;&#20998;&#65289;&#22686;&#24378;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;GNN&#30340;&#27169;&#25311;&#22120;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#12290;&#25152;&#26377;&#32463;&#36807;&#31070;&#32463;SPH&#22686;&#24378;&#30340;&#27169;&#25311;&#22120;&#37117;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smoothed particle hydrodynamics (SPH) is omnipresent in modern engineering and scientific disciplines. SPH is a class of Lagrangian schemes that discretize fluid dynamics via finite material points that are tracked through the evolving velocity field. Due to the particle-like nature of the simulation, graph neural networks (GNNs) have emerged as appealing and successful surrogates. However, the practical utility of such GNN-based simulators relies on their ability to faithfully model physics, providing accurate and stable predictions over long time horizons - which is a notoriously hard problem. In this work, we identify particle clustering originating from tensile instabilities as one of the primary pitfalls. Based on these insights, we enhance both training and rollout inference of state-of-the-art GNN-based simulators with varying components from standard SPH solvers, including pressure, viscous, and external force components. All neural SPH-enhanced simulators achieve better perfor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20110;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#29305;&#24449;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#22810;&#26679;&#21270;&#29992;&#25143;&#20559;&#22909;&#19979;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05133</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Personalized Language Modeling from Personalized Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20110;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#29305;&#24449;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#22810;&#26679;&#21270;&#29992;&#25143;&#20559;&#22909;&#19979;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#30446;&#21069;&#20027;&#27969;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#24320;&#21457;&#30340;&#31639;&#27861;&#30340;&#22522;&#26412;&#21069;&#25552;&#22312;&#29992;&#25143;&#20559;&#22909;&#22810;&#26679;&#21270;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#20986;&#29616;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#27491;&#24335;&#20171;&#32461;&#20102;&#20174;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#20219;&#21153;&#65292;&#24182;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26222;&#36890;&#30340;RLHF&#21487;&#33021;&#20250;&#23384;&#22312;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20010;&#24615;&#21270;-RLHF&#65288;P-RLHF&#65289;&#26694;&#26550;&#65292;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#27169;&#22411;&#21644;&#35821;&#35328;&#65288;&#25110;&#22870;&#21169;&#65289;&#27169;&#22411;&#12290;&#29992;&#25143;&#27169;&#22411;&#25509;&#25910;&#29992;&#25143;&#20449;&#24687;&#24182;&#36755;&#20986;&#29992;&#25143;&#34920;&#31034;&#12290;&#20854;&#32467;&#26500;&#32534;&#30721;&#20102;&#25105;&#20204;&#23545;&#21453;&#39304;&#25968;&#25454;&#20013;&#29992;&#25143;&#20559;&#22909;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#20026;&#20010;&#24615;&#21270;&#22870;&#21169;&#24314;&#27169;&#21644;&#20010;&#24615;&#21270;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#24320;&#21457;&#20102;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is the current dominating framework to fine-tune large language models to better align with human preferences. However, the underlying premise of algorithms developed under this framework can be problematic when user preferences encoded in human feedback are diverse. In this work, we aim to address this problem by developing methods for building personalized language models. We first formally introduce the task of learning from personalized human feedback and explain why vanilla RLHF can be problematic in this context. We then propose a general Personalized-RLHF (P-RLHF) framework, which requires one to jointly learn a user model and a language (or reward) model. The user model takes in user information and outputs user representations. Its structure encodes our assumptions about user preferences underlying the feedback data. We develop new learning objectives for personalized reward modeling and personalized Direct Preference Optimizat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38598;&#20013;&#20110;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#23436;&#22791;&#30340;&#35774;&#35745;GeoNGNN&#65292;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04836</link><description>&lt;p&gt;
&#20851;&#20110;&#19981;&#21464;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23436;&#22791;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Completeness of Invariant Geometric Deep Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04836
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38598;&#20013;&#20110;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#23436;&#22791;&#30340;&#35774;&#35745;GeoNGNN&#65292;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21464;&#27169;&#22411;&#26159;&#19968;&#31867;&#37325;&#35201;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#20960;&#20309;&#29305;&#24449;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#20960;&#20309;&#34920;&#31034;&#12290;&#36825;&#20123;&#27169;&#22411;&#20197;&#20854;&#31616;&#21333;&#24615;&#12289;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#20173;&#28982;&#19981;&#28165;&#26970;&#65292;&#38480;&#21046;&#20102;&#23545;&#36825;&#31181;&#27169;&#22411;&#28508;&#21147;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38598;&#20013;&#35752;&#35770;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#20005;&#26684;&#38480;&#21046;&#20102;&#26368;&#32463;&#20856;&#30340;&#19981;&#21464;&#27169;&#22411;Vanilla DisGNN&#65288;&#32467;&#21512;&#36317;&#31163;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23558;&#20854;&#19981;&#21487;&#35782;&#21035;&#30340;&#24773;&#20917;&#20165;&#38480;&#20110;&#39640;&#24230;&#23545;&#31216;&#30340;&#20960;&#20309;&#22270;&#24418;&#12290;&#20026;&#20102;&#25171;&#30772;&#36825;&#20123;&#29305;&#27530;&#24773;&#20917;&#30340;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#23436;&#22791;&#30340;&#19981;&#21464;&#35774;&#35745;&#65292;&#21363;&#23884;&#22871;Vanilla DisGNN&#30340;GeoNGNN&#12290;&#21033;&#29992;GeoNGNN&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invariant models, one important class of geometric deep learning models, are capable of generating meaningful geometric representations by leveraging informative geometric features. These models are characterized by their simplicity, good experimental results and computational efficiency. However, their theoretical expressive power still remains unclear, restricting a deeper understanding of the potential of such models. In this work, we concentrate on characterizing the theoretical expressiveness of invariant models. We first rigorously bound the expressiveness of the most classical invariant model, Vanilla DisGNN (message passing neural networks incorporating distance), restricting its unidentifiable cases to be only those highly symmetric geometric graphs. To break these corner cases' symmetry, we introduce a simple yet E(3)-complete invariant design by nesting Vanilla DisGNN, named GeoNGNN. Leveraging GeoNGNN as a theoretical tool, we for the first time prove the E(3)-completeness 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PAC-Bayesian&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#26469;&#30740;&#31350;&#23545;&#25239;&#40065;&#26834;&#24615;&#27867;&#21270;&#30028;&#38480;&#38382;&#39064;&#65292;&#38024;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#22270;&#19978;&#25193;&#25955;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#12289;&#26435;&#37325;&#30340;&#35889;&#33539;&#25968;&#21644;&#25200;&#21160;&#22240;&#23376;&#23545;&#27169;&#22411;&#30340;&#40065;&#26834;&#27867;&#21270;&#30028;&#38480;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.04038</link><description>&lt;p&gt;
PAC-Bayesian Adversarially Robust Generalization Bounds for Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
PAC-Bayesian Adversarially Robust Generalization Bounds for Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PAC-Bayesian&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#26469;&#30740;&#31350;&#23545;&#25239;&#40065;&#26834;&#24615;&#27867;&#21270;&#30028;&#38480;&#38382;&#39064;&#65292;&#38024;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#22270;&#19978;&#25193;&#25955;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#12289;&#26435;&#37325;&#30340;&#35889;&#33539;&#25968;&#21644;&#25200;&#21160;&#22240;&#23376;&#23545;&#27169;&#22411;&#30340;&#40065;&#26834;&#27867;&#21270;&#30028;&#38480;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#19982;&#22270;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#24191;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;GNNs&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#25239;&#40065;&#26834;&#24615;&#27867;&#21270;&#22312;&#24314;&#31435;&#26377;&#25928;&#30340;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#30340;&#38450;&#24481;&#31639;&#27861;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;PAC-Bayesian&#26694;&#26550;&#65292;&#20026;&#20004;&#31181;&#27969;&#34892;&#30340;GNNs&#65292;&#21363;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#21644;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#23545;&#25239;&#40065;&#26834;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22270;&#19978;&#25193;&#25955;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#12289;&#26435;&#37325;&#30340;&#35889;&#33539;&#25968;&#20197;&#21450;&#25200;&#21160;&#22240;&#23376;&#23545;&#20004;&#20010;&#27169;&#22411;&#30340;&#40065;&#26834;&#27867;&#21270;&#30028;&#38480;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#26159;&#65288;Liao&#31561;&#20154;&#65292;2020&#65289;&#20013;&#32467;&#26524;&#30340;&#38750;&#24179;&#20961;&#25512;&#24191;&#65292;&#20174;&#26631;&#20934;&#35774;&#32622;&#25193;&#23637;&#21040;&#23545;&#25239;&#35774;&#32622;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#26368;&#22823;&#33410;&#28857;&#24230;&#30340;&#25351;&#25968;&#20381;&#36182;&#12290;&#20316;&#20026;&#25512;&#35770;&#65292;&#25105;&#20204;&#24471;&#20986;&#26356;&#22909;&#30340;&#30028;&#38480;...
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have gained popularity for various graph-related tasks. However, similar to deep neural networks, GNNs are also vulnerable to adversarial attacks. Empirical studies have shown that adversarially robust generalization has a pivotal role in establishing effective defense algorithms against adversarial attacks. In this paper, we contribute by providing adversarially robust generalization bounds for two kinds of popular GNNs, graph convolutional network (GCN) and message passing graph neural network, using the PAC-Bayesian framework. Our result reveals that spectral norm of the diffusion matrix on the graph and spectral norm of the weights as well as the perturbation factor govern the robust generalization bounds of both models. Our bounds are nontrivial generalizations of the results developed in (Liao et al., 2020) from the standard setting to adversarial setting while avoiding exponential dependence of the maximum node degree. As corollaries, we derive bette
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;GNN&#30340;&#27450;&#35784;&#26816;&#27979;&#22120;SEC-GFD&#65292;&#36890;&#36807;&#28151;&#21512;&#36807;&#28388;&#27169;&#22359;&#21644;&#23616;&#37096;&#29615;&#22659;&#32422;&#26463;&#27169;&#22359;&#35299;&#20915;&#20102;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21033;&#29992;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.06441</link><description>&lt;p&gt;
&#22312;&#24322;&#36136;&#24615;&#21644;&#35889;&#38382;&#39064;&#19979;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#22270;&#30340;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Revisiting Graph-Based Fraud Detection in Sight of Heterophily and Spectrum
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;GNN&#30340;&#27450;&#35784;&#26816;&#27979;&#22120;SEC-GFD&#65292;&#36890;&#36807;&#28151;&#21512;&#36807;&#28388;&#27169;&#22359;&#21644;&#23616;&#37096;&#29615;&#22659;&#32422;&#26463;&#27169;&#22359;&#35299;&#20915;&#20102;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21033;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#27450;&#35784;&#26816;&#27979;&#65288;GFD&#65289;&#21487;&#35270;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21322;&#30417;&#30563;&#33410;&#28857;&#20108;&#20998;&#31867;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;GFD&#65292;&#36890;&#36807;&#32858;&#21512;&#37051;&#23621;&#20449;&#24687;&#26469;&#21051;&#30011;&#33410;&#28857;&#30340;&#24322;&#24120;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#27450;&#35784;&#22270;&#22312;&#26412;&#36136;&#19978;&#26159;&#24322;&#36136;&#30340;&#65292;&#22240;&#27492;&#22823;&#22810;&#25968;GNN&#30001;&#20110;&#20551;&#35774;&#21516;&#36136;&#24615;&#32780;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#24322;&#36136;&#24615;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#29616;&#26377;&#27169;&#22411;&#26410;&#20805;&#20998;&#21033;&#29992;&#23453;&#36149;&#30340;&#33410;&#28857;&#26631;&#31614;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;GNN&#30340;&#27450;&#35784;&#26816;&#27979;&#22120;SEC-GFD&#12290;&#35813;&#26816;&#27979;&#22120;&#21253;&#25324;&#28151;&#21512;&#36807;&#28388;&#27169;&#22359;&#21644;&#23616;&#37096;&#29615;&#22659;&#32422;&#26463;&#27169;&#22359;&#65292;&#36825;&#20004;&#20010;&#27169;&#22359;&#20998;&#21035;&#29992;&#20110;&#35299;&#20915;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21033;&#29992;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#27169;&#22359;&#20174;&#35889;&#22495;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#23558;&#22270;&#20998;&#21106;&#31216;&#19981;&#21516;&#30340;&#35889;&#25104;&#20998;&#65292;
&lt;/p&gt;
&lt;p&gt;
Graph-based fraud detection (GFD) can be regarded as a challenging semi-supervised node binary classification task. In recent years, Graph Neural Networks (GNN) have been widely applied to GFD, characterizing the anomalous possibility of a node by aggregating neighbor information. However, fraud graphs are inherently heterophilic, thus most of GNNs perform poorly due to their assumption of homophily. In addition, due to the existence of heterophily and class imbalance problem, the existing models do not fully utilize the precious node label information. To address the above issues, this paper proposes a semi-supervised GNN-based fraud detector SEC-GFD. This detector includes a hybrid filtering module and a local environmental constraint module, the two modules are utilized to solve heterophily and label utilization problem respectively. The first module starts from the perspective of the spectral domain, and solves the heterophily problem to a certain extent. Specifically, it divides t
&lt;/p&gt;</description></item><item><title>&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#26082;&#33021;&#25552;&#21319;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21644;&#26657;&#20934;&#24615;&#65292;&#21448;&#21487;&#33021;&#25104;&#20026;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#30340;&#22240;&#32032;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#32467;&#21512;&#36127;&#22240;&#23376;&#36827;&#34892;&#24179;&#28369;&#21487;&#26377;&#25928;&#38459;&#27490;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#65292;&#25552;&#21319;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2310.06549</link><description>&lt;p&gt;
&#35880;&#24910;&#24179;&#28369;&#26631;&#31614;&#65306;&#26631;&#31614;&#24179;&#28369;&#26082;&#21487;&#20197;&#20316;&#20026;&#38544;&#31169;&#23631;&#38556;&#65292;&#21448;&#21487;&#20197;&#25104;&#20026;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#30340;&#20652;&#21270;&#21058;
&lt;/p&gt;
&lt;p&gt;
Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06549
&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#26082;&#33021;&#25552;&#21319;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21644;&#26657;&#20934;&#24615;&#65292;&#21448;&#21487;&#33021;&#25104;&#20026;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#30340;&#22240;&#32032;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#32467;&#21512;&#36127;&#22240;&#23376;&#36827;&#34892;&#24179;&#28369;&#21487;&#26377;&#25928;&#38459;&#27490;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#65292;&#25552;&#21319;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#8212;&#8212;&#20351;&#29992;&#36719;&#21270;&#30340;&#26631;&#31614;&#32780;&#19981;&#26159;&#30828;&#26631;&#31614;&#8212;&#8212;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#22686;&#24378;&#27867;&#21270;&#21644;&#26657;&#20934;&#31561;&#22810;&#26679;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#23427;&#23545;&#20110;&#20445;&#25252;&#27169;&#22411;&#38544;&#31169;&#30340;&#24433;&#21709;&#20173;&#28982;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26631;&#31614;&#24179;&#28369;&#23545;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#65288;MIAs&#65289;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20998;&#31867;&#22120;&#20013;&#32534;&#30721;&#30340;&#30693;&#35782;&#29983;&#25104;&#20855;&#26377;&#31867;&#20195;&#34920;&#24615;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25512;&#26029;&#26377;&#20851;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#26631;&#31614;&#24179;&#28369;&#20419;&#36827;&#20102;MIAs&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#26356;&#29978;&#32773;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29992;&#36127;&#22240;&#23376;&#36827;&#34892;&#24179;&#28369;&#21487;&#20197;&#25269;&#21046;&#36825;&#19968;&#36235;&#21183;&#65292;&#38459;&#30861;&#25552;&#21462;&#19982;&#31867;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#65292;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#36825;&#30830;&#31435;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#24378;&#22823;&#30340;&#26032;&#30340;&#22686;&#24378;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06549v2 Announce Type: replace  Abstract: Label smoothing -- using softened labels instead of hard ones -- is a widely adopted regularization method for deep learning, showing diverse benefits such as enhanced generalization and calibration. Its implications for preserving model privacy, however, have remained unexplored. To fill this gap, we investigate the impact of label smoothing on model inversion attacks (MIAs), which aim to generate class-representative samples by exploiting the knowledge encoded in a classifier, thereby inferring sensitive information about its training data. Through extensive analyses, we uncover that traditional label smoothing fosters MIAs, thereby increasing a model's privacy leakage. Even more, we reveal that smoothing with negative factors counters this trend, impeding the extraction of class-related information and leading to privacy preservation, beating state-of-the-art defenses. This establishes a practical and powerful novel way for enhanc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#30340;&#21152;&#36895;&#31639;&#27861;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#31639;&#27861;&#24182;&#23454;&#29616;&#20102;&#36739;&#20248;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>https://arxiv.org/abs/2206.05248</link><description>&lt;p&gt;
&#21152;&#36895;&#31639;&#27861;&#29992;&#20110;&#32422;&#26463;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;
Accelerated Algorithms for Constrained Nonconvex-Nonconcave Min-Max Optimization and Comonotone Inclusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.05248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#30340;&#21152;&#36895;&#31639;&#27861;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#31639;&#27861;&#24182;&#23454;&#29616;&#20102;&#36739;&#20248;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#65292;&#19968;&#31867;&#32467;&#26500;&#21270;&#30340;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#20197;&#21450;&#23427;&#20204;&#23545;&#20849;&#21333;&#35843;&#21253;&#21547;&#30340;&#25512;&#24191;&#12290;&#22312;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#20013;&#65292;&#25105;&#20204;&#23558;&#26368;&#21021;&#30001;Yoon&#21644;Ryu&#65288;2021&#65289;&#25552;&#20986;&#30340;&#26080;&#32422;&#26463;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#30340;Extra Anchored Gradient&#65288;EAG&#65289;&#31639;&#27861;&#25193;&#23637;&#21040;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#25152;&#26377;&#19968;&#38454;&#26041;&#27861;&#20013;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;$O\left(\frac{1}{T}\right)$&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#36845;&#20195;&#25910;&#25947;&#21040;&#35299;&#38598;&#20013;&#30340;&#19968;&#20010;&#28857;&#12290;&#22312;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#20013;&#65292;&#25105;&#20204;&#23558;&#30001;Lee&#21644;Kim&#65288;2021&#65289;&#24320;&#21457;&#30340;&#24555;&#36895;&#39069;&#22806;&#26799;&#24230;&#65288;FEG&#65289;&#31639;&#27861;&#25193;&#23637;&#21040;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;&#65292;&#24182;&#23454;&#29616;&#20102;&#30456;&#21516;&#30340;$O\left(\frac{1}{T}\right)$&#25910;&#25947;&#36895;&#29575;&#12290;&#36825;&#20010;&#36895;&#29575;&#36866;&#29992;&#20110;&#25991;&#29486;&#20013;&#30740;&#31350;&#36807;&#30340;&#26368;&#24191;&#27867;&#30340;&#20849;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#38598;&#21512;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;s&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study constrained comonotone min-max optimization, a structured class of nonconvex-nonconcave min-max optimization problems, and their generalization to comonotone inclusion. In our first contribution, we extend the Extra Anchored Gradient (EAG) algorithm, originally proposed by Yoon and Ryu (2021) for unconstrained min-max optimization, to constrained comonotone min-max optimization and comonotone inclusion, achieving an optimal convergence rate of $O\left(\frac{1}{T}\right)$ among all first-order methods. Additionally, we prove that the algorithm's iterations converge to a point in the solution set. In our second contribution, we extend the Fast Extra Gradient (FEG) algorithm, as developed by Lee and Kim (2021), to constrained comonotone min-max optimization and comonotone inclusion, achieving the same $O\left(\frac{1}{T}\right)$ convergence rate. This rate is applicable to the broadest set of comonotone inclusion problems yet studied in the literature. Our analyses are based on s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#25351;&#20196;&#26469;&#25351;&#23548;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#24674;&#22797;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20026;&#22522;&#20110;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.16468</link><description>&lt;p&gt;
&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
High-Quality Image Restoration Following Human Instructions. (arXiv:2401.16468v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#25351;&#20196;&#26469;&#25351;&#23548;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#24674;&#22797;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20026;&#22522;&#20110;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24674;&#22797;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#28041;&#21450;&#20174;&#36864;&#21270;&#35266;&#27979;&#20013;&#24674;&#22797;&#20986;&#39640;&#36136;&#37327;&#30340;&#24178;&#20928;&#22270;&#20687;&#12290;&#20840;&#33021;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36864;&#21270;&#31867;&#22411;&#30340;&#20449;&#24687;&#20316;&#20026;&#25552;&#31034;&#26469;&#26377;&#25928;&#22320;&#24674;&#22797;&#21508;&#31181;&#31867;&#22411;&#21644;&#32423;&#21035;&#30340;&#36864;&#21270;&#22270;&#20687;&#65292;&#24182;&#24341;&#23548;&#24674;&#22797;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#25351;&#20196;&#26469;&#25351;&#23548;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20174;&#36864;&#21270;&#22270;&#20687;&#20013;&#24674;&#22797;&#20986;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#32771;&#34385;&#22810;&#31181;&#36864;&#21270;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;InstructIR&#22312;&#22270;&#20687;&#21435;&#22122;&#12289;&#38632;&#27700;&#21435;&#38500;&#12289;&#21435;&#27169;&#31946;&#12289;&#21435;&#38654;&#21644;(&#20302;&#20809;)&#22270;&#20687;&#22686;&#24378;&#31561;&#22810;&#20010;&#24674;&#22797;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;InstructIR&#22312;&#20043;&#21069;&#30340;&#20840;&#33021;&#24674;&#22797;&#26041;&#27861;&#19978;&#25552;&#39640;&#20102;1dB&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#32467;&#26524;&#20026;&#22522;&#20110;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30340;&#26032;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20195;&#30721;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image restoration is a fundamental problem that involves recovering a high-quality clean image from its degraded observation. All-In-One image restoration models can effectively restore images from various types and levels of degradation using degradation-specific information as prompts to guide the restoration model. In this work, we present the first approach that uses human-written instructions to guide the image restoration model. Given natural language prompts, our model can recover high-quality images from their degraded counterparts, considering multiple degradation types. Our method, InstructIR, achieves state-of-the-art results on several restoration tasks including image denoising, deraining, deblurring, dehazing, and (low-light) image enhancement. InstructIR improves +1dB over previous all-in-one restoration methods. Moreover, our dataset and results represent a novel benchmark for new research on text-guided image restoration and enhancement. Our code, datasets and models a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#22810;&#32454;&#32990;&#38598;&#21512;&#20307;&#30340;&#36816;&#21160;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#22810;&#32454;&#32990;&#29983;&#29289;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#22270;&#29305;&#24449;&#65292;&#24182;&#36229;&#36234;&#20256;&#32479;&#26426;&#26800;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#32773;&#24314;&#35758;&#36890;&#36807;&#21512;&#20316;&#21162;&#21147;&#26469;&#26500;&#24314;&#19968;&#20010;&#22810;&#32454;&#32990;&#25968;&#25454;&#24211;&#65292;&#20197;&#36827;&#19968;&#27493;&#25512;&#21160;&#22810;&#32454;&#32990;&#21160;&#21147;&#23398;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.12196</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#22810;&#32454;&#32990;&#22270;&#20013;&#23398;&#20064;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Learning Dynamics from Multicellular Graphs with Deep Neural Networks. (arXiv:2401.12196v1 [physics.bio-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#22810;&#32454;&#32990;&#38598;&#21512;&#20307;&#30340;&#36816;&#21160;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#22810;&#32454;&#32990;&#29983;&#29289;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#22270;&#29305;&#24449;&#65292;&#24182;&#36229;&#36234;&#20256;&#32479;&#26426;&#26800;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#32773;&#24314;&#35758;&#36890;&#36807;&#21512;&#20316;&#21162;&#21147;&#26469;&#26500;&#24314;&#19968;&#20010;&#22810;&#32454;&#32990;&#25968;&#25454;&#24211;&#65292;&#20197;&#36827;&#19968;&#27493;&#25512;&#21160;&#22810;&#32454;&#32990;&#21160;&#21147;&#23398;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#32454;&#32990;&#33258;&#32452;&#35013;&#30340;&#25512;&#26029;&#26159;&#29702;&#35299;&#24418;&#24577;&#21457;&#29983;&#30340;&#26680;&#24515;&#38382;&#39064;&#65292;&#21253;&#25324;&#32986;&#32974;&#12289;&#22120;&#23448;&#32467;&#26500;&#12289;&#32959;&#30244;&#31561;&#12290;&#28982;&#32780;&#65292;&#24456;&#38590;&#25214;&#21040;&#33021;&#22815;&#25351;&#31034;&#22810;&#32454;&#32990;&#21160;&#21147;&#23398;&#30340;&#32467;&#26500;&#29305;&#24449;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#39044;&#27979;&#33021;&#21147;&#26469;&#21457;&#29616;&#21487;&#20197;&#39044;&#27979;&#21160;&#21147;&#23398;&#30340;&#37325;&#35201;&#22270;&#29305;&#24449;&#12290;&#20026;&#20102;&#35777;&#26126;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#20010;&#29289;&#29702;&#23398;&#21551;&#21457;&#30340; GNN&#65288;piGNN&#65289;&#26469;&#39044;&#27979;&#22810;&#32454;&#32990;&#38598;&#21512;&#20307;&#30340;&#36816;&#21160;&#33021;&#21147;&#65292;&#20174;&#23427;&#20204;&#22312;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;&#30340;&#20301;&#32622;&#24555;&#29031;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102; piGNN &#33021;&#22815;&#22312;&#22810;&#32454;&#32990;&#29983;&#29289;&#31995;&#32479;&#30340;&#22797;&#26434;&#22270;&#29305;&#24449;&#20013;&#23548;&#33322;&#65292;&#36825;&#26159;&#32463;&#20856;&#26426;&#26800;&#27169;&#22411;&#26080;&#27861;&#23454;&#29616;&#30340;&#12290;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#22810;&#32454;&#32990;&#25968;&#25454;&#30340;&#31215;&#32047;&#65292;&#25105;&#20204;&#25552;&#20986;&#21487;&#20197;&#36827;&#34892;&#21512;&#20316;&#21162;&#21147;&#65292;&#21019;&#24314;&#19968;&#20010;&#22810;&#32454;&#32990;&#25968;&#25454;&#24211;&#65288;MDB&#65289;&#65292;&#20174;&#20013;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#30340;&#22810;&#32454;&#32990;&#22270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inference of multicellular self-assembly is the central quest of understanding morphogenesis, including embryos, organoids, tumors, and many others. However, it has been tremendously difficult to identify structural features that can indicate multicellular dynamics. Here we propose to harness the predictive power of graph-based deep neural networks (GNN) to discover important graph features that can predict dynamics. To demonstrate, we apply a physically informed GNN (piGNN) to predict the motility of multicellular collectives from a snapshot of their positions both in experiments and simulations. We demonstrate that piGNN is capable of navigating through complex graph features of multicellular living systems, which otherwise can not be achieved by classical mechanistic models. With increasing amounts of multicellular data, we propose that collaborative efforts can be made to create a multicellular data bank (MDB) from which it is possible to construct a large multicellular graph m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Variance-Reduced Sketching&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#24230;&#20013;&#20272;&#35745;&#23494;&#24230;&#20989;&#25968;&#21644;&#38750;&#21442;&#25968;&#22238;&#24402;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20989;&#25968;&#27010;&#24565;&#21270;&#20026;&#30697;&#38453;&#65292;&#24182;&#37319;&#29992;&#33609;&#22270;&#25216;&#26415;&#26469;&#38477;&#20302;&#32500;&#24230;&#28798;&#38590;&#24341;&#36215;&#30340;&#26041;&#24046;&#65292;&#23637;&#31034;&#20102;&#40065;&#26834;&#24615;&#33021;&#21644;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.11646</link><description>&lt;p&gt;
&#36890;&#36807;&#26041;&#24046;&#38477;&#20302;&#30340;&#33609;&#22270;&#36827;&#34892;&#38750;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Estimation via Variance-Reduced Sketching. (arXiv:2401.11646v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Variance-Reduced Sketching&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#24230;&#20013;&#20272;&#35745;&#23494;&#24230;&#20989;&#25968;&#21644;&#38750;&#21442;&#25968;&#22238;&#24402;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20989;&#25968;&#27010;&#24565;&#21270;&#20026;&#30697;&#38453;&#65292;&#24182;&#37319;&#29992;&#33609;&#22270;&#25216;&#26415;&#26469;&#38477;&#20302;&#32500;&#24230;&#28798;&#38590;&#24341;&#36215;&#30340;&#26041;&#24046;&#65292;&#23637;&#31034;&#20102;&#40065;&#26834;&#24615;&#33021;&#21644;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21442;&#25968;&#27169;&#22411;&#22312;&#21508;&#20010;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;&#32463;&#20856;&#30340;&#26680;&#26041;&#27861;&#22312;&#20302;&#32500;&#24773;&#20917;&#19979;&#20855;&#26377;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#32479;&#35745;&#21487;&#38752;&#24615;&#65292;&#20294;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30001;&#20110;&#32500;&#24230;&#28798;&#38590;&#21464;&#24471;&#19981;&#22815;&#36866;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Variance-Reduced Sketching&#65288;VRS&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#19987;&#38376;&#29992;&#20110;&#22312;&#38477;&#20302;&#32500;&#24230;&#28798;&#38590;&#30340;&#21516;&#26102;&#22312;&#39640;&#32500;&#24230;&#20013;&#20272;&#35745;&#23494;&#24230;&#20989;&#25968;&#21644;&#38750;&#21442;&#25968;&#22238;&#24402;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#22810;&#21464;&#37327;&#20989;&#25968;&#27010;&#24565;&#21270;&#20026;&#26080;&#38480;&#22823;&#23567;&#30340;&#30697;&#38453;&#65292;&#24182;&#20511;&#37492;&#20102;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#25991;&#29486;&#20013;&#30340;&#19968;&#31181;&#26032;&#30340;&#33609;&#22270;&#25216;&#26415;&#26469;&#38477;&#20302;&#20272;&#35745;&#38382;&#39064;&#20013;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#27169;&#25311;&#23454;&#39564;&#21644;&#30495;&#23454;&#25968;&#25454;&#24212;&#29992;&#23637;&#31034;&#20102;VRS&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#35768;&#22810;&#23494;&#24230;&#20272;&#35745;&#38382;&#39064;&#20013;&#65292;VRS&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#21644;&#32463;&#20856;&#30340;&#26680;&#26041;&#27861;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonparametric models are of great interest in various scientific and engineering disciplines. Classical kernel methods, while numerically robust and statistically sound in low-dimensional settings, become inadequate in higher-dimensional settings due to the curse of dimensionality. In this paper, we introduce a new framework called Variance-Reduced Sketching (VRS), specifically designed to estimate density functions and nonparametric regression functions in higher dimensions with a reduced curse of dimensionality. Our framework conceptualizes multivariable functions as infinite-size matrices, and facilitates a new sketching technique motivated by numerical linear algebra literature to reduce the variance in estimation problems. We demonstrate the robust numerical performance of VRS through a series of simulated experiments and real-world data applications. Notably, VRS shows remarkable improvement over existing neural network estimators and classical kernel methods in numerous density 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#65292;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.10134</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Large Language Model for Traffic Prediction. (arXiv:2401.10134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#65292;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#29305;&#23450;&#20301;&#32622;&#30340;&#26410;&#26469;&#20132;&#36890;&#24773;&#20917;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#24378;&#35843;&#24320;&#21457;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20294;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#24182;&#26410;&#30456;&#24212;&#25552;&#39640;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#19982;&#29616;&#26377;&#27169;&#22411;&#19981;&#21516;&#65292;LLMs&#20027;&#35201;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#26469;&#36827;&#27493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#22522;&#26412;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ST-LLM&#23558;&#27599;&#20010;&#20301;&#32622;&#30340;&#26102;&#38388;&#27493;&#38271;&#23450;&#20041;&#20026;&#26631;&#35760;&#65292;&#24182;&#32467;&#21512;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#26469;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#34920;&#31034;&#34987;&#34701;&#21512;&#20197;&#20026;&#27599;&#20010;&#26631;&#35760;&#25552;&#20379;&#32479;&#19968;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction, a critical component for intelligent transportation systems, endeavors to foresee future traffic at specific locations using historical data. Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not seen improvements accordingly. Recently, Large Language Models (LLMs) have shown outstanding capabilities in time series analysis. Differing from existing models, LLMs progress mainly through parameter expansion and extensive pre-training while maintaining their fundamental structures. In this paper, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction. Specifically, ST-LLM redefines the timesteps at each location as tokens and incorporates a spatial-temporal embedding module to learn the spatial location and global temporal representations of tokens. Then these representations are fused to provide each token with unified spatial and temporal information. Furthermore, we
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06692</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models. (arXiv:2401.06692v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06692
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25351;&#23548;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#22312;&#23454;&#29616;&#20102;&#20196;&#20154;&#24778;&#21497;&#30340;&#38646;&#23556;&#20987;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20026;&#25351;&#20196;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#22238;&#31572;&#25152;&#38656;&#30340;&#27880;&#37322;&#24037;&#20316;&#27491;&#22312;&#21464;&#24471;&#38590;&#20197;&#25215;&#21463;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#25351;&#20196;&#25968;&#25454;&#38598;&#25152;&#28085;&#30422;&#30340;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#12290;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#27744;&#20013;&#30830;&#23450;&#26377;&#29992;&#30340;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#20294;&#20854;&#39640;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#20854;&#22312;LLMs&#29615;&#22659;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#20943;&#23569;SFT&#30340;&#27880;&#37322;&#25104;&#26412;&#24182;&#35268;&#36991;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23454;&#39564;&#35774;&#35745;&#12290;&#23454;&#39564;&#35774;&#35745;&#25216;&#26415;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#65292;&#36890;&#24120;&#26368;&#22823;&#21270;&#26576;&#31181;&#19981;&#30830;&#23450;&#24615;&#21644;/&#25110;&#22810;&#26679;&#24615;&#30340;&#27010;&#24565;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#20010;&#35780;&#20272;&#22810;&#31181;&#29616;&#26377;&#21644;&#26032;&#39062;&#30340;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised finetuning (SFT) on instruction datasets has played a crucial role in achieving the remarkable zero-shot generalization capabilities observed in modern large language models (LLMs). However, the annotation efforts required to produce high quality responses for instructions are becoming prohibitively expensive, especially as the number of tasks spanned by instruction datasets continues to increase. Active learning is effective in identifying useful subsets of samples to annotate from an unlabeled pool, but its high computational cost remains a barrier to its widespread applicability in the context of LLMs. To mitigate the annotation cost of SFT and circumvent the computational bottlenecks of active learning, we propose using experimental design. Experimental design techniques select the most informative samples to label, and typically maximize some notion of uncertainty and/or diversity. In our work, we implement a framework that evaluates several existing and novel experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#30340;&#25200;&#21160;&#26469;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#25511;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#37319;&#29992;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#31561;&#26032;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04385</link><description>&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#25200;&#21160;&#23454;&#29616;&#26426;&#22120;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning through fine-grained model parameters perturbation. (arXiv:2401.04385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#30340;&#25200;&#21160;&#26469;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#25511;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#37319;&#29992;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#31561;&#26032;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#28041;&#21450;&#21040;&#25764;&#38144;&#25968;&#25454;&#35760;&#24405;&#21644;&#20943;&#23567;&#35813;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24110;&#21161;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#30446;&#26631;&#65292;&#20294;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20110;&#21442;&#25968;&#25200;&#21160;&#30340;&#26435;&#37325;&#21435;&#23398;&#20064;&#26159;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20294;&#36890;&#24120;&#28041;&#21450;&#21040;&#20840;&#23616;&#20462;&#25913;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31934;&#32454;&#30340;Top-K&#21644;Random-k&#21442;&#25968;&#25200;&#21160;&#19981;&#31934;&#30830;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#28385;&#36275;&#38544;&#31169;&#38656;&#27714;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#21487;&#25511;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#35780;&#20272;&#26426;&#22120;&#21435;&#23398;&#20064;&#25928;&#26524;&#30340;&#25361;&#25112;&#65292;&#32771;&#34385;&#20102;&#27169;&#22411;&#22312;&#21435;&#23398;&#20064;&#21644;&#21097;&#20313;&#25968;&#25454;&#19978;&#30340;&#24191;&#20041;&#24615;&#33021;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25351;&#26631;&#65292;&#21363;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19981;&#31934;&#30830;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#65292;&#29616;&#26377;&#30340;&#25351;&#26631;&#26080;&#27861;&#23545;&#21435;&#23398;&#20064;&#31243;&#24230;&#36827;&#34892;&#20934;&#30830;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning techniques, which involve retracting data records and reducing influence of said data on trained models, help with the user privacy protection objective but incur significant computational costs. Weight perturbation-based unlearning is a general approach, but it typically involves globally modifying the parameters. We propose fine-grained Top-K and Random-k parameters perturbed inexact machine unlearning strategies that address the privacy needs while keeping the computational costs tractable.  In order to demonstrate the efficacy of our strategies we also tackle the challenge of evaluating the effectiveness of machine unlearning by considering the model's generalization performance across both unlearning and remaining data. To better assess the unlearning effect and model generalization, we propose novel metrics, namely, the forgetting rate and memory retention rate. However, for inexact machine unlearning, current metrics are inadequate in quantifying the degree of
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.04131</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#23450;&#20301;&#36328;&#20219;&#21153;&#24207;&#21015;&#32487;&#32493;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Locating Cross-Task Sequence Continuation Circuits in Transformers. (arXiv:2311.04131v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04131
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;Transformer&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#22797;&#26434;&#30340;&#26550;&#26500;&#20351;&#20854;&#38590;&#20197;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#23558;Transformer&#27169;&#22411;&#36824;&#21407;&#20026;&#21487;&#35835;&#30340;&#30005;&#36335;&#34920;&#31034;&#65292;&#29992;&#20110;&#23454;&#29616;&#31639;&#27861;&#21151;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#26469;&#25193;&#23637;&#36825;&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;&#21253;&#25324;&#25968;&#23383;&#12289;&#25968;&#23383;&#35789;&#21644;&#26376;&#20221;&#30340;&#36882;&#22686;&#24207;&#21015;&#12290;&#36890;&#36807;&#24212;&#29992;&#30005;&#36335;&#20998;&#26512;&#25216;&#26415;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36127;&#36131;&#26816;&#27979;&#24207;&#21015;&#25104;&#21592;&#21644;&#39044;&#27979;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#25104;&#21592;&#30340;&#20851;&#38190;&#23376;&#30005;&#36335;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35821;&#20041;&#30456;&#20851;&#24207;&#21015;&#20381;&#36182;&#20110;&#20855;&#26377;&#31867;&#20284;&#20316;&#29992;&#30340;&#20849;&#20139;&#30005;&#36335;&#23376;&#22270;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#35760;&#24405;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#34892;&#20026;&#65292;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#34892;&#26356;&#23433;&#20840;&#30340;&#32534;&#36753;&#36807;&#31243;&#12290;&#36825;&#31181;&#23545;Transformer&#30340;&#26426;&#26800;&#29702;&#35299;&#26159;&#26500;&#24314;&#26356;&#20581;&#22766;&#12289;&#35843;&#35797;&#21644;&#32534;&#36753;&#26356;&#23433;&#20840;&#30340;&#27169;&#22411;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21160;&#24577;&#22270;&#19978;&#30340;&#26102;&#31354;&#29702;&#35299;&#33021;&#21147;&#30340;LLM4DyG&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#20102;&#19981;&#21516;&#22240;&#32032;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.17110</link><description>&lt;p&gt;
LLM4DyG&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#35299;&#20915;&#21160;&#24577;&#22270;&#19978;&#30340;&#38382;&#39064;&#21527;?
&lt;/p&gt;
&lt;p&gt;
LLM4DyG: Can Large Language Models Solve Problems on Dynamic Graphs?. (arXiv:2310.17110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21160;&#24577;&#22270;&#19978;&#30340;&#26102;&#31354;&#29702;&#35299;&#33021;&#21147;&#30340;LLM4DyG&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#20102;&#19981;&#21516;&#22240;&#32032;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22788;&#29702;&#21508;&#31181;&#20219;&#21153;&#30340;&#26102;&#20195;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#25506;&#32034;LLMs&#22312;&#22788;&#29702;&#32593;&#32476;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22270;&#24418;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#21160;&#24577;&#22270;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#25968;&#25454;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#23427;&#20204;&#25429;&#25417;&#20102;&#32593;&#32476;&#28436;&#21270;&#27169;&#24335;&#12290;&#35780;&#20272;LLMs&#22312;&#29702;&#35299;&#21160;&#24577;&#22270;&#19978;&#30340;&#26102;&#31354;&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#23545;&#20110;&#23427;&#20204;&#22312;Web&#24212;&#29992;&#20013;&#30340;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#36825;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#39318;&#27425;&#25552;&#20986;&#22312;&#21160;&#24577;&#22270;&#19978;&#35780;&#20272;LLMs&#30340;&#26102;&#31354;&#29702;&#35299;&#33021;&#21147;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM4DyG&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#20061;&#20010;&#29305;&#21035;&#35774;&#35745;&#30340;&#20219;&#21153;&#65292;&#32771;&#34385;&#20102;LLMs&#22312;&#26102;&#24577;&#21644;&#31354;&#38388;&#32500;&#24230;&#19978;&#30340;&#33021;&#21147;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#29983;&#25104;&#22120;&#12289;&#25968;&#25454;&#32479;&#35745;&#12289;&#25552;&#31034;&#25216;&#26415;&#21644;LLMs&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an era marked by the increasing adoption of Large Language Models (LLMs) for various tasks, there is a growing focus on exploring LLMs' capabilities in handling web data, particularly graph data. Dynamic graphs, which capture temporal network evolution patterns, are ubiquitous in real-world web data. Evaluating LLMs' competence in understanding spatial-temporal information on dynamic graphs is essential for their adoption in web applications, which remains unexplored in the literature. In this paper, we bridge the gap via proposing to evaluate LLMs' spatial-temporal understanding abilities on dynamic graphs, to the best of our knowledge, for the first time. Specifically, we propose the LLM4DyG benchmark, which includes nine specially designed tasks considering the capability evaluation of LLMs from both temporal and spatial dimensions. Then, we conduct extensive experiments to analyze the impacts of different data generators, data statistics, prompting techniques, and LLMs on the mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#36870;&#21521;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;LLM&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#36890;&#36807;&#25913;&#36827;&#25216;&#26415;&#65292;&#22914;Rephrase&#21644;PAL-Tools&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01991</link><description>&lt;p&gt;
&#22635;&#31354;&#39064;&#65306;&#25506;&#32034;&#24182;&#22686;&#24378;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems. (arXiv:2310.01991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#36870;&#21521;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;LLM&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#36890;&#36807;&#25913;&#36827;&#25216;&#26415;&#65292;&#22914;Rephrase&#21644;PAL-Tools&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36817;&#26399;&#30340;&#25991;&#29486;&#20013;&#24191;&#27867;&#25506;&#35752;&#20102;&#27491;&#21521;&#25512;&#29702;&#65288;&#21363;&#32473;&#23450;&#38382;&#39064;&#25214;&#31572;&#26696;&#65289;&#65292;&#20294;&#36870;&#21521;&#25512;&#29702;&#30456;&#23545;&#36739;&#23569;&#34987;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#25506;&#35752;&#65306;&#32473;&#23450;&#19968;&#20010;&#25968;&#23398;&#38382;&#39064;&#21644;&#20854;&#31572;&#26696;&#65292;&#22312;&#38382;&#39064;&#20013;&#26377;&#20123;&#32454;&#33410;&#34987;&#30465;&#30053;&#20102;&#65292;LLM&#33021;&#21542;&#26377;&#25928;&#22320;&#36824;&#21407;&#20986;&#32570;&#22833;&#30340;&#20449;&#24687;&#65311;&#26412;&#25991;&#27491;&#24335;&#23450;&#20041;&#20102;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#20462;&#25913;&#20102;&#19977;&#20010;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#36825;&#19968;&#20219;&#21153;&#65306;GSM8k&#12289;SVAMP&#21644;MultiArith&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#27491;&#21521;&#25512;&#29702;&#30456;&#27604;&#65292;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#27169;&#22411;&#65288;GPT4&#12289;GPT3.5&#12289;PaLM-2&#21644;LLaMa-2&#65289;&#22312;&#36870;&#21521;&#25512;&#29702;&#19978;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#21033;&#29992;&#35813;&#20219;&#21153;&#30340;&#29305;&#23450;&#26684;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#25913;&#36827;&#24615;&#33021;&#30340;&#26032;&#25216;&#26415;&#65306;Rephrase&#23558;&#32473;&#23450;&#30340;&#38382;&#39064;&#37325;&#36848;&#20026;&#19968;&#20010;&#27491;&#21521;&#25512;&#29702;&#38382;&#39064;&#65292;PAL-Tools&#32467;&#21512;&#20102;&#31243;&#24207;&#36741;&#21161;&#30340;LLM&#24605;&#24819;&#65292;&#29983;&#25104;&#19968;&#32452;&#26041;&#31243;&#24335;&#21487;&#20197;&#35299;&#20915;&#32570;&#22833;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
While forward reasoning (i.e. find the answer given the question) has been explored extensively in the recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information?  In this paper, we formally define the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith. Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa-2). Utilizing the specific format of this task, we propose three novel techniques that improve performance: Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that ca
&lt;/p&gt;</description></item><item><title>FUTURE-AI&#26159;&#31532;&#19968;&#20010;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2309.12325</link><description>&lt;p&gt;
FUTURE-AI&#65306;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;&#21644;&#21487;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;&#30340;&#22269;&#38469;&#20849;&#35782;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare. (arXiv:2309.12325v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12325
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI&#26159;&#31532;&#19968;&#20010;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;AI&#25216;&#26415;&#22312;&#29616;&#23454;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#37096;&#32626;&#21644;&#37319;&#29992;&#20173;&#21463;&#38480;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#21307;&#30103;AI&#30340;&#25216;&#26415;&#12289;&#20020;&#24202;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#39118;&#38505;&#25552;&#20986;&#20102;&#20851;&#27880;&#12290;&#20026;&#20102;&#22686;&#21152;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37319;&#29992;&#65292;&#21307;&#30103;AI&#24037;&#20855;&#24517;&#39035;&#24471;&#21040;&#24739;&#32773;&#12289;&#20020;&#24202;&#21307;&#29983;&#12289;&#20581;&#24247;&#32452;&#32455;&#21644;&#24403;&#23616;&#30340;&#20449;&#20219;&#21644;&#25509;&#21463;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;FUTURE-AI&#25351;&#21335;&#20316;&#20026;&#31532;&#19968;&#20010;&#29992;&#20110;&#25351;&#23548;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#12290;FUTURE-AI&#32852;&#30431;&#25104;&#31435;&#20110;2021&#24180;&#65292;&#30446;&#21069;&#21253;&#25324;&#26469;&#33258;51&#20010;&#22269;&#23478;&#30340;118&#20301;&#36328;&#23398;&#31185;&#19987;&#23478;&#65292;&#20195;&#34920;&#20102;&#25152;&#26377;&#22823;&#27954;&#65292;&#21253;&#25324;AI&#31185;&#23398;&#23478;&#12289;&#20020;&#24202;&#21307;&#29983;&#12289;&#20262;&#29702;&#23398;&#23478;&#21644;&#31038;&#20250;&#31185;&#23398;&#23478;&#12290;&#22312;&#20026;&#26399;&#20004;&#24180;&#30340;&#26102;&#38388;&#37324;&#65292;&#32852;&#30431;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#23450;&#20041;&#20102;&#21487;&#20449;AI&#30340;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20854;&#20013;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
Despite major advances in artificial intelligence (AI) for medicine and healthcare, the deployment and adoption of AI technologies remain limited in real-world clinical practice. In recent years, concerns have been raised about the technical, clinical, ethical and legal risks associated with medical AI. To increase real world adoption, it is essential that medical AI tools are trusted and accepted by patients, clinicians, health organisations and authorities. This work describes the FUTURE-AI guideline as the first international consensus framework for guiding the development and deployment of trustworthy AI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and currently comprises 118 inter-disciplinary experts from 51 countries representing all continents, including AI scientists, clinicians, ethicists, and social scientists. Over a two-year period, the consortium defined guiding principles and best practices for trustworthy AI through an iterative process comprising a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#26465;&#20214;&#19981;&#21464;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#19981;&#21464;&#24809;&#32602;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#30446;&#26631;&#39118;&#38505;&#20445;&#35777;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.10301</link><description>&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#26465;&#20214;&#19981;&#21464;&#32452;&#20214;&#30340;&#31361;&#20986;&#20316;&#29992;&#65306;&#29702;&#35770;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prominent Roles of Conditionally Invariant Components in Domain Adaptation: Theory and Algorithms. (arXiv:2309.10301v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10301
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#26465;&#20214;&#19981;&#21464;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#19981;&#21464;&#24809;&#32602;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#30446;&#26631;&#39118;&#38505;&#20445;&#35777;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#26159;&#19968;&#20010;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#65292;&#24403;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#30340;&#28304;&#25968;&#25454;&#20998;&#24067;&#19982;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#30446;&#26631;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#26102;&#20986;&#29616;&#12290;&#34429;&#28982;&#35768;&#22810;&#39046;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#24050;&#32463;&#35777;&#26126;&#20102;&#30456;&#24403;&#22823;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#26159;&#30450;&#30446;&#24212;&#29992;&#36825;&#20123;&#31639;&#27861;&#24448;&#24448;&#20250;&#23548;&#33268;&#22312;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#35201;&#30340;&#26159;&#28548;&#28165;&#39046;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#22312;&#20855;&#22791;&#33391;&#22909;&#30446;&#26631;&#24615;&#33021;&#30340;&#20551;&#35774;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#39044;&#27979;&#20013;&#20855;&#22791;&#26465;&#20214;&#19981;&#21464;&#30340;&#32452;&#20214;&#65288;CICs&#65289;&#30340;&#23384;&#22312;&#20551;&#35774;&#65292;&#36825;&#20123;&#32452;&#20214;&#22312;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#20043;&#38388;&#20445;&#25345;&#26465;&#20214;&#19981;&#21464;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;CICs&#65292;&#36890;&#36807;&#26465;&#20214;&#19981;&#21464;&#24809;&#32602;&#65288;CIP&#65289;&#21487;&#20197;&#20272;&#35745;&#65292;&#20855;&#22791;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#25552;&#20379;&#30446;&#26631;&#39118;&#38505;&#20445;&#35777;&#30340;&#19977;&#20010;&#31361;&#20986;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CICs&#30340;&#26032;&#31639;&#27861;&#65292;&#21363;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#26465;&#20214;&#19981;&#21464;&#24809;&#32602;&#65288;IW-CIP&#65289;&#65292;&#23427;&#22312;&#30446;&#26631;&#39118;&#38505;&#20445;&#35777;&#26041;&#38754;&#36229;&#36234;&#20102;&#31616;&#21333;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation (DA) is a statistical learning problem that arises when the distribution of the source data used to train a model differs from that of the target data used to evaluate the model. While many DA algorithms have demonstrated considerable empirical success, blindly applying these algorithms can often lead to worse performance on new datasets. To address this, it is crucial to clarify the assumptions under which a DA algorithm has good target performance. In this work, we focus on the assumption of the presence of conditionally invariant components (CICs), which are relevant for prediction and remain conditionally invariant across the source and target data. We demonstrate that CICs, which can be estimated through conditional invariant penalty (CIP), play three prominent roles in providing target risk guarantees in DA. First, we propose a new algorithm based on CICs, importance-weighted conditional invariant penalty (IW-CIP), which has target risk guarantees beyond simple 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#65288;GDeNet&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28909;&#21644;&#27874;&#21160;&#26041;&#31243;&#21160;&#21147;&#23398;&#29305;&#24449;&#26469;&#24674;&#22797;&#22270;&#30340;&#25299;&#25169;&#23646;&#24615;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#33719;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20063;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.09924</link><description>&lt;p&gt;
&#22522;&#20110;&#28909;&#21644;&#27874;&#21160;&#21160;&#21147;&#23398;&#29305;&#24449;&#30340;&#22270;&#25299;&#25169;&#23646;&#24615;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Graph topological property recovery with heat and wave dynamics-based features on graphsD. (arXiv:2309.09924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#65288;GDeNet&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28909;&#21644;&#27874;&#21160;&#26041;&#31243;&#21160;&#21147;&#23398;&#29305;&#24449;&#26469;&#24674;&#22797;&#22270;&#30340;&#25299;&#25169;&#23646;&#24615;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#33719;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20063;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#65288;GDeNet&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#19978;&#30340;PDE&#35299;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20026;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#33719;&#24471;&#36830;&#32493;&#30340;&#33410;&#28857;&#21644;&#22270;&#32423;&#34920;&#31034;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#28909;&#21644;&#27874;&#21160;&#26041;&#31243;&#21160;&#21147;&#23398;&#19982;&#22270;&#30340;&#35889;&#29305;&#24615;&#20197;&#21450;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#28216;&#36208;&#22312;&#22270;&#19978;&#34892;&#20026;&#20043;&#38388;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#24674;&#22797;&#38543;&#26426;&#22270;&#29983;&#25104;&#21442;&#25968;&#12289;Ricci&#26354;&#29575;&#21644;&#25345;&#20037;&#21516;&#35843;&#31561;&#26041;&#24335;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#21160;&#21147;&#23398;&#33021;&#22815;&#25429;&#25417;&#21040;&#22270;&#24418;&#20960;&#20309;&#21644;&#25299;&#25169;&#30340;&#26174;&#33879;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;GDeNet&#22312;&#21253;&#25324;&#24341;&#29992;&#22270;&#12289;&#33647;&#29289;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#22312;&#20869;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose Graph Differential Equation Network (GDeNet), an approach that harnesses the expressive power of solutions to PDEs on a graph to obtain continuous node- and graph-level representations for various downstream tasks. We derive theoretical results connecting the dynamics of heat and wave equations to the spectral properties of the graph and to the behavior of continuous-time random walks on graphs. We demonstrate experimentally that these dynamics are able to capture salient aspects of graph geometry and topology by recovering generating parameters of random graphs, Ricci curvature, and persistent homology. Furthermore, we demonstrate the superior performance of GDeNet on real-world datasets including citation graphs, drug-like molecules, and proteins.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21387;&#21147;&#21644;&#21387;&#21147;&#30456;&#20851;&#31934;&#31070;&#38556;&#30861;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#20998;&#26512;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#19988;&#29983;&#29702;&#21442;&#25968;&#22914;&#24515;&#29575;&#27979;&#37327;&#21644;&#30382;&#32932;&#21453;&#24212;&#22312;&#21387;&#21147;&#39044;&#27979;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.04616</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#22312;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#30417;&#27979;&#21387;&#21147;&#21644;&#21387;&#21147;&#30456;&#20851;&#31934;&#31070;&#38556;&#30861;&#26041;&#38754;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#33539;&#22260;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Machine Learning, Deep Learning and Data Preprocessing Techniques for Detection, Prediction, and Monitoring of Stress and Stress-related Mental Disorders: A Scoping Review. (arXiv:2308.04616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04616
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21387;&#21147;&#21644;&#21387;&#21147;&#30456;&#20851;&#31934;&#31070;&#38556;&#30861;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#20998;&#26512;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#19988;&#29983;&#29702;&#21442;&#25968;&#22914;&#24515;&#29575;&#27979;&#37327;&#21644;&#30382;&#32932;&#21453;&#24212;&#22312;&#21387;&#21147;&#39044;&#27979;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#31995;&#32479;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#22312;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#20998;&#26512;&#31934;&#31070;&#21387;&#21147;&#21450;&#20854;&#30456;&#20851;&#31934;&#31070;&#38556;&#30861;&#65288;MDs&#65289;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#33539;&#22260;&#23457;&#26597;&#36807;&#31243;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#24212;&#29992;&#20110;&#21387;&#21147;&#19982;&#21387;&#21147;&#30456;&#20851;MDs&#32972;&#26223;&#19979;&#30340;&#26368;&#26032;ML&#31639;&#27861;&#12289;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#25968;&#25454;&#31867;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21644;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#27169;&#22411;&#22312;&#25152;&#26377;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#19968;&#36143;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#32508;&#36848;&#24378;&#35843;&#65292;&#29983;&#29702;&#21442;&#25968;&#22914;&#24515;&#29575;&#27979;&#37327;&#21644;&#30382;&#32932;&#21453;&#24212;&#26159;ML&#31639;&#27861;&#20013;&#24120;&#29992;&#30340;&#21387;&#21147;&#39044;&#27979;&#22240;&#23376;&#12290;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#33021;&#25552;&#20379;&#20851;&#20110;&#21387;&#21147;&#21644;&#21387;&#21147;&#30456;&#20851;MDs&#30340;&#20016;&#23500;&#35828;&#26126;&#24615;&#20449;&#24687;&#65292;&#24182;&#19988;&#25968;&#25454;&#37319;&#38598;&#30456;&#23545;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
This comprehensive review systematically evaluates Machine Learning (ML) methodologies employed in the detection, prediction, and analysis of mental stress and its consequent mental disorders (MDs). Utilizing a rigorous scoping review process, the investigation delves into the latest ML algorithms, preprocessing techniques, and data types employed in the context of stress and stress-related MDs. The findings highlight that Support Vector Machine (SVM), Neural Network (NN), and Random Forest (RF) models consistently exhibit superior accuracy and robustness among all machine learning algorithms examined. Furthermore, the review underscores that physiological parameters, such as heart rate measurements and skin response, are prevalently used as stress predictors in ML algorithms. This is attributed to their rich explanatory information concerning stress and stress-related MDs, as well as the relative ease of data acquisition. Additionally, the application of dimensionality reduction techn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.10246</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#65306;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#65288;&#32508;&#36848;&#65289;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey). (arXiv:2307.10246v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#22914;&#20309;&#34920;&#31034;&#19981;&#21516;&#30340;&#20449;&#24687;&#27169;&#24335;&#65311;&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#20986;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#29702;&#35299;&#29992;&#25143;&#24605;&#32771;&#20869;&#23481;&#30340;&#31995;&#32479;&#65311;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#30740;&#31350;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#31561;&#22823;&#33041;&#35760;&#24405;&#26469;&#22238;&#31572;&#12290;&#20316;&#20026;&#31532;&#19968;&#27493;&#65292;&#31070;&#32463;&#31185;&#23398;&#30028;&#20026;&#34987;&#21160;&#38405;&#35835;/&#21548;&#35273;/&#35266;&#30475;&#27010;&#24565;&#35789;&#27719;&#12289;&#21465;&#36848;&#12289;&#22270;&#29255;&#21644;&#30005;&#24433;&#30456;&#20851;&#30340;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#20013;&#65292;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#30740;&#31350;&#20013;&#30340;&#39069;&#22806;&#24037;&#20855;&#65292;&#22312;&#35748;&#30693;&#31185;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#32534;&#30721;&#27169;&#22411;&#26088;&#22312;&#33258;&#21160;&#22320;&#29983;&#25104;fMRI&#22823;&#33041;&#34920;&#24449;&#65292;&#32473;&#23450;&#19968;&#20010;&#21050;&#28608;&#12290;&#23427;&#20204;&#22312;&#35780;&#20272;&#21644;&#35786;&#26029;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20197;&#21450;&#35774;&#35745;&#22823;&#33041;&#25439;&#20260;&#27835;&#30103;&#26041;&#27861;&#26041;&#38754;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#35299;&#30721;&#27169;&#22411;&#35299;&#20915;&#20102;&#26681;&#25454;fMRI&#37325;&#26500;&#21050;&#28608;&#30340;&#36870;&#38382;&#39064;&#12290;&#23427;&#20204;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#22914;&#20309;&#22788;&#29702;&#20449;&#24687;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#30340;&#21457;&#23637;&#37117;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
How does the brain represent different modes of information? Can we design a system that automatically understands what the user is thinking? Such questions can be answered by studying brain recordings like functional magnetic resonance imaging (fMRI). As a first step, the neuroscience community has contributed several large cognitive neuroscience datasets related to passive reading/listening/viewing of concept words, narratives, pictures and movies. Encoding and decoding models using these datasets have also been proposed in the past two decades. These models serve as additional tools for basic research in cognitive science and neuroscience. Encoding models aim at generating fMRI brain representations given a stimulus automatically. They have several practical applications in evaluating and diagnosing neurological conditions and thus also help design therapies for brain damage. Decoding models solve the inverse problem of reconstructing the stimuli given the fMRI. They are useful for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TbExplain&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;XAI&#25216;&#26415;&#21644;&#39044;&#35757;&#32451;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#25991;&#26412;&#24418;&#24335;&#35299;&#37322;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32416;&#27491;&#39044;&#27979;&#21644;&#36827;&#34892;&#25991;&#26412;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.10003</link><description>&lt;p&gt;
TbExplain: &#19968;&#31181;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35299;&#37322;&#26041;&#27861;&#19982;&#32479;&#35745;&#39044;&#27979;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
TbExplain: A Text-based Explanation Method for Scene Classification Models with the Statistical Prediction Correction. (arXiv:2307.10003v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TbExplain&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;XAI&#25216;&#26415;&#21644;&#39044;&#35757;&#32451;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#25991;&#26412;&#24418;&#24335;&#35299;&#37322;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32416;&#27491;&#39044;&#27979;&#21644;&#36827;&#34892;&#25991;&#26412;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;(XAI)&#30340;&#39046;&#22495;&#26088;&#22312;&#25552;&#39640;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#24314;&#31435;&#22522;&#20110;&#36755;&#20837;&#29305;&#24449;&#37325;&#35201;&#24615;&#20540;&#30340;&#28909;&#22270;&#26159;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#39044;&#27979;&#30340;&#22522;&#26412;&#26041;&#27861;&#20043;&#19968;&#12290;&#28909;&#22270;&#22312;&#20154;&#31867;&#20013;&#20960;&#20046;&#21487;&#20197;&#29702;&#35299;&#65292;&#20294;&#24182;&#38750;&#27809;&#26377;&#32570;&#38519;&#12290;&#20363;&#22914;&#65292;&#38750;&#19987;&#19994;&#29992;&#25143;&#21487;&#33021;&#19981;&#23436;&#20840;&#29702;&#35299;&#28909;&#22270;&#30340;&#36923;&#36753;&#65288;&#21363;&#20351;&#29992;&#19981;&#21516;&#24378;&#24230;&#25110;&#39068;&#33394;&#31361;&#20986;&#26174;&#31034;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#30340;&#20687;&#32032;&#30340;&#36923;&#36753;&#65289;&#12290;&#27492;&#22806;&#65292;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#30340;&#36755;&#20837;&#22270;&#20687;&#30340;&#23545;&#35937;&#21644;&#21306;&#22495;&#36890;&#24120;&#26080;&#27861;&#23436;&#20840;&#36890;&#36807;&#28909;&#22270;&#21306;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TbExplain&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;XAI&#25216;&#26415;&#21644;&#39044;&#35757;&#32451;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#65292;&#20197;&#25991;&#26412;&#24418;&#24335;&#35299;&#37322;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;TbExplain&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32416;&#27491;&#39044;&#27979;&#21644;&#36827;&#34892;&#25991;&#26412;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Explainable Artificial Intelligence (XAI) aims to improve the interpretability of black-box machine learning models. Building a heatmap based on the importance value of input features is a popular method for explaining the underlying functions of such models in producing their predictions. Heatmaps are almost understandable to humans, yet they are not without flaws. Non-expert users, for example, may not fully understand the logic of heatmaps (the logic in which relevant pixels to the model's prediction are highlighted with different intensities or colors). Additionally, objects and regions of the input image that are relevant to the model prediction are frequently not entirely differentiated by heatmaps. In this paper, we propose a framework called TbExplain that employs XAI techniques and a pre-trained object detector to present text-based explanations of scene classification models. Moreover, TbExplain incorporates a novel method to correct predictions and textually exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#23398;&#20064;&#21160;&#21147;&#23398;&#20013;&#20010;&#20307;&#32676;&#20307;&#30340;&#21147;&#37327;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#23478;&#26063;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#21160;&#21147;&#23398;&#19982;&#19968;&#31867;&#8220;&#38646;&#21644;&#8221;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#30340;&#32852;&#31995;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#20998;&#26512;&#36825;&#20123;&#21327;&#35758;&#30340;&#32676;&#20307;&#32423;&#36951;&#25022;&#12290;&#22312;&#24191;&#27867;&#30340;&#21442;&#25968;&#33539;&#22260;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2306.08670</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#23398;&#20064;&#21160;&#21147;&#23398;&#20013;&#20010;&#20307;&#32676;&#20307;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
The Power of Populations in Decentralized Learning Dynamics. (arXiv:2306.08670v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#23398;&#20064;&#21160;&#21147;&#23398;&#20013;&#20010;&#20307;&#32676;&#20307;&#30340;&#21147;&#37327;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#23478;&#26063;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#21160;&#21147;&#23398;&#19982;&#19968;&#31867;&#8220;&#38646;&#21644;&#8221;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#30340;&#32852;&#31995;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#20998;&#26512;&#36825;&#20123;&#21327;&#35758;&#30340;&#32676;&#20307;&#32423;&#36951;&#25022;&#12290;&#22312;&#24191;&#27867;&#30340;&#21442;&#25968;&#33539;&#22260;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#22312;&#19968;&#20010;&#30001;$n$&#20010;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#33410;&#28857;&#32452;&#25104;&#30340;&#31181;&#32676;&#20013;&#65292;&#37319;&#29992;&#20102;&#35875;&#35328;&#27169;&#22411;&#65306;&#27599;&#36718;&#65292;&#27599;&#20010;&#33410;&#28857;&#26412;&#22320;&#37319;&#29992;$m$&#20010;&#33218;&#20043;&#19968;&#65292;&#35266;&#23519;&#20174;&#33218;&#30340;&#65288;&#23545;&#25239;&#36873;&#25321;&#30340;&#65289;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#22870;&#21169;&#65292;&#28982;&#21518;&#19982;&#38543;&#26426;&#25277;&#21462;&#30340;&#37051;&#23621;&#36827;&#34892;&#36890;&#20449;&#65292;&#20132;&#25442;&#20449;&#24687;&#65292;&#20197;&#30830;&#23450;&#19979;&#19968;&#36718;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#20171;&#32461;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#23478;&#26063;&#65306;&#27599;&#20010;&#33410;&#28857;&#30340;&#20915;&#31574;&#23436;&#20840;&#26159;&#23616;&#37096;&#30340;&#65292;&#21482;&#20381;&#36182;&#20110;&#20854;&#26368;&#26032;&#33719;&#24471;&#30340;&#22870;&#21169;&#20197;&#21450;&#23427;&#25277;&#26679;&#30340;&#37051;&#23621;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#30340;&#20840;&#23616;&#28436;&#21270;&#19982;&#29305;&#23450;&#31867;&#22411;&#30340;&#8220;&#38646;&#21644;&#8221;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#19988;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#36825;&#20123;&#33258;&#28982;&#21327;&#35758;&#30340;&#32676;&#20307;&#32423;&#36951;&#25022;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#21442;&#25968;&#33539;&#22260;&#65288;&#21363;&#65292;&#31181;&#32676;&#30340;&#22823;&#23567;&#21644;nu&#30340;&#22823;&#23567;&#65289;&#19979;&#25512;&#23548;&#20102;&#27425;&#32447;&#24615;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a distributed multi-armed bandit setting among a population of $n$ memory-constrained nodes in the gossip model: at each round, every node locally adopts one of $m$ arms, observes a reward drawn from the arm's (adversarially chosen) distribution, and then communicates with a randomly sampled neighbor, exchanging information to determine its policy in the next round. We introduce and analyze several families of dynamics for this task that are decentralized: each node's decision is entirely local and depends only on its most recently obtained reward and that of the neighbor it sampled. We show a connection between the global evolution of these decentralized dynamics with a certain class of "zero-sum" multiplicative weights update algorithms, and we develop a general framework for analyzing the population-level regret of these natural protocols. Using this framework, we derive sublinear regret bounds under a wide range of parameter regimes (i.e., the size of the population and nu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#38750;&#32447;&#24615;&#26799;&#24230;&#27169;&#22411;&#65288;NGM&#65289;&#65292;&#23427;&#26159;&#21487;&#35299;&#26512;&#22320;&#20351;&#29992;Taylor&#32423;&#25968;&#25299;&#23637;&#23548;&#20986;&#30340;&#38381;&#21512;&#24418;&#24335;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22320;&#29699;&#31995;&#32479;&#22797;&#26434;&#36807;&#31243;&#30340;&#23376;&#32593;&#26684;&#23610;&#24230;&#65288;SGS&#65289;&#38381;&#21512;/&#21442;&#25968;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.05014</link><description>&lt;p&gt;
&#20174;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#20013;&#23398;&#20064;&#23376;&#32593;&#26684;&#23610;&#24230;&#38381;&#21512;&#24418;&#24335;&#26041;&#31243;&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Learning Closed-form Equations for Subgrid-scale Closures from High-fidelity Data: Promises and Challenges. (arXiv:2306.05014v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#38750;&#32447;&#24615;&#26799;&#24230;&#27169;&#22411;&#65288;NGM&#65289;&#65292;&#23427;&#26159;&#21487;&#35299;&#26512;&#22320;&#20351;&#29992;Taylor&#32423;&#25968;&#25299;&#23637;&#23548;&#20986;&#30340;&#38381;&#21512;&#24418;&#24335;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22320;&#29699;&#31995;&#32479;&#22797;&#26434;&#36807;&#31243;&#30340;&#23376;&#32593;&#26684;&#23610;&#24230;&#65288;SGS&#65289;&#38381;&#21512;/&#21442;&#25968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21457;&#29616;&#22320;&#29699;&#31995;&#32479;&#22797;&#26434;&#36807;&#31243;&#30340;&#23376;&#32593;&#26684;&#23610;&#24230;&#65288;SGS&#65289;&#38381;&#21512;/&#21442;&#25968;&#21270;&#30340;&#21487;&#35299;&#37322;&#24615;&#38381;&#21512;&#24418;&#24335;&#26041;&#31243;&#19978;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#34920;&#29616;&#20986;&#27987;&#21402;&#20852;&#36259;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#24191;&#27867;&#30340;&#24211;&#24212;&#29992;&#36890;&#29992;&#30340;&#26041;&#31243;&#21457;&#29616;&#25216;&#26415;&#65292;&#20174;&#32463;&#36807;&#28388;&#27874;&#30340;&#20108;&#32500;&#24378;&#36843;&#28237;&#27969;&#21644;&#29790;&#21033; - &#36125;&#32435;&#24503;&#23545;&#27969;&#30340;&#30452;&#25509;&#25968;&#20540;&#27169;&#25311;&#20013;&#23398;&#20064;&#38381;&#21512;&#24418;&#24335;&#12290;&#22312;&#24120;&#35265;&#30340;&#28388;&#27874;&#22120;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#24378;&#26377;&#21147;&#22320;&#21457;&#29616;&#20102;&#21160;&#37327;&#21644;&#28909;&#36890;&#37327;&#30340;&#30456;&#21516;&#24418;&#24335;&#30340;&#38381;&#21512;&#24418;&#24335;&#12290;&#36825;&#20123;&#38381;&#21512;&#24418;&#24335;&#21462;&#20915;&#20110;&#34987;&#36807;&#28388;&#21464;&#37327;&#65288;&#36895;&#24230;&#12289;&#28201;&#24230;&#65289;&#30340;&#26799;&#24230;&#30340;&#38750;&#32447;&#24615;&#32452;&#21512;&#65292;&#20854;&#20013;&#30340;&#24120;&#25968;&#29420;&#31435;&#20110;&#27969;&#20307;/&#27969;&#21160;&#29305;&#24615;&#65292;&#20165;&#20381;&#36182;&#20110;&#36807;&#28388;&#22120;&#31867;&#22411;/&#22823;&#23567;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20123;&#38381;&#21512;&#24418;&#24335;&#26159;&#38750;&#32447;&#24615;&#26799;&#24230;&#27169;&#22411;&#65288;NGM&#65289;&#65292;&#21487;&#20197;&#20351;&#29992;Taylor&#32423;&#25968;&#23637;&#24320;&#20998;&#26512;&#22320;&#23548;&#20986;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#20351;&#29992;&#24120;&#35265;&#30340;&#65288;&#26080;&#29289;&#29702;&#20449;&#24687;&#30340;&#65289;&#26041;&#31243;&#21457;&#29616;&#31639;&#27861;&#26102;&#65292;&#26080;&#35770;&#26159;&#20160;&#20040;&#31995;&#32479;/&#29289;&#29702;&#23398;&#65292;&#21457;&#29616;&#30340;&#38381;&#21512;&#24418;&#24335;&#22987;&#32456;&#19982;Taylor&#32423;&#25968;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is growing interest in discovering interpretable, closed-form equations for subgrid-scale (SGS) closures/parameterizations of complex processes in Earth system. Here, we apply a common equation-discovery technique with expansive libraries to learn closures from filtered direct numerical simulations of 2D forced turbulence and Rayleigh-B\'enard convection (RBC). Across common filters, we robustly discover closures of the same form for momentum and heat fluxes. These closures depend on nonlinear combinations of gradients of filtered variables (velocity, temperature), with constants that are independent of the fluid/flow properties and only depend on filter type/size. We show that these closures are the nonlinear gradient model (NGM), which is derivable analytically using Taylor-series expansions. In fact, we suggest that with common (physics-free) equation-discovery algorithms, regardless of the system/physics, discovered closures are always consistent with the Taylor-series. Like 
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#22312;&#35757;&#32451;&#23436;&#21518;&#65292;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#29983;&#25104;&#19968;&#20010;&#32463;&#20856;&#38452;&#24433;&#27169;&#22411;&#26469;&#35745;&#31639;&#20989;&#25968;&#30340;&#32463;&#20856;&#35745;&#31639;&#36817;&#20284;&#65292;&#36991;&#20813;&#20102;&#23545;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.00061</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#38452;&#24433;
&lt;/p&gt;
&lt;p&gt;
Shadows of quantum machine learning. (arXiv:2306.00061v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00061
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#22312;&#35757;&#32451;&#23436;&#21518;&#65292;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#29983;&#25104;&#19968;&#20010;&#32463;&#20856;&#38452;&#24433;&#27169;&#22411;&#26469;&#35745;&#31639;&#20989;&#25968;&#30340;&#32463;&#20856;&#35745;&#31639;&#36817;&#20284;&#65292;&#36991;&#20813;&#20102;&#23545;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#32463;&#24120;&#34987;&#35748;&#20026;&#26159;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#30340;&#26368;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#38459;&#30861;&#20854;&#22312;&#23454;&#36341;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;&#36825;&#20123;&#27169;&#22411;&#21363;&#20351;&#22312;&#35757;&#32451;&#36807;&#31243;&#21518;&#65292;&#20173;&#38656;&#35201;&#35775;&#38382;&#37327;&#23376;&#35745;&#31639;&#26426;&#25165;&#33021;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#37327;&#23376;&#27169;&#22411;&#30340;&#35757;&#32451;&#38454;&#27573;&#20043;&#21518;&#65292;&#37327;&#23376;&#35745;&#31639;&#26426;&#21487;&#20197;&#29992;&#26469;&#29983;&#25104;&#25105;&#20204;&#25152;&#35859;&#30340;&#35813;&#27169;&#22411;&#30340;&#8220;&#32463;&#20856;&#38452;&#24433;&#8221;&#65292;&#21363;&#24050;&#23398;&#20064;&#20989;&#25968;&#30340;&#32463;&#20856;&#35745;&#31639;&#36817;&#20284;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#36825;&#20010;&#24819;&#27861;&#24182;&#25552;&#20986;&#20102;&#26500;&#24314;&#36825;&#31181;&#24433;&#23376;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20063;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#32463;&#20856;&#27169;&#22411;&#21487;&#33021;&#20195;&#26367;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#39318;&#20808;&#22238;&#36991;&#20102;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#38656;&#35201;&#12290;&#26412;&#25991;&#37319;&#29992;&#26032;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#37327;&#23376;&#32447;&#24615;&#27169;&#22411;&#21644;&#32463;&#20856;&#38452;&#24433;&#37325;&#26500;&#30340;&#26694;&#26550;&#26469;&#23450;&#20041;&#38452;&#24433;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning is often highlighted as one of the most promising uses for a quantum computer to solve practical problems. However, a major obstacle to the widespread use of quantum machine learning models in practice is that these models, even once trained, still require access to a quantum computer in order to be evaluated on new data. To solve this issue, we suggest that following the training phase of a quantum model, a quantum computer could be used to generate what we call a classical shadow of this model, i.e., a classically computable approximation of the learned function. While recent works already explore this idea and suggest approaches to construct such shadow models, they also raise the possibility that a completely classical model could be trained instead, thus circumventing the need for a quantum computer in the first place. In this work, we take a novel approach to define shadow models based on the frameworks of quantum linear models and classical shadow tomogr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#27169;&#25311;&#26102;&#38388;&#28436;&#21270;&#34203;&#23450;&#35860;&#26041;&#31243;&#65292;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#37319;&#26679;&#26469;&#36866;&#24212;&#27874;&#20989;&#25968;&#30340;&#28508;&#22312;&#20302;&#32500;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#38543;&#26426;&#37327;&#23376;&#21147;&#23398;&#26041;&#31243;&#65292;&#20855;&#26377;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25968;&#20540;&#27169;&#25311;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.19685</link><description>&lt;p&gt;
&#28145;&#24230;&#38543;&#26426;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Deep Stochastic Mechanics. (arXiv:2305.19685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#27169;&#25311;&#26102;&#38388;&#28436;&#21270;&#34203;&#23450;&#35860;&#26041;&#31243;&#65292;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#37319;&#26679;&#26469;&#36866;&#24212;&#27874;&#20989;&#25968;&#30340;&#28508;&#22312;&#20302;&#32500;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#38543;&#26426;&#37327;&#23376;&#21147;&#23398;&#26041;&#31243;&#65292;&#20855;&#26377;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25968;&#20540;&#27169;&#25311;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#27169;&#25311;&#26102;&#38388;&#28436;&#21270;&#34203;&#23450;&#35860;&#26041;&#31243;&#65292;&#21463;&#38543;&#26426;&#21147;&#23398;&#21644;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#21551;&#21457;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#20174;&#39532;&#23572;&#21487;&#22827;&#25193;&#25955;&#20013;&#37319;&#26679;&#26469;&#36866;&#24212;&#27874;&#20989;&#25968;&#28508;&#22312;&#30340;&#20302;&#32500;&#32467;&#26500;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#26356;&#39640;&#30340;&#32500;&#24230;&#19978;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#38543;&#26426;&#37327;&#23376;&#21147;&#23398;&#26041;&#31243;&#65292;&#32467;&#26524;&#20855;&#26377;&#19982;&#32500;&#25968;&#25968;&#37327;&#32447;&#24615;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#24182;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#29992;&#20110;&#37327;&#23376;&#21147;&#23398;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel deep-learning-based approach for numerical simulation of a time-evolving Schr\"odinger equation inspired by stochastic mechanics and generative diffusion models. Unlike existing approaches, which exhibit computational complexity that scales exponentially in the problem dimension, our method allows us to adapt to the latent low-dimensional structure of the wave function by sampling from the Markovian diffusion. Depending on the latent dimension, our method may have far lower computational complexity in higher dimensions. Moreover, we propose novel equations for stochastic quantum mechanics, resulting in linear computational complexity with respect to the number of dimensions. Numerical simulations verify our theoretical findings and show a significant advantage of our method compared to other deep-learning-based approaches used for quantum mechanics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Phy-DRL&#65292;&#36825;&#26159;&#19968;&#20010;&#29289;&#29702;&#27169;&#22411;&#35843;&#25972;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26377;&#19977;&#20010;&#21019;&#26032;&#28857;&#65292;&#23427;&#20204;&#20998;&#21035;&#26159;: i)&#21069;&#30651;&#24615;&#30340;&#26410;&#30693;&#26410;&#30693;&#35757;&#32451;&#65292;ii)&#32467;&#21512;&#27531;&#24046;&#25511;&#21046;&#65292;&#20197;&#21450;iii)&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#36753;&#12290;Phy-DRL&#33021;&#22815;&#23481;&#24525;&#26410;&#30693;&#24178;&#25200;&#65292;&#20445;&#35777;&#23433;&#20840;&#21644;&#31283;&#23450;&#65292;&#21516;&#26102;&#36981;&#23432;Bellman&#26041;&#31243;&#21644;&#22870;&#21169;&#30456;&#20851;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.16614</link><description>&lt;p&gt;
&#29289;&#29702;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;: &#23433;&#20840;&#21644;&#26410;&#30693;&#26410;&#30693;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Physical Deep Reinforcement Learning: Safety and Unknown Unknowns. (arXiv:2305.16614v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Phy-DRL&#65292;&#36825;&#26159;&#19968;&#20010;&#29289;&#29702;&#27169;&#22411;&#35843;&#25972;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26377;&#19977;&#20010;&#21019;&#26032;&#28857;&#65292;&#23427;&#20204;&#20998;&#21035;&#26159;: i)&#21069;&#30651;&#24615;&#30340;&#26410;&#30693;&#26410;&#30693;&#35757;&#32451;&#65292;ii)&#32467;&#21512;&#27531;&#24046;&#25511;&#21046;&#65292;&#20197;&#21450;iii)&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#36753;&#12290;Phy-DRL&#33021;&#22815;&#23481;&#24525;&#26410;&#30693;&#24178;&#25200;&#65292;&#20445;&#35777;&#23433;&#20840;&#21644;&#31283;&#23450;&#65292;&#21516;&#26102;&#36981;&#23432;Bellman&#26041;&#31243;&#21644;&#22870;&#21169;&#30456;&#20851;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Phy-DRL&#65292;&#19968;&#20010;&#29289;&#29702;&#27169;&#22411;&#35843;&#33410;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#33258;&#20027;&#31995;&#32479;&#12290;Phy-DRL&#20855;&#26377;&#19977;&#31181;&#29420;&#29305;&#30340;&#21019;&#26032;&#65306;i&#65289;&#21069;&#30651;&#24615;&#30340;&#26410;&#30693;&#26410;&#30693;&#35757;&#32451;&#65292;ii&#65289;&#32467;&#21512;&#27531;&#24046;&#25511;&#21046;&#65288;&#21363;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#21644;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#25511;&#21046;&#30340;&#38598;&#25104;&#65289;&#21644;&#23433;&#20840;&#21450;&#31283;&#23450;&#24615;&#25935;&#24863;&#30340;&#22870;&#21169;&#65292;&#20197;&#21450;iii&#65289;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#36753;&#65292;&#21253;&#25324;&#38142;&#25509;&#32534;&#36753;&#21644;&#28608;&#27963;&#32534;&#36753;&#12290;&#30001;&#20110;&#36825;&#20123;&#24182;&#21457;&#35774;&#35745;&#65292;Phy-DRL&#33021;&#22815;1&#65289;&#23481;&#24525;&#26410;&#30693;&#24178;&#25200;&#65292;2&#65289;&#20445;&#35777;&#21487;&#25968;&#23398;&#35777;&#26126;&#30340;&#23433;&#20840;&#19982;&#31283;&#23450;&#24615;&#65292;&#24182;3&#65289;&#20005;&#26684;&#36981;&#23432;Bellman&#26041;&#31243;&#21644;&#22870;&#21169;&#30456;&#20851;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;&#26368;&#32456;&#65292;&#36890;&#36807;&#20498;&#31435;&#25670;&#21644;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;Phy-DRL&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;DRL&#30456;&#27604;&#65292;Phy-DRL&#20855;&#26377;&#26126;&#26174;&#26356;&#23569;&#30340;&#23398;&#20064;&#21442;&#25968;&#12289;&#21152;&#36895;&#30340;&#35757;&#32451;&#21644;&#25193;&#22823;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the Phy-DRL: a physics-model-regulated deep reinforcement learning framework for safety-critical autonomous systems. The Phy-DRL is unique in three innovations: i) proactive unknown-unknowns training, ii) conjunctive residual control (i.e., integration of data-driven control and physics-model-based control) and safety- \&amp; stability-sensitive reward, and iii) physics-model-based neural network editing, including link editing and activation editing. Thanks to the concurrent designs, the Phy-DRL is able to 1) tolerate unknown-unknowns disturbances, 2) guarantee mathematically provable safety and stability, and 3) strictly comply with physical knowledge pertaining to Bellman equation and reward. The effectiveness of the Phy-DRL is finally validated by an inverted pendulum and a quadruped robot. The experimental results demonstrate that compared with purely data-driven DRL, Phy-DRL features remarkably fewer learning parameters, accelerated training and enlarged rew
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#21487;&#33021;&#20250;&#23548;&#33268;&#35760;&#24518;&#35757;&#32451;&#22270;&#20687;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#27169;&#22411;&#26102;&#38656;&#35201;&#35880;&#24910;&#12290;</title><link>http://arxiv.org/abs/2305.07644</link><description>&lt;p&gt;
&#35686;&#24789;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687; -- &#19982; GAN &#22312;&#35760;&#24518;&#33041;&#32959;&#30244;&#22270;&#20687;&#26041;&#38754;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beware of diffusion models for synthesizing medical images -- A comparison with GANs in terms of memorizing brain tumor images. (arXiv:2305.07644v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07644
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#21487;&#33021;&#20250;&#23548;&#33268;&#35760;&#24518;&#35757;&#32451;&#22270;&#20687;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#27169;&#22411;&#26102;&#38656;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26368;&#21021;&#26159;&#20026;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#32780;&#24320;&#21457;&#30340;&#65292;&#29616;&#22312;&#20063;&#34987;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;&#22312; GAN &#20043;&#21069;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20351;&#29992;&#20102;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#25351;&#26631;&#22914; FID &#21644; IS &#24182;&#19981;&#36866;&#21512;&#30830;&#23450;&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#21482;&#26159;&#22797;&#21046;&#20102;&#35757;&#32451;&#22270;&#20687;&#12290;&#36825;&#37324;&#25105;&#20204;&#20351;&#29992; BRATS20 &#21644; BRATS21 &#25968;&#25454;&#38598;&#35757;&#32451; StyleGAN &#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#29983;&#25104;&#33041;&#32959;&#30244;&#22270;&#20687;&#65292;&#24182;&#27979;&#37327;&#21512;&#25104;&#22270;&#20687;&#19982;&#25152;&#26377;&#35757;&#32451;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#35760;&#24518;&#35757;&#32451;&#22270;&#20687;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23567;&#25968;&#25454;&#38598;&#12290;&#22914;&#26524;&#26368;&#32456;&#30446;&#26631;&#26159;&#20849;&#20139;&#21512;&#25104;&#30340;&#22270;&#20687;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#25104;&#20687;&#26102;&#24212;&#35813;&#23567;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models were initially developed for text-to-image generation and are now being utilized to generate high quality synthetic images. Preceded by GANs, diffusion models have shown impressive results using various evaluation metrics. However, commonly used metrics such as FID and IS are not suitable for determining whether diffusion models are simply reproducing the training images. Here we train StyleGAN and diffusion models, using BRATS20 and BRATS21 datasets, to synthesize brain tumor images, and measure the correlation between the synthetic images and all training images. Our results show that diffusion models are much more likely to memorize the training images, especially for small datasets. Researchers should be careful when using diffusion models for medical imaging, if the final goal is to share the synthetic images.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#31169;&#26377;&#31574;&#30053;&#21644;&#39044;&#27979;&#31283;&#24577;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#65292;&#22806;&#37096;&#35266;&#23519;&#32773;&#21487;&#20197;&#25104;&#21151;&#36827;&#34892;&#39044;&#27979;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.12561</link><description>&lt;p&gt;
&#35299;&#20915;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An active learning method for solving competitive multi-agent decision-making and control problems. (arXiv:2212.12561v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12561
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#31169;&#26377;&#31574;&#30053;&#21644;&#39044;&#27979;&#31283;&#24577;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#65292;&#22806;&#37096;&#35266;&#23519;&#32773;&#21487;&#20197;&#25104;&#21151;&#36827;&#34892;&#39044;&#27979;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#37325;&#26500;&#30001;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#20154;&#32676;&#20307;&#25191;&#34892;&#30340;&#31169;&#26377;&#31574;&#30053;&#65292;&#24182;&#39044;&#27979;&#24213;&#23618;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#36807;&#31243;&#30340;&#30830;&#20999;&#32467;&#26524;&#65292;&#36825;&#37324;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#31283;&#23450;&#30340;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#12290;&#25105;&#20204;&#35774;&#24819;&#20102;&#19968;&#20010;&#22330;&#26223;&#65292;&#22312;&#36825;&#20010;&#22330;&#26223;&#20013;&#65292;&#19968;&#20010;&#20855;&#26377;&#23398;&#20064;&#31243;&#24207;&#30340;&#22806;&#37096;&#35266;&#23519;&#32773;&#21487;&#20197;&#36890;&#36807;&#31169;&#26377;&#30340;&#34892;&#21160;-&#21453;&#24212;&#26144;&#23556;&#36827;&#34892;&#26597;&#35810;&#21644;&#35266;&#23519;&#20195;&#29702;&#20154;&#30340;&#21453;&#24212;&#65292;&#38598;&#20307;&#30340;&#19981;&#21160;&#28857;&#23545;&#24212;&#20110;&#19968;&#20010;&#31283;&#24577;&#37197;&#32622;&#25991;&#20214;&#12290;&#36890;&#36807;&#36845;&#20195;&#22320;&#25910;&#38598;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#21644;&#26356;&#26032;&#34892;&#21160;-&#21453;&#24212;&#26144;&#23556;&#30340;&#21442;&#25968;&#20272;&#35745;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#28176;&#36817;&#24615;&#36136;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#20415;&#22914;&#26524;&#25910;&#25947;&#21457;&#29983;&#65292;&#23427;&#21482;&#33021;&#26397;&#21521;&#19968;&#20010;&#31283;&#24577;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#12290;&#36825;&#19968;&#20107;&#23454;&#23548;&#33268;&#20102;&#20004;&#20010;&#20027;&#35201;&#32467;&#26524;&#65306;i&#65289;&#23398;&#20064;&#23616;&#37096;&#31934;&#30830;&#30340;&#34892;&#21160;-&#21453;&#24212;&#26144;&#23556;&#26367;&#20195;&#29289;&#20351;&#24471;&#22806;&#37096;&#35266;&#23519;&#32773;&#33021;&#22815;&#25104;&#21151;&#23436;&#25104;&#20854;&#39044;&#27979;&#20219;&#21153;&#65292;ii&#65289;&#19982;&#20195;&#29702;&#20154;&#30340;&#20114;&#21160;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#31574;&#30053;&#20197;&#36798;&#21040;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a scheme based on active learning to reconstruct private strategies executed by a population of interacting agents and predict an exact outcome of the underlying multi-agent interaction process, here identified as a stationary action profile. We envision a scenario where an external observer, endowed with a learning procedure, can make queries and observe the agents' reactions through private action-reaction mappings, whose collective fixed point corresponds to a stationary profile. By iteratively collecting sensible data and updating parametric estimates of the action-reaction mappings, we establish sufficient conditions to assess the asymptotic properties of the proposed active learning methodology so that, if convergence happens, it can only be towards a stationary action profile. This fact yields two main consequences: i) learning locally-exact surrogates of the action-reaction mappings allows the external observer to succeed in its prediction task, and ii) working with 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Passau-SFCH&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;11&#23567;&#26102;&#30340;&#24405;&#38899;&#65292;&#29992;&#20110;&#33258;&#21457;&#24189;&#40664;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#22810;&#27169;&#24577;&#30340;&#20998;&#26512;&#21644;&#29305;&#24449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#24189;&#40664;&#20197;&#21450;&#24189;&#40664;&#24773;&#24863;&#30340;&#33258;&#21160;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2209.14272</link><description>&lt;p&gt;
&#36808;&#21521;&#22810;&#27169;&#24577;&#39044;&#27979;&#33258;&#21457;&#24189;&#40664;&#65306;&#19968;&#20221;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#21644;&#21021;&#27493;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Towards Multimodal Prediction of Spontaneous Humour: A Novel Dataset and First Results. (arXiv:2209.14272v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Passau-SFCH&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;11&#23567;&#26102;&#30340;&#24405;&#38899;&#65292;&#29992;&#20110;&#33258;&#21457;&#24189;&#40664;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#22810;&#27169;&#24577;&#30340;&#20998;&#26512;&#21644;&#29305;&#24449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#24189;&#40664;&#20197;&#21450;&#24189;&#40664;&#24773;&#24863;&#30340;&#33258;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24189;&#40664;&#26159;&#20154;&#31867;&#24773;&#24863;&#21644;&#35748;&#30693;&#30340;&#37325;&#35201;&#20803;&#32032;&#12290;&#20854;&#33258;&#21160;&#29702;&#35299;&#21487;&#20197;&#20419;&#36827;&#26356;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20154;&#24615;&#21270;&#12290;&#30446;&#21069;&#30340;&#24189;&#40664;&#26816;&#27979;&#26041;&#27861;&#20165;&#22522;&#20110;&#31574;&#21010;&#25968;&#25454;&#65292;&#19981;&#33021;&#28385;&#36275;&#8220;&#29616;&#23454;&#19990;&#30028;&#8221;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;Passau-Spontaneous Football Coach Humour&#65288;Passau-SFCH&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#32422;11&#23567;&#26102;&#30340;&#24405;&#38899;&#65292;&#35299;&#20915;&#20102;&#36825;&#19968;&#32570;&#38519;&#12290;Passau-SFCH&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#26681;&#25454;Martin&#30340;&#24189;&#40664;&#39118;&#26684;&#38382;&#21367;&#25552;&#20986;&#30340;&#24189;&#40664;&#23384;&#22312;&#21450;&#20854;&#32500;&#24230;&#65288;&#24773;&#24863;&#21644;&#26041;&#21521;&#65289;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#19987;&#23478;&#35774;&#35745;&#30340;&#29305;&#24449;&#12290;&#20998;&#26512;&#20102;&#33258;&#21457;&#24189;&#40664;&#35782;&#21035;&#30340;&#27599;&#31181;&#27169;&#24577;&#65288;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#24189;&#40664;&#21450;&#20854;&#24773;&#24863;&#30340;&#33258;&#21160;&#20998;&#26512;&#65292;&#22810;&#27169;&#24577;&#32852;&#21512;&#20351;&#29992;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humour is a substantial element of human affect and cognition. Its automatic understanding can facilitate a more naturalistic human-device interaction and the humanisation of artificial intelligence. Current methods of humour detection are solely based on staged data making them inadequate for 'real-world' applications. We address this deficiency by introducing the novel Passau-Spontaneous Football Coach Humour (Passau-SFCH) dataset, comprising of about 11 hours of recordings. The Passau-SFCH dataset is annotated for the presence of humour and its dimensions (sentiment and direction) as proposed in Martin's Humor Style Questionnaire. We conduct a series of experiments, employing pretrained Transformers, convolutional neural networks, and expert-designed features. The performance of each modality (text, audio, video) for spontaneous humour recognition is analysed and their complementarity is investigated. Our findings suggest that for the automatic analysis of humour and its sentiment, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21452;&#23618; MIP &#30340;&#38381;&#29615;&#39044;&#27979;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#25104;&#26412;&#23548;&#21521;&#30340;&#39044;&#27979;&#22120;&#26469;&#25913;&#36827;&#30005;&#21147;&#31995;&#32479;&#30340;&#32463;&#27982;&#36816;&#34892;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#21453;&#39304;&#24490;&#29615;&#36845;&#20195;&#22320;&#25913;&#36827;&#39044;&#27979;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;&#26426;&#32452;&#32452;&#21512;&#30340;&#26368;&#20339;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2208.13065</link><description>&lt;p&gt;
&#25913;&#21892;&#36816;&#33829;&#32463;&#27982;&#23398;&#65306;&#22522;&#20110;&#21452;&#23618; MIP &#30340;&#38381;&#29615;&#39044;&#27979;&#20248;&#21270;&#26694;&#26550;&#26469;&#39044;&#27979;&#26426;&#32452;&#32452;&#21512;&#30340;&#25805;&#20316;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Towards Improving Operation Economics: A Bilevel MIP-Based Closed-Loop Predict-and-Optimize Framework for Prescribing Unit Commitment. (arXiv:2208.13065v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21452;&#23618; MIP &#30340;&#38381;&#29615;&#39044;&#27979;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#25104;&#26412;&#23548;&#21521;&#30340;&#39044;&#27979;&#22120;&#26469;&#25913;&#36827;&#30005;&#21147;&#31995;&#32479;&#30340;&#32463;&#27982;&#36816;&#34892;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#21453;&#39304;&#24490;&#29615;&#36845;&#20195;&#22320;&#25913;&#36827;&#39044;&#27979;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;&#26426;&#32452;&#32452;&#21512;&#30340;&#26368;&#20339;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#31995;&#32479;&#25805;&#20316;&#21592;&#22312;&#24320;&#29615;&#39044;&#27979;&#20248;&#21270;&#36807;&#31243;&#20013;&#36827;&#34892;&#30005;&#21147;&#31995;&#32479;&#30340;&#32463;&#27982;&#36816;&#34892;&#65306;&#39318;&#20808;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;(RES)&#30340;&#21487;&#29992;&#24615;&#21644;&#31995;&#32479;&#20648;&#22791;&#38656;&#27714;&#65307;&#26681;&#25454;&#36825;&#20123;&#39044;&#27979;&#65292;&#31995;&#32479;&#25805;&#20316;&#21592;&#35299;&#20915;&#35832;&#22914;&#26426;&#32452;&#32452;&#21512;(UC)&#30340;&#20248;&#21270;&#27169;&#22411;&#65292;&#20197;&#30830;&#23450;&#30456;&#24212;&#30340;&#32463;&#27982;&#36816;&#34892;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24320;&#29615;&#36807;&#31243;&#21487;&#33021;&#20250;&#23454;&#36136;&#24615;&#22320;&#25439;&#23475;&#25805;&#20316;&#32463;&#27982;&#24615;&#65292;&#22240;&#20026;&#23427;&#30340;&#39044;&#27979;&#22120;&#30446;&#20809;&#30701;&#27973;&#22320;&#23547;&#27714;&#25913;&#21892;&#21363;&#26102;&#30340;&#32479;&#35745;&#39044;&#27979;&#35823;&#24046;&#65292;&#32780;&#19981;&#26159;&#26368;&#32456;&#30340;&#25805;&#20316;&#25104;&#26412;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#29615;&#39044;&#27979;&#20248;&#21270;&#26694;&#26550;&#65292;&#25552;&#20379;&#19968;&#31181;&#39044;&#27979;&#26426;&#32452;&#32452;&#21512;&#20197;&#25913;&#21892;&#25805;&#20316;&#32463;&#27982;&#24615;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;&#21452;&#23618;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#38024;&#23545;&#26368;&#20339;&#31995;&#32479;&#25805;&#20316;&#35757;&#32451;&#25104;&#26412;&#23548;&#21521;&#30340;&#39044;&#27979;&#22120;&#12290;&#19978;&#23618;&#22522;&#20110;&#20854;&#24341;&#36215;&#30340;&#25805;&#20316;&#25104;&#26412;&#26469;&#35757;&#32451; RES &#21644;&#20648;&#22791;&#39044;&#27979;&#22120;&#65307;&#19979;&#23618;&#21017;&#22312;&#32473;&#23450;&#39044;&#27979;&#30340; RES &#21644;&#20648;&#22791;&#30340;&#24773;&#20917;&#19979;&#65292;&#20381;&#25454;&#26368;&#20339;&#25805;&#20316;&#21407;&#21017;&#27714;&#35299; UC&#12290;&#36825;&#20004;&#20010;&#23618;&#32423;&#36890;&#36807;&#21453;&#39304;&#29615;&#36335;&#36827;&#34892;&#20132;&#20114;&#24615;&#20114;&#21160;&#65292;&#30452;&#21040;&#25910;&#25947;&#20026;&#27490;&#12290;&#22312;&#20462;&#25913;&#21518;&#30340;IEEE 24-bus&#31995;&#32479;&#19978;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340; UC &#22522;&#20934;&#32447;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generally, system operators conduct the economic operation of power systems in an open-loop predict-then-optimize process: the renewable energy source (RES) availability and system reserve requirements are first predicted; given the predictions, system operators solve optimization models such as unit commitment (UC) to determine the economical operation plans accordingly. However, such an open-loop process could essentially compromise the operation economics because its predictors myopically seek to improve the immediate statistical prediction errors instead of the ultimate operation cost. To this end, this paper presents a closed-loop predict-and-optimize framework, offering a prescriptive UC to improve the operation economics. First, a bilevel mixed-integer programming model is leveraged to train cost-oriented predictors tailored for optimal system operations: the upper level trains the RES and reserve predictors based on their induced operation cost; the lower level, with given pred
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MDLatLRRv2&#30340;&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#22810;&#32423;&#20998;&#35299;&#26041;&#27861;&#24182;&#20805;&#20998;&#21033;&#29992;LatLRR&#25552;&#21462;&#30340;&#21508;&#31181;&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#20013;&#30340;&#26368;&#20808;&#36827;&#34701;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.15179</link><description>&lt;p&gt;
&#22522;&#20110;MDLatLRRv2&#30340;&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Medical Image Fusion Method based on MDLatLRRv2. (arXiv:2206.15179v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15179
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MDLatLRRv2&#30340;&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#22810;&#32423;&#20998;&#35299;&#26041;&#27861;&#24182;&#20805;&#20998;&#21033;&#29992;LatLRR&#25552;&#21462;&#30340;&#21508;&#31181;&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#20013;&#30340;&#26368;&#20808;&#36827;&#34701;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;MDLatLRR&#20165;&#32771;&#34385;&#20102;&#36890;&#36807;&#28508;&#22312;&#20302;&#31209;&#34920;&#31034;&#65288;LatLRR&#65289;&#25552;&#21462;&#30340;&#36755;&#20837;&#22270;&#20687;&#30340;&#35814;&#32454;&#37096;&#20998;&#65288;&#26174;&#33879;&#29305;&#24449;&#65289;&#65292;&#27809;&#26377;&#26377;&#25928;&#22320;&#21033;&#29992;LatLRR&#25552;&#21462;&#30340;&#22522;&#30784;&#37096;&#20998;&#65288;&#20027;&#35201;&#29305;&#24449;&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22810;&#32423;&#20998;&#35299;&#26041;&#27861;&#65292;&#31216;&#20026;MDLatLRRv2&#65292;&#35813;&#26041;&#27861;&#33021;&#26377;&#25928;&#20998;&#26512;&#21644;&#21033;&#29992;LatLRR&#33719;&#21462;&#30340;&#25152;&#26377;&#22270;&#20687;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;MDLatLRRv2&#24212;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;&#12290;&#22522;&#30784;&#37096;&#20998;&#36890;&#36807;&#24179;&#22343;&#31574;&#30053;&#36827;&#34892;&#34701;&#21512;&#65292;&#35814;&#32454;&#37096;&#20998;&#36890;&#36807;&#26680;&#33539;&#25968;&#25805;&#20316;&#36827;&#34892;&#34701;&#21512;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30340;&#27604;&#36739;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#20013;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#34701;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since MDLatLRR only considers detailed parts (salient features) of input images extracted by latent low-rank representation (LatLRR), it doesn't use base parts (principal features) extracted by LatLRR effectively. Therefore, we proposed an improved multi-level decomposition method called MDLatLRRv2 which effectively analyzes and utilizes all the image features obtained by LatLRR. Then we apply MDLatLRRv2 to medical image fusion. The base parts are fused by average strategy and the detail parts are fused by nuclear-norm operation. The comparison with the existing methods demonstrates that the proposed method can achieve state-of-the-art fusion performance in objective and subjective assessment.
&lt;/p&gt;</description></item></channel></rss>