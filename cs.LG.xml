<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2404.00297</link><description>&lt;p&gt;
TRABSA&#65306;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;BiLSTM&#21644;Twitter-RoBERTa&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00297
&lt;/p&gt;
&lt;p&gt;
TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#23545;&#20110;&#29702;&#35299;&#20844;&#20247;&#33286;&#35770;&#21644;&#28040;&#36153;&#32773;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#27169;&#22411;&#38754;&#20020;&#30528;&#35821;&#35328;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TRABSA&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#21033;&#29992;&#22312;124M&#26465;&#25512;&#25991;&#19978;&#35757;&#32451;&#30340;RoBERTa&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24046;&#36317;&#65292;&#30830;&#20445;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#23558;&#26469;&#33258;32&#20010;&#22269;&#23478;&#21644;&#32654;&#22269;&#21508;&#24030;&#30340;&#25512;&#25991;&#19982;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20845;&#31181;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#19977;&#31181;&#22522;&#20110;&#35789;&#20856;&#30340;&#26631;&#27880;&#25216;&#26415;&#65292;&#24182;&#36873;&#25321;&#20102;&#26368;&#20339;&#25216;&#26415;&#20197;&#23454;&#29616;&#26368;&#20339;&#24773;&#24863;&#20998;&#26512;&#25928;&#26524;&#12290;TRABSA&#20197;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#22686;&#30410;&#65292;&#32988;&#36807;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#20102;&#19968;&#33268;&#30340;&#20248;&#36234;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;SHAP&#21644;LIME&#20998;&#26512;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22686;&#24378;&#20102;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00297v1 Announce Type: new  Abstract: Sentiment analysis is crucial for understanding public opinion and consumer behavior. Existing models face challenges with linguistic diversity, generalizability, and explainability. We propose TRABSA, a hybrid framework integrating transformer-based architectures, attention mechanisms, and BiLSTM networks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge gaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy. Augmenting datasets with tweets from 32 countries and US states, we compare six word-embedding techniques and three lexicon-based labeling techniques, selecting the best for optimal sentiment analysis. TRABSA outperforms traditional ML and deep learning models with 94% accuracy and significant precision, recall, and F1-score gains. Evaluation across diverse datasets demonstrates consistent superiority and generalizability. SHAP and LIME analyses enhance interpretability, improving confidence i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#36319;&#36394;&#31574;&#30053;&#21644;&#33258;&#21160;&#26631;&#35760;&#31639;&#27861;&#65292;&#38024;&#23545;&#20107;&#20214;&#30456;&#26426;&#20013;&#30340;&#38590;&#20197;&#36776;&#35782;&#30340;&#23545;&#35937;&#65292;&#25581;&#31034;&#20854;&#29305;&#24449;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.18330</link><description>&lt;p&gt;
&#20351;&#29992;&#36319;&#36394;&#36741;&#21161;&#30340;&#20107;&#20214;&#30456;&#26426;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Tracking-Assisted Object Detection with Event Cameras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#36319;&#36394;&#31574;&#30053;&#21644;&#33258;&#21160;&#26631;&#35760;&#31639;&#27861;&#65292;&#38024;&#23545;&#20107;&#20214;&#30456;&#26426;&#20013;&#30340;&#38590;&#20197;&#36776;&#35782;&#30340;&#23545;&#35937;&#65292;&#25581;&#31034;&#20854;&#29305;&#24449;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20107;&#20214;&#30456;&#26426;&#20855;&#26377;&#39640;&#21160;&#24577;&#33539;&#22260;&#21644;&#26080;&#21160;&#24577;&#27169;&#31946;&#31561;&#29305;&#27530;&#23646;&#24615;&#65292;&#20107;&#20214;&#39537;&#21160;&#30446;&#26631;&#26816;&#27979;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29305;&#24449;&#30340;&#19981;&#21516;&#27493;&#24615;&#21644;&#31232;&#30095;&#24615;&#23548;&#33268;&#20102;&#30001;&#20110;&#30456;&#26426;&#19982;&#20043;&#27809;&#26377;&#30456;&#23545;&#36816;&#21160;&#32780;&#23548;&#33268;&#30340;&#30475;&#19981;&#35265;&#30340;&#23545;&#35937;&#65292;&#36825;&#23545;&#20219;&#21153;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#30475;&#19981;&#35265;&#30340;&#23545;&#35937;&#35270;&#20026;&#20266;&#36974;&#25377;&#23545;&#35937;&#65292;&#24182;&#26088;&#22312;&#25581;&#31034;&#20854;&#29305;&#24449;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#35937;&#30340;&#21487;&#35265;&#24615;&#23646;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#26631;&#35760;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#29616;&#26377;&#30340;&#20107;&#20214;&#30456;&#26426;&#25968;&#25454;&#38598;&#19978;&#38468;&#21152;&#39069;&#22806;&#30340;&#21487;&#35265;&#24615;&#26631;&#31614;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#36319;&#36394;&#31574;&#30053;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18330v1 Announce Type: cross  Abstract: Event-based object detection has recently garnered attention in the computer vision community due to the exceptional properties of event cameras, such as high dynamic range and no motion blur. However, feature asynchronism and sparsity cause invisible objects due to no relative motion to the camera, posing a significant challenge in the task. Prior works have studied various memory mechanisms to preserve as many features as possible at the current time, guided by temporal clues. While these implicit-learned memories retain some short-term information, they still struggle to preserve long-term features effectively. In this paper, we consider those invisible objects as pseudo-occluded objects and aim to reveal their features. Firstly, we introduce visibility attribute of objects and contribute an auto-labeling algorithm to append additional visibility labels on an existing event camera dataset. Secondly, we exploit tracking strategies fo
&lt;/p&gt;</description></item><item><title>NGG&#26159;&#19968;&#20010;&#31070;&#32463;&#22270;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#29305;&#24449;&#26465;&#20214;&#22270;&#29983;&#25104;&#65292;&#20855;&#26377;&#27169;&#25311;&#22797;&#26434;&#22270;&#27169;&#24335;&#21644;&#25511;&#21046;&#22270;&#29983;&#25104;&#36807;&#31243;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01535</link><description>&lt;p&gt;
&#31070;&#32463;&#22270;&#29983;&#25104;&#22120;&#65306;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#29305;&#24449;&#26465;&#20214;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neural Graph Generator: Feature-Conditioned Graph Generation using Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01535
&lt;/p&gt;
&lt;p&gt;
NGG&#26159;&#19968;&#20010;&#31070;&#32463;&#22270;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#29305;&#24449;&#26465;&#20214;&#22270;&#29983;&#25104;&#65292;&#20855;&#26377;&#27169;&#25311;&#22797;&#26434;&#22270;&#27169;&#24335;&#21644;&#25511;&#21046;&#22270;&#29983;&#25104;&#36807;&#31243;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#29983;&#25104;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#38754;&#20020;&#30528;&#29983;&#25104;&#33021;&#22815;&#20934;&#30830;&#21453;&#26144;&#29305;&#23450;&#23646;&#24615;&#30340;&#22270;&#24418;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#22270;&#29983;&#25104;&#22120;&#65288;NGG&#65289;&#36825;&#19968;&#26032;&#39062;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22270;&#29983;&#25104;&#12290;NGG&#23637;&#31034;&#20102;&#23545;&#22797;&#26434;&#22270;&#27169;&#24335;&#36827;&#34892;&#24314;&#27169;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#23545;&#22270;&#29983;&#25104;&#36807;&#31243;&#30340;&#25511;&#21046;&#12290;NGG&#21033;&#29992;&#21464;&#20998;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#22270;&#21387;&#32553;&#65292;&#21033;&#29992;&#22312;&#28508;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#30001;&#24635;&#32467;&#22270;&#32479;&#35745;&#20449;&#24687;&#30340;&#21521;&#37327;&#25351;&#23548;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;NGG&#22312;&#21508;&#31181;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#36890;&#29992;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#25429;&#25417;&#26399;&#26395;&#22270;&#23646;&#24615;&#24182;&#25512;&#24191;&#21040;&#26410;&#35265;&#22270;&#24418;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01535v1 Announce Type: new  Abstract: Graph generation has emerged as a crucial task in machine learning, with significant challenges in generating graphs that accurately reflect specific properties. Existing methods often fall short in efficiently addressing this need as they struggle with the high-dimensional complexity and varied nature of graph properties. In this paper, we introduce the Neural Graph Generator (NGG), a novel approach which utilizes conditioned latent diffusion models for graph generation. NGG demonstrates a remarkable capacity to model complex graph patterns, offering control over the graph generation process. NGG employs a variational graph autoencoder for graph compression and a diffusion process in the latent vector space, guided by vectors summarizing graph statistics. We demonstrate NGG's versatility across various graph generation tasks, showing its capability to capture desired graph properties and generalize to unseen graphs. This work signifies 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;&#20934;&#30830;&#21448;&#26131;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24230;&#19978;&#19982;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#23454;&#38469;&#19978;&#36229;&#36807;&#20102;&#21442;&#32771;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05680</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#21487;&#35299;&#37322;&#24615;&#34920;&#26684;&#25968;&#25454;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Interpretable classifiers for tabular data via discretization and feature selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05680
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;&#20934;&#30830;&#21448;&#26131;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24230;&#19978;&#19982;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#23454;&#38469;&#19978;&#36229;&#36807;&#20102;&#21442;&#32771;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;&#25152;&#24471;&#21040;&#30340;&#20998;&#31867;&#22120;&#26159;&#31616;&#30701;&#30340;DNF&#20844;&#24335;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;&#25968;&#25454;&#31163;&#25955;&#21270;&#20026;&#24067;&#23572;&#24418;&#24335;&#65292;&#28982;&#21518;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#32467;&#21512;&#38750;&#24120;&#24555;&#36895;&#30340;&#31639;&#27861;&#26469;&#20135;&#29983;&#26368;&#20339;&#30340;&#24067;&#23572;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;14&#20010;&#23454;&#39564;&#26469;&#28436;&#31034;&#35813;&#26041;&#27861;&#65292;&#24471;&#21040;&#30340;&#32467;&#26524;&#30340;&#20934;&#30830;&#24230;&#20027;&#35201;&#19982;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#20197;&#21450;&#25991;&#29486;&#20013;&#30456;&#21516;&#25968;&#25454;&#38598;&#30340;&#29616;&#26377;&#32467;&#26524;&#30456;&#20284;&#12290;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#38469;&#19978;&#22312;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#21442;&#32771;&#32467;&#26524;&#65292;&#23613;&#31649;&#25105;&#20204;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#30340;&#21363;&#26102;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#20010;&#20851;&#20110;&#20174;&#29616;&#23454;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#20998;&#31867;&#22120;&#19982;&#26469;&#33258;&#25968;&#25454;&#32972;&#26223;&#20998;&#24067;&#30340;&#26368;&#20339;&#20998;&#31867;&#22120;&#30456;&#23545;&#24212;&#30340;&#27010;&#29575;&#30340;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method for computing immediately human interpretable yet accurate classifiers from tabular data. The classifiers obtained are short DNF-formulas, computed via first discretizing the original data to Boolean form and then using feature selection coupled with a very fast algorithm for producing the best possible Boolean classifier for the setting. We demonstrate the approach via 14 experiments, obtaining results with accuracies mainly similar to ones obtained via random forests, XGBoost, and existing results for the same datasets in the literature. In several cases, our approach in fact outperforms the reference results in relation to accuracy, even though the main objective of our study is the immediate interpretability of our classifiers. We also prove a new result on the probability that the classifier we obtain from real-life data corresponds to the ideally best classifier with respect to the background distribution the data comes from.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#30446;&#30340;&#26159;&#35774;&#35745;&#19968;&#31181;&#36873;&#25321;&#26426;&#21046;&#26469;&#24179;&#34913;&#34987;&#25298;&#32477;&#30340;&#39044;&#27979;&#27604;&#20363;&#21644;&#25152;&#36873;&#39044;&#27979;&#30340;&#39044;&#27979;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.12708</link><description>&lt;p&gt;
&#29992;&#20110;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Network Benchmarks for Selective Classification. (arXiv:2401.12708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#30446;&#30340;&#26159;&#35774;&#35745;&#19968;&#31181;&#36873;&#25321;&#26426;&#21046;&#26469;&#24179;&#34913;&#34987;&#25298;&#32477;&#30340;&#39044;&#27979;&#27604;&#20363;&#21644;&#25152;&#36873;&#39044;&#27979;&#30340;&#39044;&#27979;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#20855;&#26377;&#31038;&#20250;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#20013;&#30340;&#37096;&#32626;&#22686;&#21152;&#65292;&#23545;&#21487;&#38752;&#21644;&#21487;&#20449;&#39044;&#27979;&#30340;&#38656;&#27714;&#20063;&#26085;&#30410;&#22686;&#38271;&#12290;&#23454;&#29616;&#36825;&#20123;&#35201;&#27714;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20801;&#35768;&#27169;&#22411;&#22312;&#23384;&#22312;&#39640;&#38169;&#35823;&#39118;&#38505;&#26102;&#25918;&#24323;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#38656;&#35201;&#20026;&#27169;&#22411;&#28155;&#21152;&#36873;&#25321;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#36873;&#25321;&#27169;&#22411;&#23558;&#25552;&#20379;&#39044;&#27979;&#30340;&#20363;&#23376;&#12290;&#36873;&#25321;&#24615;&#20998;&#31867;&#26694;&#26550;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#24179;&#34913;&#34987;&#25298;&#32477;&#39044;&#27979;&#27604;&#20363;&#65288;&#21363;&#27169;&#22411;&#19981;&#36827;&#34892;&#39044;&#27979;&#30340;&#20363;&#23376;&#27604;&#20363;&#65289;&#19982;&#22312;&#25152;&#36873;&#39044;&#27979;&#19978;&#30340;&#39044;&#27979;&#24615;&#33021;&#25913;&#36827;&#20043;&#38388;&#30340;&#26426;&#21046;&#12290;&#23384;&#22312;&#22810;&#20010;&#36873;&#25321;&#24615;&#20998;&#31867;&#26694;&#26550;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20381;&#36182;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#23454;&#35777;&#35780;&#20272;&#20173;&#23616;&#38480;&#20110;&#37096;&#20998;&#26041;&#27861;&#21644;&#35774;&#32622;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#32473;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#24456;&#23569;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing deployment of machine learning models in many socially-sensitive tasks, there is a growing demand for reliable and trustworthy predictions. One way to accomplish these requirements is to allow a model to abstain from making a prediction when there is a high risk of making an error. This requires adding a selection mechanism to the model, which selects those examples for which the model will provide a prediction. The selective classification framework aims to design a mechanism that balances the fraction of rejected predictions (i.e., the proportion of examples for which the model does not make a prediction) versus the improvement in predictive performance on the selected predictions. Multiple selective classification frameworks exist, most of which rely on deep neural network architectures. However, the empirical evaluation of the existing approaches is still limited to partial comparisons among methods and settings, providing practitioners with little insight into 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#35748;&#30693;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#20027;&#35201;&#20851;&#27880;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;(AD)&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#36890;&#36807;&#20998;&#26512;&#20070;&#20889;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26816;&#27979;&#20986;&#32454;&#24494;&#30340;&#21464;&#21270;&#65292;&#36825;&#23545;&#20110;&#26089;&#26399;AD&#30340;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.06697</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#35748;&#30693;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65306;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning in the Cognitive Domain: Alzheimer's Disease Study. (arXiv:2401.06697v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#35748;&#30693;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#20027;&#35201;&#20851;&#27880;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;(AD)&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#36890;&#36807;&#20998;&#26512;&#20070;&#20889;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26816;&#27979;&#20986;&#32454;&#24494;&#30340;&#21464;&#21270;&#65292;&#36825;&#23545;&#20110;&#26089;&#26399;AD&#30340;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;(AD)&#26159;&#26368;&#24120;&#35265;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#33041;&#30142;&#30149;&#65292;&#23548;&#33268;&#32769;&#24180;&#20154;&#20013;&#26174;&#33879;&#30340;&#35748;&#30693;&#21151;&#33021;&#25439;&#23475;&#12290;&#35748;&#30693;&#21151;&#33021;&#25439;&#23475;&#21487;&#20197;&#34920;&#29616;&#20026;&#21508;&#31181;&#24515;&#29702;&#33021;&#21147;&#30340;&#19979;&#38477;&#65292;&#22914;&#27880;&#24847;&#21147;&#12289;&#35760;&#24518;&#21644;&#20854;&#20182;&#39640;&#32423;&#35748;&#30693;&#33021;&#21147;&#12290;&#36825;&#20123;&#32570;&#38519;&#20250;&#20005;&#37325;&#24433;&#21709;&#20010;&#20307;&#29702;&#35299;&#20449;&#24687;&#12289;&#33719;&#21462;&#26032;&#30693;&#35782;&#21644;&#26377;&#25928;&#27807;&#36890;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#21463;&#21040;&#35748;&#30693;&#21151;&#33021;&#25439;&#23475;&#24433;&#21709;&#30340;&#27963;&#21160;&#20043;&#19968;&#26159;&#20070;&#20889;&#12290;&#36890;&#36807;&#20998;&#26512;&#20070;&#20889;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#21253;&#25324;&#21387;&#21147;&#12289;&#36895;&#24230;&#21644;&#31354;&#38388;&#32452;&#32455;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26816;&#27979;&#21040;&#32454;&#24494;&#30340;&#21464;&#21270;&#65292;&#21487;&#33021;&#34920;&#26126;&#26089;&#26399;&#30340;&#35748;&#30693;&#21151;&#33021;&#25439;&#23475;&#65292;&#29305;&#21035;&#26159;AD&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#32463;&#20856;&#30340;&#20154;&#24037;&#26234;&#33021;(AI)&#26041;&#27861;&#65292;&#36890;&#36807;&#20070;&#20889;&#20998;&#26512;&#26469;&#26816;&#27979;&#32769;&#24180;&#20154;&#20013;&#30340;AD&#12290;&#28982;&#32780;&#65292;&#20808;&#36827;&#30340;AI&#26041;&#27861;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#22240;&#20026;&#25968;&#25454;&#30340;&#35268;&#27169;&#22686;&#21152;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease (AD) is the most prevalent neurodegenerative brain disorder, which results in significant cognitive impairments, especially in the elderly population. Cognitive impairments can manifest as a decline in various mental faculties, such as concentration, memory, and other higher-order cognitive abilities. These deficits can significantly impact an individual's capacity to comprehend information, acquire new knowledge, and communicate effectively. One of the affected activities due to cognitive impairments is handwriting. By analyzing different aspects of handwriting, including pressure, velocity, and spatial organization, researchers can detect subtle alterations that might indicate early-stage cognitive impairments, especially AD. Recently, several classical artificial intelligence (AI) approaches have been proposed for detecting AD in elderly individuals through handwriting analysis. However, advanced AI methods require more computational power as the size of the data
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Hyper-STTN&#65292;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#65292;&#29992;&#20110;&#20154;&#32676;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#36229;&#22270;&#26469;&#25429;&#25417;&#25317;&#25380;&#22330;&#26223;&#20013;&#30340;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#36716;&#25442;&#22120;&#26469;&#25429;&#25417;&#34892;&#20154;&#30340;&#25104;&#23545;&#28508;&#22312;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20123;&#24322;&#26500;&#30340;&#32676;&#20307;&#38388;&#21644;&#25104;&#23545;&#38388;&#30456;&#20114;&#20316;&#29992;&#36890;&#36807;&#19968;&#20010;&#22810;&#27169;&#24577;&#36716;&#25442;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#21644;&#23545;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.06344</link><description>&lt;p&gt;
&#36229;&#32423;-STTN&#65306;&#31038;&#20132;&#32676;&#20307;&#24863;&#30693;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#29992;&#20110;&#20154;&#20307;&#36712;&#36857;&#39044;&#27979;&#19982;&#36229;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Hyper-STTN: Social Group-aware Spatial-Temporal Transformer Network for Human Trajectory Prediction with Hypergraph Reasoning. (arXiv:2401.06344v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Hyper-STTN&#65292;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#65292;&#29992;&#20110;&#20154;&#32676;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#36229;&#22270;&#26469;&#25429;&#25417;&#25317;&#25380;&#22330;&#26223;&#20013;&#30340;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#36716;&#25442;&#22120;&#26469;&#25429;&#25417;&#34892;&#20154;&#30340;&#25104;&#23545;&#28508;&#22312;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20123;&#24322;&#26500;&#30340;&#32676;&#20307;&#38388;&#21644;&#25104;&#23545;&#38388;&#30456;&#20114;&#20316;&#29992;&#36890;&#36807;&#19968;&#20010;&#22810;&#27169;&#24577;&#36716;&#25442;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#21644;&#23545;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#21253;&#25324;&#26381;&#21153;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65292;&#39044;&#27979;&#25317;&#25380;&#30340;&#24847;&#22270;&#21644;&#36712;&#36857;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#29702;&#35299;&#29615;&#22659;&#21160;&#24577;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#19981;&#20165;&#22240;&#20026;&#23545;&#24314;&#27169;&#25104;&#23545;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#24615;&#65292;&#36824;&#22240;&#20026;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#30721;&#25317;&#25380;&#22330;&#26223;&#20013;&#20840;&#38754;&#30340;&#25104;&#23545;&#21644;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Hyper-STTN&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#65292;&#29992;&#20110;&#20154;&#32676;&#36712;&#36857;&#39044;&#27979;&#12290;&#22312;Hyper-STTN&#20013;&#65292;&#36890;&#36807;&#19968;&#32452;&#22810;&#23610;&#24230;&#36229;&#22270;&#26500;&#24314;&#20102;&#25317;&#25380;&#30340;&#32676;&#20307;&#38388;&#30456;&#20851;&#24615;&#65292;&#36825;&#20123;&#36229;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#32676;&#20307;&#22823;&#23567;&#65292;&#36890;&#36807;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#27010;&#29575;&#30340;&#36229;&#22270;&#35889;&#21367;&#31215;&#36827;&#34892;&#25429;&#25417;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#20102;&#31354;&#38388;-&#26102;&#38388;&#36716;&#25442;&#22120;&#26469;&#25429;&#25417;&#34892;&#20154;&#22312;&#31354;&#38388;-&#26102;&#38388;&#32500;&#24230;&#19978;&#30340;&#23545;&#29031;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#24322;&#26500;&#30340;&#32676;&#20307;&#38388;&#21644;&#25104;&#23545;&#38388;&#30456;&#20114;&#20316;&#29992;&#36890;&#36807;&#19968;&#20010;&#22810;&#27169;&#24577;&#36716;&#25442;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#21644;&#23545;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting crowded intents and trajectories is crucial in varouls real-world applications, including service robots and autonomous vehicles. Understanding environmental dynamics is challenging, not only due to the complexities of modeling pair-wise spatial and temporal interactions but also the diverse influence of group-wise interactions. To decode the comprehensive pair-wise and group-wise interactions in crowded scenarios, we introduce Hyper-STTN, a Hypergraph-based Spatial-Temporal Transformer Network for crowd trajectory prediction. In Hyper-STTN, crowded group-wise correlations are constructed using a set of multi-scale hypergraphs with varying group sizes, captured through random-walk robability-based hypergraph spectral convolution. Additionally, a spatial-temporal transformer is adapted to capture pedestrians' pair-wise latent interactions in spatial-temporal dimensions. These heterogeneous group-wise and pair-wise are then fused and aligned though a multimodal transformer net
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#21644;FSO&#36827;&#34892;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#21644;&#26377;&#25928;&#30340;&#26435;&#37325;&#37325;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.11973</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;: &#38754;&#21521;&#35270;&#39057;&#34920;&#31034;&#30340;&#20813;&#36951;&#24536;&#20248;&#32988;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#21644;FSO&#36827;&#34892;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#21644;&#26377;&#25928;&#30340;&#26435;&#37325;&#37325;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65288;LTH&#65289;&#30340;&#21551;&#21457;&#65292;&#35813;&#20551;&#35774;&#24378;&#35843;&#22312;&#36739;&#22823;&#30340;&#23494;&#38598;&#32593;&#32476;&#20013;&#23384;&#22312;&#39640;&#25928;&#23376;&#32593;&#32476;&#65292;&#30740;&#31350;&#20102;&#22312;&#36866;&#24403;&#30340;&#31232;&#30095;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#31168;&#30340;&#20248;&#32988;&#23376;&#32593;&#32476;&#65288;WSN&#65289;&#22312;&#21508;&#31181;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#21033;&#29992;&#26469;&#33258;&#23494;&#38598;&#32593;&#32476;&#30340;&#39044;&#20808;&#23384;&#22312;&#30340;&#26435;&#37325;&#65292;&#22312;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#65288;TIL&#65289;&#22330;&#26223;&#20013;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;&#22312;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;&#36719;&#23376;&#32593;&#32476;&#65288;SoftNet&#65289;&#30340;WSN&#21464;&#20307;&#65292;&#20197;&#38450;&#27490;&#25968;&#25454;&#26679;&#26412;&#31232;&#32570;&#26102;&#30340;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#20102;WSN&#26435;&#37325;&#30340;&#31232;&#30095;&#37325;&#29992;&#65292;&#29992;&#20110;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65288;VIL&#65289;&#12290;&#32771;&#34385;&#20102;&#22312;WSN&#20013;&#20351;&#29992;&#20613;&#31435;&#21494;&#23376;&#31070;&#32463;&#36816;&#31639;&#22120;&#65288;FSO&#65289;&#65292;&#23427;&#33021;&#22815;&#23545;&#35270;&#39057;&#36827;&#34892;&#32039;&#20945;&#32534;&#30721;&#65292;&#24182;&#22312;&#19981;&#21516;&#24102;&#23485;&#19979;&#35782;&#21035;&#21487;&#37325;&#29992;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;FSO&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#36830;&#32493;&#23398;&#20064;&#26550;&#26500;&#20013;&#65292;&#21253;&#25324;VIL&#12289;TIL&#21644;FSCIL&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the existence of efficient subnetworks within larger, dense networks, a high-performing Winning Subnetwork (WSN) in terms of task performance under appropriate sparsity conditions is considered for various continual learning tasks. It leverages pre-existing weights from dense networks to achieve efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is designed to prevent overfitting when the data samples are scarce. Furthermore, the sparse reuse of WSN weights is considered for Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It enables compact encoding of videos and identifies reusable subnetworks across varying bandwidths. We have integrated FSO into different architectural frameworks for continual learning, including VIL, TIL, and FSCIL. Our c
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#35774;&#35745;&#26368;&#20248;&#30340;&#21487;&#35266;&#27979;&#37327;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#23637;&#24320;&#30340;&#24494;&#20998;&#25130;&#38754;&#21253;&#21547;&#20102;&#20851;&#20110;&#24863;&#20852;&#36259;&#21442;&#25968;&#30340;&#26368;&#22810;&#20449;&#24687;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#26500;&#36896;&#26041;&#27861;&#36827;&#34892;&#24456;&#22909;&#30340;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.08717</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#20026;&#27979;&#37327;&#35774;&#35745;&#21487;&#35266;&#27979;&#37327;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Designing Observables for Measurements with Deep Learning. (arXiv:2310.08717v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08717
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#35774;&#35745;&#26368;&#20248;&#30340;&#21487;&#35266;&#27979;&#37327;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#23637;&#24320;&#30340;&#24494;&#20998;&#25130;&#38754;&#21253;&#21547;&#20102;&#20851;&#20110;&#24863;&#20852;&#36259;&#21442;&#25968;&#30340;&#26368;&#22810;&#20449;&#24687;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#26500;&#36896;&#26041;&#27861;&#36827;&#34892;&#24456;&#22909;&#30340;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31890;&#23376;&#29289;&#29702;&#21644;&#26680;&#29289;&#29702;&#30340;&#35768;&#22810;&#20998;&#26512;&#20013;&#65292;&#20351;&#29992;&#27169;&#25311;&#26469;&#25512;&#26029;&#24213;&#23618;&#29289;&#29702;&#27169;&#22411;&#30340;&#22522;&#26412;&#12289;&#26377;&#25928;&#25110;&#29616;&#35937;&#23398;&#21442;&#25968;&#12290;&#24403;&#20351;&#29992;&#23637;&#24320;&#30340;&#25130;&#38754;&#36827;&#34892;&#25512;&#26029;&#26102;&#65292;&#21487;&#35266;&#27979;&#37327;&#26159;&#36890;&#36807;&#29289;&#29702;&#30452;&#35273;&#21644;&#32463;&#39564;&#27861;&#21017;&#36827;&#34892;&#35774;&#35745;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#35774;&#35745;&#26368;&#20248;&#30340;&#21487;&#35266;&#27979;&#37327;&#12290;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#23637;&#24320;&#30340;&#24494;&#20998;&#25130;&#38754;&#21253;&#21547;&#20102;&#20851;&#20110;&#24863;&#20852;&#36259;&#21442;&#25968;&#30340;&#26368;&#22810;&#20449;&#24687;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#26500;&#36896;&#26041;&#27861;&#36827;&#34892;&#24456;&#22909;&#30340;&#27979;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#29289;&#29702;&#27169;&#22411;&#26469;&#28436;&#31034;&#36825;&#20010;&#24819;&#27861;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#29992;&#20110;&#28145;&#24230;&#26080;&#24377;&#24615;&#25955;&#23556;&#20013;&#30340;&#21253;&#21547;&#24615;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many analyses in particle and nuclear physics use simulations to infer fundamental, effective, or phenomenological parameters of the underlying physics models. When the inference is performed with unfolded cross sections, the observables are designed using physics intuition and heuristics. We propose to design optimal observables with machine learning. Unfolded, differential cross sections in a neural network output contain the most information about parameters of interest and can be well-measured by construction. We demonstrate this idea using two physics models for inclusive measurements in deep inelastic scattering.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35760;&#24518;&#20581;&#36523;&#25151;&#65292;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#21033;&#29992;&#35760;&#24518;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20108;&#32500;&#29615;&#22659;&#21644;&#31163;&#25955;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#26080;&#23613;&#20219;&#21153;&#23545;&#35760;&#24518;&#33021;&#21147;&#12289;&#22122;&#22768;&#25239;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;Transformer-XL&#21644;Proximal Policy Optimization&#39537;&#21160;&#30340;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.17207</link><description>&lt;p&gt;
&#35760;&#24518;&#20581;&#36523;&#25151;&#65306;&#23545;&#20869;&#23384;&#20026;&#22522;&#30784;&#30340;&#26234;&#33021;&#20307;&#22312;&#26080;&#23613;&#20219;&#21153;&#20013;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Memory Gym: Partially Observable Challenges to Memory-Based Agents in Endless Episodes. (arXiv:2309.17207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35760;&#24518;&#20581;&#36523;&#25151;&#65292;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#21033;&#29992;&#35760;&#24518;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20108;&#32500;&#29615;&#22659;&#21644;&#31163;&#25955;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#26080;&#23613;&#20219;&#21153;&#23545;&#35760;&#24518;&#33021;&#21147;&#12289;&#22122;&#22768;&#25239;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;Transformer-XL&#21644;Proximal Policy Optimization&#39537;&#21160;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#20581;&#36523;&#25151;&#20171;&#32461;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#27979;&#35797;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#29305;&#21035;&#26159;&#23558;&#38376;&#24490;&#29615;&#21333;&#20803;(GRU)&#19982;Transformer-XL(TrXL)&#30456;&#27604;&#65292;&#23427;&#20204;&#23545;&#20110;&#35760;&#24518;&#38271;&#24207;&#21015;&#30340;&#33021;&#21147;&#12289;&#25239;&#22122;&#22768;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#37319;&#29992;&#20102;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20108;&#32500;&#29615;&#22659;&#21644;&#31163;&#25955;&#25511;&#21046;&#65292;&#21363;Mortar Mayhem&#12289;Mystery Path&#21644;Searing Spotlights&#12290;&#36825;&#20123;&#26368;&#21021;&#26159;&#26377;&#38480;&#30340;&#29615;&#22659;&#34987;&#25512;&#24191;&#20026;&#26032;&#39062;&#30340;&#26080;&#23613;&#20219;&#21153;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#21160;&#35838;&#31243;&#65292;&#20174;&#36710;&#28216;&#25103;"I packed my bag"&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#36825;&#20123;&#26080;&#23613;&#20219;&#21153;&#19981;&#20165;&#26377;&#21161;&#20110;&#35780;&#20272;&#25928;&#29575;&#65292;&#32780;&#19988;&#26377;&#36259;&#22320;&#35780;&#20272;&#20102;&#35760;&#24518;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#37492;&#20110;&#29616;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#35760;&#24518;&#22522;&#20934;&#30340;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#30001;TrXL&#21644;Proximal Policy Optimization&#39537;&#21160;&#30340;&#23454;&#29616;&#12290;&#26412;&#23454;&#29616;&#21033;&#29992;TrXL&#20316;&#20026;&#20197;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#20351;&#29992;&#30340;&#24773;&#33410;&#24615;&#35760;&#24518;&#12290;&#22312;&#26377;&#38480;&#29615;&#22659;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Memory Gym introduces a unique benchmark designed to test Deep Reinforcement Learning agents, specifically comparing Gated Recurrent Unit (GRU) against Transformer-XL (TrXL), on their ability to memorize long sequences, withstand noise, and generalize. It features partially observable 2D environments with discrete controls, namely Mortar Mayhem, Mystery Path, and Searing Spotlights. These originally finite environments are extrapolated to novel endless tasks that act as an automatic curriculum, drawing inspiration from the car game ``I packed my bag". These endless tasks are not only beneficial for evaluating efficiency but also intriguingly valuable for assessing the effectiveness of approaches in memory-based agents. Given the scarcity of publicly available memory baselines, we contribute an implementation driven by TrXL and Proximal Policy Optimization. This implementation leverages TrXL as episodic memory using a sliding window approach. In our experiments on the finite environment
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25915;&#20987;&#23545;&#26368;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#39640;&#36798;90&#65285;&#20197;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;&#35813;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#22522;&#30784;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2309.00236</link><description>&lt;p&gt;
&#22270;&#20687;&#21163;&#25345;&#65306;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Image Hijacking: Adversarial Images can Control Generative Models at Runtime. (arXiv:2309.00236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25915;&#20987;&#23545;&#26368;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#39640;&#36798;90&#65285;&#20197;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;&#35813;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#22522;&#30784;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20813;&#21463;&#24694;&#24847;&#34892;&#20026;&#32773;&#30340;&#25915;&#20987;&#65311;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#22270;&#20687;&#36755;&#20837;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#22270;&#20687;&#21163;&#25345;&#65292;&#21363;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34892;&#20026;&#21305;&#37197;&#8221;&#30340;&#36890;&#29992;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#65292;&#24182;&#29992;&#23427;&#26469;&#25506;&#32034;&#19977;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#20855;&#20307;&#23383;&#31526;&#20018;&#25915;&#20987;&#21487;&#20197;&#29983;&#25104;&#20219;&#24847;&#34987;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#36755;&#20986;&#65307;&#27844;&#38706;&#19978;&#19979;&#25991;&#25915;&#20987;&#21487;&#20197;&#23558;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#30340;&#20449;&#24687;&#27844;&#38706;&#21040;&#36755;&#20986;&#20013;&#65307;&#36234;&#29425;&#25915;&#20987;&#21487;&#20197;&#32469;&#36807;&#27169;&#22411;&#30340;&#23433;&#20840;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;CLIP&#21644;LLaMA-2&#30340;&#26368;&#26032;VLM&#27169;&#22411;LLaVA-2&#36827;&#34892;&#20102;&#36825;&#20123;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#25152;&#26377;&#30340;&#25915;&#20987;&#31867;&#22411;&#25104;&#21151;&#29575;&#22343;&#22312;90&#65285;&#20197;&#19978;&#12290;&#32780;&#19988;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#21482;&#38656;&#35201;&#23545;&#22270;&#20687;&#36827;&#34892;&#23567;&#30340;&#25200;&#21160;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#25552;&#20986;&#20102;&#20005;&#37325;&#30340;&#25285;&#24551;&#12290;&#22914;&#26524;&#22270;&#20687;&#21163;&#25345;&#19982;CIFAR-10&#20013;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#19968;&#26679;&#38590;&#20197;&#38450;&#24481;&#65292;&#37027;&#20040;&#21487;&#33021;&#38656;&#35201;&#24456;&#22810;&#24180;&#25165;&#33021;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are foundation models secure from malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control generative models at runtime. We introduce Behavior Matching, a general method for creating image hijacks, and we use it to explore three types of attacks. Specific string attacks generate arbitrary output of the adversary's choosing. Leak context attacks leak information from the context window into the output. Jailbreak attacks circumvent a model's safety training. We study these attacks against LLaVA-2, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all our attack types have above a 90\% success rate. Moreover, our attacks are automated and require only small image perturbations. These findings raise serious concerns about the security of foundation models. If image hijacks are as difficult to defend against as adversarial examples in CIFAR-10, then it might be many years before a s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#28369;&#36807;&#24230;&#21644;&#29305;&#24449;&#20851;&#32852;&#36807;&#39640;&#29616;&#35937;&#65292;&#21457;&#29616;&#22266;&#23450;&#19981;&#21464;&#30340;&#23376;&#31354;&#38388;&#23548;&#33268;&#20102;&#33410;&#28857;&#34920;&#31034;&#30340;&#31561;&#32423;&#23849;&#22604;&#12290;&#22312;&#35813;&#23376;&#31354;&#38388;&#20013;&#24179;&#28369;&#21521;&#37327;&#30340;&#23384;&#22312;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#20351;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#20063;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#32599;&#20869;&#20811;&#31215;&#20043;&#21644;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.16800</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31561;&#32423;&#23849;&#22604;&#23548;&#33268;&#24179;&#28369;&#36807;&#24230;&#21644;&#20851;&#32852;&#36807;&#39640;
&lt;/p&gt;
&lt;p&gt;
Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural Networks. (arXiv:2308.16800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#28369;&#36807;&#24230;&#21644;&#29305;&#24449;&#20851;&#32852;&#36807;&#39640;&#29616;&#35937;&#65292;&#21457;&#29616;&#22266;&#23450;&#19981;&#21464;&#30340;&#23376;&#31354;&#38388;&#23548;&#33268;&#20102;&#33410;&#28857;&#34920;&#31034;&#30340;&#31561;&#32423;&#23849;&#22604;&#12290;&#22312;&#35813;&#23376;&#31354;&#38388;&#20013;&#24179;&#28369;&#21521;&#37327;&#30340;&#23384;&#22312;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#20351;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#20063;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#32599;&#20869;&#20811;&#31215;&#20043;&#21644;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24179;&#28369;&#36807;&#24230;&#21644;&#29305;&#24449;&#20851;&#32852;&#36807;&#39640;&#30340;&#26032;&#29702;&#35770;&#35265;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22266;&#23450;&#19981;&#21464;&#23376;&#31354;&#38388;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#23427;&#34920;&#29616;&#20986;&#19968;&#31181;&#30456;&#23545;&#30340;&#34892;&#20026;&#65292;&#19981;&#21463;&#29305;&#24449;&#36716;&#25442;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38416;&#26126;&#20102;&#19982;&#25910;&#25947;&#21040;&#24120;&#25968;&#29366;&#24577;&#21644;&#33410;&#28857;&#29366;&#24577;&#30340;&#36807;&#20998;&#20998;&#31163;&#30456;&#20851;&#30340;&#26368;&#26032;&#35266;&#23519;&#32467;&#26524;&#65292;&#22240;&#20026;&#23376;&#31354;&#38388;&#30340;&#25918;&#22823;&#21482;&#21462;&#20915;&#20110;&#32858;&#21512;&#20989;&#25968;&#30340;&#39057;&#35889;&#12290;&#22312;&#32447;&#24615;&#22330;&#26223;&#20013;&#65292;&#36825;&#23548;&#33268;&#33410;&#28857;&#34920;&#31034;&#30001;&#20302;&#32500;&#23376;&#31354;&#38388;&#20027;&#23548;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#29305;&#24449;&#36716;&#25442;&#26080;&#20851;&#30340;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#12290;&#24403;&#24179;&#28369;&#21521;&#37327;&#36328;&#36234;&#36825;&#20010;&#23376;&#31354;&#38388;&#26102;&#65292;&#36825;&#20250;&#23548;&#33268;&#33410;&#28857;&#34920;&#31034;&#30340;&#31561;&#32423;&#23849;&#22604;&#65292;&#20174;&#32780;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#20351;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#20063;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#20851;&#32852;&#12290;&#22312;&#25105;&#20204;&#30340;&#29702;&#35770;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#32599;&#20869;&#20811;&#31215;&#20043;&#21644;&#20316;&#20026;&#19968;&#31181;&#26377;&#30410;&#29305;&#24615;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#38450;&#27490;&#36807;&#24230;&#24179;&#28369;&#12289;&#36807;&#39640;&#20851;&#32852;&#21644;&#31561;&#32423;&#23849;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our study reveals new theoretical insights into over-smoothing and feature over-correlation in deep graph neural networks. We show the prevalence of invariant subspaces, demonstrating a fixed relative behavior that is unaffected by feature transformations. Our work clarifies recent observations related to convergence to a constant state and a potential over-separation of node states, as the amplification of subspaces only depends on the spectrum of the aggregation function. In linear scenarios, this leads to node representations being dominated by a low-dimensional subspace with an asymptotic convergence rate independent of the feature transformations. This causes a rank collapse of the node representations, resulting in over-smoothing when smooth vectors span this subspace, and over-correlation even when over-smoothing is avoided. Guided by our theory, we propose a sum of Kronecker products as a beneficial property that can provably prevent over-smoothing, over-correlation, and rank c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#32422;&#26463;&#26469;&#35843;&#33410;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#19979;&#24378;&#21046;&#25191;&#34892;&#20219;&#24847;&#30340;&#36923;&#36753;&#32422;&#26463;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#12289;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#30340;&#26465;&#20214;&#37319;&#26679;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.16534</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#32422;&#26463;&#26469;&#35843;&#33410;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conditioning Score-Based Generative Models by Neuro-Symbolic Constraints. (arXiv:2308.16534v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#32422;&#26463;&#26469;&#35843;&#33410;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#19979;&#24378;&#21046;&#25191;&#34892;&#20219;&#24847;&#30340;&#36923;&#36753;&#32422;&#26463;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#12289;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#30340;&#26465;&#20214;&#37319;&#26679;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#21644;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26465;&#20214;&#21644;&#38750;&#26465;&#20214;&#29983;&#25104;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26465;&#20214;&#29983;&#25104;&#22522;&#20110;&#29305;&#23450;&#35757;&#32451;&#30340;&#26465;&#20214;&#27169;&#22411;&#25110;&#20998;&#31867;&#22120;&#25351;&#23548;&#65292;&#36825;&#38656;&#35201;&#35757;&#32451;&#19968;&#20010;&#22122;&#22768;&#20381;&#36182;&#30340;&#20998;&#31867;&#22120;&#65292;&#21363;&#20351;&#23545;&#20110;&#26410;&#25439;&#22351;&#25968;&#25454;&#30340;&#20998;&#31867;&#22120;&#24050;&#32463;&#32473;&#20986;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#38750;&#26465;&#20214;&#35780;&#20998;&#29983;&#25104;&#27169;&#22411;&#20013;&#37319;&#26679;&#65292;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#20219;&#24847;&#30340;&#36923;&#36753;&#32422;&#26463;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25805;&#32437;&#23398;&#20064;&#24471;&#21040;&#30340;&#35780;&#20998;&#65292;&#20197;&#20415;&#22312;&#29992;&#25143;&#23450;&#20041;&#30340;&#32422;&#26463;&#26465;&#20214;&#19979;&#20174;&#38750;&#24402;&#19968;&#21270;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#28789;&#27963;&#32780;&#25968;&#20540;&#31283;&#23450;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#29992;&#20110;&#32534;&#30721;&#36719;&#36923;&#36753;&#32422;&#26463;&#12290;&#23558;&#36825;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#20294;&#26159;&#36817;&#20284;&#30340;&#26465;&#20214;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25913;&#36827;&#36817;&#20284;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based and diffusion models have emerged as effective approaches for both conditional and unconditional generation. Still conditional generation is based on either a specific training of a conditional model or classifier guidance, which requires training a noise-dependent classifier, even when the classifier for uncorrupted data is given. We propose an approach to sample from unconditional score-based generative models enforcing arbitrary logical constraints, without any additional training. Firstly, we show how to manipulate the learned score in order to sample from an un-normalized distribution conditional on a user-defined constraint. Then, we define a flexible and numerically stable neuro-symbolic framework for encoding soft logical constraints. Combining these two ingredients we obtain a general, but approximate, conditional sampling algorithm. We further developed effective heuristics aimed at improving the approximation. Finally, we show the effectiveness of our approach fo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#21475;&#20449;&#24687;&#19981;&#23436;&#20840;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26367;&#20195;&#25935;&#24863;&#23646;&#24615;&#30340;&#23646;&#24615;&#20998;&#31867;&#22120;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#24182;&#23545;&#25512;&#26029;&#20986;&#30340;&#19981;&#30830;&#23450;&#24615;&#26368;&#20302;&#30340;&#20154;&#21475;&#20449;&#24687;&#26679;&#26412;&#36827;&#34892;&#20844;&#24179;&#24615;&#32422;&#26463;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2307.13081</link><description>&lt;p&gt;
&#20154;&#21475;&#31232;&#32570;&#21046;&#24230;&#19979;&#30340;&#20844;&#24179;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fairness Under Demographic Scarce Regime. (arXiv:2307.13081v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13081
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#21475;&#20449;&#24687;&#19981;&#23436;&#20840;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26367;&#20195;&#25935;&#24863;&#23646;&#24615;&#30340;&#23646;&#24615;&#20998;&#31867;&#22120;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#24182;&#23545;&#25512;&#26029;&#20986;&#30340;&#19981;&#30830;&#23450;&#24615;&#26368;&#20302;&#30340;&#20154;&#21475;&#20449;&#24687;&#26679;&#26412;&#36827;&#34892;&#20844;&#24179;&#24615;&#32422;&#26463;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#30740;&#31350;&#20551;&#35774;&#27169;&#22411;&#21487;&#20197;&#23436;&#20840;&#35775;&#38382;&#20154;&#21475;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#37319;&#38598;&#26399;&#38388;&#26410;&#20445;&#30041;&#35760;&#24405;&#25110;&#20986;&#20110;&#38544;&#31169;&#21407;&#22240;&#65292;&#23384;&#22312;&#20154;&#21475;&#20449;&#24687;&#37096;&#20998;&#21487;&#29992;&#30340;&#24773;&#20917;&#12290;&#36825;&#31181;&#24773;&#20917;&#34987;&#31216;&#20026;&#20154;&#21475;&#31232;&#32570;&#21046;&#24230;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35757;&#32451;&#19968;&#20010;&#23646;&#24615;&#20998;&#31867;&#22120;&#26469;&#26367;&#20195;&#32570;&#22833;&#30340;&#25935;&#24863;&#23646;&#24615;&#65288;&#20195;&#29702;&#65289;&#20173;&#28982;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#12290;&#28982;&#32780;&#65292;&#19982;&#30495;&#23454;&#25935;&#24863;&#23646;&#24615;&#30456;&#27604;&#65292;&#20351;&#29992;&#20195;&#29702;&#25935;&#24863;&#23646;&#24615;&#20250;&#21152;&#21095;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26500;&#24314;&#23646;&#24615;&#20998;&#31867;&#22120;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23646;&#24615;&#20998;&#31867;&#22120;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#24182;&#23545;&#20855;&#26377;&#25512;&#26029;&#20986;&#30340;&#26368;&#20302;&#19981;&#30830;&#23450;&#24615;&#30340;&#20154;&#21475;&#20449;&#24687;&#30340;&#26679;&#26412;&#24378;&#21046;&#25191;&#34892;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#26679;&#26412;&#19978;&#24378;&#21046;&#25191;&#34892;&#20844;&#24179;&#32422;&#26463;&#20250;&#25439;&#23475;&#31639;&#27861;&#30340;&#24635;&#20307;&#20934;&#30830;&#24615;&#65292;&#20294;&#21487;&#20197;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing works on fairness assume the model has full access to demographic information. However, there exist scenarios where demographic information is partially available because a record was not maintained throughout data collection or due to privacy reasons. This setting is known as demographic scarce regime. Prior research have shown that training an attribute classifier to replace the missing sensitive attributes (proxy) can still improve fairness. However, the use of proxy-sensitive attributes worsens fairness-accuracy trade-offs compared to true sensitive attributes. To address this limitation, we propose a framework to build attribute classifiers that achieve better fairness-accuracy trade-offs. Our method introduces uncertainty awareness in the attribute classifier and enforces fairness on samples with demographic information inferred with the lowest uncertainty. We show empirically that enforcing fairness constraints on samples with uncertain sensitive attributes is detr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#8221;&#29616;&#35937;&#65292;&#21363;&#22312;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#26550;&#26500;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#27169;&#22411;&#24448;&#24448;&#20250;&#23558;&#25152;&#26377;&#39044;&#27979;&#25351;&#21521;&#21516;&#19968;&#20010;&#31867;&#21035;&#12290;&#35813;&#29616;&#35937;&#23545;&#26550;&#26500;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#26377;&#23454;&#38469;&#25351;&#23548;&#24847;&#20041;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#21518;&#26524;&#65292;&#20363;&#22914;&#33410;&#28857;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23849;&#28291;&#21644;&#28145;&#24230;&#24102;&#26469;&#30340;&#38750;&#24179;&#20961;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.00809</link><description>&lt;p&gt;
&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#65306;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20542;&#21521;&#20110;&#26576;&#20123;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Initial Guessing Bias: How Untrained Networks Favor Some Classes. (arXiv:2306.00809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#8221;&#29616;&#35937;&#65292;&#21363;&#22312;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#26550;&#26500;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#27169;&#22411;&#24448;&#24448;&#20250;&#23558;&#25152;&#26377;&#39044;&#27979;&#25351;&#21521;&#21516;&#19968;&#20010;&#31867;&#21035;&#12290;&#35813;&#29616;&#35937;&#23545;&#26550;&#26500;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#26377;&#23454;&#38469;&#25351;&#23548;&#24847;&#20041;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#21518;&#26524;&#65292;&#20363;&#22914;&#33410;&#28857;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23849;&#28291;&#21644;&#28145;&#24230;&#24102;&#26469;&#30340;&#38750;&#24179;&#20961;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#21021;&#22987;&#29366;&#24577;&#22312;&#35843;&#33410;&#21518;&#32493;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#22312;&#20998;&#31867;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#21487;&#20197;&#22312;&#35757;&#32451;&#20043;&#21069;&#65292;&#29978;&#33267;&#22312;&#19981;&#23384;&#22312;&#26174;&#24335;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#27169;&#22411;&#23558;&#25152;&#26377;&#39044;&#27979;&#37117;&#25351;&#21521;&#21516;&#19968;&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#23384;&#22312;&#65292;&#31216;&#20026;&#8220;&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#8221;&#65288;Initial Guessing Bias&#65292;IGB&#65289;&#65292;&#36825;&#21462;&#20915;&#20110;&#26550;&#26500;&#36873;&#25321;&#65292;&#20363;&#22914;&#28608;&#27963;&#20989;&#25968;&#12289;&#26368;&#22823;&#27744;&#21270;&#23618;&#21644;&#32593;&#32476;&#28145;&#24230;&#12290;&#25105;&#20204;&#23545;IGB&#36827;&#34892;&#30340;&#20998;&#26512;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#65292;&#21487;&#20197;&#25351;&#23548;&#26550;&#26500;&#30340;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#29702;&#35770;&#21518;&#26524;&#65292;&#20363;&#22914;&#33410;&#28857;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23849;&#28291;&#12289;&#33258;&#24179;&#22343;&#30340;&#30772;&#22351;&#12289;&#26576;&#20123;&#22343;&#22330;&#36817;&#20284;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#28145;&#24230;&#24102;&#26469;&#30340;&#38750;&#24179;&#20961;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The initial state of neural networks plays a central role in conditioning the subsequent training dynamics. In the context of classification problems, we provide a theoretical analysis demonstrating that the structure of a neural network can condition the model to assign all predictions to the same class, even before the beginning of training, and in the absence of explicit biases. We show that the presence of this phenomenon, which we call "Initial Guessing Bias" (IGB), depends on architectural choices such as activation functions, max-pooling layers, and network depth. Our analysis of IGB has practical consequences, in that it guides architecture selection and initialization. We also highlight theoretical consequences, such as the breakdown of node-permutation symmetry, the violation of self-averaging, the validity of some mean-field approximations, and the non-trivial differences arising with depth.
&lt;/p&gt;</description></item><item><title>PlaSma&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36807;&#31243;&#30693;&#35782;&#21644;&#35745;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;</title><link>http://arxiv.org/abs/2305.19472</link><description>&lt;p&gt;
PlaSma: &#20026; (&#21453;&#20107;&#23454;) &#35745;&#21010;&#21046;&#23450;&#22686;&#24378;&#36807;&#31243;&#30693;&#35782;&#27169;&#22411;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning. (arXiv:2305.19472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19472
&lt;/p&gt;
&lt;p&gt;
PlaSma&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36807;&#31243;&#30693;&#35782;&#21644;&#35745;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#35268;&#21010;&#26159;&#26426;&#22120;&#30340;&#19968;&#39033;&#37325;&#35201;&#32780;&#21448;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#23427;&#23558;&#19968;&#20010;&#39640;&#32423;&#30446;&#26631;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#26102;&#38388;&#39034;&#24207;&#30340;&#27493;&#39588;&#12290;&#23427;&#38656;&#35201;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#25512;&#29702;&#20986;&#24120;&#24120;&#26159;&#21453;&#20107;&#23454;&#30340;&#22797;&#26434;&#24773;&#22659;&#65292;&#20363;&#22914; "&#27809;&#26377;&#30005;&#35805;&#26102;&#23433;&#25490;&#21307;&#29983;&#30340;&#32422;&#20250;"&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#21463;&#21040;&#26114;&#36149;&#30340; API &#35843;&#29992;&#21644;&#21487;&#22797;&#29616;&#24615;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#35268;&#21010;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; PlaSma&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#21452;&#37325;&#26041;&#27861;&#65292;&#20351;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36807;&#31243;&#30693;&#35782;&#21644; (&#21453;&#20107;&#23454;) &#35745;&#21010;&#33021;&#21147;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31526;&#21495;&#36807;&#31243;&#30693;&#35782;&#33976;&#39311;&#26469;&#22686;&#24378;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#21547;&#30693;&#35782;&#65292;&#20197;&#21450;&#19968;&#31181;&#25512;&#29702;&#31639;&#27861;&#26469;&#20419;&#36827;&#26356;&#32467;&#26500;&#21270;&#21644;&#20934;&#30830;&#30340;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21453;&#20107;&#23454;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex contextualized situations that are often counterfactual, e.g. "scheduling a doctor's appointment without a phone". While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (counterfactual) planning capabilities. More concretely, we develop symbolic procedural knowledge distillation to enhance the implicit knowledge in small language models and an inference-time algorithm to facilitate more structured and accurate reasoning. In addition, we introduce a novel task, Counterfactua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#23545;&#25239;&#29983;&#25104;&#30340;&#31034;&#20363;&#32570;&#20047;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20960;&#20309;&#35282;&#24230;&#20986;&#21457;&#30340;&#26032;&#39062;&#35270;&#35282;&#65292;&#20171;&#32461;&#19968;&#26063;&#27010;&#29575;&#38750;&#23616;&#37096;&#21608;&#38271;&#20989;&#25968;&#26469;&#20248;&#21270;&#27010;&#29575;&#40065;&#26834;&#23398;&#20064;&#65288;PRL&#65289;&#30340;&#21407;&#22987;&#34920;&#36848;&#65292;&#20197;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18779</link><description>&lt;p&gt;
&#20174;&#20960;&#20309;&#35282;&#24230;&#30475;&#24453;&#27010;&#29575;&#40065;&#26834;&#23398;&#20064;&#20013;&#30340;&#36793;&#30028;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
It begins with a boundary: A geometric view on probabilistically robust learning. (arXiv:2305.18779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#23545;&#25239;&#29983;&#25104;&#30340;&#31034;&#20363;&#32570;&#20047;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20960;&#20309;&#35282;&#24230;&#20986;&#21457;&#30340;&#26032;&#39062;&#35270;&#35282;&#65292;&#20171;&#32461;&#19968;&#26063;&#27010;&#29575;&#38750;&#23616;&#37096;&#21608;&#38271;&#20989;&#25968;&#26469;&#20248;&#21270;&#27010;&#29575;&#40065;&#26834;&#23398;&#20064;&#65288;PRL&#65289;&#30340;&#21407;&#22987;&#34920;&#36848;&#65292;&#20197;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#20998;&#31867;&#20219;&#21153;&#19978;&#24050;&#32463;&#23454;&#29616;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#23545;&#20110;&#23545;&#25239;&#29983;&#25104;&#30340;&#31034;&#20363;&#32570;&#20047;&#40065;&#26834;&#24615;&#65292;&#22240;&#27492;&#38656;&#35201;&#23558;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#37325;&#26500;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#26694;&#26550;&#12290;&#26368;&#36817;&#65292;&#20851;&#27880;&#28857;&#24050;&#32463;&#36716;&#21521;&#20102;&#20171;&#20110;&#23545;&#25239;&#24615;&#35757;&#32451;&#25552;&#20379;&#30340;&#40065;&#26834;&#24615;&#21644;ERM&#25552;&#20379;&#30340;&#26356;&#39640;&#24178;&#20928;&#20934;&#30830;&#24615;&#21644;&#26356;&#24555;&#35757;&#32451;&#26102;&#38388;&#20043;&#38388;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20174;&#20960;&#20309;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;&#19968;&#31181;&#36825;&#26679;&#30340;&#26041;&#27861;&#8212;&#8212;&#27010;&#29575;&#40065;&#26834;&#23398;&#20064;&#65288;PRL&#65289;&#65288;Robey&#31561;&#20154;&#65292;ICML&#65292;2022&#65289;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#20960;&#20309;&#35270;&#35282;&#30340;&#25506;&#35752;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#26694;&#26550;&#26469;&#29702;&#35299;PRL&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#20854;&#21407;&#22987;&#34920;&#36848;&#20013;&#30340;&#24494;&#22937;&#32570;&#38519;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#26063;&#27010;&#29575;&#38750;&#23616;&#37096;&#21608;&#38271;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26032;&#39062;&#30340;&#26494;&#24347;&#26041;&#27861;&#35777;&#26126;&#20102;&#35299;&#30340;&#23384;&#22312;&#65292;&#24182;&#30740;&#31350;&#20102;&#24341;&#20837;&#30340;&#38750;&#23616;&#37096;&#21608;&#38271;&#20989;&#25968;&#30340;&#29305;&#24615;&#20197;&#21450;&#23616;&#37096;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep neural networks have achieved super-human performance on many classification tasks, they often exhibit a worrying lack of robustness towards adversarially generated examples. Thus, considerable effort has been invested into reformulating Empirical Risk Minimization (ERM) into an adversarially robust framework. Recently, attention has shifted towards approaches which interpolate between the robustness offered by adversarial training and the higher clean accuracy and faster training times of ERM. In this paper, we take a fresh and geometric view on one such method -- Probabilistically Robust Learning (PRL) (Robey et al., ICML, 2022). We propose a geometric framework for understanding PRL, which allows us to identify a subtle flaw in its original formulation and to introduce a family of probabilistic nonlocal perimeter functionals to address this. We prove existence of solutions using novel relaxation methods and study properties as well as local limits of the introduced per
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#32467;&#21512;Copula&#29702;&#35770;&#26469;&#35299;&#20915;&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.17448</link><description>&lt;p&gt;
NN-Copula-CD&#65306;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#21464;&#21270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
NN-Copula-CD: A Copula-Guided Interpretable Neural Network for Change Detection in Heterogeneous Remote Sensing Images. (arXiv:2303.17448v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17448
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#32467;&#21512;Copula&#29702;&#35770;&#26469;&#35299;&#20915;&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#26159;&#19968;&#20010;&#23454;&#38469;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36807;&#21435;&#21313;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#21457;&#23637;&#35753;&#24322;&#26500;&#21464;&#21270;&#26816;&#27979;&#38382;&#39064;&#21463;&#30410;&#21290;&#27973;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;DNN&#22987;&#32456;&#20687;&#40657;&#21283;&#23376;&#19968;&#26679;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;DNN&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#21464;&#21270;&#26816;&#27979;&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#24322;&#26500;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;(NN-Copula-CD)&#12290;&#22312;NN-Copula-CD&#20013;&#65292;Copula&#30340;&#25968;&#23398;&#29305;&#24449;&#34987;&#35774;&#35745;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#30417;&#30563;&#19968;&#20010;&#31616;&#21333;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Change detection (CD) in heterogeneous remote sensing images is a practical and challenging issue for real-life emergencies. In the past decade, the heterogeneous CD problem has significantly benefited from the development of deep neural networks (DNN). However, the data-driven DNNs always perform like a black box where the lack of interpretability limits the trustworthiness and controllability of DNNs in most practical CD applications. As a strong knowledge-driven tool to measure correlation between random variables, Copula theory has been introduced into CD, yet it suffers from non-robust CD performance without manual prior selection for Copula functions. To address the above issues, we propose a knowledge-data-driven heterogeneous CD method (NN-Copula-CD) based on the Copula-guided interpretable neural network. In our NN-Copula-CD, the mathematical characteristics of Copula are designed as the losses to supervise a simple fully connected neural network to learn the correlation betwe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bayan&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#25110;&#36817;&#20284;&#20248;&#21270;&#27169;&#22359;&#24230;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36820;&#22238;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#20998;&#21306;&#65292;&#24182;&#19988;&#27604;&#20854;&#20182;&#31639;&#27861;&#24555;&#25968;&#20493;&#65292;&#24182;&#33021;&#22815;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#20934;&#30830;&#22320;&#25214;&#21040;&#22320;&#38754;&#30495;&#23454;&#31038;&#21306;&#12290;</title><link>http://arxiv.org/abs/2209.04562</link><description>&lt;p&gt;
Bayan&#31639;&#27861;&#65306;&#36890;&#36807;&#23545;&#27169;&#22359;&#24230;&#30340;&#31934;&#30830;&#21644;&#36817;&#20284;&#20248;&#21270;&#26469;&#26816;&#27979;&#32593;&#32476;&#20013;&#30340;&#31038;&#21306;
&lt;/p&gt;
&lt;p&gt;
The Bayan Algorithm: Detecting Communities in Networks Through Exact and Approximate Optimization of Modularity. (arXiv:2209.04562v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04562
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bayan&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#25110;&#36817;&#20284;&#20248;&#21270;&#27169;&#22359;&#24230;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36820;&#22238;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#20998;&#21306;&#65292;&#24182;&#19988;&#27604;&#20854;&#20182;&#31639;&#27861;&#24555;&#25968;&#20493;&#65292;&#24182;&#33021;&#22815;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#20934;&#30830;&#22320;&#25214;&#21040;&#22320;&#38754;&#30495;&#23454;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#26159;&#32593;&#32476;&#31185;&#23398;&#20013;&#30340;&#32463;&#20856;&#38382;&#39064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#20247;&#22810;&#26041;&#27861;&#20013;&#65292;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#26368;&#22823;&#21270;&#27169;&#22359;&#24230;&#12290;&#23613;&#31649;&#21551;&#21457;&#24335;&#27169;&#22359;&#24230;&#26368;&#22823;&#21270;&#31639;&#27861;&#35774;&#35745;&#29702;&#24565;&#21644;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#24456;&#23569;&#36820;&#22238;&#26368;&#20339;&#20998;&#21306;&#25110;&#31867;&#20284;&#20998;&#21306;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#30340;&#31639;&#27861;Bayan&#65292;&#23427;&#36820;&#22238;&#20855;&#26377;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#20998;&#21306;&#20445;&#35777;&#30340;&#20998;&#21306;&#12290;Bayan&#31639;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#20998;&#25903;&#38480;&#30028;&#26041;&#26696;&#65292;&#23427;&#35299;&#20915;&#20102;&#38382;&#39064;&#30340;&#25972;&#25968;&#35268;&#21010;&#20844;&#24335;&#20197;&#36798;&#21040;&#26368;&#20248;&#25110;&#36817;&#20284;&#26368;&#20248;&#30340;&#30446;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;Bayan&#22312;&#21512;&#25104;&#22522;&#20934;&#21644;&#30495;&#23454;&#32593;&#32476;&#33410;&#28857;&#26631;&#31614;&#30340;&#26816;&#32034;&#22320;&#38754;&#30495;&#23454;&#31038;&#21306;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#27604;&#20854;&#20182;21&#31181;&#31639;&#27861;&#24555;&#25968;&#20493;&#65292;&#21487;&#20197;&#25214;&#21040;&#26368;&#20248;&#20998;&#21306;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection is a classic problem in network science with extensive applications in various fields. Among numerous approaches, the most common method is modularity maximization. Despite their design philosophy and wide adoption, heuristic modularity maximization algorithms rarely return an optimal partition or anything similar. We propose a specialized algorithm, Bayan, which returns partitions with a guarantee of either optimality or proximity to an optimal partition. At the core of the Bayan algorithm is a branch-and-cut scheme that solves an integer programming formulation of the problem to optimality or approximate it within a factor. We demonstrate Bayan's distinctive accuracy and stability over 21 other algorithms in retrieving ground-truth communities in synthetic benchmarks and node labels in real networks. Bayan is several times faster than open-source and commercial solvers for modularity maximization making it capable of finding optimal partitions for instances that c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#40654;&#26364;&#20960;&#20309;&#21644;&#21494;&#38754;&#29702;&#35770;&#21019;&#26032;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#26032;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25968;&#25454;&#31354;&#38388;&#30340;&#20197;&#26354;&#29575;&#20026;&#32771;&#37327;&#22240;&#32032;&#30340; two-step spectral &#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2203.00922</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#33539;&#21494;&#38754;&#65306;&#40065;&#26834;&#24615;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Canonical foliations of neural networks: application to robustness. (arXiv:2203.00922v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#40654;&#26364;&#20960;&#20309;&#21644;&#21494;&#38754;&#29702;&#35770;&#21019;&#26032;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#26032;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25968;&#25454;&#31354;&#38388;&#30340;&#20197;&#26354;&#29575;&#20026;&#32771;&#37327;&#22240;&#32032;&#30340; two-step spectral &#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#32780;&#23545;&#25239;&#23398;&#20064;&#27491;&#22312;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#24615;&#35270;&#35282;&#65292;&#37319;&#29992;&#40654;&#26364;&#20960;&#20309;&#21644;&#21494;&#38754;&#29702;&#35770;&#12290;&#36890;&#36807;&#21019;&#24314;&#32771;&#34385;&#25968;&#25454;&#31354;&#38388;&#26354;&#29575;&#30340;&#26032;&#23545;&#25239;&#25915;&#20987;&#65292;&#21363; two-step spectral attack&#65292;&#26469;&#35828;&#26126;&#36825;&#20010;&#24819;&#27861;&#12290;&#25968;&#25454;&#31354;&#38388;&#34987;&#35270;&#20026;&#19968;&#20010;&#37197;&#22791;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340; Fisher &#20449;&#24687;&#24230;&#37327;&#65288;FIM&#65289;&#25289;&#22238;&#30340;&#65288;&#36864;&#21270;&#30340;&#65289;&#40654;&#26364;&#27969;&#24418;&#12290;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#35813;&#24230;&#37327;&#20165;&#20026;&#21322;&#27491;&#23450;&#65292;&#20854;&#20869;&#26680;&#25104;&#20026;&#30740;&#31350;&#30340;&#26680;&#24515;&#23545;&#35937;&#12290;&#20174;&#35813;&#26680;&#20013;&#23548;&#20986;&#19968;&#20010;&#35268;&#33539;&#21494;&#38754;&#12290;&#27178;&#21521;&#21494;&#30340;&#26354;&#29575;&#32473;&#20986;&#20102;&#36866;&#24403;&#30340;&#20462;&#27491;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#20004;&#27493;&#36817;&#20284;&#30340;&#27979;&#22320;&#32447;&#21644;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#23545;&#25239;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#22312;&#19968;&#20010; 2D &#29609;&#20855;&#31034;&#20363;&#20013;&#36827;&#34892;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models are known to be vulnerable to adversarial attacks. Adversarial learning is therefore becoming a crucial task. We propose a new vision on neural network robustness using Riemannian geometry and foliation theory. The idea is illustrated by creating a new adversarial attack that takes into account the curvature of the data space. This new adversarial attack called the two-step spectral attack is a piece-wise linear approximation of a geodesic in the data space. The data space is treated as a (degenerate) Riemannian manifold equipped with the pullback of the Fisher Information Metric (FIM) of the neural network. In most cases, this metric is only semi-definite and its kernel becomes a central object to study. A canonical foliation is derived from this kernel. The curvature of transverse leaves gives the appropriate correction to get a two-step approximation of the geodesic and hence a new efficient adversarial attack. The method is first illustrated on a 2D toy example
&lt;/p&gt;</description></item></channel></rss>