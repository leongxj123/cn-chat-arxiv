<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>fastprop&#26159;&#19968;&#31181;DeepQSPR&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23376;&#32423;&#25551;&#36848;&#31526;&#65292;&#22312;&#26497;&#22823;&#32553;&#30701;&#26102;&#38388;&#20869;&#65292;&#22312;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#24182;&#36229;&#36234;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02058</link><description>&lt;p&gt;
&#20855;&#26377;&#24555;&#36895;prop&#30340;&#21487;&#25512;&#24191;&#12289;&#24555;&#36895;&#21644;&#20934;&#30830;&#30340;DeepQSPR Part 1: &#26694;&#26550;&#21644;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Generalizable, Fast, and Accurate DeepQSPR with fastprop Part 1: Framework and Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02058
&lt;/p&gt;
&lt;p&gt;
fastprop&#26159;&#19968;&#31181;DeepQSPR&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23376;&#32423;&#25551;&#36848;&#31526;&#65292;&#22312;&#26497;&#22823;&#32553;&#30701;&#26102;&#38388;&#20869;&#65292;&#22312;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#24182;&#36229;&#36234;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#32467;&#26500;-&#24615;&#36136;&#20851;&#31995;&#30740;&#31350;&#26088;&#22312;&#23450;&#20041;&#20998;&#23376;&#32467;&#26500;&#19982;&#20219;&#24847;&#24863;&#20852;&#36259;&#30340;&#25968;&#37327;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#21382;&#21490;&#19978;&#65292;&#36825;&#26159;&#36890;&#36807;&#24320;&#21457;&#25551;&#36848;&#31526;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#38656;&#35201;&#26174;&#33879;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#38590;&#20197;&#27867;&#21270;&#12290;&#22240;&#27492;&#65292;&#35813;&#39046;&#22495;&#24050;&#32463;&#28436;&#21464;&#20026;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#65292;&#24182;&#36716;&#20026;&#20351;&#29992;&#39640;&#24230;&#21487;&#25512;&#24191;&#30340;&#23398;&#20064;&#34920;&#31034;&#12290;&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;fastprop&#65292;&#19968;&#31181;DeepQSPR&#26694;&#26550;&#65292;&#20351;&#29992;&#19968;&#32452;&#26126;&#26234;&#30340;&#20998;&#23376;&#32423;&#25551;&#36848;&#31526;&#65292;&#22312;&#26497;&#22823;&#32553;&#30701;&#30340;&#26102;&#38388;&#20869;&#28385;&#36275;&#24182;&#36229;&#36234;&#20102;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;fastprop&#21487;&#20197;&#22312;github&#19978;&#20813;&#36153;&#33719;&#21462;&#65292;&#32593;&#22336;&#20026;github.com/JacksonBurns/fastprop&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02058v1 Announce Type: new  Abstract: Quantitative Structure Property Relationship studies aim to define a mapping between molecular structure and arbitrary quantities of interest. This was historically accomplished via the development of descriptors which requires significant domain expertise and struggles to generalize. Thus the field has morphed into Molecular Property Prediction and been given over to learned representations which are highly generalizable. The paper introduces fastprop, a DeepQSPR framework which uses a cogent set of molecular level descriptors to meet and exceed the performance of learned representations on diverse datasets in dramatically less time. fastprop is freely available on github at github.com/JacksonBurns/fastprop.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#21463;&#25968;&#23383;&#20998;&#24067;&#35268;&#24459;&#25511;&#21046;&#30340;&#38170;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#35821;&#20041;&#19978;&#24341;&#23548;&#25968;&#23383;&#30340;&#31574;&#30053;&#65292;&#22312;&#24191;&#27867;&#33539;&#22260;&#30340;&#25968;&#23383;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#25968;&#23398;&#22522;&#30784;&#34920;&#31034;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2404.01536</link><description>&lt;p&gt;
&#25918;&#32622;&#38170;&#28857;&#65306;&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#32473;&#25968;&#23383;&#35821;&#20041;&#19978;&#30340;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Laying Anchors: Semantically Priming Numerals in Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01536
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#21463;&#25968;&#23383;&#20998;&#24067;&#35268;&#24459;&#25511;&#21046;&#30340;&#38170;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#35821;&#20041;&#19978;&#24341;&#23548;&#25968;&#23383;&#30340;&#31574;&#30053;&#65292;&#22312;&#24191;&#27867;&#33539;&#22260;&#30340;&#25968;&#23383;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#25968;&#23398;&#22522;&#30784;&#34920;&#31034;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22823;&#37327;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31649;&#32447;&#20013;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#28982;&#32780;&#36825;&#20123;&#27169;&#22411;&#26410;&#33021;&#27491;&#30830;&#32534;&#30721;&#25968;&#23383;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#25968;&#23383;&#29702;&#35299;&#30340;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#20219;&#20309;&#35821;&#26009;&#24211;&#20013;&#29983;&#25104;&#21463;&#25968;&#23383;&#20998;&#24067;&#35268;&#24459;&#25511;&#21046;&#30340;&#38170;&#28857;&#26469;&#22312;&#35821;&#20041;&#19978;&#24341;&#23548;&#25968;&#23383;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#20123;&#25968;&#23383;&#26631;&#35760;&#30340;&#25968;&#23398;&#22522;&#30784;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19968;&#31995;&#21015;&#25968;&#20540;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#30340;&#20248;&#36234;&#24615;&#65292;&#23545;&#39046;&#22495;&#20869;&#65288;&#24050;&#35265;&#65289;&#21644;&#39046;&#22495;&#22806;&#65288;&#26410;&#35265;&#65289;&#30340;&#25968;&#23383;&#37117;&#36866;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#23454;&#35777;&#35780;&#20272;&#25193;&#23637;&#21040;&#20174;1&#21040;10&#20159;&#30340;&#25968;&#23383;&#33539;&#22260;&#65292;&#27604;&#20197;&#24448;&#30456;&#21516;&#31867;&#22411;&#30740;&#31350;&#30340;&#33539;&#22260;&#24191;&#24471;&#22810;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#23398;&#24471;&#30340;&#23884;&#20837;&#21521;&#25968;&#23398;&#19978;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01536v1 Announce Type: cross  Abstract: Off-the-shelf pre-trained language models have become the de facto standard in NLP pipelines for a multitude of downstream tasks. However, the inability of these models to properly encode numerals limits their performance on tasks requiring numeric comprehension. We introduce strategies to semantically prime numerals in any corpus by generating anchors governed by the distribution of numerals in said corpus, thereby enabling mathematically grounded representations of these numeral tokens. We establish the superiority of our proposed techniques through evaluation on a range of numeracy tasks for both in-domain (seen) and out-domain (unseen) numerals. Further, we expand our empirical evaluations to numerals ranging from 1 to 10 billion, a significantly broader range compared to previous studies of the same nature, and we demonstrate significant improvements in the mathematical grounding of our learned embeddings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25991;&#26412;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;MANGO&#22522;&#20934;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#22238;&#31572;&#28041;&#21450;&#26144;&#23556;&#21644;&#23548;&#33322;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2403.19913</link><description>&lt;p&gt;
MANGO&#65306;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19913
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25991;&#26412;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;MANGO&#22522;&#20934;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#22238;&#31572;&#28041;&#21450;&#26144;&#23556;&#21644;&#23548;&#33322;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;ChatGPT&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26368;&#36817;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MANGO&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#23427;&#20204;&#25191;&#34892;&#22522;&#20110;&#25991;&#26412;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;&#19968;&#22871;&#25991;&#26412;&#28216;&#25103;&#30340;53&#20010;&#36855;&#23467;&#65306;&#27599;&#20010;&#36855;&#23467;&#37117;&#19982;&#19968;&#20010;&#28216;&#35272;&#35828;&#26126;&#37197;&#23545;&#65292;&#20854;&#20013;&#21253;&#21547;&#27599;&#20010;&#20301;&#32622;&#30340;&#35775;&#38382;&#20294;&#19981;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;&#30340;&#36335;&#24452;&#12290;&#20219;&#21153;&#26159;&#38382;&#31572;&#65306;&#23545;&#20110;&#27599;&#20010;&#36855;&#23467;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35835;&#21462;&#28216;&#35272;&#35828;&#26126;&#24182;&#22238;&#31572;&#25968;&#30334;&#20010;&#26144;&#23556;&#21644;&#23548;&#33322;&#38382;&#39064;&#65292;&#20363;&#22914;&#8220;&#20320;&#24212;&#35813;&#20174;&#25151;&#23376;&#35199;&#37096;&#22914;&#20309;&#21435;&#38401;&#27004;&#65311;&#8221;&#21644;&#8220;&#22914;&#26524;&#25105;&#20204;&#20174;&#22320;&#19979;&#23460;&#21521;&#21271;&#21644;&#19996;&#36208;&#65292;&#25105;&#20204;&#20250;&#22312;&#21738;&#37324;&#65311;&#8221;&#12290;&#23613;&#31649;&#36825;&#20123;&#38382;&#39064;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#23481;&#26131;&#65292;&#20294;&#20107;&#23454;&#35777;&#26126;&#65292;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#29978;&#33267;&#22312;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#23558;&#26377;&#21033;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19913v1 Announce Type: cross  Abstract: Large language models such as ChatGPT and GPT-4 have recently achieved astonishing performance on a variety of natural language processing tasks. In this paper, we propose MANGO, a benchmark to evaluate their capabilities to perform text-based mapping and navigation. Our benchmark includes 53 mazes taken from a suite of textgames: each maze is paired with a walkthrough that visits every location but does not cover all possible paths. The task is question-answering: for each maze, a large language model reads the walkthrough and answers hundreds of mapping and navigation questions such as "How should you go to Attic from West of House?" and "Where are we if we go north and east from Cellar?". Although these questions are easy to humans, it turns out that even GPT-4, the best-to-date language model, performs poorly at answering them. Further, our experiments suggest that a strong mapping and navigation ability would benefit large languag
&lt;/p&gt;</description></item><item><title>FOOL&#26159;&#19968;&#31181;OEC&#26412;&#22320;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#29305;&#24449;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21534;&#21520;&#37327;&#12289;&#23884;&#20837;&#19978;&#19979;&#25991;&#21644;&#21033;&#29992;&#29943;&#30742;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#38477;&#20302;&#20256;&#36755;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16677</link><description>&lt;p&gt;
FOOL: &#29992;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#35299;&#20915;&#21355;&#26143;&#35745;&#31639;&#20013;&#30340;&#19979;&#34892;&#29942;&#39048;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FOOL: Addressing the Downlink Bottleneck in Satellite Computing with Neural Feature Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16677
&lt;/p&gt;
&lt;p&gt;
FOOL&#26159;&#19968;&#31181;OEC&#26412;&#22320;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#29305;&#24449;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21534;&#21520;&#37327;&#12289;&#23884;&#20837;&#19978;&#19979;&#25991;&#21644;&#21033;&#29992;&#29943;&#30742;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#38477;&#20302;&#20256;&#36755;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#20256;&#24863;&#22120;&#30340;&#32435;&#21355;&#26143;&#26143;&#24231;&#25429;&#33719;&#22823;&#33539;&#22260;&#22320;&#29702;&#21306;&#22495;&#65292;&#20026;&#22320;&#29699;&#35266;&#27979;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#38543;&#30528;&#26143;&#24231;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#32593;&#32476;&#20105;&#29992;&#24418;&#25104;&#20102;&#19979;&#34892;&#29942;&#39048;&#12290;&#36712;&#36947;&#36793;&#32536;&#35745;&#31639;&#65288;OEC&#65289;&#21033;&#29992;&#26377;&#38480;&#30340;&#26426;&#36733;&#35745;&#31639;&#36164;&#28304;&#36890;&#36807;&#22312;&#28304;&#22836;&#22788;&#29702;&#21407;&#22987;&#25429;&#33719;&#26469;&#20943;&#23569;&#20256;&#36755;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20381;&#36182;&#31895;&#31961;&#30340;&#36807;&#28388;&#26041;&#27861;&#25110;&#36807;&#20998;&#20248;&#20808;&#32771;&#34385;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#65292;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26377;&#38480;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FOOL&#65292;&#19968;&#31181;OEC&#26412;&#22320;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#29305;&#24449;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20445;&#30041;&#39044;&#27979;&#24615;&#33021;&#12290;FOOL&#23558;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#20998;&#21306;&#65292;&#20197;&#26368;&#22823;&#21270;&#21534;&#21520;&#37327;&#12290;&#27492;&#22806;&#65292;&#23427;&#23884;&#20837;&#19978;&#19979;&#25991;&#24182;&#21033;&#29992;&#29943;&#30742;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#36739;&#20302;&#30340;&#24320;&#38144;&#38477;&#20302;&#20256;&#36755;&#25104;&#26412;&#12290;&#34429;&#28982;FOOL&#26159;&#19968;&#31181;&#29305;&#24449;&#21387;&#32553;&#22120;&#65292;&#20294;&#23427;&#21487;&#20197;&#22312;&#20302;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16677v1 Announce Type: new  Abstract: Nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for Earth observation. As constellation sizes increase, network contention poses a downlink bottleneck. Orbital Edge Computing (OEC) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source. However, current solutions have limited practicability due to reliance on crude filtering methods or over-prioritizing particular downstream tasks.   This work presents FOOL, an OEC-native and task-agnostic feature compression method that preserves prediction performance. FOOL partitions high-resolution satellite imagery to maximize throughput. Further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead. While FOOL is a feature compressor, it can recover images with competitive scores on perceptual quality measures at low
&lt;/p&gt;</description></item><item><title>Duwak&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#21452;&#37325;&#31192;&#23494;&#27169;&#24335;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27700;&#21360;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.13000</link><description>&lt;p&gt;
Duwak: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21452;&#37325;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Duwak: Dual Watermarks in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13000
&lt;/p&gt;
&lt;p&gt;
Duwak&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#21452;&#37325;&#31192;&#23494;&#27169;&#24335;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27700;&#21360;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#26085;&#30410;&#20351;&#29992;&#65292;&#23457;&#35745;&#23427;&#20204;&#30340;&#29992;&#36884;&#12289;&#31649;&#29702;&#23427;&#20204;&#30340;&#24212;&#29992;&#24182;&#20943;&#36731;&#20854;&#28508;&#22312;&#21361;&#23475;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Duwak&#65292;&#36890;&#36807;&#22312;&#20196;&#29260;&#27010;&#29575;&#20998;&#24067;&#21644;&#25277;&#26679;&#26041;&#26696;&#20013;&#23884;&#20837;&#21452;&#37325;&#31192;&#23494;&#27169;&#24335;&#65292;&#20174;&#26681;&#26412;&#19978;&#25552;&#39640;&#20102;&#27700;&#21360;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13000v1 Announce Type: cross  Abstract: As large language models (LLM) are increasingly used for text generation tasks, it is critical to audit their usages, govern their applications, and mitigate their potential harms. Existing watermark techniques are shown effective in embedding single human-imperceptible and machine-detectable patterns without significantly affecting generated text quality and semantics. However, the efficiency in detecting watermarks, i.e., the minimum number of tokens required to assert detection with significance and robustness against post-editing, is still debatable. In this paper, we propose, Duwak, to fundamentally enhance the efficiency and quality of watermarking by embedding dual secret patterns in both token probability distribution and sampling schemes. To mitigate expression degradation caused by biasing toward certain tokens, we design a contrastive search to watermark the sampling scheme, which minimizes the token repetition and enhances 
&lt;/p&gt;</description></item><item><title>MediSwift&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#24341;&#20837;&#20102;&#39640;&#25928;&#31232;&#30095;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;75%&#30340;&#26435;&#37325;&#31232;&#30095;&#24615;&#23454;&#29616;&#20102;2-2.5&#20493;&#30340;&#35757;&#32451;FLOPs&#20943;&#23569;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.00952</link><description>&lt;p&gt;
MediSwift&#65306;&#39640;&#25928;&#31232;&#30095;&#39044;&#35757;&#32451;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MediSwift: Efficient Sparse Pre-trained Biomedical Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00952
&lt;/p&gt;
&lt;p&gt;
MediSwift&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#24341;&#20837;&#20102;&#39640;&#25928;&#31232;&#30095;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;75%&#30340;&#26435;&#37325;&#31232;&#30095;&#24615;&#23454;&#29616;&#20102;2-2.5&#20493;&#30340;&#35757;&#32451;FLOPs&#20943;&#23569;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#36890;&#29992;&#28304;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#20294;&#26368;&#36817;&#39046;&#22495;&#29305;&#23450;&#30340;LLMs&#28608;&#22686;&#34920;&#26126;&#23427;&#20204;&#22312;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#65288;&#20363;&#22914;&#29983;&#29289;&#21307;&#23398;&#65289;&#20013;&#30340;&#28508;&#21147;&#36229;&#36807;&#20102;&#36890;&#29992;&#22411;&#27169;&#22411;&#12290;&#34429;&#28982;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#25552;&#39640;&#20102;&#25928;&#29575;&#24182;&#23548;&#33268;&#27169;&#22411;&#26356;&#23567;&#65292;&#20294;&#36825;&#20123;LLMs&#30340;&#35757;&#32451;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#24456;&#39640;&#65292;&#26500;&#25104;&#20102;&#39044;&#31639;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MediSwift&#65292;&#19968;&#22871;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#31232;&#30095;&#39044;&#35757;&#32451;&#30340;&#29983;&#29289;&#21307;&#23398;LM&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#24341;&#20837;&#39640;&#36798;75&#65285;&#30340;&#26435;&#37325;&#31232;&#30095;&#24615;&#65292;MediSwift&#22312;&#35757;&#32451;FLOPs&#26041;&#38754;&#23454;&#29616;&#20102;2-2.5&#20493;&#30340;&#20943;&#23569;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#26377;&#30340;&#31232;&#30095;&#39044;&#35757;&#32451;&#22343;&#22312;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#23454;&#29616;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#26435;&#37325;&#31232;&#30095;&#24615;&#30340;&#21152;&#36895;&#22909;&#22788;&#30340;Cerebras CS-2&#31995;&#32479;&#19978;&#36827;&#34892;&#65292;&#20174;&#32780;&#26174;&#30528;&#25552;&#39640;&#20102;MediSwift&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00952v1 Announce Type: new  Abstract: Large language models (LLMs) are typically trained on general source data for various domains, but a recent surge in domain-specific LLMs has shown their potential to outperform general-purpose models in domain-specific tasks (e.g., biomedicine). Although domain-specific pre-training enhances efficiency and leads to smaller models, the computational costs of training these LLMs remain high, posing budgeting challenges. We introduce MediSwift, a suite of biomedical LMs that leverage sparse pre-training on domain-specific biomedical text data. By inducing up to 75% weight sparsity during the pre-training phase, MediSwift achieves a 2-2.5x reduction in training FLOPs. Notably, all sparse pre-training was performed on the Cerebras CS-2 system, which is specifically designed to realize the acceleration benefits from unstructured weight sparsity, thereby significantly enhancing the efficiency of the MediSwift models. Through subsequent dense f
&lt;/p&gt;</description></item><item><title>&#23558;NHPPs&#30340;&#20272;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#23398;&#20064;&#27867;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#23398;&#20064;NHPPs&#30340;&#26694;&#26550;&#19982;&#20004;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#31665;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#25968;&#25454;&#37327;&#26377;&#38480;&#26102;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12808</link><description>&lt;p&gt;
&#23398;&#20064;&#38750;&#40784;&#27425;&#26102;&#38388;&#27850;&#26494;&#36807;&#31243;&#30340;&#27867;&#21270;&#21644;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning Generalization and Regularization of Nonhomogeneous Temporal Poisson Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12808
&lt;/p&gt;
&lt;p&gt;
&#23558;NHPPs&#30340;&#20272;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#23398;&#20064;&#27867;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#23398;&#20064;NHPPs&#30340;&#26694;&#26550;&#19982;&#20004;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#31665;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#25968;&#25454;&#37327;&#26377;&#38480;&#26102;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27850;&#26494;&#36807;&#31243;&#65292;&#23588;&#20854;&#26159;&#38750;&#40784;&#27425;&#27850;&#26494;&#36807;&#31243;(NHPP)&#65292;&#26159;&#19968;&#31181;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#30340;&#35745;&#25968;&#36807;&#31243;&#12290;&#30446;&#21069;&#65292;&#25991;&#29486;&#20013;&#20960;&#20046;&#25152;&#26377;&#30340;&#24037;&#20316;&#37117;&#33268;&#21147;&#20110;&#20351;&#29992;&#38750;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#31665;&#26041;&#27861;&#23545;&#20855;&#26377;&#26080;&#31351;&#25968;&#25454;&#30340;NHPP&#36827;&#34892;&#20272;&#35745;&#12290;&#26412;&#25991;&#23558;&#26377;&#38480;&#21644;&#26377;&#38480;&#25968;&#25454;&#19979;&#30340;NHPP&#20272;&#35745;&#38382;&#39064;&#20844;&#24335;&#21270;&#20026;&#19968;&#20010;&#23398;&#20064;&#27867;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#19978;&#35777;&#26126;&#65292;&#23613;&#31649;&#20998;&#31665;&#26041;&#27861;&#23545;&#20110;&#20272;&#35745;NHPPs&#24456;&#37325;&#35201;&#65292;&#20294;&#22312;&#25968;&#25454;&#37327;&#26377;&#38480;&#26102;&#20250;&#24102;&#26469;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#23398;&#20064;NHPPs&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#31665;&#26041;&#27861;&#65292;&#24110;&#21161;&#28040;&#38500;&#20998;&#31665;&#21442;&#25968;&#30340;&#21363;&#20852;&#35843;&#25972;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12808v1 Announce Type: new  Abstract: The Poisson process, especially the nonhomogeneous Poisson process (NHPP), is an essentially important counting process with numerous real-world applications. Up to date, almost all works in the literature have been on the estimation of NHPPs with infinite data using non-data driven binning methods. In this paper, we formulate the problem of estimation of NHPPs from finite and limited data as a learning generalization problem. We mathematically show that while binning methods are essential for the estimation of NHPPs, they pose a threat of overfitting when the amount of data is limited. We propose a framework for regularized learning of NHPPs with two new adaptive and data-driven binning methods that help to remove the ad-hoc tuning of binning parameters. Our methods are experimentally tested on synthetic and real-world datasets and the results show their effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21407;&#21017;&#8212;&#8212;&#24179;&#31561;&#20445;&#25252;&#65292;&#20854;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#22343;&#31561;&#21270;&#65292;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20256;&#32479;&#20998;&#31867;&#24179;&#31561;&#21407;&#21017;&#30340;&#21453;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.12062</link><description>&lt;p&gt;
&#22240;&#26524;&#24179;&#31561;&#20445;&#25252;&#19982;&#31639;&#27861;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Equal Protection as Algorithmic Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21407;&#21017;&#8212;&#8212;&#24179;&#31561;&#20445;&#25252;&#65292;&#20854;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#22343;&#31561;&#21270;&#65292;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20256;&#32479;&#20998;&#31867;&#24179;&#31561;&#21407;&#21017;&#30340;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#21746;&#23398;&#30340;&#25991;&#29486;&#24418;&#25104;&#20102;&#19981;&#21516;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#26631;&#20934;&#12290;&#20854;&#20013;&#26368;&#21463;&#20105;&#35758;&#30340;&#20998;&#31867;&#24179;&#31561;&#35201;&#27714;&#65292;&#39044;&#27979;&#31639;&#27861;&#30340;&#38169;&#35823;&#20998;&#31867;&#22312;&#34987;&#20445;&#25252;&#29305;&#24449;&#25152;&#25351;&#31034;&#30340;&#32676;&#20307;&#20013;&#20197;&#30456;&#31561;&#39057;&#29575;&#21457;&#29983;&#12290;&#23613;&#31649;&#20998;&#31867;&#24179;&#31561;&#20855;&#26377;&#30452;&#35266;&#21560;&#24341;&#21147;&#65292;&#20294;&#24050;&#21463;&#21040;&#25915;&#20987;&#12290;&#25105;&#20204;&#36716;&#21521;&#19968;&#20010;&#30456;&#20851;&#21407;&#21017;&#65292;&#21363;&#24179;&#31561;&#20445;&#25252;&#65292;&#35813;&#21407;&#21017;&#26368;&#21021;&#26159;&#22312;&#21009;&#20107;&#21496;&#27861;&#39046;&#22495;&#21457;&#23637;&#36215;&#26469;&#30340;&#12290;&#24179;&#31561;&#20445;&#25252;&#30340;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#65288;&#23558;&#22312;&#35268;&#23450;&#30340;&#24847;&#20041;&#19978;&#20855;&#20307;&#35828;&#26126;&#65289;&#36827;&#34892;&#22343;&#31561;&#21270;&#65292;&#32780;&#19981;&#26159;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#27604;&#29575;&#22343;&#31561;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24179;&#31561;&#20445;&#25252;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20998;&#31867;&#24179;&#31561;&#30340;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12062v1 Announce Type: cross  Abstract: Over the last ten years the literature in computer science and philosophy has formulated different criteria of algorithmic fairness. One of the most discussed, classification parity, requires that the erroneous classifications of a predictive algorithm occur with equal frequency for groups picked out by protected characteristics. Despite its intuitive appeal, classification parity has come under attack. Multiple scenarios can be imagined in which - intuitively - a predictive algorithm does not treat any individual unfairly, and yet classification parity is violated. To make progress, we turn to a related principle, equal protection, originally developed in the context of criminal justice. Key to equal protection is equalizing the risks of erroneous classifications (in a sense to be specified) as opposed to equalizing the rates of erroneous classifications. We show that equal protection avoids many of the counterexamples to classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#35270;&#39057;&#39044;&#35757;&#32451;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#39118;&#38505;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25163;&#35821;&#32763;&#35793;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09611</link><description>&lt;p&gt;
&#23454;&#29616;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Towards Privacy-Aware Sign Language Translation at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#35270;&#39057;&#39044;&#35757;&#32451;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#39118;&#38505;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25163;&#35821;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#32763;&#35793;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#25968;&#25454;&#31232;&#32570;&#12290;&#30446;&#21069;&#22312;&#32593;&#32476;&#19978;&#21487;&#29992;&#30340;&#22823;&#37096;&#20998;&#25163;&#35821;&#25968;&#25454;&#30001;&#20110;&#32570;&#20047;&#23545;&#40784;&#30340;&#23383;&#24149;&#32780;&#26080;&#27861;&#29992;&#20110;&#35757;&#32451;&#30417;&#30563;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#32593;&#32476;&#25235;&#21462;&#30340;&#25968;&#25454;&#38598;&#26469;&#25193;&#23637;&#25163;&#35821;&#32763;&#35793;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#65292;&#22240;&#20026;&#20854;&#20013;&#21253;&#21547;&#29983;&#29289;&#29305;&#24449;&#20449;&#24687;&#65292;&#36127;&#36131;&#20219;&#22320;&#24320;&#21457;&#25163;&#35821;&#32763;&#35793;&#25216;&#26415;&#24212;&#35813;&#32771;&#34385;&#36825;&#19968;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35268;&#27169;&#21270;&#38544;&#31169;&#24863;&#30693;&#25163;&#35821;&#32763;&#35793;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SSVP-SLT&#65292;&#23427;&#21033;&#29992;&#21311;&#21517;&#21644;&#26410;&#27880;&#37322;&#30340;&#35270;&#39057;&#36827;&#34892;&#33258;&#30417;&#30563;&#35270;&#39057;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#21033;&#29992;&#32463;&#36807;&#31579;&#36873;&#30340;&#24179;&#34892;&#25968;&#25454;&#38598;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#25163;&#35821;&#32763;&#35793;&#24494;&#35843;&#12290; SSVP-SLT&#22312;How2Sign&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24494;&#35843;&#21644;&#38646;&#27425;gloss-free&#25163;&#35821;&#32763;&#35793;&#24615;&#33021;&#65292;&#27604;&#26368;&#24378;&#30340;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;3&#20010;BLEU-4&#12290;&#36890;&#36807;&#21463;&#25511;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#35821;&#35328;&#21644;&#25163;&#35821;&#35789;&#27719;&#19978;&#37117;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09611v1 Announce Type: new  Abstract: A major impediment to the advancement of sign language translation (SLT) is data scarcity. Much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for. In this work, we propose a two-stage framework for privacy-aware SLT at scale that addresses both of these issues. We introduce SSVP-SLT, which leverages self-supervised video pretraining on anonymized and unannotated videos, followed by supervised SLT finetuning on a curated parallel dataset. SSVP-SLT achieves state-of-the-art finetuned and zero-shot gloss-free SLT performance on the How2Sign dataset, outperforming the strongest respective baselines by over 3 BLEU-4. Based on controlled experiments, we fu
&lt;/p&gt;</description></item><item><title>&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;&#65288;RATP&#65289;&#36890;&#36807;&#22810;&#27493;&#20915;&#31574;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#20197;&#21450;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38544;&#31169;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#22788;&#29702;&#38271;&#25991;&#26412;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#22788;&#29702;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07812</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#24605;&#32500;&#36807;&#31243;&#20316;&#20026;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Thought Process as Sequential Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07812
&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;&#65288;RATP&#65289;&#36890;&#36807;&#22810;&#27493;&#20915;&#31574;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#20197;&#21450;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38544;&#31169;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#22788;&#29702;&#38271;&#25991;&#26412;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#22788;&#29702;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#36741;&#21161;&#20154;&#31867;&#24182;&#23637;&#29616;&#20986;"&#26234;&#33021;&#30340;&#28779;&#33457;"&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20960;&#20010;&#24320;&#25918;&#25361;&#25112;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#65306;&#22914;&#23545;&#38544;&#31169;&#30340;&#20851;&#27880;&#12289;&#20542;&#21521;&#20110;&#20135;&#29983;&#24187;&#35273;&#12289;&#38590;&#20197;&#22788;&#29702;&#38271;&#25991;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;(RATP)&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#33719;&#21462;&#22806;&#37096;&#30693;&#35782;&#65292;RATP&#23558;LLM&#30340;&#24605;&#32771;&#29983;&#25104;&#36807;&#31243;&#23450;&#24335;&#20026;&#22810;&#27493;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#20248;&#21270;&#36825;&#31181;&#24605;&#32771;&#36807;&#31243;&#65292;RATP&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#20010;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#22312;&#22788;&#29702;&#20855;&#26377;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#26102;&#65292;LLM&#35757;&#32451;&#26041;&#27861;&#21463;&#21040;&#20262;&#29702;&#21644;&#23433;&#20840;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;RATP&#22312;&#19978;&#19979;&#25991;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated their strong ability to assist people and show "sparks of intelligence". However, several open challenges hinder their wider application: such as concerns over privacy, tendencies to produce hallucinations, and difficulties in handling long contexts. In this work, we address those challenges by introducing the Retrieval-Augmented Thought Process (RATP). Given access to external knowledge, RATP formulates the thought generation of LLMs as a multiple-step decision process. To optimize such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a Q-value estimator that permits cost-efficient inference. In addressing the task of question-answering with private data, where ethical and security concerns limit LLM training methods, RATP achieves a 50% improvement over existing in-context retrieval-augmented language models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20462;&#25913;&#22522;&#20110;&#36712;&#36857;&#25104;&#26412;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#27169;&#20223;&#22909;&#30340;&#36712;&#36857;&#21644;&#36991;&#20813;&#22351;&#30340;&#36712;&#36857;&#26469;&#25913;&#36827;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2312.10385</link><description>&lt;p&gt;
&#27169;&#20223;&#22909;&#30340;&#24182;&#36991;&#20813;&#22351;&#30340;&#65306;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#22686;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Imitate the Good and Avoid the Bad: An Incremental Approach to Safe Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10385
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20462;&#25913;&#22522;&#20110;&#36712;&#36857;&#25104;&#26412;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#27169;&#20223;&#22909;&#30340;&#36712;&#36857;&#21644;&#36991;&#20813;&#22351;&#30340;&#36712;&#36857;&#26469;&#25913;&#36827;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#25191;&#34892;&#23433;&#20840;&#21160;&#20316;&#30340;&#27969;&#34892;&#26694;&#26550;&#26159;&#32422;&#26463;RL&#65292;&#20854;&#20013;&#21033;&#29992;&#22522;&#20110;&#36712;&#36857;&#30340;&#25104;&#26412;&#32422;&#26463;&#65288;&#25110;&#20854;&#20182;&#25104;&#26412;&#24230;&#37327;&#65289;&#26469;&#25191;&#34892;&#23433;&#20840;&#25805;&#20316;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#22312;&#26368;&#22823;&#21270;&#26399;&#26395;&#22870;&#21169;&#30340;&#21516;&#26102;&#25191;&#34892;&#36825;&#20123;&#32422;&#26463;&#12290;&#26368;&#36817;&#35299;&#20915;&#32422;&#26463;RL&#30340;&#26041;&#27861;&#23558;&#22522;&#20110;&#36712;&#36857;&#30340;&#25104;&#26412;&#32422;&#26463;&#36716;&#25442;&#20026;&#19968;&#20010;&#26367;&#20195;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;RL&#26041;&#27861;&#36827;&#34892;&#36731;&#24494;&#20462;&#25913;&#26469;&#35299;&#20915;&#12290;&#36825;&#31867;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#22312;&#27599;&#20010;&#29366;&#24577;&#19978;&#23545;&#25104;&#26412;&#32422;&#26463;&#36827;&#34892;&#36807;&#24230;&#25110;&#19981;&#36275;&#20272;&#35745;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#19981;&#20462;&#25913;&#22522;&#20110;&#36712;&#36857;&#30340;&#25104;&#26412;&#32422;&#26463;&#65292;&#32780;&#26159;&#27169;&#20223;&#8220;&#22909;&#8221;&#36712;&#36857;&#24182;&#36991;&#20813;&#20174;&#36880;&#27493;&#25913;&#36827;&#30340;&#31574;&#30053;&#29983;&#25104;&#30340;&#8220;&#22351;&#8221;&#36712;&#36857;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;oracle&#65292;&#21033;&#29992;&#22870;&#21169;&#38408;&#20540;&#65288;&#38543;&#23398;&#20064;&#21464;&#21270;&#65289;&#21644;&#25972;&#20307;&#25104;&#26412;&#32422;&#26463;&#26469;&#23558;&#36712;&#36857;&#26631;&#35760;&#20026;&#8220;&#22909;&#8221;&#25110;&#8220;&#22351;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10385v3 Announce Type: replace-cross  Abstract: A popular framework for enforcing safe actions in Reinforcement Learning (RL) is Constrained RL, where trajectory based constraints on expected cost (or other cost measures) are employed to enforce safety and more importantly these constraints are enforced while maximizing expected reward. Most recent approaches for solving Constrained RL convert the trajectory based cost constraint into a surrogate problem that can be solved using minor modifications to RL methods. A key drawback with such approaches is an over or underestimation of the cost constraint at each state. Therefore, we provide an approach that does not modify the trajectory based cost constraint and instead imitates ``good'' trajectories and avoids ``bad'' trajectories generated from incrementally improving policies. We employ an oracle that utilizes a reward threshold (which is varied with learning) and the overall cost constraint to label trajectories as ``good''
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;EBCL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#20013;&#20851;&#38190;&#20107;&#20214;&#21069;&#21518;&#30340;&#25968;&#25454;&#32534;&#30721;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;EBCL&#33021;&#22815;&#20135;&#29983;&#24615;&#33021;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#23545;&#20110;&#24515;&#21147;&#34928;&#31469;&#38431;&#21015;&#30340;&#20851;&#38190;&#19979;&#28216;&#20219;&#21153;&#20855;&#26377;&#26356;&#22909;&#30340;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#33021;&#26377;&#25928;&#22320;&#23558;&#20855;&#26377;&#30456;&#20284;&#39118;&#38505;&#30340;&#24739;&#32773;&#36827;&#34892;&#32858;&#31867;&#12290;</title><link>https://arxiv.org/abs/2312.10308</link><description>&lt;p&gt;
&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Event-Based Contrastive Learning for Medical Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;EBCL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#20013;&#20851;&#38190;&#20107;&#20214;&#21069;&#21518;&#30340;&#25968;&#25454;&#32534;&#30721;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;EBCL&#33021;&#22815;&#20135;&#29983;&#24615;&#33021;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#23545;&#20110;&#24515;&#21147;&#34928;&#31469;&#38431;&#21015;&#30340;&#20851;&#38190;&#19979;&#28216;&#20219;&#21153;&#20855;&#26377;&#26356;&#22909;&#30340;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#33021;&#26377;&#25928;&#22320;&#23558;&#20855;&#26377;&#30456;&#20284;&#39118;&#38505;&#30340;&#24739;&#32773;&#36827;&#34892;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#38656;&#35201;&#30830;&#23450;&#26576;&#20010;&#20851;&#38190;&#21307;&#23398;&#20107;&#20214;&#21518;&#24739;&#32773;&#26159;&#21542;&#22788;&#20110;&#19981;&#33391;&#32467;&#26524;&#30340;&#39640;&#39118;&#38505;&#29366;&#24577;&#65292;&#20363;&#22914;&#24515;&#21147;&#34928;&#31469;&#20837;&#38498;&#21518;&#30340;&#30701;&#26399;&#27515;&#20129;&#39118;&#38505;&#12290;&#36825;&#20010;&#20219;&#21153;&#30001;&#20110;&#38271;&#26399;&#21307;&#23398;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12289;&#21464;&#24322;&#24615;&#21644;&#24322;&#36136;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20687;&#24515;&#21147;&#34928;&#31469;&#36825;&#26679;&#30340;&#24930;&#24615;&#30142;&#30149;&#24739;&#32773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;EBCL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#19981;&#21516;&#31867;&#22411;&#24739;&#32773;&#25968;&#25454;&#30340;&#23884;&#20837;&#34920;&#31034;&#65292;&#20197;&#20445;&#30041;&#20851;&#38190;&#32034;&#24341;&#20107;&#20214;&#21069;&#21518;&#30340;&#26102;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;EBCL&#20135;&#29983;&#30340;&#27169;&#22411;&#22312;&#24515;&#21147;&#34928;&#31469;&#38431;&#21015;&#30340;&#20851;&#38190;&#19979;&#28216;&#20219;&#21153;&#65288;&#21253;&#25324;30&#22825;&#20877;&#20837;&#38498;&#12289;1&#24180;&#27515;&#20129;&#29575;&#21644;1&#21608;&#20303;&#38498;&#22825;&#25968;&#65289;&#30340;&#24494;&#35843;&#24615;&#33021;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;EBCL&#39044;&#35757;&#32451;&#21333;&#29420;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#20855;&#26377;&#30456;&#20284;&#27515;&#20129;&#29575;&#21644;&#20877;&#20837;&#38498;&#39118;&#38505;&#30340;&#24739;&#32773;&#36827;&#34892;&#32858;&#31867;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In clinical practice, one often needs to identify whether a patient is at high risk of adverse outcomes after some key medical event; for example, the short-term risk of death after an admission for heart failure. This task is challenging due to the complexity, variability, and heterogeneity of longitudinal medical data, especially for individuals suffering from chronic diseases like heart failure. In this paper, we introduce Event-Based Contrastive Learning (EBCL), a method for learning embeddings of heterogeneous patient data that preserves temporal information before and after key index events. We demonstrate that EBCL produces models that yield better fine-tuning performance on critical downstream tasks for a heart failure cohort, including 30-day readmission, 1-year mortality, and 1-week length of stay, relative to other pretraining methods. Our findings also reveal that EBCL pretraining alone can effectively cluster patients with similar mortality and readmission risks, offering 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#26410;&#30693;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#19979;&#30340;SHAP&#35780;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2401.12731</link><description>&lt;p&gt;
SHAP&#35780;&#20998;&#22312;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Distributional Uncertainty of the SHAP score in Explainable Machine Learning. (arXiv:2401.12731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#26410;&#30693;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#19979;&#30340;SHAP&#35780;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#23646;&#20998;&#25968;&#21453;&#26144;&#20102;&#36755;&#20837;&#23454;&#20307;&#20013;&#30340;&#29305;&#24449;&#20540;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#12290;&#20854;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#35780;&#20998;&#20043;&#19968;&#26159;SHAP&#35780;&#20998;&#65292;&#23427;&#26159;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#20013;Shapley&#20540;&#30340;&#20855;&#20307;&#23454;&#20363;&#12290;&#35813;&#35780;&#20998;&#30340;&#23450;&#20041;&#20381;&#36182;&#20110;&#23454;&#20307;&#32676;&#20307;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#30001;&#20110;&#36890;&#24120;&#19981;&#30693;&#36947;&#31934;&#30830;&#30340;&#20998;&#24067;&#65292;&#22240;&#27492;&#38656;&#35201;&#20027;&#35266;&#22320;&#36827;&#34892;&#20998;&#37197;&#25110;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#20272;&#35745;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#29305;&#24449;&#35780;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19981;&#30693;&#36947;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#30340;SHAP&#35780;&#20998;&#25512;&#29702;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21253;&#21547;&#28508;&#22312;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#32780;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#25104;&#20026;&#22312;&#35813;&#21306;&#22495;&#19978;&#23450;&#20041;&#30340;&#19968;&#20010;&#20989;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25214;&#21040;&#35813;&#20989;&#25968;&#30340;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attribution scores reflect how important the feature values in an input entity are for the output of a machine learning model. One of the most popular attribution scores is the SHAP score, which is an instantiation of the general Shapley value used in coalition game theory. The definition of this score relies on a probability distribution on the entity population. Since the exact distribution is generally unknown, it needs to be assigned subjectively or be estimated from data, which may lead to misleading feature scores. In this paper, we propose a principled framework for reasoning on SHAP scores under unknown entity population distributions. In our framework, we consider an uncertainty region that contains the potential distributions, and the SHAP score of a feature becomes a function defined over this region. We study the basic problems of finding maxima and minima of this function, which allows us to determine tight ranges for the SHAP scores of all features. In particular, we pinp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#19988;&#20855;&#26222;&#36866;&#24615;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#35843;&#31354;&#38388;&#21644;&#20809;&#35889;&#34920;&#31034;&#65292;&#24341;&#20837;&#20809;&#35889;&#30456;&#20851;&#31995;&#25968;&#30446;&#26631;&#26469;&#25552;&#39640;&#23545;&#20013;&#38454;&#29305;&#24449;&#21644;&#19978;&#19979;&#25991;&#38271;&#31243;&#20381;&#36182;&#30340;&#25429;&#25417;&#33021;&#21147;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10373</link><description>&lt;p&gt;
&#40065;&#26834;&#19988;&#20855;&#26222;&#36866;&#24615;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#31354;&#38388;&#21644;&#20809;&#35889;&#23398;&#20064;&#30340;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
Harmonized Spatial and Spectral Learning for Robust and Generalized Medical Image Segmentation. (arXiv:2401.10373v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#19988;&#20855;&#26222;&#36866;&#24615;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#35843;&#31354;&#38388;&#21644;&#20809;&#35889;&#34920;&#31034;&#65292;&#24341;&#20837;&#20809;&#35889;&#30456;&#20851;&#31995;&#25968;&#30446;&#26631;&#26469;&#25552;&#39640;&#23545;&#20013;&#38454;&#29305;&#24449;&#21644;&#19978;&#19979;&#25991;&#38271;&#31243;&#20381;&#36182;&#30340;&#25429;&#25417;&#33021;&#21147;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31867;&#20869;&#21464;&#24322;&#24615;&#21644;&#31867;&#38388;&#29420;&#31435;&#24615;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#21516;&#19968;&#31867;&#22312;&#19981;&#21516;&#26679;&#26412;&#20013;&#34920;&#29616;&#19981;&#21516;&#65292;&#38590;&#20197;&#25429;&#25417;&#19981;&#21516;&#23545;&#35937;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#39640;&#30340;&#38169;&#35823;&#36127;&#20363;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#35843;&#31354;&#38388;&#21644;&#20809;&#35889;&#34920;&#31034;&#26469;&#22686;&#24378;&#39046;&#22495;&#36890;&#29992;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21019;&#26032;&#30340;&#20809;&#35889;&#30456;&#20851;&#31995;&#25968;&#30446;&#26631;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#25429;&#25417;&#20013;&#38454;&#29305;&#24449;&#21644;&#19978;&#19979;&#25991;&#38271;&#31243;&#20381;&#36182;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#30446;&#26631;&#36890;&#36807;&#34701;&#20837;&#26377;&#20215;&#20540;&#30340;&#20809;&#35889;&#20449;&#24687;&#26469;&#34917;&#20805;&#20256;&#32479;&#30340;&#31354;&#38388;&#30446;&#26631;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#20248;&#21270;&#36825;&#20010;&#30446;&#26631;&#19982;&#29616;&#26377;&#30340;UNet&#21644;TransUNet&#26550;&#26500;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has demonstrated remarkable achievements in medical image segmentation. However, prevailing deep learning models struggle with poor generalization due to (i) intra-class variations, where the same class appears differently in different samples, and (ii) inter-class independence, resulting in difficulties capturing intricate relationships between distinct objects, leading to higher false negative cases. This paper presents a novel approach that synergies spatial and spectral representations to enhance domain-generalized medical image segmentation. We introduce the innovative Spectral Correlation Coefficient objective to improve the model's capacity to capture middle-order features and contextual long-range dependencies. This objective complements traditional spatial objectives by incorporating valuable spectral information. Extensive experiments reveal that optimizing this objective with existing architectures like UNet and TransUNet significantly enhances generalization, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#35270;&#35273;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#30452;&#35266;&#29289;&#29702;&#12289;&#22240;&#26524;&#25512;&#29702;&#21644;&#30452;&#35266;&#24515;&#29702;&#39046;&#22495;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#23545;&#29031;&#23454;&#39564;&#21457;&#29616;&#65292;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#35299;&#37322;&#35270;&#35273;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#29087;&#32451;&#24615;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#30452;&#35266;&#29289;&#29702;&#12289;&#22240;&#26524;&#25512;&#29702;&#21644;&#30452;&#35266;&#24515;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#36824;&#26377;&#24453;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2311.16093</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#35748;&#30693;
&lt;/p&gt;
&lt;p&gt;
Visual cognition in multimodal large language models. (arXiv:2311.16093v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#35270;&#35273;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#30452;&#35266;&#29289;&#29702;&#12289;&#22240;&#26524;&#25512;&#29702;&#21644;&#30452;&#35266;&#24515;&#29702;&#39046;&#22495;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#23545;&#29031;&#23454;&#39564;&#21457;&#29616;&#65292;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#35299;&#37322;&#35270;&#35273;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#29087;&#32451;&#24615;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#30452;&#35266;&#29289;&#29702;&#12289;&#22240;&#26524;&#25512;&#29702;&#21644;&#30452;&#35266;&#24515;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#36824;&#26377;&#24453;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#26500;&#24314;&#20687;&#20154;&#31867;&#19968;&#26679;&#24605;&#32771;&#30340;&#26426;&#22120;&#12290;&#28982;&#32780;&#25454;&#35748;&#20026;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26080;&#27861;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#30740;&#31350;&#20154;&#21592;&#25351;&#20986;&#36825;&#20123;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#12289;&#30452;&#35266;&#29289;&#29702;&#21644;&#30452;&#35266;&#24515;&#29702;&#31561;&#39046;&#22495;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#38754;&#21521;&#35270;&#35273;&#22788;&#29702;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#37325;&#26032;&#24341;&#36215;&#20102;&#23545;&#27169;&#25311;&#20154;&#31867;&#31867;&#20284;&#35748;&#30693;&#33021;&#21147;&#28508;&#21147;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#35270;&#35273;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#30452;&#35266;&#29289;&#29702;&#12289;&#22240;&#26524;&#25512;&#29702;&#21644;&#30452;&#35266;&#24515;&#29702;&#39046;&#22495;&#30340;&#24403;&#21069;&#29366;&#24577;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#23545;&#29031;&#23454;&#39564;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20123;&#29616;&#20195;&#27169;&#22411;&#22312;&#29702;&#35299;&#22797;&#26434;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#12289;&#22240;&#26524;&#20851;&#31995;&#21644;&#23545;&#20182;&#20154;&#20559;&#22909;&#30340;&#30452;&#35266;&#29702;&#35299;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;,&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#35299;&#37322;&#35270;&#35273;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#29087;&#32451;&#24615;,&#28982;&#32780;&#23427;&#20204;&#22312;&#30452;&#35266;&#29289;&#29702;&#12289;&#22240;&#26524;&#25512;&#29702;&#21644;&#30452;&#35266;&#24515;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#36824;&#26377;&#24453;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
A chief goal of artificial intelligence is to build machines that think like people. Yet it has been argued that deep neural network architectures fail to accomplish this. Researchers have asserted these models' limitations in the domains of causal reasoning, intuitive physics, and intuitive psychology. Yet recent advancements, namely the rise of large language models, particularly those designed for visual processing, have rekindled interest in the potential to emulate human-like cognitive abilities. This paper evaluates the current state of vision-based large language models in the domains of intuitive physics, causal reasoning, and intuitive psychology. Through a series of controlled experiments, we investigate the extent to which these modern models grasp complex physical interactions, causal relationships, and intuitive understanding of others' preferences. Our findings reveal that, while these models demonstrate a notable proficiency in processing and interpreting visual data, th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21333;&#20301;&#21333;&#32431;&#24418;&#36827;&#34892;&#20984;&#26494;&#24347;&#65292;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#38236;&#20687;&#19979;&#38477;&#26041;&#26696;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#22312;&#30456;&#20851;&#39640;&#26031;Wigner&#27169;&#22411;&#19979;&#65292;&#21333;&#32431;&#24418;&#26494;&#24347;&#27861;&#20855;&#26377;&#21807;&#19968;&#35299;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#24674;&#22797;&#22320;&#38754;&#30495;&#23454;&#25490;&#21015;&#12290;</title><link>http://arxiv.org/abs/2310.20609</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#21333;&#32431;&#24418;&#36827;&#34892;&#20984;&#26494;&#24347;&#35299;&#20915;&#22270;&#21305;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph Matching via convex relaxation to the simplex. (arXiv:2310.20609v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21333;&#20301;&#21333;&#32431;&#24418;&#36827;&#34892;&#20984;&#26494;&#24347;&#65292;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#38236;&#20687;&#19979;&#38477;&#26041;&#26696;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#22312;&#30456;&#20851;&#39640;&#26031;Wigner&#27169;&#22411;&#19979;&#65292;&#21333;&#32431;&#24418;&#26494;&#24347;&#27861;&#20855;&#26377;&#21807;&#19968;&#35299;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#24674;&#22797;&#22320;&#38754;&#30495;&#23454;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#21305;&#37197;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#35813;&#38382;&#39064;&#21253;&#25324;&#22312;&#20004;&#20010;&#36755;&#20837;&#22270;&#20043;&#38388;&#25214;&#21040;&#26368;&#20339;&#23545;&#40784;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#32593;&#32476;&#21435;&#21311;&#21517;&#21270;&#21644;&#34507;&#30333;&#36136;&#23545;&#40784;&#31561;&#39046;&#22495;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#36890;&#36807;&#23545;NP&#38590;&#38382;&#39064;&#8220;&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;&#8221;&#65288;QAP&#65289;&#36827;&#34892;&#20984;&#26494;&#24347;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20984;&#26494;&#24347;&#26041;&#27861;&#65292;&#21363;&#23545;&#21333;&#20301;&#21333;&#32431;&#24418;&#36827;&#34892;&#26494;&#24347;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#38381;&#21512;&#36845;&#20195;&#24418;&#24335;&#30340;&#39640;&#25928;&#38236;&#20687;&#19979;&#38477;&#26041;&#26696;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#22312;&#30456;&#20851;&#39640;&#26031;Wigner&#27169;&#22411;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#32431;&#24418;&#26494;&#24347;&#27861;&#22312;&#39640;&#27010;&#29575;&#19979;&#20855;&#26377;&#21807;&#19968;&#35299;&#12290;&#22312;&#26080;&#22122;&#22768;&#24773;&#20917;&#19979;&#65292;&#36825;&#34987;&#35777;&#26126;&#21487;&#20197;&#31934;&#30830;&#24674;&#22797;&#22320;&#38754;&#30495;&#23454;&#25490;&#21015;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#36755;&#20837;&#30697;&#38453;&#20551;&#35774;&#26465;&#20214;&#65292;&#29992;&#20110;&#26631;&#20934;&#36138;&#24515;&#21462;&#25972;&#26041;&#27861;&#65292;&#24182;&#19988;&#36825;&#20010;&#26465;&#20214;&#27604;&#24120;&#29992;&#30340;&#8220;&#23545;&#35282;&#32447;&#20248;&#21183;&#8221;&#26465;&#20214;&#26356;&#23485;&#26494;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#26465;&#20214;&#35777;&#26126;&#20102;&#22320;&#38754;&#30495;&#23454;&#25490;&#21015;&#30340;&#31934;&#30830;&#19968;&#27493;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the Graph Matching problem, which consists of finding the best possible alignment between two input graphs, and has many applications in computer vision, network deanonymization and protein alignment. A common approach to tackle this problem is through convex relaxations of the NP-hard \emph{Quadratic Assignment Problem} (QAP).  Here, we introduce a new convex relaxation onto the unit simplex and develop an efficient mirror descent scheme with closed-form iterations for solving this problem. Under the correlated Gaussian Wigner model, we show that the simplex relaxation admits a unique solution with high probability. In the noiseless case, this is shown to imply exact recovery of the ground truth permutation. Additionally, we establish a novel sufficiency condition for the input matrix in standard greedy rounding methods, which is less restrictive than the commonly used `diagonal dominance' condition. We use this condition to show exact one-step recovery of the gro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65288;GuDA&#65289;&#29992;&#20110;&#25552;&#39640;&#28436;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18247</link><description>&lt;p&gt;
&#20026;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#25552;&#20379;&#25351;&#23548;&#24615;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Guided Data Augmentation for Offline Reinforcement Learning and Imitation Learning. (arXiv:2310.18247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18247
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65288;GuDA&#65289;&#29992;&#20110;&#25552;&#39640;&#28436;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#31034;&#23398;&#20064;&#26159;&#19968;&#31181;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#26469;&#23398;&#20064;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#30340;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#19987;&#23478;&#32423;&#28436;&#31034;&#30340;&#38590;&#24230;&#38480;&#21046;&#20102;&#28436;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65306;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#25910;&#38598;&#36890;&#24120;&#24456;&#26114;&#36149;&#65292;&#24182;&#19988;&#28436;&#31034;&#30340;&#36136;&#37327;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#28436;&#31034;&#32773;&#30340;&#33021;&#21147;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#19968;&#20123;&#24037;&#20316;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#24265;&#20215;&#29983;&#25104;&#39069;&#22806;&#30340;&#28436;&#31034;&#25968;&#25454;&#65292;&#20294;&#22823;&#22810;&#25968;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20197;&#38543;&#26426;&#26041;&#24335;&#29983;&#25104;&#22686;&#24378;&#25968;&#25454;&#65292;&#26368;&#32456;&#20135;&#29983;&#39640;&#24230;&#27425;&#20248;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65288;GuDA&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22686;&#24378;&#25968;&#25454;&#12290;GuDA&#30340;&#20851;&#38190;&#27934;&#35265;&#26159;&#65292;&#34429;&#28982;&#28436;&#31034;&#21160;&#20316;&#24207;&#21015;&#21487;&#33021;&#24456;&#38590;&#23637;&#31034;&#20135;&#29983;&#19987;&#23478;&#25968;&#25454;&#25152;&#38656;&#30340;&#21160;&#20316;&#24207;&#21015;&#65292;&#20294;&#29992;&#25143;&#32463;&#24120;&#21487;&#20197;&#36731;&#26494;&#22320;&#36776;&#21035;&#20986;&#22686;&#24378;&#36712;&#36857;&#27573;&#34920;&#31034;&#30340;&#20219;&#21153;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#29992;&#25143;&#21487;&#20197;&#26045;&#21152;&#19968;&#31995;&#21015;s
&lt;/p&gt;
&lt;p&gt;
Learning from demonstration (LfD) is a popular technique that uses expert demonstrations to learn robot control policies. However, the difficulty in acquiring expert-quality demonstrations limits the applicability of LfD methods: real-world data collection is often costly, and the quality of the demonstrations depends greatly on the demonstrator's abilities and safety concerns. A number of works have leveraged data augmentation (DA) to inexpensively generate additional demonstration data, but most DA works generate augmented data in a random fashion and ultimately produce highly suboptimal data. In this work, we propose Guided Data Augmentation (GuDA), a human-guided DA framework that generates expert-quality augmented data. The key insight of GuDA is that while it may be difficult to demonstrate the sequence of actions required to produce expert data, a user can often easily identify when an augmented trajectory segment represents task progress. Thus, the user can impose a series of s
&lt;/p&gt;</description></item><item><title>COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04353</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Language-Agent Approach to Formal Theorem-Proving. (arXiv:2310.04353v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04353
&lt;/p&gt;
&lt;p&gt;
COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#19982;&#22806;&#37096;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25511;&#21046;&#20219;&#21153;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents, which use a large language model (LLM) capable of in-context learning to interact with an external environment, have recently emerged as a promising approach to control tasks. We present the first language-agent approach to formal theorem-proving. Our method, COPRA, uses a high-capacity, black-box LLM (GPT-4) as part of a policy for a stateful backtracking search. During the search, the policy can select proof tactics and retrieve lemmas and definitions from an external database. Each selected tactic is executed in the underlying proof framework, and the execution feedback is used to build the prompt for the next policy invocation. The search also tracks selected information from its history and uses it to reduce hallucinations and unnecessary LLM queries.  We evaluate COPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the Compcert project. On these benchmarks, COPRA is significantly better than one-shot invocations of GPT-4, as well as state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#29992;&#20110;&#28418;&#31227;&#25511;&#21046;&#39640;&#32500;RBMs&#12290;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11651</link><description>&lt;p&gt;
&#39640;&#32500;RBM&#30340;&#28418;&#31227;&#25511;&#21046;&#65306;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Drift Control of High-Dimensional RBM: A Computational Method Based on Neural Networks. (arXiv:2309.11651v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11651
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#29992;&#20110;&#28418;&#31227;&#25511;&#21046;&#39640;&#32500;RBMs&#12290;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#25490;&#38431;&#29702;&#35770;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#29366;&#24577;&#31354;&#38388;&#20026;d&#32500;&#27491;&#21322;&#36724;&#30340;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#12290;&#25511;&#21046;&#36807;&#31243;Z&#25353;&#29031;&#19968;&#20010;&#21453;&#23556;&#24067;&#26391;&#36816;&#21160;&#28436;&#21270;&#65292;&#20854;&#21327;&#26041;&#24046;&#30697;&#38453;&#26159;&#22806;&#29983;&#25351;&#23450;&#30340;&#65292;&#21453;&#23556;&#26041;&#21521;&#26159;&#20174;&#27491;&#21322;&#36724;&#36793;&#30028;&#34920;&#38754;&#21453;&#23556;&#12290;&#31995;&#32479;&#31649;&#29702;&#21592;&#26681;&#25454;Z&#30340;&#21382;&#21490;&#36873;&#25321;&#27599;&#20010;&#26102;&#38388;&#28857;t&#19978;&#30340;&#28418;&#31227;&#21521;&#37327;&#952;(t)&#65292;&#32780;&#26102;&#38388;&#28857;t&#19978;&#30340;&#25104;&#26412;&#29575;&#21462;&#20915;&#20110;Z(t)&#21644;&#952;(t)&#12290;&#22312;&#25105;&#20204;&#30340;&#21021;&#22987;&#38382;&#39064;&#34920;&#36848;&#20013;&#65292;&#30446;&#26631;&#26159;&#22312;&#26080;&#38480;&#35268;&#21010;&#26102;&#38388;&#33539;&#22260;&#20869;&#26368;&#23567;&#21270;&#26399;&#26395;&#36148;&#29616;&#25104;&#26412;&#65292;&#20043;&#21518;&#25105;&#20204;&#22788;&#29702;&#30456;&#24212;&#30340;&#20154;&#22343;&#25511;&#21046;&#38382;&#39064;&#12290;&#20511;&#37492;&#38889;&#28023;&#20142;&#31561;&#20154;&#65288;&#22269;&#23478;&#31185;&#23398;&#38498;&#23398;&#25253;&#65292;2018, 8505-8510&#65289;&#30340;&#26089;&#26399;&#24037;&#20316;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#25105;&#20204;&#30740;&#31350;&#30340;&#27979;&#35797;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#31934;&#24230;&#22312;&#19968;&#20010;&#23567;&#25968;&#33539;&#22260;&#20869;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by applications in queueing theory, we consider a stochastic control problem whose state space is the $d$-dimensional positive orthant. The controlled process $Z$ evolves as a reflected Brownian motion whose covariance matrix is exogenously specified, as are its directions of reflection from the orthant's boundary surfaces. A system manager chooses a drift vector $\theta(t)$ at each time $t$ based on the history of $Z$, and the cost rate at time $t$ depends on both $Z(t)$ and $\theta(t)$. In our initial problem formulation, the objective is to minimize expected discounted cost over an infinite planning horizon, after which we treat the corresponding ergodic control problem. Extending earlier work by Han et al. (Proceedings of the National Academy of Sciences, 2018, 8505-8510), we develop and illustrate a simulation-based computational method that relies heavily on deep neural network technology. For test problems studied thus far, our method is accurate to within a fraction o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37327;&#21270;&#35780;&#20272;&#20998;&#31867;&#38543;&#26426;&#26862;&#26519;&#30340;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#65292;&#21457;&#29616;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;&#27604;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#26356;&#25509;&#36817;&#30495;&#23454;&#35823;&#24046;&#29575;&#65292;&#24182;&#19988;&#36825;&#19968;&#32467;&#26524;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35823;&#24046;&#20272;&#35745;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.00736</link><description>&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#20013;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Prediction Error Estimation in Random Forests. (arXiv:2309.00736v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37327;&#21270;&#35780;&#20272;&#20998;&#31867;&#38543;&#26426;&#26862;&#26519;&#30340;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#65292;&#21457;&#29616;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;&#27604;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#26356;&#25509;&#36817;&#30495;&#23454;&#35823;&#24046;&#29575;&#65292;&#24182;&#19988;&#36825;&#19968;&#32467;&#26524;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35823;&#24046;&#20272;&#35745;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#37327;&#35780;&#20272;&#20102;&#20998;&#31867;&#38543;&#26426;&#26862;&#26519;&#30340;&#35823;&#24046;&#20272;&#35745;&#12290;&#22312;Bates&#31561;&#20154;&#65288;2023&#24180;&#65289;&#24314;&#31435;&#30340;&#21021;&#27493;&#29702;&#35770;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#20174;&#29702;&#35770;&#21644;&#32463;&#39564;&#35282;&#24230;&#25506;&#35752;&#20102;&#38543;&#26426;&#26862;&#26519;&#20013;&#24120;&#35265;&#30340;&#21508;&#31181;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#22312;&#30495;&#23454;&#35823;&#24046;&#29575;&#21644;&#26399;&#26395;&#35823;&#24046;&#29575;&#26041;&#38754;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20998;&#31867;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;&#24179;&#22343;&#26356;&#25509;&#36817;&#30495;&#23454;&#35823;&#24046;&#29575;&#65292;&#32780;&#19981;&#26159;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#12290;&#19982;Bates&#31561;&#20154;&#65288;2023&#24180;&#65289;&#23545;&#36923;&#36753;&#22238;&#24402;&#30340;&#30740;&#31350;&#32467;&#26524;&#30456;&#21453;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#36825;&#20010;&#32467;&#26524;&#36866;&#29992;&#20110;&#20132;&#21449;&#39564;&#35777;&#12289;&#33258;&#20030;&#21644;&#25968;&#25454;&#21010;&#20998;&#31561;&#19981;&#21516;&#30340;&#35823;&#24046;&#20272;&#35745;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, error estimates of classification Random Forests are quantitatively assessed. Based on the initial theoretical framework built by Bates et al. (2023), the true error rate and expected error rate are theoretically and empirically investigated in the context of a variety of error estimation methods common to Random Forests. We show that in the classification case, Random Forests' estimates of prediction error is closer on average to the true error rate instead of the average prediction error. This is opposite the findings of Bates et al. (2023) which were given for logistic regression. We further show that this result holds across different error estimation strategies such as cross-validation, bagging, and data splitting.
&lt;/p&gt;</description></item><item><title>CALM&#26159;&#19968;&#20010;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30456;&#27604;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.12539</link><description>&lt;p&gt;
CALM: &#19968;&#31181;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias. (arXiv:2308.12539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12539
&lt;/p&gt;
&lt;p&gt;
CALM&#26159;&#19968;&#20010;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30456;&#27604;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#19981;&#26029;&#22686;&#24378;&#65292;&#37327;&#21270;&#21644;&#27604;&#36739;&#23427;&#20204;&#22312;&#31038;&#20250;&#21644;&#20154;&#21475;&#23398;&#20559;&#35265;&#26041;&#38754;&#30340;&#33021;&#21147;&#20197;&#21450;&#28508;&#22312;&#30340;&#21361;&#23475;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#20559;&#35265;&#27979;&#37327;&#25968;&#25454;&#38598;&#23545;&#20110;&#20154;&#24037;&#35774;&#35745;&#27169;&#26495;&#30340;&#25200;&#21160;&#25935;&#24863;&#65292;&#22240;&#27492;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#20445;&#35777;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#65288;CALM&#65289;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#37327;&#21270;LMs&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#26032;&#38395;&#25991;&#31456;&#65289;&#30340;16&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#36807;&#28388;&#20986;224&#20010;&#27169;&#26495;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;78,400&#20010;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24179;&#22343;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#27169;&#26495;&#38271;&#24230;&#30340;&#21464;&#24322;&#31243;&#24230;&#31561;&#25351;&#26631;&#65292;&#27604;&#36739;CALM&#19982;&#20808;&#21069;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#27979;&#35797;&#20854;&#23545;&#32454;&#24494;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30456;&#23545;&#20110;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#22240;&#27492;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;20&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models (LMs) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. Prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. To achieve reliability, we introduce the Comprehensive Assessment of Language Model bias (CALM), a benchmark dataset to quantify bias in LMs across three tasks. We integrate 16 existing datasets across different domains, such as Wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. We compare the diversity of CALM with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. We show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. We evaluate 20 large language 
&lt;/p&gt;</description></item><item><title>Safe DreamerV3&#26159;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#32500;&#24230;&#21644;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#20960;&#20046;&#38646;&#25104;&#26412;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.07176</link><description>&lt;p&gt;
Safe DreamerV3&#65306;&#24102;&#26377;&#19990;&#30028;&#27169;&#22411;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe DreamerV3: Safe Reinforcement Learning with World Models. (arXiv:2307.07176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07176
&lt;/p&gt;
&lt;p&gt;
Safe DreamerV3&#26159;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#32500;&#24230;&#21644;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#20960;&#20046;&#38646;&#25104;&#26412;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#36824;&#27809;&#26377;&#23454;&#29616;, &#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#20854;&#26410;&#33021;&#28385;&#36275;&#36825;&#20123;&#31995;&#32479;&#30340;&#22522;&#26412;&#23433;&#20840;&#38656;&#27714;&#12290;&#29616;&#26377;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#25104;&#26412;&#20989;&#25968;&#26469;&#22686;&#24378;&#23433;&#20840;&#24615;&#65292;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#65292;&#21253;&#25324;&#20165;&#37319;&#29992;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#65292;&#21363;&#20351;&#36827;&#34892;&#20840;&#38754;&#30340;&#25968;&#25454;&#37319;&#26679;&#21644;&#35757;&#32451;&#65292;&#20063;&#26080;&#27861;&#23454;&#29616;&#38646;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Safe DreamerV3&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#35745;&#21010;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#26032;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#22312;SafeRL&#20013;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#36827;&#27493;&#65292;&#26159;&#31532;&#19968;&#20010;&#22312;Safety-Gymnasium&#22522;&#20934;&#20013;&#23454;&#29616;&#36817;&#20046;&#38646;&#25104;&#26412;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#32593;&#31449;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#65306;https://sites.google.com/view/safedreamerv3&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread application of Reinforcement Learning (RL) in real-world situations is yet to come to fruition, largely as a result of its failure to satisfy the essential safety demands of such systems. Existing safe reinforcement learning (SafeRL) methods, employing cost functions to enhance safety, fail to achieve zero-cost in complex scenarios, including vision-only tasks, even with comprehensive data sampling and training. To address this, we introduce Safe DreamerV3, a novel algorithm that integrates both Lagrangian-based and planning-based methods within a world model. Our methodology represents a significant advancement in SafeRL as the first algorithm to achieve nearly zero-cost in both low-dimensional and vision-only tasks within the Safety-Gymnasium benchmark. Our project website can be found in: https://sites.google.com/view/safedreamerv3.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02694</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24230;&#37327;&#26041;&#27861;&#65306;&#19968;&#39033;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Loss Functions and Metrics in Deep Learning. A Review. (arXiv:2307.02694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#36873;&#25321;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#24230;&#37327;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#23427;&#20204;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#35780;&#35770;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#26368;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the essential components of deep learning is the choice of the loss function and performance metrics used to train and evaluate models. This paper reviews the most prevalent loss functions and performance measurements in deep learning. We examine the benefits and limits of each technique and illustrate their application to various deep-learning problems. Our review aims to give a comprehensive picture of the different loss functions and performance indicators used in the most common deep learning tasks and help practitioners choose the best method for their specific task.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#31216;&#25968;&#25454;&#19978;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#31561;&#21464;&#21644;&#22686;&#24378;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#32570;&#28857;&#65292;&#35777;&#26126;&#20102;&#22312;&#33258;&#28982;&#20551;&#35774;&#19979;&#31561;&#21464;&#31283;&#23450;&#28857;&#30340;&#38598;&#21512;&#21644;&#31561;&#21464;&#23618;&#30340;&#38598;&#21512;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#20294;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#23450;&#28857;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.13458</link><description>&lt;p&gt;
&#31561;&#21464;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Optimization Dynamics of Equivariant and Augmented Neural Networks. (arXiv:2303.13458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#31216;&#25968;&#25454;&#19978;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#31561;&#21464;&#21644;&#22686;&#24378;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#32570;&#28857;&#65292;&#35777;&#26126;&#20102;&#22312;&#33258;&#28982;&#20551;&#35774;&#19979;&#31561;&#21464;&#31283;&#23450;&#28857;&#30340;&#38598;&#21512;&#21644;&#31561;&#21464;&#23618;&#30340;&#38598;&#21512;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#20294;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#23450;&#28857;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23545;&#31216;&#25968;&#25454;&#19978;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#38480;&#21046;&#26550;&#26500;&#31561;&#21464;&#21644;&#20351;&#29992;&#22686;&#24378;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#23545;&#25439;&#22833;&#21644;&#38750;&#32447;&#24615;&#24615;&#36827;&#34892;&#33258;&#28982;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#31561;&#21464;&#31283;&#23450;&#28857;&#30340;&#38598;&#21512;&#23545;&#20110;&#36825;&#20004;&#31181;&#31574;&#30053;&#26159;&#30456;&#21516;&#30340;&#65292;&#24182;&#19988;&#31561;&#21464;&#23618;&#30340;&#38598;&#21512;&#22312;&#22686;&#24378;&#27169;&#22411;&#30340;&#26799;&#24230;&#27969;&#19979;&#26159;&#19981;&#21464;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#23613;&#31649;&#31561;&#21464;&#27169;&#22411;&#30340;&#31283;&#23450;&#28857;&#26159;&#31283;&#23450;&#30340;&#65292;&#22686;&#24378;&#35757;&#32451;&#30340;&#31283;&#23450;&#28857;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the optimization of multilayer perceptrons on symmetric data. We compare the strategy of constraining the architecture to be equivariant to that of using augmentation. We show that, under natural assumptions on the loss and non-linearities, the sets of equivariant stationary points are identical for the two strategies, and that the set of equivariant layers is invariant under the gradient flow for augmented models. Finally, we show that stationary points may be unstable for augmented training although they are stable for the equivariant models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20266;&#24433;&#38477;&#22122;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#31232;&#30095;&#35270;&#22270;&#19979;&#33258;&#21160;&#20986;&#34880;&#26816;&#27979;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#19982;&#23436;&#20840;&#37319;&#26679;&#30340;&#22270;&#20687;&#36827;&#34892;&#21516;&#31561;&#31934;&#30830;&#24230;&#30340;&#20998;&#31867;&#21644;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.09340</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20266;&#24433;&#38477;&#22122;&#30340;&#31232;&#30095;&#35270;&#22270;CT&#22270;&#20687;&#33258;&#21160;&#20986;&#34880;&#26816;&#27979;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Automated Hemorrhage Detection in Sparse-view Computed Tomography via Deep Convolutional Neural Network based Artifact Reduction. (arXiv:2303.09340v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20266;&#24433;&#38477;&#22122;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#31232;&#30095;&#35270;&#22270;&#19979;&#33258;&#21160;&#20986;&#34880;&#26816;&#27979;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#19982;&#23436;&#20840;&#37319;&#26679;&#30340;&#22270;&#20687;&#36827;&#34892;&#21516;&#31561;&#31934;&#30830;&#24230;&#30340;&#20998;&#31867;&#21644;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39045;&#20869;&#20986;&#34880;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#38656;&#35201;&#24555;&#36895;&#19988;&#24120;&#24120;&#38750;&#24120;&#23494;&#38598;&#30340;&#21307;&#30103;&#27835;&#30103;&#12290;&#20026;&#20102;&#35786;&#26029;&#65292;&#36890;&#24120;&#35201;&#36827;&#34892;&#39045;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CCT&#65289;&#25195;&#25551;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36752;&#23556;&#24341;&#36215;&#30340;&#22686;&#21152;&#30340;&#20581;&#24247;&#39118;&#38505;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#38477;&#20302;&#36825;&#31181;&#28508;&#22312;&#39118;&#38505;&#30340;&#26368;&#37325;&#35201;&#31574;&#30053;&#26159;&#23613;&#21487;&#33021;&#20445;&#25345;&#36752;&#23556;&#21058;&#37327;&#20302;&#65292;&#24182;&#19982;&#35786;&#26029;&#20219;&#21153;&#19968;&#33268;&#12290; &#31232;&#30095;&#35270;&#22270;CT&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#25152;&#37319;&#38598;&#30340;&#35270;&#22270;&#24635;&#25968;&#65292;&#20174;&#32780;&#38477;&#20302;&#21058;&#37327;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#20294;&#20195;&#20215;&#26159;&#38477;&#20302;&#22270;&#20687;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;U-Net&#26550;&#26500;&#26469;&#20943;&#23569;&#31232;&#30095;&#35270;&#22270;CCT&#30340;&#20266;&#24433;&#65292;&#20174;&#31232;&#30095;&#35270;&#22270;&#20013;&#39044;&#27979;&#23436;&#20840;&#37319;&#26679;&#30340;&#37325;&#24314;&#22270;&#20687;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#20986;&#34880;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#23436;&#20840;&#37319;&#26679;&#30340;CCT&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20266;&#24433;&#38477;&#22122;&#21518;&#30340;CCT&#22270;&#20687;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#21644;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#19982;&#23436;&#20840;&#37319;&#26679;&#30340;CCT&#22270;&#20687;&#27809;&#26377;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intracranial hemorrhage poses a serious health problem requiring rapid and often intensive medical treatment. For diagnosis, a Cranial Computed Tomography (CCT) scan is usually performed. However, the increased health risk caused by radiation is a concern. The most important strategy to reduce this potential risk is to keep the radiation dose as low as possible and consistent with the diagnostic task. Sparse-view CT can be an effective strategy to reduce dose by reducing the total number of views acquired, albeit at the expense of image quality. In this work, we use a U-Net architecture to reduce artifacts from sparse-view CCTs, predicting fully sampled reconstructions from sparse-view ones. We evaluate the hemorrhage detectability in the predicted CCTs with a hemorrhage classification convolutional neural network, trained on fully sampled CCTs to detect and classify different sub-types of hemorrhages. Our results suggest that the automated classification and detection accuracy of hemo
&lt;/p&gt;</description></item></channel></rss>