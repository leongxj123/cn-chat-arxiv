<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#20010;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#39046;&#22495;&#12290;&#26412;&#32508;&#36848;&#20998;&#31867;&#21644;&#20998;&#26512;&#20102;&#24050;&#26377;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;GNNs&#30340;&#26126;&#30830;&#25351;&#23548;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01138</link><description>&lt;p&gt;
&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#20010;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks in EEG-based Emotion Recognition: A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01138
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#20010;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#39046;&#22495;&#12290;&#26412;&#32508;&#36848;&#20998;&#31867;&#21644;&#20998;&#26512;&#20102;&#24050;&#26377;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;GNNs&#30340;&#26126;&#30830;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#23545;&#20110;&#20854;&#20182;&#27169;&#24335;&#65292;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#24773;&#32490;&#35782;&#21035;&#21487;&#20197;&#30452;&#35266;&#22320;&#21709;&#24212;&#20154;&#33041;&#20013;&#30340;&#24773;&#32490;&#27169;&#24335;&#65292;&#22240;&#27492;&#25104;&#20026;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#39046;&#22495;&#26368;&#20851;&#27880;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#30001;&#20110;&#22823;&#33041;&#21306;&#22495;&#20043;&#38388;&#30340;&#20381;&#36182;&#19982;&#24773;&#32490;&#23494;&#20999;&#30456;&#20851;&#65292;&#22240;&#27492;&#21457;&#23637;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#24773;&#32490;&#35782;&#21035;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#24773;&#32490;&#24615;&#33041;&#30005;&#22270;&#20013;&#30340;&#22823;&#33041;&#21306;&#22495;&#20381;&#36182;&#20855;&#26377;&#29983;&#29702;&#22522;&#30784;&#65292;&#20351;&#24471;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;GNNs&#19982;&#20854;&#20182;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;GNNs&#26377;&#25152;&#21306;&#21035;&#12290;&#27492;&#22806;&#65292;&#22312;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#24773;&#32490;&#35782;&#21035;&#20013;&#26082;&#27809;&#26377;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#20063;&#27809;&#26377;&#26500;&#24314;GNNs&#30340;&#25351;&#23548;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;&#24050;&#26377;&#26041;&#27861;&#22312;&#22270;&#26500;&#36896;&#30340;&#32479;&#19968;&#26694;&#26550;&#19979;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#25581;&#31034;&#20986;&#20854;&#20849;&#21516;&#28857;&#21644;&#24046;&#24322;&#12290;&#25105;&#20204;&#20174;&#26694;&#26550;&#30340;&#19977;&#20010;&#38454;&#27573;&#20998;&#26512;&#21644;&#20998;&#31867;&#26041;&#27861;&#65292;&#20026;&#26500;&#24314;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;GNNs&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#25351;&#23548;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#19968;&#20123;...
&lt;/p&gt;
&lt;p&gt;
Compared to other modalities, EEG-based emotion recognition can intuitively respond to the emotional patterns in the human brain and, therefore, has become one of the most concerning tasks in the brain-computer interfaces field. Since dependencies within brain regions are closely related to emotion, a significant trend is to develop Graph Neural Networks (GNNs) for EEG-based emotion recognition. However, brain region dependencies in emotional EEG have physiological bases that distinguish GNNs in this field from those in other time series fields. Besides, there is neither a comprehensive review nor guidance for constructing GNNs in EEG-based emotion recognition. In the survey, our categorization reveals the commonalities and differences of existing approaches under a unified framework of graph construction. We analyze and categorize methods from three stages in the framework to provide clear guidance on constructing GNNs in EEG-based emotion recognition. In addition, we discuss several 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#25289;&#32454;&#23391;&#21010;&#20998;&#38598;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#22240;&#23376;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#20272;&#35745;&#24322;&#36136;&#24615;&#65292;&#24182;&#23558;&#22240;&#23376;&#31354;&#38388;&#21010;&#20998;&#25104;&#21327;&#21464;&#37327;&#32452;&#21512;&#30340;&#8220;&#27744;&#8221;&#65292;&#20197;&#20415;&#21306;&#20998;&#32467;&#26524;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2404.02141</link><description>&lt;p&gt;
&#20351;&#29992;&#25289;&#32454;&#23391;&#21010;&#20998;&#22312;&#22240;&#23376;&#25968;&#25454;&#20013;&#31283;&#20581;&#20272;&#35745;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustly estimating heterogeneity in factorial data using Rashomon Partitions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02141
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#25289;&#32454;&#23391;&#21010;&#20998;&#38598;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#22240;&#23376;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#20272;&#35745;&#24322;&#36136;&#24615;&#65292;&#24182;&#23558;&#22240;&#23376;&#31354;&#38388;&#21010;&#20998;&#25104;&#21327;&#21464;&#37327;&#32452;&#21512;&#30340;&#8220;&#27744;&#8221;&#65292;&#20197;&#20415;&#21306;&#20998;&#32467;&#26524;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32479;&#35745;&#20998;&#26512;&#65292;&#26080;&#35770;&#26159;&#22312;&#35266;&#27979;&#25968;&#25454;&#36824;&#26159;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#65292;&#37117;&#20250;&#38382;&#65306;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#22914;&#20309;&#38543;&#21487;&#35266;&#23519;&#21327;&#21464;&#37327;&#32452;&#21512;&#21464;&#21270;&#65311;&#19981;&#21516;&#30340;&#33647;&#29289;&#32452;&#21512;&#22914;&#20309;&#24433;&#21709;&#20581;&#24247;&#32467;&#26524;&#65292;&#31185;&#25216;&#37319;&#32435;&#22914;&#20309;&#20381;&#36182;&#28608;&#21169;&#21644;&#20154;&#21475;&#32479;&#35745;&#23398;&#65311;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#36825;&#20010;&#22240;&#23376;&#31354;&#38388;&#21010;&#20998;&#25104;&#21327;&#21464;&#37327;&#32452;&#21512;&#30340;&#8220;&#27744;&#8221;&#65292;&#22312;&#36825;&#20123;&#27744;&#20013;&#32467;&#26524;&#20250;&#21457;&#29983;&#24046;&#24322;&#65288;&#20294;&#27744;&#20869;&#37096;&#19981;&#20250;&#21457;&#29983;&#65289;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#23547;&#25214;&#19968;&#20010;&#21333;&#19968;&#30340;&#8220;&#26368;&#20248;&#8221;&#20998;&#21106;&#65292;&#35201;&#20040;&#20174;&#21487;&#33021;&#20998;&#21106;&#30340;&#25972;&#20010;&#38598;&#21512;&#20013;&#25277;&#26679;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#24573;&#35270;&#20102;&#36825;&#26679;&#19968;&#20010;&#20107;&#23454;&#65306;&#29305;&#21035;&#26159;&#22312;&#21327;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#20197;&#35768;&#22810;&#31181;&#26041;&#24335;&#21010;&#20998;&#21327;&#21464;&#37327;&#31354;&#38388;&#65292;&#22312;&#32479;&#35745;&#19978;&#26159;&#26080;&#27861;&#21306;&#20998;&#30340;&#65292;&#23613;&#31649;&#23545;&#25919;&#31574;&#25110;&#31185;&#23398;&#26377;&#30528;&#38750;&#24120;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25289;&#32454;&#23391;&#21010;&#20998;&#38598;&#30340;&#26367;&#20195;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02141v1 Announce Type: cross  Abstract: Many statistical analyses, in both observational data and randomized control trials, ask: how does the outcome of interest vary with combinations of observable covariates? How do various drug combinations affect health outcomes, or how does technology adoption depend on incentives and demographics? Our goal is to partition this factorial space into ``pools'' of covariate combinations where the outcome differs across the pools (but not within a pool). Existing approaches (i) search for a single ``optimal'' partition under assumptions about the association between covariates or (ii) sample from the entire set of possible partitions. Both these approaches ignore the reality that, especially with correlation structure in covariates, many ways to partition the covariate space may be statistically indistinguishable, despite very different implications for policy or science. We develop an alternative perspective, called Rashomon Partition Set
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#32534;&#36753;&#65288;RAE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#21644;&#20462;&#21098;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.19631</link><description>&lt;p&gt;
&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#26816;&#32034;&#22686;&#24378;&#30693;&#35782;&#32534;&#36753;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#32534;&#36753;&#65288;RAE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#21644;&#20462;&#21098;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#39640;&#25928;&#33021;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#25972;&#21512;&#23454;&#26102;&#30693;&#35782;&#26356;&#26032;&#65292;&#23548;&#33268;&#21487;&#33021;&#36807;&#26102;&#25110;&#19981;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#24403;&#22788;&#29702;&#22810;&#36339;&#38382;&#39064;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#27714;LLMs&#26356;&#26032;&#21644;&#25972;&#21512;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#22810;&#20010;&#30693;&#35782;&#29255;&#27573;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#23450;&#21046;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#32534;&#36753;&#65288;RAE&#65289;&#26694;&#26550;&#12290;RAE&#39318;&#20808;&#26816;&#32034;&#32534;&#36753;&#21518;&#30340;&#20107;&#23454;&#65292;&#28982;&#21518;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#23436;&#21892;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26816;&#32034;&#26041;&#27861;&#22522;&#20110;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#65292;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#26469;&#35782;&#21035;&#38142;&#24335;&#20107;&#23454;&#65292;&#32780;&#22825;&#30495;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#25628;&#32034;&#21487;&#33021;&#20250;&#24573;&#30053;&#36825;&#20123;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#37319;&#29992;&#20102;&#20462;&#21098;&#31574;&#30053;&#65292;&#20174;&#26816;&#32034;&#21040;&#30340;&#20107;&#23454;&#20013;&#28040;&#38500;&#20887;&#20313;&#20449;&#24687;&#65292;&#36825;&#22686;&#24378;&#20102;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19631v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi-hop questions since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions. To tackle the problem, we propose the Retrieval-Augmented model Editing (RAE) framework tailored for multi-hop question answering. RAE first retrieves edited facts and then refines the language model through in-context learning. Specifically, our retrieval approach, based on mutual information maximization, leverages the reasoning abilities of LLMs to identify chain facts that na\"ive similarity-based searches might miss. Additionally, our framework incorporates a pruning strategy to eliminate redundant information from the retrieved facts, which enhances the edi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HandFormer&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#21644;&#31232;&#30095;&#37319;&#26679;&#30340;RGB&#24103;&#65292;&#29992;&#20110;&#26377;&#25928;&#24314;&#27169;&#25163;&#37096;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09805</link><description>&lt;p&gt;
&#20851;&#20110;3D&#25163;&#37096;&#23039;&#21183;&#22312;&#21160;&#20316;&#35782;&#21035;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Utility of 3D Hand Poses for Action Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HandFormer&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#21644;&#31232;&#30095;&#37319;&#26679;&#30340;RGB&#24103;&#65292;&#29992;&#20110;&#26377;&#25928;&#24314;&#27169;&#25163;&#37096;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#25163;&#37096;&#23039;&#21183;&#26159;&#19968;&#31181;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#21160;&#20316;&#35782;&#21035;&#27169;&#24577;&#12290;&#23039;&#21183;&#26082;&#32039;&#20945;&#21448;&#20449;&#24687;&#20016;&#23500;&#65292;&#24182;&#19988;&#21487;&#20197;&#26497;&#22823;&#22320;&#21463;&#30410;&#20110;&#35745;&#31639;&#39044;&#31639;&#26377;&#38480;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#21333;&#29420;&#30340;&#23039;&#21183;&#19981;&#33021;&#23436;&#20840;&#29702;&#35299;&#20154;&#31867;&#19982;&#20043;&#20132;&#20114;&#30340;&#29289;&#20307;&#21644;&#29615;&#22659;&#12290;&#20026;&#20102;&#26377;&#25928;&#24314;&#27169;&#25163;&#37096;&#29289;&#20307;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HandFormer&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;Transformer&#12290;HandFormer&#32467;&#21512;&#20102;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#65292;&#29992;&#20110;&#31934;&#32454;&#36816;&#21160;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#31232;&#30095;&#37319;&#26679;&#30340;RGB&#24103;&#26469;&#32534;&#30721;&#22330;&#26223;&#35821;&#20041;&#12290;&#35266;&#23519;&#25163;&#37096;&#23039;&#21183;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#25105;&#20204;&#23545;&#25163;&#37096;&#24314;&#27169;&#36827;&#34892;&#20102;&#26102;&#38388;&#20998;&#35299;&#65292;&#24182;&#36890;&#36807;&#20854;&#30701;&#26399;&#36712;&#36857;&#34920;&#31034;&#27599;&#20010;&#20851;&#33410;&#28857;&#12290;&#36825;&#31181;&#34987;&#20998;&#35299;&#30340;&#23039;&#21183;&#34920;&#31034;&#19982;&#31232;&#30095;&#30340;RGB&#37319;&#26679;&#30456;&#32467;&#21512;&#65292;&#25928;&#29575;&#38750;&#24120;&#39640;&#65292;&#24182;&#19988;&#36798;&#21040;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#20165;&#26377;&#25163;&#37096;&#23039;&#21183;&#30340;&#21333;&#27169;HandFormer&#22312;5&#20493;&#26356;&#23569;&#30340;FLO&#19979;&#32988;&#36807;&#29616;&#26377;&#22522;&#20110;&#39592;&#26550;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09805v1 Announce Type: cross  Abstract: 3D hand poses are an under-explored modality for action recognition. Poses are compact yet informative and can greatly benefit applications with limited compute budgets. However, poses alone offer an incomplete understanding of actions, as they cannot fully capture objects and environments with which humans interact. To efficiently model hand-object interactions, we propose HandFormer, a novel multimodal transformer. HandFormer combines 3D hand poses at a high temporal resolution for fine-grained motion modeling with sparsely sampled RGB frames for encoding scene semantics. Observing the unique characteristics of hand poses, we temporally factorize hand modeling and represent each joint by its short-term trajectories. This factorized pose representation combined with sparse RGB samples is remarkably efficient and achieves high accuracy. Unimodal HandFormer with only hand poses outperforms existing skeleton-based methods at 5x fewer FLO
&lt;/p&gt;</description></item><item><title>VIRUS-NeRF&#26159;&#22522;&#20110;&#35270;&#35273;&#12289;&#32418;&#22806;&#21644;&#36229;&#22768;&#27874;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#36890;&#36807;&#25972;&#21512;&#36229;&#22768;&#27874;&#21644;&#32418;&#22806;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#27979;&#37327;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22312;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#20013;&#36798;&#21040;&#19982;LiDAR&#28857;&#20113;&#30456;&#23218;&#32654;&#30340;&#26144;&#23556;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09477</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#12289;&#32418;&#22806;&#21644;&#36229;&#22768;&#27874;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#8212;&#8212;VIRUS-NeRF
&lt;/p&gt;
&lt;p&gt;
VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09477
&lt;/p&gt;
&lt;p&gt;
VIRUS-NeRF&#26159;&#22522;&#20110;&#35270;&#35273;&#12289;&#32418;&#22806;&#21644;&#36229;&#22768;&#27874;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#36890;&#36807;&#25972;&#21512;&#36229;&#22768;&#27874;&#21644;&#32418;&#22806;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#27979;&#37327;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22312;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#20013;&#36798;&#21040;&#19982;LiDAR&#28857;&#20113;&#30456;&#23218;&#32654;&#30340;&#26144;&#23556;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#22312;&#29616;&#20195;&#24037;&#21378;&#21644;&#20179;&#24211;&#25805;&#20316;&#20013;&#36215;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38556;&#30861;&#29289;&#26816;&#27979;&#12289;&#22238;&#36991;&#21644;&#36335;&#24452;&#35268;&#21010;&#26159;&#20851;&#38190;&#30340;&#23433;&#20840;&#30456;&#20851;&#20219;&#21153;&#65292;&#36890;&#24120;&#20351;&#29992;&#26114;&#36149;&#30340;LiDAR&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#25668;&#20687;&#22836;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25104;&#26412;&#25928;&#30410;&#30340;&#20302;&#20998;&#36776;&#29575;&#27979;&#36317;&#20256;&#24863;&#22120;&#65292;&#22914;&#36229;&#22768;&#27874;&#21644;&#32418;&#22806;&#26102;&#38388;&#39134;&#34892;&#20256;&#24863;&#22120;&#65292;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#35270;&#35273;&#12289;&#32418;&#22806;&#21644;&#36229;&#22768;&#27874;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;(VIRUS-NeRF)&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;VIRUS-NeRF&#26500;&#24314;&#22312;&#30636;&#26102;&#31070;&#32463;&#22270;&#24418;&#22522;&#20803;&#19982;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32534;&#30721;(Instant-NGP)&#30340;&#22522;&#30784;&#19978;&#65292;&#34701;&#21512;&#20102;&#36229;&#22768;&#27874;&#21644;&#32418;&#22806;&#20256;&#24863;&#22120;&#30340;&#28145;&#24230;&#27979;&#37327;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#26356;&#26032;&#29992;&#20110;&#20809;&#32447;&#36319;&#36394;&#30340;&#21344;&#25454;&#32593;&#26684;&#12290;&#22312;2D&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;VIRUS-NeRF&#23454;&#29616;&#20102;&#19982;LiDAR&#28857;&#20113;&#30456;&#23218;&#32654;&#30340;&#26144;&#23556;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#23567;&#22411;&#29615;&#22659;&#20013;&#65292;&#20854;&#20934;&#30830;&#24615;&#19982;LiDAR&#27979;&#37327;&#30456;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09477v1 Announce Type: cross  Abstract: Autonomous mobile robots are an increasingly integral part of modern factory and warehouse operations. Obstacle detection, avoidance and path planning are critical safety-relevant tasks, which are often solved using expensive LiDAR sensors and depth cameras. We propose to use cost-effective low-resolution ranging sensors, such as ultrasonic and infrared time-of-flight sensors by developing VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance Fields. Building upon Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from ultrasonic and infrared sensors and utilizes them to update the occupancy grid used for ray marching. Experimental evaluation in 2D demonstrates that VIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds regarding coverage. Notably, in small environments, its accuracy aligns with that of LiDAR measurements, whi
&lt;/p&gt;</description></item><item><title>ADEdgeDrop&#25552;&#20986;&#20102;&#19968;&#31181;&#25932;&#23545;&#36793;&#32536;&#21024;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25932;&#23545;&#36793;&#32536;&#39044;&#27979;&#22120;&#25351;&#23548;&#36793;&#32536;&#21024;&#38500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09171</link><description>&lt;p&gt;
ADEdgeDrop&#65306;&#29992;&#20110;&#24378;&#20581;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25932;&#23545;&#36793;&#32536;&#21024;&#38500;
&lt;/p&gt;
&lt;p&gt;
ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09171
&lt;/p&gt;
&lt;p&gt;
ADEdgeDrop&#25552;&#20986;&#20102;&#19968;&#31181;&#25932;&#23545;&#36793;&#32536;&#21024;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25932;&#23545;&#36793;&#32536;&#39044;&#27979;&#22120;&#25351;&#23548;&#36793;&#32536;&#21024;&#38500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#36807;&#21508;&#31181;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#23637;&#31034;&#20102;&#20174;&#37051;&#36817;&#33410;&#28857;&#25910;&#38598;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#22024;&#26434;&#21644;&#20887;&#20313;&#30340;&#22270;&#25968;&#25454;&#36896;&#25104;&#30340;&#24046;&#30340;&#27867;&#21270;&#21644;&#33030;&#24369;&#30340;&#31283;&#20581;&#24615;&#38480;&#21046;&#20102;GNNs&#30340;&#24615;&#33021;&#12290;&#22312;Graph Augmentation Learning&#65288;GAL&#65289;&#20013;&#65292;&#36793;&#32536;&#21024;&#38500;&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#39640;GNNs&#30340;&#31283;&#20581;&#24615;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#21024;&#38500;&#36793;&#32536;&#36890;&#24120;&#20250;&#32469;&#36807;&#20851;&#38190;&#36793;&#32536;&#65292;&#20174;&#32780;&#21066;&#24369;&#28040;&#24687;&#20256;&#36882;&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25932;&#23545;&#36793;&#32536;&#21024;&#38500;&#26041;&#27861;&#65288;ADEdgeDrop&#65289;&#65292;&#21033;&#29992;&#25932;&#23545;&#36793;&#32536;&#39044;&#27979;&#22120;&#24341;&#23548;&#36793;&#32536;&#21024;&#38500;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#25972;&#21512;&#21040;&#19981;&#21516;&#30340;GNN&#20027;&#24178;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09171v1 Announce Type: cross  Abstract: Although Graph Neural Networks (GNNs) have exhibited the powerful ability to gather graph-structured information from neighborhood nodes via various message-passing mechanisms, the performance of GNNs is limited by poor generalization and fragile robustness caused by noisy and redundant graph data. As a prominent solution, Graph Augmentation Learning (GAL) has recently received increasing attention. Among prior GAL approaches, edge-dropping methods that randomly remove edges from a graph during training are effective techniques to improve the robustness of GNNs. However, randomly dropping edges often results in bypassing critical edges, consequently weakening the effectiveness of message passing. In this paper, we propose a novel adversarial edge-dropping method (ADEdgeDrop) that leverages an adversarial edge predictor guiding the removal of edges, which can be flexibly incorporated into diverse GNN backbones. Employing an adversarial 
&lt;/p&gt;</description></item><item><title>&#22312;&#25968;&#25454;&#21098;&#26525;&#20013;&#24341;&#20837;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#39044;&#20808;&#35757;&#32451;&#30340;&#25945;&#24072;&#32593;&#32476;&#36719;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#21098;&#26525;&#26041;&#27861;&#21644;&#25152;&#26377;&#21098;&#26525;&#20998;&#25968;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.07854</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#21098;&#26525;&#20013;&#33976;&#39311;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distilling the Knowledge in Data Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07854
&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#21098;&#26525;&#20013;&#24341;&#20837;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#39044;&#20808;&#35757;&#32451;&#30340;&#25945;&#24072;&#32593;&#32476;&#36719;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#21098;&#26525;&#26041;&#27861;&#21644;&#25152;&#26377;&#21098;&#26525;&#20998;&#25968;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#19981;&#26029;&#22686;&#21152;&#65292;&#25968;&#25454;&#21098;&#26525;&#25104;&#20026;&#20102;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#25968;&#25454;&#21098;&#26525;&#31639;&#27861;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#24230;&#21098;&#26525;&#30340;&#24773;&#20917;&#19979;&#19982;&#20351;&#29992;&#23436;&#25972;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#22522;&#20110;&#21098;&#26525;&#23376;&#38598;&#30340;&#27169;&#22411;&#26102;&#65292;&#32467;&#21512;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30340;&#24212;&#29992;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#19981;&#20165;&#20381;&#36182;&#20110;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#65292;&#36824;&#20351;&#29992;&#20102;&#24050;&#22312;&#23436;&#25972;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#32769;&#24072;&#32593;&#32476;&#30340;&#36719;&#39044;&#27979;&#12290;&#36890;&#36807;&#23558;&#30693;&#35782;&#33976;&#39311;&#25972;&#21512;&#21040;&#35757;&#32451;&#20013;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#21098;&#26525;&#26041;&#27861;&#21644;&#25152;&#26377;&#21098;&#26525;&#20998;&#25968;&#19978;&#37117;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#37319;&#29992;&#33258;&#33976;&#39311;&#26469;&#25913;&#21892;&#22312;&#21098;&#26525;&#25968;&#25454;&#19978;&#30340;&#35757;&#32451;&#30340;&#29702;&#35770;&#21160;&#26426;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#36827;&#34892;&#20102;&#24341;&#20154;&#27880;&#30446;&#19988;&#39640;&#24230;&#23454;&#29992;&#30340;&#35266;&#23519;&#65306;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65292;&#31616;&#21333;&#30340;&#38543;&#26426;&#21098;&#26525;&#20063;&#20250;&#21462;&#24471;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07854v1 Announce Type: cross  Abstract: With the increasing size of datasets used for training neural networks, data pruning becomes an attractive field of research. However, most current data pruning algorithms are limited in their ability to preserve accuracy compared to models trained on the full data, especially in high pruning regimes. In this paper we explore the application of data pruning while incorporating knowledge distillation (KD) when training on a pruned subset. That is, rather than relying solely on ground-truth labels, we also use the soft predictions from a teacher network pre-trained on the complete data. By integrating KD into training, we demonstrate significant improvement across datasets, pruning methods, and on all pruning fractions. We first establish a theoretical motivation for employing self-distillation to improve training on pruned data. Then, we empirically make a compelling and highly practical observation: using KD, simple random pruning is c
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;Reset &amp; Distill&#65288;R&amp;D&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#32622;&#20195;&#29702;&#30340;&#32593;&#32476;&#21644;&#25552;&#28860;&#30693;&#35782;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05066</link><description>&lt;p&gt;
&#22797;&#20301;&#21644;&#25552;&#28860;&#65306;&#20811;&#26381;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#30340;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reset &amp; Distill: A Recipe for Overcoming Negative Transfer in Continual Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05066
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;Reset &amp; Distill&#65288;R&amp;D&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#32622;&#20195;&#29702;&#30340;&#32593;&#32476;&#21644;&#25552;&#28860;&#30693;&#35782;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;&#21457;&#23637;&#26377;&#25928;&#30340;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#65288;CRL&#65289;&#31639;&#27861;&#30340;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#26159;&#24403;&#38656;&#35201;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20250;&#21457;&#29983;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#38382;&#39064;&#22312;CRL&#20013;&#32463;&#24120;&#23384;&#22312;&#65292;&#24182;&#19988;&#26080;&#27861;&#36890;&#36807;&#26368;&#36817;&#19968;&#20123;&#26088;&#22312;&#20943;&#36731;RL&#20195;&#29702;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#30340;&#24037;&#20316;&#26469;&#26377;&#25928;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Reset &amp; Distill&#65288;R&amp;D&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20811;&#26381;CRL&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;R&amp;D&#32467;&#21512;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#21363;&#37325;&#32622;&#20195;&#29702;&#30340;&#22312;&#32447;&#28436;&#21592;&#21644;&#35780;&#35770;&#32593;&#32476;&#20197;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#20197;&#21450;&#31163;&#32447;&#23398;&#20064;&#27493;&#39588;&#65292;&#29992;&#20110;&#25552;&#28860;&#22312;&#32447;&#28436;&#21592;&#21644;&#20197;&#21069;&#19987;&#23478;&#21160;&#20316;&#27010;&#29575;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;Meta-World&#20219;&#21153;&#30340;&#38271;&#24207;&#21015;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#26368;&#36817;&#30340;&#22522;&#32447;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05066v1 Announce Type: cross  Abstract: We argue that one of the main obstacles for developing effective Continual Reinforcement Learning (CRL) algorithms is the negative transfer issue occurring when the new task to learn arrives. Through comprehensive experimental validation, we demonstrate that such issue frequently exists in CRL and cannot be effectively addressed by several recent work on mitigating plasticity loss of RL agents. To that end, we develop Reset &amp; Distill (R&amp;D), a simple yet highly effective method, to overcome the negative transfer problem in CRL. R&amp;D combines a strategy of resetting the agent's online actor and critic networks to learn a new task and an offline learning step for distilling the knowledge from the online actor and previous expert's action probabilities. We carried out extensive experiments on long sequence of Meta-World tasks and show that our method consistently outperforms recent baselines, achieving significantly higher success rates acr
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#20102;&#22823;&#37327;&#28608;&#27963;&#29616;&#35937;&#65292;&#23427;&#20204;&#20855;&#26377;&#38750;&#24120;&#22823;&#30340;&#20540;&#24182;&#19988;&#22312;&#27169;&#22411;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17762</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22823;&#37327;&#28608;&#27963;
&lt;/p&gt;
&lt;p&gt;
Massive Activations in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17762
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#20102;&#22823;&#37327;&#28608;&#27963;&#29616;&#35937;&#65292;&#23427;&#20204;&#20855;&#26377;&#38750;&#24120;&#22823;&#30340;&#20540;&#24182;&#19988;&#22312;&#27169;&#22411;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#19968;&#20010;&#32463;&#39564;&#29616;&#35937;&#8212;&#8212;&#24456;&#23569;&#30340;&#28608;&#27963;&#23637;&#29616;&#20986;&#27604;&#20854;&#20182;&#28608;&#27963;&#26126;&#26174;&#26356;&#22823;&#30340;&#20540;&#65288;&#20363;&#22914;&#65292;&#22823;&#20986; 100,000 &#20493;&#65289;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#22823;&#37327;&#28608;&#27963;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#37327;&#28608;&#27963;&#22312;&#21508;&#31181;LLMs&#20013;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#23545;&#20854;&#20301;&#32622;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#20540;&#22522;&#26412;&#19978;&#19981;&#21463;&#36755;&#20837;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;LLMs&#20013;&#36215;&#21040;&#19981;&#21487;&#25110;&#32570;&#30340;&#20559;&#32622;&#39033;&#20316;&#29992;&#12290;&#31532;&#19977;&#65292;&#36825;&#20123;&#22823;&#37327;&#28608;&#27963;&#23548;&#33268;&#20851;&#27880;&#27010;&#29575;&#38598;&#20013;&#20110;&#20854;&#23545;&#24212;&#30340;&#26631;&#35760;&#65292;&#24182;&#36827;&#19968;&#27493;&#25104;&#20026;&#33258;&#27880;&#24847;&#36755;&#20986;&#20013;&#30340;&#38544;&#24335;&#20559;&#32622;&#39033;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#35270;&#35273;Transformer&#20013;&#30340;&#22823;&#37327;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17762v1 Announce Type: new  Abstract: We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#36719;&#24178;&#39044;&#22788;&#29702;&#38544;&#24335;&#28508;&#22312;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#65292;&#22312; Variational Autoencoder (VAE) &#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#22240;&#26524;&#26426;&#21046;&#24320;&#20851;&#21464;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.11124</link><description>&lt;p&gt;
&#36890;&#36807;&#24320;&#20851;&#21464;&#37327;&#22312;&#38544;&#24335;&#22240;&#26524;&#27169;&#22411;&#20013;&#35299;&#24320;&#32416;&#32544;
&lt;/p&gt;
&lt;p&gt;
Disentanglement in Implicit Causal Models via Switch Variable
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11124
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#36719;&#24178;&#39044;&#22788;&#29702;&#38544;&#24335;&#28508;&#22312;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#65292;&#22312; Variational Autoencoder (VAE) &#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#22240;&#26524;&#26426;&#21046;&#24320;&#20851;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#21644;&#24178;&#39044;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#34920;&#24449;&#65292;&#22312;&#27809;&#26377;&#24050;&#30693;&#30340;&#22320;&#38754;&#30495;&#23454;&#22270;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#38544;&#24335;&#28508;&#22312;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#12290;&#38544;&#24335;&#23398;&#20064;&#22240;&#26524;&#26426;&#21046;&#36890;&#24120;&#28041;&#21450;&#20004;&#31867;&#24178;&#39044;&#25968;&#25454;&#65306;&#30828;&#24178;&#39044;&#21644;&#36719;&#24178;&#39044;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#65292;&#36719;&#24178;&#39044;&#36890;&#24120;&#27604;&#30828;&#24178;&#39044;&#26356;&#29616;&#23454;&#65292;&#22240;&#20026;&#21518;&#32773;&#38656;&#35201;&#23436;&#20840;&#21463;&#25511;&#30340;&#29615;&#22659;&#12290;&#19982;&#30452;&#25509;&#24378;&#21046;&#25913;&#21464;&#22240;&#26524;&#21464;&#37327;&#30340;&#30828;&#24178;&#39044;&#19981;&#21516;&#65292;&#36719;&#24178;&#39044;&#36890;&#36807;&#24433;&#21709;&#22240;&#26524;&#26426;&#21046;&#38388;&#25509;&#22320;&#20135;&#29983;&#24433;&#21709;&#12290;&#26412;&#25991;&#36890;&#36807;&#36719;&#24178;&#39044;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26694;&#26550;&#20013;&#22788;&#29702;&#38544;&#24335;&#28508;&#22312;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26088;&#22312;&#22312;&#19981;&#21516;&#22240;&#26524;&#26426;&#21046;&#20043;&#38388;&#20999;&#25442;&#30340;&#22240;&#26524;&#26426;&#21046;&#24320;&#20851;&#21464;&#37327;&#26469;&#24314;&#27169;&#36719;&#24178;&#39044;&#25928;&#26524;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22987;&#32456;&#20445;&#25345;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11124v1 Announce Type: new  Abstract: Learning causal representations from observational and interventional data in the absence of known ground-truth graph structures necessitates implicit latent causal representation learning. Implicitly learning causal mechanisms typically involves two categories of interventional data: hard and soft interventions. In real-world scenarios, soft interventions are often more realistic than hard interventions, as the latter require fully controlled environments. Unlike hard interventions, which directly force changes in a causal variable, soft interventions exert influence indirectly by affecting the causal mechanism. In this paper, we tackle implicit latent causal representation learning in a Variational Autoencoder (VAE) framework through soft interventions. Our approach models soft interventions effects by employing a causal mechanism switch variable designed to toggle between different causal mechanisms. In our experiments, we consistentl
&lt;/p&gt;</description></item><item><title>V-STaR&#21033;&#29992;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#35757;&#32451;&#39564;&#35777;&#22120;&#65292;&#29992;&#20110;&#36873;&#25321;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#33258;&#25105;&#25913;&#36827;&#21644;&#39564;&#35777;&#26041;&#27861;&#22312;&#24120;&#35265;&#20195;&#30721;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#36798;&#21040;4%&#21040;17%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.06457</link><description>&lt;p&gt;
V-STaR: &#33258;&#23398;&#25512;&#29702;&#22120;&#30340;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
V-STaR: Training Verifiers for Self-Taught Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06457
&lt;/p&gt;
&lt;p&gt;
V-STaR&#21033;&#29992;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#35757;&#32451;&#39564;&#35777;&#22120;&#65292;&#29992;&#20110;&#36873;&#25321;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#33258;&#25105;&#25913;&#36827;&#21644;&#39564;&#35777;&#26041;&#27861;&#22312;&#24120;&#35265;&#20195;&#30721;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#36798;&#21040;4%&#21040;17%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24120;&#35265;&#33258;&#25105;&#25913;&#36827;&#26041;&#27861;&#65292;&#20363;&#22914;STaR&#65288;Zelikman&#31561;&#20154;&#65292;2022&#65289;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#36845;&#20195;&#24494;&#35843;LLM&#20197;&#25552;&#39640;&#20854;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#27492;&#36807;&#31243;&#20013;&#20002;&#24323;&#20102;&#22823;&#37327;&#30340;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#33021;&#24573;&#30053;&#20102;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;V-STaR&#65292;&#23427;&#21033;&#29992;&#33258;&#25105;&#25913;&#36827;&#36807;&#31243;&#20013;&#29983;&#25104;&#30340;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20351;&#29992;DPO&#35757;&#32451;&#19968;&#20010;&#21028;&#26029;&#27169;&#22411;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#27491;&#30830;&#24615;&#30340;&#39564;&#35777;&#22120;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#36825;&#20010;&#39564;&#35777;&#22120;&#29992;&#26469;&#22312;&#20247;&#22810;&#20505;&#36873;&#35299;&#20915;&#26041;&#26696;&#20013;&#36873;&#25321;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#22810;&#27425;&#36816;&#34892;V-STaR&#20250;&#36880;&#27493;&#20135;&#29983;&#26356;&#22909;&#30340;&#25512;&#29702;&#22120;&#21644;&#39564;&#35777;&#22120;&#65292;&#22312;&#24120;&#35265;&#20195;&#30721;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20351;&#29992;LLaMA2&#27169;&#22411;&#21487;&#20197;&#21462;&#24471;4%&#21040;17%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common self-improvement approaches for large language models (LLMs), such as STaR (Zelikman et al., 2022), iteratively fine-tune LLMs on self-generated solutions to improve their problem-solving ability. However, these approaches discard the large amounts of incorrect solutions generated during this process, potentially neglecting valuable information in such solutions. To address this shortcoming, we propose V-STaR that utilizes both the correct and incorrect solutions generated during the self-improvement process to train a verifier using DPO that judges correctness of model-generated solutions. This verifier is used at inference time to select one solution among many candidate solutions. Running V-STaR for multiple iterations results in progressively better reasoners and verifiers, delivering a 4% to 17% test accuracy improvement over existing self-improvement and verification approaches on common code generation and math reasoning benchmarks with LLaMA2 models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36817;&#20284;&#31639;&#27861;&#65292;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#19968;&#31181;&#27809;&#26377;&#23398;&#20064;&#22686;&#24378;&#12290;&#36890;&#36807;&#31454;&#20105;&#27604;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#21644;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01995</link><description>&lt;p&gt;
&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#65306;&#31532;&#19968;&#27425;&#36817;&#20284;&#31639;&#27861;&#65292;&#20855;&#26377;&#20840;&#32622;&#20449;&#21306;&#38388;&#38598;&#25104;&#30340;&#23398;&#20064;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Online Uniform Risk Times Sampling: First Approximation Algorithms, Learning Augmentation with Full Confidence Interval Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36817;&#20284;&#31639;&#27861;&#65292;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#19968;&#31181;&#27809;&#26377;&#23398;&#20064;&#22686;&#24378;&#12290;&#36890;&#36807;&#31454;&#20105;&#27604;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#21644;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#20581;&#24247;&#39046;&#22495;&#65292;&#23558;&#26377;&#38480;&#30340;&#27835;&#30103;&#39044;&#31639;&#20998;&#37197;&#21040;&#21487;&#29992;&#30340;&#39118;&#38505;&#26102;&#38388;&#19978;&#26159;&#20943;&#23569;&#29992;&#25143;&#30130;&#21171;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26410;&#30693;&#30340;&#23454;&#38469;&#39118;&#38505;&#26102;&#38388;&#25968;&#37327;&#65292;&#36825;&#19968;&#31574;&#30053;&#36935;&#21040;&#20102;&#26174;&#33879;&#30340;&#38556;&#30861;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#29702;&#35770;&#20445;&#35777;&#26041;&#38754;&#36824;&#19981;&#36275;&#22815;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#38382;&#39064;&#24341;&#20837;&#36817;&#20284;&#31639;&#27861;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36817;&#20284;&#31639;&#27861;&#65292;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#19968;&#31181;&#27809;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#31454;&#20105;&#27604;&#20998;&#26512;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#23454;&#39564;&#21644;HeartSteps&#31227;&#21160;&#24212;&#29992;&#30340;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In digital health, the strategy of allocating a limited treatment budget across available risk times is crucial to reduce user fatigue. This strategy, however, encounters a significant obstacle due to the unknown actual number of risk times, a factor not adequately addressed by existing methods lacking theoretical guarantees. This paper introduces, for the first time, the online uniform risk times sampling problem within the approximation algorithm framework. We propose two online approximation algorithms for this problem, one with and one without learning augmentation, and provide rigorous theoretical performance guarantees for them using competitive ratio analysis. We assess the performance of our algorithms using both synthetic experiments and a real-world case study on HeartSteps mobile applications.
&lt;/p&gt;</description></item><item><title>DFML&#26159;&#19968;&#20010;&#26080;&#26381;&#21153;&#22120;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#36890;&#36807;&#30456;&#20114;&#23398;&#20064;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20256;&#25480;&#30693;&#35782;&#65292;&#20197;&#33719;&#24471;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#20840;&#23616;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01863</link><description>&lt;p&gt;
DFML&#65306;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DFML: Decentralized Federated Mutual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01863
&lt;/p&gt;
&lt;p&gt;
DFML&#26159;&#19968;&#20010;&#26080;&#26381;&#21153;&#22120;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#36890;&#36807;&#30456;&#20114;&#23398;&#20064;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20256;&#25480;&#30693;&#35782;&#65292;&#20197;&#33719;&#24471;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#20840;&#23616;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#35774;&#22791;&#39046;&#22495;&#20013;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#30340;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#23384;&#22312;&#36890;&#20449;&#29942;&#39048;&#21644;&#23481;&#26131;&#21463;&#21040;&#21333;&#28857;&#25925;&#38556;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#35774;&#22791;&#22266;&#26377;&#22320;&#34920;&#29616;&#20986;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12290;&#29616;&#26377;&#24037;&#20316;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#36866;&#24212;&#27492;&#24322;&#36136;&#24615;&#19988;&#19981;&#26045;&#21152;&#26550;&#26500;&#38480;&#21046;&#25110;&#20551;&#23450;&#20844;&#20849;&#25968;&#25454;&#21487;&#29992;&#30340;&#20998;&#25955;&#24335;FL&#65288;DFL&#65289;&#26694;&#26550;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#25955;&#24335;&#32852;&#37030;&#20114;&#32852;&#23398;&#20064;&#65288;DFML&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26159;&#26080;&#26381;&#21153;&#22120;&#30340;&#65292;&#25903;&#25345;&#38750;&#38480;&#21046;&#24615;&#30340;&#24322;&#26500;&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#20381;&#36182;&#20844;&#20849;&#25968;&#25454;&#12290;DFML&#36890;&#36807;&#30456;&#20114;&#23398;&#20064;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20256;&#25480;&#30693;&#35782;&#65292;&#24182;&#24490;&#29615;&#25913;&#21464;&#30417;&#30563;&#21644;&#25552;&#21462;&#20449;&#21495;&#30340;&#25968;&#37327;&#26469;&#26377;&#25928;&#22788;&#29702;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DFML&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#20840;&#23616;&#20934;&#30830;&#24615;&#26041;&#38754;&#20855;&#26377;&#19968;&#33268;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#26222;&#36941;&#23384;&#22312;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of real-world devices, centralized servers in Federated Learning (FL) present challenges including communication bottlenecks and susceptibility to a single point of failure. Additionally, contemporary devices inherently exhibit model and data heterogeneity. Existing work lacks a Decentralized FL (DFL) framework capable of accommodating such heterogeneity without imposing architectural restrictions or assuming the availability of public data. To address these issues, we propose a Decentralized Federated Mutual Learning (DFML) framework that is serverless, supports nonrestrictive heterogeneous models, and avoids reliance on public data. DFML effectively handles model and data heterogeneity through mutual learning, which distills knowledge between clients, and cyclically varying the amount of supervision and distillation signals. Extensive experimental results demonstrate consistent effectiveness of DFML in both convergence speed and global accuracy, outperforming prevalent b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#19982;&#20154;&#31867;&#22312;&#20915;&#31574;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01766</link><description>&lt;p&gt;
LLM&#25237;&#31080;&#65306;&#20154;&#31867;&#36873;&#25321;&#21644;AI&#38598;&#20307;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
LLM Voting: Human Choices and AI Collective Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#19982;&#20154;&#31867;&#22312;&#20915;&#31574;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#19982;&#20154;&#31867;&#25237;&#31080;&#27169;&#24335;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36827;&#34892;&#20154;&#31867;&#25237;&#31080;&#23454;&#39564;&#20197;&#24314;&#31435;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#20934;&#65292;&#24182;&#19982;LLM&#20195;&#29702;&#36827;&#34892;&#24179;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#32858;&#28966;&#20110;&#38598;&#20307;&#32467;&#26524;&#21644;&#20010;&#20307;&#20559;&#22909;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#21644;LLMs&#20043;&#38388;&#22312;&#20915;&#31574;&#21644;&#22266;&#26377;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#22312;&#20559;&#22909;&#22810;&#26679;&#24615;&#21644;&#19968;&#33268;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#30456;&#27604;&#20154;&#31867;&#36873;&#27665;&#30340;&#22810;&#26679;&#20559;&#22909;&#65292;LLMs&#26377;&#26356;&#36235;&#21521;&#20110;&#19968;&#33268;&#36873;&#25321;&#30340;&#20542;&#21521;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the voting behaviors of Large Language Models (LLMs), particularly OpenAI's GPT4 and LLaMA2, and their alignment with human voting patterns. Our approach included a human voting experiment to establish a baseline for human preferences and a parallel experiment with LLM agents. The study focused on both collective outcomes and individual preferences, revealing differences in decision-making and inherent biases between humans and LLMs. We observed a trade-off between preference diversity and alignment in LLMs, with a tendency towards more uniform choices as compared to the diverse preferences of human voters. This finding indicates that LLMs could lead to more homogenized collective outcomes when used in voting assistance, underscoring the need for cautious integration of LLMs into democratic processes.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;MD&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#35774;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;VAE&#27169;&#22411;&#29983;&#25104;&#26032;&#22411;Vitrimer&#24182;&#26681;&#25454;&#25152;&#38656;Tg&#25351;&#23548;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2312.03690</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;&#29983;&#25104;&#24314;&#27169;&#23454;&#29616;&#29627;&#29827;&#36716;&#21270;&#28201;&#24230;&#30340;&#36870;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Inverse Design of Vitrimeric Polymers by Molecular Dynamics and Generative Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03690
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;MD&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#35774;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;VAE&#27169;&#22411;&#29983;&#25104;&#26032;&#22411;Vitrimer&#24182;&#26681;&#25454;&#25152;&#38656;Tg&#25351;&#23548;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vitrimer&#26159;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#21160;&#24577;&#20849;&#20215;&#33258;&#36866;&#24212;&#32593;&#32476;&#37325;&#26032;&#25490;&#21015;&#32780;&#20855;&#26377;&#33258;&#25105;&#20462;&#22797;&#33021;&#21147;&#30340;&#26032;&#22411;&#21487;&#25345;&#32493;&#32858;&#21512;&#29289;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#26500;&#25104;&#20998;&#23376;&#36873;&#25321;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#36136;&#31354;&#38388;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#28508;&#22312;&#24212;&#29992;&#30340;&#20805;&#20998;&#23454;&#29616;&#12290;&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#29305;&#21035;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#29983;&#25104;&#26032;&#22411;Vitrimer&#24182;&#26681;&#25454;&#25152;&#38656;&#29627;&#29827;&#36716;&#21464;&#28201;&#24230;&#65288;Tg&#65289;&#25351;&#23548;&#20854;&#36870;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;Vitrimer&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#19968;&#30334;&#19975;&#31181;&#65292;&#24182;&#36890;&#36807;&#39640;&#36890;&#37327;MD&#27169;&#25311;&#65292;&#30001;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#26657;&#20934;&#65292;&#35745;&#31639;&#20102;8424&#31181;&#30340;Tg&#12290;&#25152;&#25552;&#20986;&#30340;VAE&#37319;&#29992;&#21452;&#22270;&#32534;&#30721;&#22120;&#21644;&#28508;&#22312;&#32500;&#24230;&#37325;&#21472;&#26041;&#26696;&#65292;&#20801;&#35768;&#22810;&#25104;&#20998;Vitrimer&#30340;&#20010;&#20307;&#34920;&#31034;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#36830;&#32493;&#30340;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03690v2 Announce Type: replace-cross  Abstract: Vitrimer is a new class of sustainable polymers with the ability of self-healing through rearrangement of dynamic covalent adaptive networks. However, a limited choice of constituent molecules restricts their property space, prohibiting full realization of their potential applications. Through a combination of molecular dynamics (MD) simulations and machine learning (ML), particularly a novel graph variational autoencoder (VAE) model, we establish a method for generating novel vitrimers and guide their inverse design based on desired glass transition temperature (Tg). We build the first vitrimer dataset of one million and calculate Tg on 8,424 of them by high-throughput MD simulations calibrated by a Gaussian process model. The proposed VAE employs dual graph encoders and a latent dimension overlapping scheme which allows for individual representation of multi-component vitrimers. By constructing a continuous latent space conta
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#21452;&#26399;&#26395;&#20998;&#20301;&#22238;&#24402;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#20219;&#24847;&#22238;&#25253;&#20998;&#24067;</title><link>https://arxiv.org/abs/2305.16877</link><description>&lt;p&gt;
&#29992;&#21452;&#26399;&#26395;&#20998;&#20301;&#22238;&#24402;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributional Reinforcement Learning with Dual Expectile-Quantile Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.16877
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#21452;&#26399;&#26395;&#20998;&#20301;&#22238;&#24402;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#20219;&#24847;&#22238;&#25253;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36817;&#20284;&#25972;&#20010;&#22238;&#25253;&#20998;&#24067;&#65292;&#24182;&#26356;&#22909;&#22320;&#21033;&#29992;&#29615;&#22659;&#26679;&#26412;&#12290;&#24120;&#29992;&#30340;&#22522;&#20110;&#19981;&#23545;&#31216;$L_1$&#25439;&#22833;&#30340;&#20998;&#24067;&#24335;RL&#30340;&#20998;&#20301;&#22238;&#24402;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#32780;&#26377;&#25928;&#30340;&#23398;&#20064;&#20219;&#24847;&#22238;&#25253;&#20998;&#24067;&#30340;&#26041;&#24335;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#25928;&#30340;&#28151;&#21512;&#19981;&#23545;&#31216;$L_1$-$L_2$ Huber&#25439;&#22833;&#26469;&#25913;&#36827;&#24448;&#24448;&#20250;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#20998;&#24067;&#20272;&#35745;&#20445;&#35777;&#28040;&#22833;&#20102;&#65292;&#25105;&#20204;&#23454;&#35777;&#35266;&#23519;&#21040;&#20272;&#35745;&#30340;&#20998;&#24067;&#20250;&#36805;&#36895;&#25910;&#25947;&#21040;&#20854;&#22343;&#20540;&#12290;&#20107;&#23454;&#19978;&#65292;&#19982;&#26399;&#26395;&#22238;&#24402;&#30456;&#23545;&#24212;&#30340;&#19981;&#23545;&#31216;$L_2$&#25439;&#22833;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;&#20998;&#24067;&#24335;&#26102;&#24207;&#24046;&#24322;&#23398;&#20064;&#12290;&#21463;&#21040;$L_2$&#20026;&#22522;&#30784;&#23398;&#20064;&#25928;&#29575;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#23398;&#20064;&#22238;&#25253;&#20998;&#24067;&#30340;&#26399;&#26395;&#20540;&#21644;&#20998;&#20301;&#25968;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.16877v2 Announce Type: replace-cross  Abstract: Distributional reinforcement learning (RL) has proven useful in multiple benchmarks as it enables approximating the full distribution of returns and makes a better use of environment samples. The commonly used quantile regression approach to distributional RL -- based on asymmetric $L_1$ losses -- provides a flexible and effective way of learning arbitrary return distributions. In practice, it is often improved by using a more efficient, hybrid asymmetric $L_1$-$L_2$ Huber loss for quantile regression. However, by doing so, distributional estimation guarantees vanish, and we empirically observe that the estimated distribution rapidly collapses to its mean. Indeed, asymmetric $L_2$ losses, corresponding to expectile regression, cannot be readily used for distributional temporal difference learning. Motivated by the efficiency of $L_2$-based learning, we propose to jointly learn expectiles and quantiles of the return distribution
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;D-Mapper&#30340;&#20998;&#24067;&#24341;&#23548;Mapper&#31639;&#27861;&#65292;&#20351;&#29992;&#27010;&#29575;&#27169;&#22411;&#21644;&#25968;&#25454;&#22266;&#26377;&#29305;&#24449;&#29983;&#25104;&#23494;&#24230;&#24341;&#23548;&#30340;&#35206;&#30422;&#65292;&#24182;&#25552;&#20379;&#22686;&#24378;&#30340;&#25299;&#25169;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.12237</link><description>&lt;p&gt;
&#19968;&#31181;&#20998;&#24067;&#24341;&#23548;&#30340;Mapper&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A distribution-guided Mapper algorithm. (arXiv:2401.12237v1 [math.AT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12237
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;D-Mapper&#30340;&#20998;&#24067;&#24341;&#23548;Mapper&#31639;&#27861;&#65292;&#20351;&#29992;&#27010;&#29575;&#27169;&#22411;&#21644;&#25968;&#25454;&#22266;&#26377;&#29305;&#24449;&#29983;&#25104;&#23494;&#24230;&#24341;&#23548;&#30340;&#35206;&#30422;&#65292;&#24182;&#25552;&#20379;&#22686;&#24378;&#30340;&#25299;&#25169;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#65306;Mapper&#31639;&#27861;&#26159;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#25506;&#32034;&#25968;&#25454;&#24418;&#29366;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#20351;&#29992;&#25968;&#25454;&#38598;&#20316;&#20026;&#36755;&#20837;&#65292;Mapper&#31639;&#27861;&#36755;&#20986;&#20195;&#34920;&#25972;&#20010;&#25968;&#25454;&#38598;&#25299;&#25169;&#29305;&#24449;&#30340;&#22270;&#24418;&#12290;&#36825;&#20010;&#22270;&#24418;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#25968;&#25454;&#30340;&#19968;&#20010;Reeb&#22270;&#30340;&#36817;&#20284;&#12290;&#32463;&#20856;&#30340;Mapper&#31639;&#27861;&#20351;&#29992;&#22266;&#23450;&#30340;&#21306;&#38388;&#38271;&#24230;&#21644;&#37325;&#21472;&#27604;&#29575;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25581;&#31034;&#25968;&#25454;&#30340;&#24494;&#22937;&#29305;&#24449;&#65292;&#23588;&#20854;&#26159;&#24403;&#24213;&#23618;&#32467;&#26500;&#22797;&#26434;&#26102;&#12290;&#32467;&#26524;&#65306;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;D-Mapper&#30340;&#20998;&#24067;&#24341;&#23548;Mapper&#31639;&#27861;&#65292;&#21033;&#29992;&#27010;&#29575;&#27169;&#22411;&#30340;&#23646;&#24615;&#21644;&#25968;&#25454;&#22266;&#26377;&#29305;&#24449;&#29983;&#25104;&#23494;&#24230;&#24341;&#23548;&#30340;&#35206;&#30422;&#65292;&#24182;&#25552;&#20379;&#22686;&#24378;&#30340;&#25299;&#25169;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#38750;&#27010;&#29575;&#24615;&#26041;&#27861;&#30340;&#26367;&#20195;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24230;&#37327;&#26469;&#32771;&#34385;&#37325;&#21472;&#32858;&#31867;&#30340;&#36136;&#37327;&#21644;&#25193;&#23637;&#25345;&#32493;&#21516;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: The Mapper algorithm is an essential tool to explore shape of data in topology data analysis. With a dataset as an input, the Mapper algorithm outputs a graph representing the topological features of the whole dataset. This graph is often regarded as an approximation of a reeb graph of data. The classic Mapper algorithm uses fixed interval lengths and overlapping ratios, which might fail to reveal subtle features of data, especially when the underlying structure is complex.  Results: In this work, we introduce a distribution guided Mapper algorithm named D-Mapper, that utilizes the property of the probability model and data intrinsic characteristics to generate density guided covers and provides enhanced topological features. Our proposed algorithm is a probabilistic model-based approach, which could serve as an alternative to non-prababilistic ones. Moreover, we introduce a metric accounting for both the quality of overlap clustering and extended persistence homology to me
&lt;/p&gt;</description></item><item><title>SymbolNet&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20462;&#21098;&#27169;&#22411;&#26435;&#37325;&#12289;&#36755;&#20837;&#29305;&#24449;&#21644;&#25968;&#23398;&#36816;&#31639;&#31526;&#65292;&#21516;&#26102;&#20248;&#21270;&#35757;&#32451;&#25439;&#22833;&#21644;&#34920;&#36798;&#24335;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#22238;&#24402;&#12290;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#33258;&#36866;&#24212;&#35843;&#25972;&#33258;&#36523;&#30340;&#24378;&#24230;&#65292;&#24182;&#25910;&#25947;&#21040;&#30446;&#26631;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;SymbolNet&#33021;&#39640;&#25928;&#22788;&#29702;&#20855;&#26377;&#36229;&#36807;10&#20010;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.09949</link><description>&lt;p&gt;
SymbolNet: &#33258;&#36866;&#24212;&#21160;&#24577;&#20462;&#21098;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
SymbolNet: Neural Symbolic Regression with Adaptive Dynamic Pruning. (arXiv:2401.09949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09949
&lt;/p&gt;
&lt;p&gt;
SymbolNet&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20462;&#21098;&#27169;&#22411;&#26435;&#37325;&#12289;&#36755;&#20837;&#29305;&#24449;&#21644;&#25968;&#23398;&#36816;&#31639;&#31526;&#65292;&#21516;&#26102;&#20248;&#21270;&#35757;&#32451;&#25439;&#22833;&#21644;&#34920;&#36798;&#24335;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#22238;&#24402;&#12290;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#33258;&#36866;&#24212;&#35843;&#25972;&#33258;&#36523;&#30340;&#24378;&#24230;&#65292;&#24182;&#25910;&#25947;&#21040;&#30446;&#26631;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;SymbolNet&#33021;&#39640;&#25928;&#22788;&#29702;&#20855;&#26377;&#36229;&#36807;10&#20010;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#36951;&#20256;&#32534;&#31243;&#30340;&#20351;&#29992;&#30456;&#21453;&#65292;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21487;&#22312;&#39640;&#36755;&#20837;&#32500;&#24230;&#19979;&#26377;&#25928;&#25193;&#23637;&#65292;&#24182;&#21033;&#29992;&#26799;&#24230;&#26041;&#27861;&#21152;&#36895;&#26041;&#31243;&#25628;&#32034;&#12290;&#24120;&#35265;&#30340;&#34920;&#36798;&#24335;&#22797;&#26434;&#24615;&#32422;&#26463;&#26041;&#27861;&#20381;&#36182;&#20110;&#22810;&#38454;&#27573;&#20462;&#21098;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#36825;&#24448;&#24448;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21363;SymbolNet&#65292;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#23454;&#29616;&#31526;&#21495;&#22238;&#24402;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#21333;&#20010;&#35757;&#32451;&#20013;&#21160;&#24577;&#20462;&#21098;&#27169;&#22411;&#26435;&#37325;&#12289;&#36755;&#20837;&#29305;&#24449;&#21644;&#25968;&#23398;&#36816;&#31639;&#31526;&#65292;&#21516;&#26102;&#20248;&#21270;&#35757;&#32451;&#25439;&#22833;&#21644;&#34920;&#36798;&#24335;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27599;&#20010;&#20462;&#21098;&#31867;&#22411;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#65292;&#35813;&#39033;&#21487;&#20197;&#33258;&#36866;&#24212;&#35843;&#25972;&#33258;&#36523;&#30340;&#24378;&#24230;&#65292;&#24182;&#23548;&#33268;&#25910;&#25947;&#21040;&#30446;&#26631;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#26080;&#27861;&#39640;&#25928;&#22788;&#29702;&#20855;&#26377;&#36229;&#36807;10&#20010;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrary to the use of genetic programming, the neural network approach to symbolic regression can scale well with high input dimension and leverage gradient methods for faster equation searching. Common ways of constraining expression complexity have relied on multistage pruning methods with fine-tuning, but these often lead to significant performance loss. In this work, we propose SymbolNet, a neural network approach to symbolic regression in a novel framework that enables dynamic pruning of model weights, input features, and mathematical operators in a single training, where both training loss and expression complexity are optimized simultaneously. We introduce a sparsity regularization term per pruning type, which can adaptively adjust its own strength and lead to convergence to a target sparsity level. In contrast to most existing symbolic regression methods that cannot efficiently handle datasets with more than $O$(10) inputs, we demonstrate the effectiveness of our model on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#20256;&#36755;&#20449;&#24687;&#29942;&#39048;&#26469;&#23454;&#29616;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21387;&#32553;&#34920;&#31034;&#20449;&#24687;&#21644;&#20445;&#30041;&#37325;&#35201;&#20449;&#24687;&#20043;&#38388;&#32500;&#25345;&#24179;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#21464;&#20998;&#25512;&#26029;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#21487;&#35745;&#31639;&#20272;&#35745;&#30340;DisTIB&#12290;</title><link>http://arxiv.org/abs/2311.01686</link><description>&lt;p&gt;
&#20351;&#29992;&#20256;&#36755;&#30340;&#20449;&#24687;&#29942;&#39048;&#23454;&#29616;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning with Transmitted Information Bottleneck. (arXiv:2311.01686v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#20256;&#36755;&#20449;&#24687;&#29942;&#39048;&#26469;&#23454;&#29616;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21387;&#32553;&#34920;&#31034;&#20449;&#24687;&#21644;&#20445;&#30041;&#37325;&#35201;&#20449;&#24687;&#20043;&#38388;&#32500;&#25345;&#24179;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#21464;&#20998;&#25512;&#26029;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#21487;&#35745;&#31639;&#20272;&#35745;&#30340;DisTIB&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#32534;&#30721;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#21407;&#22987;&#25968;&#25454;&#20449;&#24687;&#65292;&#21363;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#34429;&#28982;&#22312;&#34920;&#31034;&#20013;&#21033;&#29992;&#20449;&#24687;&#29702;&#35770;&#23545;&#20449;&#24687;&#36827;&#34892;&#35268;&#33539;&#21270;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#34920;&#31034;&#21387;&#32553;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65307;2&#65289;&#23545;&#34920;&#31034;&#30340;&#35299;&#32544;&#32422;&#26463;&#23384;&#22312;&#22797;&#26434;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20256;&#36755;&#20449;&#24687;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#26469;&#25551;&#36848;&#35299;&#32544;&#36807;&#31243;&#20013;&#36755;&#20837;&#21644;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"DisTIB"&#65288;&#29992;&#20110;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#20256;&#36755;&#20449;&#24687;&#29942;&#39048;&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#24179;&#34913;&#20449;&#24687;&#21387;&#32553;&#21644;&#20445;&#30041;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#37319;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#23548;&#20986;DisTIB&#30340;&#21487;&#35745;&#31639;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Encoding only the task-related information from the raw data, \ie, disentangled representation learning, can greatly contribute to the robustness and generalizability of models. Although significant advances have been made by regularizing the information in representations with information theory, two major challenges remain: 1) the representation compression inevitably leads to performance drop; 2) the disentanglement constraints on representations are in complicated optimization. To these issues, we introduce Bayesian networks with transmitted information to formulate the interaction among input and representations during disentanglement. Building upon this framework, we propose \textbf{DisTIB} (\textbf{T}ransmitted \textbf{I}nformation \textbf{B}ottleneck for \textbf{Dis}entangled representation learning), a novel objective that navigates the balance between information compression and preservation. We employ variational inference to derive a tractable estimation for DisTIB. This es
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;Node2Vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#65288;&#32463;&#36807;&#24230;&#20462;&#27491;&#30340;&#65289;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;k-means&#32858;&#31867;&#26041;&#27861;&#23545;&#36825;&#20123;&#23884;&#20837;&#36827;&#34892;&#31038;&#21306;&#24674;&#22797;&#26159;&#24369;&#19968;&#33268;&#30340;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#23884;&#20837;&#22312;&#33410;&#28857;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.17712</link><description>&lt;p&gt;
&#20351;&#29992;Node2Vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#36827;&#34892;&#31038;&#21306;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Community Detection and Classification Guarantees Using Embeddings Learned by Node2Vec. (arXiv:2310.17712v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;Node2Vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#65288;&#32463;&#36807;&#24230;&#20462;&#27491;&#30340;&#65289;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;k-means&#32858;&#31867;&#26041;&#27861;&#23545;&#36825;&#20123;&#23884;&#20837;&#36827;&#34892;&#31038;&#21306;&#24674;&#22797;&#26159;&#24369;&#19968;&#33268;&#30340;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#23884;&#20837;&#22312;&#33410;&#28857;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#32593;&#32476;&#30340;&#33410;&#28857;&#23884;&#20837;&#21040;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#30446;&#26631;&#65292;&#26377;&#21508;&#31181;&#24037;&#20855;&#21487;&#29992;&#12290;&#36825;&#20123;&#23884;&#20837;&#21487;&#20197;&#29992;&#20316;&#31038;&#21306;&#26816;&#27979;/&#33410;&#28857;&#32858;&#31867;&#25110;&#38142;&#25509;&#39044;&#27979;&#31561;&#20219;&#21153;&#30340;&#29305;&#24449;&#65292;&#20854;&#24615;&#33021;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#38500;&#20102;&#35889;&#32858;&#31867;&#26041;&#27861;&#20043;&#22806;&#65292;&#23545;&#20110;&#20854;&#20182;&#24120;&#29992;&#30340;&#23398;&#20064;&#23884;&#20837;&#26041;&#27861;&#65292;&#32570;&#20047;&#29702;&#35770;&#19978;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#30001;node2vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#29702;&#35770;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;node2vec&#29983;&#25104;&#30340;&#23884;&#20837;&#21521;&#37327;&#24212;&#29992;k-means&#32858;&#31867;&#21487;&#20197;&#23545;&#65288;&#32463;&#36807;&#24230;&#20462;&#27491;&#30340;&#65289;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#24369;&#19968;&#33268;&#30340;&#31038;&#21306;&#24674;&#22797;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36825;&#20123;&#23884;&#20837;&#22312;&#33410;&#28857;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20010;&#32467;&#26524;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#19982;&#32593;&#32476;&#25968;&#25454;&#30340;&#20854;&#20182;&#23884;&#20837;&#24037;&#20855;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding the nodes of a large network into an Euclidean space is a common objective in modern machine learning, with a variety of tools available. These embeddings can then be used as features for tasks such as community detection/node clustering or link prediction, where they achieve state of the art performance. With the exception of spectral clustering methods, there is little theoretical understanding for other commonly used approaches to learning embeddings. In this work we examine the theoretical properties of the embeddings learned by node2vec. Our main result shows that the use of k-means clustering on the embedding vectors produced by node2vec gives weakly consistent community recovery for the nodes in (degree corrected) stochastic block models. We also discuss the use of these embeddings for node and link prediction tasks. We demonstrate this result empirically, and examine how this relates to other embedding tools for network data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#25351;&#23548;&#30340;&#26041;&#24335;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03710</link><description>&lt;p&gt;
&#20195;&#29702;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#36890;&#29992;&#30340;&#38646;-shot&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Agent Instructs Large Language Models to be General Zero-Shot Reasoners. (arXiv:2310.03710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03710
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#25351;&#23548;&#30340;&#26041;&#24335;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#38646;-shot&#25512;&#29702;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#20027;&#20195;&#29702;&#65292;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#36827;&#19968;&#27493;&#37322;&#25918;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#25512;&#29702;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#26356;&#22810;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#29983;&#25104;&#12289;&#20998;&#31867;&#21644;&#25512;&#29702;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#20219;&#21153;&#65292;&#24182;&#22312;&#25105;&#20204;&#35780;&#20272;&#30340;29&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#22312;20&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;Vicuna-13b&#65288;13.3%&#65289;&#65292;Llama-2-70b-chat&#65288;23.2%&#65289;&#21644;GPT-3.5 Turbo&#65288;17.0%&#65289;&#12290;&#19982;&#38646;-shot&#24605;&#32500;&#38142;&#30456;&#27604;&#65292;&#25105;&#20204;&#23545;&#25512;&#29702;&#30340;&#25913;&#36827;&#24456;&#26126;&#26174;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;10.5%&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Llama-2-70b-chat&#30340;&#24615;&#33021;&#36229;&#36807;&#38646;-shot GPT-3.5 Turbo 10.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks. Specifically, we build an autonomous agent to instruct the reasoning process of large language models. We show this approach further unleashes the zero-shot reasoning abilities of large language models to more tasks. We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning. We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate. For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement in reasoning is striking, with an average increase of 10.5%. With our method, Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoCLIP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;AutoCLIP&#36890;&#36807;&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20174;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#25512;&#23548;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.16414</link><description>&lt;p&gt;
AutoCLIP: &#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models. (arXiv:2309.16414v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoCLIP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;AutoCLIP&#36890;&#36807;&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20174;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#25512;&#23548;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#26500;&#24314;&#30340;&#20998;&#31867;&#22120;&#22312;&#24191;&#27867;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#26681;&#25454;&#25552;&#31034;&#27169;&#26495;&#33258;&#21160;&#21019;&#24314;&#27599;&#20010;&#31867;&#21035;&#30340;&#25551;&#36848;&#31526;&#38598;&#30340;&#19981;&#21516;&#26041;&#24335;&#65292;&#21253;&#25324;&#25163;&#24037;&#35774;&#35745;&#30340;&#27169;&#26495;&#12289;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#30340;&#27169;&#26495;&#20197;&#21450;&#20174;&#38543;&#26426;&#21333;&#35789;&#21644;&#23383;&#31526;&#26500;&#24314;&#30340;&#27169;&#26495;&#12290;&#28982;&#32780;&#65292;&#20174;&#30456;&#24212;&#30340;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#23548;&#20986;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#65306;&#23558;&#22270;&#20687;&#30340;&#24179;&#22343;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#19982;&#32534;&#30721;&#22270;&#20687;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#26368;&#22823;&#21270;&#20197;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#24403;&#26576;&#20123;&#25551;&#36848;&#31526;&#27604;&#20854;&#20182;&#25551;&#36848;&#31526;&#26356;&#22909;&#22320;&#21305;&#37197;&#32473;&#23450;&#22270;&#20687;&#19978;&#30340;&#35270;&#35273;&#32447;&#32034;&#26102;&#65292;&#23558;&#25152;&#26377;&#31867;&#21035;&#25551;&#36848;&#31526;&#31561;&#26435;&#37325;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35843;&#35856;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;AutoCLIP&#12290;AutoCLIP&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#20102;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#36825;&#20123;&#26435;&#37325;&#26159;&#20174;s
&lt;/p&gt;
&lt;p&gt;
Classifiers built upon vision-language models such as CLIP have shown remarkable zero-shot performance across a broad range of image classification tasks. Prior work has studied different ways of automatically creating descriptor sets for every class based on prompt templates, ranging from manually engineered templates over templates obtained from a large language model to templates built from random words and characters. In contrast, deriving zero-shot classifiers from the respective encoded class descriptors has remained nearly unchanged, that is: classify to the class that maximizes the cosine similarity between its averaged encoded class descriptors and the encoded image. However, weighting all class descriptors equally can be suboptimal when certain descriptors match visual clues on a given image better than others. In this work, we propose AutoCLIP, a method for auto-tuning zero-shot classifiers. AutoCLIP assigns to each prompt template per-image weights, which are derived from s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26469;&#25554;&#20540;&#23665;&#21306;&#22825;&#27668;&#39044;&#25253;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24403;&#21069;&#35266;&#27979;&#25968;&#25454;&#21644;&#21608;&#22260;&#24179;&#21407;&#30340;&#39044;&#25253;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#22320;&#24418;&#20013;&#25968;&#20540;&#27169;&#25311;&#31934;&#24230;&#38477;&#20302;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#38477;&#27700;&#39044;&#27979;&#20013;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.13983</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#23665;&#21306;&#22825;&#27668;&#39044;&#25253;&#30340;&#25554;&#20540;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Interpolation of mountain weather forecasts by machine learning. (arXiv:2308.13983v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26469;&#25554;&#20540;&#23665;&#21306;&#22825;&#27668;&#39044;&#25253;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24403;&#21069;&#35266;&#27979;&#25968;&#25454;&#21644;&#21608;&#22260;&#24179;&#21407;&#30340;&#39044;&#25253;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#22320;&#24418;&#20013;&#25968;&#20540;&#27169;&#25311;&#31934;&#24230;&#38477;&#20302;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#38477;&#27700;&#39044;&#27979;&#20013;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#25968;&#20540;&#27169;&#25311;&#26041;&#27861;&#30340;&#36827;&#23637;&#25552;&#39640;&#20102;&#22825;&#27668;&#39044;&#25253;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#22320;&#24418;&#22914;&#23665;&#22320;&#22320;&#21306;&#65292;&#30001;&#20110;&#25968;&#20540;&#27169;&#25311;&#20013;&#20351;&#29992;&#20102;&#20960;&#20844;&#37324;&#24179;&#26041;&#30340;&#32593;&#26684;&#65292;&#31934;&#24230;&#20250;&#38477;&#20302;&#12290;&#34429;&#28982;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30452;&#25509;&#24212;&#29992;&#38590;&#20197;&#21033;&#29992;&#29289;&#29702;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#24403;&#21069;&#35266;&#27979;&#25968;&#25454;&#21644;&#21608;&#22260;&#24179;&#21407;&#30340;&#39044;&#25253;&#25968;&#25454;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#8220;&#25554;&#20540;&#8221;&#26410;&#26469;&#23665;&#21306;&#30340;&#22825;&#27668;&#12290;&#36890;&#24120;&#65292;&#22825;&#27668;&#39044;&#27979;&#20381;&#36182;&#20110;&#25968;&#20540;&#27169;&#25311;&#65292;&#22240;&#27492;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#34987;&#35270;&#20026;&#38388;&#25509;&#34701;&#21512;&#25968;&#20540;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#12290;&#36824;&#30740;&#31350;&#20102;&#22312;&#38477;&#27700;&#39044;&#27979;&#20013;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in numerical simulation methods based on physical models have enhanced the accuracy of weather forecasts. However, the precision diminishes in complex terrains like mountainous regions due to the several kilometers square grid used in numerical simulations. While statistical machine learning has also significantly advanced, its direct application is difficult to utilize physics knowledge. This paper proposes a method that employs machine learning to ``interpolate'' future weather in mountainous regions using current observed data and forecast data from surrounding plains. Generally, weather prediction relies on numerical simulations, so this approach can be considered a hybrid method that indirectly merges numerical simulation and machine learning. The use of binary cross-entropy in precipitation prediction is also examined.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#21487;&#25193;&#23637;&#28145;&#24230;&#21387;&#32553;&#24863;&#30693;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#27979;&#37327;-&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#30340;&#37319;&#26679;&#27604;&#29575;&#21644;&#30697;&#38453;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#21452;&#22495;&#25439;&#22833;&#21644;&#22235;&#20010;&#24674;&#22797;&#38454;&#27573;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25968;&#25454;/&#20449;&#24687;&#21033;&#29992;&#29575;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13777</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#21487;&#25193;&#23637;&#28145;&#24230;&#21387;&#32553;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Scalable Deep Compressed Sensing. (arXiv:2308.13777v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#21487;&#25193;&#23637;&#28145;&#24230;&#21387;&#32553;&#24863;&#30693;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#27979;&#37327;-&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#30340;&#37319;&#26679;&#27604;&#29575;&#21644;&#30697;&#38453;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#21452;&#22495;&#25439;&#22833;&#21644;&#22235;&#20010;&#24674;&#22797;&#38454;&#27573;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25968;&#25454;/&#20449;&#24687;&#21033;&#29992;&#29575;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21387;&#32553;&#24863;&#30693;&#65288;CS&#65289;&#26159;&#38477;&#20302;&#37319;&#26679;&#25104;&#26412;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#24037;&#20855;&#12290;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;CS&#26041;&#27861;&#22312;&#25910;&#38598;&#26631;&#35760;&#30340;&#27979;&#37327;-&#22320;&#38754;&#30495;&#23454;&#65288;GT&#65289;&#25968;&#25454;&#21644;&#25512;&#24191;&#21040;&#23454;&#38469;&#24212;&#29992;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#21487;&#25193;&#23637;&#28145;&#24230;CS&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#20010;&#31216;&#20026;SCL&#30340;&#23398;&#20064;&#26041;&#26696;&#21644;&#19968;&#20010;&#21517;&#20026;SCNet&#30340;&#32593;&#32476;&#31995;&#21015;&#65292;&#23427;&#19981;&#38656;&#35201;GT&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#19968;&#26086;&#22312;&#37096;&#20998;&#27979;&#37327;&#38598;&#19978;&#35757;&#32451;&#23436;&#27605;&#23601;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#30340;&#37319;&#26679;&#27604;&#29575;&#21644;&#30697;&#38453;&#12290;&#25105;&#20204;&#30340;SCL&#21253;&#21547;&#19968;&#20010;&#21452;&#22495;&#25439;&#22833;&#21644;&#19968;&#20010;&#22235;&#38454;&#27573;&#24674;&#22797;&#31574;&#30053;&#12290;&#21069;&#32773;&#40723;&#21169;&#20004;&#20010;&#27979;&#37327;&#37096;&#20998;&#30340;&#20132;&#21449;&#19968;&#33268;&#24615;&#20197;&#21450;&#37319;&#26679;-&#37325;&#26500;&#24490;&#29615;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#25968;&#25454;/&#20449;&#24687;&#21033;&#29992;&#29575;&#12290;&#21518;&#32773;&#21487;&#20197;&#36880;&#27493;&#21033;&#29992;&#22806;&#37096;&#27979;&#37327;&#20013;&#30340;&#24120;&#35265;&#20449;&#21495;&#20808;&#39564;&#21644;&#27979;&#35797;&#26679;&#26412;&#20197;&#21450;&#23398;&#20064;&#30340;NN&#30340;&#20869;&#37096;&#29305;&#24449;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compressed sensing (CS) is a promising tool for reducing sampling costs. Current deep neural network (NN)-based CS methods face challenges in collecting labeled measurement-ground truth (GT) data and generalizing to real applications. This paper proposes a novel $\mathbf{S}$elf-supervised s$\mathbf{C}$alable deep CS method, comprising a $\mathbf{L}$earning scheme called $\mathbf{SCL}$ and a family of $\mathbf{Net}$works named $\mathbf{SCNet}$, which does not require GT and can handle arbitrary sampling ratios and matrices once trained on a partial measurement set. Our SCL contains a dual-domain loss and a four-stage recovery strategy. The former encourages a cross-consistency on two measurement parts and a sampling-reconstruction cycle-consistency regarding arbitrary ratios and matrices to maximize data/information utilization. The latter can progressively leverage common signal prior in external measurements and internal characteristics of test samples and learned NNs to improve accur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#26410;&#30417;&#27979;&#31449;&#28857;&#30340;&#27700;&#36164;&#28304;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#27827;&#27969;&#27969;&#37327;&#12289;&#27700;&#36136;&#31561;&#30456;&#20851;&#21464;&#37327;&#65292;&#24182;&#35752;&#35770;&#20102;&#34701;&#21512;&#38598;&#27700;&#21306;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.09766</link><description>&lt;p&gt;
&#26410;&#30417;&#27979;&#31449;&#28857;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#27700;&#36164;&#28304;&#20013;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Time Series Predictions in Unmonitored Sites: A Survey of Machine Learning Techniques in Water Resources. (arXiv:2308.09766v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#26410;&#30417;&#27979;&#31449;&#28857;&#30340;&#27700;&#36164;&#28304;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#27827;&#27969;&#27969;&#37327;&#12289;&#27700;&#36136;&#31561;&#30456;&#20851;&#21464;&#37327;&#65292;&#24182;&#35752;&#35770;&#20102;&#34701;&#21512;&#38598;&#27700;&#21306;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26410;&#30417;&#27979;&#31449;&#28857;&#20013;&#21160;&#24577;&#29615;&#22659;&#21464;&#37327;&#30340;&#39044;&#27979;&#20173;&#28982;&#26159;&#27700;&#36164;&#28304;&#31185;&#23398;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#19990;&#30028;&#19978;&#22823;&#37096;&#20998;&#30340;&#28129;&#27700;&#36164;&#28304;&#27809;&#26377;&#36866;&#24403;&#30340;&#30417;&#27979;&#20851;&#38190;&#29615;&#22659;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#32780;&#23545;&#27827;&#27969;&#27969;&#37327;&#21644;&#27700;&#36136;&#31561;&#27700;&#25991;&#21464;&#37327;&#36827;&#34892;&#24191;&#27867;&#39044;&#27979;&#30340;&#38656;&#27714;&#30001;&#20110;&#27668;&#20505;&#21644;&#22303;&#22320;&#21033;&#29992;&#21464;&#21270;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#36234;&#26469;&#36234;&#36843;&#20999;&#65292;&#24182;&#24433;&#21709;&#30528;&#27700;&#36164;&#28304;&#12290;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#20174;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#36807;&#31243;&#21644;&#32463;&#39564;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#27700;&#25991;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#36234;&#26469;&#36234;&#20248;&#36234;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#27827;&#27969;&#27969;&#37327;&#12289;&#27700;&#36136;&#21644;&#20854;&#20182;&#27700;&#36164;&#28304;&#39044;&#27979;&#20013;&#30340;&#30456;&#20851;&#26368;&#26032;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#21033;&#29992;&#26032;&#20852;&#26041;&#27861;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#22312;&#38598;&#27700;&#21306;&#29305;&#24449;&#34701;&#21512;&#26041;&#38754;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction of dynamic environmental variables in unmonitored sites remains a long-standing challenge for water resources science. The majority of the world's freshwater resources have inadequate monitoring of critical environmental variables needed for management. Yet, the need to have widespread predictions of hydrological variables such as river flow and water quality has become increasingly urgent due to climate and land use change over the past decades, and their associated impacts on water resources. Modern machine learning methods increasingly outperform their process-based and empirical model counterparts for hydrologic time series prediction with their ability to extract information from large, diverse data sets. We review relevant state-of-the art applications of machine learning for streamflow, water quality, and other water resources prediction and discuss opportunities to improve the use of machine learning with emerging methods for incorporating watershed characteristics i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#22270;&#19978;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#37325;&#26032;&#34920;&#36848;&#20026;&#38750;&#20984;&#25512;&#24191;&#65292;&#36890;&#36807;&#27969;&#24418;&#23545;&#40784;&#38382;&#39064;&#30340;&#35299;&#26469;&#25214;&#21040;&#22909;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#20013;&#24515;&#24230;&#24230;&#37327;&#26469;&#36873;&#25321;&#26377;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#26412;&#26041;&#27861;&#22312;&#20302;&#26631;&#31614;&#29575;&#19979;&#20855;&#26377;&#26356;&#20302;&#30340;&#20998;&#31867;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.00142</link><description>&lt;p&gt;
&#22312;Stiefel&#27969;&#24418;&#19978;&#30340;&#21322;&#30417;&#30563;Laplacian&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Laplacian Learning on Stiefel Manifolds. (arXiv:2308.00142v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#22270;&#19978;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#37325;&#26032;&#34920;&#36848;&#20026;&#38750;&#20984;&#25512;&#24191;&#65292;&#36890;&#36807;&#27969;&#24418;&#23545;&#40784;&#38382;&#39064;&#30340;&#35299;&#26469;&#25214;&#21040;&#22909;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#20013;&#24515;&#24230;&#24230;&#37327;&#26469;&#36873;&#25321;&#26377;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#26412;&#26041;&#27861;&#22312;&#20302;&#26631;&#31614;&#29575;&#19979;&#20855;&#26377;&#26356;&#20302;&#30340;&#20998;&#31867;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20302;&#26631;&#31614;&#29575;&#19979;&#32463;&#20856;Laplace&#23398;&#20064;&#31639;&#27861;&#36864;&#21270;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22522;&#20110;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#37325;&#26032;&#34920;&#36848;&#20026;\emph{Trust-Region Subproblem} (TRS) &#30340;&#38750;&#20984;&#25512;&#24191;&#12290;&#36825;&#20010;&#25913;&#36827;&#26159;&#21463;&#21040;Laplacian&#29305;&#24449;&#21521;&#37327;&#22312;&#26080;&#38480;&#26410;&#26631;&#35760;&#25968;&#25454;&#26497;&#38480;&#19979;&#30340;&#21487;&#35299;&#24615;&#30340;&#21551;&#21457;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#19968;&#38454;&#26465;&#20214;&#26263;&#31034;&#20102;&#27969;&#24418;&#23545;&#40784;&#38382;&#39064;&#30340;&#35299;&#65292;&#32780;&#19988;&#32463;&#20856;&#30340;\emph{Orthogonal Procrustes} &#38382;&#39064;&#30340;&#35299;&#21487;&#20197;&#34987;&#29992;&#20110;&#39640;&#25928;&#22320;&#25214;&#21040;&#36866;&#20110;&#36827;&#19968;&#27493;&#32454;&#21270;&#30340;&#22909;&#30340;&#20998;&#31867;&#22120;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#20302;&#26631;&#31614;&#29575;&#19979;&#36873;&#25321;&#26377;&#30417;&#30563;&#26679;&#26412;&#30340;&#20851;&#38190;&#24615;&#12290;&#25105;&#20204;&#29992;&#22270;Laplacian&#30340;&#26576;&#20010;&#23376;&#30697;&#38453;&#30340;&#20027;&#29305;&#24449;&#21521;&#37327;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20013;&#24515;&#24230;&#24230;&#37327;&#65292;&#26469;&#34920;&#24449;&#20449;&#24687;&#26679;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#20302;&#30340;&#20998;&#31867;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the need to address the degeneracy of canonical Laplace learning algorithms in low label rates, we propose to reformulate graph-based semi-supervised learning as a nonconvex generalization of a \emph{Trust-Region Subproblem} (TRS). This reformulation is motivated by the well-posedness of Laplacian eigenvectors in the limit of infinite unlabeled data. To solve this problem, we first show that a first-order condition implies the solution of a manifold alignment problem and that solutions to the classical \emph{Orthogonal Procrustes} problem can be used to efficiently find good classifiers that are amenable to further refinement. Next, we address the criticality of selecting supervised samples at low-label rates. We characterize informative samples with a novel measure of centrality derived from the principal eigenvectors of a certain submatrix of the graph Laplacian. We demonstrate that our framework achieves lower classification error compared to recent state-of-the-art and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#23545;&#25239;&#24615;&#30340;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#33719;&#24471;&#26368;&#20248;&#30340;&#38169;&#26631;&#29575;&#12290;&#22312;&#27809;&#26377;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23454;&#29616;&#19982;&#27931;&#20234;&#24503;&#31639;&#27861;&#31867;&#20284;&#30340;&#29702;&#35770;&#20445;&#35777;.</title><link>http://arxiv.org/abs/2306.09977</link><description>&lt;p&gt;
&#24102;&#26377;&#26368;&#20248;&#24615;&#20445;&#35777;&#30340;&#23545;&#25239;&#40065;&#26834;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Adversarially robust clustering with optimality guarantees. (arXiv:2306.09977v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#23545;&#25239;&#24615;&#30340;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#33719;&#24471;&#26368;&#20248;&#30340;&#38169;&#26631;&#29575;&#12290;&#22312;&#27809;&#26377;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#23454;&#29616;&#19982;&#27931;&#20234;&#24503;&#31639;&#27861;&#31867;&#20284;&#30340;&#29702;&#35770;&#20445;&#35777;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23545;&#26469;&#33258;&#20122;&#39640;&#26031;&#28151;&#21512;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#21487;&#35777;&#26126;&#36798;&#21040;&#26368;&#20248;&#38169;&#26631;&#29575;&#30340;&#26041;&#27861;&#65292;&#22914;&#27931;&#20234;&#24503;&#31639;&#27861;&#65292;&#36890;&#24120;&#23481;&#26131;&#21463;&#21040;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#20284;&#20046;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#32858;&#31867;&#26041;&#27861;&#19981;&#30693;&#36947;&#26159;&#21542;&#28385;&#36275;&#26368;&#20248;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21363;&#20351;&#20801;&#35768;&#20986;&#29616;&#23545;&#25239;&#24615;&#30340;&#24322;&#24120;&#20540;&#65292;&#20063;&#33021;&#33719;&#24471;&#26368;&#20248;&#30340;&#38169;&#26631;&#29575;&#12290;&#24403;&#28385;&#36275;&#24369;&#21021;&#22987;&#21270;&#26465;&#20214;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#24120;&#25968;&#27425;&#36845;&#20195;&#20013;&#23454;&#29616;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;&#22312;&#27809;&#26377;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22266;&#23450;&#32500;&#24230;&#19978;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20445;&#35777;&#19982;&#27931;&#20234;&#24503;&#31639;&#27861;&#31867;&#20284;&#12290;&#22312;&#21508;&#31181;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20197;&#25903;&#25345;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of clustering data points coming from sub-Gaussian mixtures. Existing methods that provably achieve the optimal mislabeling error, such as the Lloyd algorithm, are usually vulnerable to outliers. In contrast, clustering methods seemingly robust to adversarial perturbations are not known to satisfy the optimal statistical guarantees. We propose a simple algorithm that obtains the optimal mislabeling rate even when we allow adversarial outliers to be present. Our algorithm achieves the optimal error rate in constant iterations when a weak initialization condition is satisfied. In the absence of outliers, in fixed dimensions, our theoretical guarantees are similar to that of the Lloyd algorithm. Extensive experiments on various simulated data sets are conducted to support the theoretical guarantees of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.08158</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#23398;&#20064;&#21040;&#38750;&#39044;&#26399;&#30340;&#20559;&#35265;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#35770;&#25991;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20559;&#35265;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#21361;&#23475;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#20511;&#37492;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20559;&#35265;&#31867;&#22411;&#12289;&#37327;&#21270;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#12290;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#23545;&#20110;&#37327;&#21270;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#20559;&#35265;&#24230;&#37327;&#24182;&#19981;&#28041;&#21450;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20559;&#35265;&#65292;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#26159;&#34920;&#38754;&#30340;&#65292;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
&lt;/p&gt;</description></item><item><title>LearnDefend&#26159;&#19968;&#31181;&#23398;&#20064;&#38450;&#24481;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#25239;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#26377;&#38024;&#23545;&#24615;&#27169;&#22411;&#20013;&#27602;&#25915;&#20987;&#12290;&#23427;&#20351;&#29992;&#19968;&#20010;&#36739;&#23567;&#30340;&#38450;&#24481;&#25968;&#25454;&#38598;&#65292;&#20272;&#35745;&#23458;&#25143;&#31471;&#26356;&#26032;&#34987;&#27745;&#26579;&#30340;&#27010;&#29575;&#65292;&#36890;&#36807;&#23398;&#20064;&#27602;&#25968;&#25454;&#26816;&#27979;&#22120;&#27169;&#22411;&#24182;&#20351;&#29992;&#32806;&#21512;&#30340;&#20248;&#21270;&#26041;&#27861;&#20272;&#35745;&#27602;&#25968;&#25454;&#26816;&#27979;&#22120;&#21644;&#23458;&#25143;&#31471;&#37325;&#35201;&#24615;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.02022</link><description>&lt;p&gt;
LearnDefend&#65306;&#23398;&#20064;&#23545;&#25239;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26377;&#38024;&#23545;&#24615;&#30340;&#27169;&#22411;&#20013;&#27602;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
LearnDefend: Learning to Defend against Targeted Model-Poisoning Attacks on Federated Learning. (arXiv:2305.02022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02022
&lt;/p&gt;
&lt;p&gt;
LearnDefend&#26159;&#19968;&#31181;&#23398;&#20064;&#38450;&#24481;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#25239;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#26377;&#38024;&#23545;&#24615;&#27169;&#22411;&#20013;&#27602;&#25915;&#20987;&#12290;&#23427;&#20351;&#29992;&#19968;&#20010;&#36739;&#23567;&#30340;&#38450;&#24481;&#25968;&#25454;&#38598;&#65292;&#20272;&#35745;&#23458;&#25143;&#31471;&#26356;&#26032;&#34987;&#27745;&#26579;&#30340;&#27010;&#29575;&#65292;&#36890;&#36807;&#23398;&#20064;&#27602;&#25968;&#25454;&#26816;&#27979;&#22120;&#27169;&#22411;&#24182;&#20351;&#29992;&#32806;&#21512;&#30340;&#20248;&#21270;&#26041;&#27861;&#20272;&#35745;&#27602;&#25968;&#25454;&#26816;&#27979;&#22120;&#21644;&#23458;&#25143;&#31471;&#37325;&#35201;&#24615;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#26377;&#38024;&#23545;&#24615;&#27169;&#22411;&#20013;&#27602;&#25915;&#20987;&#26500;&#25104;&#20102;&#24040;&#22823;&#30340;&#23041;&#32961;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#30446;&#26631;&#36793;&#32536;&#26696;&#20363;&#22411;&#25915;&#20987;&#65288;&#23545;&#36755;&#20837;&#31354;&#38388;&#30340;&#19968;&#23567;&#37096;&#20998;&#36827;&#34892;&#38024;&#23545;&#24615;&#25915;&#20987;&#65289;&#20960;&#20046;&#26080;&#27861;&#36890;&#36807;&#29616;&#26377;&#30340;&#38450;&#24481;&#31574;&#30053;&#36827;&#34892;&#21453;&#20987;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#36739;&#23567;&#30340;&#38450;&#24481;&#25968;&#25454;&#38598;&#35774;&#35745;&#19968;&#31181;&#23398;&#20064;&#38450;&#24481;&#31574;&#30053;&#26469;&#24212;&#23545;&#27492;&#31867;&#25915;&#20987;&#12290;&#38450;&#24481;&#25968;&#25454;&#38598;&#21487;&#20197;&#30001;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#30340;&#20013;&#22830;&#31649;&#29702;&#26426;&#26500;&#25910;&#38598;&#65292;&#20854;&#20013;&#24212;&#21253;&#21547;&#19968;&#20123;&#34987;&#27745;&#26579;&#30340;&#21644;&#27809;&#26377;&#34987;&#27745;&#26579;&#30340;&#31034;&#20363;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;LearnDefend&#20250;&#20272;&#35745;&#23458;&#25143;&#31471;&#26356;&#26032;&#20855;&#26377;&#24694;&#24847;&#30340;&#27010;&#29575;&#12290;&#38450;&#24481;&#25968;&#25454;&#38598;&#20013;&#30340;&#31034;&#20363;&#19981;&#38656;&#35201;&#20107;&#20808;&#26631;&#35760;&#20026;&#34987;&#27745;&#26579;&#25110;&#26410;&#34987;&#27745;&#26579;&#12290;&#25105;&#20204;&#36824;&#23398;&#20064;&#20102;&#19968;&#20010;&#21487;&#29992;&#20110;&#26631;&#35760;&#38450;&#24481;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#31034;&#20363;&#20026;&#24178;&#20928;&#25110;&#27745;&#26579;&#30340;&#27602;&#25968;&#25454;&#26816;&#27979;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#32806;&#21512;&#30340;&#20248;&#21270;&#26041;&#27861;&#26469;&#20272;&#35745;&#27602;&#25968;&#25454;&#26816;&#27979;&#22120;&#21644;&#23458;&#25143;&#31471;&#37325;&#35201;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LearnDefend&#33021;&#22815;&#25104;&#21151;&#24212;&#23545;&#26377;&#38024;&#23545;&#24615;&#27169;&#22411;&#20013;&#27602;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Targeted model poisoning attacks pose a significant threat to federated learning systems. Recent studies show that edge-case targeted attacks, which target a small fraction of the input space are nearly impossible to counter using existing fixed defense strategies. In this paper, we strive to design a learned-defense strategy against such attacks, using a small defense dataset. The defense dataset can be collected by the central authority of the federated learning task, and should contain a mix of poisoned and clean examples. The proposed framework, LearnDefend, estimates the probability of a client update being malicious. The examples in defense dataset need not be pre-marked as poisoned or clean. We also learn a poisoned data detector model which can be used to mark each example in the defense dataset as clean or poisoned. We estimate the poisoned data detector and the client importance models in a coupled optimization approach. Our experiments demonstrate that LearnDefend is capable
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#25193;&#25955;&#26144;&#23556;&#31890;&#23376;&#31995;&#32479;(DMPS)&#65292;&#21487;&#20197;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;&#24314;&#27169;&#65292;&#23454;&#39564;&#34920;&#26126;&#22312;&#21253;&#21547;&#27969;&#24418;&#32467;&#26500;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.00200</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#30340;&#31890;&#23376;&#31995;&#32479;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion map particle systems for generative modeling. (arXiv:2304.00200v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#25193;&#25955;&#26144;&#23556;&#31890;&#23376;&#31995;&#32479;(DMPS)&#65292;&#21487;&#20197;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;&#24314;&#27169;&#65292;&#23454;&#39564;&#34920;&#26126;&#22312;&#21253;&#21547;&#27969;&#24418;&#32467;&#26500;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#26144;&#23556;&#31890;&#23376;&#31995;&#32479;(DMPS)&#65292;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#21644;Laplacian&#35843;&#25972;&#30340;Wasserstein&#26799;&#24230;&#19979;&#38477;&#65288;LAWGD&#65289;&#12290;&#25193;&#25955;&#26144;&#23556;&#34987;&#29992;&#26469;&#20174;&#26679;&#26412;&#20013;&#36817;&#20284;Langevin&#25193;&#25955;&#36807;&#31243;&#30340;&#29983;&#25104;&#22120;&#65292;&#20174;&#32780;&#23398;&#20064;&#28508;&#22312;&#30340;&#25968;&#25454;&#29983;&#25104;&#27969;&#24418;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;LAWGD&#33021;&#22815;&#22312;&#21512;&#36866;&#30340;&#26680;&#20989;&#25968;&#36873;&#25321;&#19979;&#39640;&#25928;&#22320;&#20174;&#30446;&#26631;&#20998;&#24067;&#20013;&#25277;&#26679;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#36890;&#36807;&#25193;&#25955;&#26144;&#23556;&#35745;&#31639;&#29983;&#25104;&#22120;&#30340;&#35889;&#36924;&#36817;&#26469;&#26500;&#36896;&#26680;&#20989;&#25968;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;&#20855;&#26377;&#27969;&#24418;&#32467;&#26500;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel diffusion map particle system (DMPS) for generative modeling, based on diffusion maps and Laplacian-adjusted Wasserstein gradient descent (LAWGD). Diffusion maps are used to approximate the generator of the Langevin diffusion process from samples, and hence to learn the underlying data-generating manifold. On the other hand, LAWGD enables efficient sampling from the target distribution given a suitable choice of kernel, which we construct here via a spectral approximation of the generator, computed with diffusion maps. Numerical experiments show that our method outperforms others on synthetic datasets, including examples with manifold structure.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26368;&#20248;&#25191;&#34892;&#20132;&#26131;&#26234;&#33021;&#20307;&#19982;&#21453;&#24212;&#24335;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#37329;&#34701;&#24066;&#22330;&#27169;&#22411;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#24179;&#34913;&#25191;&#34892;&#24046;&#20215;&#21644;&#26410;&#33021;&#21450;&#26102;&#25191;&#34892;&#35746;&#21333;&#30340;&#24809;&#32602;&#65292;&#35828;&#26126;&#20102;&#22870;&#21169;&#20989;&#25968;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#25968;&#37327;&#12289;&#21021;&#22987;&#35746;&#21333;&#22823;&#23567;&#21644;&#29366;&#24577;&#31354;&#38388;&#30340;&#21464;&#21270;&#65292;&#20250;&#23545;&#26368;&#23567;&#26234;&#33021;&#24066;&#22330;&#27169;&#25311;&#36896;&#25104;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.07393</link><description>&lt;p&gt;
&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#19982;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#24066;&#22330;&#27169;&#22411;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Many learning agents interacting with an agent-based market model. (arXiv:2303.07393v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26368;&#20248;&#25191;&#34892;&#20132;&#26131;&#26234;&#33021;&#20307;&#19982;&#21453;&#24212;&#24335;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#37329;&#34701;&#24066;&#22330;&#27169;&#22411;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#24179;&#34913;&#25191;&#34892;&#24046;&#20215;&#21644;&#26410;&#33021;&#21450;&#26102;&#25191;&#34892;&#35746;&#21333;&#30340;&#24809;&#32602;&#65292;&#35828;&#26126;&#20102;&#22870;&#21169;&#20989;&#25968;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#25968;&#37327;&#12289;&#21021;&#22987;&#35746;&#21333;&#22823;&#23567;&#21644;&#29366;&#24577;&#31354;&#38388;&#30340;&#21464;&#21270;&#65292;&#20250;&#23545;&#26368;&#23567;&#26234;&#33021;&#24066;&#22330;&#27169;&#25311;&#36896;&#25104;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26368;&#20248;&#25191;&#34892;&#20132;&#26131;&#26234;&#33021;&#20307;&#19982;&#22312;&#20107;&#20214;&#26102;&#38388;&#19979;&#30340;&#21453;&#24212;&#24335;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#37329;&#34701;&#24066;&#22330;&#27169;&#22411;&#30340;&#21160;&#24577;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#27169;&#22411;&#20195;&#34920;&#20102;&#19968;&#20010;&#24066;&#22330;&#29983;&#24577;&#31995;&#32479;&#65292;&#30001;&#19977;&#20010;&#33829;&#20859;&#32423;&#21035;&#20195;&#34920;&#65306;&#26368;&#20248;&#25191;&#34892;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#26368;&#23567;&#26234;&#33021;&#30340;&#27969;&#21160;&#24615;&#38656;&#35201;&#32773;&#21644;&#24555;&#36895;&#30340;&#30005;&#23376;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#12290;&#26368;&#20248;&#25191;&#34892;&#20195;&#29702;&#31867;&#21035;&#21253;&#25324;&#20080;&#20837;&#21644;&#21334;&#20986;&#20195;&#29702;&#65292;&#21487;&#20197;&#20351;&#29992;&#38480;&#20215;&#21333;&#21644;&#24066;&#20215;&#21333;&#30340;&#32452;&#21512;&#65292;&#25110;&#32773;&#20165;&#20351;&#29992;&#24066;&#20215;&#21333;&#36827;&#34892;&#20132;&#26131;&#12290;&#22870;&#21169;&#20989;&#25968;&#26126;&#30830;&#24179;&#34913;&#20102;&#20132;&#26131;&#25191;&#34892;&#24046;&#20215;&#19982;&#26410;&#33021;&#21450;&#26102;&#25191;&#34892;&#35746;&#21333;&#30340;&#24809;&#32602;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22810;&#20010;&#31454;&#20105;&#23398;&#20064;&#26234;&#33021;&#20307;&#22914;&#20309;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#12289;&#21021;&#22987;&#35746;&#21333;&#30340;&#22823;&#23567;&#21644;&#29992;&#20110;&#23398;&#20064;&#30340;&#29366;&#24577;&#31354;&#38388;&#30340;&#20989;&#25968;&#24433;&#21709;&#26368;&#23567;&#26234;&#33021;&#24066;&#22330;&#27169;&#25311;&#12290;&#25105;&#20204;&#20351;&#29992;&#30456;&#31354;&#38388;&#22270;&#26469;&#30740;&#31350;ABM&#30340;&#21160;&#24577;&#65292;&#24403;&#29305;&#23450;&#35268;&#33539;&#34987;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
We consider the dynamics and the interactions of multiple reinforcement learning optimal execution trading agents interacting with a reactive Agent-Based Model (ABM) of a financial market in event time. The model represents a market ecology with 3-trophic levels represented by: optimal execution learning agents, minimally intelligent liquidity takers, and fast electronic liquidity providers. The optimal execution agent classes include buying and selling agents that can either use a combination of limit orders and market orders, or only trade using market orders. The reward function explicitly balances trade execution slippage against the penalty of not executing the order timeously. This work demonstrates how multiple competing learning agents impact a minimally intelligent market simulation as functions of the number of agents, the size of agents' initial orders, and the state spaces used for learning. We use phase space plots to examine the dynamics of the ABM, when various specifica
&lt;/p&gt;</description></item></channel></rss>