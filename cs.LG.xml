<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#21487;&#23398;&#20064;&#24310;&#36831;&#32447;&#23454;&#29616;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#30340;&#21442;&#25968;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23460;&#20869;&#22768;&#23398;&#29305;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2404.00082</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#21644;&#21487;&#23398;&#20064;&#24310;&#36831;&#32447;&#30340;&#25968;&#25454;&#39537;&#21160;&#23460;&#20869;&#22768;&#23398;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Room Acoustic Modeling Via Differentiable Feedback Delay Networks With Learnable Delay Lines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00082
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#23398;&#20064;&#24310;&#36831;&#32447;&#23454;&#29616;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#30340;&#21442;&#25968;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23460;&#20869;&#22768;&#23398;&#29305;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#35774;&#35745;&#20154;&#24037;&#28151;&#21709;&#31639;&#27861;&#65292;&#26088;&#22312;&#27169;&#25311;&#29289;&#29702;&#29615;&#22659;&#30340;&#23460;&#20869;&#22768;&#23398;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24310;&#36831;&#32593;&#32476;&#27169;&#22411;&#30340;&#33258;&#21160;&#21442;&#25968;&#35843;&#25972;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#65288;FDN&#65289;&#30340;&#21442;&#25968;&#65292;&#20351;&#20854;&#36755;&#20986;&#21576;&#29616;&#20986;&#25152;&#27979;&#24471;&#30340;&#23460;&#20869;&#33033;&#20914;&#21709;&#24212;&#30340;&#24863;&#30693;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00082v1 Announce Type: cross  Abstract: Over the past few decades, extensive research has been devoted to the design of artificial reverberation algorithms aimed at emulating the room acoustics of physical environments. Despite significant advancements, automatic parameter tuning of delay-network models remains an open challenge. We introduce a novel method for finding the parameters of a Feedback Delay Network (FDN) such that its output renders the perceptual qualities of a measured room impulse response. The proposed approach involves the implementation of a differentiable FDN with trainable delay lines, which, for the first time, allows us to simultaneously learn each and every delay-network parameter via backpropagation. The iterative optimization process seeks to minimize a time-domain loss function incorporating differentiable terms accounting for energy decay and echo density. Through experimental validation, we show that the proposed method yields time-invariant freq
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#22522;&#20934;&#65288;SRB&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#23545;&#21508;&#31181;&#30772;&#22351;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;&#26576;&#20123;&#24314;&#27169;&#36873;&#25321;&#26377;&#21161;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#19981;&#21516;&#20154;&#21475;&#20122;&#32452;&#19978;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.07937</link><description>&lt;p&gt;
&#35821;&#38899;&#40065;&#26834;&#22522;&#20934;&#65306;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#40065;&#26834;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Speech Robust Bench: A Robustness Benchmark For Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07937
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#22522;&#20934;&#65288;SRB&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#23545;&#21508;&#31181;&#30772;&#22351;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;&#26576;&#20123;&#24314;&#27169;&#36873;&#25321;&#26377;&#21161;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#19981;&#21516;&#20154;&#21475;&#20122;&#32452;&#19978;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#30830;&#20445;&#23427;&#20204;&#22312;&#29289;&#29702;&#19990;&#30028;&#21644;&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#21508;&#31181;&#30772;&#22351;&#19979;&#36827;&#34892;&#21487;&#38752;&#39044;&#27979;&#21464;&#24471;&#24840;&#21457;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#38899;&#40065;&#26834;&#22522;&#20934;&#65288;SRB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;ASR&#27169;&#22411;&#23545;&#21508;&#31181;&#30772;&#22351;&#30340;&#40065;&#26834;&#24615;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;SRB&#30001;69&#20010;&#36755;&#20837;&#25200;&#21160;&#32452;&#25104;&#65292;&#26088;&#22312;&#27169;&#25311;ASR&#27169;&#22411;&#21487;&#33021;&#22312;&#29289;&#29702;&#19990;&#30028;&#21644;&#25968;&#23383;&#19990;&#30028;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#30772;&#22351;&#12290;&#25105;&#20204;&#20351;&#29992;SRB&#26469;&#35780;&#20272;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;ASR&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#27169;&#22411;&#22823;&#23567;&#21644;&#26576;&#20123;&#24314;&#27169;&#36873;&#25321;&#65288;&#22914;&#31163;&#25955;&#34920;&#31034;&#21644;&#33258;&#25105;&#35757;&#32451;&#65289;&#20284;&#20046;&#26377;&#21161;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#27492;&#20998;&#26512;&#25193;&#23637;&#21040;&#34913;&#37327;ASR&#27169;&#22411;&#22312;&#26469;&#33258;&#21508;&#31181;&#20154;&#21475;&#20122;&#32452;&#30340;&#25968;&#25454;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#21363;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20351;&#29992;&#32773;&#20197;&#21450;&#30007;&#24615;&#21644;&#22899;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#22312;&#19981;&#21516;&#20122;&#32452;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07937v1 Announce Type: cross  Abstract: As Automatic Speech Recognition (ASR) models become ever more pervasive, it is important to ensure that they make reliable predictions under corruptions present in the physical and digital world. We propose Speech Robust Bench (SRB), a comprehensive benchmark for evaluating the robustness of ASR models to diverse corruptions. SRB is composed of 69 input perturbations which are intended to simulate various corruptions that ASR models may encounter in the physical and digital world. We use SRB to evaluate the robustness of several state-of-the-art ASR models and observe that model size and certain modeling choices such as discrete representations, and self-training appear to be conducive to robustness. We extend this analysis to measure the robustness of ASR models on data from various demographic subgroups, namely English and Spanish speakers, and males and females, and observed noticeable disparities in the model's robustness across su
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#20010;&#20511;&#21161;LLM&#25216;&#26415;&#26500;&#24314;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2403.00781</link><description>&lt;p&gt;
ChatDiet&#65306;&#36890;&#36807;LLM&#22686;&#24378;&#26694;&#26550;&#36171;&#33021;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00781
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#20010;&#20511;&#21161;LLM&#25216;&#26415;&#26500;&#24314;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#23545;&#20581;&#24247;&#30340;&#28145;&#36828;&#24433;&#21709;&#20351;&#24471;&#20808;&#36827;&#30340;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#26381;&#21153;&#25104;&#20026;&#24517;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#32570;&#20047;&#20010;&#24615;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#20114;&#21160;&#24615;&#31561;&#20851;&#38190;&#20803;&#32032;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24102;&#26469;&#20102;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#23427;&#20204;&#21333;&#29420;&#30340;&#20351;&#29992;&#26410;&#33021;&#23454;&#29616;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChatDiet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#39537;&#21160;&#26694;&#26550;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#20010;&#24615;&#21270;&#33829;&#20859;&#23548;&#21521;&#39135;&#21697;&#25512;&#33616;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;ChatDiet&#38598;&#25104;&#20102;&#20010;&#20154;&#21644;&#20154;&#32676;&#27169;&#22411;&#65292;&#36741;&#20197;&#19968;&#20010;&#21327;&#35843;&#22120;&#65292;&#26080;&#32541;&#26816;&#32034;&#21644;&#22788;&#29702;&#30456;&#20851;&#20449;&#24687;&#12290;&#20854;&#32467;&#26524;&#26159;&#21160;&#24577;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#39135;&#21697;&#25512;&#33616;&#65292;&#26681;&#25454;&#20010;&#20154;&#29992;&#25143;&#21916;&#22909;&#23450;&#21046;&#12290;&#25105;&#20204;&#23545;ChatDiet&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#19968;&#20010;&#24341;&#20154;&#20837;&#32988;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#22312;&#26696;&#20363;&#30740;&#31350;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#22240;&#26524;&#20010;&#20154;&#27169;&#22411;&#26469;&#20272;&#35745;&#20010;&#20154;&#33829;&#20859;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00781v1 Announce Type: cross  Abstract: The profound impact of food on health necessitates advanced nutrition-oriented food recommendation services. Conventional methods often lack the crucial elements of personalization, explainability, and interactivity. While Large Language Models (LLMs) bring interpretability and explainability, their standalone use falls short of achieving true personalization. In this paper, we introduce ChatDiet, a novel LLM-powered framework designed specifically for personalized nutrition-oriented food recommendation chatbots. ChatDiet integrates personal and population models, complemented by an orchestrator, to seamlessly retrieve and process pertinent information. The result is a dynamic delivery of personalized and explainable food recommendations, tailored to individual user preferences. Our evaluation of ChatDiet includes a compelling case study, where we establish a causal personal model to estimate individual nutrition effects. Our assessmen
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#30340;&#26032;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21487;&#20197;&#19982;&#20043;&#21363;&#25554;&#21363;&#29992;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#23545;&#38899;&#20048;&#36136;&#37327;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;</title><link>https://arxiv.org/abs/2402.14285</link><description>&lt;p&gt;
&#20855;&#26377;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#25193;&#25955;&#30340;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14285
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#30340;&#26032;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21487;&#20197;&#19982;&#20043;&#21363;&#25554;&#21363;&#29992;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#23545;&#38899;&#20048;&#36136;&#37327;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#27493;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#38382;&#39064;&#65288;&#20363;&#22914;&#29983;&#25104;&#38050;&#29748;&#21367;&#35889;&#65289;&#65292;&#25216;&#26415;&#37325;&#28857;&#25918;&#22312;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#24341;&#23548;&#19978;&#12290;&#38899;&#20048;&#35268;&#21017;&#36890;&#24120;&#20197;&#31526;&#21495;&#24418;&#24335;&#34920;&#36798;&#22312;&#38899;&#31526;&#29305;&#24449;&#19978;&#65292;&#22914;&#38899;&#31526;&#23494;&#24230;&#25110;&#21644;&#24358;&#36827;&#34892;&#65292;&#35768;&#22810;&#35268;&#21017;&#26159;&#19981;&#21487;&#24494;&#20998;&#30340;&#65292;&#36825;&#22312;&#20351;&#29992;&#23427;&#20204;&#36827;&#34892;&#24341;&#23548;&#25193;&#25955;&#26102;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#26041;&#27861;&#65292;&#31216;&#20026;&#38543;&#26426;&#25511;&#21046;&#24341;&#23548;&#65288;SCG&#65289;&#65292;&#23427;&#20165;&#38656;&#35201;&#23545;&#35268;&#21017;&#20989;&#25968;&#36827;&#34892;&#21069;&#21521;&#35780;&#20272;&#65292;&#21487;&#20197;&#19982;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#19968;&#36215;&#24037;&#20316;&#65292;&#20174;&#32780;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;&#19981;&#21487;&#24494;&#20998;&#35268;&#21017;&#30340;&#26080;&#35757;&#32451;&#24341;&#23548;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#21487;&#20197;&#19982;SCG&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#32452;&#21512;&#12290;&#19982;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#26631;&#20934;&#24378;&#22522;&#32447;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#22312;&#38899;&#20048;&#36136;&#37327;&#26041;&#38754;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14285v1 Announce Type: cross  Abstract: We study the problem of symbolic music generation (e.g., generating piano rolls), with a technical focus on non-differentiable rule guidance. Musical rules are often expressed in symbolic form on note characteristics, such as note density or chord progression, many of which are non-differentiable which pose a challenge when using them for guided diffusion. We propose Stochastic Control Guidance (SCG), a novel guidance method that only requires forward evaluation of rule functions that can work with pre-trained diffusion models in a plug-and-play way, thus achieving training-free guidance for non-differentiable rules for the first time. Additionally, we introduce a latent diffusion architecture for symbolic music generation with high time resolution, which can be composed with SCG in a plug-and-play fashion. Compared to standard strong baselines in symbolic music generation, this framework demonstrates marked advancements in music quali
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#25913;&#36827;&#21644;&#25512;&#24191;&#20102;ABCD&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#25551;&#36848;&#20449;&#21495;&#21644;&#32972;&#26223;&#30340;&#26041;&#24335;&#65292;&#24182;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#21644;&#19981;&#21516;&#21487;&#35266;&#27979;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#25552;&#39640;&#27979;&#37327;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.08001</link><description>&lt;p&gt;
&#25913;&#36827;&#24182;&#25512;&#24191;&#20855;&#26377;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;ABCD&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improvement and generalization of ABCD method with Bayesian inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08001
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#65292;&#25913;&#36827;&#21644;&#25512;&#24191;&#20102;ABCD&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#25551;&#36848;&#20449;&#21495;&#21644;&#32972;&#26223;&#30340;&#26041;&#24335;&#65292;&#24182;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#21644;&#19981;&#21516;&#21487;&#35266;&#27979;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#25552;&#39640;&#27979;&#37327;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;LHC&#19978;&#23547;&#25214;&#26032;&#29289;&#29702;&#25110;&#31934;&#30830;&#25105;&#20204;&#23545;&#26631;&#20934;&#27169;&#22411;&#30340;&#35748;&#35782;&#26159;&#19968;&#20010;&#28041;&#21450;&#22810;&#20010;&#22240;&#32032;&#30340;&#20225;&#19994;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#21033;&#29992;&#29616;&#26377;&#20449;&#24687;&#65292;&#24182;&#37325;&#26032;&#24605;&#32771;&#24120;&#35268;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;ABCD&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26469;&#25913;&#36827;&#21644;&#25512;&#24191;&#23427;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#28151;&#21512;&#27169;&#22411;&#26469;&#25551;&#36848;&#30001;&#20449;&#21495;&#21644;&#22810;&#20010;&#32972;&#26223;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#21644;&#20107;&#20214;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#24456;&#22909;&#22320;&#25552;&#21462;&#20986;&#26679;&#26412;&#20013;&#30340;&#20449;&#21495;&#12289;&#32972;&#26223;&#21450;&#20854;&#30456;&#23545;&#20998;&#25968;&#12290;&#19982;ABCD&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#23545;&#19981;&#21516;&#32972;&#26223;&#30340;&#26576;&#20123;&#23646;&#24615;&#30340;&#29702;&#35299;&#20197;&#21450;&#27599;&#20010;&#20107;&#20214;&#20013;&#38656;&#35201;&#27979;&#37327;&#30340;&#36229;&#36807;&#20004;&#20010;&#29420;&#31435;&#21487;&#35266;&#27979;&#37327;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#36125;&#21494;&#26031;&#26694;&#26550;&#20351;&#29992;&#36830;&#32493;&#20998;&#24067;&#30340;&#20449;&#24687;&#26469;&#23450;&#20041;&#21306;&#22495;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#30828;&#20999;&#21106;&#23450;&#20041;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
To find New Physics or to refine our knowledge of the Standard Model at the LHC is an enterprise that involves many factors. We focus on taking advantage of available information and pour our effort in re-thinking the usual data-driven ABCD method to improve it and to generalize it using Bayesian Machine Learning tools. We propose that a dataset consisting of a signal and many backgrounds is well described through a mixture model. Signal, backgrounds and their relative fractions in the sample can be well extracted by exploiting the prior knowledge and the dependence between the different observables at the event-by-event level with Bayesian tools. We show how, in contrast to the ABCD method, one can take advantage of understanding some properties of the different backgrounds and of having more than two independent observables to measure in each event. In addition, instead of regions defined through hard cuts, the Bayesian framework uses the information of continuous distribution to obt
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#21463;&#38480;&#65292;&#26080;&#27861;&#23454;&#29616;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#21644;&#20551;&#35774;&#27979;&#35797;&#30340;MatSci-LLMs&#26694;&#26550;&#65292;&#24182;&#25551;&#36848;&#20102;&#20851;&#38190;&#30340;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.05200</link><description>&lt;p&gt;
LLMs&#26159;&#21542;&#20934;&#22791;&#22909;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#26448;&#26009;&#21457;&#29616;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are LLMs Ready for Real-World Materials Discovery?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05200
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#21463;&#38480;&#65292;&#26080;&#27861;&#23454;&#29616;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#21644;&#20551;&#35774;&#27979;&#35797;&#30340;MatSci-LLMs&#26694;&#26550;&#65292;&#24182;&#25551;&#36848;&#20102;&#20851;&#38190;&#30340;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24378;&#22823;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#21487;&#33021;&#24615;&#65292;&#21152;&#24555;&#20102;&#26448;&#26009;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20173;&#23384;&#22312;&#19981;&#36275;&#65292;&#26080;&#27861;&#25104;&#20026;&#23454;&#29992;&#30340;&#26448;&#26009;&#31185;&#23398;&#24037;&#20855;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;LLMs&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#30456;&#20851;&#22833;&#36133;&#26696;&#20363;&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#29702;&#35299;&#21644;&#25512;&#29702;&#22797;&#26434;&#12289;&#30456;&#20114;&#20851;&#32852;&#30340;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#26041;&#38754;&#30340;&#29616;&#26377;&#38480;&#21046;&#12290;&#37492;&#20110;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21457;&#22522;&#20110;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#21644;&#20551;&#35774;&#29983;&#25104;&#19982;&#27979;&#35797;&#30340;&#26448;&#26009;&#31185;&#23398;LLMs&#65288;MatSci-LLMs&#65289;&#30340;&#26694;&#26550;&#12290;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;MatSci-LLMs&#30340;&#36335;&#24452;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#24314;&#31435;&#39640;&#36136;&#37327;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#31185;&#23398;&#25991;&#29486;&#65292;&#20854;&#20013;&#23384;&#22312;&#21508;&#31181;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20851;&#38190;&#30340;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) create exciting possibilities for powerful language processing tools to accelerate research in materials science. While LLMs have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools. In this position paper, we show relevant failure cases of LLMs in materials science that reveal current limitations of LLMs related to comprehending and reasoning over complex, interconnected materials science knowledge. Given those shortcomings, we outline a framework for developing Materials Science LLMs (MatSci-LLMs) that are grounded in materials science knowledge and hypothesis generation followed by hypothesis testing. The path to attaining performant MatSci-LLMs rests in large part on building high-quality, multi-modal datasets sourced from scientific literature where various information extraction challenges persist. As such, we describe key materials science information extraction cha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#23450;&#20301;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#26469;&#35745;&#31639;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#20197;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.02872</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65311;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#26159;&#19978;&#19979;&#25991;&#22836;&#37096;&#36827;&#34892;&#24230;&#37327;&#23398;&#20064;&#30340;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#23450;&#20301;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#26469;&#35745;&#31639;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#20197;&#23398;&#20064;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#23450;&#20301;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20551;&#35774;&#12290;&#22312;&#27973;&#23618;&#20013;&#65292;&#28436;&#31034;&#30340;&#29305;&#24449;&#34987;&#21512;&#24182;&#21040;&#30456;&#24212;&#30340;&#26631;&#31614;&#20013;&#65292;&#36755;&#20837;&#25991;&#26412;&#30340;&#29305;&#24449;&#34987;&#32858;&#21512;&#21040;&#26368;&#21518;&#19968;&#20010;&#26631;&#35760;&#20013;&#12290;&#22312;&#28145;&#23618;&#20013;&#65292;&#19978;&#19979;&#25991;&#22836;&#37096;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#27599;&#20010;&#19978;&#19979;&#25991;&#22836;&#37096;&#20013;&#65292;&#20540;-&#36755;&#20986;&#30697;&#38453;&#25552;&#21462;&#20102;&#26631;&#31614;&#30340;&#29305;&#24449;&#12290;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#35745;&#31639;&#20102;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#27880;&#24847;&#21147;&#26435;&#37325;&#36234;&#22823;&#65292;&#36234;&#22810;&#30340;&#26631;&#31614;&#20449;&#24687;&#34987;&#20256;&#36755;&#21040;&#26368;&#21518;&#19968;&#20010;&#26631;&#35760;&#20013;&#65292;&#29992;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#21333;&#35789;&#12290;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#21487;&#20197;&#34987;&#35270;&#20026;&#23398;&#20064;&#36755;&#20837;&#25991;&#26412;&#19982;&#27599;&#20010;&#28436;&#31034;&#20043;&#38388;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#22522;&#20110;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#19981;&#24179;&#34913;&#30340;&#26631;&#31614;&#21644;&#28436;&#31034;&#39034;&#24207;&#20250;&#24433;&#21709;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;GPT2&#22823;&#22411;&#12289;Llama 7B&#12289;13B&#21644;30B&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#25903;&#25345;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the mechanism of in-context learning and propose a hypothesis using locate-and-project method. In shallow layers, the features of demonstrations are merged into their corresponding labels, and the features of the input text are aggregated into the last token. In deep layers, in-context heads make great contributions. In each in-context head, the value-output matrix extracts the labels' features. Query and key matrices compute the attention weights between the input text and each demonstration. The larger the attention weight is, the more label information is transferred into the last token for predicting the next word. Query and key matrices can be regarded as two towers for learning the similarity metric between the input text and each demonstration. Based on this hypothesis, we explain why imbalanced labels and demonstration order affect predictions. We conduct experiments on GPT2 large, Llama 7B, 13B and 30B. The results can support our analysis. Overall, our study provid
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#20219;&#20309;&#25915;&#20987;&#30340;&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;&#65288;CAD&#65289;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2312.09481</link><description>&lt;p&gt;
&#25345;&#32493;&#19981;&#26029;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Continual Adversarial Defense
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09481
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#20219;&#20309;&#25915;&#20987;&#30340;&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;&#65288;CAD&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#27599;&#26376;&#38024;&#23545;&#35270;&#35273;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#24555;&#36895;&#28436;&#21464;&#30340;&#29305;&#24615;&#65292;&#20154;&#20204;&#25552;&#20986;&#20102;&#35768;&#22810;&#38450;&#24481;&#26041;&#27861;&#65292;&#26088;&#22312;&#23613;&#21487;&#33021;&#36890;&#29992;&#21270;&#20197;&#25269;&#24481;&#23613;&#21487;&#33021;&#22810;&#30340;&#24050;&#30693;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#23545;&#25239;&#25152;&#26377;&#31867;&#22411;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#24182;&#19981;&#29616;&#23454;&#65292;&#22240;&#20026;&#38450;&#24481;&#31995;&#32479;&#36816;&#34892;&#30340;&#29615;&#22659;&#26159;&#21160;&#24577;&#30340;&#65292;&#21253;&#21547;&#38543;&#30528;&#26102;&#38388;&#20986;&#29616;&#30340;&#21508;&#31181;&#29420;&#29305;&#25915;&#20987;&#12290;&#38450;&#24481;&#31995;&#32479;&#24517;&#39035;&#25910;&#38598;&#22312;&#32447;&#23569;&#26679;&#26412;&#23545;&#25239;&#21453;&#39304;&#20197;&#36805;&#36895;&#22686;&#24378;&#33258;&#36523;&#65292;&#20805;&#20998;&#21033;&#29992;&#20869;&#23384;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#20219;&#20309;&#25915;&#20987;&#30340;&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;&#65288;CAD&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21508;&#31181;&#25915;&#20987;&#36880;&#20010;&#38454;&#27573;&#20986;&#29616;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;CAD&#22522;&#20110;&#22235;&#39033;&#21407;&#21017;&#36827;&#34892;&#24314;&#27169;&#65306;(1) &#25345;&#32493;&#36866;&#24212;&#26032;&#25915;&#20987;&#32780;&#26080;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;(2) &#23569;&#26679;&#26412;&#36866;&#24212;&#65292;(3) &#20869;&#23384;&#39640;&#25928;&#36866;&#24212;&#65292;&#20197;&#21450;(4) &#39640;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09481v2 Announce Type: replace-cross  Abstract: In response to the rapidly evolving nature of adversarial attacks against visual classifiers on a monthly basis, numerous defenses have been proposed to generalize against as many known attacks as possible. However, designing a defense method that generalizes to all types of attacks is not realistic because the environment in which defense systems operate is dynamic and comprises various unique attacks that emerge as time goes on. The defense system must gather online few-shot defense feedback to promptly enhance itself, leveraging efficient memory utilization. Therefore, we propose the first continual adversarial defense (CAD) framework that adapts to any attacks in a dynamic scenario, where various attacks emerge stage by stage. In practice, CAD is modeled under four principles: (1) continual adaptation to new attacks without catastrophic forgetting, (2) few-shot adaptation, (3) memory-efficient adaptation, and (4) high accur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20026;&#27668;&#21160;&#32764;&#27969;&#22330;&#27169;&#25311;&#35757;&#32451;&#20102;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20195;&#29702;&#27169;&#22411;&#65292;&#25104;&#21151;&#25429;&#25417;&#25972;&#20010;&#35299;&#20998;&#24067;&#24182;&#20934;&#30830;&#20272;&#35745;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.05320</link><description>&lt;p&gt;
&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#27668;&#21160;&#32764;&#27969;&#22330;&#27169;&#25311;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Surrogate Models for Airfoil Flow Simulations with Denoising Diffusion Probabilistic Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20026;&#27668;&#21160;&#32764;&#27969;&#22330;&#27169;&#25311;&#35757;&#32451;&#20102;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20195;&#29702;&#27169;&#22411;&#65292;&#25104;&#21151;&#25429;&#25417;&#25972;&#20010;&#35299;&#20998;&#24067;&#24182;&#20934;&#30830;&#20272;&#35745;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#28237;&#27969;&#27169;&#25311;&#30340;&#20195;&#29702;&#27169;&#22411;&#26159;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#35805;&#39064;&#12290;&#21516;&#26102;&#65292;&#22312;&#20195;&#29702;&#27169;&#22411;&#30340;&#39044;&#27979;&#20013;&#20307;&#29616;&#27169;&#25311;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#28237;&#27969;&#27169;&#25311;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20195;&#29702;&#27169;&#22411;&#12290;&#30001;&#20110;&#20854;&#26222;&#36941;&#24615;&#65292;&#36873;&#25321;&#20197;&#21508;&#31181;&#24418;&#29366;&#12289;&#38647;&#35834;&#25968;&#21644;&#25915;&#35282;&#30340;&#27668;&#21160;&#32764;&#27969;&#22330;&#27169;&#25311;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DDPMs&#33021;&#25104;&#21151;&#25429;&#25417;&#35299;&#30340;&#25972;&#20010;&#20998;&#24067;&#65292;&#20174;&#32780;&#20934;&#30830;&#20272;&#35745;&#27169;&#25311;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;DDPMs&#30340;&#24615;&#33021;&#36824;&#19982;&#20197;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#24322;&#26041;&#24046;&#27169;&#22411;&#24418;&#24335;&#30340;&#19981;&#21516;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DDPMs&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05320v2 Announce Type: replace-cross  Abstract: Leveraging neural networks as surrogate models for turbulence simulation is a topic of growing interest. At the same time, embodying the inherent uncertainty of simulations in the predictions of surrogate models remains very challenging. The present study makes a first attempt to use denoising diffusion probabilistic models (DDPMs) to train an uncertainty-aware surrogate model for turbulence simulations. Due to its prevalence, the simulation of flows around airfoils with various shapes, Reynolds numbers, and angles of attack is chosen as the learning objective. Our results show that DDPMs can successfully capture the whole distribution of solutions and, as a consequence, accurately estimate the uncertainty of the simulations. The performance of DDPMs is also compared with varying baselines in the form of Bayesian neural networks and heteroscedastic models. Experiments demonstrate that DDPMs outperform the other methods regardin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;O-RAN&#20013;&#32593;&#32476;&#20999;&#29255;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#20004;&#20010;xAPPs&#65292;&#20998;&#21035;&#22788;&#29702;&#21151;&#29575;&#25511;&#21046;&#21644;&#29289;&#29702;&#36164;&#28304;&#22359;&#20998;&#37197;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#29992;&#25143;&#35774;&#22791;&#20043;&#38388;&#23454;&#29616;&#26368;&#22823;&#21270;&#30340;&#21152;&#26435;&#21534;&#21520;&#37327;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#22686;&#24378;&#22411;&#31227;&#21160;&#23485;&#24102;&#21644;&#36229;&#21487;&#38752;&#20302;&#24310;&#36831;&#36890;&#20449;&#36825;&#20004;&#31181;&#26381;&#21153;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.08861</link><description>&lt;p&gt;
O-RAN&#20013;&#21033;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32593;&#32476;&#20999;&#29255;&#30340;&#36164;&#28304;&#20998;&#37197;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Learning Approach for Efficient Resource Allocation with Network Slicing in O-RAN. (arXiv:2401.08861v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;O-RAN&#20013;&#32593;&#32476;&#20999;&#29255;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#20004;&#20010;xAPPs&#65292;&#20998;&#21035;&#22788;&#29702;&#21151;&#29575;&#25511;&#21046;&#21644;&#29289;&#29702;&#36164;&#28304;&#22359;&#20998;&#37197;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#29992;&#25143;&#35774;&#22791;&#20043;&#38388;&#23454;&#29616;&#26368;&#22823;&#21270;&#30340;&#21152;&#26435;&#21534;&#21520;&#37327;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#22686;&#24378;&#22411;&#31227;&#21160;&#23485;&#24102;&#21644;&#36229;&#21487;&#38752;&#20302;&#24310;&#36831;&#36890;&#20449;&#36825;&#20004;&#31181;&#26381;&#21153;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;O-RAN&#65289;&#25216;&#26415;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#32593;&#32476;&#36816;&#33829;&#21830;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#21644;&#26377;&#21033;&#30340;&#29615;&#22659;&#12290;&#22312;O-RAN&#20869;&#30830;&#20445;&#26377;&#25928;&#22320;&#21327;&#35843;x&#24212;&#29992;&#31243;&#24207;&#65288;xAPPs&#65289;&#23545;&#20110;&#32593;&#32476;&#20999;&#29255;&#21644;&#36164;&#28304;&#20998;&#37197;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#65292;&#26088;&#22312;&#21327;&#35843;O-RAN&#20013;&#22810;&#20010;&#29420;&#31435;xAPPs&#30340;&#21327;&#35843;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#22312;&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#20043;&#38388;&#26368;&#22823;&#21270;&#21152;&#26435;&#21534;&#21520;&#37327;&#65292;&#24182;&#20998;&#37197;&#29289;&#29702;&#36164;&#28304;&#22359;&#65288;PRBs&#65289;&#12290;&#25105;&#20204;&#20248;&#20808;&#32771;&#34385;&#22686;&#24378;&#22411;&#31227;&#21160;&#23485;&#24102;&#21644;&#36229;&#21487;&#38752;&#20302;&#24310;&#36831;&#36890;&#20449;&#36825;&#20004;&#31181;&#26381;&#21153;&#31867;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;xAPPs&#65306;&#27599;&#20010;UE&#30340;&#21151;&#29575;&#25511;&#21046;xAPP&#21644;PRB&#20998;&#37197;xAPP&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#37096;&#20998;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#20854;&#20013;&#31532;&#19968;&#37096;&#20998;&#20351;&#29992;&#24102;&#26377;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Open Radio Access Network (O-RAN) technology has emerged as a promising solution for network operators, providing them with an open and favorable environment. Ensuring effective coordination of x-applications (xAPPs) is crucial to enhance flexibility and optimize network performance within the O-RAN. In this paper, we introduce an innovative approach to the resource allocation problem, aiming to coordinate multiple independent xAPPs for network slicing and resource allocation in O-RAN. Our proposed method focuses on maximizing the weighted throughput among user equipments (UE), as well as allocating physical resource blocks (PRBs). We prioritize two service types, namely enhanced Mobile Broadband and Ultra Reliable Low Latency Communication. To achieve this, we have designed two xAPPs: a power control xAPP for each UE and a PRB allocation xAPP. The proposed method consists of a two-part training phase, where the first part uses supervised learning with a Variational Autoencoder tra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#32597;&#35265;&#24773;&#20917;&#19979;&#30340;&#32959;&#30244;&#26816;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20462;&#25913;&#26679;&#26412;&#25968;&#37327;&#21644;&#24739;&#32773;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.03302</link><description>&lt;p&gt;
&#34892;&#21160;&#20013;&#30340;&#29616;&#23454;&#20027;&#20041;&#65306;&#20351;&#29992;YOLOv8&#21644;DeiT&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#35786;&#26029;&#33041;&#32959;&#30244;&#30340;&#24322;&#24120;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#32597;&#35265;&#24773;&#20917;&#19979;&#30340;&#32959;&#30244;&#26816;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20462;&#25913;&#26679;&#26412;&#25968;&#37327;&#21644;&#24739;&#32773;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#31185;&#23398;&#39046;&#22495;&#65292;&#30001;&#20110;&#33041;&#32959;&#30244;&#22312;&#24739;&#32773;&#20013;&#30340;&#32597;&#35265;&#31243;&#24230;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#20173;&#28982;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#24322;&#24120;&#24773;&#20917;&#19979;&#26816;&#27979;&#32959;&#30244;&#30340;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#24739;&#32773;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#12290;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#65288;NBML&#65289;&#30340;&#31934;&#36873;&#25968;&#25454;&#38598;&#21253;&#25324;81&#21517;&#24739;&#32773;&#65292;&#20854;&#20013;&#21253;&#25324;30&#20363;&#32959;&#30244;&#30149;&#20363;&#21644;51&#20363;&#27491;&#24120;&#30149;&#20363;&#12290;&#26816;&#27979;&#21644;&#20998;&#31867;&#27969;&#31243;&#34987;&#20998;&#20026;&#20004;&#20010;&#36830;&#32493;&#30340;&#20219;&#21153;&#12290;&#26816;&#27979;&#38454;&#27573;&#21253;&#25324;&#20840;&#38754;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#39044;&#22788;&#29702;&#65292;&#20197;&#20462;&#25913;&#22270;&#20687;&#26679;&#26412;&#21644;&#27599;&#20010;&#31867;&#21035;&#30340;&#24739;&#32773;&#25968;&#37327;&#65292;&#20197;&#31526;&#21512;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#20998;&#24067;&#65288;9&#20010;&#27491;&#24120;&#26679;&#26412;&#23545;&#24212;1&#20010;&#32959;&#30244;&#26679;&#26412;&#65289;&#12290;&#27492;&#22806;&#65292;&#22312;&#27979;&#35797;&#20013;&#38500;&#20102;&#24120;&#35265;&#30340;&#35780;&#20272;&#25351;&#26631;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;... [&#25688;&#35201;&#38271;&#24230;&#24050;&#36798;&#21040;&#19978;&#38480;]
&lt;/p&gt;
&lt;p&gt;
In the field of medical sciences, reliable detection and classification of brain tumors from images remains a formidable challenge due to the rarity of tumors within the population of patients. Therefore, the ability to detect tumors in anomaly scenarios is paramount for ensuring timely interventions and improved patient outcomes. This study addresses the issue by leveraging deep learning (DL) techniques to detect and classify brain tumors in challenging situations. The curated data set from the National Brain Mapping Lab (NBML) comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The detection and classification pipelines are separated into two consecutive tasks. The detection phase involved comprehensive data analysis and pre-processing to modify the number of image samples and the number of patients of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with real world scenarios. Next, in addition to common evaluation metrics for the testing, we emplo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#65292;&#25105;&#20204;&#23450;&#20301;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#25214;&#21040;&#20102;&#23384;&#20648;&#20102;&#26377;&#20851;&#8220;&#27861;&#22269;&#65292;&#39318;&#37117;&#65292;&#24052;&#40654;&#8221;&#30340;&#30693;&#35782;&#30340;&#20301;&#32622;&#12290;</title><link>http://arxiv.org/abs/2312.12141</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23450;&#20301;&#20107;&#23454;&#30693;&#35782;&#65306;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Locating Factual Knowledge in Large Language Models: Exploring the Residual Stream and Analyzing Subvalues in Vocabulary Space. (arXiv:2312.12141v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12141
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#65292;&#25105;&#20204;&#23450;&#20301;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#25214;&#21040;&#20102;&#23384;&#20648;&#20102;&#26377;&#20851;&#8220;&#27861;&#22269;&#65292;&#39318;&#37117;&#65292;&#24052;&#40654;&#8221;&#30340;&#30693;&#35782;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#26102;&#65292;&#23376;&#20540;&#20855;&#26377;&#21487;&#20154;&#31867;&#35299;&#37322;&#30340;&#27010;&#24565;&#30340;&#21407;&#22240;&#12290;&#23376;&#20540;&#30340;softmax&#20043;&#21069;&#30340;&#20540;&#36890;&#36807;&#19968;&#20010;&#21152;&#27861;&#20989;&#25968;&#30456;&#21152;&#65292;&#22240;&#27492;&#35789;&#27719;&#31354;&#38388;&#20013;&#21069;&#20960;&#20010;&#26631;&#35760;&#30340;&#27010;&#29575;&#20250;&#22686;&#21152;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#23545;&#25968;&#27010;&#29575;&#22686;&#21152;&#26469;&#35745;&#31639;&#23618;&#21644;&#23376;&#20540;&#30340;&#37325;&#35201;&#24615;&#27604;&#27010;&#29575;&#22686;&#21152;&#26356;&#22909;&#65292;&#22240;&#20026;&#23545;&#25968;&#27010;&#29575;&#22686;&#21152;&#30340;&#26354;&#32447;&#21576;&#32447;&#24615;&#21333;&#35843;&#22686;&#24418;&#29366;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35745;&#31639;&#20869;&#31215;&#26469;&#35780;&#20272;&#21069;&#39304;&#32593;&#32476;&#65288;FFN&#65289;&#30340;&#23376;&#20540;&#34987;&#21069;&#38754;&#30340;&#23618;&#28608;&#27963;&#30340;&#31243;&#24230;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#20107;&#23454;&#30693;&#35782;&#8220;&#27861;&#22269;&#65292;&#39318;&#37117;&#65292;&#24052;&#40654;&#8221;&#23384;&#20648;&#30340;&#20301;&#32622;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#27880;&#24847;&#21147;&#23618;&#23384;&#20648;&#8220;&#24052;&#40654;&#19982;&#27861;&#22269;&#30456;&#20851;&#8221;&#12290;FFN&#23618;&#23384;&#20648;&#8220;&#24052;&#40654;&#26159;&#19968;&#20010;&#39318;&#37117;/&#22478;&#24066;&#8221;&#65292;&#30001;&#27880;&#24847;&#21147;&#23376;&#20540;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
We find the location of factual knowledge in large language models by exploring the residual stream and analyzing subvalues in vocabulary space. We find the reason why subvalues have human-interpretable concepts when projecting into vocabulary space. The before-softmax values of subvalues are added by an addition function, thus the probability of top tokens in vocabulary space will increase. Based on this, we find using log probability increase to compute the significance of layers and subvalues is better than probability increase, since the curve of log probability increase has a linear monotonically increasing shape. Moreover, we calculate the inner products to evaluate how much a feed-forward network (FFN) subvalue is activated by previous layers. Base on our methods, we find where factual knowledge &lt;France, capital, Paris&gt; is stored. Specifically, attention layers store "Paris is related to France". FFN layers store "Paris is a capital/city", activated by attention subvalues relate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#37327;&#23376;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;QLSTM&#65289;&#21644;&#32463;&#20856;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#22312;&#22826;&#38451;&#33021;&#21457;&#30005;&#37327;&#39044;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;QLSTM&#20855;&#26377;&#21152;&#24555;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#21644;&#20943;&#23567;&#27979;&#35797;&#25439;&#22833;&#30340;&#20248;&#21183;&#65292;&#25317;&#26377;&#21560;&#32435;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#20851;&#31995;&#30340;&#28508;&#21147;&#65292;&#20294;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.17032</link><description>&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#37327;&#23376;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;QLSTM&#65289;&#19982;&#32463;&#20856;LSTM&#30340;&#27604;&#36739;&#30740;&#31350;&#65306;&#20197;&#22826;&#38451;&#33021;&#39044;&#27979;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Quantum Long Short-Term Memory (QLSTM) vs Classical LSTM in Time Series Forecasting: A Comparative Study in Solar Power Forecasting. (arXiv:2310.17032v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#37327;&#23376;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;QLSTM&#65289;&#21644;&#32463;&#20856;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#22312;&#22826;&#38451;&#33021;&#21457;&#30005;&#37327;&#39044;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;QLSTM&#20855;&#26377;&#21152;&#24555;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#21644;&#20943;&#23567;&#27979;&#35797;&#25439;&#22833;&#30340;&#20248;&#21183;&#65292;&#25317;&#26377;&#21560;&#32435;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#20851;&#31995;&#30340;&#28508;&#21147;&#65292;&#20294;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#21521;&#21487;&#25345;&#32493;&#33021;&#28304;&#31995;&#32479;&#21457;&#23637;&#30340;&#36807;&#31243;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#22826;&#38451;&#33021;&#21457;&#30005;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#37327;&#23376;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;QLSTM&#65289;&#21644;&#32463;&#20856;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#22312;&#22826;&#38451;&#33021;&#21457;&#30005;&#39044;&#27979;&#26041;&#38754;&#36827;&#34892;&#20102;&#20180;&#32454;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;QLSTM&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#21253;&#25324;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#21152;&#24555;&#21644;&#22312;&#21021;&#22987;&#38454;&#27573;&#26126;&#26174;&#38477;&#20302;&#30340;&#27979;&#35797;&#25439;&#22833;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#32463;&#20856;LSTM&#27169;&#22411;&#12290;&#36825;&#20123;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;QLSTM&#26377;&#28508;&#21147;&#24555;&#36895;&#21560;&#32435;&#22797;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#20851;&#31995;&#65292;&#36825;&#24471;&#30410;&#20110;&#37327;&#23376;&#29616;&#35937;&#65288;&#22914;&#21472;&#21152;&#65289;&#12290;&#28982;&#32780;&#65292;&#35201;&#23454;&#29616;QLSTM&#30340;&#20840;&#37096;&#33021;&#21147;&#65292;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#27169;&#22411;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#39564;&#35777;&#12289;&#31995;&#32479;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#30828;&#20214;&#22122;&#22768;&#40065;&#26834;&#24615;&#65292;&#20197;&#21450;&#30456;&#20851;&#21487;&#20877;&#29983;&#33021;&#28304;&#39044;&#27979;&#38382;&#39064;&#30340;&#24212;&#29992;&#12290;&#38543;&#30528;&#19981;&#26029;&#30340;&#36827;&#23637;&#65292;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#22312;&#39044;&#27979;&#21644;&#20248;&#21270;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#24102;&#26469;&#21464;&#38761;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately forecasting solar power generation is crucial in the global progression towards sustainable energy systems. In this study, we conduct a meticulous comparison between Quantum Long Short-Term Memory (QLSTM) and classical Long Short-Term Memory (LSTM) models for solar power production forecasting. Our controlled experiments reveal promising advantages of QLSTMs, including accelerated training convergence and substantially reduced test loss within the initial epoch compared to classical LSTMs. These empirical findings demonstrate QLSTM's potential to swiftly assimilate complex time series relationships, enabled by quantum phenomena like superposition. However, realizing QLSTM's full capabilities necessitates further research into model validation across diverse conditions, systematic hyperparameter optimization, hardware noise resilience, and applications to correlated renewable forecasting problems. With continued progress, quantum machine learning can offer a paradigm shift in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#25442;&#22120;&#21644;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#25913;&#21892;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#24120;&#35265;&#25104;&#20687;&#21464;&#24322;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15952</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#24341;&#23548;&#25193;&#25955;&#21644;&#23884;&#22871;&#38598;&#25104;&#25913;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness and Reliability in Medical Image Classification with Latent-Guided Diffusion and Nested-Ensembles. (arXiv:2310.15952v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#25442;&#22120;&#21644;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#25913;&#21892;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#24120;&#35265;&#25104;&#20687;&#21464;&#24322;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#30495;&#23454;&#20020;&#24202;&#29615;&#22659;&#20013;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#23427;&#20204;&#23545;&#25152;&#33719;&#21462;&#30340;&#22270;&#20687;&#30340;&#21464;&#24322;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#35768;&#22810;&#26041;&#27861;&#20250;&#23545;&#35757;&#32451;&#25968;&#25454;&#24212;&#29992;&#39044;&#23450;&#20041;&#30340;&#36716;&#25442;&#65292;&#20197;&#22686;&#24378;&#27979;&#35797;&#26102;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#36825;&#20123;&#36716;&#25442;&#21487;&#33021;&#26080;&#27861;&#30830;&#20445;&#27169;&#22411;&#23545;&#24739;&#32773;&#22270;&#20687;&#20013;&#30340;&#22810;&#26679;&#24615;&#21464;&#24322;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#22120;&#21644;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#19977;&#38454;&#27573;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#23454;&#36341;&#20013;&#24120;&#35265;&#30340;&#25104;&#20687;&#21464;&#24322;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#39044;&#20808;&#30830;&#23450;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#22810;&#20010;&#22270;&#20687;&#32534;&#30721;&#22120;&#39318;&#20808;&#23398;&#20064;&#20998;&#23618;&#29305;&#24449;&#34920;&#31034;&#26469;&#26500;&#24314;&#36776;&#21035;&#28508;&#22312;&#31354;&#38388;&#12290;&#25509;&#19979;&#26469;&#65292;&#19968;&#20010;&#30001;&#28508;&#22312;&#20195;&#30721;&#24341;&#23548;&#30340;&#36870;&#25193;&#25955;&#36807;&#31243;&#20316;&#29992;&#20110;&#26377;&#20449;&#24687;&#20808;&#39564;&#65292;&#24182;&#25552;&#20986;&#39044;&#27979;&#20505;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning models have achieved remarkable success across a range of medical image analysis tasks, deployment of these models in real clinical contexts requires that they be robust to variability in the acquired images. While many methods apply predefined transformations to augment the training data to enhance test-time robustness, these transformations may not ensure the model's robustness to the diverse variability seen in patient images. In this paper, we introduce a novel three-stage approach based on transformers coupled with conditional diffusion models, with the goal of improving model robustness to the kinds of imaging variability commonly encountered in practice without the need for pre-determined data augmentation strategies. To this end, multiple image encoders first learn hierarchical feature representations to build discriminative latent spaces. Next, a reverse diffusion process, guided by the latent code, acts on an informative prior and proposes prediction candi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#36890;&#36135;&#32039;&#32553;&#21464;&#37327;&#26059;&#36716;&#30340;&#25311;&#21512;&#22240;&#23376;&#20998;&#26512;&#26041;&#27861;&#65292;&#22312;&#27599;&#19968;&#34892;&#19978;&#36880;&#27493;&#27714;&#35299;&#27491;&#20132;&#30697;&#38453;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#35745;&#31639;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.10545</link><description>&lt;p&gt;
&#20248;&#21270;&#25311;&#21512;&#22240;&#23376;&#20998;&#26512;&#19982;&#36890;&#36135;&#32039;&#32553;&#21464;&#37327;&#26059;&#36716;
&lt;/p&gt;
&lt;p&gt;
Optimal vintage factor analysis with deflation varimax. (arXiv:2310.10545v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#36890;&#36135;&#32039;&#32553;&#21464;&#37327;&#26059;&#36716;&#30340;&#25311;&#21512;&#22240;&#23376;&#20998;&#26512;&#26041;&#27861;&#65292;&#22312;&#27599;&#19968;&#34892;&#19978;&#36880;&#27493;&#27714;&#35299;&#27491;&#20132;&#30697;&#38453;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#35745;&#31639;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36135;&#32039;&#32553;&#21464;&#37327;&#26059;&#36716;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#22240;&#23376;&#20998;&#26512;&#26041;&#27861;&#65292;&#26088;&#22312;&#39318;&#20808;&#25214;&#21040;&#21407;&#22987;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#28982;&#21518;&#23547;&#27714;&#26059;&#36716;&#65292;&#20351;&#26059;&#36716;&#21518;&#30340;&#20302;&#32500;&#34920;&#31034;&#20855;&#26377;&#31185;&#23398;&#24847;&#20041;&#12290;&#23613;&#31649;Principal Component Analysis (PCA) followed by the varimax rotation&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25311;&#21512;&#22240;&#23376;&#20998;&#26512;&#65292;&#20294;&#30001;&#20110;varimax rotation&#38656;&#35201;&#22312;&#27491;&#20132;&#30697;&#38453;&#38598;&#21512;&#19978;&#35299;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#27492;&#24456;&#38590;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27714;&#35299;&#27491;&#20132;&#30697;&#38453;&#30340;&#36890;&#36135;&#32039;&#32553;&#21464;&#37327;&#26059;&#36716;&#36807;&#31243;&#12290;&#38500;&#20102;&#22312;&#35745;&#31639;&#19978;&#30340;&#20248;&#21183;&#21644;&#28789;&#27963;&#24615;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#33021;&#22312;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#23545;&#25152;&#25552;&#20986;&#30340;&#36807;&#31243;&#36827;&#34892;&#23436;&#20840;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;PCA&#20043;&#21518;&#37319;&#29992;&#36825;&#31181;&#26032;&#30340;varimax&#26041;&#27861;&#20316;&#20026;&#31532;&#20108;&#27493;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#36825;&#20010;&#20004;&#27493;&#36807;&#31243;&#22312;&#19968;&#20010;&#26356;&#19968;&#33324;&#30340;&#22240;&#23376;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vintage factor analysis is one important type of factor analysis that aims to first find a low-dimensional representation of the original data, and then to seek a rotation such that the rotated low-dimensional representation is scientifically meaningful. Perhaps the most widely used vintage factor analysis is the Principal Component Analysis (PCA) followed by the varimax rotation. Despite its popularity, little theoretical guarantee can be provided mainly because varimax rotation requires to solve a non-convex optimization over the set of orthogonal matrices.  In this paper, we propose a deflation varimax procedure that solves each row of an orthogonal matrix sequentially. In addition to its net computational gain and flexibility, we are able to fully establish theoretical guarantees for the proposed procedure in a broad context.  Adopting this new varimax approach as the second step after PCA, we further analyze this two step procedure under a general class of factor models. Our resul
&lt;/p&gt;</description></item><item><title>&#38750;&#22238;&#28335;&#22270;&#31070;&#32463;&#32593;&#32476;(NBA-GNN)&#36890;&#36807;&#19981;&#32771;&#34385;&#20808;&#21069;&#35775;&#38382;&#33410;&#28857;&#30340;&#28040;&#24687;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#26412;&#22320;&#26356;&#26032;&#20013;&#30340;&#20887;&#20313;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#24674;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07430</link><description>&lt;p&gt;
&#38750;&#22238;&#28335;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Non-backtracking Graph Neural Networks. (arXiv:2310.07430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07430
&lt;/p&gt;
&lt;p&gt;
&#38750;&#22238;&#28335;&#22270;&#31070;&#32463;&#32593;&#32476;(NBA-GNN)&#36890;&#36807;&#19981;&#32771;&#34385;&#20808;&#21069;&#35775;&#38382;&#33410;&#28857;&#30340;&#28040;&#24687;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#26412;&#22320;&#26356;&#26032;&#20013;&#30340;&#20887;&#20313;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#24674;&#22797;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33879;&#21517;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28040;&#24687;&#20256;&#36882;&#26356;&#26032;&#20801;&#35768;&#20351;&#29992;&#26412;&#22320;&#21644;&#35745;&#31639;&#19978;&#21487;&#36319;&#36394;&#30340;&#26356;&#26032;&#26469;&#34920;&#31034;&#22823;&#35268;&#27169;&#22270;&#12290;&#28982;&#32780;&#65292;&#26412;&#22320;&#26356;&#26032;&#21463;&#21040;&#22238;&#28335;&#30340;&#24433;&#21709;&#65292;&#21363;&#28040;&#24687;&#36890;&#36807;&#21516;&#19968;&#26465;&#36793;&#20004;&#27425;&#27969;&#21160;&#24182;&#37325;&#35775;&#20808;&#21069;&#35775;&#38382;&#30340;&#33410;&#28857;&#12290;&#30001;&#20110;&#28040;&#24687;&#27969;&#30340;&#25968;&#37327;&#38543;&#30528;&#26356;&#26032;&#30340;&#27425;&#25968;&#21576;&#25351;&#25968;&#32423;&#22686;&#21152;&#65292;&#26412;&#22320;&#26356;&#26032;&#20013;&#30340;&#20887;&#20313;&#38459;&#30861;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#35782;&#21035;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#23450;&#28040;&#24687;&#27969;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#38750;&#22238;&#28335;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;NBA-GNN&#65289;&#35299;&#20915;&#20102;&#36825;&#31181;&#20887;&#20313;&#65292;&#35813;&#32593;&#32476;&#22312;&#26356;&#26032;&#28040;&#24687;&#26102;&#19981;&#32771;&#34385;&#20808;&#21069;&#35775;&#38382;&#33410;&#28857;&#30340;&#28040;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;NBA-GNN&#22914;&#20309;&#32531;&#35299;GNN&#30340;&#36807;&#24230;&#21387;&#32553;&#65292;&#24182;&#24314;&#31435;&#20102;NBA-GNN&#21644;&#38750;&#22238;&#28335;&#26356;&#26032;&#22312;&#38543;&#26426;&#22359;&#27169;&#22411;&#24674;&#22797;&#26041;&#38754;&#20986;&#33394;&#24615;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;NBA-
&lt;/p&gt;
&lt;p&gt;
The celebrated message-passing updates for graph neural networks allow the representation of large-scale graphs with local and computationally tractable updates. However, the local updates suffer from backtracking, i.e., a message flows through the same edge twice and revisits the previously visited node. Since the number of message flows increases exponentially with the number of updates, the redundancy in local updates prevents the graph neural network from accurately recognizing a particular message flow for downstream tasks. In this work, we propose to resolve such a redundancy via the non-backtracking graph neural network (NBA-GNN) that updates a message without incorporating the message from the previously visited node. We further investigate how NBA-GNN alleviates the over-squashing of GNNs, and establish a connection between NBA-GNN and the impressive performance of non-backtracking updates for stochastic block model recovery. We empirically verify the effectiveness of our NBA-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.03234</link><description>&lt;p&gt;
&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#65292;&#36890;&#36807;&#25193;&#23637;&#24050;&#26377;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#26469;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#26032;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#31216;&#20026;&#38750;&#20809;&#28369;&#24369;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#21512;&#20248;&#21270;(NSWC FCCO)&#12290;&#30001;&#20110;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#20197;&#21450;&#20854;&#35299;&#20915;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#38543;&#26426;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;FCCO&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;FCCO&#30340;&#30740;&#31350;&#20551;&#35774;&#20869;&#22806;&#20989;&#25968;&#37117;&#26159;&#20809;&#28369;&#30340;&#65292;&#38480;&#21046;&#20102;&#20854;&#33021;&#22815;&#35299;&#20915;&#26356;&#22810;&#31181;&#31867;&#30340;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#38750;&#20809;&#28369;&#24369;&#20984;FCCO&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20854;&#20013;&#22806;&#20989;&#25968;&#26159;&#24369;&#20984;&#19988;&#38750;&#36882;&#20943;&#30340;&#65292;&#20869;&#20989;&#25968;&#26159;&#24369;&#20984;&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#31639;&#27861;&#65292;&#24182;&#30830;&#23450;&#20854;&#22312;&#25214;&#21040;Moreau&#29615;&#30340;&#949;-&#31283;&#23450;&#28857;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates new families of compositional optimization problems, called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\epsilon$-stationary point of the Moreau env
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27700;&#21360;&#30340;&#26694;&#26550;WASA&#65292;&#36890;&#36807;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#26377;&#23884;&#20837;&#28304;&#20449;&#24687;&#30340;&#21512;&#25104;&#25991;&#26412;&#27700;&#21360;&#26469;&#35299;&#20915;&#28304;&#24402;&#23646;&#21644;&#25968;&#25454;&#26469;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00646</link><description>&lt;p&gt;
WASA&#65306;&#22522;&#20110;&#27700;&#21360;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#30340;&#28304;&#24402;&#23646;
&lt;/p&gt;
&lt;p&gt;
WASA: WAtermark-based Source Attribution for Large Language Model-Generated Data. (arXiv:2310.00646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27700;&#21360;&#30340;&#26694;&#26550;WASA&#65292;&#36890;&#36807;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24102;&#26377;&#23884;&#20837;&#28304;&#20449;&#24687;&#30340;&#21512;&#25104;&#25991;&#26412;&#27700;&#21360;&#26469;&#35299;&#20915;&#28304;&#24402;&#23646;&#21644;&#25968;&#25454;&#26469;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#33394;&#24615;&#33021;&#21644;&#20854;&#21830;&#19994;&#21270;&#30340;&#24040;&#22823;&#28508;&#21147;&#24341;&#21457;&#20102;&#23545;&#20854;&#35757;&#32451;&#25968;&#25454;&#30693;&#35782;&#20135;&#26435;&#65288;IP&#65289;&#30340;&#20005;&#37325;&#20851;&#27880;&#12290;&#29305;&#21035;&#26159;&#65292;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#25991;&#26412;&#21487;&#33021;&#20405;&#29359;&#34987;&#29992;&#20110;&#35757;&#32451;LLM&#30340;&#25968;&#25454;&#30340;&#30693;&#35782;&#20135;&#26435;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#33021;&#22815;&#65288;a&#65289;&#36890;&#36807;&#27700;&#21360;&#35782;&#21035;&#20986;&#23545;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#25991;&#26412;&#20570;&#20986;&#36129;&#29486;&#30340;&#25968;&#25454;&#25552;&#20379;&#32773;&#65288;&#28304;&#24402;&#23646;&#65289;&#65307;&#20197;&#21450;&#65288;b&#65289;&#39564;&#35777;&#25991;&#26412;&#25968;&#25454;&#26159;&#21542;&#26469;&#33258;&#20110;&#26576;&#20010;&#25968;&#25454;&#25552;&#20379;&#32773;&#23545;LLM&#36827;&#34892;&#20102;&#35757;&#32451;&#65288;&#25968;&#25454;&#26469;&#28304;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#27700;&#21360;&#25216;&#26415;&#21487;&#20197;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#35753;LLM&#29983;&#25104;&#20855;&#26377;&#23884;&#20837;&#28304;&#20449;&#24687;&#30340;&#21512;&#25104;&#25991;&#26412;&#27700;&#21360;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#31181;&#27700;&#21360;&#25216;&#26415;&#26694;&#26550;&#30340;&#20851;&#38190;&#29305;&#24615;&#65288;&#20363;&#22914;&#28304;&#24402;&#23646;&#20934;&#30830;&#24615;&#12289;&#25269;&#25239;&#23545;&#25163;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#30340;WAtermarking for Source Attribution&#65288;WASA&#65289;&#26694;&#26550;.
&lt;/p&gt;
&lt;p&gt;
The impressive performances of large language models (LLMs) and their immense potential for commercialization have given rise to serious concerns over the intellectual property (IP) of their training data. In particular, the synthetic texts generated by LLMs may infringe the IP of the data being used to train the LLMs. To this end, it is imperative to be able to (a) identify the data provider who contributed to the generation of a synthetic text by an LLM (source attribution) and (b) verify whether the text data from a data provider has been used to train an LLM (data provenance). In this paper, we show that both problems can be solved by watermarking, i.e., by enabling an LLM to generate synthetic texts with embedded watermarks that contain information about their source(s). We identify the key properties of such watermarking frameworks (e.g., source attribution accuracy, robustness against adversaries), and propose a WAtermarking for Source Attribution (WASA) framework that satisfies
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;15&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#23384;&#22312;&#35748;&#30693;&#20559;&#24046;&#65292;&#23588;&#20854;&#22312;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#20559;&#35265;&#65292;&#36825;&#23545;&#20854;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17012</link><description>&lt;p&gt;
&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35748;&#30693;&#20559;&#24046;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Cognitive Biases in Large Language Models as Evaluators. (arXiv:2309.17012v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;15&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#23384;&#22312;&#35748;&#30693;&#20559;&#24046;&#65292;&#23588;&#20854;&#22312;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#20559;&#35265;&#65292;&#36825;&#23545;&#20854;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#38750;&#24120;&#26377;&#25928;&#12290;&#26412;&#30740;&#31350;&#32452;&#35013;&#20102;15&#20010;&#22823;&#23567;&#19981;&#21516;&#30340;LLMs&#65292;&#24182;&#36890;&#36807;&#20854;&#20182;LLMs&#30340;&#20559;&#22909;&#25490;&#21517;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#36755;&#20986;&#21709;&#24212;&#65292;&#20363;&#22914;System Star&#27604;System Square&#26356;&#22909;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#36755;&#20986;&#20013;&#20845;&#31181;&#19981;&#21516;&#35748;&#30693;&#20559;&#24046;&#30340;&#35748;&#30693;&#20559;&#24046;&#22522;&#20934;&#27979;&#35797;&#65288;CoBBLEr&#65289;&#65292;&#22914;&#33258;&#25105;&#20013;&#24515;&#20559;&#24046;&#65292;&#21363;&#27169;&#22411;&#26356;&#21916;&#27426;&#23558;&#33258;&#24049;&#30340;&#36755;&#20986;&#22312;&#35780;&#20272;&#20013;&#25490;&#21517;&#36739;&#39640;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#26159;&#26377;&#20559;&#35265;&#30340;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;&#22120;&#65292;&#22312;&#27599;&#20010;&#35780;&#20272;&#20013;&#37117;&#34920;&#29616;&#20986;&#23545;&#25105;&#20204;&#20559;&#35265;&#22522;&#20934;&#30340;&#24378;&#28872;&#36857;&#35937;&#65288;&#22312;&#25152;&#26377;&#27169;&#22411;&#19978;&#30340;&#24179;&#22343;&#27604;&#36739;&#32422;&#20026;40%&#65289;&#65292;&#36825;&#23545;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#40065;&#26834;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20559;&#22909;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#35745;&#31639;&#20102;&#24179;&#22343;&#30340;Rank-Biased O&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased O
&lt;/p&gt;</description></item><item><title>&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20256;&#24863;&#22120;/&#20195;&#29702;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36153;&#33293;&#23572;&#20449;&#24687;&#25110;&#26368;&#23567;&#21270;Cramer-Rao&#30028;&#26469;&#35299;&#20915;&#20256;&#24863;&#22120;/&#20195;&#29702;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#35774;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06442</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#20013;&#30340;&#21327;&#20316;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Collaboration in Distributed Parameter Estimation with Resource Constraints. (arXiv:2307.06442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06442
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20256;&#24863;&#22120;/&#20195;&#29702;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36153;&#33293;&#23572;&#20449;&#24687;&#25110;&#26368;&#23567;&#21270;Cramer-Rao&#30028;&#26469;&#35299;&#20915;&#20256;&#24863;&#22120;/&#20195;&#29702;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#35774;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32771;&#34385;&#36164;&#28304;&#32422;&#26463;&#21644;&#19981;&#21516;&#20256;&#24863;&#22120;/&#20195;&#29702;&#25910;&#38598;&#30340;&#35266;&#27979;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#21442;&#25968;&#20272;&#35745;&#30340;&#20256;&#24863;&#22120;/&#20195;&#29702;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#32452;&#20256;&#24863;&#22120;/&#20195;&#29702;&#65292;&#27599;&#20010;&#20256;&#24863;&#22120;/&#20195;&#29702;&#26679;&#26412;&#26469;&#33258;&#22810;&#20803;&#39640;&#26031;&#20998;&#24067;&#30340;&#19981;&#21516;&#21464;&#37327;&#65292;&#24182;&#19988;&#20855;&#26377;&#19981;&#21516;&#30340;&#20272;&#35745;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#20256;&#24863;&#22120;/&#20195;&#29702;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21327;&#20316;&#31574;&#30053;&#35774;&#35745;&#38382;&#39064;&#38416;&#36848;&#20026;&#36153;&#33293;&#23572;&#20449;&#24687;&#26368;&#22823;&#21270;&#65288;&#25110;Cramer-Rao&#30028;&#26368;&#23567;&#21270;&#65289;&#38382;&#39064;&#12290;&#24403;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30693;&#35782;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#22320;&#35782;&#21035;&#20986;&#20004;&#20010;&#29305;&#23450;&#24773;&#20917;&#65306;&#65288;1&#65289;&#19981;&#33021;&#21033;&#29992;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30693;&#35782;&#36827;&#34892;&#21327;&#20316;&#20272;&#35745;&#30340;&#24773;&#20917;&#65292;&#65288;2&#65289;&#26368;&#20248;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#28041;&#21450;&#25237;&#36164;&#26377;&#38480;&#36164;&#28304;&#20197;&#21327;&#20316;&#37319;&#26679;&#21644;&#36716;&#31227;&#24050;&#30693;&#32479;&#35745;&#20449;&#24687;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study sensor/agent data collection and collaboration policies for parameter estimation, accounting for resource constraints and correlation between observations collected by distinct sensors/agents. Specifically, we consider a group of sensors/agents each samples from different variables of a multivariate Gaussian distribution and has different estimation objectives, and we formulate a sensor/agent's data collection and collaboration policy design problem as a Fisher information maximization (or Cramer-Rao bound minimization) problem. When the knowledge of correlation between variables is available, we analytically identify two particular scenarios: (1) where the knowledge of the correlation between samples cannot be leveraged for collaborative estimation purposes and (2) where the optimal data collection policy involves investing scarce resources to collaboratively sample and transfer information that is not of immediate interest and whose statistics are already known, with the sol
&lt;/p&gt;</description></item><item><title>FastServe&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#26381;&#21153;&#31995;&#32479;&#65292;&#21033;&#29992;&#25250;&#21344;&#24335;&#35843;&#24230;&#21644;&#36339;&#36807;-&#36830;&#25509;&#22810;&#32423;&#21453;&#39304;&#38431;&#21015;&#65292;&#26368;&#23567;&#21270;&#27169;&#22411;&#25512;&#26029;&#30340;&#20316;&#19994;&#23436;&#25104;&#26102;&#38388;(JCT)&#12290;</title><link>http://arxiv.org/abs/2305.05920</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#20998;&#24067;&#24335;&#25512;&#26029;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Fast Distributed Inference Serving for Large Language Models. (arXiv:2305.05920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05920
&lt;/p&gt;
&lt;p&gt;
FastServe&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#26381;&#21153;&#31995;&#32479;&#65292;&#21033;&#29992;&#25250;&#21344;&#24335;&#35843;&#24230;&#21644;&#36339;&#36807;-&#36830;&#25509;&#22810;&#32423;&#21453;&#39304;&#38431;&#21015;&#65292;&#26368;&#23567;&#21270;&#27169;&#22411;&#25512;&#26029;&#30340;&#20316;&#19994;&#23436;&#25104;&#26102;&#38388;(JCT)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#25512;&#21160;&#20102;&#20197;ChatGPT&#20026;&#20195;&#34920;&#30340;&#26032;&#19968;&#20195;&#20114;&#21160;AI&#24212;&#29992;&#31243;&#24207;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#20132;&#20114;&#24615;&#35201;&#27714;&#27169;&#22411;&#25512;&#26029;&#30340;&#20302;&#20316;&#19994;&#23436;&#25104;&#26102;&#38388;(JCT)&#12290;&#29616;&#26377;&#30340;LLM&#26381;&#21153;&#31995;&#32479;&#20351;&#29992;&#30340;&#26159;&#36816;&#34892;&#21040;&#23436;&#25104;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#23384;&#22312;&#22836;&#37096;&#38459;&#22622;&#21644;&#38271;JCT&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FastServe&#65292;&#19968;&#31181;&#38024;&#23545;LLMs&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#26381;&#21153;&#31995;&#32479;&#12290;FastServe&#21033;&#29992;LLM&#25512;&#29702;&#30340;&#33258;&#22238;&#24402;&#27169;&#24335;&#65292;&#20197;&#27599;&#20010;&#36755;&#20986;&#26631;&#35760;&#30340;&#31890;&#24230;&#23454;&#29616;&#25250;&#21344;&#24335;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#36339;&#36807;-&#36830;&#25509;&#22810;&#32423;&#21453;&#39304;&#38431;&#21015;&#35843;&#24230;&#22120;&#26368;&#23567;&#21270;JCT&#12290;&#22522;&#20110;LLM&#25512;&#29702;&#30340;&#26032;&#21322;&#20449;&#24687;&#19981;&#21487;&#30693;&#35774;&#32622;&#65292;&#35843;&#24230;&#31243;&#24207;&#21033;&#29992;&#36755;&#20837;&#38271;&#24230;&#20449;&#24687;&#26469;&#20026;&#27599;&#20010;&#21040;&#36798;&#20316;&#19994;&#20998;&#37197;&#36866;&#24403;&#30340;&#21021;&#22987;&#38431;&#21015;&#26469;&#36830;&#25509;&#12290;&#39640;&#20110;&#25152;&#36830;&#25509;&#38431;&#21015;&#30340;&#20248;&#20808;&#32423;&#38431;&#21015;&#34987;&#36339;&#36807;&#20197;&#20943;&#23569;&#38477;&#32423;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;GPU&#20869;&#23384;&#31649;&#29702;&#26426;&#21046;&#65292;&#20197;&#25552;&#21069;&#28165;&#38500;&#19981;&#20877;&#20351;&#29992;&#30340;GPU&#32531;&#23384;&#65292;&#24182;&#23545;&#24120;&#29992;&#27169;&#22411;&#36827;&#34892;&#32531;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) power a new generation of interactive AI applications exemplified by ChatGPT. The interactive nature of these applications demand low job completion time (JCT) for model inference. Existing LLM serving systems use run-to-completion processing for inference jobs, which suffers from head-of-line blocking and long JCT. We present FastServe, a distributed inference serving system for LLMs. FastServe exploits the autoregressive pattern of LLM inference to enable preemption at the granularity of each output token. FastServe uses preemptive scheduling to minimize JCT with a novel skip-join Multi-Level Feedback Queue scheduler. Based on the new semi information-agnostic setting of LLM inference, the scheduler leverages the input length information to assign an appropriate initial queue for each arrival job to join. The higher priority queues than the joined queue are skipped to reduce demotions. We design an efficient GPU memory management mechanism that proactivel
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;ICU&#33041;&#30005;&#30417;&#27979;&#20013;&#24120;&#35265;&#30340;6&#31181;&#33041;&#27874;&#22270;&#26696;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#21644;&#19977;&#31181;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#20020;&#24202;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2211.05207</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26469;&#35782;&#21035;&#30315;&#30187;-&#38388;&#38553;-&#25439;&#20260;&#36830;&#32493;&#29366;&#24577;&#19979;&#30340;&#33041;&#30005;&#22270;&#22270;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Machine Learning System to Identify EEG Patterns on the Ictal-Interictal-Injury Continuum. (arXiv:2211.05207v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;ICU&#33041;&#30005;&#30417;&#27979;&#20013;&#24120;&#35265;&#30340;6&#31181;&#33041;&#27874;&#22270;&#26696;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#21644;&#19977;&#31181;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#20020;&#24202;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#21307;&#23398;&#39046;&#22495;&#65292;&#20154;&#20204;&#21628;&#21505;&#22312;&#29992;&#20110;&#20020;&#24202;&#24037;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#22686;&#21152;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;ICU&#33041;&#30005;&#30417;&#27979;&#20013;&#24120;&#35265;&#30340;6&#31181;&#33041;&#27874;&#22270;&#26696;&#65288;&#30315;&#30187;&#12289;LPD&#12289;GPD&#12289;LRDA&#12289;GRDA&#12289;&#20854;&#20182;&#65289;&#30340;&#23384;&#22312;&#12290;&#27599;&#20010;&#39044;&#27979;&#37117;&#37197;&#26377;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#65292;&#20511;&#21161;&#20110;&#19987;&#38376;&#30340;&#29992;&#25143;&#30028;&#38754;&#25552;&#20379;&#25903;&#25345;&#12290;&#27492;&#26032;&#22411;&#27169;&#22411;&#26550;&#26500;&#23398;&#20064;&#20102;&#19968;&#32452;&#21407;&#22411;&#31034;&#20363;&#65288;&#8220;&#21407;&#22411;&#8221;&#65289;&#65292;&#24182;&#36890;&#36807;&#23558;&#26032;&#30340;EEG&#29255;&#27573;&#19982;&#36825;&#20123;&#21407;&#22411;&#36827;&#34892;&#27604;&#36739;&#26469;&#20570;&#20986;&#20915;&#31574;&#12290;&#36825;&#20123;&#21407;&#22411;&#21487;&#20197;&#26159;&#21333;&#31867;&#65288;&#20165;&#19982;&#19968;&#20010;&#31867;&#30456;&#20851;&#65289;&#25110;&#21452;&#31867;&#65288;&#19982;&#20004;&#20010;&#31867;&#30456;&#20851;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65306;1&#65289;&#20351;&#29992;&#20840;&#23616;&#32467;&#26500;&#20445;&#25345;&#26041;&#27861;&#65292;&#23558;1275&#32500;cEEG&#28508;&#22312;&#29305;&#24449;&#26144;&#23556;&#21040;&#20108;&#32500;&#31354;&#38388;&#20013;&#65292;&#21487;&#35270;&#21270;&#30315;&#30187;-&#38388;&#38553;-&#25439;&#20260;&#36830;&#32493;&#29366;&#24577;&#65292;&#20174;&#32780;&#28145;&#20837;&#20102;&#35299;&#20854;&#39640;&#32500;&#32467;&#26500;&#12290;2&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#35299;&#37322;&#26041;&#27861;&#65292;&#20351;&#20154;&#31867;&#19987;&#23478;&#33021;&#22815;&#26597;&#35810;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#20197;&#33258;&#28982;&#35821;&#35328;&#25509;&#25910;&#32463;&#36807;&#19987;&#23478;&#39564;&#35777;&#30340;&#35299;&#37322;&#12290;3&#65289;&#25105;&#20204;&#21487;&#35270;&#21270;&#20102;&#23548;&#33268;&#27169;&#22411;&#20570;&#20986;&#26576;&#20010;&#20915;&#31574;&#30340;&#36755;&#20837;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#65292;&#20801;&#35768;&#35814;&#32454;&#26816;&#26597;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35299;&#37322;&#24615;&#27169;&#22411;&#20998;&#31867;EEG&#22270;&#26696;&#21644;&#25552;&#20379;&#19987;&#23478;&#21451;&#22909;&#30340;&#35299;&#37322;&#30340;&#23454;&#29992;&#24615;&#65292;&#36825;&#20004;&#20010;&#26041;&#38754;&#23545;&#20110;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#20020;&#24202;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many medical subfields, there is a call for greater interpretability in the machine learning systems used for clinical work. In this paper, we design an interpretable deep learning model to predict the presence of 6 types of brainwave patterns (Seizure, LPD, GPD, LRDA, GRDA, other) commonly encountered in ICU EEG monitoring. Each prediction is accompanied by a high-quality explanation delivered with the assistance of a specialized user interface. This novel model architecture learns a set of prototypical examples (``prototypes'') and makes decisions by comparing a new EEG segment to these prototypes. These prototypes are either single-class (affiliated with only one class) or dual-class (affiliated with two classes).  We present three main ways of interpreting the model: 1) Using global-structure preserving methods, we map the 1275-dimensional cEEG latent features to a 2D space to visualize the ictal-interictal-injury continuum and gain insight into its high-dimensional structure. 2
&lt;/p&gt;</description></item></channel></rss>