<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#30340;&#26080;&#21442;&#25968;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20808;&#39564;&#20102;&#35299;&#38382;&#39064;&#21442;&#25968;&#65292;&#22312;&#24179;&#28369;&#38543;&#26426;&#20984;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#24182;&#34920;&#29616;&#24378;&#22823;&#12290;</title><link>https://arxiv.org/abs/2404.00666</link><description>&lt;p&gt;
&#21152;&#36895;&#30340;&#26080;&#21442;&#25968;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Accelerated Parameter-Free Stochastic Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00666
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#30340;&#26080;&#21442;&#25968;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20808;&#39564;&#20102;&#35299;&#38382;&#39064;&#21442;&#25968;&#65292;&#22312;&#24179;&#28369;&#38543;&#26426;&#20984;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#24182;&#34920;&#29616;&#24378;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#24179;&#28369;&#38543;&#26426;&#20984;&#20248;&#21270;&#30340;&#36817;&#20046;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#19988;&#22522;&#26412;&#19978;&#19981;&#38656;&#35201;&#20808;&#39564;&#20102;&#35299;&#38382;&#39064;&#21442;&#25968;&#12290;&#36825;&#25913;&#36827;&#20102;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#38656;&#35201;&#33267;&#23569;&#30693;&#36947;&#21040;&#26368;&#20248;&#35299;&#30340;&#21021;&#22987;&#36317;&#31163; d0&#12290;&#25105;&#20204;&#30340;&#26041;&#27861; U-DoG &#23558; UniXGrad (Kavis &#31561;&#20154;&#65292;2019) &#21644; DoG (Ivgi &#31561;&#20154;&#65292;2023) &#19982;&#26032;&#39062;&#30340;&#36845;&#20195;&#31283;&#23450;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#23427;&#20165;&#38656;&#35201;&#23545; d0 &#21644;&#22122;&#22768;&#24133;&#24230;&#26377;&#26494;&#25955;&#30340;&#30028;&#38480;&#65292;&#22312;&#27425;&#39640;&#26031;&#22122;&#22768;&#19979;&#25552;&#20379;&#39640;&#27010;&#29575;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#38750;&#24179;&#28369;&#24773;&#20917;&#19979;&#20063;&#25509;&#36817;&#26368;&#20339;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20984;&#38382;&#39064;&#19978;&#33021;&#22815;&#31283;&#23450;&#22320;&#34920;&#29616;&#24378;&#22823;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#19978;&#26377;&#30528;&#19981;&#21516;&#30340;&#25104;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00666v1 Announce Type: new  Abstract: We propose a method that achieves near-optimal rates for smooth stochastic convex optimization and requires essentially no prior knowledge of problem parameters. This improves on prior work which requires knowing at least the initial distance to optimality d0. Our method, U-DoG, combines UniXGrad (Kavis et al., 2019) and DoG (Ivgi et al., 2023) with novel iterate stabilization techniques. It requires only loose bounds on d0 and the noise magnitude, provides high probability guarantees under sub-Gaussian noise, and is also near-optimal in the non-smooth case. Our experiments show consistent, strong performance on convex problems and mixed results on neural network training.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;$\text{CardioVAE}_\text{X,G}$&#65289;&#65292;&#23558;&#20302;&#25104;&#26412;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#24418;&#24335;&#25972;&#21512;&#36215;&#26469;&#65292;&#24182;&#23454;&#29616;&#20102;&#20849;&#20139;&#29305;&#24449;&#21644;&#29420;&#29305;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.13658</link><description>&lt;p&gt;
&#29992;&#20110;&#20302;&#25104;&#26412;&#24515;&#33039;&#34880;&#28082;&#21160;&#21147;&#23398;&#19981;&#31283;&#23450;&#24615;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics Instability Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13658
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;$\text{CardioVAE}_\text{X,G}$&#65289;&#65292;&#23558;&#20302;&#25104;&#26412;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#24418;&#24335;&#25972;&#21512;&#36215;&#26469;&#65292;&#24182;&#23454;&#29616;&#20102;&#20849;&#20139;&#29305;&#24449;&#21644;&#29420;&#29305;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#38750;&#20405;&#20837;&#24615;&#26816;&#27979;&#24515;&#33039;&#34880;&#28082;&#21160;&#21147;&#23398;&#19981;&#31283;&#23450;&#24615;&#65288;CHDI&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#21333;&#19968;&#25968;&#25454;&#24418;&#24335;&#65292;&#22914;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;&#26631;&#35760;&#30340;&#24739;&#32773;&#25968;&#25454;&#37327;&#26377;&#38480;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#25928;&#26524;&#19981;&#20339;&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#30340;&#24120;&#35265;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#26041;&#27861;&#26469;&#30740;&#31350;CHDI&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#26114;&#36149;&#30340;&#25968;&#25454;&#24418;&#24335;&#65292;&#22914;&#24515;&#33039;MRI&#21644;&#24515;&#33039;&#36229;&#22768;&#22270;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;$\text{CardioVAE}_\text{X,G}$&#65289;&#26469;&#25972;&#21512;&#20302;&#25104;&#26412;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#24418;&#24335;&#65292;&#24182;&#22312;&#22823;&#22411;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;$\text{CardioVAE}_\text{X,G}$&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#27969;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#23398;&#20064;&#20849;&#20139;&#29305;&#24449;&#21644;&#21508;&#25968;&#25454;&#24418;&#24335;&#29420;&#26377;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;fi
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13658v1 Announce Type: new  Abstract: Recent advancements in non-invasive detection of cardiac hemodynamic instability (CHDI) primarily focus on applying machine learning techniques to a single data modality, e.g. cardiac magnetic resonance imaging (MRI). Despite their potential, these approaches often fall short especially when the size of labeled patient data is limited, a common challenge in the medical domain. Furthermore, only a few studies have explored multimodal methods to study CHDI, which mostly rely on costly modalities such as cardiac MRI and echocardiogram. In response to these limitations, we propose a novel multimodal variational autoencoder ($\text{CardioVAE}_\text{X,G}$) to integrate low-cost chest X-ray (CXR) and electrocardiogram (ECG) modalities with pre-training on a large unlabeled dataset. Specifically, $\text{CardioVAE}_\text{X,G}$ introduces a novel tri-stream pre-training strategy to learn both shared and modality-specific features, thus enabling fi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32479;&#19968;&#24322;&#24120;&#26816;&#27979;&#30340;Hierarchical Gaussian mixture normalizing flow (HGAD)&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#39640;&#26031;&#28151;&#21512;&#24314;&#27169;&#26469;&#25552;&#21319;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.13349</link><description>&lt;p&gt;
&#20998;&#23618;&#39640;&#26031;&#28151;&#21512;&#27491;&#35268;&#21270;&#27969;&#24314;&#27169;&#29992;&#20110;&#32479;&#19968;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13349
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32479;&#19968;&#24322;&#24120;&#26816;&#27979;&#30340;Hierarchical Gaussian mixture normalizing flow (HGAD)&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#39640;&#26031;&#28151;&#21512;&#24314;&#27169;&#26469;&#25552;&#21319;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#19968;&#24322;&#24120;&#26816;&#27979;&#26159;&#24322;&#24120;&#26816;&#27979;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#20854;&#20013;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#31867;&#21035;&#30340;&#27491;&#24120;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#30446;&#26631;&#26159;&#26816;&#27979;&#36825;&#20123;&#31867;&#21035;&#20013;&#30340;&#24322;&#24120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#39640;&#26031;&#28151;&#21512;&#27491;&#35268;&#21270;&#27969;&#24314;&#27169;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;HGAD&#65292;&#29992;&#20110;&#23436;&#25104;&#32479;&#19968;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;HGAD&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#36328;&#31867;&#21035;&#39640;&#26031;&#28151;&#21512;&#24314;&#27169;&#21644;&#31867;&#20869;&#28151;&#21512;&#31867;&#20013;&#24515;&#23398;&#20064;&#12290;&#19982;&#20808;&#21069;&#22522;&#20110;NF&#30340;AD&#26041;&#27861;&#30456;&#27604;&#65292;&#20998;&#23618;&#39640;&#26031;&#28151;&#21512;&#24314;&#27169;&#26041;&#27861;&#21487;&#20197;&#20026;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#24102;&#26469;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13349v1 Announce Type: new  Abstract: Unified anomaly detection (AD) is one of the most challenges for anomaly detection, where one unified model is trained with normal samples from multiple classes with the objective to detect anomalies in these classes. For such a challenging task, popular normalizing flow (NF) based AD methods may fall into a "homogeneous mapping" issue,where the NF-based AD models are biased to generate similar latent representations for both normal and abnormal features, and thereby lead to a high missing rate of anomalies. In this paper, we propose a novel Hierarchical Gaussian mixture normalizing flow modeling method for accomplishing unified Anomaly Detection, which we call HGAD. Our HGAD consists of two key components: inter-class Gaussian mixture modeling and intra-class mixed class centers learning. Compared to the previous NF-based AD methods, the hierarchical Gaussian mixture modeling approach can bring stronger representation capability to the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#34920;&#29616;&#24615;&#20260;&#23475;&#21644;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;&#30340;&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.13213</link><description>&lt;p&gt;
&#20174;&#34920;&#29616;&#24615;&#20260;&#23475;&#21040;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;:&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#34920;&#29616;&#24615;&#20260;&#23475;&#21644;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;&#30340;&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#23548;&#33268;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36827;&#27493;&#20063;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#20854;&#23545;&#24050;&#32463;&#36793;&#32536;&#21270;&#20154;&#32676;&#30340;&#19981;&#21033;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20943;&#36731;&#25514;&#26045;&#26469;&#24320;&#21457;&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#65292;&#27604;&#22914;&#30417;&#30563;&#24335;&#30340;&#23433;&#20840;&#23450;&#21521;&#24494;&#35843;&#21644;&#21033;&#29992;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#20294;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#20869;&#22312;&#20559;&#35265;&#20173;&#23384;&#22312;&#22810;&#37325;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#20026;&#20102;&#23433;&#20840;&#32780;&#20248;&#21270;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#23637;&#31034;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#65292;&#27604;&#22914;&#20986;&#20110;&#39044;&#38450;&#25514;&#26045;&#32780;&#20542;&#21521;&#20110;&#19981;&#22238;&#24212;&#26576;&#20123;&#35831;&#27714;&#12290;&#22240;&#27492;&#65292;&#25991;&#29486;&#20013;&#24050;&#32463;&#35760;&#24405;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26126;&#26174;&#26435;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23433;&#20840;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13213v1 Announce Type: cross  Abstract: Recent progress in large language models (LLMs) has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations. Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluatin
&lt;/p&gt;</description></item><item><title>DexCap&#26159;&#19968;&#20010;&#21487;&#31227;&#26893;&#30340;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#65292;&#32467;&#21512;DexIL&#31639;&#27861;&#20174;&#20154;&#31867;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#20013;&#35757;&#32451;&#26426;&#22120;&#20154;&#25216;&#33021;&#65292;&#20855;&#26377;&#31934;&#30830;&#36861;&#36394;&#21644;&#22797;&#21046;&#20154;&#31867;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.07788</link><description>&lt;p&gt;
DexCap&#65306;&#29992;&#20110;&#28789;&#24039;&#25805;&#20316;&#30340;&#21487;&#25193;&#23637;&#21644;&#21487;&#31227;&#26893;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#25910;&#38598;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07788
&lt;/p&gt;
&lt;p&gt;
DexCap&#26159;&#19968;&#20010;&#21487;&#31227;&#26893;&#30340;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#65292;&#32467;&#21512;DexIL&#31639;&#27861;&#20174;&#20154;&#31867;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#20013;&#35757;&#32451;&#26426;&#22120;&#20154;&#25216;&#33021;&#65292;&#20855;&#26377;&#31934;&#30830;&#36861;&#36394;&#21644;&#22797;&#21046;&#20154;&#31867;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#26159;&#20026;&#26426;&#22120;&#20154;&#36171;&#20104;&#31867;&#20154;&#28789;&#24039;&#22312;&#29616;&#23454;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#36884;&#24452;&#65292;&#28982;&#32780;&#65292;&#29616;&#23384;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#30340;&#21487;&#31227;&#26893;&#24615;&#20197;&#21450;&#23558;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#36716;&#21270;&#20026;&#26377;&#25928;&#25511;&#21046;&#31574;&#30053;&#30340;&#22256;&#38590;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DexCap&#65292;&#19968;&#20010;&#20415;&#25658;&#24335;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#65292;&#20197;&#21450;DexIL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#20223;&#31639;&#27861;&#65292;&#21487;&#30452;&#25509;&#20174;&#20154;&#31867;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#35757;&#32451;&#28789;&#24039;&#26426;&#22120;&#20154;&#25216;&#33021;&#12290;DexCap&#22522;&#20110;SLAM&#21644;&#30005;&#30913;&#22330;&#20197;&#21450;&#29615;&#22659;&#30340;3D&#35266;&#23519;&#65292;&#25552;&#20379;&#20102;&#23545;&#25163;&#33109;&#21644;&#25163;&#25351;&#36816;&#21160;&#30340;&#31934;&#30830;&#12289;&#25239;&#36974;&#25377;&#30340;&#36319;&#36394;&#12290;&#21033;&#29992;&#36825;&#19968;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;DexIL&#37319;&#29992;&#36870;&#36816;&#21160;&#23398;&#21644;&#22522;&#20110;&#28857;&#20113;&#30340;&#27169;&#20223;&#23398;&#20064;&#26469;&#22797;&#21046;&#20154;&#31867;&#21160;&#20316;&#19982;&#26426;&#22120;&#20154;&#25163;&#12290;&#38500;&#20102;&#20174;&#20154;&#31867;&#36816;&#21160;&#20013;&#23398;&#20064;&#22806;&#65292;DexCap&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;op
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07788v1 Announce Type: cross  Abstract: Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the difficulty of translating mocap data into effective control policies. To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to replicate human actions with robot hands. Beyond learning from human motion, DexCap also offers an op
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#36861;&#36394;&#20013;&#20010;&#20307;&#38382;&#39064;&#20449;&#24687;&#24314;&#27169;&#21644;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#30340;&#37325;&#35201;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.07322</link><description>&lt;p&gt;
&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07322
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#36861;&#36394;&#20013;&#20010;&#20307;&#38382;&#39064;&#20449;&#24687;&#24314;&#27169;&#21644;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#30340;&#37325;&#35201;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#22312;&#36890;&#36807;&#20998;&#26512;&#23398;&#29983;&#21382;&#21490;&#23398;&#20064;&#36807;&#31243;&#26469;&#39044;&#27979;&#20854;&#26410;&#26469;&#34920;&#29616;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#30693;&#35782;&#36861;&#36394;&#38382;&#39064;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#27169;&#25311;&#30693;&#35782;&#36861;&#36394;&#36807;&#31243;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#22312;&#20110;&#23558;&#38382;&#39064;&#30340;&#20010;&#20307;&#20449;&#24687;&#34701;&#20837;&#24314;&#27169;&#20013;&#12290;&#36825;&#24456;&#20851;&#38190;&#65292;&#22240;&#20026;&#23613;&#31649;&#38382;&#39064;&#20849;&#20139;&#30456;&#21516;&#30340;&#30693;&#35782;&#32452;&#20214;&#65288;KC&#65289;&#65292;&#20294;&#23398;&#29983;&#23545;&#21516;&#36136;&#38382;&#39064;&#30340;&#30693;&#35782;&#20064;&#24471;&#21487;&#20197;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#22312;&#20110;&#35299;&#37322;&#29616;&#26377;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#65292;&#34429;&#28982;&#21487;&#33021;&#24182;&#19981;&#38656;&#35201;&#23436;&#20840;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#20294;&#20851;&#38190;&#26159;&#20197;&#32769;&#24072;&#33021;&#29702;&#35299;&#30340;&#26041;&#24335;&#21576;&#29616;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07322v1 Announce Type: cross  Abstract: Knowledge tracing (KT) plays a crucial role in predicting students' future performance by analyzing their historical learning processes. Deep neural networks (DNNs) have shown great potential in solving the KT problem. However, there still exist some important challenges when applying deep learning techniques to model the KT process. The first challenge lies in taking the individual information of the question into modeling. This is crucial because, despite questions sharing the same knowledge component (KC), students' knowledge acquisition on homogeneous questions can vary significantly. The second challenge lies in interpreting the prediction results from existing deep learning-based KT models. In real-world applications, while it may not be necessary to have complete transparency and interpretability of the model parameters, it is crucial to present the model's prediction results in a manner that teachers find interpretable. This ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.06725</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#37325;&#35201;&#24615;&#26426;&#21046;&#24494;&#35843;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#26088;&#22312;&#22522;&#20110;&#23398;&#29983;&#30340;&#21382;&#21490;&#20114;&#21160;&#26469;&#20272;&#35745;&#20182;&#20204;&#30340;&#30693;&#35782;&#25484;&#25569;&#31243;&#24230;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;KT&#65288;DLKT&#65289;&#26041;&#27861;&#22312;KT&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#22914;&#39044;&#31639;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#35266;&#23519;&#21040;&#30340;&#20114;&#21160;&#38750;&#24120;&#26377;&#38480;&#65292;&#21363;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#12290;&#30452;&#25509;&#22312;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;DLKT&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#24182;&#19988;&#24456;&#38590;&#36873;&#25321;&#36866;&#24403;&#30340;&#28145;&#24230;&#31070;&#32463;&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;KT&#26694;&#26550;&#26469;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#12290;&#21463;&#30427;&#34892;&#30340;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06725v1 Announce Type: cross  Abstract: Knowledge tracing (KT) aims to estimate student's knowledge mastery based on their historical interactions. Recently, the deep learning based KT (DLKT) approaches have achieved impressive performance in the KT task. These DLKT models heavily rely on the large number of available student interactions. However, due to various reasons such as budget constraints and privacy concerns, observed interactions are very limited in many real-world scenarios, a.k.a, low-resource KT datasets. Directly training a DLKT model on a low-resource KT dataset may lead to overfitting and it is difficult to choose the appropriate deep neural architecture. Therefore, in this paper, we propose a low-resource KT framework called LoReKT to address above challenges. Inspired by the prevalent "pre-training and fine-tuning" paradigm, we aim to learn transferable parameters and representations from rich-resource KT datasets during the pre-training stage and subseque
&lt;/p&gt;</description></item><item><title>&#21306;&#22359;&#38142;&#25216;&#26415;&#34987;&#25972;&#21512;&#21040;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#20197;&#25552;&#20379;&#26356;&#24378;&#30340;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#32593;&#32476;&#12289;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.00873</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#36171;&#33021;&#30340;&#32852;&#37030;&#23398;&#20064;: &#22909;&#22788;&#12289;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Blockchain-empowered Federated Learning: Benefits, Challenges, and Solutions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00873
&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#25216;&#26415;&#34987;&#25972;&#21512;&#21040;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#20197;&#25552;&#20379;&#26356;&#24378;&#30340;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#32593;&#32476;&#12289;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#24182;&#22312;&#21442;&#25968;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#32858;&#21512;&#26469;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;&#23613;&#31649;&#22312;&#20445;&#25252;&#38544;&#31169;&#26041;&#38754;&#26377;&#25928;&#65292;&#20294;FL&#31995;&#32479;&#38754;&#20020;&#21333;&#28857;&#25925;&#38556;&#12289;&#32570;&#20047;&#28608;&#21169;&#21644;&#19981;&#36275;&#30340;&#23433;&#20840;&#24615;&#31561;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#23558;&#21306;&#22359;&#38142;&#25216;&#26415;&#25972;&#21512;&#21040;FL&#31995;&#32479;&#20013;&#65292;&#20197;&#25552;&#20379;&#26356;&#24378;&#30340;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#28982;&#32780;&#65292;&#21306;&#22359;&#38142;&#36171;&#33021;&#30340;FL(BC-FL)&#31995;&#32479;&#23545;&#32593;&#32476;&#12289;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#25552;&#20986;&#20102;&#39069;&#22806;&#30340;&#38656;&#27714;&#12290;&#26412;&#35843;&#26597;&#20840;&#38754;&#23457;&#26597;&#20102;&#26368;&#36817;&#20851;&#20110;BC-FL&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#19982;&#21306;&#22359;&#38142;&#25972;&#21512;&#30456;&#20851;&#30340;&#22909;&#22788;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21306;&#22359;&#38142;&#20026;&#20309;&#36866;&#29992;&#20110;FL&#65292;&#22914;&#20309;&#23454;&#26045;&#20197;&#21450;&#25972;&#21512;&#30340;&#25361;&#25112;&#21644;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00873v1 Announce Type: cross  Abstract: Federated learning (FL) is a distributed machine learning approach that protects user data privacy by training models locally on clients and aggregating them on a parameter server. While effective at preserving privacy, FL systems face limitations such as single points of failure, lack of incentives, and inadequate security. To address these challenges, blockchain technology is integrated into FL systems to provide stronger security, fairness, and scalability. However, blockchain-empowered FL (BC-FL) systems introduce additional demands on network, computing, and storage resources. This survey provides a comprehensive review of recent research on BC-FL systems, analyzing the benefits and challenges associated with blockchain integration. We explore why blockchain is applicable to FL, how it can be implemented, and the challenges and existing solutions for its integration. Additionally, we offer insights on future research directions fo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#31471;&#21040;&#31471;&#22320;&#24378;&#21270;&#23398;&#20064;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#21644;&#21518;&#24724;-&#21069;&#36827;&#20196;&#29260;&#26469;&#33719;&#21462;&#20219;&#21153;&#20449;&#24687;&#24182;&#20570;&#20986;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.17423</link><description>&lt;p&gt;
&#21152;&#24378;&#19978;&#19979;&#25991;&#40657;&#30418;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reinforced In-Context Black-Box Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17423
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#31471;&#21040;&#31471;&#22320;&#24378;&#21270;&#23398;&#20064;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#21644;&#21518;&#24724;-&#21069;&#36827;&#20196;&#29260;&#26469;&#33719;&#21462;&#20219;&#21153;&#20449;&#24687;&#24182;&#20570;&#20986;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#20248;&#21270;&#65288;BBO&#65289;&#24050;&#32463;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#20803;&#23398;&#20064;BBO&#31639;&#27861;&#30340;&#29305;&#23450;&#32452;&#20214;&#65292;&#20197;&#21152;&#24555;&#20248;&#21270;&#36895;&#24230;&#24182;&#25670;&#33073;&#32321;&#29712;&#30340;&#25163;&#24037;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#20316;&#20026;&#25193;&#23637;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#25972;&#20010;&#31639;&#27861;&#38656;&#35201;&#19987;&#23478;&#26368;&#23569;&#30340;&#24037;&#20316;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#20379;&#26368;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RIBBO&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#24378;&#21270;&#23398;&#20064;BBO&#31639;&#27861;&#12290;RIBBO&#21033;&#29992;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#26469;&#23398;&#20064;&#22810;&#20010;&#34892;&#20026;&#31639;&#27861;&#21644;&#20219;&#21153;&#20135;&#29983;&#30340;&#20248;&#21270;&#21382;&#21490;&#65292;&#21033;&#29992;&#22823;&#22411;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#25552;&#21462;&#20219;&#21153;&#20449;&#24687;&#24182;&#30456;&#24212;&#22320;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#22686;&#21152;&#21518;&#24724;-&#21069;&#36827;&#20196;&#29260;&#26469;&#22686;&#24378;&#20248;&#21270;&#21382;&#21490;&#65292;&#36825;&#20123;&#20196;&#29260;&#26088;&#22312;&#22522;&#20110;&#32047;&#31215;&#34920;&#29616;&#26469;&#34920;&#31034;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17423v1 Announce Type: cross  Abstract: Black-Box Optimization (BBO) has found successful applications in many fields of science and engineering. Recently, there has been a growing interest in meta-learning particular components of BBO algorithms to speed up optimization and get rid of tedious hand-crafted heuristics. As an extension, learning the entire algorithm from data requires the least labor from experts and can provide the most flexibility. In this paper, we propose RIBBO, a method to reinforce-learn a BBO algorithm from offline data in an end-to-end fashion. RIBBO employs expressive sequence models to learn the optimization histories produced by multiple behavior algorithms and tasks, leveraging the in-context learning ability of large models to extract task information and make decisions accordingly. Central to our method is to augment the optimization histories with regret-to-go tokens, which are designed to represent the performance of an algorithm based on cumul
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#25552;&#20986;&#20102;Quantum-SMOTE&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26059;&#36716;&#35282;&#24230;&#12289;&#23569;&#25968;&#31867;&#30334;&#20998;&#27604;&#21644;&#20998;&#35010;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.17398</link><description>&lt;p&gt;
&#37327;&#23376;&#26041;&#27861;&#30740;&#31350;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;
&lt;/p&gt;
&lt;p&gt;
A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17398
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#25552;&#20986;&#20102;Quantum-SMOTE&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26059;&#36716;&#35282;&#24230;&#12289;&#23569;&#25968;&#31867;&#30334;&#20998;&#27604;&#21644;&#20998;&#35010;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Quantum-SMOTE&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;Quantum-SMOTE&#21463;&#21040;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#30340;&#21551;&#21457;&#65292;&#21033;&#29992;&#37327;&#23376;&#36807;&#31243;&#22914;&#20132;&#25442;&#27979;&#35797;&#21644;&#37327;&#23376;&#26059;&#36716;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#28857;&#12290;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;SMOTE&#31639;&#27861;&#20351;&#29992;K-&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#21644;&#27431;&#27663;&#36317;&#31163;&#30340;&#26041;&#24335;&#26377;&#25152;&#19981;&#21516;&#65292;&#33021;&#22815;&#20174;&#23569;&#25968;&#31867;&#25968;&#25454;&#28857;&#29983;&#25104;&#21512;&#25104;&#23454;&#20363;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#37051;&#36817;&#24615;&#12290;&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#26059;&#36716;&#35282;&#24230;&#12289;&#23569;&#25968;&#31867;&#30334;&#20998;&#27604;&#21644;&#20998;&#35010;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#38656;&#27714;&#30340;&#23450;&#21046;&#12290;&#35813;&#26041;&#27861;&#22312;TelecomChurn&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#19982;&#20004;&#31181;&#20027;&#35201;&#30340;&#20998;&#31867;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17398v1 Announce Type: cross  Abstract: The paper proposes the Quantum-SMOTE method, a novel solution that uses quantum computing techniques to solve the prevalent problem of class imbalance in machine learning datasets. Quantum-SMOTE, inspired by the Synthetic Minority Oversampling Technique (SMOTE), generates synthetic data points using quantum processes such as swap tests and quantum rotation. The process varies from the conventional SMOTE algorithm's usage of K-Nearest Neighbors (KNN) and Euclidean distances, enabling synthetic instances to be generated from minority class data points without relying on neighbor proximity. The algorithm asserts greater control over the synthetic data generation process by introducing hyperparameters such as rotation angle, minority percentage, and splitting factor, which allow for customization to specific dataset requirements. The approach is tested on a public dataset of TelecomChurn and evaluated alongside two prominent classification
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#20302;&#31209;&#29615;&#22659;&#20013;&#20855;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#36172;&#24466;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#31574;&#30053;&#35780;&#20272;&#12289;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#21644;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#24182;&#19988;&#22312;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#21644;&#31574;&#30053;&#35780;&#20272;&#26041;&#38754;&#30340;&#31639;&#27861;&#20960;&#20046;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.15739</link><description>&lt;p&gt;
&#20302;&#31209;&#36172;&#24466;&#36890;&#36807;&#32039;&#32477;&#23545;&#21040;&#26080;&#31351;&#22855;&#24322;&#23376;&#31354;&#38388;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15739
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#20302;&#31209;&#29615;&#22659;&#20013;&#20855;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#36172;&#24466;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#31574;&#30053;&#35780;&#20272;&#12289;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#21644;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#24182;&#19988;&#22312;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#21644;&#31574;&#30053;&#35780;&#20272;&#26041;&#38754;&#30340;&#31639;&#27861;&#20960;&#20046;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#30340;&#24773;&#22659;&#36172;&#24466;&#38382;&#39064;&#65292;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#22914;&#26524;&#36873;&#25321;&#20102;(&#24773;&#22659;&#65292;&#21160;&#20316;)&#23545;$(i,j)\in [m]\times [n]$&#65292;&#23398;&#20064;&#32773;&#20250;&#35266;&#23519;&#19968;&#20010;&#26410;&#30693;&#20302;&#31209;&#22870;&#21169;&#30697;&#38453;&#30340;$(i,j)$-th&#20837;&#21475;&#30340;&#22024;&#26434;&#26679;&#26412;&#12290;&#36830;&#32493;&#30340;&#24773;&#22659;&#20197;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#26041;&#24335;&#38543;&#26426;&#29983;&#25104;&#24182;&#36879;&#38706;&#32473;&#23398;&#20064;&#32773;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#36172;&#24466;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#29992;&#20110;&#31574;&#30053;&#35780;&#20272;&#12289;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#21644;&#36951;&#25022;&#26368;&#23567;&#21270;&#12290;&#23545;&#20110;&#31574;&#30053;&#35780;&#20272;&#21644;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20960;&#20046;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;&#20363;&#22914;&#65292;&#20026;&#20102;&#20197;&#33267;&#23569;$1-\delta$&#30340;&#27010;&#29575;&#36820;&#22238;&#19968;&#20010;$\varepsilon$-&#26368;&#20339;&#31574;&#30053;&#65292;&#36890;&#24120;&#38656;&#35201;&#30340;&#26679;&#26412;&#25968;&#22823;&#33268;&#25353;&#29031;${m+n\over \varepsilon^2}\log(1/\delta)$&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#20139;&#26377;&#30340;&#26497;&#23567;&#26497;&#22823;&#20445;&#35777;&#25353;&#29031;$r^{7/4}(m+n)^{3/4}\sqrt{T}$&#32553;&#25918;&#65292;&#36825;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;&#25152;&#26377;&#25552;&#20986;&#30340;&#31639;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15739v1 Announce Type: new  Abstract: We study contextual bandits with low-rank structure where, in each round, if the (context, arm) pair $(i,j)\in [m]\times [n]$ is selected, the learner observes a noisy sample of the $(i,j)$-th entry of an unknown low-rank reward matrix. Successive contexts are generated randomly in an i.i.d. manner and are revealed to the learner. For such bandits, we present efficient algorithms for policy evaluation, best policy identification and regret minimization. For policy evaluation and best policy identification, we show that our algorithms are nearly minimax optimal. For instance, the number of samples required to return an $\varepsilon$-optimal policy with probability at least $1-\delta$ typically scales as ${m+n\over \varepsilon^2}\log(1/\delta)$. Our regret minimization algorithm enjoys minimax guarantees scaling as $r^{7/4}(m+n)^{3/4}\sqrt{T}$, which improves over existing algorithms. All the proposed algorithms consist of two phases: they
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27425;&#27169;&#22359;&#30446;&#26631;&#20989;&#25968;&#36807;&#28388;&#22823;&#37327;&#33258;&#21160;&#35825;&#23548;&#35268;&#21017;&#30340;&#31639;&#27861; FAIR&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#20013;&#32570;&#20047;&#22823;&#37327;&#24050;&#27880;&#37322;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15472</link><description>&lt;p&gt;
FAIR&#65306;&#33258;&#21160;&#35825;&#23548;&#35268;&#21017;&#30340;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
FAIR: Filtering of Automatically Induced Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27425;&#27169;&#22359;&#30446;&#26631;&#20989;&#25968;&#36807;&#28388;&#22823;&#37327;&#33258;&#21160;&#35825;&#23548;&#35268;&#21017;&#30340;&#31639;&#27861; FAIR&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#20013;&#32570;&#20047;&#22823;&#37327;&#24050;&#27880;&#37322;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#24050;&#27880;&#37322;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21487;&#33021;&#26159;&#25104;&#21151;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20851;&#38190;&#29942;&#39048;&#65292;&#29305;&#21035;&#26159;&#24403;&#24212;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#39046;&#22495;&#26102;&#12290;&#24369;&#30417;&#30563;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21152;&#36895;&#20351;&#29992;&#29305;&#23450;&#39046;&#22495;&#35268;&#21017;&#21019;&#24314;&#24102;&#26631;&#31614;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23427;&#35201;&#27714;&#29992;&#25143;&#32534;&#20889;&#22810;&#26679;&#21270;&#19988;&#39640;&#36136;&#37327;&#30340;&#35268;&#21017;&#38598;&#65292;&#20197;&#23558;&#26631;&#31614;&#20998;&#37197;&#32473;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#33258;&#21160;&#35268;&#21017;&#35825;&#23548;&#65288;ARI&#65289;&#26041;&#27861;&#36890;&#36807;&#20174;&#23569;&#37327;&#24102;&#26631;&#31614;&#38598;&#19978;&#30340;&#29305;&#24449;&#33258;&#21160;&#21019;&#24314;&#35268;&#21017;&#24182;&#20174;&#20013;&#36807;&#28388;&#20986;&#26368;&#32456;&#19968;&#32452;&#35268;&#21017;&#26469;&#36991;&#24320;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;ARI&#26041;&#27861;&#20013;&#65292;&#20851;&#38190;&#27493;&#39588;&#26159;&#20174;&#22823;&#37327;&#33258;&#21160;&#21019;&#24314;&#30340;&#35268;&#21017;&#20013;&#36807;&#28388;&#20986;&#19968;&#32452;&#39640;&#36136;&#37327;&#26377;&#29992;&#30340;&#35268;&#21017;&#23376;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#31639;&#27861;&#65288;&#33258;&#21160;&#35825;&#23548;&#35268;&#21017;&#30340;&#36807;&#28388;&#65289;&#65292;&#20351;&#29992;&#32771;&#34385;&#21040;&#27425;&#27169;&#22359;&#30446;&#26631;&#20989;&#25968;&#30340;&#26041;&#27861;&#20174;&#22823;&#37327;&#33258;&#21160;&#35825;&#23548;&#35268;&#21017;&#20013;&#36807;&#28388;&#20986;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15472v1 Announce Type: new  Abstract: The availability of large annotated data can be a critical bottleneck in training machine learning algorithms successfully, especially when applied to diverse domains. Weak supervision offers a promising alternative by accelerating the creation of labeled training data using domain-specific rules. However, it requires users to write a diverse set of high-quality rules to assign labels to the unlabeled data. Automatic Rule Induction (ARI) approaches circumvent this problem by automatically creating rules from features on a small labeled set and filtering a final set of rules from them. In the ARI approach, the crucial step is to filter out a set of a high-quality useful subset of rules from the large set of automatically created rules. In this paper, we propose an algorithm (Filtering of Automatically Induced Rules) to filter rules from a large number of automatically induced rules using submodular objective functions that account for the
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#23545;&#25239;&#28216;&#25103;(ICAG)&#38450;&#24481;&#36234;&#29425;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13148</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#23545;&#25239;&#28216;&#25103;&#38450;&#24481;&#36234;&#29425;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Defending Jailbreak Prompts via In-Context Adversarial Game
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13148
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#23545;&#25239;&#28216;&#25103;(ICAG)&#38450;&#24481;&#36234;&#29425;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20986;&#22312;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20854;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#23545;&#36234;&#29425;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#20173;&#28982;&#23384;&#22312;&#12290;&#21463;&#21040;&#28145;&#24230;&#23398;&#20064;&#20013;&#23545;&#25239;&#35757;&#32451;&#21644;LLM&#20195;&#29702;&#23398;&#20064;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#23545;&#25239;&#28216;&#25103;(ICAG)&#26469;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#65292;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;ICAG&#21033;&#29992;&#20195;&#29702;&#23398;&#20064;&#36827;&#34892;&#23545;&#25239;&#28216;&#25103;&#65292;&#26088;&#22312;&#21160;&#24577;&#25193;&#23637;&#30693;&#35782;&#20197;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#12290;&#19982;&#20381;&#36182;&#38745;&#24577;&#25968;&#25454;&#38598;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;ICAG&#37319;&#29992;&#36845;&#20195;&#36807;&#31243;&#26469;&#22686;&#24378;&#38450;&#24481;&#21644;&#25915;&#20987;&#20195;&#29702;&#12290;&#36825;&#19968;&#25345;&#32493;&#25913;&#36827;&#36807;&#31243;&#21152;&#24378;&#20102;&#23545;&#26032;&#29983;&#25104;&#30340;&#36234;&#29425;&#25552;&#31034;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#35777;&#23454;&#20102;ICAG&#30340;&#26377;&#25928;&#24615;&#65292;&#32463;&#30001;ICAG&#20445;&#25252;&#30340;LLMs&#22312;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#36234;&#29425;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13148v1 Announce Type: new  Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities across diverse applications. However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist. Drawing inspiration from adversarial training in deep learning and LLM agent learning processes, we introduce the In-Context Adversarial Game (ICAG) for defending against jailbreaks without the need for fine-tuning. ICAG leverages agent learning to conduct an adversarial game, aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents. This continuous improvement process strengthens defenses against newly generated jailbreak prompts. Our empirical studies affirm ICAG's efficacy, where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios. Mo
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#21487;&#23398;&#20064;&#20998;&#35299;&#31574;&#30053;&#21644;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21516;&#26102;&#25429;&#25417;&#36328;&#31995;&#21015;&#20381;&#36182;&#21644;&#20869;&#37096;&#21464;&#21270;&#65292;&#20197;&#24212;&#23545;&#22797;&#26434;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12694</link><description>&lt;p&gt;
&#22797;&#20852;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#21487;&#23398;&#20064;&#20998;&#35299;&#19982;&#36328;&#31995;&#21015;&#20381;&#36182;&#20851;&#31995;&#21644;&#20869;&#37096;&#21464;&#21270;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12694
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#21487;&#23398;&#20064;&#20998;&#35299;&#31574;&#30053;&#21644;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21516;&#26102;&#25429;&#25417;&#36328;&#31995;&#21015;&#20381;&#36182;&#21644;&#20869;&#37096;&#21464;&#21270;&#65292;&#20197;&#24212;&#23545;&#22797;&#26434;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#35201;&#27714;&#31934;&#30830;&#24314;&#27169;&#38169;&#32508;&#22797;&#26434;&#27169;&#24335;&#65292;&#21253;&#25324;&#36328;&#26102;&#38388;&#24207;&#21015;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#20869;&#37096;&#21464;&#21160;&#12290;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#29420;&#29305;&#30340;&#36235;&#21183;&#29305;&#24449;&#24102;&#26469;&#25361;&#25112;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#22522;&#26412;&#30340;&#31227;&#21160;&#24179;&#22343;&#26680;&#21487;&#33021;&#38590;&#20197;&#22788;&#29702;&#29616;&#23454;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#32467;&#26500;&#21644;&#22797;&#26434;&#36235;&#21183;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20998;&#35299;&#31574;&#30053;&#65292;&#26356;&#21512;&#29702;&#22320;&#25429;&#25417;&#21160;&#24577;&#36235;&#21183;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#19987;&#38376;&#29992;&#20110;&#21516;&#26102;&#25429;&#25417;&#36328;&#31995;&#21015;&#20381;&#36182;&#20851;&#31995;&#21644;&#20869;&#37096;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#20854;&#20013;&#36890;&#36807;&#36890;&#36947;&#33258;&#27880;&#24847;&#21147;&#21644;&#33258;&#22238;&#24402;&#33258;&#27880;&#24847;&#21147;&#23454;&#29616;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#20843;&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36890;&#36807;&#27604;&#36739;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340; Leddam...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12694v1 Announce Type: new  Abstract: Predicting multivariate time series is crucial, demanding precise modeling of intricate patterns, including inter-series dependencies and intra-series variations. Distinctive trend characteristics in each time series pose challenges, and existing methods, relying on basic moving average kernels, may struggle with the non-linear structure and complex trends in real-world data. Given that, we introduce a learnable decomposition strategy to capture dynamic trend information more reasonably. Additionally, we propose a dual attention module tailored to capture inter-series dependencies and intra-series variations simultaneously for better time series forecasting, which is implemented by channel-wise self-attention and autoregressive self-attention. To evaluate the effectiveness of our method, we conducted experiments across eight open-source datasets and compared it with the state-of-the-art methods. Through the comparison results, our Leddam
&lt;/p&gt;</description></item><item><title>LoRA+&#36890;&#36807;&#35774;&#32622;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#25913;&#36827;&#21407;&#22987;LoRA&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#24494;&#35843;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.12354</link><description>&lt;p&gt;
LoRA+: &#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#39640;&#25928;&#20302;&#31209;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
LoRA+: Efficient Low Rank Adaptation of Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12354
&lt;/p&gt;
&lt;p&gt;
LoRA+&#36890;&#36807;&#35774;&#32622;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#25913;&#36827;&#21407;&#22987;LoRA&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#24494;&#35843;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26368;&#21021;&#30001;&#32993;&#31561;&#20154;&#65288;2021&#24180;&#65289;&#24341;&#20837;&#65292;&#23548;&#33268;&#23545;&#20855;&#26377;&#22823;&#23485;&#24230;&#65288;&#23884;&#20837;&#32500;&#24230;&#65289;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26102;&#34920;&#29616;&#20122;&#20248;&#12290;&#36825;&#26159;&#22240;&#20026;LoRA&#20013;&#30340;&#36866;&#37197;&#22120;&#30697;&#38453;A&#21644;B&#20351;&#29992;&#30456;&#21516;&#30340;&#23398;&#20064;&#29575;&#36827;&#34892;&#26356;&#26032;&#12290;&#36890;&#36807;&#23545;&#22823;&#23485;&#24230;&#32593;&#32476;&#36827;&#34892;&#32553;&#25918;&#21442;&#25968;&#30340;&#35770;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#36866;&#37197;&#22120;&#30697;&#38453;A&#21644;B&#20351;&#29992;&#30456;&#21516;&#30340;&#23398;&#20064;&#29575;&#19981;&#21033;&#20110;&#26377;&#25928;&#30340;&#29305;&#24449;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;LoRA&#30340;&#36825;&#31181;&#27425;&#20248;&#24615;&#21487;&#20197;&#31616;&#21333;&#22320;&#36890;&#36807;&#20026;LoRA&#36866;&#37197;&#22120;&#30697;&#38453;A&#21644;B&#35774;&#32622;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#20197;&#21450;&#19968;&#20010;&#31934;&#24515;&#36873;&#25321;&#30340;&#27604;&#29575;&#26469;&#36827;&#34892;&#26657;&#27491;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#25552;&#20986;&#30340;&#31639;&#27861;&#31216;&#20026;LoRA$+$&#12290;&#22312;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;LoRA$+$&#22312;&#30456;&#21516;&#35745;&#31639;&#25104;&#26412;&#19979;&#25552;&#39640;&#20102;&#24615;&#33021;&#65288;1-2&#65285;&#30340;&#25913;&#36827;&#65289;&#21644;&#24494;&#35843;&#36895;&#24230;&#65288;&#26368;&#22810;&#25552;&#36895;&#32422;2&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12354v1 Announce Type: cross  Abstract: In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\%$ improvements) and finetuning speed (up to $\sim$ 2X SpeedUp), at the same computational cost as LoRA.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#26102;&#38388;&#20449;&#24687;&#21644;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#65292;&#38381;&#28304;&#27169;&#22411;&#21487;&#33021;&#26263;&#31034;&#20102;&#19981;&#30830;&#23450;&#24615;&#35748;&#35782;&#19982;&#38169;&#35823;&#22238;&#24212;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.11997</link><description>&lt;p&gt;
&#22238;&#24518;&#37027;&#19968;&#24180;&#21457;&#29983;&#30340;&#20107;&#20214;&#65311;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#21644;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11997
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#26102;&#38388;&#20449;&#24687;&#21644;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#65292;&#38381;&#28304;&#27169;&#22411;&#21487;&#33021;&#26263;&#31034;&#20102;&#19981;&#30830;&#23450;&#24615;&#35748;&#35782;&#19982;&#38169;&#35823;&#22238;&#24212;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#25512;&#29702;&#21644;&#20445;&#30041;&#26102;&#38388;&#20449;&#24687;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29702;&#35299;&#20107;&#20214;&#30340;&#39034;&#24207;&#24615;&#23545;&#20851;&#38190;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#25968;&#25454;&#38598;\textbf{TempUN}&#19978;&#23545;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#26102;&#38388;&#20445;&#30041;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26174;&#33879;&#38480;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#38381;&#28304;&#27169;&#22411;&#26356;&#39057;&#32321;&#22320;&#26174;&#31034;&#20986;&#30693;&#35782;&#24046;&#36317;&#65292;&#21487;&#33021;&#26263;&#31034;&#20102;&#19981;&#30830;&#23450;&#24615;&#35748;&#35782;&#21644;&#38169;&#35823;&#22238;&#24212;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25506;&#32034;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;&#24182;&#27809;&#26377;&#24102;&#26469;&#20027;&#35201;&#24615;&#33021;&#25913;&#36827;&#12290;&#30456;&#20851;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#33719;&#24471;&#65288;https://github.com/lingoiitgn/TempUN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11997v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their ability to reason about and retain temporal information remains limited. This hinders their application in real-world scenarios where understanding the sequential nature of events is crucial. This paper experiments with state-of-the-art models on a novel, large-scale temporal dataset, \textbf{TempUN}, to reveal significant limitations in temporal retention and reasoning abilities. Interestingly, closed-source models indicate knowledge gaps more frequently, potentially suggesting a trade-off between uncertainty awareness and incorrect responses. Further, exploring various fine-tuning approaches yielded no major performance improvements. The associated dataset and code are available at the following URL (https://github.com/lingoiitgn/TempUN).
&lt;/p&gt;</description></item><item><title>U$^2$MRPD&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#24341;&#23548;&#65292;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#27424;&#37319;&#26679;MRI&#37325;&#24314;&#65292;&#33021;&#22815;&#25903;&#25345;&#22270;&#20687;&#29305;&#23450;&#30340;MRI&#37325;&#24314;&#65292;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#30417;&#30563;&#21644;MRI&#25193;&#25955;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10609</link><description>&lt;p&gt;
U$^2$MRPD: &#36890;&#36807;&#22823;&#22411;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#24341;&#23548;&#30340;&#26080;&#30417;&#30563;MRI&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
U$^2$MRPD: Unsupervised undersampled MRI reconstruction by prompting a large latent diffusion model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10609
&lt;/p&gt;
&lt;p&gt;
U$^2$MRPD&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#24341;&#23548;&#65292;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#27424;&#37319;&#26679;MRI&#37325;&#24314;&#65292;&#33021;&#22815;&#25903;&#25345;&#22270;&#20687;&#29305;&#23450;&#30340;MRI&#37325;&#24314;&#65292;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#30417;&#30563;&#21644;MRI&#25193;&#25955;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10609v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LLDM)&#20013;&#34164;&#21547;&#30528;&#20016;&#23500;&#32780;&#20551;&#35774;&#19978;&#26222;&#36941;&#36866;&#29992;&#20110;&#33258;&#28982;&#21644;&#21307;&#23398;&#22270;&#20687;&#30340;&#38544;&#21547;&#35270;&#35273;&#30693;&#35782;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;U$^2$MRPD&#65289;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#27424;&#37319;&#26679;MRI&#37325;&#24314;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#12289;&#30417;&#30563;&#30340;&#27424;&#37319;&#26679;MRI&#37325;&#24314;&#32593;&#32476;&#36890;&#24120;&#20855;&#26377;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#36866;&#24212;&#24615;&#65292;&#19981;&#36275;&#20197;&#24212;&#23545;&#21508;&#31181;&#25968;&#25454;&#37319;&#38598;&#22330;&#26223;&#65307;&#28982;&#32780;&#65292;U$^2$MRPD&#36890;&#36807;&#20351;&#29992;&#37327;&#36523;&#23450;&#21046;&#30340;MRSampler&#65292;&#25903;&#25345;&#22270;&#20687;&#29305;&#23450;&#30340;MRI&#37325;&#24314;&#65292;&#35813;MRSampler&#36866;&#29992;&#20110;&#22797;&#20540;MRI&#22270;&#20687;&#12290;&#36890;&#36807;&#20219;&#20309;&#21333;&#19968;&#26469;&#28304;&#25110;&#22810;&#28304;MRI&#25968;&#25454;&#38598;&#65292;U$^2$MRPD&#30340;&#24615;&#33021;&#36824;&#21487;&#20197;&#36890;&#36807;MRAdapter&#36827;&#34892;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#22270;&#20687;&#20808;&#39564;&#19981;&#21464;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;U$^2$MRPD&#23454;&#29616;&#20102;&#19982;&#30417;&#30563;&#21644;MRI&#25193;&#25955;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10609v1 Announce Type: cross  Abstract: Implicit visual knowledge in a large latent diffusion model (LLDM) pre-trained on natural images is rich and hypothetically universal to natural and medical images. To test this hypothesis, we introduce a novel framework for Unsupervised Undersampled MRI Reconstruction by Prompting a pre-trained large latent Diffusion model ( U$^2$MRPD). Existing data-driven, supervised undersampled MRI reconstruction networks are typically of limited generalizability and adaptability toward diverse data acquisition scenarios; yet U$^2$MRPD supports image-specific MRI reconstruction by prompting an LLDM with an MRSampler tailored for complex-valued MRI images. With any single-source or diverse-source MRI dataset, U$^2$MRPD's performance is further boosted by an MRAdapter while keeping the generative image priors intact. Experiments on multiple datasets show that U$^2$MRPD achieves comparable or better performance than supervised and MRI diffusion metho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#20551;&#35774;&#20102;&#32534;&#30721;&#26144;&#23556;&#21644;&#35299;&#30721;&#26144;&#23556;&#20026;&#20219;&#24847;&#20809;&#28369;&#20989;&#25968;&#65292;&#24182;&#19988;&#33021;&#22815;&#25551;&#36848;&#22797;&#26434;&#19988;&#22823;&#22411;&#26550;&#26500;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;</title><link>https://arxiv.org/abs/2402.09142</link><description>&lt;p&gt;
&#24403;&#34920;&#31034;&#23545;&#40784;&#26102;&#65306;&#34920;&#31034;&#23398;&#20064;&#21160;&#21147;&#23398;&#20013;&#30340;&#26222;&#36941;&#24615;
&lt;/p&gt;
&lt;p&gt;
When Representations Align: Universality in Representation Learning Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#20551;&#35774;&#20102;&#32534;&#30721;&#26144;&#23556;&#21644;&#35299;&#30721;&#26144;&#23556;&#20026;&#20219;&#24847;&#20809;&#28369;&#20989;&#25968;&#65292;&#24182;&#19988;&#33021;&#22815;&#25551;&#36848;&#22797;&#26434;&#19988;&#22823;&#22411;&#26550;&#26500;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26377;&#35768;&#22810;&#19981;&#21516;&#30340;&#22823;&#23567;&#21644;&#32467;&#26500;&#12290;&#26550;&#26500;&#30340;&#36873;&#25321;&#65292;&#32467;&#21512;&#25968;&#25454;&#38598;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#26222;&#36941;&#35748;&#20026;&#24433;&#21709;&#20102;&#23398;&#20064;&#21040;&#30340;&#31070;&#32463;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#19981;&#21516;&#30340;&#26550;&#26500;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20855;&#26377;&#24778;&#20154;&#30340;&#23450;&#24615;&#30456;&#20284;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#23558;&#36755;&#20837;&#21040;&#38544;&#34255;&#34920;&#31034;&#30340;&#32534;&#30721;&#26144;&#23556;&#21644;&#20174;&#34920;&#31034;&#21040;&#36755;&#20986;&#30340;&#35299;&#30721;&#26144;&#23556;&#37117;&#26159;&#20219;&#24847;&#20809;&#28369;&#20989;&#25968;&#30340;&#20551;&#35774;&#19979;&#65292;&#25512;&#23548;&#20102;&#34920;&#31034;&#23398;&#20064;&#30340;&#26377;&#25928;&#29702;&#35770;&#12290;&#22312;&#22797;&#26434;&#21644;&#22823;&#22411;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#38544;&#34255;&#34920;&#31034;&#27809;&#26377;&#34987;&#21442;&#25968;&#21270;&#24378;&#32422;&#26463;&#65292;&#35813;&#29702;&#35770;&#27010;&#25324;&#20102;&#34920;&#31034;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20010;&#26377;&#25928;&#29702;&#35770;&#25551;&#36848;&#20102;&#20855;&#26377;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#21644;&#26550;&#26500;&#30340;&#28145;&#24230;&#32593;&#32476;&#20013;&#34920;&#31034;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#19968;&#20123;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09142v1 Announce Type: new Abstract: Deep neural networks come in many sizes and architectures. The choice of architecture, in conjunction with the dataset and learning algorithm, is commonly understood to affect the learned neural representations. Yet, recent results have shown that different architectures learn representations with striking qualitative similarities. Here we derive an effective theory of representation learning under the assumption that the encoding map from input to hidden representation and the decoding map from representation to output are arbitrary smooth functions. This theory schematizes representation learning dynamics in the regime of complex, large architectures, where hidden representations are not strongly constrained by the parametrization. We show through experiments that the effective theory describes aspects of representation learning dynamics across a range of deep networks with different activation functions and architectures, and exhibits 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22343;&#22330;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243; (SDE) &#30340;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#25913;&#36827;&#30340;&#20445;&#35777;&#65292;&#21253;&#25324;&#22312;&#22343;&#22330;&#21306;&#22495;&#20248;&#21270;&#26576;&#20123;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#22909;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.07355</link><description>&lt;p&gt;
&#20174;&#22343;&#22330;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Sampling from the Mean-Field Stationary Distribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22343;&#22330;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243; (SDE) &#30340;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#25913;&#36827;&#30340;&#20445;&#35777;&#65292;&#21253;&#25324;&#22312;&#22343;&#22330;&#21306;&#22495;&#20248;&#21270;&#26576;&#20123;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#22909;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#22343;&#22330;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243; (SDE) &#30340;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#31561;&#20215;&#22320;&#65292;&#21363;&#21253;&#21547;&#20132;&#20114;&#39033;&#30340;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#19978;&#30340;&#26368;&#23567;&#21270;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#27934;&#23519;&#26159;&#23558;&#36825;&#20010;&#38382;&#39064;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#35299;&#32806;&#65306;(1) &#36890;&#36807;&#26377;&#38480;&#31890;&#23376;&#31995;&#32479;&#36924;&#36817;&#22343;&#22330;SDE&#65292;&#36890;&#36807;&#26102;&#38388;&#22343;&#21248;&#20256;&#25773;&#28151;&#27788;&#65292;&#21644;(2) &#36890;&#36807;&#26631;&#20934;&#23545;&#25968;&#20985;&#25277;&#26679;&#22120;&#20174;&#26377;&#38480;&#31890;&#23376;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27010;&#24565;&#19978;&#26356;&#31616;&#21333;&#65292;&#20854;&#28789;&#27963;&#24615;&#20801;&#35768;&#32467;&#21512;&#29992;&#20110;&#31639;&#27861;&#21644;&#29702;&#35770;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#36825;&#23548;&#33268;&#22312;&#35768;&#22810;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#20445;&#35777;&#65292;&#21253;&#25324;&#22312;&#22343;&#22330;&#21306;&#22495;&#20248;&#21270;&#26576;&#20123;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#22909;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the complexity of sampling from the stationary distribution of a mean-field SDE, or equivalently, the complexity of minimizing a functional over the space of probability measures which includes an interaction term.   Our main insight is to decouple the two key aspects of this problem: (1) approximation of the mean-field SDE via a finite-particle system, via uniform-in-time propagation of chaos, and (2) sampling from the finite-particle stationary distribution, via standard log-concave samplers. Our approach is conceptually simpler and its flexibility allows for incorporating the state-of-the-art for both algorithms and theory. This leads to improved guarantees in numerous settings, including better guarantees for optimizing certain two-layer neural networks in the mean-field regime.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#36866;&#24212;&#24615;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#23545;&#20110;&#20984;&#38382;&#39064;&#19981;&#21463;&#20256;&#32479;&#20551;&#35774;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23616;&#37096;&#26799;&#24230;H\"older&#36830;&#32493;&#24615;&#26465;&#20214;&#19979;&#25910;&#25947;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#32447;&#25628;&#32034;&#27493;&#39588;&#21644;&#36817;&#20284;&#30340;&#20351;&#29992;&#12290;&#23545;&#23616;&#37096;H\"older&#24120;&#25968;&#21644;H\"older&#36830;&#32493;&#24615;&#39034;&#24207;&#30340;&#20808;&#39564;&#30693;&#35782;&#20063;&#19981;&#26159;&#24517;&#38656;&#30340;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#19982;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616; H\"older &#35774;&#32622;&#12290;</title><link>https://arxiv.org/abs/2402.06271</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#22312;&#27809;&#26377;&#36817;&#20284;&#30340;&#24773;&#20917;&#19979;&#26159;&#36890;&#29992;&#30340;
&lt;/p&gt;
&lt;p&gt;
Adaptive proximal gradient methods are universal without approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06271
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#36866;&#24212;&#24615;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#23545;&#20110;&#20984;&#38382;&#39064;&#19981;&#21463;&#20256;&#32479;&#20551;&#35774;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23616;&#37096;&#26799;&#24230;H\"older&#36830;&#32493;&#24615;&#26465;&#20214;&#19979;&#25910;&#25947;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#32447;&#25628;&#32034;&#27493;&#39588;&#21644;&#36817;&#20284;&#30340;&#20351;&#29992;&#12290;&#23545;&#23616;&#37096;H\"older&#24120;&#25968;&#21644;H\"older&#36830;&#32493;&#24615;&#39034;&#24207;&#30340;&#20808;&#39564;&#30693;&#35782;&#20063;&#19981;&#26159;&#24517;&#38656;&#30340;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#19982;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616; H\"older &#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#20984;&#38382;&#39064;&#65292;&#36866;&#24212;&#24615;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#19981;&#21463;&#20256;&#32479;&#21033;&#26222;&#24076;&#20857;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#31867;&#26080;&#38656;&#32447;&#25628;&#32034;&#30340;&#26041;&#27861;&#22312;&#20165;&#20855;&#26377;&#23616;&#37096;H\"older&#26799;&#24230;&#36830;&#32493;&#24615;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#25910;&#25947;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#36830;&#32493;&#21487;&#24494;&#30340;&#21322;&#20195;&#25968;&#20989;&#25968;&#12290;&#20026;&#20102;&#24357;&#34917;&#32570;&#20047;&#23616;&#37096;&#21033;&#26222;&#24076;&#20857;&#36830;&#32493;&#24615;&#30340;&#38382;&#39064;&#65292;&#24120;&#35265;&#30340;&#26041;&#27861;&#21253;&#25324;$\varepsilon$-oracle&#21644;/&#25110;&#32447;&#25628;&#32034;&#27493;&#39588;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#26222;&#36890;&#30340;H\"older&#19981;&#31561;&#24335;&#32780;&#19981;&#28041;&#21450;&#20219;&#20309;&#36817;&#20284;&#65292;&#21516;&#26102;&#20445;&#25345;&#36866;&#24212;&#24615;&#26041;&#26696;&#26080;&#38656;&#32447;&#25628;&#32034;&#30340;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#23616;&#37096;H\"older&#24120;&#25968;&#25110;H\"older&#36830;&#32493;&#24615;&#30340;&#39034;&#24207;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#23436;&#20840;&#30340;&#24207;&#21015;&#25910;&#25947;&#24615;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#20102;&#22522;&#20934;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#28085;&#30422;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616; H\"older &#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that adaptive proximal gradient methods for convex problems are not restricted to traditional Lipschitzian assumptions. Our analysis reveals that a class of linesearch-free methods is still convergent under mere local H\"older gradient continuity, covering in particular continuously differentiable semi-algebraic functions. To mitigate the lack of local Lipschitz continuity, popular approaches revolve around $\varepsilon$-oracles and/or linesearch procedures. In contrast, we exploit plain H\"older inequalities not entailing any approximation, all while retaining the linesearch-free nature of adaptive schemes. Furthermore, we prove full sequence convergence without prior knowledge of local H\"older constants nor of the order of H\"older continuity. In numerical experiments we present comparisons to baseline methods on diverse tasks from machine learning covering both the locally and the globally H\"older setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28201;&#24230;&#32553;&#25918;&#23545;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#26657;&#20934;&#23545;&#33258;&#36866;&#24212;C&#26041;&#27861;&#20135;&#29983;&#20102;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.05806</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#20998;&#31867;&#22120;&#30340;&#26657;&#20934;&#21644;&#31526;&#21512;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Calibration and Conformal Prediction of Deep Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28201;&#24230;&#32553;&#25918;&#23545;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#26657;&#20934;&#23545;&#33258;&#36866;&#24212;C&#26041;&#27861;&#20135;&#29983;&#20102;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20998;&#31867;&#24212;&#29992;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#38656;&#35201;&#20276;&#38543;&#19968;&#20123;&#32622;&#20449;&#24230;&#25351;&#31034;&#12290;&#38024;&#23545;&#36825;&#20010;&#30446;&#26631;&#65292;&#26377;&#20004;&#31181;&#27969;&#34892;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65306;1&#65289;&#26657;&#20934;&#65306;&#20462;&#25913;&#20998;&#31867;&#22120;&#30340;softmax&#20540;&#65292;&#20351;&#20854;&#26368;&#22823;&#20540;&#65288;&#19982;&#39044;&#27979;&#30456;&#20851;&#65289;&#26356;&#22909;&#22320;&#20272;&#35745;&#27491;&#30830;&#27010;&#29575;&#65307;&#21644;2&#65289;&#31526;&#21512;&#39044;&#27979;&#65288;CP&#65289;&#65306;&#35774;&#35745;&#19968;&#20010;&#22522;&#20110;softmax&#20540;&#30340;&#20998;&#25968;&#65292;&#20174;&#20013;&#20135;&#29983;&#19968;&#32452;&#39044;&#27979;&#65292;&#20855;&#26377;&#29702;&#35770;&#19978;&#20445;&#35777;&#27491;&#30830;&#31867;&#21035;&#36793;&#38469;&#35206;&#30422;&#30340;&#29305;&#24615;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#20004;&#31181;&#25351;&#31034;&#37117;&#21487;&#33021;&#26159;&#38656;&#35201;&#30340;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28201;&#24230;&#32553;&#25918;&#65292;&#36825;&#26159;&#26368;&#24120;&#35265;&#30340;&#26657;&#20934;&#25216;&#26415;&#65292;&#23545;&#37325;&#35201;&#30340;CP&#26041;&#27861;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#19968;&#39033;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#20854;&#20013;&#26174;&#31034;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#27934;&#23519;&#65292;&#20854;&#20013;&#21253;&#25324;&#20196;&#20154;&#24778;&#35766;&#30340;&#21457;&#29616;&#65292;&#21363;&#26657;&#20934;&#23545;&#27969;&#34892;&#30340;&#33258;&#36866;&#24212;C&#26041;&#27861;&#20135;&#29983;&#20102;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many classification applications, the prediction of a deep neural network (DNN) based classifier needs to be accompanied with some confidence indication. Two popular post-processing approaches for that aim are: 1) calibration: modifying the classifier's softmax values such that their maximum (associated with the prediction) better estimates the correctness probability; and 2) conformal prediction (CP): devising a score (based on the softmax values) from which a set of predictions with theoretically guaranteed marginal coverage of the correct class is produced. While in practice both types of indications can be desired, so far the interplay between them has not been investigated. Toward filling this gap, in this paper we study the effect of temperature scaling, arguably the most common calibration technique, on prominent CP methods. We start with an extensive empirical study that among other insights shows that, surprisingly, calibration has a detrimental effect on popular adaptive C
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#22270;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#65292;&#33021;&#22815;&#22312;&#26410;&#30693;&#30340;&#19977;&#32500;&#29615;&#22659;&#20013;&#26144;&#23556;&#20986;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.04894</link><description>&lt;p&gt;
&#21033;&#29992;&#21160;&#24577;&#22270;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#33258;&#36866;&#24212;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning with Dynamic Graphs for Adaptive Informative Path Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04894
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#22270;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#65292;&#33021;&#22815;&#22312;&#26410;&#30693;&#30340;&#19977;&#32500;&#29615;&#22659;&#20013;&#26144;&#23556;&#20986;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#24120;&#24120;&#34987;&#29992;&#20110;&#25968;&#25454;&#25910;&#38598;&#65292;&#22240;&#20026;&#23427;&#20204;&#39640;&#25928;&#19988;&#21171;&#21160;&#25104;&#26412;&#20302;&#12290;&#26426;&#22120;&#20154;&#25968;&#25454;&#37319;&#38598;&#30340;&#20851;&#38190;&#20219;&#21153;&#26159;&#22312;&#21021;&#22987;&#26410;&#30693;&#29615;&#22659;&#20013;&#35268;&#21010;&#36335;&#24452;&#65292;&#20197;&#28385;&#36275;&#24179;&#21488;&#29305;&#23450;&#30340;&#36164;&#28304;&#32422;&#26463;&#65292;&#20363;&#22914;&#26377;&#38480;&#30340;&#30005;&#27744;&#23551;&#21629;&#12290;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#22312;&#32447;&#36335;&#24452;&#35268;&#21010;&#38754;&#20020;&#30528;&#24456;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#22823;&#37327;&#26377;&#25928;&#21160;&#20316;&#30340;&#23384;&#22312;&#20197;&#21450;&#26410;&#30693;&#36974;&#25377;&#29289;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#37325;&#26032;&#35268;&#21010;&#26426;&#22120;&#20154;&#36335;&#24452;&#20197;&#22312;&#26410;&#30693;&#30340;&#19977;&#32500;&#29615;&#22659;&#20013;&#26144;&#23556;&#20986;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#20043;&#22788;&#22312;&#20110;&#26500;&#24314;&#21160;&#24577;&#22270;&#65292;&#23558;&#35268;&#21010;&#21160;&#20316;&#38480;&#21046;&#22312;&#26426;&#22120;&#20154;&#38468;&#36817;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#24555;&#36895;&#21709;&#24212;&#26032;&#21457;&#29616;&#30340;&#38556;&#30861;&#21644;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#12290;&#23545;&#20110;&#37325;&#26032;&#35268;&#21010;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24179;&#34913;&#25506;&#32034;&#26410;&#30693;&#29615;&#22659;&#21644;&#21033;&#29992;&#22312;&#32447;&#25910;&#38598;&#30340;&#26377;&#20851;&#24863;&#20852;&#36259;&#30446;&#26631;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous robots are often employed for data collection due to their efficiency and low labour costs. A key task in robotic data acquisition is planning paths through an initially unknown environment to collect observations given platform-specific resource constraints, such as limited battery life. Adaptive online path planning in 3D environments is challenging due to the large set of valid actions and the presence of unknown occlusions. To address these issues, we propose a novel deep reinforcement learning approach for adaptively replanning robot paths to map targets of interest in unknown 3D environments. A key aspect of our approach is a dynamically constructed graph that restricts planning actions local to the robot, allowing us to quickly react to newly discovered obstacles and targets of interest. For replanning, we propose a new reward function that balances between exploring the unknown environment and exploiting online-collected data about the targets of interest. Our experi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;</title><link>https://arxiv.org/abs/2402.04854</link><description>&lt;p&gt;
&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#23398;&#26415;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04854
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32570;&#20047;&#30740;&#31350;&#22521;&#35757;&#30340;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#26469;&#35828;&#65292;&#30740;&#31350;&#35843;&#26597;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#36825;&#20123;&#30740;&#31350;&#32773;&#22312;&#30701;&#26102;&#38388;&#20869;&#24456;&#38590;&#29702;&#35299;&#20182;&#20204;&#30740;&#31350;&#20027;&#39064;&#20869;&#30340;&#26041;&#21521;&#65292;&#20197;&#21450;&#21457;&#29616;&#26032;&#30340;&#30740;&#31350;&#21457;&#29616;&#12290;&#20026;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#25552;&#20379;&#30452;&#35266;&#30340;&#24110;&#21161;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#25552;&#20379;&#30456;&#20851;&#30340;&#30693;&#35782;&#22270;&#35889;(KG)&#24182;&#25512;&#33616;&#30456;&#20851;&#30340;&#23398;&#26415;&#35770;&#25991;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#20027;&#35201;&#20381;&#36182;&#20110;&#30740;&#31350;&#39046;&#22495;&#30340;&#20851;&#38190;&#23383;&#65292;&#24120;&#24120;&#26080;&#27861;&#28165;&#26970;&#22320;&#21576;&#29616;&#22810;&#20010;&#30456;&#20851;&#35770;&#25991;&#20043;&#38388;&#30340;&#36923;&#36753;&#23618;&#27425;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20165;&#20165;&#20381;&#36182;&#20110;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#35753;&#30740;&#31350;&#20154;&#21592;&#22256;&#24785;&#20026;&#20160;&#20040;&#25512;&#33616;&#20102;&#29305;&#23450;&#30340;&#25991;&#31456;&#12290;&#20182;&#20204;&#21487;&#33021;&#32570;&#20047;&#20102;&#35299;&#20851;&#20110;&#20182;&#20204;&#24076;&#26395;&#33719;&#24471;&#30340;"&#38382;&#39064;&#35299;&#20915;"&#21644;"&#38382;&#39064;&#21457;&#29616;"&#20043;&#38388;&#30340;&#35265;&#35299;&#36830;&#25509;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25903;&#25345;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between "Issue resolved" and "Issue finding" that they hope to obtain. To address these issues, this study aims to support research insight surveys for begi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#29699;&#38754;&#39640;&#26031;&#32422;&#26463;&#30340;&#25193;&#25955;&#31639;&#27861;&#65288;DSG&#65289;&#65292;&#35299;&#20915;&#20102;&#22312;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#20013;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#27969;&#24418;&#20559;&#31163;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#36890;&#36807;&#20248;&#21270;&#23558;&#27493;&#39588;&#38480;&#21046;&#22312;&#20013;&#38388;&#25968;&#25454;&#27969;&#24418;&#20869;&#65292;&#24182;&#33021;&#22815;&#20351;&#29992;&#36739;&#22823;&#30340;&#24341;&#23548;&#27493;&#38271;&#12290;</title><link>https://arxiv.org/abs/2402.03201</link><description>&lt;p&gt;
&#29992;&#29699;&#38754;&#39640;&#26031;&#32422;&#26463;&#36827;&#34892;&#26465;&#20214;&#25193;&#25955;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Guidance with Spherical Gaussian Constraint for Conditional Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#29699;&#38754;&#39640;&#26031;&#32422;&#26463;&#30340;&#25193;&#25955;&#31639;&#27861;&#65288;DSG&#65289;&#65292;&#35299;&#20915;&#20102;&#22312;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#20013;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#27969;&#24418;&#20559;&#31163;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#36890;&#36807;&#20248;&#21270;&#23558;&#27493;&#39588;&#38480;&#21046;&#22312;&#20013;&#38388;&#25968;&#25454;&#27969;&#24418;&#20869;&#65292;&#24182;&#33021;&#22815;&#20351;&#29992;&#36739;&#22823;&#30340;&#24341;&#23548;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#23581;&#35797;&#36890;&#36807;&#21033;&#29992;&#21487;&#24494;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#25351;&#23548;&#26469;&#22788;&#29702;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#22312;&#26679;&#26412;&#36136;&#37327;&#19978;&#20570;&#20986;&#22949;&#21327;&#65292;&#24182;&#38656;&#35201;&#36739;&#23567;&#30340;&#24341;&#23548;&#27493;&#38271;&#65292;&#23548;&#33268;&#37319;&#26679;&#36807;&#31243;&#21464;&#38271;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#22312;&#24341;&#23548;&#25439;&#22833;&#30340;&#37319;&#26679;&#36807;&#31243;&#20013;&#27969;&#24418;&#20559;&#31163;&#30340;&#26681;&#26412;&#38382;&#39064;&#25152;&#22312;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#25439;&#22833;&#24341;&#23548;&#30340;&#20272;&#35745;&#35823;&#24046;&#30340;&#29305;&#23450;&#19979;&#30028;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#27969;&#24418;&#20559;&#31163;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#29699;&#38754;&#39640;&#26031;&#32422;&#26463;&#65288;DSG&#65289;&#30340;&#25193;&#25955;&#65292;&#20174;&#39640;&#32500;&#39640;&#26031;&#20998;&#24067;&#30340;&#38598;&#20013;&#29616;&#35937;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;DSG&#36890;&#36807;&#20248;&#21270;&#26377;&#25928;&#22320;&#23558;&#24341;&#23548;&#27493;&#39588;&#32422;&#26463;&#22312;&#20013;&#38388;&#25968;&#25454;&#27969;&#24418;&#20869;&#65292;&#24182;&#33021;&#22815;&#20351;&#29992;&#36739;&#22823;&#30340;&#24341;&#23548;&#27493;&#38271;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#24335;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in diffusion models attempt to handle conditional generative tasks by utilizing a differentiable loss function for guidance without the need for additional training. While these methods achieved certain success, they often compromise on sample quality and require small guidance step sizes, leading to longer sampling processes. This paper reveals that the fundamental issue lies in the manifold deviation during the sampling process when loss guidance is employed. We theoretically show the existence of manifold deviation by establishing a certain lower bound for the estimation error of the loss guidance. To mitigate this problem, we propose Diffusion with Spherical Gaussian constraint (DSG), drawing inspiration from the concentration phenomenon in high-dimensional Gaussian distributions. DSG effectively constrains the guidance step within the intermediate data manifold through optimization and enables the use of larger guidance steps. Furthermore, we present a closed-form 
&lt;/p&gt;</description></item><item><title>DexDiffuser&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#28789;&#24039;&#25235;&#21462;&#23039;&#21183;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29289;&#20307;&#28857;&#20113;&#30340;&#29983;&#25104;&#12289;&#35780;&#20272;&#21644;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#25235;&#21462;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02989</link><description>&lt;p&gt;
DexDiffuser: &#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#28789;&#24039;&#25235;&#21462;&#23039;&#21183;
&lt;/p&gt;
&lt;p&gt;
DexDiffuser: Generating Dexterous Grasps with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02989
&lt;/p&gt;
&lt;p&gt;
DexDiffuser&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#28789;&#24039;&#25235;&#21462;&#23039;&#21183;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29289;&#20307;&#28857;&#20113;&#30340;&#29983;&#25104;&#12289;&#35780;&#20272;&#21644;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#25235;&#21462;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;DexDiffuser&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#28789;&#24039;&#25235;&#21462;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#37096;&#20998;&#29289;&#20307;&#28857;&#20113;&#19978;&#29983;&#25104;&#12289;&#35780;&#20272;&#21644;&#20248;&#21270;&#25235;&#21462;&#23039;&#21183;&#12290;DexDiffuser&#21253;&#25324;&#26465;&#20214;&#25193;&#25955;&#22411;&#25235;&#21462;&#37319;&#26679;&#22120;DexSampler&#21644;&#28789;&#24039;&#25235;&#21462;&#35780;&#20272;&#22120;DexEvaluator&#12290;DexSampler&#36890;&#36807;&#23545;&#38543;&#26426;&#25235;&#21462;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#65292;&#29983;&#25104;&#19982;&#29289;&#20307;&#28857;&#20113;&#26465;&#20214;&#30456;&#20851;&#30340;&#39640;&#36136;&#37327;&#25235;&#21462;&#23039;&#21183;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#31181;&#25235;&#21462;&#20248;&#21270;&#31574;&#30053;&#65306;&#22522;&#20110;&#35780;&#20272;&#22120;&#30340;&#25193;&#25955;(Evaluator-Guided Diffusion&#65292;EGD)&#21644;&#22522;&#20110;&#35780;&#20272;&#22120;&#30340;&#37319;&#26679;&#20248;&#21270;(Evaluator-based Sampling Refinement&#65292;ESR)&#12290;&#25105;&#20204;&#22312;&#34394;&#25311;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;Allegro Hand&#36827;&#34892;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;DexDiffuser&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#22810;&#25351;&#25235;&#21462;&#29983;&#25104;&#26041;&#27861;FFHNet&#65292;&#24179;&#22343;&#25235;&#21462;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;21.71-22.20%&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce DexDiffuser, a novel dexterous grasping method that generates, evaluates, and refines grasps on partial object point clouds. DexDiffuser includes the conditional diffusion-based grasp sampler DexSampler and the dexterous grasp evaluator DexEvaluator. DexSampler generates high-quality grasps conditioned on object point clouds by iterative denoising of randomly sampled grasps. We also introduce two grasp refinement strategies: Evaluator-Guided Diffusion (EGD) and Evaluator-based Sampling Refinement (ESR). Our simulation and real-world experiments on the Allegro Hand consistently demonstrate that DexDiffuser outperforms the state-of-the-art multi-finger grasp generation method FFHNet with an, on average, 21.71--22.20\% higher grasp success rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#20272;&#35745;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#20013;&#21518;&#39564;&#27010;&#29575;&#21644;&#22122;&#22768;&#21442;&#25968;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26465;&#20214;&#26631;&#20934;&#21270;&#27969;&#26469;&#36817;&#20284;&#21518;&#39564;&#27010;&#29575;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#27979;&#37327;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.02964</link><description>&lt;p&gt;
&#28151;&#21512;&#22122;&#22768;&#19982;&#26465;&#20214;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#19979;&#30340;&#21518;&#39564;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Mixed Noise and Posterior Estimation with Conditional DeepGEM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#20272;&#35745;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#20013;&#21518;&#39564;&#27010;&#29575;&#21644;&#22122;&#22768;&#21442;&#25968;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26465;&#20214;&#26631;&#20934;&#21270;&#27969;&#26469;&#36817;&#20284;&#21518;&#39564;&#27010;&#29575;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#27979;&#37327;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#28151;&#21512;&#22122;&#22768;&#27169;&#22411;&#30340;&#38388;&#25509;&#27979;&#37327;&#21644;&#32435;&#31859;&#35745;&#37327;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#32852;&#21512;&#20272;&#35745;&#36125;&#21494;&#26031;&#36870;&#38382;&#39064;&#20013;&#30340;&#21518;&#39564;&#27010;&#29575;&#21644;&#22122;&#22768;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#24403;&#21069;&#30340;&#22122;&#22768;&#21442;&#25968;&#65292;&#25105;&#20204;&#22312;E&#27493;&#20013;&#23398;&#20064;&#20102;&#19968;&#20010;&#26465;&#20214;&#26631;&#20934;&#21270;&#27969;&#65292;&#20197;&#36817;&#20284;&#21518;&#39564;&#27010;&#29575;&#12290;&#22312;M&#27493;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20877;&#27425;&#36890;&#36807;EM&#31639;&#27861;&#25214;&#21040;&#22122;&#22768;&#21442;&#25968;&#30340;&#26356;&#26032;&#65292;&#20854;&#20855;&#26377;&#35299;&#26512;&#20844;&#24335;&#12290;&#25105;&#20204;&#23558;&#26465;&#20214;&#26631;&#20934;&#21270;&#27969;&#30340;&#35757;&#32451;&#19982;&#21069;&#21521;&#21644;&#21453;&#21521;KL&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#26469;&#33258;&#35768;&#22810;&#27979;&#37327;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#20687;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by indirect measurements and applications from nanometrology with a mixed noise model, we develop a novel algorithm for jointly estimating the posterior and the noise parameters in Bayesian inverse problems. We propose to solve the problem by an expectation maximization (EM) algorithm. Based on the current noise parameters, we learn in the E-step a conditional normalizing flow that approximates the posterior. In the M-step, we propose to find the noise parameter updates again by an EM algorithm, which has analytical formulas. We compare the training of the conditional normalizing flow with the forward and reverse KL, and show that our model is able to incorporate information from many measurements, unlike previous approaches.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;FPGA&#19978;&#36827;&#34892;&#39640;&#24615;&#33021;&#21943;&#27880;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#24863;&#30693;&#30340;&#35757;&#32451;&#21644;&#39640;&#25928;&#30340;&#30828;&#20214;&#23454;&#29616;&#65292;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#20302;&#36164;&#28304;&#28040;&#32791;&#65292;&#20026;&#39640;&#20142;&#24230;&#38454;&#27573;&#26631;&#35760;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#21021;&#22987;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.01876</link><description>&lt;p&gt;
&#38598;&#21512;&#23601;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#65306;&#29992;&#20110;HL-LHC&#30340;&#36229;&#24555;&#36895;&#21943;&#27880;&#20998;&#31867;&#22312;FPGAs&#19978;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Sets are all you need: Ultrafast jet classification on FPGAs for HL-LHC
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;FPGA&#19978;&#36827;&#34892;&#39640;&#24615;&#33021;&#21943;&#27880;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#24863;&#30693;&#30340;&#35757;&#32451;&#21644;&#39640;&#25928;&#30340;&#30828;&#20214;&#23454;&#29616;&#65292;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#20302;&#36164;&#28304;&#28040;&#32791;&#65292;&#20026;&#39640;&#20142;&#24230;&#38454;&#27573;&#26631;&#35760;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#21021;&#22987;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#21487;&#32534;&#31243;&#36923;&#36753;&#38376;&#38453;&#21015;&#19978;&#36827;&#34892;&#20934;&#30830;&#30340;&#21943;&#27880;&#39118;&#21619;&#20998;&#31867;&#65292;&#24182;&#23637;&#31034;&#20102;&#24310;&#36831;&#21644;&#36164;&#28304;&#28040;&#32791;&#22914;&#20309;&#38543;&#30528;&#36755;&#20837;&#22823;&#23567;&#21644;&#31639;&#27861;&#36873;&#25321;&#32780;&#21464;&#21270;&#12290;&#36825;&#20123;&#26550;&#26500;&#20026;&#22312;CERN LHC&#30340;&#39640;&#20142;&#24230;&#38454;&#27573;&#26631;&#35760;&#25152;&#33021;&#20351;&#29992;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#21021;&#22987;&#35774;&#35745;&#12290;&#39640;&#20142;&#24230;&#21319;&#32423;&#23558;&#23548;&#33268;&#36136;&#23376;-&#36136;&#23376;&#30896;&#25758;&#30340;&#30636;&#26102;&#20142;&#24230;&#22686;&#21152;&#20116;&#20493;&#65292;&#36827;&#32780;&#20135;&#29983;&#26356;&#39640;&#30340;&#25968;&#25454;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#20363;&#22914;&#21943;&#27880;&#32452;&#25104;&#37096;&#20998;&#30340;&#21487;&#29992;&#24615;&#12290;&#36890;&#36807;&#37327;&#21270;&#24863;&#30693;&#30340;&#35757;&#32451;&#21644;&#39640;&#25928;&#30340;&#30828;&#20214;&#23454;&#29616;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22797;&#26434;&#26550;&#26500;&#65288;&#22914;&#28145;&#24230;&#38598;&#21512;&#21644;&#20132;&#20114;&#32593;&#32476;&#65289;&#30340;&#25512;&#26029;&#26102;&#38388;&#21487;&#20197;&#36798;&#21040;O&#65288;100&#65289;ns&#65292;&#24182;&#19988;&#35745;&#31639;&#36164;&#28304;&#25104;&#26412;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study various machine learning based algorithms for performing accurate jet flavor classification on field-programmable gate arrays and demonstrate how latency and resource consumption scale with the input size and choice of algorithm. These architectures provide an initial design for models that could be used for tagging at the CERN LHC during its high-luminosity phase. The high-luminosity upgrade will lead to a five-fold increase in its instantaneous luminosity for proton-proton collisions and, in turn, higher data volume and complexity, such as the availability of jet constituents. Through quantization-aware training and efficient hardware implementations, we show that O(100) ns inference of complex architectures such as deep sets and interaction networks is feasible at a low computational resource cost.
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.01695</link><description>&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65306;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Language-Guided World Models: A Model-Based Approach to AI Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01695
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27010;&#29575;&#19990;&#30028;&#27169;&#22411;&#23433;&#35013;&#21040;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20013;&#65292;&#20026;&#20154;&#31867;&#19982;&#36825;&#20123;&#20195;&#29702;&#27807;&#36890;&#21644;&#25511;&#21046;&#25171;&#24320;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#28192;&#36947;&#12290;&#38500;&#20102;&#26356;&#26032;&#20195;&#29702;&#31574;&#30053;&#65292;&#20154;&#31867;&#36824;&#21487;&#20197;&#20462;&#25913;&#20182;&#20204;&#30340;&#20869;&#37096;&#19990;&#30028;&#27169;&#22411;&#65292;&#20197;&#24433;&#21709;&#20195;&#29702;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#29616;&#26377;&#30340;&#19990;&#30028;&#27169;&#22411;&#38590;&#20197;&#36866;&#24212;&#20154;&#31867;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#33258;&#28982;&#30340;&#36890;&#20449;&#30028;&#38754;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#12290;&#36825;&#20123;&#27169;&#22411;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;&#23427;&#20204;&#36824;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#26368;&#21021;&#29992;&#20110;&#25351;&#23548;&#20154;&#31867;&#30340;&#25991;&#26412;&#20013;&#36827;&#34892;&#33258;&#25105;&#23398;&#20064;&#12290;&#20026;&#20102;&#20419;&#36827;LWMs&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;MESSENGER&#28216;&#25103;&#65288;Hanjie&#31561;&#20154;&#65292;2021&#65289;&#30340;&#25361;&#25112;&#22522;&#20934;&#65292;&#38656;&#35201;&#23545;&#26032;&#22330;&#26223;&#36827;&#34892;&#32452;&#21512;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Installing probabilistic world models into artificial agents opens an efficient channel for humans to communicate with and control these agents. In addition to updating agent policies, humans can modify their internal world models in order to influence their decisions. The challenge, however, is that currently existing world models are difficult for humans to adapt because they lack a natural communication interface. Aimed at addressing this shortcoming, we develop Language-Guided World Models (LWMs), which can capture environment dynamics by reading language descriptions. These models enhance agent communication efficiency, allowing humans to simultaneously alter their behavior on multiple tasks with concise language feedback. They also enable agents to self-learn from texts originally written to instruct humans. To facilitate the development of LWMs, we design a challenging benchmark based on the game of MESSENGER (Hanjie et al., 2021), requiring compositional generalization to new l
&lt;/p&gt;</description></item><item><title>KVQuant&#26159;&#19968;&#31181;&#35299;&#20915;LLM&#25512;&#29702;&#20013;&#22823;&#37327;&#20869;&#23384;&#28040;&#32791;&#30340;KV&#32531;&#23384;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#36890;&#36947;&#38190;&#37327;&#21270;&#12289;RoPE&#21069;&#37327;&#21270;&#38190;&#21644;&#38750;&#22343;&#21248;KV&#32531;&#23384;&#37327;&#21270;&#65292;&#20934;&#30830;&#34920;&#31034;&#36229;&#20302;&#31934;&#24230;&#30340;KV&#28608;&#27963;&#12290;</title><link>https://arxiv.org/abs/2401.18079</link><description>&lt;p&gt;
KVQuant: &#20197;KV&#32531;&#23384;&#37327;&#21270;&#23454;&#29616;1000&#19975;&#19978;&#19979;&#25991;&#38271;&#24230;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18079
&lt;/p&gt;
&lt;p&gt;
KVQuant&#26159;&#19968;&#31181;&#35299;&#20915;LLM&#25512;&#29702;&#20013;&#22823;&#37327;&#20869;&#23384;&#28040;&#32791;&#30340;KV&#32531;&#23384;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#36890;&#36947;&#38190;&#37327;&#21270;&#12289;RoPE&#21069;&#37327;&#21270;&#38190;&#21644;&#38750;&#22343;&#21248;KV&#32531;&#23384;&#37327;&#21270;&#65292;&#20934;&#30830;&#34920;&#31034;&#36229;&#20302;&#31934;&#24230;&#30340;KV&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#25991;&#26723;&#20998;&#26512;&#21644;&#25688;&#35201;&#31561;&#38656;&#35201;&#22823;&#31383;&#21475;&#19978;&#19979;&#25991;&#30340;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;KV&#32531;&#23384;&#28608;&#27963;&#25104;&#20026;&#35760;&#24518;&#28040;&#32791;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#12290;&#37327;&#21270;&#26159;&#19968;&#31181;&#21387;&#32553;KV&#32531;&#23384;&#28608;&#27963;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#36229;&#20302;&#31934;&#24230;&#65288;&#22914;&#20302;&#20110;4&#20301;&#65289;&#30340;&#28608;&#27963;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;KVQuant&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#26041;&#27861;&#37327;&#21270;&#32531;&#23384;&#30340;KV&#28608;&#27963;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#65306;(i)&#20998;&#36890;&#36947;&#38190;&#37327;&#21270;&#65292;&#22312;&#37327;&#21270;&#38190;&#28608;&#27963;&#26102;&#35843;&#25972;&#32500;&#24230;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20998;&#24067;&#65307;(ii)RoPE&#21069;&#37327;&#21270;&#38190;&#65292;&#22312;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#20043;&#21069;&#37327;&#21270;&#38190;&#28608;&#27963;&#20197;&#20943;&#36731;&#20854;&#23545;&#37327;&#21270;&#30340;&#24433;&#21709;&#65307;(iii)&#38750;&#22343;&#21248;KV&#32531;&#23384;&#37327;&#21270;&#65292;&#22312;&#27599;&#23618;&#25512;&#23548;&#20986;&#26435;&#37325;&#24863;&#30693;&#30340;&#38750;&#22343;&#21248;&#25968;&#25454;&#31867;&#22411;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#19981;&#21516;&#23618;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in ultra-low precisions, such as sub-4-bit. In this work, we present KVQuant, which addresses this problem by incorporating novel methods for quantizing cached KV activations, including: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;Contrastive Activation Addition&#65288;CAA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#28608;&#27963;&#26469;&#31934;&#30830;&#25511;&#21046;&#30446;&#26631;&#34892;&#20026;&#30340;&#31243;&#24230;&#65292;&#26174;&#33879;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#24182;&#22312;&#24494;&#35843;&#21644;&#31995;&#32479;&#25552;&#31034;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#25552;&#20379;&#39069;&#22806;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.06681</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#28608;&#27963;&#21152;&#27861;&#25351;&#23548;Llama 2
&lt;/p&gt;
&lt;p&gt;
Steering Llama 2 via Contrastive Activation Addition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06681
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Contrastive Activation Addition&#65288;CAA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#28608;&#27963;&#26469;&#31934;&#30830;&#25511;&#21046;&#30446;&#26631;&#34892;&#20026;&#30340;&#31243;&#24230;&#65292;&#26174;&#33879;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#24182;&#22312;&#24494;&#35843;&#21644;&#31995;&#32479;&#25552;&#31034;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#25552;&#20379;&#39069;&#22806;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;Contrastive Activation Addition&#65288;CAA&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;&#22312;&#21069;&#21521;&#20256;&#36882;&#36807;&#31243;&#20013;&#20462;&#25913;&#20854;&#28608;&#27963;&#26469;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#12290;CAA&#36890;&#36807;&#23545;&#26576;&#31181;&#34892;&#20026;&#30340;&#27491;&#38754;&#21644;&#36127;&#38754;&#31034;&#20363;&#20043;&#38388;&#27531;&#24046;&#27969;&#28608;&#27963;&#30340;&#24046;&#24322;&#27714;&#24179;&#22343;&#65292;&#35745;&#31639;&#20986;&#8220;&#25351;&#23548;&#21521;&#37327;&#8221;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#22312;&#29992;&#25143;&#25552;&#31034;&#21518;&#30340;&#25152;&#26377;token&#20301;&#32622;&#19978;&#20197;&#27491;&#36127;&#31995;&#25968;&#28155;&#21152;&#36825;&#20123;&#25351;&#23548;&#21521;&#37327;&#65292;&#20174;&#32780;&#31934;&#30830;&#25511;&#21046;&#30446;&#26631;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#36873;&#25321;&#34892;&#20026;&#38382;&#39064;&#25968;&#25454;&#38598;&#21644;&#24320;&#25918;&#24335;&#29983;&#25104;&#20219;&#21153;&#22312;Llama 2 Chat&#19978;&#35780;&#20272;&#20102;CAA&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;CAA&#26174;&#30528;&#25913;&#21464;&#20102;&#27169;&#22411;&#34892;&#20026;&#65292;&#19981;&#20165;&#22312;&#20256;&#32479;&#26041;&#27861;&#22914;&#24494;&#35843;&#21644;&#31995;&#32479;&#25552;&#31034;&#35774;&#35745;&#30340;&#22522;&#30784;&#19978;&#26377;&#25928;&#65292;&#32780;&#19988;&#26368;&#23567;&#31243;&#24230;&#22320;&#38477;&#20302;&#20102;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#34892;&#20026;&#20570;&#20986;&#20102;&#26356;&#28145;&#20837;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06681v3 Announce Type: replace-cross  Abstract: We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes "steering vectors" by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights in
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MT-HCCAR&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20113;&#23646;&#24615;&#26816;&#32034;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#20113;&#23646;&#24615;&#26816;&#32034;&#20219;&#21153;&#20043;&#38388;&#30340;&#23618;&#32423;&#20851;&#31995;&#65292;&#24182;&#20855;&#26377;&#23545;&#19981;&#21516;&#20256;&#24863;&#22120;&#25968;&#25454;&#38598;&#20855;&#26377;&#20581;&#22766;&#27867;&#21270;&#33021;&#21147;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.16520</link><description>&lt;p&gt;
MT-HCCAR: &#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#19982;&#23618;&#32423;&#20998;&#31867;&#30340;&#27880;&#24847;&#21147;&#22238;&#24402;&#29992;&#20110;&#20113;&#23646;&#24615;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
MT-HCCAR: Multi-Task Deep Learning with Hierarchical Classification and Attention-based Regression for Cloud Property Retrieval. (arXiv:2401.16520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16520
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MT-HCCAR&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20113;&#23646;&#24615;&#26816;&#32034;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#20113;&#23646;&#24615;&#26816;&#32034;&#20219;&#21153;&#20043;&#38388;&#30340;&#23618;&#32423;&#20851;&#31995;&#65292;&#24182;&#20855;&#26377;&#23545;&#19981;&#21516;&#20256;&#24863;&#22120;&#25968;&#25454;&#38598;&#20855;&#26377;&#20581;&#22766;&#27867;&#21270;&#33021;&#21147;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#26377;&#25928;&#30340;&#20113;&#23646;&#24615;&#26816;&#32034;&#21253;&#25324;&#20113;&#36974;&#34109;&#12289;&#20113;&#30456;&#20998;&#31867;&#21644;&#20113;&#20809;&#23398;&#21402;&#24230;&#65288;COT&#65289;&#39044;&#27979;&#20173;&#28982;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#20256;&#24863;&#22120;&#20202;&#22120;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#29420;&#29305;&#30340;&#20809;&#35889;&#29305;&#24449;&#12290;&#26368;&#36817;&#65292;&#22312;&#22320;&#29699;&#31185;&#23398;&#30740;&#31350;&#20013;&#37319;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#21355;&#26143;&#25968;&#25454;&#38598;&#30340;&#20809;&#35889;&#35266;&#27979;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#32771;&#34385;&#26816;&#32034;&#20219;&#21153;&#20043;&#38388;&#23618;&#32423;&#20851;&#31995;&#30340;&#21019;&#26032;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#29616;&#26377;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#20809;&#35889;&#22810;&#26679;&#24615;&#65292;&#24320;&#21457;&#20855;&#26377;&#23545;&#19981;&#21516;&#20256;&#24863;&#22120;&#25968;&#25454;&#38598;&#20855;&#26377;&#20581;&#22766;&#27867;&#21270;&#33021;&#21147;&#30340;&#27169;&#22411;&#26159;&#24517;&#35201;&#30340;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#30446;&#21069;&#32570;&#20047;&#35299;&#20915;&#22810;&#26679;&#25968;&#25454;&#38598;&#19979;&#36873;&#25321;&#26368;&#20248;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;MT-HCCAR&#65292;&#36825;&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of Earth science, effective cloud property retrieval, encompassing cloud masking, cloud phase classification, and cloud optical thickness (COT) prediction, remains pivotal. Traditional methodologies necessitate distinct models for each sensor instrument due to their unique spectral characteristics. Recent strides in Earth Science research have embraced machine learning and deep learning techniques to extract features from satellite datasets' spectral observations. However, prevailing approaches lack novel architectures accounting for hierarchical relationships among retrieval tasks. Moreover, considering the spectral diversity among existing sensors, the development of models with robust generalization capabilities over different sensor datasets is imperative. Surprisingly, there is a dearth of methodologies addressing the selection of an optimal model for diverse datasets. In response, this paper introduces MT-HCCAR, an end-to-end deep learning model employing multi-task 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;EAUC&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20197;&#25581;&#31034;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;RMSE&#21644;MAE&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#31181;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10690</link><description>&lt;p&gt;
&#36229;&#36234;RMSE&#21644;MAE&#65306;&#24341;&#20837;EAUC&#26469;&#25581;&#31034;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#30340;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models. (arXiv:2401.10690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;EAUC&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20197;&#25581;&#31034;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;RMSE&#21644;MAE&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#19968;&#23545;&#23454;&#20307;&#30340;&#23454;&#20540;&#32467;&#26524;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#26159;&#22522;&#30784;&#30340;&#65288;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#39044;&#27979;&#29992;&#25143;&#23545;&#20135;&#21697;&#30340;&#35780;&#20998;&#65289;&#65292;&#22312;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#20013;&#20063;&#26377;&#35768;&#22810;&#28508;&#21147;&#20294;&#23578;&#26410;&#28145;&#20837;&#25506;&#32034;&#65288;&#20363;&#22914;&#65292;&#22312;&#20010;&#24615;&#21270;&#33647;&#29702;&#23398;&#20013;&#36817;&#20284;&#30830;&#23450;&#24739;&#32773;&#30340;&#36866;&#24403;&#21058;&#37327;&#65289;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20010;&#20307;&#23454;&#20307;&#35266;&#23519;&#20540;&#20998;&#24067;&#30340;&#38750;&#22343;&#21248;&#24615;&#23548;&#33268;&#20102;&#26368;&#20808;&#36827;&#27169;&#22411;&#20013;&#30340;&#20005;&#37325;&#20559;&#35265;&#39044;&#27979;&#65292;&#20559;&#21521;&#20110;&#23454;&#20307;&#30340;&#35266;&#23519;&#36807;&#21435;&#20540;&#30340;&#24179;&#22343;&#20540;&#65292;&#24182;&#22312;&#21478;&#31867;&#20294;&#21516;&#26679;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#27604;&#38543;&#26426;&#39044;&#27979;&#26356;&#24046;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#19981;&#36275;&#20197;&#25429;&#25417;&#21040;&#36825;&#31181;&#29616;&#35937;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#21478;&#31867;&#20559;&#35265;&#65292;&#24182;&#24341;&#20837;&#21478;&#31867;-&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;EAUC&#65289;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#34917;&#20805;&#24230;&#37327;&#65292;&#21487;&#20197;&#22312;&#25152;&#26377;&#30740;&#31350;&#30340;&#27169;&#22411;&#20013;&#37327;&#21270;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dyadic regression models, which predict real-valued outcomes for pairs of entities, are fundamental in many domains (e.g. predicting the rating of a user to a product in Recommender Systems) and promising and under exploration in many others (e.g. approximating the adequate dosage of a drug for a patient in personalized pharmacology). In this work, we demonstrate that non-uniformity in the observed value distributions of individual entities leads to severely biased predictions in state-of-the-art models, skewing predictions towards the average of observed past values for the entity and providing worse-than-random predictive power in eccentric yet equally important cases. We show that the usage of global error metrics like Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) is insufficient to capture this phenomenon, which we name eccentricity bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as a new complementary metric that can quantify it in all studied models
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;ChatGPT&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#23545;&#25512;&#33616;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;RecLLMs&#20013;&#24341;&#20837;&#29305;&#23450;&#30340;&#31995;&#32479;&#35282;&#33394;&#21644;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#22686;&#24378;&#25512;&#33616;&#30340;&#20844;&#24179;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;GPT-based&#27169;&#22411;&#20542;&#21521;&#20110;&#25512;&#33616;&#26368;&#26032;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#30005;&#24433;&#27969;&#27966;&#12290;</title><link>http://arxiv.org/abs/2401.10545</link><description>&lt;p&gt;
&#29702;&#35299;ChatGPT&#22522;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#65306;&#20379;&#24212;&#21830;&#20844;&#24179;&#24615;&#12289;&#26102;&#38388;&#31283;&#23450;&#24615;&#21644;&#26368;&#26032;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency. (arXiv:2401.10545v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10545
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;ChatGPT&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#23545;&#25512;&#33616;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;RecLLMs&#20013;&#24341;&#20837;&#29305;&#23450;&#30340;&#31995;&#32479;&#35282;&#33394;&#21644;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#22686;&#24378;&#25512;&#33616;&#30340;&#20844;&#24179;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;GPT-based&#27169;&#22411;&#20542;&#21521;&#20110;&#25512;&#33616;&#26368;&#26032;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#30005;&#24433;&#27969;&#27966;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;RecLLMs&#65289;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#32454;&#24494;&#33021;&#21147;&#21644;&#22266;&#26377;&#20559;&#35265;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#22522;&#20110;ChatGPT&#30340;&#31995;&#32479;&#12290;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#21644;&#20256;&#32479;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#22312;&#30005;&#24433;&#25512;&#33616;&#20013;&#30340;&#24046;&#24322;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#20027;&#35201;&#35843;&#26597;&#20102;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#21450;&#20854;&#23545;&#25512;&#33616;&#36136;&#37327;&#30340;&#21508;&#20010;&#26041;&#38754;&#65288;&#21253;&#25324;&#20934;&#30830;&#24615;&#12289;&#20379;&#24212;&#21830;&#20844;&#24179;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#31283;&#23450;&#24615;&#12289;&#27969;&#34892;&#31867;&#22411;&#21644;&#26102;&#25928;&#24615;&#65289;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;RecLLMs&#20013;&#24341;&#20837;&#29305;&#23450;&#30340;&#8220;&#31995;&#32479;&#35282;&#33394;&#8221;&#21644;&#8220;&#25552;&#31034;&#31574;&#30053;&#8221;&#26174;&#33879;&#24433;&#21709;&#20854;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#22522;&#20110;&#35282;&#33394;&#30340;&#25552;&#31034;&#21487;&#20197;&#22686;&#24378;&#25512;&#33616;&#30340;&#20844;&#24179;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#20943;&#36731;&#27969;&#34892;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#22522;&#20110;GPT&#30340;&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#33021;&#19982;&#20256;&#32479;&#21327;&#21516;&#36807;&#28388;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#21305;&#37197;&#65292;&#20294;&#23427;&#20204;&#20542;&#21521;&#20110;&#25512;&#33616;&#26356;&#26032;&#12289;&#26356;&#22810;&#26679;&#21270;&#30340;&#30005;&#24433;&#27969;&#27966;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GPT-base
&lt;/p&gt;
&lt;p&gt;
This study explores the nuanced capabilities and inherent biases of Recommender Systems using Large Language Models (RecLLMs), with a focus on ChatGPT-based systems. It studies into the contrasting behaviors of generative models and traditional collaborative filtering models in movie recommendations. The research primarily investigates prompt design strategies and their impact on various aspects of recommendation quality, including accuracy, provider fairness, diversity, stability, genre dominance, and temporal freshness (recency).  Our experimental analysis reveals that the introduction of specific 'system roles' and 'prompt strategies' in RecLLMs significantly influences their performance. For instance, role-based prompts enhance fairness and diversity in recommendations, mitigating popularity bias. We find that while GPT-based models do not always match the performance of CF baselines, they exhibit a unique tendency to recommend newer and more diverse movie genres. Notably, GPT-base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#65288;LAACA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#33402;&#26415;&#21697;&#20813;&#21463;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;NST&#65289;&#30340;&#28389;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#19981;&#21487;&#23519;&#35273;&#30340;&#24773;&#20917;&#19979;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#65292;&#20135;&#29983;&#23545;NST&#20855;&#26377;&#24178;&#25200;&#20316;&#29992;&#30340;&#25200;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.09673</link><description>&lt;p&gt;
&#20351;&#29992;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#23545;&#33402;&#26415;&#21697;&#36827;&#34892;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#30340;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Artwork Protection Against Neural Style Transfer Using Locally Adaptive Adversarial Color Attack. (arXiv:2401.09673v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#65288;LAACA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#33402;&#26415;&#21697;&#20813;&#21463;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;NST&#65289;&#30340;&#28389;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#19981;&#21487;&#23519;&#35273;&#30340;&#24773;&#20917;&#19979;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#65292;&#20135;&#29983;&#23545;NST&#20855;&#26377;&#24178;&#25200;&#20316;&#29992;&#30340;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;NST&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#20219;&#24847;&#39118;&#26684;&#30340;&#26032;&#22270;&#20687;&#12290;&#36825;&#20010;&#36807;&#31243;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#39118;&#26684;&#22270;&#20687;&#30340;&#32654;&#23398;&#20803;&#32032;&#19982;&#20869;&#23481;&#22270;&#20687;&#30340;&#32467;&#26500;&#22240;&#32032;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#24418;&#25104;&#19968;&#20010;&#21644;&#35856;&#25972;&#21512;&#30340;&#35270;&#35273;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26410;&#32463;&#25480;&#26435;&#30340;NST&#21487;&#33021;&#20250;&#28389;&#29992;&#33402;&#26415;&#21697;&#12290;&#36825;&#31181;&#28389;&#29992;&#24341;&#36215;&#20102;&#20851;&#20110;&#33402;&#26415;&#23478;&#26435;&#21033;&#30340;&#31038;&#20250;&#25216;&#26415;&#38382;&#39064;&#65292;&#24182;&#20419;&#20351;&#24320;&#21457;&#25216;&#26415;&#26041;&#27861;&#26469;&#31215;&#26497;&#20445;&#25252;&#21407;&#22987;&#21019;&#20316;&#12290;&#23545;&#25239;&#24615;&#25915;&#20987;&#20027;&#35201;&#22312;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#20013;&#36827;&#34892;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#36825;&#19968;&#25216;&#26415;&#24341;&#20837;&#21040;&#20445;&#25252;&#33402;&#26415;&#23478;&#30693;&#35782;&#20135;&#26435;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#65288;LAACA&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20197;&#23545;&#20154;&#30524;&#19981;&#21487;&#23519;&#35273;&#20294;&#23545;NST&#20135;&#29983;&#24178;&#25200;&#30340;&#26041;&#24335;&#20462;&#25913;&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#38024;&#23545;&#39640;&#39057;&#20869;&#23481;&#20016;&#23500;&#21306;&#22495;&#30340;&#25200;&#21160;&#65292;&#36825;&#20123;&#25200;&#21160;&#30001;&#20013;&#38388;&#29305;&#24449;&#30340;&#30772;&#22351;&#20135;&#29983;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#21644;&#29992;&#25143;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural style transfer (NST) is widely adopted in computer vision to generate new images with arbitrary styles. This process leverages neural networks to merge aesthetic elements of a style image with the structural aspects of a content image into a harmoniously integrated visual result. However, unauthorized NST can exploit artwork. Such misuse raises socio-technical concerns regarding artists' rights and motivates the development of technical approaches for the proactive protection of original creations. Adversarial attack is a concept primarily explored in machine learning security. Our work introduces this technique to protect artists' intellectual property. In this paper Locally Adaptive Adversarial Color Attack (LAACA), a method for altering images in a manner imperceptible to the human eyes but disruptive to NST. Specifically, we design perturbations targeting image areas rich in high-frequency content, generated by disrupting intermediate features. Our experiments and user study
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#32479;&#35745;&#29702;&#35770;&#65292;&#21253;&#25324;&#36817;&#20284;&#26041;&#27861;&#12289;&#35757;&#32451;&#21160;&#24577;&#21644;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#38750;&#21442;&#25968;&#26694;&#26550;&#20013;&#65292;&#32467;&#26524;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#39118;&#38505;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#29575;&#65292;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#26799;&#24230;&#26041;&#27861;&#35757;&#32451;&#32593;&#32476;&#20197;&#25214;&#21040;&#33391;&#22909;&#30340;&#27867;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.07187</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#32479;&#35745;&#29702;&#35770;&#32508;&#36848;&#65306;&#36817;&#20284;&#65292;&#35757;&#32451;&#21160;&#24577;&#21644;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Survey on Statistical Theory of Deep Learning: Approximation, Training Dynamics, and Generative Models. (arXiv:2401.07187v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#32479;&#35745;&#29702;&#35770;&#65292;&#21253;&#25324;&#36817;&#20284;&#26041;&#27861;&#12289;&#35757;&#32451;&#21160;&#24577;&#21644;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#38750;&#21442;&#25968;&#26694;&#26550;&#20013;&#65292;&#32467;&#26524;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#39118;&#38505;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#29575;&#65292;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#26799;&#24230;&#26041;&#27861;&#35757;&#32451;&#32593;&#32476;&#20197;&#25214;&#21040;&#33391;&#22909;&#30340;&#27867;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20174;&#19977;&#20010;&#35282;&#24230;&#22238;&#39038;&#20102;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#32479;&#35745;&#29702;&#35770;&#30340;&#25991;&#29486;&#12290;&#31532;&#19968;&#37096;&#20998;&#22238;&#39038;&#20102;&#22312;&#22238;&#24402;&#25110;&#20998;&#31867;&#30340;&#38750;&#21442;&#25968;&#26694;&#26550;&#19979;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#39118;&#38505;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#32467;&#26524;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26174;&#24335;&#26500;&#36896;&#65292;&#20197;&#21450;&#37319;&#29992;&#20102;&#36817;&#20284;&#29702;&#35770;&#30340;&#24037;&#20855;&#65292;&#23548;&#33268;&#36807;&#24230;&#39118;&#38505;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#29575;&#12290;&#36890;&#36807;&#36825;&#20123;&#26500;&#36896;&#65292;&#21487;&#20197;&#29992;&#26679;&#26412;&#22823;&#23567;&#12289;&#25968;&#25454;&#32500;&#24230;&#21644;&#20989;&#25968;&#24179;&#28369;&#24615;&#26469;&#34920;&#36798;&#32593;&#32476;&#30340;&#23485;&#24230;&#21644;&#28145;&#24230;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#22522;&#26412;&#20998;&#26512;&#20165;&#36866;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39640;&#24230;&#38750;&#20984;&#30340;&#20840;&#23616;&#26497;&#23567;&#20540;&#28857;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#22312;&#31532;&#20108;&#37096;&#20998;&#22238;&#39038;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#37027;&#20123;&#35797;&#22270;&#22238;&#31572;&#8220;&#22522;&#20110;&#26799;&#24230;&#26041;&#27861;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#25214;&#21040;&#33021;&#22815;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#26377;&#33391;&#22909;&#27867;&#21270;&#24615;&#33021;&#30340;&#35299;&#8221;&#30340;&#35770;&#25991;&#12290;&#23588;&#20854;&#26159;&#20004;&#20010;&#30693;&#21517;&#30340;
&lt;/p&gt;
&lt;p&gt;
In this article, we review the literature on statistical theories of neural networks from three perspectives. In the first part, results on excess risks for neural networks are reviewed in the nonparametric framework of regression or classification. These results rely on explicit constructions of neural networks, leading to fast convergence rates of excess risks, in that tools from the approximation theory are adopted. Through these constructions, the width and depth of the networks can be expressed in terms of sample size, data dimension, and function smoothness. Nonetheless, their underlying analysis only applies to the global minimizer in the highly non-convex landscape of deep neural networks. This motivates us to review the training dynamics of neural networks in the second part. Specifically, we review papers that attempt to answer ``how the neural network trained via gradient-based methods finds the solution that can generalize well on unseen data.'' In particular, two well-know
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#24863;&#30693;&#22810;&#22336;&#35775;&#38382;&#26041;&#26696;&#65292;&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#21033;&#29992;&#19982;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#65292;&#24182;&#32771;&#34385;&#29992;&#25143;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#28385;&#36275;&#26410;&#26469;6G&#24212;&#29992;&#30340;&#35201;&#27714;&#21644;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06308</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20998;&#24067;&#24335;&#12289;&#21160;&#24577;6G&#24212;&#29992;&#30340;&#35821;&#20041;&#24863;&#30693;&#22810;&#22336;&#35775;&#38382;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Semantic-Aware Multiple Access Scheme for Distributed, Dynamic 6G-Based Applications. (arXiv:2401.06308v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#24863;&#30693;&#22810;&#22336;&#35775;&#38382;&#26041;&#26696;&#65292;&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#21033;&#29992;&#19982;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#65292;&#24182;&#32771;&#34385;&#29992;&#25143;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#28385;&#36275;&#26410;&#26469;6G&#24212;&#29992;&#30340;&#35201;&#27714;&#21644;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#24863;&#30693;&#33539;&#24335;&#30340;&#20986;&#29616;&#20026;&#21019;&#26032;&#30340;&#26381;&#21153;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#23588;&#20854;&#26159;&#22312;&#22522;&#20110;6G&#30340;&#24212;&#29992;&#29615;&#22659;&#20013;&#12290;&#23613;&#31649;&#22312;&#35821;&#20041;&#25552;&#21462;&#25216;&#26415;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23558;&#35821;&#20041;&#20449;&#24687;&#32435;&#20837;&#36164;&#28304;&#20998;&#37197;&#20915;&#31574;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#32570;&#20047;&#23545;&#26410;&#26469;&#31995;&#32479;&#38656;&#27714;&#21644;&#29305;&#24615;&#30340;&#32771;&#34385;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#32447;&#39057;&#35889;&#22810;&#22336;&#35775;&#38382;&#38382;&#39064;&#30340;&#24314;&#27169;&#12290;&#23427;&#26088;&#22312;&#20248;&#21270;&#21033;&#29992;&#29575;&#19982;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#65292;&#20351;&#29992;&#945;-&#20844;&#24179;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#33258;&#21161;&#21534;&#21520;&#37327;&#21644;&#21327;&#21161;&#21534;&#21520;&#37327;&#30340;&#27010;&#24565;&#26469;&#32771;&#34385;&#29992;&#25143;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#12290;&#39318;&#20808;&#65292;&#20998;&#26512;&#20102;&#35813;&#38382;&#39064;&#65292;&#25214;&#20986;&#20102;&#26368;&#20248;&#35299;&#12290;&#25509;&#19979;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;&#22810;&#20027;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#35821;&#20041;&#24863;&#30693;&#22810;&#26234;&#33021;&#20307;&#21452;&#37325;&#21644;&#20915;&#26007;&#28145;&#24230;Q&#23398;&#20064; (SAMA-D3QL) &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of the semantic-aware paradigm presents opportunities for innovative services, especially in the context of 6G-based applications. Although significant progress has been made in semantic extraction techniques, the incorporation of semantic information into resource allocation decision-making is still in its early stages, lacking consideration of the requirements and characteristics of future systems. In response, this paper introduces a novel formulation for the problem of multiple access to the wireless spectrum. It aims to optimize the utilization-fairness trade-off, using the $\alpha$-fairness metric, while accounting for user data correlation by introducing the concepts of self- and assisted throughputs. Initially, the problem is analyzed to identify its optimal solution. Subsequently, a Semantic-Aware Multi-Agent Double and Dueling Deep Q-Learning (SAMA-D3QL) technique is proposed. This method is grounded in Model-free Multi-Agent Deep Reinforcement Learning (MADRL),
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Gromov-Monge&#23884;&#20837;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#35782;&#21035;&#25968;&#25454;&#32972;&#21518;&#30340;&#24213;&#23618;&#32467;&#26500;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#20013;&#23545;&#21021;&#22987;&#26465;&#20214;&#25935;&#24863;&#24615;&#21644;&#27169;&#24335;&#23849;&#28291;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.01375</link><description>&lt;p&gt;
&#36890;&#36807;Gromov-Monge&#23884;&#20837;&#23454;&#29616;&#21333;&#35843;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Monotone Generative Modeling via a Gromov-Monge Embedding. (arXiv:2311.01375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01375
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Gromov-Monge&#23884;&#20837;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#35782;&#21035;&#25968;&#25454;&#32972;&#21518;&#30340;&#24213;&#23618;&#32467;&#26500;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#20013;&#23545;&#21021;&#22987;&#26465;&#20214;&#25935;&#24863;&#24615;&#21644;&#27169;&#24335;&#23849;&#28291;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#21019;&#24314;&#26032;&#20869;&#23481;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#38754;&#20020;&#30528;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#25935;&#24863;&#24615;&#21644;&#27169;&#24335;&#23849;&#28291;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Gromov-Monge&#23884;&#20837;&#65288;GME&#65289;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;&#23427;&#24110;&#21161;&#35782;&#21035;&#25968;&#25454;&#32972;&#21518;&#30340;&#24213;&#23618;&#27979;&#24230;&#30340;&#20302;&#32500;&#32467;&#26500;&#65292;&#28982;&#21518;&#23558;&#20854;&#26144;&#23556;&#21040;&#20445;&#25345;&#20960;&#20309;&#24615;&#36136;&#30340;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#27979;&#24230;&#65292;&#24182;&#23558;&#20854;&#26368;&#20248;&#22320;&#20256;&#36755;&#21040;&#21442;&#32771;&#27979;&#24230;&#12290;&#36890;&#36807;GME&#30340;&#20445;&#25345;&#24213;&#23618;&#20960;&#20309;&#24615;&#36136;&#21644;&#29983;&#25104;&#26144;&#23556;&#30340;$c$-&#21608;&#26399;&#24615;&#21333;&#35843;&#24615;&#26469;&#20445;&#35777;&#23427;&#20204;&#12290;&#21518;&#19968;&#29305;&#24615;&#26159;&#30830;&#20445;&#26356;&#22909;&#30340;&#21442;&#25968;&#21021;&#22987;&#21270;&#40065;&#26834;&#24615;&#21644;&#27169;&#24335;&#23849;&#28291;&#30340;&#31532;&#19968;&#27493;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#12289;&#36991;&#20813;&#27169;&#24335;&#23849;&#28291;&#21644;&#23545;&#19981;&#21516;&#36215;&#22987;&#26465;&#20214;&#20855;&#26377;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) are powerful tools for creating new content, but they face challenges such as sensitivity to starting conditions and mode collapse. To address these issues, we propose a deep generative model that utilizes the Gromov-Monge embedding (GME). It helps identify the low-dimensional structure of the underlying measure of the data and then maps it, while preserving its geometry, into a measure in a low-dimensional latent space, which is then optimally transported to the reference measure. We guarantee the preservation of the underlying geometry by the GME and $c$-cyclical monotonicity of the generative map, where $c$ is an intrinsic embedding cost employed by the GME. The latter property is a first step in guaranteeing better robustness to initialization of parameters and mode collapse. Numerical experiments demonstrate the effectiveness of our approach in generating high-quality images, avoiding mode collapse, and exhibiting robustness to different star
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;Wasserstein&#31354;&#38388;&#20013;&#24212;&#29992;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#29983;&#25104;&#25968;&#25454;&#20998;&#24067;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.17582</link><description>&lt;p&gt;
&#22312;Wasserstein&#31354;&#38388;&#20013;&#36890;&#36807;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of flow-based generative models via proximal gradient descent in Wasserstein space. (arXiv:2310.17582v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;Wasserstein&#31354;&#38388;&#20013;&#24212;&#29992;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#29983;&#25104;&#25968;&#25454;&#20998;&#24067;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#35745;&#31639;&#25968;&#25454;&#29983;&#25104;&#21644;&#20284;&#28982;&#20989;&#25968;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#26368;&#36817;&#22312;&#23454;&#35777;&#34920;&#29616;&#19978;&#26174;&#31034;&#20986;&#31454;&#20105;&#21147;&#12290;&#19982;&#30456;&#20851;&#22522;&#20110;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#30340;&#31215;&#32047;&#29702;&#35770;&#30740;&#31350;&#30456;&#27604;&#65292;&#23545;&#20110;&#22312;&#27491;&#21521;&#65288;&#25968;&#25454;&#21040;&#22122;&#22768;&#65289;&#21644;&#21453;&#21521;&#65288;&#22122;&#22768;&#21040;&#25968;&#25454;&#65289;&#26041;&#21521;&#19978;&#37117;&#26159;&#30830;&#23450;&#24615;&#30340;&#27969;&#27169;&#22411;&#30340;&#20998;&#26512;&#36824;&#24456;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#24402;&#19968;&#21270;&#27969;&#32593;&#32476;&#20013;&#23454;&#26045;Jordan-Kinderleherer-Otto&#65288;JKO&#65289;&#26041;&#26696;&#30340;&#25152;&#35859;JKO&#27969;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#36890;&#36807;&#28176;&#36827;&#27969;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#20998;&#24067;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#21033;&#29992;Wasserstein&#31354;&#38388;&#20013;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30340;&#25351;&#25968;&#25910;&#25947;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;JKO&#27969;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#30340;Kullback-Leibler&#65288;KL&#65289;&#20445;&#35777;&#20026;$O(\varepsilon^2)$&#65292;&#20854;&#20013;&#20351;&#29992;$N \lesssim \log (1/\varepsilon)$&#20010;JKO&#27493;&#39588;&#65288;&#27969;&#20013;&#30340;$N$&#20010;&#27531;&#24046;&#22359;&#65289;&#65292;&#20854;&#20013;$\varepsilon$&#26159;&#27599;&#27493;&#19968;&#38454;&#26465;&#20214;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flow-based generative models enjoy certain advantages in computing the data generation and the likelihood, and have recently shown competitive empirical performance. Compared to the accumulating theoretical studies on related score-based diffusion models, analysis of flow-based models, which are deterministic in both forward (data-to-noise) and reverse (noise-to-data) directions, remain sparse. In this paper, we provide a theoretical guarantee of generating data distribution by a progressive flow model, the so-called JKO flow model, which implements the Jordan-Kinderleherer-Otto (JKO) scheme in a normalizing flow network. Leveraging the exponential convergence of the proximal gradient descent (GD) in Wasserstein space, we prove the Kullback-Leibler (KL) guarantee of data generation by a JKO flow model to be $O(\varepsilon^2)$ when using $N \lesssim \log (1/\varepsilon)$ many JKO steps ($N$ Residual Blocks in the flow) where $\varepsilon $ is the error in the per-step first-order condit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#20132;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#36816;&#31639;&#31526;&#65292;&#36890;&#36807;&#26680;&#31215;&#20998;&#31639;&#23376;&#30340;&#29305;&#24449;&#20998;&#35299;&#21644;&#31070;&#32463;&#36817;&#20284;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#19978;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20845;&#20010;&#26631;&#20934;&#31070;&#32463;&#36816;&#31639;&#31526;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.12487</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#20132;&#27880;&#24847;&#21147;&#25552;&#21319;&#36816;&#31639;&#31526;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improved Operator Learning by Orthogonal Attention. (arXiv:2310.12487v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#20132;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#36816;&#31639;&#31526;&#65292;&#36890;&#36807;&#26680;&#31215;&#20998;&#31639;&#23376;&#30340;&#29305;&#24449;&#20998;&#35299;&#21644;&#31070;&#32463;&#36817;&#20284;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#19978;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20845;&#20010;&#26631;&#20934;&#31070;&#32463;&#36816;&#31639;&#31526;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36816;&#31639;&#31526;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65292;&#21463;&#21040;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#36816;&#31639;&#31526;&#24050;&#25104;&#20026;&#30456;&#20851;&#30740;&#31350;&#30340;&#20027;&#27969;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#24847;&#26426;&#21046;&#20013;&#21442;&#25968;&#25968;&#37327;&#24040;&#22823;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#26680;&#31215;&#20998;&#31639;&#23376;&#30340;&#29305;&#24449;&#20998;&#35299;&#21644;&#31070;&#32463;&#36817;&#20284;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#27491;&#20132;&#27880;&#24847;&#21147;&#12290;&#27491;&#20132;&#21270;&#33258;&#28982;&#22320;&#23545;&#32467;&#26524;&#31070;&#32463;&#36816;&#31639;&#31526;&#26045;&#21152;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#26377;&#21161;&#20110;&#25269;&#25239;&#36807;&#25311;&#21512;&#21644;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#21253;&#25324;&#27491;&#24120;&#21644;&#38750;&#27491;&#24120;&#20960;&#20309;&#24418;&#29366;&#30340;&#20845;&#20010;&#26631;&#20934;&#31070;&#32463;&#36816;&#31639;&#31526;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#31454;&#20105;&#23545;&#25163;&#65292;&#24182;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators, as an efficient surrogate model for learning the solutions of PDEs, have received extensive attention in the field of scientific machine learning. Among them, attention-based neural operators have become one of the mainstreams in related research. However, existing approaches overfit the limited training data due to the considerable number of parameters in the attention mechanism. To address this, we develop an orthogonal attention based on the eigendecomposition of the kernel integral operator and the neural approximation of eigenfunctions. The orthogonalization naturally poses a proper regularization effect on the resulting neural operator, which aids in resisting overfitting and boosting generalization. Experiments on six standard neural operator benchmark datasets comprising both regular and irregular geometries show that our method can outperform competing baselines with decent margins.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#33021;&#40657;&#30418;&#35775;&#38382;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30772;&#35299;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;PAIR&#36890;&#24120;&#21482;&#38656;&#35201;&#23569;&#20110;&#20108;&#21313;&#20010;&#26597;&#35810;&#26469;&#29983;&#25104;&#30772;&#35299;&#65292;&#26159;&#29616;&#26377;&#31639;&#27861;&#30340;&#25968;&#20010;&#25968;&#37327;&#32423;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.08419</link><description>&lt;p&gt;
&#22312;&#20108;&#21313;&#20010;&#26597;&#35810;&#20013;&#30772;&#35299;&#40657;&#30418;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Black Box Large Language Models in Twenty Queries. (arXiv:2310.08419v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08419
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#33021;&#40657;&#30418;&#35775;&#38382;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30772;&#35299;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;PAIR&#36890;&#24120;&#21482;&#38656;&#35201;&#23569;&#20110;&#20108;&#21313;&#20010;&#26597;&#35810;&#26469;&#29983;&#25104;&#30772;&#35299;&#65292;&#26159;&#29616;&#26377;&#31639;&#27861;&#30340;&#25968;&#20010;&#25968;&#37327;&#32423;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#30340;&#19968;&#33268;&#24615;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#30772;&#35299;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#36843;&#20351;LLMs&#36229;&#36234;&#20854;&#23433;&#20840;&#38450;&#25252;&#25514;&#26045;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#36825;&#20123;&#28431;&#27934;&#23545;&#20110;&#29702;&#35299;&#22266;&#26377;&#24369;&#28857;&#24182;&#38450;&#27490;&#26410;&#26469;&#30340;&#19981;&#24403;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prompt Automatic Iterative Refinement&#65288;PAIR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20165;&#36890;&#36807;&#23545;LLM&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#31639;&#27861;&#29983;&#25104;&#35821;&#20041;&#30772;&#35299;&#12290;PAIR&#21463;&#21040;&#31038;&#20250;&#24037;&#31243;&#25915;&#20987;&#30340;&#21551;&#21457;&#65292;&#20351;&#29992;&#25915;&#20987;&#32773;LLM&#33258;&#21160;&#29983;&#25104;&#38024;&#23545;&#30446;&#26631;LLM&#30340;&#30772;&#35299;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#25915;&#20987;&#32773;LLM&#36890;&#36807;&#36845;&#20195;&#26597;&#35810;&#30446;&#26631;LLM&#26469;&#26356;&#26032;&#21644;&#25913;&#36827;&#20505;&#36873;&#30772;&#35299;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;PAIR&#36890;&#24120;&#21482;&#38656;&#35201;&#23569;&#20110;&#20108;&#21313;&#20010;&#26597;&#35810;&#26469;&#29983;&#25104;&#30772;&#35299;&#65292;&#36825;&#27604;&#29616;&#26377;&#31639;&#27861;&#39640;&#25928;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;PAIR&#36824;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#30772;&#35299;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbrea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04444</link><description>&lt;p&gt;
&#39764;&#27861;&#35789;&#26159;&#20160;&#20040;&#65311;LLM&#25552;&#31034;&#30340;&#25511;&#21046;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#22312;LLM&#30340;&#37096;&#32626;&#20013;&#26159;&#26377;&#25928;&#21644;&#37325;&#35201;&#30340;&#65292;&#20294;&#22312;&#25968;&#23398;&#19978;&#29702;&#35299;&#19981;&#36275;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#25552;&#31034;&#34987;&#35748;&#20026;&#26159;&#35843;&#33410;LLM&#36755;&#20986;&#20998;&#24067;&#30340;&#25511;&#21046;&#21464;&#37327;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;token&#24207;&#21015;&#65292;&#26159;&#21542;&#24635;&#23384;&#22312;&#19968;&#20010;&#25105;&#20204;&#21487;&#20197;&#28155;&#21152;&#30340;&#25552;&#31034;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65311;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#26368;&#20248;&#25552;&#31034;&#31216;&#20026;&#39764;&#27861;&#35789;&#65292;&#22240;&#20026;&#28155;&#21152;&#25552;&#31034;&#20250;&#23548;&#33268;LLM&#36755;&#20986;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#22914;&#26524;&#23384;&#22312;&#39764;&#27861;&#35789;&#65292;&#25105;&#20204;&#33021;&#21542;&#25214;&#21040;&#23427;&#20204;&#65311;&#22914;&#26524;&#21487;&#20197;&#65292;&#23427;&#20204;&#30340;&#29305;&#24615;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#25552;&#20379;&#20102;&#23558;&#25511;&#21046;&#29702;&#35770;&#24212;&#29992;&#20110;&#33258;&#27880;&#24847;&#21147;&#22836;&#30340;&#20998;&#26512;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#20854;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20989;&#25968;&#20026;&#21487;&#25511;&#21046;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#20511;&#37492;&#25511;&#21046;&#29702;&#35770;&#26469;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;$k-\epsilon$&#21487;&#25511;&#21046;&#24615;&#30340;&#25351;&#26631;&#65292;&#29992;&#20110;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of the self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k-\epsilon$ controllability to characterize LLM steerability. We comput
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24322;&#26500;&#20020;&#24202;&#25968;&#25454;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#19988;&#23545;FLamby&#22522;&#20934;&#36827;&#34892;&#20102;&#23454;&#36136;&#24615;&#25913;&#36827;&#21644;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.16825</link><description>&lt;p&gt;
FENDA-FL&#65306;&#24322;&#26500;&#20020;&#24202;&#25968;&#25454;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FENDA-FL: Personalized Federated Learning on Heterogeneous Clinical Datasets. (arXiv:2309.16825v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16825
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24322;&#26500;&#20020;&#24202;&#25968;&#25454;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#19988;&#23545;FLamby&#22522;&#20934;&#36827;&#34892;&#20102;&#23454;&#36136;&#24615;&#25913;&#36827;&#21644;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#34987;&#36234;&#26469;&#36234;&#35748;&#20026;&#26159;&#20811;&#26381;&#20020;&#24202;&#29615;&#22659;&#20013;&#25968;&#25454;&#23396;&#31435;&#38382;&#39064;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#22312;&#20020;&#24202;&#24212;&#29992;&#30340;FL&#30740;&#31350;&#20013;&#20570;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#23558;FENDA&#26041;&#27861;&#65288;Kim&#31561;&#20154;&#65292;2016&#65289;&#25193;&#23637;&#21040;FL&#30340;&#26041;&#27861;&#12290;&#22312;FLamby&#22522;&#20934;&#65288;du Terrail&#31561;&#20154;&#65292;2022a&#65289;&#21644;GEMINI&#25968;&#25454;&#38598;&#65288;Verma&#31561;&#20154;&#65292;2017&#65289;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#23545;&#24322;&#26500;&#20020;&#24202;&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#36890;&#24120;&#20248;&#20110;&#29616;&#26377;&#30340;&#20840;&#23616;&#21644;&#20010;&#24615;&#21270;FL&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#32467;&#26524;&#22312;&#21407;&#26377;&#30340;FLamby&#22522;&#20934;&#19978;&#34920;&#31034;&#20986;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#65292;&#24182;&#25193;&#23637;&#20102;&#36825;&#20123;&#22522;&#20934;&#20197;&#21253;&#25324;&#20010;&#24615;&#21270;FL&#26041;&#27861;&#30340;&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20513;&#24314;&#31435;&#19968;&#20010;&#20840;&#38754;&#30340;FL&#26816;&#26597;&#28857;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#23454;&#38469;&#29615;&#22659;&#24182;&#25552;&#20379;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is increasingly being recognized as a key approach to overcoming the data silos that so frequently obstruct the training and deployment of machine-learning models in clinical settings. This work contributes to a growing body of FL research specifically focused on clinical applications along three important directions. First, an extension of the FENDA method (Kim et al., 2016) to the FL setting is proposed. Experiments conducted on the FLamby benchmarks (du Terrail et al., 2022a) and GEMINI datasets (Verma et al., 2017) show that the approach is robust to heterogeneous clinical data and often outperforms existing global and personalized FL techniques. Further, the experimental results represent substantive improvements over the original FLamby benchmarks and expand such benchmarks to include evaluation of personalized FL methods. Finally, we advocate for a comprehensive checkpointing and evaluation framework for FL to better reflect practical settings and provide
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28151;&#21512;&#29289;&#29702;-&#26426;&#22120;&#23398;&#20064;&#27668;&#20505;&#27169;&#25311;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#22312;&#32447;&#24314;&#27169;&#38169;&#35823;&#37319;&#26679;&#21644;&#35780;&#20272;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;&#35774;&#35745;&#20013;&#21457;&#29616;&#20102;&#25913;&#36827;&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.16177</link><description>&lt;p&gt;
&#31995;&#32479;&#21270;&#37319;&#26679;&#21644;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;&#22312;&#27668;&#20505;&#27169;&#22411;&#20013;&#30340;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Systematic Sampling and Validation of Machine Learning-Parameterizations in Climate Models. (arXiv:2309.16177v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28151;&#21512;&#29289;&#29702;-&#26426;&#22120;&#23398;&#20064;&#27668;&#20505;&#27169;&#25311;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#22312;&#32447;&#24314;&#27169;&#38169;&#35823;&#37319;&#26679;&#21644;&#35780;&#20272;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;&#35774;&#35745;&#20013;&#21457;&#29616;&#20102;&#25913;&#36827;&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#29289;&#29702;-&#26426;&#22120;&#23398;&#20064;&#27668;&#20505;&#27169;&#25311;&#30340;&#36827;&#23637;&#21463;&#21040;&#33719;&#21462;&#39640;&#24615;&#33021;&#32806;&#21512;&#65288;&#21363;&#22312;&#32447;&#65289;&#27169;&#25311;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#34429;&#28982;&#22312;&#33073;&#26426;&#29615;&#22659;&#20013;&#35780;&#20272;&#25968;&#30334;&#20010;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;&#23376;&#32593;&#26684;&#38381;&#21512;&#65288;&#22914;&#23545;&#27969;&#21644;&#36752;&#23556;&#65289;&#26159;&#30452;&#25509;&#30340;&#65292;&#20294;&#22312;&#30456;&#21516;&#35268;&#27169;&#19978;&#30340;&#22312;&#32447;&#35780;&#20272;&#22312;&#25216;&#26415;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#36719;&#20214;&#33258;&#21160;&#21270;&#23454;&#29616;&#20102;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#22810;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#22312;&#32447;&#24314;&#27169;&#38169;&#35823;&#37319;&#26679;&#12290;&#21033;&#29992;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35780;&#20272;&#28151;&#21512;&#27668;&#20505;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#21046;&#23450;&#25913;&#36827;&#31574;&#30053;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#21253;&#21547;&#35760;&#24518;&#12289;&#30456;&#23545;&#28287;&#24230;&#36755;&#20837;&#29305;&#24449;&#36716;&#25442;&#21644;&#39069;&#22806;&#36755;&#20837;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#30340;&#22312;&#32447;&#24615;&#33021;&#26377;&#25152;&#25552;&#39640;&#12290;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#22312;&#32447;&#38169;&#35823;&#30340;&#26174;&#33879;&#24046;&#24322;&#20197;&#21450;&#33073;&#26426;&#19982;&#22312;&#32447;&#38169;&#35823;&#32479;&#35745;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#38656;&#35201;&#22312;&#32447;&#35780;&#20272;&#25968;&#30334;&#20010;&#20505;&#36873;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#26816;&#27979;&#21442;&#25968;&#21270;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Progress in hybrid physics-machine learning (ML) climate simulations has been limited by the difficulty of obtaining performant coupled (i.e. online) simulations. While evaluating hundreds of ML parameterizations of subgrid closures (here of convection and radiation) offline is straightforward, online evaluation at the same scale is technically challenging. Our software automation achieves an order-of-magnitude larger sampling of online modeling errors than has previously been examined. Using this, we evaluate the hybrid climate model performance and define strategies to improve it. We show that model online performance improves when incorporating memory, a relative humidity input feature transformation, and additional input variables. We also reveal substantial variation in online error and inconsistencies between offline vs. online error statistics. The implication is that hundreds of candidate ML models should be evaluated online to detect the effects of parameterization design choi
&lt;/p&gt;</description></item><item><title>FUTURE-AI&#26159;&#31532;&#19968;&#20010;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2309.12325</link><description>&lt;p&gt;
FUTURE-AI&#65306;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;&#21644;&#21487;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;&#30340;&#22269;&#38469;&#20849;&#35782;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare. (arXiv:2309.12325v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12325
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI&#26159;&#31532;&#19968;&#20010;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;AI&#25216;&#26415;&#22312;&#29616;&#23454;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#37096;&#32626;&#21644;&#37319;&#29992;&#20173;&#21463;&#38480;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#21307;&#30103;AI&#30340;&#25216;&#26415;&#12289;&#20020;&#24202;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#39118;&#38505;&#25552;&#20986;&#20102;&#20851;&#27880;&#12290;&#20026;&#20102;&#22686;&#21152;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37319;&#29992;&#65292;&#21307;&#30103;AI&#24037;&#20855;&#24517;&#39035;&#24471;&#21040;&#24739;&#32773;&#12289;&#20020;&#24202;&#21307;&#29983;&#12289;&#20581;&#24247;&#32452;&#32455;&#21644;&#24403;&#23616;&#30340;&#20449;&#20219;&#21644;&#25509;&#21463;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;FUTURE-AI&#25351;&#21335;&#20316;&#20026;&#31532;&#19968;&#20010;&#29992;&#20110;&#25351;&#23548;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#12290;FUTURE-AI&#32852;&#30431;&#25104;&#31435;&#20110;2021&#24180;&#65292;&#30446;&#21069;&#21253;&#25324;&#26469;&#33258;51&#20010;&#22269;&#23478;&#30340;118&#20301;&#36328;&#23398;&#31185;&#19987;&#23478;&#65292;&#20195;&#34920;&#20102;&#25152;&#26377;&#22823;&#27954;&#65292;&#21253;&#25324;AI&#31185;&#23398;&#23478;&#12289;&#20020;&#24202;&#21307;&#29983;&#12289;&#20262;&#29702;&#23398;&#23478;&#21644;&#31038;&#20250;&#31185;&#23398;&#23478;&#12290;&#22312;&#20026;&#26399;&#20004;&#24180;&#30340;&#26102;&#38388;&#37324;&#65292;&#32852;&#30431;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#23450;&#20041;&#20102;&#21487;&#20449;AI&#30340;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20854;&#20013;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
Despite major advances in artificial intelligence (AI) for medicine and healthcare, the deployment and adoption of AI technologies remain limited in real-world clinical practice. In recent years, concerns have been raised about the technical, clinical, ethical and legal risks associated with medical AI. To increase real world adoption, it is essential that medical AI tools are trusted and accepted by patients, clinicians, health organisations and authorities. This work describes the FUTURE-AI guideline as the first international consensus framework for guiding the development and deployment of trustworthy AI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and currently comprises 118 inter-disciplinary experts from 51 countries representing all continents, including AI scientists, clinicians, ethicists, and social scientists. Over a two-year period, the consortium defined guiding principles and best practices for trustworthy AI through an iterative process comprising a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#20363;&#31354;&#38388;&#20998;&#26512;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#38382;&#39064;&#30340;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#36873;&#25321;&#31995;&#32479;&#65292;&#39044;&#27979;&#20102;&#22312;&#32473;&#23450;&#29305;&#24449;&#19979;&#26368;&#36866;&#21512;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#28145;&#20837;&#20102;&#35299;&#20102;&#31639;&#27861;&#24615;&#33021;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.03229</link><description>&lt;p&gt;
&#22914;&#20309;&#36873;&#25321;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#20013;&#30340;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Which algorithm to select in sports timetabling?. (arXiv:2309.03229v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#20363;&#31354;&#38388;&#20998;&#26512;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#38382;&#39064;&#30340;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#36873;&#25321;&#31995;&#32479;&#65292;&#39044;&#27979;&#20102;&#22312;&#32473;&#23450;&#29305;&#24449;&#19979;&#26368;&#36866;&#21512;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#28145;&#20837;&#20102;&#35299;&#20102;&#31639;&#27861;&#24615;&#33021;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#20307;&#32946;&#31454;&#36187;&#37117;&#38656;&#35201;&#19968;&#20010;&#36187;&#31243;&#23433;&#25490;&#65292;&#30830;&#23450;&#27604;&#36187;&#38431;&#20237;&#20309;&#26102;&#20309;&#22320;&#30456;&#36935;&#12290;&#26368;&#36817;&#30340;&#22269;&#38469;&#36187;&#31243;&#23433;&#25490;&#31454;&#36187;(ITC2021)&#25581;&#31034;&#20102;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#34429;&#28982;&#21487;&#33021;&#24320;&#21457;&#20986;&#36890;&#29992;&#31639;&#27861;&#65292;&#20294;&#27599;&#20010;&#31639;&#27861;&#22312;&#38382;&#39064;&#23454;&#20363;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#24456;&#22823;&#12290;&#26412;&#25991;&#22312;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#26041;&#38754;&#25552;&#20379;&#20102;&#23454;&#20363;&#31354;&#38388;&#20998;&#26512;&#65292;&#20174;&#32780;&#28145;&#20837;&#20102;&#35299;&#20102;&#20843;&#31181;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#36873;&#25321;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#20307;&#32946;&#36187;&#31243;&#23433;&#25490;&#38382;&#39064;&#23454;&#20363;&#30340;&#29305;&#24449;&#39044;&#27979;&#21738;&#31181;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#21487;&#33021;&#34920;&#29616;&#26368;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#21738;&#20123;&#29305;&#24449;&#22312;&#20570;&#20986;&#39044;&#27979;&#26102;&#24456;&#37325;&#35201;&#65292;&#20174;&#32780;&#28145;&#20837;&#20102;&#35299;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#24314;&#35758;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#23454;&#20363;&#30340;&#32463;&#39564;&#38590;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22522;&#20110;&#22823;&#35268;&#27169;&#35745;&#31639;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Any sports competition needs a timetable, specifying when and where teams meet each other. The recent International Timetabling Competition (ITC2021) on sports timetabling showed that, although it is possible to develop general algorithms, the performance of each algorithm varies considerably over the problem instances. This paper provides an instance space analysis for sports timetabling, resulting in powerful insights into the strengths and weaknesses of eight state-of-the-art algorithms. Based on machine learning techniques, we propose an algorithm selection system that predicts which algorithm is likely to perform best when given the characteristics of a sports timetabling problem instance. Furthermore, we identify which characteristics are important in making that prediction, providing insights in the performance of the algorithms, and suggestions to further improve them. Finally, we assess the empirical hardness of the instances. Our results are based on large computational exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#21521;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#23545;&#29992;&#26799;&#24230;&#27969;&#35757;&#32451;&#30340;&#38750;&#32447;&#24615;&#32593;&#32476;&#30340;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#22312;&#32593;&#32476;&#20197;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#31163;&#25955;&#21270;&#24418;&#24335;&#21021;&#22987;&#21270;&#21518;&#65292;&#36825;&#31181;&#31163;&#25955;&#21270;&#23558;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2309.01213</link><description>&lt;p&gt;
&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#19982;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Implicit regularization of deep residual networks towards neural ODEs. (arXiv:2309.01213v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#21521;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#23545;&#29992;&#26799;&#24230;&#27969;&#35757;&#32451;&#30340;&#38750;&#32447;&#24615;&#32593;&#32476;&#30340;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#22312;&#32593;&#32476;&#20197;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#31163;&#25955;&#21270;&#24418;&#24335;&#21021;&#22987;&#21270;&#21518;&#65292;&#36825;&#31181;&#31163;&#25955;&#21270;&#23558;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#26159;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#23427;&#20204;&#30340;&#36830;&#32493;&#28145;&#24230;&#27169;&#25311;&#31216;&#20026;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#65292;&#20063;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#31163;&#25955;&#27169;&#22411;&#19982;&#36830;&#32493;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#20173;&#32570;&#20047;&#22362;&#23454;&#30340;&#25968;&#23398;&#22522;&#30784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#38024;&#23545;&#29992;&#26799;&#24230;&#27969;&#35757;&#32451;&#30340;&#38750;&#32447;&#24615;&#32593;&#32476;&#30340;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#21521;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#26469;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#32593;&#32476;&#30340;&#21021;&#22987;&#21270;&#26159;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#31163;&#25955;&#21270;&#65292;&#21017;&#36825;&#31181;&#31163;&#25955;&#21270;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;&#26377;&#38480;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#35757;&#32451;&#26102;&#38388;&#36235;&#20110;&#26080;&#31351;&#22823;&#37117;&#25104;&#31435;&#65292;&#21482;&#35201;&#32593;&#32476;&#28385;&#36275;Polyak-Lojasiewicz&#26465;&#20214;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20010;&#26465;&#20214;&#36866;&#29992;&#20110;&#19968;&#20010;&#27531;&#24046;&#32593;&#32476;&#23478;&#26063;&#65292;&#20854;&#20013;&#27531;&#24046;&#26159;&#20004;&#23618;&#24863;&#30693;&#26426;&#65292;&#22312;&#23485;&#24230;&#19978;&#21482;&#26159;&#32447;&#24615;&#36229;&#21442;&#25968;&#21270;&#65292;&#24182;&#19988;&#26263;&#31034;&#20102;&#26799;&#24230;&#27969;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Residual neural networks are state-of-the-art deep learning models. Their continuous-depth analog, neural ordinary differential equations (ODEs), are also widely used. Despite their success, the link between the discrete and continuous models still lacks a solid mathematical foundation. In this article, we take a step in this direction by establishing an implicit regularization of deep residual networks towards neural ODEs, for nonlinear networks trained with gradient flow. We prove that if the network is initialized as a discretization of a neural ODE, then such a discretization holds throughout training. Our results are valid for a finite training time, and also as the training time tends to infinity provided that the network satisfies a Polyak-Lojasiewicz condition. Importantly, this condition holds for a family of residual networks where the residuals are two-layer perceptrons with an overparameterization in width that is only linear, and implies the convergence of gradient flow to
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;QCNNs&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#36890;&#36807;&#38544;&#34255;&#29305;&#24449;&#26144;&#23556;&#23884;&#20837;&#29289;&#29702;&#31995;&#32479;&#21442;&#25968;&#65292;&#24182;&#19988;&#21033;&#29992;&#37327;&#23376;&#20020;&#30028;&#24615;&#29983;&#25104;&#36866;&#21512;&#30340;&#22522;&#20989;&#25968;&#38598;&#65292;&#27744;&#21270;&#23618;&#36873;&#25321;&#33021;&#22815;&#24418;&#25104;&#39640;&#24615;&#33021;&#20915;&#31574;&#36793;&#30028;&#30340;&#22522;&#20989;&#25968;&#65292;&#32780;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#20381;&#36182;&#20110;&#23884;&#20837;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.16664</link><description>&lt;p&gt;
&#25105;&#20204;&#21487;&#20197;&#20174;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#21040;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What can we learn from quantum convolutional neural networks?. (arXiv:2308.16664v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16664
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;QCNNs&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#36890;&#36807;&#38544;&#34255;&#29305;&#24449;&#26144;&#23556;&#23884;&#20837;&#29289;&#29702;&#31995;&#32479;&#21442;&#25968;&#65292;&#24182;&#19988;&#21033;&#29992;&#37327;&#23376;&#20020;&#30028;&#24615;&#29983;&#25104;&#36866;&#21512;&#30340;&#22522;&#20989;&#25968;&#38598;&#65292;&#27744;&#21270;&#23618;&#36873;&#25321;&#33021;&#22815;&#24418;&#25104;&#39640;&#24615;&#33021;&#20915;&#31574;&#36793;&#30028;&#30340;&#22522;&#20989;&#25968;&#65292;&#32780;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#20381;&#36182;&#20110;&#23884;&#20837;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;QCNNs&#65289;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#20986;&#20197;&#19979;&#32467;&#35770;&#65306;1&#65289;&#36890;&#36807;&#38544;&#34255;&#29305;&#24449;&#26144;&#23556;&#65292;&#24037;&#20316;&#20110;&#37327;&#23376;&#25968;&#25454;&#21487;&#20197;&#34987;&#35270;&#20026;&#23884;&#20837;&#29289;&#29702;&#31995;&#32479;&#21442;&#25968;&#65307;2&#65289;&#23545;&#20110;&#37327;&#23376;&#30456;&#20301;&#35782;&#21035;&#65292;&#20854;&#39640;&#24615;&#33021;&#21487;&#20197;&#24402;&#22240;&#20110;&#22312;&#22522;&#24577;&#23884;&#20837;&#26399;&#38388;&#29983;&#25104;&#38750;&#24120;&#36866;&#21512;&#30340;&#22522;&#20989;&#25968;&#38598;&#65292;&#20854;&#20013;&#33258;&#26059;&#27169;&#22411;&#30340;&#37327;&#23376;&#20020;&#30028;&#24615;&#23548;&#33268;&#20855;&#26377;&#24555;&#36895;&#21464;&#21270;&#29305;&#24449;&#30340;&#22522;&#20989;&#25968;&#65307;3&#65289;QCNN&#30340;&#27744;&#21270;&#23618;&#36127;&#36131;&#36873;&#25321;&#37027;&#20123;&#33021;&#22815;&#26377;&#21161;&#20110;&#24418;&#25104;&#39640;&#24615;&#33021;&#20915;&#31574;&#36793;&#30028;&#30340;&#22522;&#20989;&#25968;&#65292;&#23398;&#20064;&#36807;&#31243;&#23545;&#24212;&#20110;&#36866;&#24212;&#24615;&#27979;&#37327;&#65292;&#20351;&#24471;&#23569;&#37327;&#37327;&#23376;&#27604;&#29305;&#31639;&#31526;&#26144;&#23556;&#21040;&#25972;&#20010;&#23492;&#23384;&#22120;&#21487;&#35266;&#27979;&#37327;&#65307;4&#65289;QCNN&#27169;&#22411;&#30340;&#27867;&#21270;&#24378;&#28872;&#20381;&#36182;&#20110;&#23884;&#20837;&#31867;&#22411;&#65292;&#22522;&#20110;&#20613;&#37324;&#21494;&#22522;&#30340;&#26059;&#36716;&#29305;&#24449;&#26144;&#23556;&#38656;&#35201;&#20180;&#32454;&#30340;&#29305;&#24449;&#24037;&#31243;&#65307;5&#65289;&#22522;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#27979;&#37327;&#27425;&#25968;&#30340;&#35835;&#20986;&#30340;QCNN&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#20542;&#21521;&#20110;&#22320;&#38754;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
We can learn from analyzing quantum convolutional neural networks (QCNNs) that: 1) working with quantum data can be perceived as embedding physical system parameters through a hidden feature map; 2) their high performance for quantum phase recognition can be attributed to generation of a very suitable basis set during the ground state embedding, where quantum criticality of spin models leads to basis functions with rapidly changing features; 3) pooling layers of QCNNs are responsible for picking those basis functions that can contribute to forming a high-performing decision boundary, and the learning process corresponds to adapting the measurement such that few-qubit operators are mapped to full-register observables; 4) generalization of QCNN models strongly depends on the embedding type, and that rotation-based feature maps with the Fourier basis require careful feature engineering; 5) accuracy and generalization of QCNNs with readout based on a limited number of shots favor the groun
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#29109;&#30340;&#25439;&#22833;&#39033;&#65292;&#36890;&#36807;&#27979;&#37327;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#25968;&#25454;&#26102;&#30340;&#29109;&#21464;&#21270;&#65292;&#25351;&#23548;&#31070;&#32463;&#32593;&#32476;&#20197;&#26356;&#24555;&#36895;&#30340;&#25910;&#25947;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#23398;&#20064;&#20016;&#23500;&#30340;&#28508;&#22312;&#25968;&#25454;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.14938</link><description>&lt;p&gt;
&#22522;&#20110;&#29109;&#30340;&#25351;&#23548;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#25910;&#25947;&#21644;&#25913;&#21892;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Entropy-based Guidance of Deep Neural Networks for Accelerated Convergence and Improved Performance. (arXiv:2308.14938v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#29109;&#30340;&#25439;&#22833;&#39033;&#65292;&#36890;&#36807;&#27979;&#37327;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#25968;&#25454;&#26102;&#30340;&#29109;&#21464;&#21270;&#65292;&#25351;&#23548;&#31070;&#32463;&#32593;&#32476;&#20197;&#26356;&#24555;&#36895;&#30340;&#25910;&#25947;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#23398;&#20064;&#20016;&#23500;&#30340;&#28508;&#22312;&#25968;&#25454;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26497;&#22823;&#22320;&#22686;&#21152;&#20102;&#25105;&#20204;&#20174;&#22823;&#35268;&#27169;&#12289;&#39640;&#32500;&#24230;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#36328;&#36234;&#26080;&#25968;&#23398;&#31185;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20915;&#31574;&#19981;&#26131;&#35299;&#37322;&#65292;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#24314;&#31435;&#21644;&#35757;&#32451;&#23427;&#20204;&#26159;&#19981;&#30830;&#23450;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#32473;&#36825;&#20123;&#21162;&#21147;&#22686;&#21152;&#32467;&#26500;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#25968;&#23398;&#32467;&#26524;&#65292;&#20197;&#39640;&#25928;&#22320;&#27979;&#37327;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#25968;&#25454;&#26102;&#30340;&#29109;&#21464;&#21270;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#29109;&#30340;&#25439;&#22833;&#39033;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22270;&#20687;&#21387;&#32553;&#21644;&#22270;&#20687;&#20998;&#31867;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#25439;&#22833;&#39033;&#25351;&#23548;&#31070;&#32463;&#32593;&#32476;&#20197;&#26356;&#23569;&#30340;&#32500;&#24230;&#23398;&#20064;&#20016;&#23500;&#30340;&#28508;&#22312;&#25968;&#25454;&#34920;&#31034;&#65292;&#25910;&#25947;&#20110;&#26356;&#23569;&#30340;&#35757;&#32451;&#36718;&#27425;&#65292;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#27979;&#35797;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have dramatically increased our capacity to learn from large, high-dimensional datasets across innumerable disciplines. However, their decisions are not easily interpretable, their computational costs are high, and building and training them are uncertain processes. To add structure to these efforts, we derive new mathematical results to efficiently measure the changes in entropy as fully-connected and convolutional neural networks process data, and introduce entropy-based loss terms. Experiments in image compression and image classification on benchmark datasets demonstrate these losses guide neural networks to learn rich latent data representations in fewer dimensions, converge in fewer training epochs, and achieve better test metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#32972;&#26223;&#22270;&#20013;&#26597;&#25214;&#22810;&#20010;&#23884;&#20837;&#30340;&#27169;&#26495;&#22270;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#24809;&#32602;&#30456;&#20284;&#24230;&#30697;&#38453;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#21305;&#37197;&#30340;&#21457;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#21152;&#36895;&#25514;&#26045;&#12290;&#22312;&#29702;&#35770;&#39564;&#35777;&#21644;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13451</link><description>&lt;p&gt;
&#25235;&#20303;&#23427;&#20204;&#65306;&#22270;&#21305;&#37197;&#21305;&#37197;&#28388;&#27874;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#22810;&#26679;&#21270;
&lt;/p&gt;
&lt;p&gt;
Gotta match 'em all: Solution diversification in graph matching matched filters. (arXiv:2308.13451v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#32972;&#26223;&#22270;&#20013;&#26597;&#25214;&#22810;&#20010;&#23884;&#20837;&#30340;&#27169;&#26495;&#22270;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#24809;&#32602;&#30456;&#20284;&#24230;&#30697;&#38453;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#21305;&#37197;&#30340;&#21457;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#21152;&#36895;&#25514;&#26045;&#12290;&#22312;&#29702;&#35770;&#39564;&#35777;&#21644;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#24120;&#22823;&#30340;&#32972;&#26223;&#22270;&#20013;&#26597;&#25214;&#22810;&#20010;&#23884;&#20837;&#22312;&#20854;&#20013;&#30340;&#27169;&#26495;&#22270;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;Sussman&#31561;&#20154;&#25552;&#20986;&#30340;&#22270;&#21305;&#37197;&#21305;&#37197;&#28388;&#27874;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#21305;&#37197;&#28388;&#27874;&#31639;&#27861;&#20013;&#36845;&#20195;&#22320;&#24809;&#32602;&#21512;&#36866;&#30340;&#33410;&#28857;&#23545;&#30456;&#20284;&#24230;&#30697;&#38453;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#21305;&#37197;&#30340;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31639;&#27861;&#21152;&#36895;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25105;&#20204;&#30340;&#21305;&#37197;&#28388;&#27874;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#22312;&#30456;&#20851;&#30340;Erdos-Renyi&#22270;&#35774;&#32622;&#20013;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#19978;&#30340;&#39564;&#35777;&#65292;&#26174;&#31034;&#20854;&#22312;&#28201;&#21644;&#30340;&#27169;&#22411;&#26465;&#20214;&#19979;&#33021;&#22815;&#39034;&#24207;&#22320;&#21457;&#29616;&#22810;&#20010;&#27169;&#26495;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20351;&#29992;&#27169;&#25311;&#27169;&#22411;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#20154;&#33041;&#36830;&#25509;&#32452;&#21644;&#22823;&#22411;&#20132;&#26131;&#30693;&#35782;&#24211;&#65289;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach for finding multiple noisily embedded template graphs in a very large background graph. Our method builds upon the graph-matching-matched-filter technique proposed in Sussman et al., with the discovery of multiple diverse matchings being achieved by iteratively penalizing a suitable node-pair similarity matrix in the matched filter algorithm. In addition, we propose algorithmic speed-ups that greatly enhance the scalability of our matched-filter approach. We present theoretical justification of our methodology in the setting of correlated Erdos-Renyi graphs, showing its ability to sequentially discover multiple templates under mild model conditions. We additionally demonstrate our method's utility via extensive experiments both using simulated models and real-world dataset, include human brain connectomes and a large transactional knowledge base.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;K-Means&#21644;P-Median&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;&#26469;&#20248;&#21270;&#22312;&#22478;&#24066;&#29615;&#22659;&#19979;&#29289;&#27969;&#38598;&#25955;&#22320;&#30340;&#20301;&#32622;&#24067;&#32622;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#36317;&#31163;&#21644;&#30899;&#36275;&#36857;&#12290;</title><link>http://arxiv.org/abs/2308.11038</link><description>&lt;p&gt;
&#29289;&#27969;&#38598;&#25955;&#22320;&#20301;&#32622;&#20248;&#21270;&#65306;&#19968;&#31181;&#22522;&#20110;K-Means&#21644;P-Median&#27169;&#22411;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Logistics Hub Location Optimization: A K-Means and P-Median Model Hybrid Approach Using Road Network Distances. (arXiv:2308.11038v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;K-Means&#21644;P-Median&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;&#26469;&#20248;&#21270;&#22312;&#22478;&#24066;&#29615;&#22659;&#19979;&#29289;&#27969;&#38598;&#25955;&#22320;&#30340;&#20301;&#32622;&#24067;&#32622;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#36317;&#31163;&#21644;&#30899;&#36275;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#27969;&#38598;&#25955;&#22320;&#22312;&#26368;&#21518;&#19968;&#20844;&#37324;&#37197;&#36865;&#36317;&#31163;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65307;&#21363;&#20351;&#36317;&#31163;&#24494;&#23567;&#22686;&#21152;&#20063;&#20250;&#23545;&#30005;&#23376;&#21830;&#21153;&#34892;&#19994;&#30340;&#19994;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#21516;&#26102;&#36824;&#20250;&#22686;&#21152;&#20854;&#30899;&#36275;&#36857;&#12290;&#29305;&#21035;&#26159;&#22312;Covid-19&#20043;&#21518;&#65292;&#35813;&#34892;&#19994;&#30340;&#22686;&#38271;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#20248;&#21270;&#29289;&#27969;&#38598;&#25955;&#22320;&#30340;&#24067;&#32622;&#12290;&#35813;&#26041;&#27861;&#20381;&#27425;&#37319;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#31354;&#38388;&#20301;&#32622;&#65292;&#20351;&#29992;K-Means&#23545;&#20132;&#20184;&#28857;&#36827;&#34892;&#32858;&#31867;&#12290;&#32858;&#31867;&#26041;&#27861;&#20351;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;&#65292;&#32780;&#19981;&#26159;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#12290;&#36991;&#20813;&#20351;&#29992;&#38750;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#23548;&#33268;&#38169;&#35823;&#21644;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;P-Median&#26041;&#27861;&#30830;&#23450;&#38598;&#25955;&#22320;&#30340;&#20301;&#32622;&#12290;P-Median&#26041;&#27861;&#36824;&#23558;&#20132;&#20184;&#25968;&#37327;&#21644;&#20154;&#21475;&#20316;&#20026;&#26435;&#37325;&#32771;&#34385;&#22312;&#20869;&#12290;&#20351;&#29992;Muller&#21644;Phipps&#65288;M&#65286;P&#65289;&#30340;&#23454;&#38469;&#20132;&#20184;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Logistic hubs play a pivotal role in the last-mile delivery distance; even a slight increment in distance negatively impacts the business of the e-commerce industry while also increasing its carbon footprint. The growth of this industry, particularly after Covid-19, has further intensified the need for optimized allocation of resources in an urban environment. In this study, we use a hybrid approach to optimize the placement of logistic hubs. The approach sequentially employs different techniques. Initially, delivery points are clustered using K-Means in relation to their spatial locations. The clustering method utilizes road network distances as opposed to Euclidean distances. Non-road network-based approaches have been avoided since they lead to erroneous and misleading results. Finally, hubs are located using the P-Median method. The P-Median method also incorporates the number of deliveries and population as weights. Real-world delivery data from Muller and Phipps (M&amp;P) is used to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;&#65288;TIN&#65289;&#65292;&#29992;&#20110;&#25429;&#25417;&#34892;&#20026;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;&#22235;&#37325;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20197;&#39044;&#27979;&#28857;&#20987;&#29575;&#30340;&#25928;&#26524;&#21644;&#24050;&#26377;&#26041;&#27861;&#23545;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#23398;&#20064;&#31243;&#24230;&#23578;&#19981;&#28165;&#26970;&#12290;</title><link>http://arxiv.org/abs/2308.08487</link><description>&lt;p&gt;
&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Temporal Interest Network for Click-Through Rate Prediction. (arXiv:2308.08487v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;&#65288;TIN&#65289;&#65292;&#29992;&#20110;&#25429;&#25417;&#34892;&#20026;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;&#22235;&#37325;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20197;&#39044;&#27979;&#28857;&#20987;&#29575;&#30340;&#25928;&#26524;&#21644;&#24050;&#26377;&#26041;&#27861;&#23545;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#23398;&#20064;&#31243;&#24230;&#23578;&#19981;&#28165;&#26970;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#34892;&#20026;&#30340;&#21382;&#21490;&#26159;&#39044;&#27979;&#28857;&#20987;&#29575;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;&#30446;&#26631;&#39033;&#30446;&#20855;&#26377;&#24378;&#28872;&#30340;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#34429;&#28982;&#24050;&#26377;&#25991;&#29486;&#20998;&#21035;&#30740;&#31350;&#20102;&#36825;&#20123;&#30456;&#20851;&#24615;&#65292;&#20294;&#23578;&#26410;&#20998;&#26512;&#23427;&#20204;&#30340;&#32452;&#21512;&#65292;&#21363;&#34892;&#20026;&#35821;&#20041;&#12289;&#30446;&#26631;&#35821;&#20041;&#12289;&#34892;&#20026;&#26102;&#38388;&#21644;&#30446;&#26631;&#26102;&#38388;&#30340;&#22235;&#37325;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#30456;&#20851;&#24615;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#20197;&#21450;&#29616;&#26377;&#26041;&#27861;&#23398;&#20064;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#31243;&#24230;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;&#23454;&#36341;&#20013;&#27979;&#37327;&#20102;&#22235;&#37325;&#30456;&#20851;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#30452;&#35266;&#32780;&#24378;&#22823;&#30340;&#22235;&#37325;&#27169;&#24335;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;&#20960;&#31181;&#20195;&#34920;&#24615;&#30340;&#29992;&#25143;&#34892;&#20026;&#26041;&#27861;&#30340;&#23398;&#20064;&#30456;&#20851;&#24615;&#65292;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#20204;&#37117;&#27809;&#26377;&#23398;&#20064;&#21040;&#36825;&#26679;&#30340;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#26102;&#38388;&#27169;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;&#65288;TIN&#65289;&#26469;&#25429;&#25417;&#34892;&#20026;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;&#22235;&#37325;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The history of user behaviors constitutes one of the most significant characteristics in predicting the click-through rate (CTR), owing to their strong semantic and temporal correlation with the target item. While the literature has individually examined each of these correlations, research has yet to analyze them in combination, that is, the quadruple correlation of (behavior semantics, target semantics, behavior temporal, and target temporal). The effect of this correlation on performance and the extent to which existing methods learn it remain unknown. To address this gap, we empirically measure the quadruple correlation and observe intuitive yet robust quadruple patterns. We measure the learned correlation of several representative user behavior methods, but to our surprise, none of them learn such a pattern, especially the temporal one.  In this paper, we propose the Temporal Interest Network (TIN) to capture the quadruple semantic and temporal correlation between behaviors and th
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#21464;&#25442;&#27169;&#22411;&#22312;OOD&#27867;&#21270;&#26041;&#38754;&#23384;&#22312;&#31070;&#31192;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#30740;&#31350;&#20154;&#21592;&#35266;&#23519;&#21040;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#27867;&#21270;&#25968;&#23383;&#36816;&#31639;&#26102;&#30340;&#34892;&#20026;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#65292;&#24182;&#23581;&#35797;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#26410;&#35299;&#20915;&#26412;&#36136;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.08268</link><description>&lt;p&gt;
&#23427;&#20854;&#23454;&#19981;&#37027;&#20040;&#31967;&#31957;&#65306;&#29702;&#35299;&#29983;&#25104;&#21464;&#25442;&#27169;&#22411;&#23545;OOD&#27867;&#21270;&#30340;&#31070;&#31192;&#24615;&#33021;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
It Ain't That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models. (arXiv:2308.08268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08268
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#21464;&#25442;&#27169;&#22411;&#22312;OOD&#27867;&#21270;&#26041;&#38754;&#23384;&#22312;&#31070;&#31192;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#30740;&#31350;&#20154;&#21592;&#35266;&#23519;&#21040;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#27867;&#21270;&#25968;&#23383;&#36816;&#31639;&#26102;&#30340;&#34892;&#20026;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#65292;&#24182;&#23581;&#35797;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#26410;&#35299;&#20915;&#26412;&#36136;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#21464;&#25442;&#27169;&#22411;&#22312;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#27809;&#26377;&#23436;&#20840;&#34987;&#29702;&#35299;&#65292;&#24182;&#19988;&#24182;&#19981;&#24635;&#26159;&#20196;&#20154;&#28385;&#24847;&#12290;&#30740;&#31350;&#20154;&#21592;&#20174;&#22522;&#26412;&#30340;&#25968;&#23398;&#20219;&#21153;&#65288;&#22914;n&#20301;&#25968;&#30340;&#21152;&#27861;&#25110;&#20056;&#27861;&#65289;&#24320;&#22987;&#65292;&#20316;&#20026;&#37325;&#35201;&#35270;&#35282;&#26469;&#30740;&#31350;&#27169;&#22411;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#35266;&#23519;&#21040;&#24403;&#27169;&#22411;&#22312;n&#20301;&#25968;&#36816;&#31639;&#65288;&#20363;&#22914;&#21152;&#27861;&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#38271;&#24230;&#20026;n&#20301;&#30340;&#36755;&#20837;&#19978;&#21487;&#20197;&#25104;&#21151;&#27867;&#21270;&#65288;&#21363;&#20869;&#20998;&#24067;&#27867;&#21270;&#65289;&#65292;&#20294;&#22312;&#38271;&#24230;&#26356;&#38271;&#12289;&#26410;&#35265;&#36807;&#30340;&#24773;&#20917;&#19979;&#65288;&#21363;&#22806;&#20998;&#24067;&#27867;&#21270;&#65289;&#20250;&#22833;&#36133;&#24182;&#19988;&#34920;&#29616;&#31070;&#31192;&#12290;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#20462;&#25913;&#20301;&#32622;&#23884;&#20837;&#12289;&#24494;&#35843;&#21644;&#24341;&#20837;&#26356;&#24191;&#27867;&#25110;&#26356;&#26377;&#25351;&#23548;&#24615;&#30340;&#25968;&#25454;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#35299;&#20915;&#26412;&#36136;&#26426;&#21046;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#30340;&#31283;&#20581;&#24615;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Transformer-based models have achieved remarkable proficiency on solving diverse problems. However, their generalization ability is not fully understood and not always satisfying. Researchers take basic mathematical tasks like n-digit addition or multiplication as important perspectives for investigating their generalization behaviors. Curiously, it is observed that when training on n-digit operations (e.g., additions) in which both input operands are n-digit in length, models generalize successfully on unseen n-digit inputs (in-distribution (ID) generalization), but fail miserably and mysteriously on longer, unseen cases (out-of-distribution (OOD) generalization). Studies try to bridge this gap with workarounds such as modifying position embedding, fine-tuning, and priming with more extensive or instructive data. However, without addressing the essential mechanism, there is hardly any guarantee regarding the robustness of these solutions. We bring this unexplained performan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22635;&#20805;&#21644;&#32622;&#25442;&#25351;&#32441;&#32534;&#30721;&#30340;&#26041;&#27861;&#26469;&#20135;&#29983;&#22256;&#38590;&#23454;&#20363;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#25552;&#20379;&#24179;&#28369;&#19979;&#30028;&#12290;&#36825;&#26041;&#27861;&#36866;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#24179;&#22343;&#38382;&#39064;&#21644;&#36817;&#20284;k.</title><link>http://arxiv.org/abs/2307.07604</link><description>&lt;p&gt;
&#36890;&#36807;&#22635;&#20805;&#21644;&#32622;&#25442;&#25351;&#32441;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#23545;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#25552;&#20379;&#24179;&#28369;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes. (arXiv:2307.07604v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22635;&#20805;&#21644;&#32622;&#25442;&#25351;&#32441;&#32534;&#30721;&#30340;&#26041;&#27861;&#26469;&#20135;&#29983;&#22256;&#38590;&#23454;&#20363;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#25552;&#20379;&#24179;&#28369;&#19979;&#30028;&#12290;&#36825;&#26041;&#27861;&#36866;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#24179;&#22343;&#38382;&#39064;&#21644;&#36817;&#20284;k.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#32441;&#32534;&#30721;&#26041;&#27861;&#26159;&#26368;&#24191;&#27867;&#29992;&#20110;&#30830;&#23450;&#32422;&#26463;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25110;&#38169;&#35823;&#29575;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#24046;&#20998;&#38544;&#31169;&#38382;&#39064;&#65292;&#25105;&#20204;&#24182;&#19981;&#30693;&#36947;&#36866;&#24403;&#30340;&#19979;&#30028;&#65292;&#24182;&#19988;&#21363;&#20351;&#23545;&#20110;&#25105;&#20204;&#30693;&#36947;&#30340;&#38382;&#39064;&#65292;&#19979;&#30028;&#20063;&#19981;&#24179;&#28369;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#35823;&#24046;&#22823;&#20110;&#26576;&#20010;&#38408;&#20540;&#26102;&#21464;&#24471;&#26080;&#24847;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22635;&#20805;&#21644;&#32622;&#25442;&#36716;&#25442;&#24212;&#29992;&#20110;&#25351;&#32441;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22256;&#38590;&#23454;&#20363;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#25552;&#20379;&#26032;&#30340;&#19979;&#30028;&#26469;&#35828;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65306;1. &#20302;&#20934;&#30830;&#24230;&#24773;&#26223;&#19979;&#24046;&#20998;&#38544;&#31169;&#24179;&#22343;&#38382;&#39064;&#30340;&#32039;&#23494;&#19979;&#30028;&#65292;&#36825;&#23588;&#20854;&#24847;&#21619;&#30528;&#26032;&#30340;&#31169;&#26377;1&#31751;&#38382;&#39064;&#30340;&#19979;&#30028; 2. &#36817;&#20284;k
&lt;/p&gt;
&lt;p&gt;
Fingerprinting arguments, first introduced by Bun, Ullman, and Vadhan (STOC 2014), are the most widely used method for establishing lower bounds on the sample complexity or error of approximately differentially private (DP) algorithms. Still, there are many problems in differential privacy for which we don't know suitable lower bounds, and even for problems that we do, the lower bounds are not smooth, and usually become vacuous when the error is larger than some threshold.  In this work, we present a simple method to generate hard instances by applying a padding-and-permuting transformation to a fingerprinting code. We illustrate the applicability of this method by providing new lower bounds in various settings:  1. A tight lower bound for DP averaging in the low-accuracy regime, which in particular implies a new lower bound for the private 1-cluster problem introduced by Nissim, Stemmer, and Vadhan (PODS 2016).  2. A lower bound on the additive error of DP algorithms for approximate k
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22826;&#36203;&#20857;&#27969;&#23548;&#21521;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23450;&#20301;&#31934;&#24230;&#20302;&#21644;&#26080;&#27861;&#20840;&#23616;&#23450;&#20301;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05551</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22826;&#36203;&#20857;&#27969;&#23548;&#21521;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network-enabled Terahertz-based Flow-guided Nanoscale Localization. (arXiv:2307.05551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22826;&#36203;&#20857;&#27969;&#23548;&#21521;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23450;&#20301;&#31934;&#24230;&#20302;&#21644;&#26080;&#27861;&#20840;&#23616;&#23450;&#20301;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32435;&#31859;&#25216;&#26415;&#21644;&#20808;&#36827;&#26448;&#26009;&#30340;&#31185;&#23398;&#36827;&#23637;&#20026;&#20307;&#20869;&#31934;&#20934;&#21307;&#23398;&#30340;&#32435;&#31859;&#23610;&#24230;&#35013;&#32622;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#20854;&#20013;&#21253;&#25324;&#38598;&#25104;&#24863;&#24212;&#12289;&#35745;&#31639;&#12289;&#36890;&#20449;&#12289;&#25968;&#25454;&#21644;&#33021;&#37327;&#23384;&#20648;&#33021;&#21147;&#12290;&#22312;&#20154;&#20307;&#24515;&#34880;&#31649;&#31995;&#32479;&#20013;&#65292;&#36825;&#20123;&#35013;&#32622;&#34987;&#35774;&#24819;&#20026;&#34987;&#21160;&#27969;&#21160;&#24182;&#25345;&#32493;&#24863;&#30693;&#20197;&#20415;&#26816;&#27979;&#35786;&#26029;&#24863;&#20852;&#36259;&#30340;&#20107;&#20214;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#20107;&#20214;&#30340;&#29289;&#29702;&#20301;&#32622;&#65288;&#22914;&#36523;&#20307;&#21306;&#22495;&#65289;&#20998;&#37197;&#32473;&#23427;&#20204;&#65292;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#21040;&#36825;&#20123;&#20107;&#20214;&#30340;&#35786;&#26029;&#20215;&#20540;&#65292;&#36825;&#26159;&#27969;&#23548;&#21521;&#23450;&#20301;&#30340;&#20027;&#35201;&#21629;&#39064;&#12290;&#24403;&#21069;&#30340;&#27969;&#23548;&#21521;&#23450;&#20301;&#26041;&#27861;&#23384;&#22312;&#23450;&#20301;&#31934;&#24230;&#20302;&#21644;&#26080;&#27861;&#22312;&#25972;&#20010;&#24515;&#34880;&#31649;&#31995;&#32479;&#20869;&#26412;&#22320;&#21270;&#20107;&#20214;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26469;&#36827;&#34892;&#23450;&#20301;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23450;&#20301;&#31934;&#24230;&#21644;&#35206;&#30422;&#33539;&#22260;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific advancements in nanotechnology and advanced materials are paving the way toward nanoscale devices for in-body precision medicine; comprising integrated sensing, computing, communication, data and energy storage capabilities. In the human cardiovascular system, such devices are envisioned to be passively flowing and continuously sensing for detecting events of diagnostic interest. The diagnostic value of detecting such events can be enhanced by assigning to them their physical locations (e.g., body region), which is the main proposition of flow-guided localization. Current flow-guided localization approaches suffer from low localization accuracy and they are by-design unable to localize events within the entire cardiovascular system. Toward addressing this issue, we propose the utilization of Graph Neural Networks (GNNs) for this purpose, and demonstrate localization accuracy and coverage enhancements of our proposal over the existing State of the Art (SotA) approaches. Based
&lt;/p&gt;</description></item><item><title>Engression&#26159;&#19968;&#31181;&#38750;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#22238;&#24402;&#25216;&#26415;&#21644;&#39044;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#26679;&#26412;&#33539;&#22260;&#36793;&#30028;&#22806;&#20063;&#33021;&#21487;&#38752;&#22320;&#36827;&#34892;&#22806;&#25512;&#12290;</title><link>http://arxiv.org/abs/2307.00835</link><description>&lt;p&gt;
Engression: &#38750;&#32447;&#24615;&#22238;&#24402;&#30340;&#22806;&#25512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Engression: Extrapolation for Nonlinear Regression?. (arXiv:2307.00835v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00835
&lt;/p&gt;
&lt;p&gt;
Engression&#26159;&#19968;&#31181;&#38750;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#22238;&#24402;&#25216;&#26415;&#21644;&#39044;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#26679;&#26412;&#33539;&#22260;&#36793;&#30028;&#22806;&#20063;&#33021;&#21487;&#38752;&#22320;&#36827;&#34892;&#22806;&#25512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#25512;&#23545;&#20110;&#35768;&#22810;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#24120;&#24120;&#20250;&#36935;&#21040;&#36229;&#20986;&#35757;&#32451;&#26679;&#26412;&#33539;&#22260;&#30340;&#27979;&#35797;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#26469;&#35828;&#65292;&#22806;&#25512;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#36890;&#24120;&#36935;&#21040;&#22256;&#38590;&#65306;&#26641;&#38598;&#25104;&#27169;&#22411;&#22312;&#25903;&#25345;&#33539;&#22260;&#22806;&#25552;&#20379;&#36830;&#32493;&#30340;&#39044;&#27979;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#24448;&#24448;&#21464;&#24471;&#19981;&#21487;&#25511;&#12290;&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#38750;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#20854;&#21487;&#38752;&#24615;&#22312;&#35757;&#32451;&#26679;&#26412;&#33539;&#22260;&#36793;&#30028;&#19981;&#20250;&#31435;&#21363;&#23849;&#28291;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#21517;&#20026;&#8220;engression&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#39044;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#30340;&#20998;&#24067;&#22238;&#24402;&#25216;&#26415;&#65292;&#20854;&#20013;&#22122;&#22768;&#28155;&#21152;&#21040;&#21327;&#21464;&#37327;&#19978;&#24182;&#24212;&#29992;&#38750;&#32447;&#24615;&#36716;&#25442;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#36890;&#24120;&#36866;&#29992;&#20110;&#35768;&#22810;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;engression&#21487;&#20197;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#25104;&#21151;&#36827;&#34892;&#22806;&#25512;&#65292;&#20363;&#22914;&#20005;&#26684;&#38480;&#21046;&#22122;&#22768;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extrapolation is crucial in many statistical and machine learning applications, as it is common to encounter test data outside the training support. However, extrapolation is a considerable challenge for nonlinear models. Conventional models typically struggle in this regard: while tree ensembles provide a constant prediction beyond the support, neural network predictions tend to become uncontrollable. This work aims at providing a nonlinear regression methodology whose reliability does not break down immediately at the boundary of the training support. Our primary contribution is a new method called `engression' which, at its core, is a distributional regression technique for pre-additive noise models, where the noise is added to the covariates before applying a nonlinear transformation. Our experimental results indicate that this model is typically suitable for many real data sets. We show that engression can successfully perform extrapolation under some assumptions such as a strictl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#26080;&#35770;&#32422;&#26463;&#20989;&#25968;&#30340;&#29305;&#24615;&#22914;&#20309;&#65292;&#37117;&#33021;&#28385;&#36275;&#23433;&#20840;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.17815</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#20449;&#24515;&#39044;&#27979;&#23454;&#29616;&#20855;&#22791;&#24418;&#24335;&#23433;&#20840;&#20445;&#35777;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization with Formal Safety Guarantees via Online Conformal Prediction. (arXiv:2306.17815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#26080;&#35770;&#32422;&#26463;&#20989;&#25968;&#30340;&#29305;&#24615;&#22914;&#20309;&#65292;&#37117;&#33021;&#28385;&#36275;&#23433;&#20840;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#38646;&#38454;&#20248;&#21270;&#26159;&#37329;&#34701;&#12289;&#29289;&#29702;&#21644;&#24037;&#31243;&#31561;&#39046;&#22495;&#24212;&#29992;&#30340;&#26680;&#24515;&#22522;&#26412;&#25805;&#20316;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#24418;&#24335;&#20013;&#65292;&#35774;&#35745;&#32773;&#39034;&#24207;&#23581;&#35797;&#20505;&#36873;&#35299;&#65292;&#24182;&#20174;&#31995;&#32479;&#20013;&#25509;&#25910;&#21040;&#20851;&#20110;&#27599;&#20010;&#23581;&#35797;&#20540;&#30340;&#22122;&#22768;&#21453;&#39304;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#36824;&#25552;&#20379;&#20102;&#26377;&#20851;&#23581;&#35797;&#35299;&#30340;&#23433;&#20840;&#24615;&#30340;&#21453;&#39304;&#65292;&#24182;&#19988;&#20248;&#21270;&#22120;&#34987;&#38480;&#21046;&#22312;&#25972;&#20010;&#20248;&#21270;&#36807;&#31243;&#20013;&#23581;&#35797;&#30340;&#19981;&#23433;&#20840;&#35299;&#30340;&#25968;&#37327;&#19978;&#12290;&#22312;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#30340;&#26041;&#27861;&#19978;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;SAFEOPT&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#21482;&#35201;&#28385;&#36275;&#23545;&#23433;&#20840;&#32422;&#26463;&#20989;&#25968;&#30340;&#20005;&#26684;&#20551;&#35774;&#65292;&#23601;&#33021;&#22815;&#20197;&#21487;&#25511;&#30340;&#27010;&#29575;&#22312;&#21453;&#39304;&#22122;&#22768;&#19978;&#36991;&#20813;&#36873;&#25321;&#20219;&#20309;&#19981;&#23433;&#20840;&#30340;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;BO&#30340;&#26041;&#27861;&#65292;&#26080;&#35770;&#32422;&#26463;&#20989;&#25968;&#30340;&#29305;&#24615;&#22914;&#20309;&#65292;&#37117;&#33021;&#28385;&#36275;&#23433;&#20840;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box zero-th order optimization is a central primitive for applications in fields as diverse as finance, physics, and engineering. In a common formulation of this problem, a designer sequentially attempts candidate solutions, receiving noisy feedback on the value of each attempt from the system. In this paper, we study scenarios in which feedback is also provided on the safety of the attempted solution, and the optimizer is constrained to limit the number of unsafe solutions that are tried throughout the optimization process. Focusing on methods based on Bayesian optimization (BO), prior art has introduced an optimization scheme -- referred to as SAFEOPT -- that is guaranteed not to select any unsafe solution with a controllable probability over feedback noise as long as strict assumptions on the safety constraint function are met. In this paper, a novel BO-based approach is introduced that satisfies safety requirements irrespective of properties of the constraint function. This s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24310;&#36831;&#32447;&#25506;&#27979;&#22120;&#26102;&#31354;&#22810;&#20107;&#20214;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#20197;&#25104;&#21151;&#22320;&#37325;&#26500;&#22810;&#20010;&#31890;&#23376;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#22352;&#26631;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#39640;&#25928;&#21644;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2306.09359</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24310;&#36831;&#32447;&#25506;&#27979;&#22120;&#26102;&#31354;&#22810;&#20107;&#20214;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Based Spatiotemporal Multi-Event Reconstruction for Delay Line Detectors. (arXiv:2306.09359v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24310;&#36831;&#32447;&#25506;&#27979;&#22120;&#26102;&#31354;&#22810;&#20107;&#20214;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#20197;&#25104;&#21151;&#22320;&#37325;&#26500;&#22810;&#20010;&#31890;&#23376;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#22352;&#26631;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#39640;&#25928;&#21644;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29289;&#29702;&#23398;&#20013;&#65292;&#35266;&#23519;&#38750;&#24120;&#30701;&#30340;&#26102;&#38388;&#31383;&#21475;&#20869;&#20004;&#20010;&#25110;&#26356;&#22810;&#31890;&#23376;&#30340;&#20301;&#32622;&#19968;&#30452;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#23545;&#20110;&#20302;&#33021;&#30005;&#23376;&#65292;&#21487;&#20197;&#20351;&#29992;&#24494;&#36890;&#36947;&#26495;&#21644;&#24310;&#36831;&#32447;&#30340;&#32452;&#21512;&#26500;&#25104;&#24310;&#36831;&#32447;&#25506;&#27979;&#22120;&#65292;&#20197;&#35835;&#20986;&#20837;&#23556;&#31890;&#23376;&#30340;&#20301;&#32622;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23436;&#20840;&#37325;&#26500;&#22810;&#20010;&#31890;&#23376;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#22352;&#26631;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#20010;&#25509;&#36817;&#30340;&#31890;&#23376;&#65292;&#20256;&#32479;&#30340;&#23792;&#20540;&#26597;&#25214;&#31639;&#27861;&#26080;&#27861;&#26377;&#25928;&#35782;&#21035;&#21644;&#37325;&#26500;&#20107;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#31354;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#21367;&#31215;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#30701;&#26102;&#38388;&#31383;&#21475;&#20869;&#25552;&#21462;&#21644;&#32452;&#21512;&#22810;&#20010;&#31890;&#23376;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#25104;&#21151;&#22320;&#37325;&#24314;&#20102;&#21253;&#21547;&#22810;&#36798;&#22235;&#20010;&#31890;&#23376;&#30340;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate observation of two or more particles within a very narrow time window has always been a challenge in modern physics. It creates the possibility of correlation experiments, such as the ground-breaking Hanbury Brown-Twiss experiment, leading to new physical insights. For low-energy electrons, one possibility is to use a microchannel plate with subsequent delay lines for the readout of the incident particle hits, a setup called a Delay Line Detector. The spatial and temporal coordinates of more than one particle can be fully reconstructed outside a region called the dead radius. For interesting events, where two electrons are close in space and time, the determination of the individual positions of the electrons requires elaborate peak finding algorithms. While classical methods work well with single particle hits, they fail to identify and reconstruct events caused by multiple nearby particles. To address this challenge, we present a new spatiotemporal machine learning model to 
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#8212;&#8212;&#36234;&#30028;&#36951;&#24536;&#65292;&#24403;&#32473;&#23450;&#31867;&#21035;&#24341;&#20837;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#23427;&#20250;&#26174;&#30528;&#21066;&#24369;&#35813;&#31867;&#21035;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.00427</link><description>&lt;p&gt;
&#38024;&#23545;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#30340;&#36807;&#24230;&#36951;&#24536;&#65306;&#36830;&#32493;&#23398;&#20064;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution forgetting: vulnerability of continual learning to intra-class distribution shift. (arXiv:2306.00427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00427
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#8212;&#8212;&#36234;&#30028;&#36951;&#24536;&#65292;&#24403;&#32473;&#23450;&#31867;&#21035;&#24341;&#20837;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#23427;&#20250;&#26174;&#30528;&#21066;&#24369;&#35813;&#31867;&#21035;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26159;&#35753;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#24037;&#20316;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#22312;&#32852;&#21512;&#23398;&#20064;&#20013;&#65292;&#20154;&#20204;&#24050;&#32463;&#30693;&#36947;&#30001;&#24847;&#22270;&#25915;&#20987;&#25110;&#29615;&#22659;&#25200;&#21160;&#24341;&#36215;&#30340;&#36234;&#30028;&#38382;&#39064;&#20005;&#37325;&#24433;&#21709;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#36830;&#32493;&#23398;&#20064;&#35774;&#32622;&#20013;&#30001;&#36234;&#30028;&#38382;&#39064;&#24341;&#36215;&#30340;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#36234;&#30028;&#36951;&#24536;&#65288;OODF&#65289;&#12290;&#22312;&#36830;&#32493;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#38024;&#23545;&#32473;&#23450;&#31867;&#21035;&#65292;&#24341;&#20837;&#31867;&#20869;&#20998;&#24067;&#36716;&#31227;&#26174;&#30528;&#21066;&#24369;&#20102;&#21518;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#35813;&#31867;&#21035;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#31181;&#29616;&#35937;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#32780;&#35328;&#26159;&#29305;&#27530;&#30340;&#65292;&#22240;&#20026;&#21516;&#26679;&#32423;&#21035;&#30340;&#20998;&#24067;&#36716;&#31227;&#21482;&#26377;&#24494;&#19981;&#36275;&#36947;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) is an important technique to allow artificial neural networks to work in open environments. CL enables a system to learn new tasks without severe interference to its performance on old tasks, i.e., overcome the problems of catastrophic forgetting. In joint learning, it is well known that the out-of-distribution (OOD) problem caused by intentional attacks or environmental perturbations will severely impair the ability of networks to generalize. In this work, we reported a special form of catastrophic forgetting raised by the OOD problem in continual learning settings, and we named it out-of-distribution forgetting (OODF). In continual image classification tasks, we found that for a given category, introducing an intra-class distribution shift significantly impaired the recognition accuracy of CL methods for that category during subsequent learning. Interestingly, this phenomenon is special for CL as the same level of distribution shift had only negligible effects
&lt;/p&gt;</description></item><item><title>FedMR&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#37325;&#32452;&#30340;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#36890;&#36807;&#28151;&#27927;&#21644;&#37325;&#32452;&#27599;&#20010;&#23618;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#32531;&#35299;&#23376;&#20248;&#25110;&#26377;&#20559;&#23616;&#37096;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.10730</link><description>&lt;p&gt;
FedMR&#65306;&#22522;&#20110;&#27169;&#22411;&#37325;&#32452;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedMR: Federated Learning via Model Recombination. (arXiv:2305.10730v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10730
&lt;/p&gt;
&lt;p&gt;
FedMR&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#37325;&#32452;&#30340;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#36890;&#36807;&#28151;&#27927;&#21644;&#37325;&#32452;&#27599;&#20010;&#23618;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#32531;&#35299;&#23376;&#20248;&#25110;&#26377;&#20559;&#23616;&#37096;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#24471;&#23458;&#25143;&#31471;&#22312;&#19981;&#27844;&#38706;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20840;&#23616;&#27169;&#22411;&#35757;&#32451;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20110;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedMR&#65288;&#32852;&#37030;&#27169;&#22411;&#37325;&#32452;&#65289;&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#23545;&#25910;&#38598;&#21040;&#30340;&#26412;&#22320;&#27169;&#22411;&#30340;&#27599;&#20010;&#23618;&#36827;&#34892;&#28151;&#27927;&#21644;&#37325;&#32452;&#65292;&#22312;&#23458;&#25143;&#31471;&#36827;&#34892;&#23616;&#37096;&#35757;&#32451;&#26102;&#21487;&#20197;&#33719;&#24471;&#20855;&#26377;&#22810;&#26679;&#30340;&#21021;&#22987;&#27169;&#22411;&#30340;&#26032;&#27169;&#22411;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#23376;&#20248;&#25110;&#26377;&#20559;&#23616;&#37096;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Federated Learning (FL) enables global model training across clients without compromising their raw data, existing Federated Averaging (FedAvg)-based methods suffer from the problem of low inference performance, especially for unevenly distributed data among clients. This is mainly because i) FedAvg initializes client models with the same global models, which makes the local training hard to escape from the local search for optimal solutions; and ii) by averaging model parameters in a coarse manner, FedAvg eclipses the individual characteristics of local models. To address such issues that strongly limit the inference capability of FL, we propose a novel and effective FL paradigm named FedMR (Federated Model Recombination). Unlike conventional FedAvg-based methods, the cloud server of FedMR shuffles each layer of collected local models and recombines them to achieve new models for local training on clients. Due to the diversified initialization models for clients coupled with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21487;&#35777;&#40657;&#30418;&#25915;&#20987;&#65292;&#33021;&#22815;&#20445;&#35777;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#35774;&#35745;&#20102;&#22810;&#31181;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2304.04343</link><description>&lt;p&gt;
&#21487;&#35777;&#40657;&#30418;&#25915;&#20987;&#65306;&#30830;&#20445;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;
&lt;/p&gt;
&lt;p&gt;
Certifiable Black-Box Attack: Ensuring Provably Successful Attack for Adversarial Examples. (arXiv:2304.04343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21487;&#35777;&#40657;&#30418;&#25915;&#20987;&#65292;&#33021;&#22815;&#20445;&#35777;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#35774;&#35745;&#20102;&#22810;&#31181;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#30772;&#22351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24378;&#22823;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#36890;&#36807;&#36845;&#20195;&#26597;&#35810;&#30446;&#26631;&#27169;&#22411;&#21644;/&#25110;&#21033;&#29992;&#26412;&#22320;&#20195;&#29702;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#26469;&#21046;&#20316;&#23545;&#25239;&#26679;&#26412;&#12290;&#24403;&#23454;&#39564;&#35774;&#35745;&#25915;&#20987;&#26102;&#65292;&#25915;&#20987;&#26159;&#21542;&#25104;&#21151;&#23545;&#25915;&#20987;&#32773;&#26469;&#35828;&#20173;&#28982;&#26159;&#26410;&#30693;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#20462;&#25913;&#38543;&#26426;&#24179;&#28369;&#24615;&#29702;&#35770;&#65292;&#39318;&#27425;&#30740;&#31350;&#20102;&#21487;&#35777;&#40657;&#30418;&#25915;&#20987;&#30340;&#26032;&#33539;&#20363;&#65292;&#33021;&#22815;&#20445;&#35777;&#21046;&#20316;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#20026;&#27492;&#35774;&#35745;&#20102;&#22810;&#31181;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box adversarial attacks have shown strong potential to subvert machine learning models. Existing black-box adversarial attacks craft the adversarial examples by iteratively querying the target model and/or leveraging the transferability of a local surrogate model. Whether such attack can succeed remains unknown to the adversary when empirically designing the attack. In this paper, to our best knowledge, we take the first step to study a new paradigm of adversarial attacks -- certifiable black-box attack that can guarantee the attack success rate of the crafted adversarial examples. Specifically, we revise the randomized smoothing to establish novel theories for ensuring the attack success rate of the adversarial examples. To craft the adversarial examples with the certifiable attack success rate (CASR) guarantee, we design several novel techniques, including a randomized query method to query the target model, an initialization method with smoothed self-supervised perturbation to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#24615;&#22122;&#22768;&#21644;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#25216;&#26415;&#26469;&#27169;&#25311;&#30005;&#23376;&#26174;&#24494;&#38236;&#27491;&#21521;&#31639;&#23376;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#31890;&#23376;&#23450;&#20301;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.02011</link><description>&lt;p&gt;
FakET: &#21033;&#29992;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#27169;&#25311;&#20919;&#20923;&#30005;&#23376;&#26029;&#23618;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer. (arXiv:2304.02011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#24615;&#22122;&#22768;&#21644;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#25216;&#26415;&#26469;&#27169;&#25311;&#30005;&#23376;&#26174;&#24494;&#38236;&#27491;&#21521;&#31639;&#23376;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#31890;&#23376;&#23450;&#20301;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31890;&#23376;&#23450;&#20301;&#21644;&#20998;&#31867;&#26159;&#35745;&#31639;&#26174;&#24494;&#23398;&#20013;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#36825;&#20123;&#30417;&#30563;&#24335;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#28857;&#26159;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36890;&#24120;&#26159;&#19982;&#27169;&#25311;&#36879;&#23556;&#30005;&#23376;&#26174;&#24494;&#38236;&#29289;&#29702;&#30340;&#22797;&#26434;&#25968;&#20540;&#27491;&#21521;&#27169;&#22411;&#20013;&#30340;&#31890;&#23376;&#27169;&#22411;&#32467;&#21512;&#29983;&#25104;&#30340;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#26426;&#23454;&#29616;&#38750;&#24120;&#32791;&#36153;&#35745;&#31639;&#36164;&#28304;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#33539;&#22260;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#24615;&#22122;&#22768;&#21644;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#25216;&#26415;&#27169;&#25311;&#30005;&#23376;&#26174;&#24494;&#38236;&#27491;&#21521;&#31639;&#23376;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#24050;&#32463;&#24314;&#31435;&#30340;&#29366;&#24577;&#20043;&#19968;&#23545;&#23450;&#20301;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#19982;&#22522;&#20934;&#27979;&#35797;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21152;&#36895;&#20102;&#36816;&#31639;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Particle localization and -classification constitute two of the most fundamental problems in computational microscopy. In recent years, deep learning based approaches have been introduced for these tasks with great success. A key shortcoming of these supervised learning methods is their need for large training data sets, typically generated from particle models in conjunction with complex numerical forward models simulating the physics of transmission electron microscopes. Computer implementations of such forward models are computationally extremely demanding and limit the scope of their applicability. In this paper we propose a simple method for simulating the forward operator of an electron microscope based on additive noise and Neural Style Transfer techniques. We evaluate the method on localization and classification tasks using one of the established state-of-the-art architectures showing performance on par with the benchmark. In contrast to previous approaches, our method acceler
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;RoConE&#65292;&#23427;&#20801;&#35768;&#23398;&#20064;&#20851;&#31995;&#27169;&#24335;&#24182;&#25552;&#39640;&#20102;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11858</link><description>&lt;p&gt;
&#23545;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#36923;&#36753;&#26597;&#35810;&#24212;&#31572;&#30340;&#20851;&#31995;&#27169;&#24335;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Relational Patterns for Logical Query Answering over Knowledge Graphs. (arXiv:2303.11858v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;RoConE&#65292;&#23427;&#20801;&#35768;&#23398;&#20064;&#20851;&#31995;&#27169;&#24335;&#24182;&#25552;&#39640;&#20102;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#36827;&#34892;&#19968;&#38454;&#36923;&#36753;&#65288;FOL&#65289;&#26597;&#35810;&#30340;&#22238;&#31572;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;KG&#19981;&#23436;&#25972;&#24615;&#32780;&#23548;&#33268;&#30340;&#12290;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#36923;&#36753;&#26597;&#35810;&#30340;&#20302;&#32500;&#24230;&#21521;&#37327;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;KG&#34920;&#29616;&#20986;&#23545;&#31216;&#24615;&#21644;&#32452;&#21512;&#24615;&#31561;&#20851;&#31995;&#27169;&#24335;&#65292;&#24314;&#27169;&#36825;&#20123;&#27169;&#24335;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26597;&#35810;&#23884;&#20837;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#24335;&#22312;&#26597;&#35810;&#23884;&#20837;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#23578;&#26410;&#22312;&#25991;&#29486;&#20013;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20801;&#35768;&#23398;&#20064;&#20851;&#31995;&#27169;&#24335;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#21152;&#24378;FOL&#26597;&#35810;&#25512;&#29702;&#30340;&#27169;&#24335;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;RoConE&#65292;&#23427;&#23558;&#26597;&#35810;&#21306;&#22495;&#23450;&#20041;&#20026;&#20960;&#20309;&#38181;&#20307;&#65292;&#24182;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#26059;&#36716;&#20195;&#25968;&#26597;&#35810;&#31639;&#23376;&#12290;RoConE&#32467;&#21512;&#20102;&#20960;&#20309;&#38181;&#20307;&#20316;&#20026;&#21487;&#20197;&#26126;&#30830;&#23450;&#20041;&#26597;&#35810;&#34920;&#31034;&#30340;&#20960;&#20309;&#34920;&#31034;&#21644;&#26059;&#36716;&#20195;&#25968;&#26597;&#35810;&#31639;&#23376;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering first-order logical (FOL) queries over knowledge graphs (KG) remains a challenging task mainly due to KG incompleteness. Query embedding approaches this problem by computing the low-dimensional vector representations of entities, relations, and logical queries. KGs exhibit relational patterns such as symmetry and composition and modeling the patterns can further enhance the performance of query embedding models. However, the role of such patterns in answering FOL queries by query embedding models has not been yet studied in the literature. In this paper, we fill in this research gap and empower FOL queries reasoning with pattern inference by introducing an inductive bias that allows for learning relation patterns. To this end, we develop a novel query embedding method, RoConE, that defines query regions as geometric cones and algebraic query operators by rotations in complex space. RoConE combines the advantages of Cone as a well-specified geometric representation for query e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#39640;&#32500;&#24230;&#29699;&#20307;&#19978;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#20351;&#29992;SGD&#31639;&#27861;&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11602</link><description>&lt;p&gt;
&#24102;&#24212;&#29992;&#20110;&#21464;&#20998;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#30340;&#21442;&#25968;&#21270;&#29699;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convergence of stochastic gradient descent on parameterized sphere with applications to variational Monte Carlo simulation. (arXiv:2303.11602v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#39640;&#32500;&#24230;&#29699;&#20307;&#19978;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#20351;&#29992;SGD&#31639;&#27861;&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#30001;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#20026;&#24120;&#25968;&#20493;&#30340;&#39640;&#32500;&#29699;&#19978;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31867;&#22411;&#31639;&#27861;&#12290;&#25105;&#20204;&#20026;&#26377;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#25968;&#20540;&#19978;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#39318;&#27425;&#25552;&#20379;&#20102;&#26080;&#30417;&#30563;&#35774;&#32622;&#30340;&#25910;&#25947;&#35777;&#26126;&#65292;&#35813;&#35774;&#32622;&#23545;&#24212;&#20110;&#37327;&#23376;&#29289;&#29702;&#23398;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#21464;&#20998;&#33945;&#29305;&#21345;&#32599;&#65288;VMC&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze stochastic gradient descent (SGD) type algorithms on a high-dimensional sphere which is parameterized by a neural network up to a normalization constant. We provide a new algorithm for the setting of supervised learning and show its convergence both theoretically and numerically. We also provide the first proof of convergence for the unsupervised setting, which corresponds to the widely used variational Monte Carlo (VMC) method in quantum physics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#31354;&#38388;&#20998;&#35299;&#26469;&#36817;&#20284;&#30005;&#21147;&#31995;&#32479;&#21160;&#21147;&#23398;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#21152;&#36895;&#20223;&#30495;&#65292;&#25552;&#39640;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.10256</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#31354;&#38388;&#20998;&#35299;&#22312;&#30005;&#21147;&#31995;&#32479;&#21160;&#21147;&#23398;&#20013;&#27714;&#35299;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Solving Differential-Algebraic Equations in Power Systems Dynamics with Neural Networks and Spatial Decomposition. (arXiv:2303.10256v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#31354;&#38388;&#20998;&#35299;&#26469;&#36817;&#20284;&#30005;&#21147;&#31995;&#32479;&#21160;&#21147;&#23398;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#21152;&#36895;&#20223;&#30495;&#65292;&#25552;&#39640;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#30001;&#19968;&#32452;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#25551;&#36848;&#12290;&#26102;&#38388;&#22495;&#20223;&#30495;&#29992;&#20110;&#29702;&#35299;&#31995;&#32479;&#21160;&#24577;&#30340;&#28436;&#21464;&#12290;&#30001;&#20110;&#31995;&#32479;&#30340;&#21018;&#24230;&#38656;&#35201;&#20351;&#29992;&#31934;&#32454;&#31163;&#25955;&#21270;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#22240;&#27492;&#36825;&#20123;&#20223;&#30495;&#21487;&#33021;&#20855;&#26377;&#35745;&#31639;&#20195;&#20215;&#36739;&#39640;&#30340;&#29305;&#28857;&#12290;&#36890;&#36807;&#22686;&#21152;&#20801;&#35768;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#25105;&#20204;&#26088;&#22312;&#21152;&#24555;&#36825;&#26679;&#30340;&#20223;&#30495;&#12290;&#26412;&#25991;&#20351;&#29992;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#23613;&#31649;&#21508;&#20010;&#32452;&#20214;&#20351;&#29992;&#20195;&#25968;&#21644;&#24494;&#20998;&#26041;&#31243;&#26469;&#25551;&#36848;&#65292;&#20294;&#23427;&#20204;&#30340;&#32806;&#21512;&#20165;&#28041;&#21450;&#20195;&#25968;&#26041;&#31243;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26469;&#36817;&#20284;&#32452;&#20214;&#29366;&#24577;&#28436;&#21464;&#65292;&#20174;&#32780;&#20135;&#29983;&#24555;&#36895;&#12289;&#20934;&#30830;&#21644;&#25968;&#20540;&#31283;&#23450;&#30340;&#36817;&#20284;&#22120;&#65292;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#26356;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;&#20026;&#20102;&#35299;&#37322;&#32593;&#32476;&#23545;&#32452;&#20214;&#20197;&#21450;&#32452;&#20214;&#23545;&#32593;&#32476;&#30340;&#24433;&#21709;&#65292;NN&#23558;&#32806;&#21512;&#20195;&#25968;&#21464;&#37327;&#30340;&#26102;&#38388;&#28436;&#21270;&#20316;&#20026;&#20854;&#39044;&#27979;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#26368;&#21021;&#20351;&#29992;&#31354;&#38388;&#20998;&#35299;&#26041;&#27861;&#26469;&#20272;&#35745;NN&#65292;&#20854;&#20013;&#31995;&#32479;&#34987;&#20998;&#25104;&#31354;&#38388;&#21306;&#22495;&#65292;&#27599;&#20010;&#21306;&#22495;&#26377;&#21333;&#29420;&#30340;NN&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;NN&#30340;&#20223;&#30495;&#19982;&#20256;&#32479;&#30340;&#25968;&#20540;&#31215;&#20998;&#26041;&#26696;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamics of the power system are described by a system of differential-algebraic equations. Time-domain simulations are used to understand the evolution of the system dynamics. These simulations can be computationally expensive due to the stiffness of the system which requires the use of finely discretized time-steps. By increasing the allowable time-step size, we aim to accelerate such simulations. In this paper, we use the observation that even though the individual components are described using both algebraic and differential equations, their coupling only involves algebraic equations. Following this observation, we use Neural Networks (NNs) to approximate the components' state evolution, leading to fast, accurate, and numerically stable approximators, which enable larger time-steps. To account for effects of the network on the components and vice-versa, the NNs take the temporal evolution of the coupling algebraic variables as an input for their prediction. We initially estima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#40657;&#30418;&#25915;&#20987;&#20113;&#26381;&#21153;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#30452;&#25509;&#24212;&#29992;&#20110;&#20113;API&#32780;&#19981;&#26159;&#26412;&#22320;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#21033;&#29992;&#36127;&#36733;&#24179;&#34913;&#23454;&#29616;&#20102;&#25915;&#20987;&#26102;&#38388;&#30340;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2210.16371</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#40657;&#30418;&#25915;&#20987;&#20113;&#26381;&#21153;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Distributed Black-box Attack against Image Classification Cloud Services. (arXiv:2210.16371v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#40657;&#30418;&#25915;&#20987;&#20113;&#26381;&#21153;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#30452;&#25509;&#24212;&#29992;&#20110;&#20113;API&#32780;&#19981;&#26159;&#26412;&#22320;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#21033;&#29992;&#36127;&#36733;&#24179;&#34913;&#23454;&#29616;&#20102;&#25915;&#20987;&#26102;&#38388;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#21487;&#20197;&#27450;&#39575;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#22312;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#32467;&#26500;&#21644;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#23545;&#22270;&#20687;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25253;&#21578;&#20102;&#36229;&#36807;95%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#26597;&#35810;&#27425;&#25968;&#23569;&#20110;1000&#27425;&#12290;&#28982;&#21518;&#20135;&#29983;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#40657;&#30418;&#25915;&#20987;&#26159;&#21542;&#24050;&#32463;&#25104;&#20026;&#20381;&#36182;&#20113;API&#23454;&#29616;&#22270;&#20687;&#20998;&#31867;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#30495;&#27491;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#25104;&#21151;&#29575;&#21644;&#20943;&#23569;&#26597;&#35810;&#27425;&#25968;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#40657;&#30418;&#25915;&#20987;&#20113;API&#32780;&#35328;&#65292;&#25915;&#20987;&#25152;&#38656;&#30340;&#26102;&#38388;&#20063;&#26159;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#25991;&#23558;&#40657;&#30418;&#25915;&#20987;&#30452;&#25509;&#24212;&#29992;&#20110;&#20113;API&#65292;&#32780;&#19981;&#26159;&#26412;&#22320;&#27169;&#22411;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#22312;&#22270;&#20687;&#32534;&#30721;&#21644;&#39044;&#22788;&#29702;&#20043;&#21069;&#24212;&#29992;&#25200;&#21160;&#36896;&#25104;&#30340;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#36127;&#36733;&#24179;&#34913;&#23454;&#29616;&#20102;&#20998;&#24067;&#24335;&#40657;&#30418;&#25915;&#20987;&#65292;&#21487;&#20197;&#23558;&#25915;&#20987;&#26102;&#38388;&#32553;&#30701;&#32422;&#20116;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box adversarial attacks can fool image classifiers into misclassifying images without requiring access to model structure and weights. Recent studies have reported attack success rates of over 95% with less than 1,000 queries. The question then arises of whether black-box attacks have become a real threat against IoT devices that rely on cloud APIs to achieve image classification. To shed some light on this, note that prior research has primarily focused on increasing the success rate and reducing the number of queries. However, another crucial factor for black-box attacks against cloud APIs is the time required to perform the attack. This paper applies black-box attacks directly to cloud APIs rather than to local models, thereby avoiding mistakes made in prior research that applied the perturbation before image encoding and pre-processing. Further, we exploit load balancing to enable distributed black-box attacks that can reduce the attack time by a factor of about five for both
&lt;/p&gt;</description></item></channel></rss>