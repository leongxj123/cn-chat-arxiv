<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Winner-Take-All&#65288;WTA&#65289;&#36873;&#25321;&#24615;&#21644;&#29983;&#29289;&#21551;&#21457;&#30340;&#31283;&#24577;&#26426;&#21046;&#30456;&#32467;&#21512;&#30340;&#8220;&#33258;&#23450;&#20041;&#30446;&#26631;&#8221;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#36793;&#32536;AI&#30828;&#20214;&#19978;&#30340;&#35745;&#31639;&#36164;&#28304;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12116</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#23450;&#20041;&#29983;&#29289;&#21551;&#21457;&#30446;&#26631;&#30340;&#26080;&#30417;&#30563;&#31471;&#21040;&#31471;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Unsupervised End-to-End Training with a Self-Defined Bio-Inspired Target
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Winner-Take-All&#65288;WTA&#65289;&#36873;&#25321;&#24615;&#21644;&#29983;&#29289;&#21551;&#21457;&#30340;&#31283;&#24577;&#26426;&#21046;&#30456;&#32467;&#21512;&#30340;&#8220;&#33258;&#23450;&#20041;&#30446;&#26631;&#8221;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#36793;&#32536;AI&#30828;&#20214;&#19978;&#30340;&#35745;&#31639;&#36164;&#28304;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#33258;&#30417;&#30563;&#23398;&#20064;&#65289;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#25110;&#32773;&#37319;&#29992;&#36890;&#36807;&#31867;&#20284;Hebbian&#23398;&#20064;&#30340;&#29983;&#29289;&#21551;&#21457;&#26041;&#27861;&#36880;&#23618;&#35757;&#32451;&#65292;&#20351;&#29992;&#19982;&#30417;&#30563;&#23398;&#20064;&#19981;&#20860;&#23481;&#30340;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#32593;&#32476;&#26368;&#32456;&#23618;&#30340;&#32988;&#32773;&#36890;&#21507;&#65288;WTA&#65289;&#36873;&#25321;&#24615;&#30340;&#8220;&#33258;&#23450;&#20041;&#30446;&#26631;&#8221;&#65292;&#24182;&#36890;&#36807;&#29983;&#29289;&#21551;&#21457;&#30340;&#31283;&#24577;&#26426;&#21046;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12116v1 Announce Type: cross  Abstract: Current unsupervised learning methods depend on end-to-end training via deep learning techniques such as self-supervised learning, with high computational requirements, or employ layer-by-layer training using bio-inspired approaches like Hebbian learning, using local learning rules incompatible with supervised learning. Both approaches are problematic for edge AI hardware that relies on sparse computational resources and would strongly benefit from alternating between unsupervised and supervised learning phases - thus leveraging widely available unlabeled data from the environment as well as labeled training datasets. To solve this challenge, in this work, we introduce a 'self-defined target' that uses Winner-Take-All (WTA) selectivity at the network's final layer, complemented by regularization through biologically inspired homeostasis mechanism. This approach, framework-agnostic and compatible with both global (Backpropagation) and l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20272;&#35745;&#38543;&#26426;&#36882;&#24402;&#26641;&#20013;&#39030;&#28857;&#21040;&#36798;&#39034;&#24207;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Jordan&#20013;&#24515;&#24615;&#24230;&#37327;&#30340;&#39034;&#24207;&#20272;&#35745;&#22120;&#65292;&#24182;&#35777;&#26126;&#20854;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.09755</link><description>&lt;p&gt;
&#20272;&#35745;&#38543;&#26426;&#36882;&#24402;&#26641;&#30340;&#21382;&#21490;
&lt;/p&gt;
&lt;p&gt;
Estimating the history of a random recursive tree
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20272;&#35745;&#38543;&#26426;&#36882;&#24402;&#26641;&#20013;&#39030;&#28857;&#21040;&#36798;&#39034;&#24207;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Jordan&#20013;&#24515;&#24615;&#24230;&#37327;&#30340;&#39034;&#24207;&#20272;&#35745;&#22120;&#65292;&#24182;&#35777;&#26126;&#20854;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20272;&#35745;&#38543;&#26426;&#36882;&#24402;&#26641;&#20013;&#39030;&#28857;&#21040;&#36798;&#39034;&#24207;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#22522;&#26412;&#27169;&#22411;&#65306;&#22343;&#21248;&#36830;&#25509;&#27169;&#22411;&#21644;&#32447;&#24615;&#20248;&#20808;&#36830;&#25509;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Jordan&#20013;&#24515;&#24615;&#24230;&#37327;&#30340;&#39034;&#24207;&#20272;&#35745;&#22120;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#26063;&#39118;&#38505;&#24230;&#37327;&#26469;&#37327;&#21270;&#25490;&#24207;&#36807;&#31243;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#36825;&#20010;&#38382;&#39064;&#24314;&#31435;&#20102;&#26497;&#23567;-&#26368;&#22823;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#20248;&#20110;&#22522;&#20110;&#24230;&#25968;&#21644;&#35889;&#25490;&#24207;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09755v1 Announce Type: cross  Abstract: This paper studies the problem of estimating the order of arrival of the vertices in a random recursive tree. Specifically, we study two fundamental models: the uniform attachment model and the linear preferential attachment model. We propose an order estimator based on the Jordan centrality measure and define a family of risk measures to quantify the quality of the ordering procedure. Moreover, we establish a minimax lower bound for this problem, and prove that the proposed estimator is nearly optimal. Finally, we numerically demonstrate that the proposed estimator outperforms degree-based and spectral ordering procedures.
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#22686;&#24378;&#19981;&#36807;&#26159;&#26356;&#22909;&#22320;&#24494;&#35843;&#27169;&#22411;&#65292;&#38646;&#21761;&#24577;&#21644;&#23569;&#26679;&#26412;&#25968;&#25454;&#29983;&#25104;&#21487;&#25552;&#39640;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.14895</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#24050;&#27515;&#65292;&#25968;&#25454;&#22686;&#24378;&#19975;&#23681;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation is Dead, Long Live Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14895
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#19981;&#36807;&#26159;&#26356;&#22909;&#22320;&#24494;&#35843;&#27169;&#22411;&#65292;&#38646;&#21761;&#24577;&#21644;&#23569;&#26679;&#26412;&#25968;&#25454;&#29983;&#25104;&#21487;&#25552;&#39640;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#26159;&#19968;&#20010;&#32321;&#33635;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#19981;&#26029;&#25552;&#20986;&#26032;&#39062;&#30340;&#25216;&#26415;&#26469;&#21019;&#24314;&#20154;&#24037;&#25968;&#25454;&#65292;&#24050;&#32463;&#22312;&#23567;&#25968;&#25454;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#29575;&#65292;&#33267;&#23569;&#23545;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#32780;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#36825;&#20123;&#32467;&#26524;&#65292;&#34920;&#26126;&#32463;&#20856;&#30340;&#25968;&#25454;&#22686;&#24378;&#21482;&#26159;&#19968;&#31181;&#26356;&#22909;&#22320;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#24335;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#25968;&#25454;&#22686;&#24378;&#20043;&#21069;&#33457;&#26356;&#22810;&#26102;&#38388;&#36827;&#34892;&#24494;&#35843;&#20250;&#25269;&#28040;&#20854;&#25928;&#26524;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#36129;&#29486;&#65292;&#22240;&#20026;&#23427;&#22238;&#31572;&#20102;&#26368;&#36817;&#20960;&#24180;&#30041;&#19979;&#30340;&#20960;&#20010;&#38382;&#39064;&#65292;&#21363;&#65306;&#21738;&#31181;DA&#25216;&#26415;&#34920;&#29616;&#26368;&#20339;&#65288;&#21482;&#35201;&#23427;&#20204;&#29983;&#25104;&#30340;&#25968;&#25454;&#19982;&#35757;&#32451;&#38598;&#36275;&#22815;&#25509;&#36817;&#65292;&#19981;&#20250;&#25439;&#23475;&#35757;&#32451;&#65289;&#65292;&#20026;&#20160;&#20040;DA&#34920;&#29616;&#20986;&#31215;&#26497;&#30340;&#32467;&#26524;&#65288;&#31616;&#21270;&#32593;&#32476;&#35757;&#32451;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#23545;&#35805;&#20195;&#29702;&#65288;&#22914;ChatGPT&#25110;LLama2&#65289;&#38646;&#21761;&#24577;&#21644;&#23569;&#26679;&#26412;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20174;&#32780;&#24471;&#20986;&#20102;&#32467;&#35770;&#65292;&#27492;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14895v1 Announce Type: cross  Abstract: Textual data augmentation (DA) is a prolific field of study where novel techniques to create artificial data are regularly proposed, and that has demonstrated great efficiency on small data settings, at least for text classification tasks. In this paper, we challenge those results, showing that classical data augmentation is simply a way of performing better fine-tuning, and that spending more time fine-tuning before applying data augmentation negates its effect. This is a significant contribution as it answers several questions that were left open in recent years, namely~: which DA technique performs best (all of them as long as they generate data close enough to the training set as to not impair training) and why did DA show positive results (facilitates training of network). We furthermore show that zero and few-shot data generation via conversational agents such as ChatGPT or LLama2 can increase performances, concluding that this f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#39640;&#21442;&#25968;PAC&#23398;&#20064;&#29702;&#35770;&#65292;&#21033;&#29992;&#32467;&#26500;&#21270;&#30456;&#20851;&#24615;&#21644;&#20132;&#25442;&#20998;&#24067;&#21462;&#20195;i.i.d.&#25277;&#26679;&#65292;&#35777;&#26126;&#20102;&#32479;&#35745;&#23398;&#20064;&#22522;&#26412;&#23450;&#29702;&#30340;&#39640;&#32500;&#29256;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.14294</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#20114;&#25442;&#24615;&#23454;&#29616;&#39640;&#21442;&#25968;PAC&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
High-arity PAC learning via exchangeability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14294
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#39640;&#21442;&#25968;PAC&#23398;&#20064;&#29702;&#35770;&#65292;&#21033;&#29992;&#32467;&#26500;&#21270;&#30456;&#20851;&#24615;&#21644;&#20132;&#25442;&#20998;&#24067;&#21462;&#20195;i.i.d.&#25277;&#26679;&#65292;&#35777;&#26126;&#20102;&#32479;&#35745;&#23398;&#20064;&#22522;&#26412;&#23450;&#29702;&#30340;&#39640;&#32500;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#32500;PAC&#23398;&#20064;&#29702;&#35770;&#65292;&#21363;&#22312;&#8220;&#32467;&#26500;&#21270;&#30456;&#20851;&#24615;&#8221;&#23384;&#22312;&#30340;&#32479;&#35745;&#23398;&#20064;&#20013;&#12290; &#22312;&#36825;&#20010;&#29702;&#35770;&#20013;&#65292;&#20551;&#35774;&#21487;&#20197;&#26159;&#22270;&#24418;&#12289;&#36229;&#22270;&#65292;&#25110;&#32773;&#26356;&#19968;&#33324;&#22320;&#35828;&#65292;&#26159;&#26377;&#38480;&#20851;&#31995;&#35821;&#35328;&#20013;&#30340;&#32467;&#26500;&#65292;&#24182;&#19988;i.i.d.&#25277;&#26679;&#34987;&#25277;&#26679;&#20135;&#29983;&#21487;&#20114;&#25442;&#20998;&#24067;&#30340;&#35825;&#23548;&#23376;&#32467;&#26500;&#21462;&#20195;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#32479;&#35745;&#23398;&#20064;&#22522;&#26412;&#23450;&#29702;&#30340;&#39640;&#32500;&#29256;&#26412;&#65292;&#36890;&#36807;&#34920;&#24449;&#39640;&#32500;&#65288;agnostic&#65289;PAC&#21487;&#23398;&#24615;&#65292;&#20197;&#32431;&#32452;&#21512;&#32500;&#24230;&#30340;&#26377;&#38480;&#24615;&#21450;&#36866;&#24403;&#29256;&#26412;&#30340;&#22343;&#21248;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14294v1 Announce Type: new  Abstract: We develop a theory of high-arity PAC learning, which is statistical learning in the presence of "structured correlation". In this theory, hypotheses are either graphs, hypergraphs or, more generally, structures in finite relational languages, and i.i.d. sampling is replaced by sampling an induced substructure, producing an exchangeable distribution. We prove a high-arity version of the fundamental theorem of statistical learning by characterizing high-arity (agnostic) PAC learnability in terms of finiteness of a purely combinatorial dimension and in terms of an appropriate version of uniform convergence.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36125;&#21494;&#26031;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#20197;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#20445;&#30041;&#65292;&#24182;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#35821;&#38899;&#21512;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2402.12220</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20197;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12220
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36125;&#21494;&#26031;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#20197;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#20445;&#30041;&#65292;&#24182;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#35821;&#38899;&#21512;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#21021;&#26159;&#34987;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#25152;&#28608;&#21457;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#26356;&#36890;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26159;&#36827;&#34892;&#36825;&#31181;&#33258;&#36866;&#24212;&#30340;&#36866;&#24403;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#20173;&#28982;&#26159;PEFT&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#23427;&#25439;&#23475;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22266;&#26377;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#29616;&#26377;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#24212;&#29992;&#20110;PEFT&#65292;&#20197;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21482;&#35201;&#33021;&#22815;&#21487;&#24494;&#22320;&#35745;&#31639;&#24494;&#35843;&#23618;&#30340;&#21442;&#25968;&#36716;&#25442;&#12290;&#22312;&#19968;&#31995;&#21015;&#20851;&#20110;&#35821;&#35328;&#24314;&#27169;&#21644;&#35821;&#38899;&#21512;&#25104;&#20219;&#21153;&#30340;&#22522;&#30784;&#24615;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#24314;&#31435;&#30340;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#65292;&#21253;&#25324;&#23545;&#35282;&#32447;&#21644;Kronecker&#20998;&#35299;&#26041;&#27861;&#65292;&#26469;&#27491;&#21017;&#21270;PEFT&#19982;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#24182;&#27604;&#36739;&#23427;&#20204;&#22312;&#20445;&#30041;&#39044;&#35757;&#32451;&#30693;&#35782;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#24494;&#35843;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12220v1 Announce Type: cross  Abstract: Although motivated by the adaptation of text-to-speech synthesis models, we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. However, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model's inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#32500;&#26446;&#32676;&#19978;&#20248;&#21270;&#21160;&#24577;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21160;&#24577;&#31995;&#32479;&#34920;&#31034;&#20026;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#22312;&#26446;&#32676;&#19978;&#21046;&#23450;&#20248;&#21270;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22312;&#26446;&#20195;&#25968;&#32423;&#21035;&#34920;&#31034;&#31995;&#32479;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#19968;&#20010;&#20363;&#23376;&#20013;&#65292;&#22788;&#29702;&#20102;&#21018;&#20307;&#25511;&#21046;&#30340;&#26368;&#20248;&#21183;&#33021;&#22609;&#24418;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#25511;&#21046;&#22120;&#26469;&#39564;&#35777;&#26368;&#32456;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.15107</link><description>&lt;p&gt;
&#22312;&#26446;&#32676;&#19978;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23545;SE(3)&#30340;&#20248;&#21270;&#28508;&#21147;&#22609;&#36896;
&lt;/p&gt;
&lt;p&gt;
Optimal Potential Shaping on SE(3) via Neural ODEs on Lie Groups. (arXiv:2401.15107v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#32500;&#26446;&#32676;&#19978;&#20248;&#21270;&#21160;&#24577;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21160;&#24577;&#31995;&#32479;&#34920;&#31034;&#20026;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#22312;&#26446;&#32676;&#19978;&#21046;&#23450;&#20248;&#21270;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22312;&#26446;&#20195;&#25968;&#32423;&#21035;&#34920;&#31034;&#31995;&#32479;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#19968;&#20010;&#20363;&#23376;&#20013;&#65292;&#22788;&#29702;&#20102;&#21018;&#20307;&#25511;&#21046;&#30340;&#26368;&#20248;&#21183;&#33021;&#22609;&#24418;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#25511;&#21046;&#22120;&#26469;&#39564;&#35777;&#26368;&#32456;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#26377;&#38480;&#32500;&#26446;&#32676;&#19978;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#23558;&#21160;&#24577;&#31995;&#32479;&#37325;&#26032;&#34920;&#36848;&#20026;&#25152;&#35859;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(neural ODEs)&#65292;&#24182;&#22312;&#26446;&#32676;&#19978;&#21046;&#23450;&#20248;&#21270;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#31639;&#27861;&#26469;&#35299;&#20915;&#25968;&#20540;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#25193;&#23637;&#65292;&#24182;&#36866;&#29992;&#20110;&#20219;&#20309;&#26377;&#38480;&#32500;&#26446;&#32676;&#65292;&#21253;&#25324;&#30697;&#38453;&#26446;&#32676;&#12290;&#36890;&#36807;&#22312;&#26446;&#20195;&#25968;&#32423;&#21035;&#34920;&#31034;&#31995;&#32479;&#65292;&#20943;&#23569;&#20102;&#26799;&#24230;&#35745;&#31639;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#19968;&#20010;&#24191;&#27867;&#30340;&#20363;&#23376;&#20013;&#65292;&#22788;&#29702;&#20102;&#23545;&#21018;&#20307;&#25511;&#21046;&#30340;&#26368;&#20248;&#21183;&#33021;&#22609;&#24418;&#12290;&#23558;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#34920;&#36848;&#20026;&#23545;&#26446;&#32676;SE(3)&#19978;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#30340;&#20248;&#21270;&#65292;&#24182;&#23545;&#25511;&#21046;&#22120;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#12290;&#26368;&#21518;&#65292;&#22312;&#29366;&#24577;&#35843;&#33410;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#26368;&#32456;&#30340;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a novel approach for the optimization of dynamic systems on finite-dimensional Lie groups. We rephrase dynamic systems as so-called neural ordinary differential equations (neural ODEs), and formulate the optimization problem on Lie groups. A gradient descent optimization algorithm is presented to tackle the optimization numerically. Our algorithm is scalable, and applicable to any finite dimensional Lie group, including matrix Lie groups. By representing the system at the Lie algebra level, we reduce the computational cost of the gradient computation. In an extensive example, optimal potential energy shaping for control of a rigid body is treated. The optimal control problem is phrased as an optimization of a neural ODE on the Lie group SE(3), and the controller is iteratively optimized. The final controller is validated on a state-regulation task.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#29420;&#31435;&#30340;&#19987;&#38376;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2310.03986</link><description>&lt;p&gt;
&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#65292;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation. (arXiv:2310.03986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03986
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#29420;&#31435;&#30340;&#19987;&#38376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#26469;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#19968;&#20123;&#30456;&#20851;&#30340;&#27169;&#24577;&#20013;&#35266;&#23519;&#21040;&#65292;&#22914;&#26524;&#22312;&#27979;&#35797;&#26102;&#38388;&#32570;&#23569;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#31616;&#21333;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#31243;&#24207;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#26469;&#34917;&#20607;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#31181;&#36866;&#24212;&#21487;&#20197;&#37096;&#20998;&#24357;&#34917;&#30001;&#20110;&#32570;&#22833;&#27169;&#24577;&#32780;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#38024;&#23545;&#21487;&#29992;&#27169;&#24577;&#32452;&#21512;&#36827;&#34892;&#35757;&#32451;&#30340;&#29420;&#31435;&#30340;&#12289;&#19987;&#38376;&#30340;&#32593;&#32476;&#12290;&#25152;&#25552;&#20986;&#30340;&#36866;&#24212;&#25152;&#38656;&#30340;&#21442;&#25968;&#38750;&#24120;&#23569;&#65288;&#20363;&#22914;&#65292;&#23569;&#20110;&#65289;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning seeks to utilize data from multiple sources to improve the overall performance of downstream tasks. It is desirable for redundancies in the data to make multimodal systems robust to missing or corrupted observations in some correlated modalities. However, we observe that the performance of several existing multimodal networks significantly deteriorates if one or multiple modalities are absent at test time. To enable robustness to missing modalities, we propose simple and parameter-efficient adaptation procedures for pretrained multimodal networks. In particular, we exploit low-rank adaptation and modulation of intermediate features to compensate for the missing modalities. We demonstrate that such adaptation can partially bridge performance drop due to missing modalities and outperform independent, dedicated networks trained for the available modality combinations in some cases. The proposed adaptation requires extremely small number of parameters (e.g., fewer than 
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322;&#65288;LSX&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#32473;&#20104;&#35299;&#37322;&#21644;&#25209;&#35780;&#32773;&#30340;&#21453;&#39304;&#26469;&#25913;&#36827;&#23398;&#20064;&#32773;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#31561;&#22522;&#26412;&#20219;&#21153;&#65292;&#24182;&#26377;&#28508;&#21147;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.08395</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Learning by Self-Explaining. (arXiv:2309.08395v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08395
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322;&#65288;LSX&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#32473;&#20104;&#35299;&#37322;&#21644;&#25209;&#35780;&#32773;&#30340;&#21453;&#39304;&#26469;&#25913;&#36827;&#23398;&#20064;&#32773;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#31561;&#22522;&#26412;&#20219;&#21153;&#65292;&#24182;&#26377;&#28508;&#21147;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#20174;&#29983;&#29289;&#23398;&#20013;&#23547;&#25214;&#28789;&#24863;&#65292;&#29305;&#21035;&#26159;&#20154;&#31867;&#26234;&#33021;&#12290;&#19982;&#30446;&#21069;&#20027;&#35201;&#23558;&#35299;&#37322;&#35270;&#20026;&#27169;&#22411;&#26816;&#26597;&#25163;&#27573;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30456;&#27604;&#65292;&#20174;&#24515;&#29702;&#23398;&#20013;&#21457;&#29616;&#33258;&#25105;&#35299;&#37322;&#22312;&#20195;&#29702;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#22909;&#22788;&#26377;&#20123;&#34987;&#24573;&#35270;&#20102;&#12290;&#21463;&#21040;&#36825;&#20010;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026;&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322; (LSX)&#12290;&#20854;&#20013;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#65292;&#19968;&#20010;&#23398;&#20064;&#27169;&#22359; (&#23398;&#20064;&#32773;) &#25191;&#34892;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#27604;&#22914;&#22270;&#20687;&#20998;&#31867;&#65292;&#24182;&#23545;&#20854;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;&#20869;&#37096;&#25209;&#35780;&#32773;&#27169;&#22359;&#22522;&#20110;&#21407;&#22987;&#20219;&#21153;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#23398;&#20064;&#32773;&#36890;&#36807;&#25209;&#35780;&#32773;&#30340;&#21453;&#39304;&#24471;&#21040;&#25913;&#36827;&#65292;&#24182;&#26681;&#25454;&#38656;&#35201;&#37325;&#22797;&#36825;&#20010;&#24490;&#29615;&#12290;&#32972;&#21518;&#30340;&#30452;&#35273;&#26159;&#65292;&#22914;&#26524;&#25209;&#35780;&#32773;&#33021;&#22815;&#26681;&#25454;&#30456;&#24212;&#30340;&#35299;&#37322;&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65292;&#21017;&#35813;&#35299;&#37322;&#34987;&#35748;&#20026;&#26159;&#8220;&#22909;&#8221;&#30340;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#23454;&#29616;&#21487;&#33021;&#24615;&#65292;&#20294;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#23454;&#26045;&#23398;&#20064;&#36890;&#36807;&#33258;&#25105;&#35299;&#37322;&#30340;&#19968;&#33324;&#25351;&#23548;&#21407;&#21017;&#12290;&#26377;&#24453;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#26469;&#25506;&#32034;&#36825;&#19968;&#23398;&#20064;&#33539;&#24335;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) research has a long track record of drawing inspirations from findings from biology, in particular human intelligence. In contrast to current AI research that mainly treats explanations as a means for model inspection, a somewhat neglected finding from human psychology is the benefit of self-explaining in an agents' learning process. Motivated by this, we introduce a novel learning paradigm, termed Learning by Self-Explaining (LSX). The underlying idea is that a learning module (learner) performs a base task, e.g. image classification, and provides explanations to its decisions. An internal critic module next evaluates the quality of these explanations given the original task. Finally, the learner is refined with the critic's feedback and the loop is repeated as required. The intuition behind this is that an explanation is considered "good" if the critic can perform the same task given the respective explanation. Despite many implementation possibilities th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;YOLOv8&#27169;&#22411;&#21644;&#21019;&#26032;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#30340;&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20004;&#38454;&#27573;&#39044;&#27979;&#31574;&#30053;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20803;&#32032;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#20248;&#20110;&#21333;&#20010;&#22522;&#30784;&#26550;&#26500;&#65292;&#24182;&#35299;&#20915;&#20102;BaDLAD&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;OCR&#21644;&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.00848</link><description>&lt;p&gt;
&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;-&#19968;&#31181;&#22522;&#20110;YOLOv8&#30340;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bengali Document Layout Analysis -- A YOLOV8 Based Ensembling Approach. (arXiv:2309.00848v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;YOLOv8&#27169;&#22411;&#21644;&#21019;&#26032;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#30340;&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20004;&#38454;&#27573;&#39044;&#27979;&#31574;&#30053;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20803;&#32032;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#20248;&#20110;&#21333;&#20010;&#22522;&#30784;&#26550;&#26500;&#65292;&#24182;&#35299;&#20915;&#20102;BaDLAD&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;OCR&#21644;&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20391;&#37325;&#20110;&#21033;&#29992;YOLOv8&#27169;&#22411;&#21644;&#21019;&#26032;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#25552;&#21319;&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#65288;DLA&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#20197;&#24212;&#23545;&#23391;&#21152;&#25289;&#22797;&#26434;&#25991;&#23383;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#32463;&#36807;&#20005;&#26684;&#30340;&#39564;&#35777;&#38598;&#35780;&#20272;&#65292;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20803;&#32032;&#20998;&#21106;&#30340;&#20004;&#38454;&#27573;&#39044;&#27979;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#38598;&#25104;&#27169;&#22411;&#32467;&#21512;&#21518;&#22788;&#29702;&#24615;&#33021;&#20248;&#20110;&#21333;&#20010;&#22522;&#30784;&#26550;&#26500;&#65292;&#35299;&#20915;&#20102;BaDLAD&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#26088;&#22312;&#25512;&#21160;&#23391;&#21152;&#25289;&#25991;&#26723;&#20998;&#26512;&#30340;&#21457;&#23637;&#65292;&#25552;&#39640;OCR&#21644;&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#65292;&#21516;&#26102;BaDLAD&#20316;&#20026;&#22522;&#30784;&#36164;&#28304;&#26377;&#21161;&#20110;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#20026;&#23558;&#26032;&#31574;&#30053;&#32435;&#20837;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on enhancing Bengali Document Layout Analysis (DLA) using the YOLOv8 model and innovative post-processing techniques. We tackle challenges unique to the complex Bengali script by employing data augmentation for model robustness. After meticulous validation set evaluation, we fine-tune our approach on the complete dataset, leading to a two-stage prediction strategy for accurate element segmentation. Our ensemble model, combined with post-processing, outperforms individual base architectures, addressing issues identified in the BaDLAD dataset. By leveraging this approach, we aim to advance Bengali document analysis, contributing to improved OCR and document comprehension and BaDLAD serves as a foundational resource for this endeavor, aiding future research in the field. Furthermore, our experiments provided key insights to incorporate new strategies into the established solution.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25299;&#25169;&#24615;&#36136;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;Transformer&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#65292;&#24320;&#36767;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#24212;&#29992;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.11295</link><description>&lt;p&gt;
&#36890;&#36807;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#25299;&#25169;&#20998;&#26512;&#26469;&#20272;&#31639;Transformer&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Estimation of Transformers' Predictions via Topological Analysis of the Attention Matrices. (arXiv:2308.11295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25299;&#25169;&#24615;&#36136;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;Transformer&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#65292;&#24320;&#36767;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#30830;&#23450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#23545;&#20110;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#24182;&#19981;&#26377;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20219;&#21153;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#28857;&#26159;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#25903;&#25345;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20196;&#29260;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#12290;&#25105;&#20204;&#21033;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#25506;&#32034;&#20869;&#37096;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#39044;&#27979;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25299;&#25169;&#24615;&#36136;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#36136;&#37327;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#36767;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#24212;&#29992;&#39046;&#22495;&#65292;&#20294;&#38656;&#35201;...
&lt;/p&gt;
&lt;p&gt;
Determining the degree of confidence of deep learning model in its prediction is an open problem in the field of natural language processing. Most of the classical methods for uncertainty estimation are quite weak for text classification models. We set the task of obtaining an uncertainty estimate for neural networks based on the Transformer architecture. A key feature of such mo-dels is the attention mechanism, which supports the information flow between the hidden representations of tokens in the neural network. We explore the formed relationships between internal representations using Topological Data Analysis methods and utilize them to predict model's confidence. In this paper, we propose a method for uncertainty estimation based on the topological properties of the attention mechanism and compare it with classical methods. As a result, the proposed algorithm surpasses the existing methods in quality and opens up a new area of application of the attention mechanism, but requires t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20811;&#26381;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#24615;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#21457;&#29616;&#19968;&#31181;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#23454;&#39564;&#20013;&#22823;&#24133;&#20943;&#23569;&#20102;&#32593;&#32476;&#26356;&#26032;&#30340;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.01904</link><description>&lt;p&gt;
&#20811;&#26381;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#24615;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Overcoming the Stability Gap in Continual Learning. (arXiv:2306.01904v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20811;&#26381;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#24615;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#21457;&#29616;&#19968;&#31181;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#23454;&#39564;&#20013;&#22823;&#24133;&#20943;&#23569;&#20102;&#32593;&#32476;&#26356;&#26032;&#30340;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#38543;&#30528;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#22686;&#38271;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24448;&#24448;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#12290;&#32771;&#34385;&#21040;&#37325;&#26032;&#35757;&#32451;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20154;&#20204;&#35748;&#20026;&#36830;&#32493;&#23398;&#20064;&#21487;&#20197;&#20351;&#32593;&#32476;&#26356;&#26032;&#26356;&#21152;&#39640;&#25928;&#12290;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#38556;&#30861;&#26159;&#31283;&#23450;&#24615;&#24046;&#36317;&#65292;&#21363;&#22312;&#26356;&#26032;&#26032;&#25968;&#25454;&#26102;&#65292;&#20808;&#21069;&#23398;&#20064;&#30340;&#25968;&#25454;&#24615;&#33021;&#20250;&#19979;&#38477;&#65292;&#28982;&#21518;&#25165;&#24471;&#20197;&#24674;&#22797;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#20943;&#23569;&#32593;&#32476;&#26356;&#26032;&#30340;&#27425;&#25968;&#65292;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#32531;&#35299;&#31283;&#23450;&#24615;&#24046;&#36317;&#65292;&#24182;&#27979;&#35797;&#20102;&#22810;&#31181;&#20551;&#35774;&#20197;&#20102;&#35299;&#20854;&#20135;&#29983;&#21407;&#22240;&#12290;&#36825;&#20351;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26174;&#33879;&#20943;&#23569;&#31283;&#23450;&#24615;&#24046;&#36317;&#30340;&#26041;&#27861;&#12290;&#22312;&#22823;&#35268;&#27169;&#30340;&#22686;&#37327;&#31867;&#21035;&#23398;&#20064;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#36830;&#32493;&#23398;&#20064;&#25152;&#38656;&#30340;&#32593;&#32476;&#26356;&#26032;&#27425;&#25968;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26377;&#21487;&#33021;&#25512;&#21160;&#36830;&#32493;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world applications, deep neural networks are retrained from scratch as a dataset grows in size. Given the computational expense for retraining networks, it has been argued that continual learning could make updating networks more efficient. An obstacle to achieving this goal is the stability gap, which refers to an observation that when updating on new data, performance on previously learned data degrades before recovering. Addressing this problem would enable learning new data with fewer network updates, resulting in increased computational efficiency. We study how to mitigate the stability gap. We test a variety of hypotheses to understand why the stability gap occurs. This leads us to discover a method that vastly reduces this gap. In large-scale class incremental learning experiments, we are able to significantly reduce the number of network updates needed for continual learning. Our work has the potential to advance the state-of-the-art in continual learning for real-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#21512;&#21516;&#31639;&#23376;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#22312;&#26377;&#30028;&#20056;&#27861;&#22122;&#22768;&#21644;&#21152;&#24615;&#27425;&#39640;&#26031;&#22122;&#22768;&#35774;&#32622;&#19979;&#30340;&#27987;&#24230;&#34892;&#20026;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#25910;&#25947;&#35823;&#24046;&#30340;&#26497;&#22823;&#27987;&#24230;&#19981;&#31561;&#24335;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#35823;&#24046;&#20855;&#26377;&#20122;&#39640;&#26031;&#23614;&#24052;&#25110;&#32773;&#36229;&#22810;&#39033;&#24335;&#23614;&#24052;&#12290;&#21516;&#26102;&#36824;&#21457;&#29616;&#65292;&#20056;&#27861;&#22122;&#22768;&#24773;&#20917;&#19979;&#19968;&#33324;&#19981;&#21487;&#33021;&#23454;&#29616;&#20122;&#25351;&#25968;&#23614;&#24052;&#12290;</title><link>http://arxiv.org/abs/2303.15740</link><description>&lt;p&gt;
&#21512;&#21516;&#25193;&#24352;&#38543;&#26426;&#36817;&#20284;&#30340;&#27987;&#24230;&#65306;&#21152;&#27861;&#21644;&#20056;&#27861;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Concentration of Contractive Stochastic Approximation: Additive and Multiplicative Noise. (arXiv:2303.15740v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#21512;&#21516;&#31639;&#23376;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#22312;&#26377;&#30028;&#20056;&#27861;&#22122;&#22768;&#21644;&#21152;&#24615;&#27425;&#39640;&#26031;&#22122;&#22768;&#35774;&#32622;&#19979;&#30340;&#27987;&#24230;&#34892;&#20026;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#25910;&#25947;&#35823;&#24046;&#30340;&#26497;&#22823;&#27987;&#24230;&#19981;&#31561;&#24335;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#35823;&#24046;&#20855;&#26377;&#20122;&#39640;&#26031;&#23614;&#24052;&#25110;&#32773;&#36229;&#22810;&#39033;&#24335;&#23614;&#24052;&#12290;&#21516;&#26102;&#36824;&#21457;&#29616;&#65292;&#20056;&#27861;&#22122;&#22768;&#24773;&#20917;&#19979;&#19968;&#33324;&#19981;&#21487;&#33021;&#23454;&#29616;&#20122;&#25351;&#25968;&#23614;&#24052;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20219;&#20309;&#33539;&#25968;&#19979;&#65292;&#20855;&#26377;&#21512;&#21516;&#31639;&#23376;&#30340;&#38543;&#26426;&#36924;&#36817;(SA)&#31639;&#27861;&#30340;&#27987;&#24230;&#34892;&#20026;&#12290; &#25105;&#20204;&#32771;&#34385;&#20004;&#31181;&#24773;&#20917;&#65292;&#20854;&#20013;&#36845;&#20195;&#21487;&#33021;&#26080;&#30028;&#65306;&#65288;1&#65289;&#26377;&#30028;&#20056;&#27861;&#22122;&#22768;&#65292;&#65288;2&#65289;&#21152;&#24615;&#27425;&#39640;&#26031;&#22122;&#22768;&#12290; &#25105;&#20204;&#24471;&#21040;&#20102;&#20851;&#20110;&#25910;&#25947;&#35823;&#24046;&#30340;&#26497;&#22823;&#27987;&#24230;&#19981;&#31561;&#24335;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#35823;&#24046;&#22312;&#21152;&#24615;&#22122;&#22768;&#35774;&#32622;&#19979;&#20855;&#26377;&#20122;&#39640;&#26031;&#23614;&#24052;&#65292;&#22312;&#20056;&#27861;&#22122;&#22768;&#35774;&#32622;&#19979;&#20855;&#26377;&#36229;&#22810;&#39033;&#24335;&#23614;&#24052;&#65288;&#24555;&#20110;&#22810;&#39033;&#24335;&#34928;&#20943;&#65289;&#12290; &#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19981;&#21487;&#33021;&#32467;&#26524;&#65292;&#26174;&#31034;&#36890;&#24120;&#26080;&#27861;&#36890;&#36807;&#20056;&#27861;&#22122;&#22768;&#30340;SA&#23454;&#29616;&#20122;&#25351;&#25968;&#23614;&#24052;&#12290; &#20026;&#20102;&#30830;&#31435;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#20030;&#35770;&#35777;&#65292;&#20854;&#20013;&#28041;&#21450;&#36793;&#30028;&#35823;&#24046;&#30340;&#24191;&#20041;Moreau&#21253;&#32476;&#30340;&#30697;&#29983;&#25104;&#20989;&#25968;&#21644;&#25351;&#25968;&#36229;&#39532;&#23572;&#21487;&#22827;&#26500;&#36896;&#65292;&#20197;&#21551;&#29992;&#20351;&#29992;Ville&#30340;&#26497;&#22823;&#19981;&#31561;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the concentration behavior of a stochastic approximation (SA) algorithm under a contractive operator with respect to an arbitrary norm. We consider two settings where the iterates are potentially unbounded: (1) bounded multiplicative noise, and (2) additive sub-Gaussian noise. We obtain maximal concentration inequalities on the convergence errors, and show that these errors have sub-Gaussian tails in the additive noise setting, and super-polynomial tails (faster than polynomial decay) in the multiplicative noise setting. In addition, we provide an impossibility result showing that it is in general not possible to achieve sub-exponential tails for SA with multiplicative noise. To establish these results, we develop a novel bootstrapping argument that involves bounding the moment generating function of the generalized Moreau envelope of the error and the construction of an exponential supermartingale to enable using Ville's maximal inequality.  To demonstrate the a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#65288;P2PFL&#65289;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#65292;&#21033;&#29992;&#32467;&#26500;&#22270;&#23646;&#24615;&#36873;&#25321;&#24694;&#24847;&#33410;&#28857;&#65292;&#23454;&#29616;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;&#21516;&#26102;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#25915;&#20987;&#22312;&#22810;&#31181;&#29616;&#23454;&#26465;&#20214;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2301.09732</link><description>&lt;p&gt;
&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Backdoor Attacks in Peer-to-Peer Federated Learning. (arXiv:2301.09732v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#65288;P2PFL&#65289;&#30340;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#65292;&#21033;&#29992;&#32467;&#26500;&#22270;&#23646;&#24615;&#36873;&#25321;&#24694;&#24847;&#33410;&#28857;&#65292;&#23454;&#29616;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;&#21516;&#26102;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#25915;&#20987;&#22312;&#22810;&#31181;&#29616;&#23454;&#26465;&#20214;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#20381;&#36182;&#20110;&#38598;&#20013;&#24335;&#23398;&#20064;&#36807;&#31243;&#65292;&#36825;&#24320;&#25918;&#20102;&#26333;&#20809;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#39118;&#38505;&#12290;&#23613;&#31649;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#32531;&#35299;&#20102;&#36825;&#20123;&#38544;&#31169;&#39118;&#38505;&#65292;&#20294;&#23427;&#20173;&#20381;&#36182;&#20110;&#21487;&#20449;&#30340;&#32858;&#21512;&#26381;&#21153;&#22120;&#26469;&#35757;&#32451;&#20849;&#20139;&#20840;&#23616;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28857;&#23545;&#28857;&#32852;&#37030;&#23398;&#20064;&#65288;P2PFL&#65289;&#30340;&#26032;&#20998;&#24067;&#24335;&#23398;&#20064;&#26550;&#26500;&#22312;&#38544;&#31169;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#37117;&#25552;&#20379;&#20102;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#23545;&#27602;&#21270;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;P2PFL&#21518;&#38376;&#25915;&#20987;&#65292;&#21033;&#29992;&#32467;&#26500;&#22270;&#23646;&#24615;&#36873;&#25321;&#24694;&#24847;&#33410;&#28857;&#65292;&#23454;&#29616;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#23454;&#38469;&#26465;&#20214;&#19979;&#35780;&#20272;&#25105;&#20204;&#30340;&#25915;&#20987;&#65292;&#21253;&#25324;&#22810;&#20010;&#22270;&#24418;&#25299;&#25169;&#12289;&#32593;&#32476;&#20013;&#26377;&#38480;&#30340;&#25932;&#23545;&#33021;&#35265;&#24230;&#20197;&#21450;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;FL&#20013;&#36866;&#24212;&#30340;&#29616;&#26377;&#38450;&#24481;&#25514;&#26045;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most machine learning applications rely on centralized learning processes, opening up the risk of exposure of their training datasets. While federated learning (FL) mitigates to some extent these privacy risks, it relies on a trusted aggregation server for training a shared global model. Recently, new distributed learning architectures based on Peer-to-Peer Federated Learning (P2PFL) offer advantages in terms of both privacy and reliability. Still, their resilience to poisoning attacks during training has not been investigated. In this paper, we propose new backdoor attacks for P2PFL that leverage structural graph properties to select the malicious nodes, and achieve high attack success, while remaining stealthy. We evaluate our attacks under various realistic conditions, including multiple graph topologies, limited adversarial visibility of the network, and clients with non-IID data. Finally, we show the limitations of existing defenses adapted from FL and design a new defense that su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#21147;&#31995;&#32479;&#29702;&#35770;&#20013;&#30340;&#20960;&#20010;&#24037;&#20855;&#26469;&#35299;&#20915;&#38750;&#20984;&#37319;&#26679;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#23545;&#20110;&#19968;&#22823;&#31867;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#23427;&#20204;&#22312;Wasserstein&#36317;&#31163;&#19979;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#21487;&#20197;&#24402;&#32467;&#20026;&#23545;&#23427;&#20204;&#30340;&#36830;&#32493;&#26102;&#38388;&#23545;&#24212;&#29289;&#30340;&#30740;&#31350;&#65292;&#36825;&#26159;&#26356;&#22909;&#29702;&#35299;&#30340;&#12290;</title><link>http://arxiv.org/abs/2210.13867</link><description>&lt;p&gt;
Langevin-Based Non-Convex Sampling&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Dynamical System View of Langevin-Based Non-Convex Sampling. (arXiv:2210.13867v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#21147;&#31995;&#32479;&#29702;&#35770;&#20013;&#30340;&#20960;&#20010;&#24037;&#20855;&#26469;&#35299;&#20915;&#38750;&#20984;&#37319;&#26679;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#23545;&#20110;&#19968;&#22823;&#31867;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#23427;&#20204;&#22312;Wasserstein&#36317;&#31163;&#19979;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#21487;&#20197;&#24402;&#32467;&#20026;&#23545;&#23427;&#20204;&#30340;&#36830;&#32493;&#26102;&#38388;&#23545;&#24212;&#29289;&#30340;&#30740;&#31350;&#65292;&#36825;&#26159;&#26356;&#22909;&#29702;&#35299;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new framework that uses tools from the theory of dynamical systems to address important challenges in non-convex sampling. For a large class of state-of-the-art sampling schemes, their last-iterate convergence in Wasserstein distances can be reduced to the study of their continuous-time counterparts, which is much better understood.
&lt;/p&gt;
&lt;p&gt;
&#38750;&#20984;&#37319;&#26679;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#38750;&#20984;&#20248;&#21270;&#20197;&#21450;&#36817;&#20284;&#27010;&#29575;&#25512;&#26029;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#29702;&#35770;&#19978;&#20173;&#23384;&#22312;&#35768;&#22810;&#37325;&#35201;&#25361;&#25112;&#65306;&#29616;&#26377;&#30340;&#20445;&#35777;&#36890;&#24120;&#20165;&#36866;&#29992;&#20110;&#24179;&#22343;&#36845;&#20195;&#32780;&#19981;&#26159;&#26356;&#29702;&#24819;&#30340;&#26368;&#21518;&#36845;&#20195;&#65292;&#32570;&#20047;&#25429;&#25417;&#21464;&#37327;&#23610;&#24230;&#65288;&#22914;Wasserstein&#36317;&#31163;&#65289;&#30340;&#25910;&#25947;&#24230;&#37327;&#65292;&#20027;&#35201;&#36866;&#29992;&#20110;&#38543;&#26426;&#26799;&#24230;Langevin&#21160;&#21147;&#23398;&#31561;&#22522;&#26412;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#21147;&#31995;&#32479;&#29702;&#35770;&#20013;&#30340;&#20960;&#20010;&#24037;&#20855;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#32467;&#26524;&#26159;&#65292;&#23545;&#20110;&#19968;&#22823;&#31867;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#23427;&#20204;&#22312;Wasserstein&#36317;&#31163;&#19979;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#21487;&#20197;&#24402;&#32467;&#20026;&#23545;&#23427;&#20204;&#30340;&#36830;&#32493;&#26102;&#38388;&#23545;&#24212;&#29289;&#30340;&#30740;&#31350;&#65292;&#36825;&#26159;&#26356;&#22909;&#29702;&#35299;&#30340;&#12290;&#32467;&#21512;MCMC&#37319;&#26679;&#30340;&#26631;&#20934;&#20551;&#35774;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#31435;&#21363;&#20135;&#29983;&#20102;
&lt;/p&gt;
&lt;p&gt;
Non-convex sampling is a key challenge in machine learning, central to non-convex optimization in deep learning as well as to approximate probabilistic inference. Despite its significance, theoretically there remain many important challenges: Existing guarantees (1) typically only hold for the averaged iterates rather than the more desirable last iterates, (2) lack convergence metrics that capture the scales of the variables such as Wasserstein distances, and (3) mainly apply to elementary schemes such as stochastic gradient Langevin dynamics. In this paper, we develop a new framework that lifts the above issues by harnessing several tools from the theory of dynamical systems. Our key result is that, for a large class of state-of-the-art sampling schemes, their last-iterate convergence in Wasserstein distances can be reduced to the study of their continuous-time counterparts, which is much better understood. Coupled with standard assumptions of MCMC sampling, our theory immediately yie
&lt;/p&gt;</description></item><item><title>ES-GNN&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#36793;&#20998;&#21106;&#23558;&#22270;&#20998;&#21106;&#20026;&#20004;&#20010;&#23376;&#22270;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#21306;&#20998;&#23545;&#23398;&#20064;&#20219;&#21153;&#30456;&#20851;&#25110;&#19981;&#30456;&#20851;&#30340;&#22270;&#36793;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;GNN&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13700</link><description>&lt;p&gt;
ES-GNN: &#36890;&#36807;&#36793;&#20998;&#21106;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#24191;&#21040;&#24322;&#36136;&#22270;
&lt;/p&gt;
&lt;p&gt;
ES-GNN: Generalizing Graph Neural Networks Beyond Homophily with Edge Splitting. (arXiv:2205.13700v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13700
&lt;/p&gt;
&lt;p&gt;
ES-GNN&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#36793;&#20998;&#21106;&#23558;&#22270;&#20998;&#21106;&#20026;&#20004;&#20010;&#23376;&#22270;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#21306;&#20998;&#23545;&#23398;&#20064;&#20219;&#21153;&#30456;&#20851;&#25110;&#19981;&#30456;&#20851;&#30340;&#22270;&#36793;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;GNN&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20010;&#22270;&#20998;&#26512;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#29616;&#20195;&#21464;&#20307;&#20027;&#35201;&#20381;&#36182;&#20110;&#21516;&#36136;&#24615;&#30340;&#24378;&#24402;&#32435;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#36890;&#24120;&#21516;&#26102;&#26174;&#31034;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#30340;&#38142;&#25509;&#27169;&#24335;&#65292;&#20854;&#20013;&#30456;&#37051;&#33410;&#28857;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#23646;&#24615;&#21644;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;GNN&#22312;&#25972;&#20307;&#19978;&#24179;&#28369;&#33410;&#28857;&#25509;&#36817;&#24615;&#21487;&#33021;&#20250;&#32858;&#21512;&#20219;&#21153;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#65288;&#29978;&#33267;&#26377;&#23475;&#65289;&#30340;&#20449;&#24687;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#25512;&#24191;&#21040;&#24322;&#36136;&#22270;&#30340;&#33021;&#21147;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#38750;&#40065;&#26834;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36793;&#20998;&#21106;GNN&#65288;ES-GNN&#65289;&#26694;&#26550;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#21306;&#20998;&#23545;&#23398;&#20064;&#20219;&#21153;&#30456;&#20851;&#25110;&#19981;&#30456;&#20851;&#30340;&#22270;&#36793;&#12290;&#36825;&#23558;&#21407;&#22987;&#22270;&#36716;&#21270;&#20026;&#20004;&#20010;&#20855;&#26377;&#30456;&#21516;&#33410;&#28857;&#38598;&#20294;&#20855;&#26377;&#29420;&#21344;&#36793;&#38598;&#30340;&#23376;&#22270;&#12290;&#22312;&#36825;&#20004;&#20010;&#23376;&#22270;&#19978;&#20998;&#21035;&#36827;&#34892;&#20449;&#24687;&#20256;&#25773;&#21644;&#36793;&#20998;&#21106;&#65292;&#20174;&#32780;&#20351;&#20449;&#24687;&#20256;&#25773;&#21644;&#36793;&#20998;&#21106;&#20132;&#26367;&#36827;&#34892;&#65292;&#23454;&#29616;&#20102;&#35299;&#32806;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Graph Neural Networks (GNNs) have achieved enormous success in multiple graph analytical tasks, modern variants mostly rely on the strong inductive bias of homophily. However, real-world networks typically exhibit both homophilic and heterophilic linking patterns, wherein adjacent nodes may share dissimilar attributes and distinct labels. Therefore, GNNs smoothing node proximity holistically may aggregate both task-relevant and irrelevant (even harmful) information, limiting their ability to generalize to heterophilic graphs and potentially causing non-robustness. In this work, we propose a novel edge splitting GNN (ES-GNN) framework to adaptively distinguish between graph edges either relevant or irrelevant to learning tasks. This essentially transfers the original graph into two subgraphs with the same node set but exclusive edge sets dynamically. Given that, information propagation separately on these subgraphs and edge splitting are alternatively conducted, thus disentangling
&lt;/p&gt;</description></item></channel></rss>