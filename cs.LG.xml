<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;Transformer&#27169;&#22411;&#32467;&#21512;&#36328;&#36890;&#36947;&#27880;&#24847;&#21147;&#65292;&#25512;&#21160;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#21333;&#33033;&#20914;&#30005;&#21050;&#28608;&#21709;&#24212;&#30340;SOZ&#26412;&#22320;&#21270;&#65292;&#22312;&#35780;&#20272;&#20013;&#23637;&#31034;&#20102;&#27169;&#22411;&#23545;&#26410;&#35265;&#24739;&#32773;&#21644;&#30005;&#26497;&#25918;&#32622;&#30340;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.20324</link><description>&lt;p&gt;
&#20351;&#29992;&#21464;&#21387;&#22120;&#20174;&#21333;&#33033;&#20914;&#30005;&#21050;&#28608;&#21709;&#24212;&#20013;&#23450;&#20301;&#30315;&#30187;&#21457;&#20316;&#36215;&#22987;&#21306;
&lt;/p&gt;
&lt;p&gt;
Localising the Seizure Onset Zone from Single-Pulse Electrical Stimulation Responses with a Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;Transformer&#27169;&#22411;&#32467;&#21512;&#36328;&#36890;&#36947;&#27880;&#24847;&#21147;&#65292;&#25512;&#21160;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#21333;&#33033;&#20914;&#30005;&#21050;&#28608;&#21709;&#24212;&#30340;SOZ&#26412;&#22320;&#21270;&#65292;&#22312;&#35780;&#20272;&#20013;&#23637;&#31034;&#20102;&#27169;&#22411;&#23545;&#26410;&#35265;&#24739;&#32773;&#21644;&#30005;&#26497;&#25918;&#32622;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#26368;&#24120;&#35265;&#30340;&#31070;&#32463;&#30142;&#30149;&#20043;&#19968;&#65292;&#35768;&#22810;&#24739;&#32773;&#22312;&#33647;&#29289;&#26080;&#27861;&#25511;&#21046;&#30315;&#30187;&#21457;&#20316;&#26102;&#38656;&#35201;&#25163;&#26415;&#24178;&#39044;&#12290;&#20026;&#20102;&#21462;&#24471;&#26377;&#25928;&#30340;&#25163;&#26415;&#32467;&#26524;&#65292;&#20934;&#30830;&#23450;&#20301;&#30315;&#30187;&#21457;&#20316;&#36215;&#22987;&#21306; - &#36890;&#24120;&#36817;&#20284;&#20026;&#30315;&#30187;&#21457;&#20316;&#36215;&#22987;&#21306; (SOZ) - &#33267;&#20851;&#37325;&#35201;&#20294;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#30005;&#21050;&#28608;&#36827;&#34892;&#20027;&#21160;&#25506;&#27979;&#24050;&#32463;&#25104;&#20026;&#35782;&#21035;&#30315;&#30187;&#21457;&#20316;&#21306;&#22495;&#30340;&#26631;&#20934;&#20020;&#24202;&#23454;&#36341;&#12290;&#26412;&#25991;&#25512;&#21160;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#20351;&#29992;&#21333;&#33033;&#20914;&#30005;&#21050;&#28608; (SPES) &#21709;&#24212;&#36827;&#34892; SOZ &#23450;&#20301;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;&#36328;&#36890;&#36947;&#27880;&#24847;&#21147;&#30340;Transformer&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#22312;&#20445;&#30041;&#30340;&#24739;&#32773;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#23545;&#26410;&#35265;&#24739;&#32773;&#21644;&#30005;&#26497;&#25918;&#32622;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20324v1 Announce Type: new  Abstract: Epilepsy is one of the most common neurological disorders, and many patients require surgical intervention when medication fails to control seizures. For effective surgical outcomes, precise localisation of the epileptogenic focus - often approximated through the Seizure Onset Zone (SOZ) - is critical yet remains a challenge. Active probing through electrical stimulation is already standard clinical practice for identifying epileptogenic areas. This paper advances the application of deep learning for SOZ localisation using Single Pulse Electrical Stimulation (SPES) responses. We achieve this by introducing Transformer models that incorporate cross-channel attention. We evaluate these models on held-out patient test sets to assess their generalisability to unseen patients and electrode placements.   Our study makes three key contributions: Firstly, we implement an existing deep learning model to compare two SPES analysis paradigms - namel
&lt;/p&gt;</description></item><item><title>RFMP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#27969;&#21305;&#37197;&#30340;&#20248;&#21183;&#22312;&#26426;&#22120;&#20154;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#20013;&#20855;&#26377;&#39640;&#25928;&#35757;&#32451;&#21644;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#34701;&#21512;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#20960;&#20309;&#24847;&#35782;&#65292;&#25552;&#20379;&#26356;&#24179;&#28369;&#30340;&#21160;&#20316;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2403.10672</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#30340;&#40654;&#26364;&#27969;&#21305;&#37197;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Riemannian Flow Matching Policy for Robot Motion Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10672
&lt;/p&gt;
&lt;p&gt;
RFMP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#27969;&#21305;&#37197;&#30340;&#20248;&#21183;&#22312;&#26426;&#22120;&#20154;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#20013;&#20855;&#26377;&#39640;&#25928;&#35757;&#32451;&#21644;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#34701;&#21512;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#20960;&#20309;&#24847;&#35782;&#65292;&#25552;&#20379;&#26356;&#24179;&#28369;&#30340;&#21160;&#20316;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#40654;&#26364;&#27969;&#21305;&#37197;&#31574;&#30053;&#65288;RFMP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#21512;&#25104;&#26426;&#22120;&#20154;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#12290;RFMP&#21033;&#29992;&#20102;&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#39640;&#25928;&#35757;&#32451;&#21644;&#25512;&#26029;&#33021;&#21147;&#12290;&#36890;&#36807;&#35774;&#35745;&#65292;RFMP&#32487;&#25215;&#20102;&#27969;&#21305;&#37197;&#30340;&#20248;&#21183;&#65306;&#33021;&#22815;&#32534;&#30721;&#39640;&#32500;&#24230;&#22810;&#27169;&#24577;&#20998;&#24067;&#65292;&#22312;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#24120;&#35265;&#65292;&#24182;&#19988;&#20855;&#26377;&#38750;&#24120;&#31616;&#21333;&#21644;&#24555;&#36895;&#30340;&#25512;&#26029;&#36807;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RFMP&#22312;&#22522;&#20110;&#29366;&#24577;&#21644;&#22522;&#20110;&#35270;&#35273;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#31574;&#30053;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#30001;&#20110;&#26426;&#22120;&#20154;&#29366;&#24577;&#23384;&#22312;&#20110;&#40654;&#26364;&#27969;&#24418;&#19978;&#65292;RFMP&#22312;&#26412;&#36136;&#19978;&#34701;&#21512;&#20102;&#20960;&#20309;&#24847;&#35782;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35780;&#20272;RFMP&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#27010;&#24565;&#39564;&#35777;&#23454;&#39564;&#65292;&#23558;&#20854;&#24615;&#33021;&#19982;&#25193;&#25955;&#31574;&#30053;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23613;&#31649;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#25152;&#32771;&#34385;&#30340;&#20219;&#21153;&#65292;&#20294;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;RFMP&#25552;&#20379;&#20102;&#26356;&#24179;&#28369;&#30340;&#21160;&#20316;&#36712;&#36857;&#65292;&#26174;&#33879;&#22320;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10672v1 Announce Type: cross  Abstract: We introduce Riemannian Flow Matching Policies (RFMP), a novel model for learning and synthesizing robot visuomotor policies. RFMP leverages the efficient training and inference capabilities of flow matching methods. By design, RFMP inherits the strengths of flow matching: the ability to encode high-dimensional multimodal distributions, commonly encountered in robotic tasks, and a very simple and fast inference process. We demonstrate the applicability of RFMP to both state-based and vision-conditioned robot motion policies. Notably, as the robot state resides on a Riemannian manifold, RFMP inherently incorporates geometric awareness, which is crucial for realistic robotic tasks. To evaluate RFMP, we conduct two proof-of-concept experiments, comparing its performance against Diffusion Policies. Although both approaches successfully learn the considered tasks, our results show that RFMP provides smoother action trajectories with signifi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#32852;&#21512;&#35774;&#32622;&#20013;&#36827;&#34892;&#22240;&#26524;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;FedCMFS&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#31639;&#27861;&#36890;&#36807;&#19977;&#20010;&#26032;&#39062;&#23376;&#31243;&#24207;&#65292;&#22312;&#19981;&#38598;&#20013;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27599;&#20010;&#31867;&#26631;&#31614;&#30340;&#30456;&#20851;&#29305;&#24449;&#65288;&#20505;&#36873;&#29238;&#33410;&#28857;&#21644;&#23376;&#33410;&#28857;&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.06419</link><description>&lt;p&gt;
&#32852;&#21512;&#29615;&#22659;&#20013;&#22240;&#26524;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Causal Multi-Label Feature Selection in Federated Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06419
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#32852;&#21512;&#35774;&#32622;&#20013;&#36827;&#34892;&#22240;&#26524;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;FedCMFS&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#31639;&#27861;&#36890;&#36807;&#19977;&#20010;&#26032;&#39062;&#23376;&#31243;&#24207;&#65292;&#22312;&#19981;&#38598;&#20013;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27599;&#20010;&#31867;&#26631;&#31614;&#30340;&#30456;&#20851;&#29305;&#24449;&#65288;&#20505;&#36873;&#29238;&#33410;&#28857;&#21644;&#23376;&#33410;&#28857;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;&#20316;&#20026;&#22788;&#29702;&#39640;&#32500;&#22810;&#26631;&#31614;&#25968;&#25454;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#20026;&#20102;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#29616;&#26377;&#30340;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#23558;&#26469;&#33258;&#22810;&#20010;&#28304;&#30340;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#38598;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#21512;&#35774;&#32622;&#20013;&#65292;&#20174;&#25152;&#26377;&#26469;&#28304;&#38598;&#20013;&#25968;&#25454;&#24182;&#23558;&#20854;&#21512;&#24182;&#20026;&#21333;&#20010;&#25968;&#25454;&#38598;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32852;&#21512;&#29615;&#22659;&#20013;&#22240;&#26524;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#19977;&#20010;&#26032;&#39062;&#23376;&#31243;&#24207;&#30340;&#32852;&#21512;&#22240;&#26524;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;&#65288;FedCMFS&#65289;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06419v1 Announce Type: new  Abstract: Multi-label feature selection serves as an effective mean for dealing with high-dimensional multi-label data. To achieve satisfactory performance, existing methods for multi-label feature selection often require the centralization of substantial data from multiple sources. However, in Federated setting, centralizing data from all sources and merging them into a single dataset is not feasible. To tackle this issue, in this paper, we study a challenging problem of causal multi-label feature selection in federated setting and propose a Federated Causal Multi-label Feature Selection (FedCMFS) algorithm with three novel subroutines. Specifically, FedCMFS first uses the FedCFL subroutine that considers the correlations among label-label, label-feature, and feature-feature to learn the relevant features (candidate parents and children) of each class label while preserving data privacy without centralizing data. Second, FedCMFS employs the FedCF
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#20102;&#19968;&#31181;&#21327;&#20316;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26631;&#35760;&#32423;&#21035;&#20132;&#38169;&#29983;&#25104;&#26469;&#25945;&#25480;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#65292;&#26080;&#38656;&#30452;&#25509;&#30417;&#30563;&#65292;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#34701;&#21512;&#27599;&#20010;&#27169;&#22411;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#32852;&#21512;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03870</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to Decode Collaboratively with Multiple Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03870
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20102;&#19968;&#31181;&#21327;&#20316;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26631;&#35760;&#32423;&#21035;&#20132;&#38169;&#29983;&#25104;&#26469;&#25945;&#25480;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#65292;&#26080;&#38656;&#30452;&#25509;&#30417;&#30563;&#65292;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#34701;&#21512;&#27599;&#20010;&#27169;&#22411;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#32852;&#21512;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26631;&#35760;&#32423;&#21035;&#20132;&#38169;&#23427;&#20204;&#30340;&#29983;&#25104;&#26469;&#25945;&#25480;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21327;&#20316;&#12290;&#25105;&#20204;&#23558;&#19979;&#19968;&#20010;&#26631;&#35760;&#30001;&#21738;&#20010;LLM&#29983;&#25104;&#30340;&#20915;&#31574;&#24314;&#27169;&#20026;&#28508;&#21464;&#37327;&#12290;&#36890;&#36807;&#22312;&#25105;&#20204;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#19979;&#20248;&#21270;&#35757;&#32451;&#38598;&#30340;&#36793;&#38469;&#20284;&#28982;&#65292;&#22522;&#30784;LLM&#33258;&#21160;&#23398;&#20064;&#20309;&#26102;&#29983;&#25104;&#33258;&#36523;&#20197;&#21450;&#20309;&#26102;&#35843;&#29992;&#20854;&#20013;&#19968;&#20010;&#8220;&#21161;&#25163;&#8221;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#30452;&#25509;&#30417;&#30563;&#12290;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#36827;&#34892;&#26631;&#35760;&#32423;&#21327;&#20316;&#20801;&#35768;&#34701;&#21512;&#27599;&#20010;&#27169;&#22411;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#20197;&#31526;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#21327;&#20316;&#35299;&#30721;&#22312;&#36328;&#39046;&#22495;&#35774;&#32622;&#20013;&#29305;&#21035;&#26377;&#29992;&#65292;&#20854;&#20013;&#36890;&#29992;&#22522;&#30784;LLM&#23398;&#20250;&#35843;&#29992;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#12290;&#22312;&#25191;&#34892;&#25351;&#20196;&#12289;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#21644;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32852;&#21512;&#31995;&#32479;&#30340;&#24615;&#33021;&#20248;&#20110;&#21333;&#29420;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#23398;&#20064;&#21040;&#30340;&#28508;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03870v1 Announce Type: new  Abstract: We propose a method to teach multiple large language models (LLM) to collaborate by interleaving their generations at the token level. We model the decision of which LLM generates the next token as a latent variable. By optimizing the marginal likelihood of a training set under our latent variable model, the base LLM automatically learns when to generate itself and when to call on one of the ``assistant'' language models to generate, all without direct supervision. Token-level collaboration during decoding allows for a fusion of each model's expertise in a manner tailored to the specific task at hand. Our collaborative decoding is especially useful in cross-domain settings where a generalist base LLM learns to invoke domain expert models. On instruction-following, domain-specific QA, and reasoning tasks, we show that the performance of the joint system exceeds that of the individual models. Through qualitative analysis of the learned lat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;DNN&#30340;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30830;&#20445;&#38381;&#29615;&#31283;&#23450;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#20248;&#21270;&#21442;&#25968;&#20197;&#23454;&#29616;&#25913;&#36827;&#30340;&#25511;&#21046;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#36319;&#36394;&#35823;&#24046;&#30340;&#26126;&#30830;&#19978;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.00381</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25289;&#26684;&#26391;&#26085;&#31995;&#32479;&#21453;&#27493;&#36712;&#36857;&#36319;&#36394;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Structured Deep Neural Networks-Based Backstepping Trajectory Tracking Control for Lagrangian Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00381
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;DNN&#30340;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30830;&#20445;&#38381;&#29615;&#31283;&#23450;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#20248;&#21270;&#21442;&#25968;&#20197;&#23454;&#29616;&#25913;&#36827;&#30340;&#25511;&#21046;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#36319;&#36394;&#35823;&#24046;&#30340;&#26126;&#30830;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#23398;&#20064;&#25511;&#21046;&#22120;&#65292;&#22240;&#20026;&#20854;&#20986;&#33394;&#30340;&#36924;&#36817;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#23545;&#38381;&#29615;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#24615;&#33021;&#20998;&#26512;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;DNN&#30340;&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#37319;&#29992;&#21453;&#25512;&#25216;&#26415;&#23454;&#29616;&#25289;&#26684;&#26391;&#26085;&#31995;&#32479;&#30340;&#36712;&#36857;&#36319;&#36394;&#25511;&#21046;&#12290;&#36890;&#36807;&#36866;&#24403;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#22120;&#21487;&#20197;&#30830;&#20445;&#20219;&#20309;&#20860;&#23481;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#23454;&#29616;&#38381;&#29615;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#36827;&#19968;&#27493;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36319;&#36394;&#35823;&#24046;&#30340;&#26126;&#30830;&#19978;&#38480;&#65292;&#36825;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#25511;&#21046;&#21442;&#25968;&#26469;&#23454;&#29616;&#25152;&#38656;&#30340;&#36319;&#36394;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#24403;&#31995;&#32479;&#27169;&#22411;&#26410;&#30693;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25289;&#26684;&#26391;&#26085;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00381v1 Announce Type: cross  Abstract: Deep neural networks (DNN) are increasingly being used to learn controllers due to their excellent approximation capabilities. However, their black-box nature poses significant challenges to closed-loop stability guarantees and performance analysis. In this paper, we introduce a structured DNN-based controller for the trajectory tracking control of Lagrangian systems using backing techniques. By properly designing neural network structures, the proposed controller can ensure closed-loop stability for any compatible neural network parameters. In addition, improved control performance can be achieved by further optimizing neural network parameters. Besides, we provide explicit upper bounds on tracking errors in terms of controller parameters, which allows us to achieve the desired tracking performance by properly selecting the controller parameters. Furthermore, when system models are unknown, we propose an improved Lagrangian neural net
&lt;/p&gt;</description></item><item><title>RAVEL&#25968;&#25454;&#38598;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MDAS&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#24320;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#65292;&#24378;&#35843;&#20102;&#36328;&#28608;&#27963;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17700</link><description>&lt;p&gt;
RAVEL: &#22312;&#35299;&#24320;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#26041;&#38754;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17700
&lt;/p&gt;
&lt;p&gt;
RAVEL&#25968;&#25454;&#38598;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MDAS&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#24320;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#65292;&#24378;&#35843;&#20102;&#36328;&#28608;&#27963;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#21035;&#31070;&#32463;&#20803;&#21442;&#19982;&#22810;&#20010;&#39640;&#32423;&#27010;&#24565;&#30340;&#34920;&#31034;&#12290;&#19981;&#21516;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#33021;&#25104;&#21151;&#35299;&#24320;&#36825;&#20123;&#35282;&#33394;&#65311;&#20026;&#20102;&#24110;&#21161;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RAVEL&#65288;Resolving Attribute-Value Entanglements in Language Models&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#22810;&#31181;&#29616;&#26377;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#32039;&#23494;&#25511;&#21046;&#30340;&#23450;&#37327;&#27604;&#36739;&#12290;&#25105;&#20204;&#21033;&#29992;&#30001;&#27492;&#20135;&#29983;&#30340;&#27010;&#24565;&#26694;&#26550;&#26469;&#23450;&#20041;&#26032;&#30340;Multi-task Distributed Alignment Search&#65288;MDAS&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25214;&#21040;&#28385;&#36275;&#22810;&#20010;&#22240;&#26524;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#12290;&#20197;Llama2-7B&#20316;&#20026;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#65292;MDAS&#22312;RAVEL&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#65292;&#23637;&#31034;&#20102;&#36229;&#36234;&#31070;&#32463;&#20803;&#32423;&#21035;&#20998;&#26512;&#20197;&#35782;&#21035;&#36328;&#28608;&#27963;&#30340;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#22312;https://github.com/explanare/ravel&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17700v1 Announce Type: new  Abstract: Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at https://github.com/explanare/ravel.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#21152;&#26435;&#28966;&#28857;&#21487;&#24494;MCC&#65292;&#29992;&#20110;&#25913;&#21892;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#39044;&#27979;&#21754;&#20083;&#21160;&#29289;&#34507;&#30333;&#36136;&#20013;&#30340;O-GlcNAcylation&#20301;&#28857;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;</title><link>https://arxiv.org/abs/2402.17131</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#21644;RNN&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#19979;&#39044;&#27979;&#21754;&#20083;&#21160;&#29289;&#34507;&#30333;&#36136;&#20013;&#30340;O-GlcNAcylation&#20301;&#28857;
&lt;/p&gt;
&lt;p&gt;
Predicting O-GlcNAcylation Sites in Mammalian Proteins with Transformers and RNNs Trained with a New Loss Function
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#21152;&#26435;&#28966;&#28857;&#21487;&#24494;MCC&#65292;&#29992;&#20110;&#25913;&#21892;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#39044;&#27979;&#21754;&#20083;&#21160;&#29289;&#34507;&#30333;&#36136;&#20013;&#30340;O-GlcNAcylation&#20301;&#28857;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#22522;&#21270;&#26159;&#19968;&#31181;&#34507;&#30333;&#36136;&#20462;&#39280;&#65292;&#22312;&#21151;&#33021;&#21644;&#32467;&#26500;&#19978;&#36215;&#30528;&#22810;&#31181;&#37325;&#35201;&#20316;&#29992;&#12290;O-GlcNAcylation&#26159;&#31958;&#22522;&#21270;&#30340;&#19968;&#31181;&#20122;&#22411;&#65292;&#26377;&#28508;&#21147;&#25104;&#20026;&#27835;&#30103;&#30340;&#37325;&#35201;&#38774;&#28857;&#65292;&#20294;&#22312;2023&#24180;&#20043;&#21069;&#23578;&#26410;&#26377;&#21487;&#38752;&#39044;&#27979;O-GlcNAcylation&#20301;&#28857;&#30340;&#26041;&#27861;&#65307;2021&#24180;&#30340;&#19968;&#31687;&#35780;&#35770;&#27491;&#30830;&#25351;&#20986;&#24050;&#21457;&#34920;&#30340;&#27169;&#22411;&#19981;&#36275;&#65292;&#24182;&#19988;&#26410;&#33021;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#27169;&#22411;&#24050;&#19981;&#20877;&#21487;&#29992;&#12290;2023&#24180;&#65292;&#19968;&#31687;&#20855;&#26377;F$_1$&#20998;&#25968;36.17%&#21644;MCC&#20998;&#25968;34.57%&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#30340;&#26174;&#30528;&#26356;&#22909;&#30340;RNN&#27169;&#22411;&#34987;&#21457;&#34920;&#12290;&#26412;&#25991;&#39318;&#27425;&#35797;&#22270;&#36890;&#36807;Transformer&#32534;&#30721;&#22120;&#25552;&#39640;&#36825;&#20123;&#25351;&#26631;&#12290;&#23613;&#31649;Transformer&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#24615;&#33021;&#20173;&#19981;&#21450;&#20808;&#21069;&#21457;&#34920;&#30340;RNN&#12290;&#28982;&#21518;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#21152;&#26435;&#28966;&#28857;&#21487;&#24494;MCC&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17131v1 Announce Type: new  Abstract: Glycosylation, a protein modification, has multiple essential functional and structural roles. O-GlcNAcylation, a subtype of glycosylation, has the potential to be an important target for therapeutics, but methods to reliably predict O-GlcNAcylation sites had not been available until 2023; a 2021 review correctly noted that published models were insufficient and failed to generalize. Moreover, many are no longer usable. In 2023, a considerably better RNN model with an F$_1$ score of 36.17% and an MCC of 34.57% on a large dataset was published. This article first sought to improve these metrics using transformer encoders. While transformers displayed high performance on this dataset, their performance was inferior to that of the previously published RNN. We then created a new loss function, which we call the weighted focal differentiable MCC, to improve the performance of classification models. RNN models trained with this new function di
&lt;/p&gt;</description></item><item><title>&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;GRU&#26550;&#26500;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#20013;&#22797;&#26434;&#25705;&#25830;&#23450;&#24459;&#21160;&#21147;&#23398;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#27169;&#25311;&#25705;&#25830;&#36807;&#31243;&#29289;&#29702;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14148</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#19982;&#25705;&#25830;&#65306;&#28369;&#21160;&#12289;&#20445;&#25345;&#12289;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neural Networks and Friction: Slide, Hold, Learn
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14148
&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;GRU&#26550;&#26500;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#20013;&#22797;&#26434;&#25705;&#25830;&#23450;&#24459;&#21160;&#21147;&#23398;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#27169;&#25311;&#25705;&#25830;&#36807;&#31243;&#29289;&#29702;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#26550;&#26500;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#20855;&#26377;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#20013;&#36895;&#29575;&#19982;&#29366;&#24577;&#25705;&#25830;&#23450;&#24459;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#33021;&#21147;&#12290;&#29992;&#20110;&#35757;&#32451;&#32593;&#32476;&#30340;&#25968;&#25454;&#36890;&#36807;&#24212;&#29992;&#20256;&#32479;&#36895;&#29575;&#19982;&#29366;&#24577;&#25705;&#25830;&#26041;&#31243;&#32467;&#21512;&#29366;&#24577;&#28436;&#21270;&#32769;&#21270;&#23450;&#24459;&#29983;&#25104;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#21046;&#23450;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#26126;&#30830;&#32771;&#34385;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21021;&#22987;&#26465;&#20214;&#12289;&#30452;&#25509;&#25928;&#24212;&#20197;&#21450;&#29366;&#24577;&#21464;&#37327;&#30340;&#28436;&#21464;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#26377;GRU&#26550;&#26500;&#30340;RNN&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#39044;&#27979;&#25705;&#25830;&#31995;&#25968;&#30001;&#20110;&#36895;&#24230;&#36339;&#36291;&#32780;&#20135;&#29983;&#30340;&#21464;&#21270;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#27169;&#25311;&#25705;&#25830;&#36807;&#31243;&#29289;&#29702;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14148v1 Announce Type: cross  Abstract: In this study, it is demonstrated that Recurrent Neural Networks (RNNs), specifically those utilizing Gated Recurrent Unit (GRU) architecture, possess the capability to learn the complex dynamics of rate-and-state friction laws from synthetic data. The data employed for training the network is generated through the application of traditional rate-and-state friction equations coupled with the aging law for state evolution. A novel aspect of our approach is the formulation of a loss function that explicitly accounts for initial conditions, the direct effect, and the evolution of state variables during training. It is found that the RNN, with its GRU architecture, effectively learns to predict changes in the friction coefficient resulting from velocity jumps, thereby showcasing the potential of machine learning models in understanding and simulating the physics of frictional processes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934; StrongREJECT&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#36136;&#37327;&#30340;&#38382;&#39064;&#65292;&#26356;&#22909;&#22320;&#21306;&#20998;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#31354;&#30772;&#35299;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10260</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#31354;&#30772;&#35299;&#30340;&#24378;REJECT&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A StrongREJECT for Empty Jailbreaks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10260
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934; StrongREJECT&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#36136;&#37327;&#30340;&#38382;&#39064;&#65292;&#26356;&#22909;&#22320;&#21306;&#20998;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#31354;&#30772;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#24341;&#36215;&#20102;&#23545;&#8220;&#30772;&#35299;&#8221;&#30340;&#20851;&#27880;&#65292;&#36825;&#31181;&#30772;&#35299;&#20801;&#35768;&#27169;&#22411;&#34987;&#24694;&#24847;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#26631;&#20934;&#30340;&#22522;&#20934;&#26469;&#34913;&#37327;&#30772;&#35299;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#23548;&#33268;&#30772;&#35299;&#35770;&#25991;&#30340;&#20316;&#32773;&#19981;&#24471;&#19981;&#33258;&#34892;&#21019;&#24314;&#26631;&#20934;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;&#22522;&#20934;&#32463;&#24120;&#21253;&#21547;&#27169;&#26865;&#20004;&#21487;&#25110;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20542;&#21521;&#20110;&#39640;&#20272;&#20302;&#36136;&#37327;&#27169;&#22411;&#21709;&#24212;&#30340;&#28389;&#29992;&#28508;&#21147;&#30340;&#35780;&#20998;&#26631;&#20934;&#12290;&#19968;&#20123;&#30772;&#35299;&#25216;&#26415;&#20351;&#38382;&#39064;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#23427;&#20204;&#21363;&#20351;&#23545;&#20110;&#33391;&#24615;&#38382;&#39064;&#20063;&#20250;&#38477;&#20302;&#27169;&#22411;&#21709;&#24212;&#30340;&#36136;&#37327;&#65306;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#31181;&#30772;&#35299;&#25216;&#26415;&#26174;&#30528;&#38477;&#20302;&#20102;GPT-4&#22312;MMLU&#19978;&#30340;&#38646;&#23556;&#20987;&#34920;&#29616;&#12290;&#30772;&#35299;&#36824;&#20250;&#20351;&#20174;&#8220;&#26410;&#32463;&#23457;&#26597;&#8221;&#30340;&#24320;&#28304;&#27169;&#22411;&#20013;&#33719;&#21462;&#26377;&#23475;&#21709;&#24212;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;StrongREJECT&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#36136;&#37327;&#30340;&#38382;&#39064;&#26356;&#22909;&#22320;&#21306;&#20998;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#30772;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10260v1 Announce Type: cross  Abstract: The rise of large language models (LLMs) has drawn attention to the existence of "jailbreaks" that allow the models to be used maliciously. However, there is no standard benchmark for measuring the severity of a jailbreak, leaving authors of jailbreak papers to create their own. We show that these benchmarks often include vague or unanswerable questions and use grading criteria that are biased towards overestimating the misuse potential of low-quality model responses. Some jailbreak techniques make the problem worse by decreasing the quality of model responses even on benign questions: we show that several jailbreaking techniques substantially reduce the zero-shot performance of GPT-4 on MMLU. Jailbreaks can also make it harder to elicit harmful responses from an "uncensored" open-source model. We present a new benchmark, StrongREJECT, which better discriminates between effective and ineffective jailbreaks by using a higher-quality que
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22810;&#20010;&#24230;&#37327;&#25351;&#26631;&#19978;&#35780;&#20272;&#22810;&#31181;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#26469;&#30740;&#31350;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#26041;&#27861;&#22635;&#34917;&#20102;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#27604;&#36739;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#25512;&#36827;&#20102;&#35780;&#20272;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2402.09766</link><description>&lt;p&gt;
&#20174;&#21464;&#21160;&#24615;&#21040;&#31283;&#23450;&#24615;&#65306;&#25512;&#33616;&#31995;&#32479;&#22522;&#20934;&#21270;&#23454;&#36341;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
From Variability to Stability: Advancing RecSys Benchmarking Practices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22810;&#20010;&#24230;&#37327;&#25351;&#26631;&#19978;&#35780;&#20272;&#22810;&#31181;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#26469;&#30740;&#31350;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36825;&#19968;&#26041;&#27861;&#22635;&#34917;&#20102;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#27604;&#36739;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#25512;&#36827;&#20102;&#35780;&#20272;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#20013;&#65292;&#26032;&#30340;&#31639;&#27861;&#32463;&#24120;&#36890;&#36807;&#23545;&#19968;&#32452;&#26377;&#38480;&#30340;&#20219;&#24847;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#26469;&#22768;&#31216;&#33258;&#24049;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20840;&#38754;&#21453;&#26144;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#20844;&#24179;&#21644;&#31283;&#20581;&#30340;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#27604;&#36739;&#65292;&#20174;&#32780;&#25512;&#36827;&#35780;&#20272;&#23454;&#36341;&#12290;&#36890;&#36807;&#21033;&#29992;&#21253;&#25324;&#26412;&#25991;&#20171;&#32461;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;30&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;9&#20010;&#24230;&#37327;&#25351;&#26631;&#19978;&#35780;&#20272;11&#31181;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23558;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#32858;&#21512;&#25104;&#19968;&#20010;&#32479;&#19968;&#25490;&#21517;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09766v1 Announce Type: cross  Abstract: In the rapidly evolving domain of Recommender Systems (RecSys), new algorithms frequently claim state-of-the-art performance based on evaluations over a limited set of arbitrarily selected datasets. However, this approach may fail to holistically reflect their effectiveness due to the significant impact of dataset characteristics on algorithm performance. Addressing this deficiency, this paper introduces a novel benchmarking methodology to facilitate a fair and robust comparison of RecSys algorithms, thereby advancing evaluation practices. By utilizing a diverse set of $30$ open datasets, including two introduced in this work, and evaluating $11$ collaborative filtering algorithms across $9$ metrics, we critically examine the influence of dataset characteristics on algorithm performance. We further investigate the feasibility of aggregating outcomes from multiple datasets into a unified ranking. Through rigorous experimental analysis, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#38598;&#19978;&#35745;&#31639;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#26512;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#35745;&#31639;&#30340;Hessian&#30697;&#38453;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#31867;&#38388;&#24179;&#22343;&#36317;&#31163;&#21644;&#26368;&#23567;&#21270;&#31867;&#20869;&#26041;&#24046;&#65292;&#20197;&#21450;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20004;&#20010;&#30697;&#38453;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#26041;&#21521;&#30340;&#32452;&#21512;&#31354;&#38388;&#26469;&#23454;&#29616;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#23454;&#35777;&#39564;&#35777;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09281</link><description>&lt;p&gt;
&#25552;&#21319;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#21327;&#26041;&#24046;&#21644;Hessian&#30697;&#38453;&#30340;&#21327;&#21516;&#29305;&#24449;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Synergistic eigenanalysis of covariance and Hessian matrices for enhanced binary classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#38598;&#19978;&#35745;&#31639;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#26512;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#35745;&#31639;&#30340;Hessian&#30697;&#38453;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#31867;&#38388;&#24179;&#22343;&#36317;&#31163;&#21644;&#26368;&#23567;&#21270;&#31867;&#20869;&#26041;&#24046;&#65292;&#20197;&#21450;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20004;&#20010;&#30697;&#38453;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#26041;&#21521;&#30340;&#32452;&#21512;&#31354;&#38388;&#26469;&#23454;&#29616;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#23454;&#35777;&#39564;&#35777;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#21327;&#26041;&#24046;&#21644;Hessian&#30697;&#38453;&#20998;&#21035;&#34987;&#21333;&#29420;&#20998;&#26512;&#65292;&#20294;&#26159;&#23558;&#36825;&#20123;&#30697;&#38453;&#38598;&#25104;&#36215;&#26469;&#21487;&#20197;&#22686;&#24378;&#23427;&#20204;&#22312;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#38598;&#19978;&#35745;&#31639;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#26512;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#35745;&#31639;&#30340;Hessian&#30697;&#38453;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24418;&#24335;&#21270;&#35777;&#26126;&#35777;&#26126;&#20102;&#23427;&#21487;&#20197;&#26368;&#22823;&#21270;&#31867;&#38388;&#24179;&#22343;&#36317;&#31163;&#24182;&#26368;&#23567;&#21270;&#31867;&#20869;&#26041;&#24046;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20004;&#20010;&#30697;&#38453;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#26041;&#21521;&#30340;&#32452;&#21512;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#25353;&#29031;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#30340;&#26631;&#20934;&#23454;&#29616;&#20102;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#23545;&#31070;&#32463;&#32593;&#32476;&#21644;&#20581;&#24247;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#39564;&#35777;&#22987;&#32456;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09281v1 Announce Type: cross Abstract: Covariance and Hessian matrices have been analyzed separately in the literature for classification problems. However, integrating these matrices has the potential to enhance their combined power in improving classification performance. We present a novel approach that combines the eigenanalysis of a covariance matrix evaluated on a training set with a Hessian matrix evaluated on a deep learning model to achieve optimal class separability in binary classification tasks. Our approach is substantiated by formal proofs that establish its capability to maximize between-class mean distance and minimize within-class variances. By projecting data into the combined space of the most relevant eigendirections from both matrices, we achieve optimal class separability as per the linear discriminant analysis (LDA) criteria. Empirical validation across neural and health datasets consistently supports our theoretical framework and demonstrates that our
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#20154;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#20174;&#30446;&#26631;&#39046;&#22495;&#37319;&#38598;&#26080;&#20559;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#20174;&#28304;&#39046;&#22495;&#33719;&#21462;&#25968;&#25454;&#30340;&#25104;&#26412;&#25928;&#30410;&#24615;&#12290;&#21516;&#26102;&#65292;&#24635;&#32467;&#20102;&#19981;&#21516;&#38382;&#39064;&#35774;&#32622;&#19979;&#30340;&#35774;&#35745;&#32771;&#34385;&#21644;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.04580</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04580
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#20154;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#20174;&#30446;&#26631;&#39046;&#22495;&#37319;&#38598;&#26080;&#20559;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#20174;&#28304;&#39046;&#22495;&#33719;&#21462;&#25968;&#25454;&#30340;&#25104;&#26412;&#25928;&#30410;&#24615;&#12290;&#21516;&#26102;&#65292;&#24635;&#32467;&#20102;&#19981;&#21516;&#38382;&#39064;&#35774;&#32622;&#19979;&#30340;&#35774;&#35745;&#32771;&#34385;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#34028;&#21187;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#22823;&#37327;&#25968;&#25454;&#30340;&#38656;&#27714;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26114;&#36149;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#21644;&#20005;&#26684;&#30340;&#23433;&#20840;&#35201;&#27714;&#65292;&#20174;&#30446;&#26631;&#39046;&#22495;&#25910;&#38598;&#36275;&#22815;&#30340;&#26080;&#20559;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#37319;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#28304;&#39046;&#22495;&#25968;&#25454;&#65288;&#20363;&#22914;&#27169;&#25311;&#21644;&#23454;&#39564;&#23460;&#29615;&#22659;&#65289;&#65292;&#20197;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#30340;&#25968;&#25454;&#33719;&#21462;&#21644;&#24555;&#36895;&#27169;&#22411;&#36845;&#20195;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28304;&#39046;&#22495;&#30340;&#29615;&#22659;&#21644;&#20855;&#36523;&#26041;&#24335;&#21487;&#33021;&#19982;&#30446;&#26631;&#39046;&#22495;&#30340;&#29305;&#24449;&#30456;&#24046;&#24456;&#22823;&#65292;&#24378;&#35843;&#20102;&#26377;&#25928;&#30340;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#23545;&#29616;&#26377;&#30340;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#12290;&#36890;&#36807;&#23545;&#39046;&#22495;&#24046;&#36317;&#30340;&#31934;&#32454;&#20998;&#31867;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#27599;&#20010;&#38382;&#39064;&#35774;&#32622;&#30340;&#24635;&#20307;&#35265;&#35299;&#21644;&#35774;&#35745;&#32771;&#34385;&#12290;&#25105;&#20204;&#36824;&#23601;&#20351;&#29992;&#30340;&#20851;&#38190;&#26041;&#27861;&#36827;&#34892;&#20102;&#39640;&#23618;&#27425;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
The burgeoning fields of robot learning and embodied AI have triggered an increasing demand for large quantities of data. However, collecting sufficient unbiased data from the target domain remains a challenge due to costly data collection processes and stringent safety requirements. Consequently, researchers often resort to data from easily accessible source domains, such as simulation and laboratory environments, for cost-effective data acquisition and rapid model iteration. Nevertheless, the environments and embodiments of these source domains can be quite different from their target domain counterparts, underscoring the need for effective cross-domain policy transfer approaches. In this paper, we conduct a systematic review of existing cross-domain policy transfer methods. Through a nuanced categorization of domain gaps, we encapsulate the overarching insights and design considerations of each problem setting. We also provide a high-level discussion about the key methodologies used
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#38142;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#26679;&#26412;&#36716;&#21270;&#20026;&#38750;&#32447;&#24615;&#22495;&#24182;&#21033;&#29992;&#23398;&#20064;&#26426;&#21046;&#26500;&#24314;&#33258;&#34920;&#31034;&#30697;&#38453;&#65292;&#21516;&#26102;&#24341;&#20837;&#23616;&#37096;&#30456;&#20284;&#24615;&#27491;&#21017;&#21270;&#20197;&#22686;&#24378;&#20998;&#32452;&#25928;&#24212;&#65292;&#25913;&#21892;&#32858;&#31867;&#32467;&#26524;&#36136;&#37327;&#65292;&#24182;&#19988;&#22312;&#20445;&#35777;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#21069;&#25552;&#19979;&#33719;&#24471;&#33391;&#22909;&#30340;&#32858;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02051</link><description>&lt;p&gt;
&#22522;&#20110;&#21151;&#33021;&#38142;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#23376;&#31354;&#38388;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Nonlinear subspace clustering by functional link neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#38142;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#26679;&#26412;&#36716;&#21270;&#20026;&#38750;&#32447;&#24615;&#22495;&#24182;&#21033;&#29992;&#23398;&#20064;&#26426;&#21046;&#26500;&#24314;&#33258;&#34920;&#31034;&#30697;&#38453;&#65292;&#21516;&#26102;&#24341;&#20837;&#23616;&#37096;&#30456;&#20284;&#24615;&#27491;&#21017;&#21270;&#20197;&#22686;&#24378;&#20998;&#32452;&#25928;&#24212;&#65292;&#25913;&#21892;&#32858;&#31867;&#32467;&#26524;&#36136;&#37327;&#65292;&#24182;&#19988;&#22312;&#20445;&#35777;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#21069;&#25552;&#19979;&#33719;&#24471;&#33391;&#22909;&#30340;&#32858;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#27604;&#19968;&#20123;&#20808;&#36827;&#30340;&#23376;&#31354;&#38388;&#32858;&#31867;&#31639;&#27861;&#25552;&#20379;&#26356;&#22909;&#30340;&#32858;&#31867;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#38656;&#35201;&#36827;&#34892;&#24179;&#34913;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#21151;&#33021;&#38142;&#25509;&#31070;&#32463;&#32593;&#32476;&#23558;&#25968;&#25454;&#26679;&#26412;&#36716;&#21270;&#20026;&#38750;&#32447;&#24615;&#22495;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#26426;&#21046;&#21033;&#29992;&#26144;&#23556;&#26679;&#26412;&#26500;&#24314;&#33258;&#34920;&#31034;&#30697;&#38453;&#12290;&#30001;&#20110;&#21151;&#33021;&#38142;&#25509;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#20010;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20445;&#35777;&#33391;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#30340;&#21516;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#30456;&#20284;&#24615;&#27491;&#21017;&#21270;&#20197;&#22686;&#24378;&#20998;&#32452;&#25928;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#32858;&#31867;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#20984;&#32452;&#21512;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#26696;&#65292;&#23427;&#32467;&#21512;&#20102;&#32447;&#24615;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear subspace clustering based on a feed-forward neural network has been demonstrated to provide better clustering accuracy than some advanced subspace clustering algorithms. While this approach demonstrates impressive outcomes, it involves a balance between effectiveness and computational cost. In this study, we employ a functional link neural network to transform data samples into a nonlinear domain. Subsequently, we acquire a self-representation matrix through a learning mechanism that builds upon the mapped samples. As the functional link neural network is a single-layer neural network, our proposed method achieves high computational efficiency while ensuring desirable clustering performance. By incorporating the local similarity regularization to enhance the grouping effect, our proposed method further improves the quality of the clustering results. Additionally, we introduce a convex combination subspace clustering scheme, which combining a linear subspace clustering method 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#35745;&#31639;&#36229;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#24182;&#36827;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#36229;&#22270;&#20013;&#33410;&#28857;&#36317;&#31163;&#35745;&#31639;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25299;&#23637;&#20102;&#36229;&#22270;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2401.13054</link><description>&lt;p&gt;
&#26080;&#35745;&#31639;&#22256;&#38590;&#30340;&#24555;&#36895;&#35745;&#31639;&#36229;&#22270;&#33410;&#28857;&#36317;&#31163;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Frustrated Random Walks: A Fast Method to Compute Node Distances on Hypergraphs. (arXiv:2401.13054v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#35745;&#31639;&#36229;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#24182;&#36827;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#36229;&#22270;&#20013;&#33410;&#28857;&#36317;&#31163;&#35745;&#31639;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25299;&#23637;&#20102;&#36229;&#22270;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#26159;&#22270;&#30340;&#25512;&#24191;&#65292;&#24403;&#32771;&#34385;&#23454;&#20307;&#38388;&#30340;&#23646;&#24615;&#20849;&#20139;&#26102;&#20250;&#33258;&#28982;&#20135;&#29983;&#12290;&#23613;&#31649;&#21487;&#20197;&#36890;&#36807;&#23558;&#36229;&#36793;&#25193;&#23637;&#20026;&#23436;&#20840;&#36830;&#25509;&#30340;&#23376;&#22270;&#26469;&#23558;&#36229;&#22270;&#36716;&#25442;&#20026;&#22270;&#65292;&#20294;&#36870;&#21521;&#25805;&#20316;&#22312;&#35745;&#31639;&#19978;&#38750;&#24120;&#22797;&#26434;&#19988;&#23646;&#20110;NP-complete&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20551;&#35774;&#36229;&#22270;&#21253;&#21547;&#27604;&#22270;&#26356;&#22810;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#30452;&#25509;&#25805;&#20316;&#36229;&#22270;&#27604;&#23558;&#20854;&#25193;&#23637;&#20026;&#22270;&#26356;&#20026;&#26041;&#20415;&#12290;&#36229;&#22270;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#26159;&#22914;&#20309;&#31934;&#30830;&#39640;&#25928;&#22320;&#35745;&#31639;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#36890;&#36807;&#20272;&#35745;&#33410;&#28857;&#36317;&#31163;&#65292;&#25105;&#20204;&#33021;&#22815;&#25214;&#21040;&#33410;&#28857;&#30340;&#26368;&#36817;&#37051;&#23621;&#65292;&#24182;&#20351;&#29992;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#26041;&#27861;&#22312;&#36229;&#22270;&#19978;&#25191;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#36229;&#22270;&#19978;&#36827;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#25105;&#20204;&#23558;&#33410;&#28857;&#36317;&#31163;&#20272;&#35745;&#20026;&#38543;&#26426;&#28216;&#36208;&#30340;&#39044;&#26399;&#21040;&#36798;&#26102;&#38388;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#31616;&#21333;&#38543;&#26426;&#28216;&#36208;&#65288;SRW&#65289;&#26080;&#27861;&#20934;&#30830;&#25551;&#36848;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#22240;&#27492;&#25105;&#20204;&#24341;&#20837;&#20102;"frustrated"&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
A hypergraph is a generalization of a graph that arises naturally when attribute-sharing among entities is considered. Although a hypergraph can be converted into a graph by expanding its hyperedges into fully connected subgraphs, going the reverse way is computationally complex and NP-complete. We therefore hypothesize that a hypergraph contains more information than a graph. In addition, it is more convenient to manipulate a hypergraph directly, rather than expand it into a graph. An open problem in hypergraphs is how to accurately and efficiently calculate their node distances. Estimating node distances enables us to find a node's nearest neighbors, and perform label propagation on hypergraphs using a K-nearest neighbors (KNN) approach. In this paper, we propose a novel approach based on random walks to achieve label propagation on hypergraphs. We estimate node distances as the expected hitting times of random walks. We note that simple random walks (SRW) cannot accurately describe 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#33258;&#28982;&#23398;&#20064;&#29615;&#22659;&#20013;&#36890;&#36807;&#22238;&#24518;&#26041;&#27861;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#24178;&#25200;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#21151;&#29575;&#27861;&#21017;&#30340;&#21551;&#21457;&#12290;</title><link>http://arxiv.org/abs/2401.10393</link><description>&lt;p&gt;
&#33258;&#28982;&#30340;&#21151;&#29575;&#27861;&#21017;&#23398;&#20064;&#29615;&#22659;&#20013;&#33021;&#22815;&#20943;&#36731;&#28798;&#38590;&#24615;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Catastrophic Interference is Mitigated in Naturalistic Power-Law Learning Environments. (arXiv:2401.10393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#33258;&#28982;&#23398;&#20064;&#29615;&#22659;&#20013;&#36890;&#36807;&#22238;&#24518;&#26041;&#27861;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#24178;&#25200;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#21151;&#29575;&#27861;&#21017;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#36973;&#21463;&#28798;&#38590;&#24615;&#24178;&#25200;&#65288;CI&#65289;&#65306;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#34920;&#29616;&#26174;&#33879;&#19979;&#38477;&#12290;&#36825;&#19982;&#20154;&#31867;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#20154;&#31867;&#21487;&#20197;&#36830;&#32493;&#23398;&#20064;&#26032;&#20219;&#21153;&#32780;&#19981;&#20250;&#26126;&#26174;&#24536;&#35760;&#20808;&#21069;&#30340;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#20943;&#36731;CI&#30340;&#25216;&#26415;&#65292;&#20363;&#22914;&#27491;&#21017;&#21270;&#12289;&#22238;&#24518;&#12289;&#29983;&#25104;&#24615;&#22238;&#25918;&#21644;&#27987;&#32553;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#30340;&#25351;&#23548;&#65292;&#35813;&#30740;&#31350;&#34920;&#26126;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#65292;&#36935;&#21040;&#20219;&#21153;&#30340;&#27010;&#29575;&#19982;&#26368;&#21518;&#19968;&#27425;&#25191;&#34892;&#20219;&#21153;&#30340;&#26102;&#38388;&#25104;&#21151;&#29575;&#27861;&#21017;&#36882;&#20943;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#27169;&#25311;&#33258;&#28982;&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#20943;&#36731;CI&#25216;&#26415;&#30340;&#30495;&#23454;&#35780;&#20272;&#26159;&#24517;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#31867;&#20284;&#20154;&#31867;&#38754;&#20020;&#30340;&#21151;&#29575;&#27861;&#21017;&#29615;&#22659;&#20013;&#35757;&#32451;&#31616;&#21333;&#30340;&#22238;&#24518;&#26041;&#27861;&#26102;&#65292;CI&#30340;&#20943;&#36731;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#36825;&#31181;&#22522;&#20110;&#22238;&#24518;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks often suffer from catastrophic interference (CI): performance on previously learned tasks drops off significantly when learning a new task. This contrasts strongly with humans, who can sequentially learn new tasks without appreciably forgetting previous tasks. Prior work has explored various techniques for mitigating CI such as regularization, rehearsal, generative replay, and distillation methods. The current work takes a different approach, one guided by cognitive science research showing that in naturalistic environments, the probability of encountering a task decreases as a power-law of the time since it was last performed. We argue that a realistic evaluation of techniques for the mitigation of CI should be performed in simulated naturalistic learning environments. Thus, we evaluate the extent of mitigation of CI when training simple rehearsal-based methods in power-law environments similar to the ones humans face. Our work explores this novel rehearsal-based appro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#21345;&#36710;&#22810;&#20998;&#27573;&#38656;&#27714;&#36335;&#24452;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#25193;&#23637;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#20379;&#24212;&#38142;&#29289;&#27969;&#20013;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.08669</link><description>&lt;p&gt;
&#22810;&#21345;&#36710;&#22810;&#20998;&#27573;&#38656;&#27714;&#36335;&#24452;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems with Multi-Leg Demand Routes. (arXiv:2401.08669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22810;&#21345;&#36710;&#22810;&#20998;&#27573;&#38656;&#27714;&#36335;&#24452;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#25193;&#23637;&#65292;&#23454;&#29616;&#20102;&#22312;&#24037;&#19994;&#20379;&#24212;&#38142;&#29289;&#27969;&#20013;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#22312;&#35299;&#20915;&#19968;&#20123;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#26426;&#21046;&#29983;&#25104;&#30340;&#31574;&#30053;&#26102;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#20123;&#30456;&#23545;&#31616;&#21333;&#30340;&#38382;&#39064;&#23454;&#20363;&#65292;&#36825;&#20123;&#25216;&#26415;&#24050;&#32463;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#19968;&#20123;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#21644;&#38750;&#24120;&#22797;&#26434;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#21464;&#20307;&#65292;&#23578;&#26410;&#35777;&#26126;&#26377;&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#29992;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#19968;&#31181;&#36825;&#26679;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#36742;&#21345;&#36710;&#21644;&#22810;&#20998;&#27573;&#36335;&#24452;&#35201;&#27714;&#12290;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#38656;&#27714;&#38656;&#35201;&#27839;&#30528;&#33410;&#28857;&#24207;&#21015;&#31227;&#21160;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20174;&#36215;&#28857;&#21040;&#32456;&#28857;&#12290;&#20026;&#20102;&#20351;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25104;&#20026;&#36866;&#29992;&#20110;&#23454;&#38469;&#24037;&#19994;&#35268;&#27169;&#30340;&#20379;&#24212;&#38142;&#29289;&#27969;&#30340;&#26377;&#25928;&#31574;&#30053;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#20102;&#26032;&#25193;&#23637;&#65292;&#20351;&#20854;&#33021;&#22788;&#29702;&#22810;&#21345;&#36710;&#21644;&#22810;&#20998;&#27573;&#36335;&#24452;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#36825;&#26679;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#35757;&#32451;&#19979;&#36827;&#34892;&#65292;&#24182;&#33021;&#22312;&#24037;&#19994;&#20379;&#24212;&#38142;&#29289;&#27969;&#20013;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (RL) has been shown to be effective in producing approximate solutions to some vehicle routing problems (VRPs), especially when using policies generated by encoder-decoder attention mechanisms. While these techniques have been quite successful for relatively simple problem instances, there are still under-researched and highly complex VRP variants for which no effective RL method has been demonstrated. In this work we focus on one such VRP variant, which contains multiple trucks and multi-leg routing requirements. In these problems, demand is required to move along sequences of nodes, instead of just from a start node to an end node. With the goal of making deep RL a viable strategy for real-world industrial-scale supply chain logistics, we develop new extensions to existing encoder-decoder attention models which allow them to handle multiple trucks and multi-leg routing requirements. Our models have the advantage that they can be trained for a small number 
&lt;/p&gt;</description></item><item><title>LQ-LoRA&#26159;&#19968;&#31181;&#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#20869;&#23384;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#23558;&#27599;&#20010;&#39044;&#35757;&#32451;&#30697;&#38453;&#20998;&#35299;&#20026;&#39640;&#31934;&#24230;&#20302;&#31209;&#37096;&#20998;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#37327;&#21270;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#37197;&#32622;&#37327;&#21270;&#21442;&#25968;&#20197;&#21450;&#23545;&#37325;&#26500;&#30446;&#26631;&#36827;&#34892;&#21152;&#26435;&#30340;&#20248;&#21270;&#65292;&#24182;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;QLoRA&#21644;GPTQ-LoRA&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.12023</link><description>&lt;p&gt;
LQ-LoRA: &#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#29992;&#20110;&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning. (arXiv:2311.12023v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.12023
&lt;/p&gt;
&lt;p&gt;
LQ-LoRA&#26159;&#19968;&#31181;&#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#20869;&#23384;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#23558;&#27599;&#20010;&#39044;&#35757;&#32451;&#30697;&#38453;&#20998;&#35299;&#20026;&#39640;&#31934;&#24230;&#20302;&#31209;&#37096;&#20998;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#37327;&#21270;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#37197;&#32622;&#37327;&#21270;&#21442;&#25968;&#20197;&#21450;&#23545;&#37325;&#26500;&#30446;&#26631;&#36827;&#34892;&#21152;&#26435;&#30340;&#20248;&#21270;&#65292;&#24182;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;QLoRA&#21644;GPTQ-LoRA&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20869;&#23384;&#39640;&#25928;&#30340;&#33258;&#36866;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#36845;&#20195;&#31639;&#27861;&#23558;&#27599;&#20010;&#39044;&#35757;&#32451;&#30697;&#38453;&#20998;&#35299;&#20026;&#39640;&#31934;&#24230;&#20302;&#31209;&#37096;&#20998;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#37327;&#21270;&#37096;&#20998;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#37327;&#21270;&#37096;&#20998;&#20445;&#25345;&#22266;&#23450;&#65292;&#21482;&#26377;&#20302;&#31209;&#37096;&#20998;&#34987;&#26356;&#26032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#37327;&#21270;&#37096;&#20998;&#30340;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#34920;&#36798;&#65292;&#21487;&#20197;&#26681;&#25454;&#24635;&#20307;&#20869;&#23384;&#39044;&#31639;&#21160;&#24577;&#37197;&#32622;&#37327;&#21270;&#21442;&#25968;&#65288;&#20363;&#22914;&#27604;&#29305;&#23485;&#24230;&#12289;&#22359;&#22823;&#23567;&#65289;&#32473;&#23450;&#27599;&#20010;&#30697;&#38453;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#25968;&#25454;&#24863;&#30693;&#29256;&#26412;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#36817;&#20284;&#26469;&#21152;&#26435;&#30697;&#38453;&#20998;&#35299;&#36807;&#31243;&#20013;&#30340;&#37325;&#26500;&#30446;&#26631;&#12290;&#22312;RoBERTa&#21644;LLaMA-2&#65288;7B&#21644;70B&#65289;&#30340;&#24494;&#35843;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65288;LQ-LoRA&#65289;&#20248;&#20110;&#24378;&#22522;&#32447;&#26041;&#27861;QLoRA&#21644;GPTQ-LoRA&#65292;&#24182;&#23454;&#29616;&#20102;&#28608;&#36827;&#30340;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20844;&#24179;&#20108;&#20998;&#31867;&#22120;&#20013;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#24182;&#25581;&#31034;&#20102;&#38024;&#23545;&#20844;&#24179;&#22686;&#24378;&#27169;&#22411;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#26080;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20844;&#24179;&#24615;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#20013;&#22823;&#22810;&#25968;&#23376;&#32676;&#20307;&#30340;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2311.03865</link><description>&lt;p&gt;
&#24403;&#20844;&#24179;&#24615;&#36935;&#35265;&#38544;&#31169;&#65306;&#36890;&#36807;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#25506;&#32034;&#20844;&#24179;&#20108;&#20998;&#31867;&#22120;&#20013;&#30340;&#38544;&#31169;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
When Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary Classifiers through Membership Inference Attacks. (arXiv:2311.03865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20844;&#24179;&#20108;&#20998;&#31867;&#22120;&#20013;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#24182;&#25581;&#31034;&#20102;&#38024;&#23545;&#20844;&#24179;&#22686;&#24378;&#27169;&#22411;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#26080;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20844;&#24179;&#24615;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#20013;&#22823;&#22810;&#25968;&#23376;&#32676;&#20307;&#30340;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#38024;&#23545;&#20855;&#26377;&#27495;&#35270;&#34892;&#20026;&#30340;&#26377;&#20559;&#27169;&#22411;&#30340;&#20844;&#24179;&#26041;&#27861;&#65292;&#20197;&#36798;&#21040;&#20844;&#24179;&#39044;&#27979;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22522;&#20110;&#20998;&#25968;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20013;&#23384;&#22312;&#28508;&#22312;&#30340;&#28431;&#27934;&#12290;&#22312;&#36825;&#20123;&#25915;&#20987;&#20013;&#65292;&#23545;&#25163;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#39044;&#27979;&#20998;&#25968;&#25512;&#26029;&#20986;&#29305;&#23450;&#25968;&#25454;&#26679;&#26412;&#26159;&#21542;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#21457;&#29616;&#65292;&#38024;&#23545;&#20844;&#24179;&#22686;&#24378;&#27169;&#22411;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26159;&#26080;&#25928;&#30340;&#12290;&#38024;&#23545;&#36825;&#20123;&#25915;&#20987;&#35757;&#32451;&#30340;&#27169;&#22411;&#36864;&#21270;&#20026;&#31616;&#21333;&#30340;&#38408;&#20540;&#27169;&#22411;&#65292;&#20174;&#32780;&#23548;&#33268;&#25915;&#20987;&#24615;&#33021;&#38477;&#20302;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20844;&#24179;&#24615;&#26041;&#27861;&#24448;&#24448;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22823;&#22810;&#25968;&#23376;&#32676;&#20307;&#30340;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#25552;&#39640;&#20102;&#25104;&#21151;&#25915;&#20987;&#30340;&#38590;&#24230;&#65292;&#21516;&#26102;&#25193;&#22823;&#20102;&#25104;&#21592;&#21644;&#38750;&#25104;&#21592;&#25968;&#25454;&#20043;&#38388;&#30340;&#39044;&#27979;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have developed fairness methods for biased models that exhibit discriminatory behaviors towards specific subgroups. While these models have shown promise in achieving fair predictions, recent research has identified their potential vulnerability to score-based membership inference attacks (MIAs). In these attacks, adversaries can infer whether a particular data sample was used during training by analyzing the model's prediction scores. However, our investigations reveal that these score-based MIAs are ineffective when targeting fairness-enhanced models in binary classifications. The attack models trained to launch the MIAs degrade into simplistic threshold models, resulting in lower attack performance. Meanwhile, we observe that fairness methods often lead to prediction performance degradation for the majority subgroups of the training data. This raises the barrier to successful attacks and widens the prediction gaps between member and non-member data. Building upon th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21487;&#36870;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#31185;&#22827;&#38142;&#65292;&#21363;&#8220;&#22240;&#26524;Zig-Zag&#37319;&#26679;&#22120;&#8221;&#65292;&#29992;&#20110;&#25512;&#26029;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#37327;&#21464;&#37327;&#65292;&#35813;&#37319;&#26679;&#22120;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#28151;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05655</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#37327;&#36827;&#34892;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#65306;&#22312;DAG&#30340;Markov&#31561;&#20215;&#31867;&#19978;&#37319;&#26679;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Causal structure learning with momentum: Sampling distributions over Markov Equivalence Classes of DAGs. (arXiv:2310.05655v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21487;&#36870;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#31185;&#22827;&#38142;&#65292;&#21363;&#8220;&#22240;&#26524;Zig-Zag&#37319;&#26679;&#22120;&#8221;&#65292;&#29992;&#20110;&#25512;&#26029;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#37327;&#21464;&#37327;&#65292;&#35813;&#37319;&#26679;&#22120;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#28151;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#26029;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#65288;&#26377;&#21521;&#26080;&#29615;&#22270;&#65292;DAG&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#38750;&#21487;&#36870;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#31185;&#22827;&#38142;&#65292;&#21363;&#8220;&#22240;&#26524;Zig-Zag&#37319;&#26679;&#22120;&#8221;&#65292;&#35813;&#37319;&#26679;&#22120;&#38024;&#23545;&#19968;&#31867;&#35266;&#27979;&#31561;&#20215;&#65288;Markov&#31561;&#20215;&#65289;DAG&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#36825;&#20123;&#31867;&#21035;&#20197;&#23436;&#25104;&#30340;&#37096;&#20998;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;CPDAG&#65289;&#34920;&#31034;&#12290;&#38750;&#21487;&#36870;&#39532;&#23572;&#31185;&#22827;&#38142;&#20381;&#36182;&#20110;Chickering&#30340;&#36138;&#23146;&#31561;&#20215;&#25628;&#32034;&#65288;GES&#65289;&#20013;&#20351;&#29992;&#30340;&#25805;&#20316;&#31526;&#65292;&#24182;&#19988;&#20855;&#26377;&#19968;&#20010;&#21160;&#37327;&#21464;&#37327;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#28151;&#21512;&#24615;&#33021;&#12290;&#21487;&#33021;&#30340;&#30446;&#26631;&#20998;&#24067;&#21253;&#25324;&#22522;&#20110;DAG&#20808;&#39564;&#21644;Markov&#31561;&#20215;&#20284;&#28982;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#23454;&#29616;&#65292;&#20854;&#20013;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#21015;&#20030;&#12289;&#35745;&#25968;&#12289;&#22343;&#21248;&#37319;&#26679;&#21644;&#24212;&#29992;GES&#25805;&#20316;&#31526;&#30340;&#21487;&#33021;&#31227;&#21160;&#65292;&#25152;&#26377;&#36825;&#20123;&#31639;&#27861;&#37117;&#26174;&#33879;&#25913;&#36827;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of inferring a Bayesian network structure (directed acyclic graph, DAG for short), we devise a non-reversible continuous time Markov chain, the "Causal Zig-Zag sampler", that targets a probability distribution over classes of observationally equivalent (Markov equivalent) DAGs. The classes are represented as completed partially directed acyclic graphs (CPDAGs). The non-reversible Markov chain relies on the operators used in Chickering's Greedy Equivalence Search (GES) and is endowed with a momentum variable, which improves mixing significantly as we show empirically. The possible target distributions include posterior distributions based on a prior over DAGs and a Markov equivalent likelihood. We offer an efficient implementation wherein we develop new algorithms for listing, counting, uniformly sampling, and applying possible moves of the GES operators, all of which significantly improve upon the state-of-the-art.
&lt;/p&gt;</description></item><item><title>EFFL&#26159;&#19968;&#31181;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#22343;&#31561;&#20844;&#24179;&#24615;&#20197;&#20943;&#36731;&#39532;&#22826;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#20445;&#20840;&#23616;&#27169;&#22411;&#20855;&#26377;&#22343;&#31561;&#30340;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#20559;&#35265;&#26469;&#20943;&#23569;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#36164;&#28304;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.16338</link><description>&lt;p&gt;
EFFL: &#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#22343;&#31561;&#20844;&#24179;&#24615;&#20197;&#20943;&#36731;&#39532;&#22826;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
EFFL: Egalitarian Fairness in Federated Learning for Mitigating Matthew Effect. (arXiv:2309.16338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16338
&lt;/p&gt;
&lt;p&gt;
EFFL&#26159;&#19968;&#31181;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#22343;&#31561;&#20844;&#24179;&#24615;&#20197;&#20943;&#36731;&#39532;&#22826;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#20445;&#20840;&#23616;&#27169;&#22411;&#20855;&#26377;&#22343;&#31561;&#30340;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#20559;&#35265;&#26469;&#20943;&#23569;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#36164;&#28304;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#20174;&#22823;&#35268;&#27169;&#21644;&#24191;&#27867;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#21327;&#20316;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#38598;&#24322;&#26500;&#26102;&#65292;&#20256;&#32479;&#30340;FL&#26426;&#21046;&#20135;&#29983;&#30340;&#20840;&#23616;&#27169;&#22411;&#19981;&#33021;&#20805;&#20998;&#20195;&#34920;&#25317;&#26377;&#26377;&#38480;&#25968;&#25454;&#36164;&#28304;&#30340;&#36739;&#36139;&#22256;&#23458;&#25143;&#31471;&#65292;&#22312;&#20854;&#26412;&#22320;&#25968;&#25454;&#19978;&#23548;&#33268;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#39640;&#30340;&#20559;&#35265;&#12290;&#26681;&#25454;&#39532;&#22826;&#25928;&#24212;&#30340;&#25551;&#36848;&#65292;&#21363;&#20248;&#21183;&#32773;&#33719;&#24471;&#26356;&#22810;&#20248;&#21183;&#65292;&#21155;&#21183;&#32773;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#22833;&#21435;&#26356;&#22810;&#65292;&#23558;&#36825;&#26679;&#30340;&#20840;&#23616;&#27169;&#22411;&#37096;&#32626;&#22312;&#23458;&#25143;&#31471;&#24212;&#29992;&#20013;&#21487;&#33021;&#21152;&#21095;&#23458;&#25143;&#20043;&#38388;&#30340;&#36164;&#28304;&#24046;&#36317;&#65292;&#24182;&#25439;&#23475;&#31038;&#20250;&#31119;&#21033;&#21644;&#20844;&#24179;&#30340;&#21407;&#21017;&#12290;&#20026;&#20102;&#20943;&#36731;&#39532;&#22826;&#25928;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22343;&#31561;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#65288;EFFL&#65289;&#65292;&#20854;&#20013;&#22343;&#31561;&#20844;&#24179;&#24615;&#25351;&#30340;&#26159;&#20174;FL&#23398;&#20064;&#21040;&#30340;&#20840;&#23616;&#27169;&#22411;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;1&#65289;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#30456;&#31561;&#65307;&#65288;2&#65289;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20915;&#31574;&#20559;&#35265;&#30456;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in federated learning (FL) enable collaborative training of machine learning (ML) models from large-scale and widely dispersed clients while protecting their privacy. However, when different clients' datasets are heterogeneous, traditional FL mechanisms produce a global model that does not adequately represent the poorer clients with limited data resources, resulting in lower accuracy and higher bias on their local data. According to the Matthew effect, which describes how the advantaged gain more advantage and the disadvantaged lose more over time, deploying such a global model in client applications may worsen the resource disparity among the clients and harm the principles of social welfare and fairness. To mitigate the Matthew effect, we propose Egalitarian Fairness Federated Learning (EFFL), where egalitarian fairness refers to the global model learned from FL has: (1) equal accuracy among clients; (2) equal decision bias among clients. Besides achieving egalitaria
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;TVLARS&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#21487;&#37197;&#32622;&#30340;&#20989;&#25968;&#26367;&#20195;&#20102;&#28909;&#36523;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;TVLARS&#27604;LARS&#21644;LAMB&#37117;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2309.14053</link><description>&lt;p&gt;
&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#25209;&#37327;&#35757;&#32451;&#27867;&#21270;&#24615;&#33021;&#30340;LARS&#20877;&#23457;&#35270;
&lt;/p&gt;
&lt;p&gt;
Revisiting LARS for Large Batch Training Generalization of Neural Networks. (arXiv:2309.14053v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;TVLARS&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#21487;&#37197;&#32622;&#30340;&#20989;&#25968;&#26367;&#20195;&#20102;&#28909;&#36523;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;TVLARS&#27604;LARS&#21644;LAMB&#37117;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#20351;&#29992;&#36880;&#23618;&#33258;&#36866;&#24212;&#32553;&#25918;&#27604;(LARS)&#26469;&#25506;&#32034;&#22823;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;&#20855;&#26377;&#28909;&#36523;&#38454;&#27573;&#30340;LARS&#31639;&#27861;&#30001;&#20110;&#20887;&#20313;&#30340;&#27604;&#20363;&#32553;&#25918;&#23548;&#33268;&#22312;&#26089;&#26399;&#38519;&#20837;&#23574;&#38160;&#30340;&#26497;&#23567;&#21270;&#22120;&#12290;&#27492;&#22806;&#65292;&#21518;&#26399;&#22266;&#23450;&#30340;&#38497;&#23789;&#19979;&#38477;&#38480;&#21046;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#36941;&#21382;&#26089;&#26399;&#23574;&#38160;&#30340;&#26497;&#23567;&#21270;&#22120;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;Time Varying LARS (TVLARS)&#65292;&#23427;&#29992;&#21487;&#37197;&#32622;&#30340;&#31867;&#20284;sigmoid&#20989;&#25968;&#26367;&#20195;&#20102;&#28909;&#36523;&#38454;&#27573;&#65292;&#20197;&#23454;&#29616;&#22312;&#21021;&#22987;&#38454;&#27573;&#30340;&#31283;&#20581;&#35757;&#32451;&#12290;TVLARS&#22312;&#26089;&#26399;&#20419;&#36827;&#20102;&#26799;&#24230;&#25506;&#32034;&#65292;&#36229;&#36234;&#20102;&#23574;&#38160;&#30340;&#20248;&#21270;&#22120;&#65292;&#24182;&#36880;&#28176;&#36807;&#28193;&#21040;LARS&#20197;&#23454;&#29616;&#21518;&#26399;&#30340;&#31283;&#20581;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;TVLARS&#22987;&#32456;&#20248;&#20110;LARS&#21644;LAMB&#65292;&#20998;&#31867;&#22330;&#26223;&#20013;&#30340;&#25913;&#36827;&#36798;&#21040;2\%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#25152;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26696;&#20363;&#20013;&#65292;TVLARS&#37117;&#32988;&#36807;&#20102;LARS&#21644;LAMB&#65292;&#24182;&#19988;&#24615;&#33021;&#25552;&#21319;&#20102;
&lt;/p&gt;
&lt;p&gt;
This paper explores Large Batch Training techniques using layer-wise adaptive scaling ratio (LARS) across diverse settings, uncovering insights. LARS algorithms with warm-up tend to be trapped in sharp minimizers early on due to redundant ratio scaling. Additionally, a fixed steep decline in the latter phase restricts deep neural networks from effectively navigating early-phase sharp minimizers. Building on these findings, we propose Time Varying LARS (TVLARS), a novel algorithm that replaces warm-up with a configurable sigmoid-like function for robust training in the initial phase. TVLARS promotes gradient exploration early on, surpassing sharp optimizers and gradually transitioning to LARS for robustness in later phases. Extensive experiments demonstrate that TVLARS consistently outperforms LARS and LAMB in most cases, with up to 2\% improvement in classification scenarios. Notably, in all self-supervised learning cases, TVLARS dominates LARS and LAMB with performance improvements of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#19981;&#35268;&#21017;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#20132;&#21449;&#21475;&#20135;&#29983;&#30340;&#24322;&#27493;&#31354;&#38388;&#20381;&#36182;&#12289;&#19981;&#35268;&#21017;&#26102;&#38388;&#20381;&#36182;&#21644;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#39044;&#27979;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.16818</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#19981;&#35268;&#21017;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Network. (arXiv:2308.16818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16818
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#19981;&#35268;&#21017;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#20132;&#21449;&#21475;&#20135;&#29983;&#30340;&#24322;&#27493;&#31354;&#38388;&#20381;&#36182;&#12289;&#19981;&#35268;&#21017;&#26102;&#38388;&#20381;&#36182;&#21644;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#39044;&#27979;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26234;&#33021;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31995;&#32479;&#20013;&#21463;&#26234;&#33021;&#20132;&#21449;&#21475;&#25511;&#21046;&#30340;&#20132;&#21449;&#21475;&#30340;&#20132;&#36890;&#27969;&#37327;&#23545;&#20110;&#25552;&#21319;&#20132;&#36890;&#20986;&#34892;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26234;&#33021;&#20132;&#21449;&#21475;&#20135;&#29983;&#30340;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#19981;&#35268;&#21017;&#65292;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#24182;&#19988;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#24322;&#27493;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#65292;2&#65289;&#20132;&#36890;&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;3) &#38656;&#35201;&#39044;&#27979;&#30340;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#65292;&#20005;&#37325;&#24433;&#21709;&#20102;&#24403;&#21069;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;(ASeer)&#26469;&#39044;&#27979;&#26234;&#33021;&#20132;&#21449;&#21475;&#36827;&#20837;&#36710;&#36947;&#30340;&#20132;&#36890;&#29366;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#20132;&#36890;&#25193;&#25955;&#22270;&#19978;&#36830;&#25509;&#36710;&#36947;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#22270;&#25193;&#25955;&#32593;&#32476;&#26469;&#27169;&#25311;&#36710;&#36947;&#30340;&#24322;&#27493;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate traffic forecasting at intersections governed by intelligent traffic signals is critical for the advancement of an effective intelligent traffic signal control system. However, due to the irregular traffic time series produced by intelligent intersections, the traffic forecasting task becomes much more intractable and imposes three major new challenges: 1) asynchronous spatial dependency, 2) irregular temporal dependency among traffic data, and 3) variable-length sequence to be predicted, which severely impede the performance of current traffic forecasting methods. To this end, we propose an Asynchronous Spatio-tEmporal graph convolutional nEtwoRk (ASeer) to predict the traffic states of the lanes entering intelligent intersections in a future time window. Specifically, by linking lanes via a traffic diffusion graph, we first propose an Asynchronous Graph Diffusion Network to model the asynchronous spatial dependency between the time-misaligned traffic state measurements of la
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20027;&#21160;&#21644;&#34987;&#21160;&#22240;&#26524;&#25512;&#26029;&#23398;&#20064;&#30340;&#37325;&#35201;&#20551;&#35774;&#21644;&#25216;&#26415;&#65292;&#24182;&#20197;&#35752;&#35770;&#22240;&#26524;&#25512;&#26029;&#30340;&#32570;&#22833;&#26041;&#38754;&#32467;&#26463;&#65292;&#20026;&#35835;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#26679;&#24615;&#36215;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.09248</link><description>&lt;p&gt;
&#20027;&#21160;&#21644;&#34987;&#21160;&#22240;&#26524;&#25512;&#26029;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active and Passive Causal Inference Learning. (arXiv:2308.09248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20027;&#21160;&#21644;&#34987;&#21160;&#22240;&#26524;&#25512;&#26029;&#23398;&#20064;&#30340;&#37325;&#35201;&#20551;&#35774;&#21644;&#25216;&#26415;&#65292;&#24182;&#20197;&#35752;&#35770;&#22240;&#26524;&#25512;&#26029;&#30340;&#32570;&#22833;&#26041;&#38754;&#32467;&#26463;&#65292;&#20026;&#35835;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#26679;&#24615;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#12289;&#24037;&#31243;&#24072;&#21644;&#23398;&#29983;&#23545;&#22240;&#26524;&#25512;&#26029;&#24863;&#20852;&#36259;&#20294;&#23578;&#26410;&#29087;&#24713;&#30340;&#19968;&#20010;&#36215;&#28857;&#12290;&#25105;&#20204;&#39318;&#20808;&#21015;&#20030;&#20102;&#19968;&#32452;&#37325;&#35201;&#30340;&#29992;&#20110;&#22240;&#26524;&#35782;&#21035;&#30340;&#20551;&#35774;&#65292;&#22914;&#21487;&#20132;&#25442;&#24615;&#12289;&#31215;&#26497;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#24178;&#25200;&#30340;&#32570;&#22833;&#12290;&#22522;&#20110;&#36825;&#20123;&#20551;&#35774;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#22871;&#37325;&#35201;&#30340;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#20004;&#31867;&#65306;&#20027;&#21160;&#21644;&#34987;&#21160;&#26041;&#27861;&#12290;&#25105;&#20204;&#25551;&#36848;&#21644;&#35752;&#35770;&#20102;&#20027;&#21160;&#26041;&#27861;&#20013;&#30340;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#25551;&#36848;&#20102;&#34987;&#21160;&#26041;&#27861;&#20013;&#30340;&#32463;&#20856;&#26041;&#27861;&#65292;&#22914;&#21305;&#37197;&#21644;&#36870;&#27010;&#29575;&#21152;&#26435;&#65292;&#20197;&#21450;&#26368;&#36817;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#20171;&#32461;&#26412;&#25991;&#20013;&#19968;&#20123;&#22240;&#26524;&#25512;&#26029;&#30340;&#32570;&#22833;&#26041;&#38754;&#65292;&#22914;&#30896;&#25758;&#20559;&#24046;&#65292;&#25105;&#20204;&#26399;&#26395;&#26412;&#25991;&#20026;&#35835;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#26679;&#24615;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper serves as a starting point for machine learning researchers, engineers and students who are interested in but not yet familiar with causal inference. We start by laying out an important set of assumptions that are collectively needed for causal identification, such as exchangeability, positivity, consistency and the absence of interference. From these assumptions, we build out a set of important causal inference techniques, which we do so by categorizing them into two buckets; active and passive approaches. We describe and discuss randomized controlled trials and bandit-based approaches from the active category. We then describe classical approaches, such as matching and inverse probability weighting, in the passive category, followed by more recent deep learning based algorithms. By finishing the paper with some of the missing aspects of causal inference from this paper, such as collider biases, we expect this paper to provide readers with a diverse set of starting points f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#20113;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VF-Net&#65289;&#29992;&#20110;&#29273;&#31185;&#28857;&#20113;&#25968;&#25454;&#30340;&#22788;&#29702;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#32593;&#26684;&#29983;&#25104;&#12289;&#24418;&#29366;&#23436;&#25972;&#21644;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.10895</link><description>&lt;p&gt;
&#29273;&#31185;&#28857;&#20113;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Variational Autoencoding of Dental Point Clouds. (arXiv:2307.10895v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#20113;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VF-Net&#65289;&#29992;&#20110;&#29273;&#31185;&#28857;&#20113;&#25968;&#25454;&#30340;&#22788;&#29702;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#32593;&#26684;&#29983;&#25104;&#12289;&#24418;&#29366;&#23436;&#25972;&#21644;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#29273;&#31185;&#23398;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20173;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FDI 16&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#20102;&#22823;&#37327;&#29273;&#40831;&#32593;&#26684;&#21644;&#28857;&#20113;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65306;&#21464;&#20998;FoldingNet&#65288;VF-Net&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#28857;&#20113;&#35774;&#35745;&#30340;&#23436;&#20840;&#27010;&#29575;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20808;&#21069;&#30340;&#28857;&#20113;&#28508;&#21464;&#37327;&#27169;&#22411;&#32570;&#20047;&#36755;&#20837;&#21644;&#36755;&#20986;&#28857;&#20043;&#38388;&#30340;&#19968;&#19968;&#23545;&#24212;&#20851;&#31995;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#20248;&#21270;Chamfer&#36317;&#31163;&#65292;&#36825;&#26159;&#19968;&#31181;&#32570;&#20047;&#24402;&#19968;&#21270;&#20998;&#24067;&#23545;&#24212;&#30340;&#24230;&#37327;&#65292;&#22240;&#27492;&#19981;&#36866;&#21512;&#27010;&#29575;&#24314;&#27169;&#12290;&#25105;&#20204;&#29992;&#21512;&#36866;&#30340;&#32534;&#30721;&#22120;&#21462;&#20195;&#20102;&#26126;&#30830;&#30340;&#26368;&#23567;&#21270;Chamfer&#36317;&#31163;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#31616;&#21270;&#20102;&#27010;&#29575;&#25193;&#23637;&#12290;&#36825;&#20351;&#24471;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#65292;&#21253;&#25324;&#32593;&#26684;&#29983;&#25104;&#12289;&#24418;&#29366;&#23436;&#25972;&#21644;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29273;&#40831;&#37325;&#24314;&#20013;&#36739;&#20302;&#30340;&#37325;&#24314;&#35823;&#24046;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital dentistry has made significant advancements, yet numerous challenges remain. This paper introduces the FDI 16 dataset, an extensive collection of tooth meshes and point clouds. Additionally, we present a novel approach: Variational FoldingNet (VF-Net), a fully probabilistic variational autoencoder designed for point clouds. Notably, prior latent variable models for point clouds lack a one-to-one correspondence between input and output points. Instead, they rely on optimizing Chamfer distances, a metric that lacks a normalized distributional counterpart, rendering it unsuitable for probabilistic modeling. We replace the explicit minimization of Chamfer distances with a suitable encoder, increasing computational efficiency while simplifying the probabilistic extension. This allows for straightforward application in various tasks, including mesh generation, shape completion, and representation learning. Empirically, we provide evidence of lower reconstruction error in dental recon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#40654;&#26364;&#24230;&#37327;&#26469;&#25913;&#36827;SPD&#31070;&#32463;&#32593;&#32476;&#30340;&#27425;&#20248;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#24230;&#37327;&#33021;&#20351;&#32593;&#32476;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.15477</link><description>&lt;p&gt;
&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#19978;&#30340;&#33258;&#36866;&#24212;&#40654;&#26364;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Adaptive Riemannian Metrics on SPD Manifolds. (arXiv:2303.15477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#40654;&#26364;&#24230;&#37327;&#26469;&#25913;&#36827;SPD&#31070;&#32463;&#32593;&#32476;&#30340;&#27425;&#20248;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#24230;&#37327;&#33021;&#20351;&#32593;&#32476;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20869;&#22312;&#33021;&#22815;&#32534;&#30721;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#32467;&#26500;&#30456;&#20851;&#24615;&#65292;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#30697;&#38453;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#21453;&#26144;SPD&#27969;&#24418;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25104;&#21151;&#30340;&#40654;&#26364;&#24230;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22266;&#23450;&#24230;&#37327;&#24352;&#37327;&#21487;&#33021;&#20250;&#23548;&#33268;SPD&#30697;&#38453;&#23398;&#20064;&#30340;&#27425;&#20248;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;SPD&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#21033;&#29992;&#25289;&#22238;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;SPD&#27969;&#24418;&#30340;&#40654;&#26364;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#30340;&#24230;&#37327;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#12290;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37197;&#22791;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#30340;SPD&#32593;&#32476;&#21487;&#20197;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetric Positive Definite (SPD) matrices have received wide attention in machine learning due to their intrinsic capacity of encoding underlying structural correlation in data. To reflect the non-Euclidean geometry of SPD manifolds, many successful Riemannian metrics have been proposed. However, existing fixed metric tensors might lead to sub-optimal performance for SPD matrices learning, especially for SPD neural networks. To remedy this limitation, we leverage the idea of pullback and propose adaptive Riemannian metrics for SPD manifolds. Moreover, we present comprehensive theories for our metrics. Experiments on three datasets demonstrate that equipped with the proposed metrics, SPD networks can exhibit superior performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35838;&#31243;&#20171;&#32461;&#20102;&#27969;&#34892;&#30340;&#25968;&#25454;&#31185;&#23398;&#35821;&#35328;R&#65292;&#24182;&#26088;&#22312;&#22521;&#20859;&#23398;&#29983;&#12289;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#32773;&#25104;&#20026;&#29420;&#31435;&#30340;R&#35821;&#35328;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2301.01188</link><description>&lt;p&gt;
&#28145;&#24230;R&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep R Programming. (arXiv:2301.01188v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01188
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35838;&#31243;&#20171;&#32461;&#20102;&#27969;&#34892;&#30340;&#25968;&#25454;&#31185;&#23398;&#35821;&#35328;R&#65292;&#24182;&#26088;&#22312;&#22521;&#20859;&#23398;&#29983;&#12289;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#32773;&#25104;&#20026;&#29420;&#31435;&#30340;R&#35821;&#35328;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;R&#32534;&#31243;&#26159;&#19968;&#38376;&#20840;&#38754;&#30340;&#35838;&#31243;&#65292;&#37325;&#28857;&#20171;&#32461;&#25968;&#25454;&#31185;&#23398;&#20013;&#26368;&#27969;&#34892;&#30340;&#35821;&#35328;&#20043;&#19968;&#8212;&#8212;R&#35821;&#35328;&#65288;&#32479;&#35745;&#35745;&#31639;&#12289;&#25968;&#25454;&#21487;&#35270;&#21270;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#25968;&#25454;&#28165;&#27927;&#21644;&#20998;&#26512;&#65289;&#12290;&#23427;&#28145;&#20837;&#20171;&#32461;&#20102;R&#35821;&#35328;&#30340;&#22522;&#30784;&#30693;&#35782;&#65292;&#26088;&#22312;&#22521;&#20859;&#26377;&#25265;&#36127;&#30340;&#23398;&#29983;&#12289;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#32773;&#65292;&#35753;&#20182;&#20204;&#25104;&#20026;&#29420;&#31435;&#20351;&#29992;&#36825;&#20010;&#24378;&#22823;&#29615;&#22659;&#30340;&#29992;&#25143;&#12290;&#36825;&#20010;&#25945;&#26448;&#26159;&#19968;&#20010;&#38750;&#30408;&#21033;&#39033;&#30446;&#65292;&#23427;&#30340;&#22312;&#32447;&#21644;PDF&#29256;&#26412;&#21487;&#20197;&#22312; &lt;https://deepr.gagolewski.com/&gt; &#20813;&#36153;&#33719;&#21462;&#12290;&#24076;&#26395;&#36825;&#20010;&#26089;&#26399;&#33609;&#26696;&#21457;&#25918;&#20986;&#26469;&#21518;&#33021;&#23545;&#35835;&#32773;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep R Programming is a comprehensive course on one of the most popular languages in data science (statistical computing, graphics, machine learning, data wrangling and analytics). It introduces the base language in-depth and is aimed at ambitious students, practitioners, and researchers who would like to become independent users of this powerful environment. This textbook is a non-profit project. Its online and PDF versions are freely available at &lt;https://deepr.gagolewski.com/&gt;. This early draft is distributed in the hope that it will be useful.
&lt;/p&gt;</description></item></channel></rss>