<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;HTTP&#21709;&#24212;&#22836;&#35774;&#35745;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#22312;&#36328;&#27983;&#35272;&#22120;&#29615;&#22659;&#19979;&#26377;&#25928;&#26816;&#27979;Web&#36861;&#36394;&#22120;&#65292;&#32467;&#26524;&#22312;Chrome&#21644;Firefox&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01240</link><description>&lt;p&gt;
&#36229;&#36234;&#35831;&#27714;&#65306;&#21033;&#29992;HTTP&#21709;&#24212;&#22836;&#22312;&#19981;&#24179;&#34913;&#29615;&#22659;&#20013;&#36827;&#34892;&#36328;&#27983;&#35272;&#22120;Web&#36861;&#36394;&#22120;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Beyond the Request: Harnessing HTTP Response Headers for Cross-Browser Web Tracker Classification in an Imbalanced Setting
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;HTTP&#21709;&#24212;&#22836;&#35774;&#35745;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#22312;&#36328;&#27983;&#35272;&#22120;&#29615;&#22659;&#19979;&#26377;&#25928;&#26816;&#27979;Web&#36861;&#36394;&#22120;&#65292;&#32467;&#26524;&#22312;Chrome&#21644;Firefox&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19975;&#32500;&#32593;&#30340;&#36830;&#36890;&#24615;&#20027;&#35201;&#24402;&#22240;&#20110;HTTP&#21327;&#35758;&#65292;&#20854;&#20013;&#30340;HTTP&#28040;&#24687;&#25552;&#20379;&#20102;&#26377;&#20851;&#32593;&#32476;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#20449;&#24687;&#22836;&#23383;&#27573;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;Web&#36861;&#36394;&#12290;&#23613;&#31649;&#24050;&#26377;&#30740;&#31350;&#21033;&#29992;HTTP/S&#35831;&#27714;&#28040;&#24687;&#26469;&#35782;&#21035;Web&#36861;&#36394;&#22120;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;HTTP/S&#21709;&#24212;&#22836;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35774;&#35745;&#20351;&#29992;HTTP/S&#21709;&#24212;&#22836;&#36827;&#34892;Web&#36861;&#36394;&#22120;&#26816;&#27979;&#30340;&#26377;&#25928;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#27983;&#35272;&#22120;&#25193;&#23637;&#31243;&#24207;T.EX&#33719;&#21462;&#30340;Chrome&#12289;Firefox&#21644;Brave&#27983;&#35272;&#22120;&#30340;&#25968;&#25454;&#20316;&#20026;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;Chrome&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;11&#20010;&#30417;&#30563;&#27169;&#22411;&#65292;&#24182;&#22312;&#25152;&#26377;&#27983;&#35272;&#22120;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;Chrome&#21644;Firefox&#19978;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12289;F1&#20998;&#25968;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#26368;&#23567;&#23545;&#25968;&#25439;&#22833;&#35823;&#24046;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;Brave&#27983;&#35272;&#22120;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#20854;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#29305;&#24449;&#38598;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;Web&#36861;&#36394;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The World Wide Web's connectivity is greatly attributed to the HTTP protocol, with HTTP messages offering informative header fields that appeal to disciplines like web security and privacy, especially concerning web tracking. Despite existing research employing HTTP/S request messages to identify web trackers, HTTP/S response headers are often overlooked. This study endeavors to design effective machine learning classifiers for web tracker detection using HTTP/S response headers. Data from the Chrome, Firefox, and Brave browsers, obtained through the traffic monitoring browser extension T.EX, serves as our data set. Eleven supervised models were trained on Chrome data and tested across all browsers. The results demonstrated high accuracy, F1-score, precision, recall, and minimal log-loss error for Chrome and Firefox, but subpar performance on Brave, potentially due to its distinct data distribution and feature set. The research suggests that these classifiers are viable for detecting w
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#36965;&#24863;&#25968;&#25454;&#22320;&#36136;&#21046;&#22270;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;</title><link>https://arxiv.org/abs/2404.02180</link><description>&lt;p&gt;
&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#22320;&#36136;&#21046;&#22270;&#30340;&#36965;&#24863;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Remote sensing framework for geological mapping via stacked autoencoders and clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02180
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#36965;&#24863;&#25968;&#25454;&#22320;&#36136;&#21046;&#22270;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#36965;&#24863;&#22320;&#36136;&#21046;&#22270;&#20013;&#38754;&#20020;&#30528;&#30001;&#20110;&#20934;&#30830;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#32780;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25581;&#31034;&#36965;&#24863;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#32467;&#26500;&#12290;&#38477;&#32500;&#26041;&#27861;&#20855;&#26377;&#22312;&#25552;&#39640;&#22320;&#36136;&#22270;&#20934;&#30830;&#24615;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#38477;&#32500;&#26041;&#27861;&#21487;&#33021;&#22312;&#38750;&#32447;&#24615;&#25968;&#25454;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#20294;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#27169;&#25311;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#20855;&#26377;&#22810;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#23618;&#65292;&#29992;&#20110;&#25429;&#33719;&#23545;&#36965;&#24863;&#25968;&#25454;&#26377;&#29992;&#30340;&#20998;&#23618;&#25968;&#25454;&#34920;&#31034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#22788;&#29702;&#36965;&#24863;&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02180v1 Announce Type: cross  Abstract: Supervised learning methods for geological mapping via remote sensing face limitations due to the scarcity of accurately labelled training data. In contrast, unsupervised learning methods, such as dimensionality reduction and clustering have the ability to uncover patterns and structures in remote sensing data without relying on predefined labels. Dimensionality reduction methods have the potential to play a crucial role in improving the accuracy of geological maps. Although conventional dimensionality reduction methods may struggle with nonlinear data, unsupervised deep learning models such as autoencoders have the ability to model nonlinear relationship in data. Stacked autoencoders feature multiple interconnected layers to capture hierarchical data representations that can be useful for remote sensing data. In this study, we present an unsupervised machine learning framework for processing remote sensing data by utilizing stacked au
&lt;/p&gt;</description></item><item><title>fastprop&#26159;&#19968;&#31181;DeepQSPR&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23376;&#32423;&#25551;&#36848;&#31526;&#65292;&#22312;&#26497;&#22823;&#32553;&#30701;&#26102;&#38388;&#20869;&#65292;&#22312;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#24182;&#36229;&#36234;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02058</link><description>&lt;p&gt;
&#20855;&#26377;&#24555;&#36895;prop&#30340;&#21487;&#25512;&#24191;&#12289;&#24555;&#36895;&#21644;&#20934;&#30830;&#30340;DeepQSPR Part 1: &#26694;&#26550;&#21644;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Generalizable, Fast, and Accurate DeepQSPR with fastprop Part 1: Framework and Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02058
&lt;/p&gt;
&lt;p&gt;
fastprop&#26159;&#19968;&#31181;DeepQSPR&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23376;&#32423;&#25551;&#36848;&#31526;&#65292;&#22312;&#26497;&#22823;&#32553;&#30701;&#26102;&#38388;&#20869;&#65292;&#22312;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#24182;&#36229;&#36234;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#32467;&#26500;-&#24615;&#36136;&#20851;&#31995;&#30740;&#31350;&#26088;&#22312;&#23450;&#20041;&#20998;&#23376;&#32467;&#26500;&#19982;&#20219;&#24847;&#24863;&#20852;&#36259;&#30340;&#25968;&#37327;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#21382;&#21490;&#19978;&#65292;&#36825;&#26159;&#36890;&#36807;&#24320;&#21457;&#25551;&#36848;&#31526;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#38656;&#35201;&#26174;&#33879;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#38590;&#20197;&#27867;&#21270;&#12290;&#22240;&#27492;&#65292;&#35813;&#39046;&#22495;&#24050;&#32463;&#28436;&#21464;&#20026;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#65292;&#24182;&#36716;&#20026;&#20351;&#29992;&#39640;&#24230;&#21487;&#25512;&#24191;&#30340;&#23398;&#20064;&#34920;&#31034;&#12290;&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;fastprop&#65292;&#19968;&#31181;DeepQSPR&#26694;&#26550;&#65292;&#20351;&#29992;&#19968;&#32452;&#26126;&#26234;&#30340;&#20998;&#23376;&#32423;&#25551;&#36848;&#31526;&#65292;&#22312;&#26497;&#22823;&#32553;&#30701;&#30340;&#26102;&#38388;&#20869;&#28385;&#36275;&#24182;&#36229;&#36234;&#20102;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;fastprop&#21487;&#20197;&#22312;github&#19978;&#20813;&#36153;&#33719;&#21462;&#65292;&#32593;&#22336;&#20026;github.com/JacksonBurns/fastprop&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02058v1 Announce Type: new  Abstract: Quantitative Structure Property Relationship studies aim to define a mapping between molecular structure and arbitrary quantities of interest. This was historically accomplished via the development of descriptors which requires significant domain expertise and struggles to generalize. Thus the field has morphed into Molecular Property Prediction and been given over to learned representations which are highly generalizable. The paper introduces fastprop, a DeepQSPR framework which uses a cogent set of molecular level descriptors to meet and exceed the performance of learned representations on diverse datasets in dramatically less time. fastprop is freely available on github at github.com/JacksonBurns/fastprop.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#31867;&#21644;&#35282;&#33394;&#20998;&#26512;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2404.00282</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#26597;:&#27010;&#24565;&#12289;&#20998;&#31867;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00282
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#31867;&#21644;&#35282;&#33394;&#20998;&#26512;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25317;&#26377;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#39640;&#32423;&#36890;&#29992;&#33021;&#21147;&#65292;&#23427;&#20204;&#22312;&#22686;&#24378;&#23398;&#20064;&#26041;&#38754;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#20219;&#21153;&#35268;&#21010;&#31561;&#26041;&#38754;&#23637;&#29616;&#20986;&#28508;&#21147;&#12290;&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#29616;&#26377;$\textit{LLM&#22686;&#24378;RL}$&#25991;&#29486;&#65292;&#24635;&#32467;&#20102;&#20854;&#19982;&#20256;&#32479;RL&#26041;&#27861;&#30340;&#29305;&#24449;&#65292;&#26088;&#22312;&#28548;&#28165;&#30740;&#31350;&#33539;&#22260;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#21033;&#29992;&#32463;&#20856;&#30340;Agent-&#29615;&#22659;&#20132;&#20114;&#33539;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20998;&#31867;&#27861;&#65292;&#31995;&#32479;&#22320;&#23558;LLMs&#22312;RL&#20013;&#30340;&#21151;&#33021;&#20998;&#31867;&#65292;&#21253;&#25324;&#22235;&#31181;&#35282;&#33394;&#65306;&#20449;&#24687;&#22788;&#29702;&#22120;&#12289;&#22870;&#21169;&#35774;&#35745;&#32773;&#12289;&#20915;&#31574;&#32773;&#21644;&#29983;&#25104;&#22120;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#27599;&#20010;&#35282;&#33394;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#26041;&#27861;&#35770;&#65292;&#20998;&#26512;&#20102;&#32531;&#35299;&#30340;&#29305;&#23450;RL&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#28508;&#22312;&#24212;&#29992;&#12289;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00282v1 Announce Type: cross  Abstract: With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and task planning. In this survey, we provide a comprehensive review of the existing literature in $\textit{LLM-enhanced RL}$ and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. Additionally, for each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, potential applications, prospecti
&lt;/p&gt;</description></item><item><title>SugarcaneNet2024&#26159;&#36890;&#36807;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24555;&#36895;&#20934;&#30830;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.18870</link><description>&lt;p&gt;
SugarcaneNet2024: LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
SugarcaneNet2024: An Optimized Weighted Average Ensemble Approach of LASSO Regularized Pre-trained Models for Sugarcane Disease Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18870
&lt;/p&gt;
&lt;p&gt;
SugarcaneNet2024&#26159;&#36890;&#36807;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24555;&#36895;&#20934;&#30830;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29976;&#34071;&#20316;&#20026;&#19990;&#30028;&#31958;&#19994;&#30340;&#20851;&#38190;&#20316;&#29289;&#65292;&#23481;&#26131;&#21463;&#22810;&#31181;&#30149;&#23475;&#20405;&#23475;&#65292;&#36825;&#20123;&#30149;&#23475;&#23545;&#20854;&#20135;&#37327;&#21644;&#36136;&#37327;&#37117;&#26377;&#37325;&#22823;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#26377;&#25928;&#31649;&#29702;&#21644;&#23454;&#26045;&#39044;&#38450;&#25514;&#26045;&#65292;&#24517;&#39035;&#21450;&#26102;&#20934;&#30830;&#22320;&#26816;&#27979;&#30149;&#23475;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SugarcaneNet2024&#30340;&#29420;&#29305;&#27169;&#22411;&#65292;&#36890;&#36807;&#21494;&#29255;&#22270;&#20687;&#22788;&#29702;&#65292;&#33021;&#22815;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#33258;&#21160;&#24555;&#36895;&#26816;&#27979;&#29976;&#34071;&#30149;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#27719;&#24635;&#20102;&#19971;&#20010;&#23450;&#21046;&#30340;&#12289;&#32463;&#36807;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;&#65292;&#29305;&#21035;&#26159;InceptionV3&#12289;InceptionResNetV2&#12289;DenseNet201&#12289;DenseNet169&#12289;Xception&#21644;ResNet152V2&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#22312;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#24213;&#37096;&#28155;&#21152;&#20102;&#19977;&#23618;&#26356;&#23494;&#38598;&#23618;&#65292;&#20855;&#26377;0.0001&#30340;LASSO&#27491;&#21017;&#21270;&#65292;&#19977;&#20010;30%&#30340;dropout&#23618;&#21644;&#19977;&#20010;&#21551;&#29992;renorm&#30340;&#25209;&#37327;&#24402;&#19968;&#21270;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18870v1 Announce Type: cross  Abstract: Sugarcane, a key crop for the world's sugar industry, is prone to several diseases that have a substantial negative influence on both its yield and quality. To effectively manage and implement preventative initiatives, diseases must be detected promptly and accurately. In this study, we present a unique model called sugarcaneNet2024 that outperforms previous methods for automatically and quickly detecting sugarcane disease through leaf image processing. Our proposed model consolidates an optimized weighted average ensemble of seven customized and LASSO-regularized pre-trained models, particularly InceptionV3, InceptionResNetV2, DenseNet201, DenseNet169, Xception, and ResNet152V2. Initially, we added three more dense layers with 0.0001 LASSO regularization, three 30% dropout layers, and three batch normalizations with renorm enabled at the bottom of these pre-trained models to improve the performance. The accuracy of sugarcane leaf dise
&lt;/p&gt;</description></item><item><title>GeNet&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#12289;&#21033;&#29992;&#32534;&#30721;&#22120;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#24182;&#21033;&#29992;&#35299;&#30721;&#22120;&#37325;&#24314;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25239;&#22122;&#22768;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#12290;</title><link>https://arxiv.org/abs/2403.18296</link><description>&lt;p&gt;
GeNet:&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25239;&#22122;&#22768;&#20219;&#21153;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic Communication Paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18296
&lt;/p&gt;
&lt;p&gt;
GeNet&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#12289;&#21033;&#29992;&#32534;&#30721;&#22120;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#24182;&#21033;&#29992;&#35299;&#30721;&#22120;&#37325;&#24314;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25239;&#22122;&#22768;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35821;&#20041;&#36890;&#20449;&#20219;&#21153;&#26041;&#27861;&#20381;&#36182;&#20110;&#20102;&#35299;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#26469;&#20943;&#36731;&#36890;&#36947;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22312;&#29305;&#23450;&#30340;SNR&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GeNet&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#65292;&#26088;&#22312;&#25269;&#25239;&#22122;&#22768;&#65292;&#20174;&#32780;&#20419;&#36827;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#65288;TOC&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#36755;&#20837;&#25968;&#25454;&#22270;&#20687;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#12290;&#28982;&#21518;&#21033;&#29992;&#22522;&#20110;GNN&#30340;&#32534;&#30721;&#22120;&#20174;&#28304;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#20123;&#25552;&#21462;&#30340;&#35821;&#20041;&#20449;&#24687;&#28982;&#21518;&#36890;&#36807;&#36890;&#36947;&#20256;&#36755;&#12290;&#22312;&#25509;&#25910;&#31471;&#65292;&#20351;&#29992;&#22522;&#20110;GNN&#30340;&#35299;&#30721;&#22120;&#20174;&#28304;&#25968;&#25454;&#20013;&#37325;&#24314;&#30456;&#20851;&#30340;&#35821;&#20041;&#20449;&#24687;&#20197;&#29992;&#20110;TOC&#12290;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GeNet&#22312;&#25239;&#22122;&#22768;TOC&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18296v1 Announce Type: cross  Abstract: Traditional approaches to semantic communication tasks rely on the knowledge of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these methods necessitate training under specific SNR conditions, entailing considerable time and computational resources. In this paper, we propose GeNet, a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at combating noise, thereby facilitating Task-Oriented Communication (TOC). We propose a novel approach where we first transform the input data image into graph structures. Then we leverage a GNN-based encoder to extract semantic information from the source data. This extracted semantic information is then transmitted through the channel. At the receiver's end, a GNN-based decoder is utilized to reconstruct the relevant semantic information from the source data for TOC. Through experimental evaluation, we show GeNet's effectiveness in anti-noise TOC while decoup
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.15744</link><description>&lt;p&gt;
&#35770;&#20027;&#21160;&#23398;&#20064;&#32773;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Fragility of Active Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#36845;&#20195;&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#23454;&#20363;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#26631;&#27880;&#39044;&#31639;&#12290;&#28982;&#32780;&#65292;&#19982;&#38543;&#26426;&#25277;&#26679;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#65288;&#20363;&#22914;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#20998;&#31867;&#22120;&#65289;&#65292;&#23427;&#20204;&#30340;&#30410;&#22788;&#24182;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22240;&#32032;&#30340;&#32452;&#21512;&#22914;&#20309;&#21487;&#33021;&#25513;&#30422;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#30340;&#20219;&#20309;&#25910;&#30410;&#12290;&#19987;&#27880;&#20110;&#25991;&#26412;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;&#36827;&#34892;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;AL&#25216;&#26415;&#65292;&#36825;&#20123;&#23454;&#39564;&#22312;&#25968;&#25454;&#38598;&#12289;&#25209;&#22823;&#23567;&#12289;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#26041;&#38754;&#21464;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;AL&#21482;&#22312;&#19968;&#32452;&#26377;&#38480;&#30340;&#24773;&#22659;&#20013;&#26377;&#25928;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#20351;&#29992;&#19982;&#29616;&#23454;&#19990;&#30028;&#26399;&#26395;&#26356;&#22909;&#23545;&#40784;&#30340;&#24230;&#37327;&#30340;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#24433;&#21709;&#22312;&#20110;&#23545;&#20174;&#19994;&#32773;&#30340;&#27934;&#23519;&#65306;(a) &#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#30340;&#36873;&#25321;&#19982;AL&#25216;&#26415;&#30340;&#36873;&#25321;&#19968;&#26679;&#37325;&#35201;&#65292;(b) &#36873;&#25321;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15744v1 Announce Type: cross  Abstract: Active learning (AL) techniques aim to maximally utilize a labeling budget by iteratively selecting instances that are most likely to improve prediction accuracy. However, their benefit compared to random sampling has not been consistent across various setups, e.g., different datasets, classifiers. In this empirical study, we examine how a combination of different factors might obscure any gains from an AL technique.   Focusing on text classification, we rigorously evaluate AL techniques over around 1000 experiments that vary wrt the dataset, batch size, text representation and the classifier. We show that AL is only effective in a narrow set of circumstances. We also address the problem of using metrics that are better aligned with real world expectations.   The impact of this study is in its insights for a practitioner: (a) the choice of text representation and classifier is as important as that of an AL technique, (b) choice of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#21069;&#21521;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#26799;&#24230;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;&#27425;&#32447;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12511</link><description>&lt;p&gt;
&#22522;&#20110;&#21069;&#21521;&#26799;&#24230;&#30340;Frank-Wolfe&#20248;&#21270;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Forward Gradient-Based Frank-Wolfe Optimization for Memory Efficient Deep Neural Network Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#21069;&#21521;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#26799;&#24230;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;&#27425;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#22312;&#27599;&#20010;&#32423;&#21035;&#35745;&#31639;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#25110;&#21453;&#21521;&#27169;&#24335;&#24494;&#20998;&#35745;&#31639;&#26799;&#24230;&#38656;&#35201;&#28040;&#32791;&#22823;&#37327;&#20869;&#23384;&#65292;&#20351;&#21453;&#21521;&#20256;&#25773;&#25104;&#20026;&#35745;&#31639;&#26799;&#24230;&#30340;&#19968;&#31181;&#20302;&#25928;&#26041;&#27861;&#12290;&#26412;&#25991;&#37325;&#28857;&#20998;&#26512;&#20102;&#33879;&#21517;&#30340;Frank-Wolfe&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21363;&#26377;&#26465;&#20214;&#30340;&#26799;&#24230;&#31639;&#27861;&#65292;&#36890;&#36807;&#35775;&#38382;&#21069;&#21521;&#33258;&#21160;&#24494;&#20998;&#20197;&#35745;&#31639;&#26799;&#24230;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#36890;&#36807;&#35775;&#38382;&#22312;&#21069;&#21521;&#33258;&#21160;&#24494;&#20998;&#20013;&#33719;&#24471;&#30340;&#30495;&#26799;&#24230;&#30340;&#26377;&#22122;&#22768;&#20272;&#35745;&#65292; &#21363;&#31216;&#20026;Projected Forward Gradient&#65292;&#25910;&#25947;&#20110;&#26368;&#20248;&#35299;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;&#27425;&#32447;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26631;&#20934;&#30340;Frank-Wolfe&#31639;&#27861;&#65292;&#22312;&#25552;&#20379;Projected Fors
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12511v1 Announce Type: new  Abstract: Training a deep neural network using gradient-based methods necessitates the calculation of gradients at each level. However, using backpropagation or reverse mode differentiation, to calculate the gradients necessities significant memory consumption, rendering backpropagation an inefficient method for computing gradients. This paper focuses on analyzing the performance of the well-known Frank-Wolfe algorithm, a.k.a. conditional gradient algorithm by having access to the forward mode of automatic differentiation to compute gradients. We provide in-depth technical details that show the proposed Algorithm does converge to the optimal solution with a sub-linear rate of convergence by having access to the noisy estimate of the true gradient obtained in the forward mode of automated differentiation, referred to as the Projected Forward Gradient. In contrast, the standard Frank-Wolfe algorithm, when provided with access to the Projected Forwar
&lt;/p&gt;</description></item><item><title>DreamSampler&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;&#21453;&#21521;&#25193;&#25955;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#65292;&#25552;&#20379;&#20102;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20998;&#25968;&#33976;&#39311;&#26131;&#23849;&#28291;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#20687;&#32534;&#36753;&#21644;&#37325;&#26500;&#20013;&#23637;&#29616;&#20102;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11415</link><description>&lt;p&gt;
DreamSampler&#65306;&#32479;&#19968;&#25193;&#25955;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#20197;&#29992;&#20110;&#22270;&#20687;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
DreamSampler: Unifying Diffusion Sampling and Score Distillation for Image Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11415
&lt;/p&gt;
&lt;p&gt;
DreamSampler&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;&#21453;&#21521;&#25193;&#25955;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#65292;&#25552;&#20379;&#20102;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20998;&#25968;&#33976;&#39311;&#26131;&#23849;&#28291;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#20687;&#32534;&#36753;&#21644;&#37325;&#26500;&#20013;&#23637;&#29616;&#20102;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#24050;&#25104;&#20026;&#26368;&#36817;&#20960;&#24180;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDMs&#65289;&#36827;&#34892;&#22270;&#20687;&#22788;&#29702;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#21453;&#21521;&#25193;&#25955;&#37319;&#26679;&#36890;&#24120;&#38656;&#35201;&#35843;&#25972;LDM&#26550;&#26500;&#25110;&#29305;&#24449;&#24037;&#31243;&#65292;&#20998;&#25968;&#33976;&#39311;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#20294;&#24448;&#24448;&#23481;&#26131;&#21457;&#29983;&#27169;&#24335;&#23849;&#28291;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#24182;&#21033;&#29992;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;DreamSampler&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#28508;&#22312;&#20248;&#21270;&#30340;&#35270;&#35282;&#26080;&#32541;&#22320;&#25972;&#21512;&#20102;&#36825;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#31867;&#20284;&#20110;&#20998;&#25968;&#33976;&#39311;&#65292;DreamSampler&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;LDM&#26550;&#26500;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#20294;&#23427;&#20801;&#35768;&#22312;&#22270;&#20687;&#32534;&#36753;&#21644;&#37325;&#26500;&#20013;&#36827;&#34892;&#33976;&#39311;&#21644;&#21453;&#21521;&#37319;&#26679;&#65292;&#24182;&#25552;&#20379;&#39069;&#22806;&#30340;&#25351;&#23548;&#12290;&#36890;&#36807;&#28041;&#21450;&#22270;&#20687;&#32534;&#36753;&#12289;SVG&#37325;&#26500;&#31561;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31454;&#20105;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11415v1 Announce Type: cross  Abstract: Reverse sampling and score-distillation have emerged as main workhorses in recent years for image manipulation using latent diffusion models (LDMs). While reverse diffusion sampling often requires adjustments of LDM architecture or feature engineering, score distillation offers a simple yet powerful model-agnostic approach, but it is often prone to mode-collapsing. To address these limitations and leverage the strengths of both approaches, here we introduce a novel framework called {\em DreamSampler}, which seamlessly integrates these two distinct approaches through the lens of regularized latent optimization. Similar to score-distillation, DreamSampler is a model-agnostic approach applicable to any LDM architecture, but it allows both distillation and reverse sampling with additional guidance for image editing and reconstruction. Through experiments involving image editing, SVG reconstruction and etc, we demonstrate the competitive pe
&lt;/p&gt;</description></item><item><title>ClaimVer&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#29992;&#25143;&#23545;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#30340;&#20449;&#20219;&#24182;&#24378;&#35843;&#32454;&#31890;&#24230;&#35777;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09724</link><description>&lt;p&gt;
ClaimVer&#65306;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09724
&lt;/p&gt;
&lt;p&gt;
ClaimVer&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#29992;&#25143;&#23545;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#30340;&#20449;&#20219;&#24182;&#24378;&#35843;&#32454;&#31890;&#24230;&#35777;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#20256;&#25773;&#30340;&#20449;&#24687;&#35823;&#23548;&#21644;&#31038;&#20132;&#23186;&#20307;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#28608;&#22686;&#20013;&#65292;&#39564;&#35777;&#21644;&#20449;&#20219;&#25152;&#36935;&#21040;&#30340;&#20449;&#24687;&#21464;&#24471;&#26085;&#30410;&#22256;&#38590;&#12290;&#35768;&#22810;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#21644;&#24037;&#20855;&#24050;&#34987;&#24320;&#21457;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#36866;&#24403;&#30340;&#21487;&#35299;&#37322;&#24615;&#25110;&#32454;&#31890;&#24230;&#65292;&#26080;&#27861;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#12289;&#21487;&#35775;&#38382;&#19988;&#33021;&#22815;&#25191;&#34892;&#32454;&#31890;&#24230;&#35777;&#25454;&#24402;&#22240;&#30340;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24314;&#31435;&#29992;&#25143;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;&#20449;&#20219;&#38656;&#35201;&#21576;&#29616;&#27599;&#20010;&#39044;&#27979;&#32972;&#21518;&#30340;&#29702;&#30001;&#65292;&#22240;&#20026;&#30740;&#31350;&#34920;&#26126;&#36825;&#26174;&#33879;&#24433;&#21709;&#20154;&#20204;&#23545;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;&#23558;&#29992;&#25143;&#20851;&#27880;&#37325;&#28857;&#25918;&#22312;&#20855;&#20307;&#30340;&#38382;&#39064;&#20869;&#23481;&#19978;&#65292;&#32780;&#19981;&#26159;&#25552;&#20379;&#31616;&#21333;&#30340;&#31548;&#32479;&#26631;&#31614;&#20063;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{ClaimVer&#65292;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;}$&#65292;&#26088;&#22312;&#28385;&#36275;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09724v1 Announce Type: new  Abstract: In the midst of widespread misinformation and disinformation through social media and the proliferation of AI-generated texts, it has become increasingly difficult for people to validate and trust information they encounter. Many fact-checking approaches and tools have been developed, but they often lack appropriate explainability or granularity to be useful in various contexts. A text validation method that is easy to use, accessible, and can perform fine-grained evidence attribution has become crucial. More importantly, building user trust in such a method requires presenting the rationale behind each prediction, as research shows this significantly influences people's belief in automated systems. It is also paramount to localize and bring users' attention to the specific problematic content, instead of providing simple blanket labels. In this paper, we present $\textit{ClaimVer, a human-centric framework}$ tailored to meet users' info
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#29702;&#35770;&#33719;&#21462;&#20989;&#25968;&#65292;&#29992;&#20110;&#24179;&#34913;&#22312;&#36830;&#32493;&#30340;&#20248;&#21270;&#20219;&#21153;&#20013;&#33719;&#24471;&#26368;&#20248;&#20540;&#25110;&#35299;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.09570</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20445;&#30495;&#24230;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21450;&#36328;&#20219;&#21153;&#21487;&#36716;&#31227;&#30340;&#26368;&#22823;&#20540;&#29109;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#29702;&#35770;&#33719;&#21462;&#20989;&#25968;&#65292;&#29992;&#20110;&#24179;&#34913;&#22312;&#36830;&#32493;&#30340;&#20248;&#21270;&#20219;&#21153;&#20013;&#33719;&#24471;&#26368;&#20248;&#20540;&#25110;&#35299;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#35774;&#35745;&#32773;&#38754;&#20020;&#19968;&#31995;&#21015;&#20248;&#21270;&#20219;&#21153;&#65292;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#26114;&#36149;&#35780;&#20272;&#30340;&#40657;&#30418;&#20989;&#25968;&#24418;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#29702;&#35770;&#33719;&#21462;&#20989;&#25968;&#65292;&#29992;&#20110;&#24179;&#34913;&#38656;&#35201;&#33719;&#21462;&#19981;&#21516;&#20219;&#21153;&#30340;&#26368;&#20248;&#20540;&#25110;&#35299;&#30340;&#20449;&#24687;&#21644;&#36890;&#36807;&#21442;&#25968;&#30340;&#36716;&#31227;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09570v1 Announce Type: new  Abstract: In many applications, ranging from logistics to engineering, a designer is faced with a sequence of optimization tasks for which the objectives are in the form of black-box functions that are costly to evaluate. For example, the designer may need to tune the hyperparameters of neural network models for different learning tasks over time. Rather than evaluating the objective function for each candidate solution, the designer may have access to approximations of the objective functions, for which higher-fidelity evaluations entail a larger cost. Existing multi-fidelity black-box optimization strategies select candidate solutions and fidelity levels with the goal of maximizing the information accrued about the optimal value or solution for the current task. Assuming that successive optimization tasks are related, this paper introduces a novel information-theoretic acquisition function that balances the need to acquire information about the 
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#19968;&#31181;&#26080;&#21019;&#31958;&#23615;&#30149;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#24179;&#34913;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;</title><link>https://arxiv.org/abs/2403.07483</link><description>&lt;p&gt;
&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#31958;&#23615;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach to Diabetes Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07483
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#19968;&#31181;&#26080;&#21019;&#31958;&#23615;&#30149;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#24179;&#34913;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#26159;&#30001;&#33008;&#23707;&#32032;&#20135;&#29983;&#25110;&#21033;&#29992;&#19981;&#36275;&#23548;&#33268;&#30340;&#65292;&#23545;&#36523;&#20307;&#36896;&#25104;&#20102;&#24191;&#27867;&#30340;&#21361;&#23475;&#12290;&#29616;&#26377;&#30340;&#35786;&#26029;&#26041;&#27861;&#36890;&#24120;&#26159;&#20405;&#20837;&#24615;&#30340;&#65292;&#24182;&#20276;&#26377;&#35832;&#22810;&#32570;&#28857;&#65292;&#27604;&#22914;&#25104;&#26412;&#38480;&#21046;&#12290;&#23613;&#31649;&#23384;&#22312;&#20687;&#31867;&#38388;k&#26368;&#36817;&#37051;(CkNN)&#21644;&#36890;&#29992;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;(GRNN)&#36825;&#26679;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#21033;&#29992;&#20256;&#24863;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24102;&#26377;&#25209;&#37327;&#26631;&#20934;&#21270;&#30340;&#21453;&#21521;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;(BPNN)&#36827;&#34892;&#26080;&#21019;&#31958;&#23615;&#30149;&#35786;&#26029;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#25968;&#25454;&#37325;&#37319;&#26679;&#21644;&#24402;&#19968;&#21270;&#20197;&#23454;&#29616;&#31867;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#35832;&#22810;&#25361;&#25112;&#65292;&#27604;&#22914;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#20851;&#30340;&#24615;&#33021;&#21463;&#38480;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#25972;&#20307;&#20934;&#30830;&#24615;&#12289;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07483v1 Announce Type: cross  Abstract: Diabetes, resulting from inadequate insulin production or utilization, causes extensive harm to the body. Existing diagnostic methods are often invasive and come with drawbacks, such as cost constraints. Although there are machine learning models like Classwise k Nearest Neighbor (CkNN) and General Regression Neural Network (GRNN), they struggle with imbalanced data and result in under-performance. Leveraging advancements in sensor technology and machine learning, we propose a non-invasive diabetes diagnosis using a Back Propagation Neural Network (BPNN) with batch normalization, incorporating data re-sampling and normalization for class balancing. Our method addresses existing challenges such as limited performance associated with traditional machine learning. Experimental results on three datasets show significant improvements in overall accuracy, sensitivity, and specificity compared to traditional methods. Notably, we achieve accur
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#65292;&#22312;&#26080;&#26799;&#24230;&#31070;&#32463;&#25299;&#25169;&#20248;&#21270;&#20013;&#23454;&#29616;&#20102;&#36845;&#20195;&#27425;&#25968;&#30340;&#26174;&#33879;&#38477;&#20302;&#65292;&#36825;&#23558;&#24320;&#36767;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.04937</link><description>&lt;p&gt;
&#26080;&#26799;&#24230;&#31070;&#32463;&#25299;&#25169;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Gradient-free neural topology optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04937
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#65292;&#22312;&#26080;&#26799;&#24230;&#31070;&#32463;&#25299;&#25169;&#20248;&#21270;&#20013;&#23454;&#29616;&#20102;&#36845;&#20195;&#27425;&#25968;&#30340;&#26174;&#33879;&#38477;&#20302;&#65292;&#36825;&#23558;&#24320;&#36767;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#26799;&#24230;&#20248;&#21270;&#22120;&#21487;&#20197;&#35299;&#20915;&#38382;&#39064;&#65292;&#26080;&#35770;&#20854;&#30446;&#26631;&#20989;&#25968;&#30340;&#24179;&#28369;&#24615;&#25110;&#21487;&#24494;&#24615;&#22914;&#20309;&#65292;&#20294;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#30456;&#27604;&#65292;&#23427;&#20204;&#38656;&#35201;&#26356;&#22810;&#30340;&#36845;&#20195;&#25165;&#33021;&#25910;&#25947;&#12290;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#25299;&#25169;&#20248;&#21270;&#20013;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#27599;&#27425;&#36845;&#20195;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#24182;&#19988;&#38382;&#39064;&#30340;&#32500;&#24230;&#20063;&#24456;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#37325;&#26032;&#21442;&#25968;&#21270;&#31574;&#30053;&#65292;&#24403;&#22312;&#28508;&#22312;&#31354;&#38388;&#20248;&#21270;&#35774;&#35745;&#26102;&#65292;&#36845;&#20195;&#27425;&#25968;&#33267;&#23569;&#20943;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#20351;&#29992;&#28508;&#22312;&#37325;&#26032;&#21442;&#25968;&#21270;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#24191;&#27867;&#30340;&#35745;&#31639;&#23454;&#39564;&#65292;&#22312;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#20013;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#23613;&#31649;&#22522;&#20110;&#26799;&#24230;&#30340;&#25299;&#25169;&#20248;&#21270;&#23545;&#20110;&#21487;&#24494;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#32467;&#26500;&#30340;&#21512;&#35268;&#24615;&#20248;&#21270;&#65292;&#20173;&#28982;&#26356;&#26377;&#25928;&#65292;&#20294;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#37027;&#20123;&#38656;&#35201;&#26080;&#26799;&#24230;&#26041;&#27861;&#30340;&#38382;&#39064;&#24320;&#36767;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04937v1 Announce Type: new  Abstract: Gradient-free optimizers allow for tackling problems regardless of the smoothness or differentiability of their objective function, but they require many more iterations to converge when compared to gradient-based algorithms. This has made them unviable for topology optimization due to the high computational cost per iteration and high dimensionality of these problems. We propose a pre-trained neural reparameterization strategy that leads to at least one order of magnitude decrease in iteration count when optimizing the designs in latent space, as opposed to the conventional approach without latent reparameterization. We demonstrate this via extensive computational experiments in- and out-of-distribution with the training data. Although gradient-based topology optimization is still more efficient for differentiable problems, such as compliance optimization of structures, we believe this work will open up a new path for problems where gra
&lt;/p&gt;</description></item><item><title>&#20027;&#39064;&#24314;&#27169;&#20013;&#25552;&#20986;&#20102;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#35789;&#27719;&#36873;&#25321;&#26469;&#25913;&#21892;&#38544;&#31169;&#39118;&#38505;</title><link>https://arxiv.org/abs/2403.04451</link><description>&lt;p&gt;
&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#19982;&#20027;&#39064;&#24314;&#27169;&#20013;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attacks and Privacy in Topic Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04451
&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#20013;&#25552;&#20986;&#20102;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#35789;&#27719;&#36873;&#25321;&#26469;&#25913;&#21892;&#38544;&#31169;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25512;&#29702;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#30340;&#38544;&#31169;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#26356;&#31616;&#21333;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20363;&#22914;&#20027;&#39064;&#27169;&#22411;&#65292;&#26159;&#21542;&#23384;&#22312;&#31867;&#20284;&#30340;&#28431;&#27934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20027;&#39064;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#21487;&#20197;&#33258;&#20449;&#22320;&#35782;&#21035;Latent Dirichlet Allocation&#20013;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#21592;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22823;&#22411;&#31070;&#32463;&#27169;&#22411;&#30456;&#20851;&#32852;&#30340;&#38544;&#31169;&#39118;&#38505;&#24182;&#19981;&#20165;&#38480;&#20110;&#22823;&#22411;&#31070;&#32463;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#28431;&#27934;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20027;&#39064;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31169;&#23494;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#23558;DP&#35789;&#27719;&#36873;&#25321;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#24182;&#23637;&#31034;&#23427;&#19981;&#20165;&#25913;&#21892;&#20102;&#38544;&#31169;&#24615;&#65292;&#32780;&#19988;&#22312;&#23454;&#29992;&#24615;&#26041;&#38754;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04451v1 Announce Type: cross  Abstract: Recent research shows that large language models are susceptible to privacy attacks that infer aspects of the training data. However, it is unclear if simpler generative models, like topic models, share similar vulnerabilities. In this work, we propose an attack against topic models that can confidently identify members of the training data in Latent Dirichlet Allocation. Our results suggest that the privacy risks associated with generative modeling are not restricted to large neural models. Additionally, to mitigate these vulnerabilities, we explore differentially private (DP) topic modeling. We propose a framework for private topic modeling that incorporates DP vocabulary selection as a pre-processing step, and show that it improves privacy while having limited effects on practical utility.
&lt;/p&gt;</description></item><item><title>&#23616;&#37096;&#26354;&#29575;&#21464;&#21270;&#23548;&#33268;&#31995;&#32479;&#20174;&#33391;&#24615;&#19988;&#23500;&#26377;&#20449;&#24687;&#30340;&#23616;&#37096;&#26223;&#35266;&#36880;&#28176;&#38519;&#20837;&#26080;&#20449;&#24687;&#30340;&#36855;&#23467;&#65292;&#20851;&#38190;&#36716;&#21464;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;Hessian&#30340;&#38408;&#20540;&#26377;&#20851;&#12290;</title><link>https://arxiv.org/abs/2403.02418</link><description>&lt;p&gt;
&#20174;&#38646;&#21040;&#33521;&#38596;&#65306;&#26080;&#30693;&#21021;&#20540;&#22788;&#30340;&#23616;&#37096;&#26354;&#29575;&#22914;&#20309;&#36828;&#31163;&#31967;&#31957;&#30340;&#26497;&#23567;&#20540;
&lt;/p&gt;
&lt;p&gt;
From Zero to Hero: How local curvature at artless initial conditions leads away from bad minima
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02418
&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#26354;&#29575;&#21464;&#21270;&#23548;&#33268;&#31995;&#32479;&#20174;&#33391;&#24615;&#19988;&#23500;&#26377;&#20449;&#24687;&#30340;&#23616;&#37096;&#26223;&#35266;&#36880;&#28176;&#38519;&#20837;&#26080;&#20449;&#24687;&#30340;&#36855;&#23467;&#65292;&#20851;&#38190;&#36716;&#21464;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;Hessian&#30340;&#38408;&#20540;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26799;&#24230;&#19979;&#38477;&#22312;&#38750;&#20984;&#21644;&#39640;&#32500;&#35774;&#32622;&#20013;&#30340;&#20248;&#21270;&#21160;&#21147;&#23398;&#65292;&#37325;&#28857;&#20851;&#27880;&#30456;&#20301;&#24674;&#22797;&#38382;&#39064;&#20316;&#20026;&#22797;&#26434;&#25439;&#22833;&#26223;&#35266;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#36890;&#36807;&#20998;&#26512;&#23616;&#37096;&#26354;&#29575;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20013;&#38388;&#20449;&#22122;&#27604;&#19979;&#65292;Hessian&#22312;&#19979;&#38477;&#30340;&#31532;&#19968;&#20010;&#38454;&#27573;&#26174;&#31034;&#20986;&#25351;&#21521;&#22909;&#26497;&#23567;&#20540;&#30340;&#19979;&#38477;&#26041;&#21521;&#65292;&#28982;&#21518;&#22312;&#32467;&#26463;&#26102;&#34987;&#22256;&#22312;&#31967;&#31957;&#30340;&#26497;&#23567;&#20540;&#20013;&#12290;&#22240;&#27492;&#65292;&#23616;&#37096;&#26223;&#35266;&#36215;&#21021;&#26159;&#33391;&#24615;&#19988;&#23500;&#26377;&#20449;&#24687;&#30340;&#65292;&#28982;&#21518;&#26799;&#24230;&#19979;&#38477;&#23558;&#31995;&#32479;&#24102;&#20837;&#26080;&#20449;&#24687;&#30340;&#36855;&#23467;&#12290;&#20004;&#20010;&#38454;&#27573;&#20043;&#38388;&#30340;&#36716;&#21464;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;Hessian&#30340;BBP&#31867;&#22411;&#38408;&#20540;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02418v1 Announce Type: new  Abstract: We investigate the optimization dynamics of gradient descent in a non-convex and high-dimensional setting, with a focus on the phase retrieval problem as a case study for complex loss landscapes. We first study the high-dimensional limit where both the number $M$ and the dimension $N$ of the data are going to infinity at fixed signal-to-noise ratio $\alpha = M/N$. By analyzing how the local curvature changes during optimization, we uncover that for intermediate $\alpha$, the Hessian displays a downward direction pointing towards good minima in the first regime of the descent, before being trapped in bad minima at the end. Hence, the local landscape is benign and informative at first, before gradient descent brings the system into a uninformative maze. The transition between the two regimes is associated to a BBP-type threshold in the time-dependent Hessian. Through both theoretical analysis and numerical experiments, we show that in prac
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;LLMs&#22312;&#37319;&#29992;&#20018;&#32852;&#25512;&#29702;&#26102;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20018;&#32852;&#25512;&#29702;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#25104;&#21151;&#35268;&#36991;&#20102;&#36825;&#31181;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.14899</link><description>&lt;p&gt;
&#20572;&#27490;&#25512;&#29702;&#65281;&#24403;&#22810;&#27169;&#24577;LLMs&#19982;&#20018;&#32852;&#25512;&#29702;&#36935;&#21040;&#23545;&#25239;&#24615;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14899
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;LLMs&#22312;&#37319;&#29992;&#20018;&#32852;&#25512;&#29702;&#26102;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20018;&#32852;&#25512;&#29702;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#25104;&#21151;&#35268;&#36991;&#20102;&#36825;&#31181;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;LLMs&#65288;MLLMs&#65289;&#23637;&#31034;&#20102;&#24456;&#24378;&#30340;&#29702;&#35299;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20687;&#20256;&#32479;&#35270;&#35273;&#27169;&#22411;&#19968;&#26679;&#65292;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#22270;&#20687;&#30340;&#25915;&#20987;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20018;&#32852;&#25512;&#29702;&#65288;CoT&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#22312;MLLMs&#19978;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#36890;&#36807;&#25552;&#20379;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#20851;&#20110;MLLMs&#22312;CoT&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#65292;&#20197;&#21450;&#22312;MLLMs&#29992;&#23545;&#25239;&#24615;&#22270;&#20687;&#25512;&#26029;&#38169;&#35823;&#31572;&#26696;&#26102;&#25512;&#29702;&#30340;&#21512;&#29702;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#37319;&#29992;CoT&#25512;&#29702;&#26102;MLLMs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;CoT&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#25269;&#25239;&#20102;&#24050;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;CoT&#24341;&#36215;&#30340;&#40065;&#26834;&#24615;&#22686;&#24378;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CoT&#25512;&#29702;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14899v1 Announce Type: cross  Abstract: Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand images. However, like traditional vision models, they are still vulnerable to adversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely explored on MLLMs, which not only improves model's performance, but also enhances model's explainability by giving intermediate reasoning steps. Nevertheless, there is still a lack of study regarding MLLMs' adversarial robustness with CoT and an understanding of what the rationale looks like when MLLMs infer wrong answers with adversarial images. Our research evaluates the adversarial robustness of MLLMs when employing CoT reasoning, finding that CoT marginally improves adversarial robustness against existing attack methods. Moreover, we introduce a novel stop-reasoning attack technique that effectively bypasses the CoT-induced robustness enhancements. Finally, we demonstrate the alterations in CoT reasonin
&lt;/p&gt;</description></item><item><title>&#24605;&#32500;&#38142;&#36171;&#20104;&#21464;&#21387;&#22120;&#27169;&#22411;&#25191;&#34892;&#22266;&#26377;&#20018;&#34892;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#21464;&#21387;&#22120;&#22312;&#31639;&#26415;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12875</link><description>&lt;p&gt;
&#24605;&#32500;&#38142;&#28608;&#21457;&#21464;&#21387;&#22120;&#35299;&#20915;&#22266;&#26377;&#20018;&#34892;&#38382;&#39064;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Chain of Thought Empowers Transformers to Solve Inherently Serial Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12875
&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#36171;&#20104;&#21464;&#21387;&#22120;&#27169;&#22411;&#25191;&#34892;&#22266;&#26377;&#20018;&#34892;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#21464;&#21387;&#22120;&#22312;&#31639;&#26415;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#19968;&#31995;&#21015;&#20013;&#38388;&#27493;&#39588;&#65292;&#21363;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#26159;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31639;&#26415;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#20934;&#30830;&#24615;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;CoT&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#19981;&#28165;&#26970;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#34920;&#36798;&#24615;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#23545;&#35299;&#30721;&#22120;&#19987;&#29992;&#21464;&#21387;&#22120;&#30340;CoT&#33021;&#21147;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#27010;&#24565;&#19978;&#65292;CoT&#36171;&#20104;&#27169;&#22411;&#25191;&#34892;&#22266;&#26377;&#20018;&#34892;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#32780;&#36825;&#31181;&#33021;&#21147;&#22312;&#21464;&#21387;&#22120;&#20013;&#32570;&#20047;&#65292;&#29305;&#21035;&#26159;&#24403;&#28145;&#24230;&#36739;&#20302;&#26102;&#12290;&#20808;&#21069;&#30340;&#20316;&#21697;&#24050;&#32463;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;CoT&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#26377;&#38480;&#31934;&#24230;$\mathsf{poly}(n)$&#23884;&#20837;&#23610;&#23544;&#30340;&#24658;&#23450;&#28145;&#24230;&#21464;&#21387;&#22120;&#21482;&#33021;&#22312;$\mathsf{TC}^0$&#20013;&#35299;&#20915;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20855;&#26377;&#24120;&#25968;&#20301;&#31934;&#24230;&#30340;&#24658;&#23450;&#28145;&#24230;&#21464;&#21387;&#22120;&#30340;&#26356;&#32039;&#23494;&#30340;&#34920;&#36798;&#24615;&#19978;&#30028;&#65292;&#23427;&#21482;&#33021;&#35299;&#20915;$\mathsf{AC}^0$&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12875v1 Announce Type: new  Abstract: Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have shown that constant-depth transformers with finite precision $\mathsf{poly}(n)$ embedding size can only solve problems in $\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\mathsf{AC}^0$, a 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#26041;&#27861;&#65292;&#20854;&#20013;&#28216;&#25103;&#21442;&#19982;&#32773;&#36890;&#36807;Bayesian&#23398;&#20064;&#35843;&#25972;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#31574;&#30053;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26368;&#20339;&#25311;&#21512;&#65292;&#25552;&#39640;&#20102;&#25512;&#28436;&#22312;&#29468;&#24819;&#27169;&#22411;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12499</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#23454;&#29616;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Automated Security Response through Online Learning with Adaptive Conjectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12499
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#26041;&#27861;&#65292;&#20854;&#20013;&#28216;&#25103;&#21442;&#19982;&#32773;&#36890;&#36807;Bayesian&#23398;&#20064;&#35843;&#25972;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#31574;&#30053;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26368;&#20339;&#25311;&#21512;&#65292;&#25552;&#39640;&#20102;&#25512;&#28436;&#22312;&#29468;&#24819;&#27169;&#22411;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#65292;&#24182;&#23558;&#25915;&#20987;&#32773;&#21644;&#38450;&#24481;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#24418;&#24335;&#34920;&#36848;&#20026;&#19968;&#20010;&#37096;&#20998;&#35266;&#27979;&#12289;&#38750;&#24179;&#31283;&#21338;&#24328;&#12290;&#25105;&#20204;&#25918;&#23485;&#20102;&#28216;&#25103;&#27169;&#22411;&#27491;&#30830;&#35268;&#23450;&#30340;&#26631;&#20934;&#20551;&#35774;&#65292;&#24182;&#32771;&#34385;&#27599;&#20010;&#21442;&#19982;&#32773;&#23545;&#27169;&#22411;&#26377;&#19968;&#20010;&#27010;&#29575;&#24615;&#29468;&#24819;&#65292;&#21487;&#33021;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#38169;&#35823;&#35268;&#23450;&#65292;&#21363;&#30495;&#23454;&#27169;&#22411;&#30340;&#27010;&#29575;&#20026;0&#12290;&#36825;&#31181;&#24418;&#24335;&#20801;&#35768;&#25105;&#20204;&#25429;&#25417;&#20851;&#20110;&#22522;&#30784;&#35774;&#26045;&#21644;&#21442;&#19982;&#32773;&#24847;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#22312;&#32447;&#23398;&#20064;&#26377;&#25928;&#30340;&#28216;&#25103;&#31574;&#30053;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#21442;&#19982;&#32773;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#36845;&#20195;&#22320;&#35843;&#25972;&#20854;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#20854;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29468;&#24819;&#20250;&#25910;&#25947;&#21040;&#26368;&#20339;&#25311;&#21512;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#20855;&#26377;&#29468;&#27979;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25512;&#28436;&#23454;&#29616;&#24615;&#33021;&#25913;&#36827;&#30340;&#19978;&#38480;&#12290;&#20026;&#20102;&#21051;&#30011;&#28216;&#25103;&#30340;&#31283;&#23450;&#29366;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Berk-Nash&#24179;&#34913;&#30340;&#19968;&#20010;&#21464;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12499v1 Announce Type: cross  Abstract: We study automated security response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed, non-stationary game. We relax the standard assumption that the game model is correctly specified and consider that each player has a probabilistic conjecture about the model, which may be misspecified in the sense that the true model has probability 0. This formulation allows us to capture uncertainty about the infrastructure and the intents of the players. To learn effective game strategies online, we design a novel method where a player iteratively adapts its conjecture using Bayesian learning and updates its strategy through rollout. We prove that the conjectures converge to best fits, and we provide a bound on the performance improvement that rollout enables with a conjectured model. To characterize the steady state of the game, we propose a variant of the Berk-Nash equilibrium. We 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAMP&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#23545;&#22810;&#20010;$l_p$&#25200;&#21160;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;$l_p$&#25915;&#20987;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;&#24182;&#35774;&#35745;&#36923;&#36753;&#37197;&#23545;&#25439;&#22833;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35757;&#32451;&#19982;&#23545;&#25239;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#25972;&#21512;&#26377;&#29992;&#20449;&#24687;&#20197;&#35843;&#21644;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.06827</link><description>&lt;p&gt;
RAMP&#65306;&#22686;&#24378;&#23545;&#22810;&#20010;$l_p$&#25200;&#21160;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
RAMP: Boosting Adversarial Robustness Against Multiple $l_p$ Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06827
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAMP&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#23545;&#22810;&#20010;$l_p$&#25200;&#21160;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;$l_p$&#25915;&#20987;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;&#24182;&#35774;&#35745;&#36923;&#36753;&#37197;&#23545;&#25439;&#22833;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35757;&#32451;&#19982;&#23545;&#25239;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#25972;&#21512;&#26377;&#29992;&#20449;&#24687;&#20197;&#35843;&#21644;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#39640;&#23545;&#21333;&#20010;$l_p$&#33539;&#25968;&#21463;&#38480;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#65292;&#24050;&#32463;&#26377;&#30456;&#24403;&#22810;&#30340;&#24037;&#20316;&#22312;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#36827;&#34892;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;AT&#27169;&#22411;&#30340;&#22810;&#33539;&#25968;&#40065;&#26834;&#24615;&#65288;&#20849;&#21516;&#20934;&#30830;&#24615;&#65289;&#20173;&#28982;&#36739;&#20302;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21516;&#26102;&#33719;&#24471;&#33391;&#22909;&#30340;&#20849;&#21516;&#20934;&#30830;&#24615;&#21644;&#28165;&#27905;&#20934;&#30830;&#24615;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#22312;&#22810;&#20010;$l_p$&#25200;&#21160;&#20043;&#38388;&#23384;&#22312;&#40065;&#26834;&#24615;&#12289;&#20934;&#30830;&#24615;/&#40065;&#26834;&#24615;/&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#20174;&#20998;&#24067;&#36716;&#21464;&#30340;&#35282;&#24230;&#20998;&#26512;&#36825;&#20123;&#26435;&#34913;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;$l_p$&#25915;&#20987;&#20043;&#38388;&#30340;&#20851;&#38190;&#26435;&#34913;&#23545;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#36923;&#36753;&#37197;&#23545;&#25439;&#22833;&#26469;&#25552;&#39640;&#20849;&#21516;&#20934;&#30830;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#26799;&#24230;&#25237;&#24433;&#23558;&#33258;&#28982;&#35757;&#32451;&#19982;AT&#30456;&#36830;&#25509;&#65292;&#20197;&#20174;&#33258;&#28982;&#35757;&#32451;&#20013;&#25214;&#21040;&#24182;&#25972;&#21512;&#26377;&#29992;&#30340;&#20449;&#24687;&#21040;AT&#20013;&#65292;&#20174;&#32780;&#35843;&#21644;&#20934;&#30830;&#24615;/&#40065;&#26834;&#24615;&#30340;&#26435;&#34913;&#12290;&#32467;&#21512;&#25105;&#20204;&#30340;&#36129;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;\textbf{RAMP}&#30340;&#26694;&#26550;&#65292;&#26469;&#25552;&#39640;&#23545;&#22810;&#20010;$l_p$&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;\textbf{RAMP}&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36866;&#24212;...
&lt;/p&gt;
&lt;p&gt;
There is considerable work on improving robustness against adversarial attacks bounded by a single $l_p$ norm using adversarial training (AT). However, the multiple-norm robustness (union accuracy) of AT models is still low. We observe that simultaneously obtaining good union and clean accuracy is hard since there are tradeoffs between robustness against multiple $l_p$ perturbations, and accuracy/robustness/efficiency. By analyzing the tradeoffs from the lens of distribution shifts, we identify the key tradeoff pair among $l_p$ attacks to boost efficiency and design a logit pairing loss to improve the union accuracy. Next, we connect natural training with AT via gradient projection, to find and incorporate useful information from natural training into AT, which moderates the accuracy/robustness tradeoff. Combining our contributions, we propose a framework called \textbf{RAMP}, to boost the robustness against multiple $l_p$ perturbations. We show \textbf{RAMP} can be easily adapted for 
&lt;/p&gt;</description></item><item><title>ForestColl&#26159;&#19968;&#31181;&#38024;&#23545;&#20219;&#24847;&#32593;&#32476;&#25299;&#25169;&#29983;&#25104;&#39640;&#25928;&#35843;&#24230;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#26500;&#24314;&#24191;&#25773;/&#32858;&#21512;&#29983;&#25104;&#36328;&#36234;&#26641;&#30340;&#36890;&#20449;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#29702;&#35770;&#19978;&#30340;&#26368;&#23567;&#32593;&#32476;&#25317;&#22622;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#39640;&#20110;&#20379;&#24212;&#21830;&#33258;&#24102;&#36890;&#20449;&#24211;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06787</link><description>&lt;p&gt;
ForestColl: &#24322;&#26500;&#32593;&#32476;&#32467;&#26500;&#19978;&#39640;&#25928;&#30340;&#38598;&#21512;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
ForestColl: Efficient Collective Communications on Heterogeneous Network Fabrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06787
&lt;/p&gt;
&lt;p&gt;
ForestColl&#26159;&#19968;&#31181;&#38024;&#23545;&#20219;&#24847;&#32593;&#32476;&#25299;&#25169;&#29983;&#25104;&#39640;&#25928;&#35843;&#24230;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#26500;&#24314;&#24191;&#25773;/&#32858;&#21512;&#29983;&#25104;&#36328;&#36234;&#26641;&#30340;&#36890;&#20449;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#29702;&#35770;&#19978;&#30340;&#26368;&#23567;&#32593;&#32476;&#25317;&#22622;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#39640;&#20110;&#20379;&#24212;&#21830;&#33258;&#24102;&#36890;&#20449;&#24211;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36234;&#26469;&#36234;&#22823;&#65292;&#21152;&#36895;&#22120;&#20043;&#38388;&#30340;&#38598;&#21512;&#36890;&#20449;&#65288;&#22914;allreduce&#31561;&#65289;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;&#22312;&#24403;&#20170;&#39640;&#24230;&#22810;&#26679;&#21270;&#21644;&#24322;&#26500;&#30340;&#32593;&#32476;&#32467;&#26500;&#19979;&#35774;&#35745;&#39640;&#25928;&#30340;&#36890;&#20449;&#35843;&#24230;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ForestColl&#30340;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#20026;&#20219;&#24847;&#32593;&#32476;&#25299;&#25169;&#29983;&#25104;&#39640;&#25928;&#30340;&#35843;&#24230;&#12290;ForestColl&#20351;&#29992;&#24191;&#25773;/&#32858;&#21512;&#29983;&#25104;&#36328;&#36234;&#26641;&#20316;&#20026;&#36890;&#20449;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#29702;&#35770;&#19978;&#30340;&#26368;&#23567;&#32593;&#32476;&#25317;&#22622;&#12290;&#20854;&#35843;&#24230;&#29983;&#25104;&#36816;&#34892;&#22312;&#24378;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#65292;&#19988;&#20855;&#26377;&#39640;&#25193;&#23637;&#24615;&#12290;ForestColl&#25903;&#25345;&#21253;&#25324;&#20132;&#25442;&#32593;&#32476;&#21644;&#30452;&#25509;&#36830;&#25509;&#22312;&#20869;&#30340;&#20219;&#20309;&#32593;&#32476;&#32467;&#26500;&#65292;&#20197;&#21450;&#20219;&#20309;&#32593;&#32476;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#22810;&#38598;&#32676;&#30340;AMD MI250&#21644;NVIDIA A100&#24179;&#21488;&#19978;&#35780;&#20272;&#20102;ForestColl&#12290;&#19982;&#20379;&#24212;&#21830;&#33258;&#24049;&#20248;&#21270;&#30340;&#36890;&#20449;&#24211;RCCL&#21644;NCCL&#30456;&#27604;&#65292;ForestColl&#30340;&#35843;&#24230;&#24615;&#33021;&#25552;&#39640;&#20102;&#39640;&#36798;52&#65285;&#12290;ForestColl&#36824;&#20248;&#20110;&#20854;&#20182;...
&lt;/p&gt;
&lt;p&gt;
As modern DNN models grow ever larger, collective communications between the accelerators (allreduce, etc.) emerge as a significant performance bottleneck. Designing efficient communication schedules is challenging given today's highly diverse and heterogeneous network fabrics. In this paper, we present ForestColl, a tool that generates efficient schedules for any network topology. ForestColl constructs broadcast/aggregation spanning trees as the communication schedule, achieving theoretically minimum network congestion. Its schedule generation runs in strongly polynomial time and is highly scalable. ForestColl supports any network fabrics, including both switching fabrics and direct connections, as well as any network graph structure. We evaluated ForestColl on multi-cluster AMD MI250 and NVIDIA A100 platforms. ForestColl's schedules achieved up to 52\% higher performance compared to the vendors' own optimized communication libraries, RCCL and NCCL. ForestColl also outperforms other s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.06165</link><description>&lt;p&gt;
&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#34920;&#31034;&#26469;&#36827;&#34892;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Contrastive Feature Representations for Facial Action Unit Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06165
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#65288;AU&#65289;&#26816;&#27979;&#30340;&#20027;&#35201;&#26041;&#27861;&#28041;&#21450;&#30417;&#30563;&#30340;&#22810;&#26631;&#31614;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24120;&#24120;&#23545;AU&#30340;&#20687;&#32032;&#32423;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#25552;&#20986;&#20102;&#24456;&#22823;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#22122;&#22768;AU&#26631;&#31614;&#65292;&#36825;&#31181;&#20570;&#27861;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#22686;&#24378;&#12290;&#30446;&#26631;&#26159;&#22312;AU&#26816;&#27979;&#39046;&#22495;&#20013;&#25670;&#33073;&#20256;&#32479;&#30340;&#20687;&#32032;&#32423;&#23398;&#20064;&#33539;&#24335;&#65292;&#33719;&#24471;&#21028;&#21035;&#29305;&#24449;&#12290;&#20026;&#20102;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#30417;&#30563;&#20449;&#21495;&#12290;&#36825;&#31181;&#22686;&#24378;&#26159;&#36890;&#36807;&#27491;&#26679;&#26412;&#25277;&#26679;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#27491;&#26679;&#26412;&#23545;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#27599;&#20010;AU&#31867;&#22411;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we empl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#23558;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#36864;&#21270;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#20013;&#12290;&#36890;&#36807;&#32771;&#34385;&#20855;&#26377;&#38646;&#24130;&#25351;&#25968;&#20026;2&#30340;&#38646;&#24130;&#22522;&#21521;&#37327;&#65292;&#21487;&#20197;&#27867;&#21270;&#22522;&#20110;&#20108;&#27425;&#25968;&#30340;&#26041;&#27861;&#24182;&#25429;&#25417;&#23454;&#20307;&#23884;&#20837;&#20013;&#32570;&#20047;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#27169;&#24335;&#12290;&#30740;&#31350;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#27169;&#22411;&#26469;&#21457;&#29616;&#20195;&#25968;&#30340;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#38646;&#24130;&#21521;&#37327;&#26377;&#21161;&#20110;&#25429;&#25417;&#23454;&#20307;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.04870</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#36864;&#21270;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#20013;
&lt;/p&gt;
&lt;p&gt;
Embedding Knowledge Graphs in Degenerate Clifford Algebras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04870
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#23558;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#36864;&#21270;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#20013;&#12290;&#36890;&#36807;&#32771;&#34385;&#20855;&#26377;&#38646;&#24130;&#25351;&#25968;&#20026;2&#30340;&#38646;&#24130;&#22522;&#21521;&#37327;&#65292;&#21487;&#20197;&#27867;&#21270;&#22522;&#20110;&#20108;&#27425;&#25968;&#30340;&#26041;&#27861;&#24182;&#25429;&#25417;&#23454;&#20307;&#23884;&#20837;&#20013;&#32570;&#20047;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#27169;&#24335;&#12290;&#30740;&#31350;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#27169;&#22411;&#26469;&#21457;&#29616;&#20195;&#25968;&#30340;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#38646;&#24130;&#21521;&#37327;&#26377;&#21161;&#20110;&#25429;&#25417;&#23454;&#20307;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#26159;&#23454;&#25968;&#12289;&#22797;&#25968;&#21644;&#22235;&#20803;&#25968;&#30340;&#33258;&#28982;&#25512;&#24191;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#32972;&#26223;&#19979;&#65292;&#21482;&#26377;&#24418;&#24335;&#20026;$Cl_{p,q}$&#65288;&#21363;&#27809;&#26377;&#38646;&#24130;&#22522;&#21521;&#37327;&#30340;&#20195;&#25968;&#65289;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#21463;&#21040;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#38646;&#24130;&#22522;&#21521;&#37327;&#65292;&#20854;&#24130;&#25351;&#25968;&#20026;2&#12290;&#22312;&#36825;&#20123;&#31354;&#38388;&#20013;&#65292;&#34987;&#31216;&#20026;$Cl_{p,q,r}$&#65292;&#21487;&#20197;&#27867;&#21270;&#22522;&#20110;&#20108;&#27425;&#25968;&#30340;&#26041;&#27861;&#65288;&#26080;&#27861;&#20351;&#29992;$Cl_{p,q}$&#36827;&#34892;&#24314;&#27169;&#65289;&#24182;&#25429;&#25417;&#28304;&#20110;&#23454;&#25968;&#21644;&#22797;&#25968;&#37096;&#20998;&#38388;&#32570;&#20047;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#23454;&#20307;&#23884;&#20837;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#27169;&#22411;&#26469;&#21457;&#29616;&#21442;&#25968;$p$&#65292;$q$&#21644;$r$&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#20351;&#29992;&#36138;&#23146;&#25628;&#32034;&#20248;&#21270;$p$&#65292;$q$&#21644;$r$&#12290;&#31532;&#20108;&#20010;&#27169;&#22411;&#22522;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#30340;&#36755;&#20837;&#30693;&#35782;&#22270;&#35889;&#30340;&#23884;&#20837;&#26469;&#39044;&#27979;$(p, q, r)$&#12290;&#25105;&#20204;&#22312;&#19971;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#38646;&#24130;&#21521;&#37327;&#26377;&#21161;&#20110;&#25429;&#25417;&#23454;&#20307;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clifford algebras are a natural generalization of the real numbers, the complex numbers, and the quaternions. So far, solely Clifford algebras of the form $Cl_{p,q}$ (i.e., algebras without nilpotent base vectors) have been studied in the context of knowledge graph embeddings. We propose to consider nilpotent base vectors with a nilpotency index of two. In these spaces, denoted $Cl_{p,q,r}$, allows generalizing over approaches based on dual numbers (which cannot be modelled using $Cl_{p,q}$) and capturing patterns that emanate from the absence of higher-order interactions between real and complex parts of entity embeddings. We design two new models for the discovery of the parameters $p$, $q$, and $r$. The first model uses a greedy search to optimize $p$, $q$, and $r$. The second predicts $(p, q,r)$ based on an embedding of the input knowledge graph computed using neural networks. The results of our evaluation on seven benchmark datasets suggest that nilpotent vectors can help capture 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#39044;&#27979;&#31639;&#27861;&#33539;&#24335;&#19979;&#35774;&#35745;&#25903;&#25345;&#39030;&#28857;&#22833;&#36133;&#30340;&#36830;&#36890;&#24615;&#39044;&#27979;&#22120;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#32467;&#26500;&#65292;&#33021;&#22815;&#20197;&#39044;&#22788;&#29702;&#26102;&#38388;&#21644;&#26597;&#35810;&#26102;&#38388;&#30340;&#22810;&#39033;&#24335;&#20851;&#31995;&#26469;&#22788;&#29702;&#22833;&#36133;&#39030;&#28857;&#38598;&#21512;&#12290;</title><link>https://arxiv.org/abs/2312.08489</link><description>&lt;p&gt;
&#39044;&#27979;&#39030;&#28857;&#22833;&#36133;&#30340;&#36830;&#36890;&#24615;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Connectivity Oracles for Predictable Vertex Failures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08489
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#39044;&#27979;&#31639;&#27861;&#33539;&#24335;&#19979;&#35774;&#35745;&#25903;&#25345;&#39030;&#28857;&#22833;&#36133;&#30340;&#36830;&#36890;&#24615;&#39044;&#27979;&#22120;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#32467;&#26500;&#65292;&#33021;&#22815;&#20197;&#39044;&#22788;&#29702;&#26102;&#38388;&#21644;&#26597;&#35810;&#26102;&#38388;&#30340;&#22810;&#39033;&#24335;&#20851;&#31995;&#26469;&#22788;&#29702;&#22833;&#36133;&#39030;&#28857;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#25903;&#25345;&#39030;&#28857;&#22833;&#36133;&#30340;&#36830;&#36890;&#24615;&#39044;&#27979;&#22120;&#26159;&#38024;&#23545;&#26080;&#21521;&#22270;&#30340;&#22522;&#26412;&#25968;&#25454;&#32467;&#26500;&#38382;&#39064;&#20043;&#19968;&#12290;&#24050;&#26377;&#30340;&#30740;&#31350;&#22312;&#26597;&#35810;&#26102;&#38388;&#26041;&#38754;&#24050;&#32463;&#26377;&#20102;&#24456;&#22909;&#30340;&#29702;&#35299;&#65306;&#20197;&#21069;&#30340;&#20316;&#21697;[Duan-Pettie STOC'10; Long-Saranurak FOCS'22]&#23454;&#29616;&#20102;&#19982;&#22833;&#36133;&#39030;&#28857;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#30340;&#26597;&#35810;&#26102;&#38388;&#65292;&#24182;&#19988;&#22312;&#38656;&#35201;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#39044;&#22788;&#29702;&#21644;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#26356;&#26032;&#30340;&#26465;&#20214;&#19979;&#26159;&#26377;&#26465;&#20214;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#22312;&#39044;&#27979;&#31639;&#27861;&#30340;&#33539;&#24335;&#19979;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#38382;&#65292;&#22914;&#26524;&#21487;&#20197;&#39044;&#27979;&#21040;&#22833;&#36133;&#39030;&#28857;&#38598;&#21512;&#65292;&#26597;&#35810;&#26102;&#38388;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25968;&#25454;&#32467;&#26500;&#65292;&#32473;&#23450;&#19968;&#20010;&#22270;G=(V,E)&#21644;&#19968;&#20010;&#39044;&#27979;&#20250;&#22833;&#36133;&#30340;&#39030;&#28857;&#38598;&#21512;\widehat{D} \subseteq V&#65288;&#20854;&#20013;d=|\widehat{D}|&#65289;&#65292;&#23558;&#20854;&#39044;&#22788;&#29702;&#26102;&#38388;&#20026;$\tilde{O}(d|E|)$&#65292;&#28982;&#21518;&#21487;&#20197;&#25509;&#25910;&#19968;&#20010;&#26356;&#26032;&#65292;&#35813;&#26356;&#26032;&#20197;&#23545;&#31216;&#24046;&#20998;&#24418;&#24335;&#32473;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08489v2 Announce Type: replace-cross  Abstract: The problem of designing connectivity oracles supporting vertex failures is one of the basic data structures problems for undirected graphs. It is already well understood: previous works [Duan--Pettie STOC'10; Long--Saranurak FOCS'22] achieve query time linear in the number of failed vertices, and it is conditionally optimal as long as we require preprocessing time polynomial in the size of the graph and update time polynomial in the number of failed vertices.   We revisit this problem in the paradigm of algorithms with predictions: we ask if the query time can be improved if the set of failed vertices can be predicted beforehand up to a small number of errors. More specifically, we design a data structure that, given a graph $G=(V,E)$ and a set of vertices predicted to fail $\widehat{D} \subseteq V$ of size $d=|\widehat{D}|$, preprocesses it in time $\tilde{O}(d|E|)$ and then can receive an update given as the symmetric differ
&lt;/p&gt;</description></item><item><title>Symphony&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$E(3)$-&#31561;&#21464;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#28857;&#23545;&#31216;&#29699;&#24418;&#35856;&#27874;&#20449;&#21495;&#26469;&#39640;&#25928;&#24314;&#27169;&#20998;&#23376;&#30340;3D&#20960;&#20309;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2311.16199</link><description>&lt;p&gt;
Symphony: &#23545;&#20998;&#23376;&#29983;&#25104;&#30340;&#28857;&#23545;&#31216;&#29699;&#24418;&#35856;&#27874;&#30340;&#23545;&#31216;&#31561;&#21464;&#33258;&#22238;&#24402;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Symphony: Symmetry-Equivariant Point-Centered Spherical Harmonics for Molecule Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16199
&lt;/p&gt;
&lt;p&gt;
Symphony&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$E(3)$-&#31561;&#21464;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#28857;&#23545;&#31216;&#29699;&#24418;&#35856;&#27874;&#20449;&#21495;&#26469;&#39640;&#25928;&#24314;&#27169;&#20998;&#23376;&#30340;3D&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Symphony&#65292;&#36825;&#26159;&#19968;&#20010;$E(3)$-&#31561;&#21464;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;3D&#20998;&#23376;&#20960;&#20309;&#32467;&#26500;&#30340;&#26500;&#24314;&#65292;&#36890;&#36807;&#20174;&#20998;&#23376;&#30862;&#29255;&#20013;&#36845;&#20195;&#22320;&#26500;&#24314;&#20998;&#23376;&#12290;&#29616;&#26377;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#22914;G-SchNet&#21644;G-SphereNet&#29992;&#20110;&#20998;&#23376;&#30340;&#26059;&#36716;&#19981;&#21464;&#29305;&#24449;&#26469;&#23562;&#37325;&#20998;&#23376;&#30340;3D&#23545;&#31216;&#24615;&#12290;&#30456;&#21453;&#65292;Symphony&#20351;&#29992;&#24102;&#26377;&#26356;&#39640;&#27425;$E(3)$-&#31561;&#21464;&#29305;&#24449;&#30340;&#28040;&#24687;&#20256;&#36882;&#12290;&#36825;&#20351;&#24471;&#36890;&#36807;&#29699;&#35856;&#20449;&#21495;&#26377;&#25928;&#22320;&#24314;&#27169;&#20998;&#23376;&#30340;3D&#20960;&#20309;&#30340;&#27010;&#29575;&#20998;&#24067;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Symphony&#33021;&#22815;&#20934;&#30830;&#22320;&#20174;QM9&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#23567;&#20998;&#23376;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#25509;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16199v2 Announce Type: replace  Abstract: We present Symphony, an $E(3)$-equivariant autoregressive generative model for 3D molecular geometries that iteratively builds a molecule from molecular fragments. Existing autoregressive models such as G-SchNet and G-SphereNet for molecules utilize rotationally invariant features to respect the 3D symmetries of molecules. In contrast, Symphony uses message-passing with higher-degree $E(3)$-equivariant features. This allows a novel representation of probability distributions via spherical harmonic signals to efficiently model the 3D geometry of molecules. We show that Symphony is able to accurately generate small molecules from the QM9 dataset, outperforming existing autoregressive models and approaching the performance of diffusion models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26497;&#31471;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36719;&#26631;&#31614;&#21407;&#22411;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#22312;&#22823;&#22411;&#12289;&#39640;&#32500;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2210.17437</link><description>&lt;p&gt;
&#29992;&#36719;&#26631;&#31614;&#21407;&#22411;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Learning New Tasks from a Few Examples with Soft-Label Prototypes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.17437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26497;&#31471;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36719;&#26631;&#31614;&#21407;&#22411;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#22312;&#22823;&#22411;&#12289;&#39640;&#32500;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#23545;&#20854;&#24494;&#35843;&#65292;&#20197;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#36827;&#34892;&#27867;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#8220;&#26497;&#31471;&#8221;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#27169;&#22411;&#21482;&#38656;&#25509;&#35302;&#27599;&#20010;&#31867;&#21035;&#33267;&#23569;4&#20010;&#31034;&#20363;&#65292;&#36825;&#20123;&#31034;&#20363;&#22522;&#20110;&#36719;&#26631;&#31614;&#21407;&#22411;&#65292;&#36825;&#20123;&#36719;&#26631;&#31614;&#21407;&#22411;&#20849;&#21516;&#25429;&#33719;&#20102;&#36755;&#20837;&#22495;&#31354;&#38388;&#20013;&#19981;&#21516;&#31867;&#21035;&#30340;&#20998;&#24067;&#12290;&#21463;&#21040;&#20808;&#21069;&#20851;&#20110;&#19968;&#20803;&#25110;&#31616;&#21333;&#22810;&#20803;&#65288;&#21512;&#25104;&#65289;&#25968;&#25454;&#65288;Sucholutsky&#31561;&#20154;&#65292;2021&#65289;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#12289;&#39640;&#32500;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#31070;&#32463;&#26694;&#26550;&#65288;DeepSLP&#65289;&#20013;&#23398;&#20064;&#36719;&#26631;&#31614;&#21407;&#22411;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#65292;&#23427;&#22312;31/48&#20010;&#27979;&#35797;&#20219;&#21153;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#19982;&#24378;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20174;v&#20013;&#23398;&#20064;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;NLP&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.17437v3 Announce Type: replace-cross  Abstract: Existing approaches to few-shot learning in NLP rely on large language models and fine-tuning of these to generalise on out-of-distribution data. In this work, we propose a simple yet powerful approach to "extreme" few-shot learning, wherein models are exposed to as little as 4 examples per class, based on soft-label prototypes that collectively capture the distribution of different classes across the input domain space. Inspired by previous work (Sucholutsky et al., 2021) on univariate or simple multivariate (synthetic) data, we propose a novel approach that is effective on large, high-dimensional and real-world datasets. We learn soft-label prototypes within a neural framework (DeepSLP) and we experimentally demonstrate that it achieves superior performance on 31/48 tested tasks and few-shot settings while closely matching the performance of strong baselines on the rest. We focus on learning previously unseen NLP tasks from v
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#36793;&#32536;&#35745;&#31639;&#36801;&#31227;&#30340;&#25925;&#38556;&#33258;&#36866;&#24212;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550; FIRE&#65292;&#24341;&#20837;ImRE&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36793;&#32536;&#35745;&#31639;&#25968;&#23383;&#23402;&#29983;&#29615;&#22659;&#20013;&#35757;&#32451;RL&#31574;&#30053;&#26469;&#36866;&#24212;&#32597;&#35265;&#20107;&#20214;&#65292;&#35299;&#20915;&#20102;RL&#26694;&#26550;&#22312;&#22788;&#29702;&#20598;&#21457;&#26381;&#21153;&#22120;&#25925;&#38556;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2209.14399</link><description>&lt;p&gt;
FIRE&#65306;&#38754;&#21521;&#36793;&#32536;&#35745;&#31639;&#36801;&#31227;&#30340;&#25925;&#38556;&#33258;&#36866;&#24212;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FIRE: A Failure-Adaptive Reinforcement Learning Framework for Edge Computing Migrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.14399
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#36793;&#32536;&#35745;&#31639;&#36801;&#31227;&#30340;&#25925;&#38556;&#33258;&#36866;&#24212;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550; FIRE&#65292;&#24341;&#20837;ImRE&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36793;&#32536;&#35745;&#31639;&#25968;&#23383;&#23402;&#29983;&#29615;&#22659;&#20013;&#35757;&#32451;RL&#31574;&#30053;&#26469;&#36866;&#24212;&#32597;&#35265;&#20107;&#20214;&#65292;&#35299;&#20915;&#20102;RL&#26694;&#26550;&#22312;&#22788;&#29702;&#20598;&#21457;&#26381;&#21153;&#22120;&#25925;&#38556;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36793;&#32536;&#35745;&#31639;&#20013;&#65292;&#29992;&#25143;&#26381;&#21153;&#37197;&#32622;&#25991;&#20214;&#30001;&#20110;&#29992;&#25143;&#31227;&#21160;&#32780;&#36827;&#34892;&#36801;&#31227;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#26469;&#36827;&#34892;&#36801;&#31227;&#65292;&#36890;&#24120;&#26159;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RL&#26694;&#26550;&#24573;&#35270;&#20102;&#20598;&#21457;&#30340;&#26381;&#21153;&#22120;&#25925;&#38556;&#65292;&#23613;&#31649;&#32597;&#35265;&#65292;&#20294;&#20250;&#24433;&#21709;&#21040;&#20687;&#33258;&#21160;&#39550;&#39542;&#21644;&#23454;&#26102;&#38556;&#30861;&#26816;&#27979;&#31561;&#23545;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#65288;&#32597;&#35265;&#20107;&#20214;&#65289;&#25925;&#38556;&#34429;&#28982;&#22312;&#21382;&#21490;&#35757;&#32451;&#25968;&#25454;&#20013;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#20195;&#34920;&#65292;&#21364;&#23545;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;RL&#31639;&#27861;&#26500;&#25104;&#25361;&#25112;&#12290;&#30001;&#20110;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#35843;&#25972;&#25925;&#38556;&#39057;&#29575;&#36827;&#34892;&#35757;&#32451;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FIRE&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#22312;&#36793;&#32536;&#35745;&#31639;&#25968;&#23383;&#23402;&#29983;&#29615;&#22659;&#20013;&#35757;&#32451;RL&#31574;&#30053;&#26469;&#36866;&#24212;&#32597;&#35265;&#20107;&#20214;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ImRE&#65292;&#19968;&#31181;&#22522;&#20110;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;Q-learning&#31639;&#27861;&#65292;&#23427;&#26681;&#25454;&#32597;&#35265;&#20107;&#20214;&#23545;&#20540;&#20989;&#25968;&#30340;&#24433;&#21709;&#36827;&#34892;&#27604;&#20363;&#25277;&#26679;&#12290;FIRE&#32771;&#34385;&#20102;&#24310;&#36831;&#12289;&#36801;&#31227;&#12289;&#25925;&#38556;&#21644;&#22791;&#20221;pl
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.14399v2 Announce Type: replace-cross  Abstract: In edge computing, users' service profiles are migrated due to user mobility. Reinforcement learning (RL) frameworks have been proposed to do so, often trained on simulated data. However, existing RL frameworks overlook occasional server failures, which although rare, impact latency-sensitive applications like autonomous driving and real-time obstacle detection. Nevertheless, these failures (rare events), being not adequately represented in historical training data, pose a challenge for data-driven RL algorithms. As it is impractical to adjust failure frequency in real-world applications for training, we introduce FIRE, a framework that adapts to rare events by training a RL policy in an edge computing digital twin environment. We propose ImRE, an importance sampling-based Q-learning algorithm, which samples rare events proportionally to their impact on the value function. FIRE considers delay, migration, failure, and backup pl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#31454;&#20105;&#39044;&#27979;&#21644;&#20998;&#31867;&#33021;&#21147;&#30340;&#28207;&#21475;&#36816;&#33829;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#33337;&#33334;&#22312;&#28207;&#21475;&#30340;&#24635;&#26102;&#38388;&#21644;&#24310;&#36831;&#26102;&#38388;&#65292;&#22635;&#34917;&#20102;&#28207;&#21475;&#20998;&#26512;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#30340;&#31354;&#30333;&#65292;&#24182;&#20026;&#28023;&#20107;&#29289;&#27969;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2401.14498</link><description>&lt;p&gt;
&#20248;&#21270;&#28207;&#21475;&#36816;&#33829;&#30340;&#39044;&#27979;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Predictive Analysis for Optimizing Port Operations. (arXiv:2401.14498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#31454;&#20105;&#39044;&#27979;&#21644;&#20998;&#31867;&#33021;&#21147;&#30340;&#28207;&#21475;&#36816;&#33829;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#33337;&#33334;&#22312;&#28207;&#21475;&#30340;&#24635;&#26102;&#38388;&#21644;&#24310;&#36831;&#26102;&#38388;&#65292;&#22635;&#34917;&#20102;&#28207;&#21475;&#20998;&#26512;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#30340;&#31354;&#30333;&#65292;&#24182;&#20026;&#28023;&#20107;&#29289;&#27969;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#36816;&#26159;&#36828;&#36317;&#31163;&#21644;&#22823;&#23447;&#36135;&#29289;&#36816;&#36755;&#30340;&#37325;&#35201;&#29289;&#27969;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36816;&#36755;&#27169;&#24335;&#20013;&#22797;&#26434;&#30340;&#35268;&#21010;&#32463;&#24120;&#21463;&#21040;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#22825;&#27668;&#26465;&#20214;&#12289;&#36135;&#29289;&#22810;&#26679;&#24615;&#21644;&#28207;&#21475;&#21160;&#24577;&#65292;&#23548;&#33268;&#25104;&#26412;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#20272;&#35745;&#33337;&#33334;&#22312;&#28207;&#21475;&#20572;&#30041;&#30340;&#24635;&#26102;&#38388;&#21644;&#28508;&#22312;&#24310;&#36831;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#20415;&#22312;&#28207;&#21475;&#36816;&#33829;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#35268;&#21010;&#21644;&#23433;&#25490;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#20855;&#26377;&#31454;&#20105;&#39044;&#27979;&#21644;&#20998;&#31867;&#33021;&#21147;&#30340;&#28207;&#21475;&#36816;&#33829;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20272;&#35745;&#33337;&#33334;&#30340;&#24635;&#26102;&#38388;&#21644;&#24310;&#36831;&#26102;&#38388;&#12290;&#35813;&#30740;&#31350;&#22635;&#34917;&#20102;&#28207;&#21475;&#20998;&#26512;&#27169;&#22411;&#22312;&#33337;&#33334;&#20572;&#30041;&#21644;&#24310;&#36831;&#26102;&#38388;&#26041;&#38754;&#30340;&#37325;&#35201;&#31354;&#30333;&#65292;&#20026;&#28023;&#20107;&#29289;&#27969;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36129;&#29486;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26088;&#22312;&#21327;&#21161;&#28207;&#21475;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#24182;&#39044;&#27979;&#26381;&#21153;&#24310;&#36831;&#12290;&#36890;&#36807;&#23545;&#24052;&#35199;&#28207;&#21475;&#30340;&#26696;&#20363;&#30740;&#31350;&#36827;&#34892;&#39564;&#35777;&#65292;&#21516;&#26102;&#20351;&#29992;&#29305;&#24449;&#20998;&#26512;&#26469;&#29702;&#35299;...
&lt;/p&gt;
&lt;p&gt;
Maritime transport is a pivotal logistics mode for the long-distance and bulk transportation of goods. However, the intricate planning involved in this mode is often hindered by uncertainties, including weather conditions, cargo diversity, and port dynamics, leading to increased costs. Consequently, accurately estimating vessel total (stay) time at port and potential delays becomes imperative for effective planning and scheduling in port operations. This study aims to develop a port operation solution with competitive prediction and classification capabilities for estimating vessel Total and Delay times. This research addresses a significant gap in port analysis models for vessel Stay and Delay times, offering a valuable contribution to the field of maritime logistics. The proposed solution is designed to assist decision-making in port environments and predict service delays. This is demonstrated through a case study on Brazil ports. Additionally, feature analysis is used to understand
&lt;/p&gt;</description></item><item><title>&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#35201;&#27714;&#65292;&#38656;&#35201;&#23545;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-Product Networks&#27169;&#25311;&#23547;&#25214;&#39640;&#21487;&#33021;&#24615;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#28385;&#36275;&#22810;&#20010;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#20339;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.14086</link><description>&lt;p&gt;
&#20351;&#29992;Sum-Product Networks&#29983;&#25104;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generating Likely Counterfactuals Using Sum-Product Networks. (arXiv:2401.14086v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14086
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#35201;&#27714;&#65292;&#38656;&#35201;&#23545;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-Product Networks&#27169;&#25311;&#23547;&#25214;&#39640;&#21487;&#33021;&#24615;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#28385;&#36275;&#22810;&#20010;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#20339;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#65288;GDPR&#12289;AI&#27861;&#26696;&#65289;&#65292;&#38656;&#35201;&#35299;&#37322;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#12290;&#36825;&#20123;&#20915;&#31574;&#24448;&#24448;&#21482;&#33021;&#22312;&#20107;&#21518;&#35299;&#37322;&#65292;&#21453;&#20107;&#23454;&#25512;&#29702;&#25104;&#20026;&#24120;&#35265;&#30340;&#35299;&#37322;&#26041;&#24335;&#12290;&#20160;&#20040;&#26500;&#25104;&#20102;&#26368;&#20339;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#24517;&#39035;&#32771;&#34385;&#22810;&#20010;&#26041;&#38754;&#65292;&#20854;&#20013;&#8220;&#26679;&#26412;&#36317;&#31163;&#8221;&#26159;&#26368;&#24120;&#35265;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#19968;&#35201;&#27714;&#32463;&#24120;&#20250;&#23548;&#33268;&#19981;&#22826;&#21487;&#33021;&#19988;&#22240;&#27492;&#20215;&#20540;&#26377;&#38480;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#39640;&#21487;&#33021;&#24615;&#35299;&#37322;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#65288;MIO&#65289;&#27169;&#25311;&#23547;&#25214;&#28385;&#36275;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#35768;&#22810;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#26377;&#21487;&#33021;&#35299;&#37322;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sum-Product Network&#65288;SPN&#65289;&#30340;MIO&#34920;&#36798;&#65292;&#24182;&#20351;&#29992;SPN&#20272;&#35745;&#21453;&#20107;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#23545;&#29420;&#31435;&#30340;&#20852;&#36259;&#20063;&#26377;&#29992;&#12290;&#19982;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20960;&#31181;&#26041;&#27861;&#36827;&#34892;&#25968;&#20540;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to user demand and recent regulation (GDPR, AI Act), decisions made by AI systems need to be explained. These decisions are often explainable only post hoc, where counterfactual explanations are popular. The question of what constitutes the best counterfactual explanation must consider multiple aspects, where "distance from the sample" is the most common. We argue that this requirement frequently leads to explanations that are unlikely and, therefore, of limited value. Here, we present a system that provides high-likelihood explanations. We show that the search for the most likely explanations satisfying many common desiderata for counterfactual explanations can be modeled using mixed-integer optimization (MIO). In the process, we propose an MIO formulation of a Sum-Product Network (SPN) and use the SPN to estimate the likelihood of a counterfactual, which can be of independent interest. A numerical comparison against several methods for generating counterfactual explanations is pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#26089;&#26399;&#23545;&#40784;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#23567;&#21021;&#22987;&#21270;&#21644;&#19968;&#20010;&#38544;&#34255;&#30340;ReLU&#23618;&#32593;&#32476;&#20013;&#65292;&#31070;&#32463;&#20803;&#20250;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#21521;&#20851;&#38190;&#26041;&#21521;&#36827;&#34892;&#23545;&#40784;&#65292;&#23548;&#33268;&#32593;&#32476;&#31232;&#30095;&#34920;&#31034;&#20197;&#21450;&#26799;&#24230;&#27969;&#22312;&#25910;&#25947;&#26102;&#30340;&#38544;&#21547;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31232;&#30095;&#35825;&#23548;&#30340;&#23545;&#40784;&#20063;&#20351;&#24471;&#35757;&#32451;&#30446;&#26631;&#30340;&#26368;&#23567;&#21270;&#21464;&#24471;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.10791</link><description>&lt;p&gt;
&#20004;&#23618;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#26089;&#26399;&#23545;&#40784;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;
&lt;/p&gt;
&lt;p&gt;
Early alignment in two-layer networks training is a two-edged sword. (arXiv:2401.10791v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#26089;&#26399;&#23545;&#40784;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#23567;&#21021;&#22987;&#21270;&#21644;&#19968;&#20010;&#38544;&#34255;&#30340;ReLU&#23618;&#32593;&#32476;&#20013;&#65292;&#31070;&#32463;&#20803;&#20250;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#21521;&#20851;&#38190;&#26041;&#21521;&#36827;&#34892;&#23545;&#40784;&#65292;&#23548;&#33268;&#32593;&#32476;&#31232;&#30095;&#34920;&#31034;&#20197;&#21450;&#26799;&#24230;&#27969;&#22312;&#25910;&#25947;&#26102;&#30340;&#38544;&#21547;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31232;&#30095;&#35825;&#23548;&#30340;&#23545;&#40784;&#20063;&#20351;&#24471;&#35757;&#32451;&#30446;&#26631;&#30340;&#26368;&#23567;&#21270;&#21464;&#24471;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26159;&#28145;&#24230;&#23398;&#20064;&#25104;&#21151;&#30340;&#26680;&#24515;&#12290;&#21021;&#22987;&#21270;&#30340;&#35268;&#27169;&#26159;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#22240;&#20026;&#23567;&#30340;&#21021;&#22987;&#21270;&#36890;&#24120;&#19982;&#29305;&#24449;&#23398;&#20064;&#27169;&#24335;&#30456;&#20851;&#65292;&#22312;&#36825;&#31181;&#27169;&#24335;&#19979;&#65292;&#26799;&#24230;&#19979;&#38477;&#23545;&#31616;&#21333;&#35299;&#38544;&#21547;&#20559;&#22909;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26089;&#26399;&#23545;&#40784;&#38454;&#27573;&#30340;&#26222;&#36941;&#21644;&#37327;&#21270;&#25551;&#36848;&#65292;&#26368;&#21021;&#30001;Maennel&#31561;&#20154;&#25552;&#20986;&#12290;&#23545;&#20110;&#23567;&#21021;&#22987;&#21270;&#21644;&#19968;&#20010;&#38544;&#34255;&#30340;ReLU&#23618;&#32593;&#32476;&#65292;&#35757;&#32451;&#21160;&#24577;&#30340;&#26089;&#26399;&#38454;&#27573;&#23548;&#33268;&#31070;&#32463;&#20803;&#21521;&#20851;&#38190;&#26041;&#21521;&#36827;&#34892;&#23545;&#40784;&#12290;&#36825;&#31181;&#23545;&#40784;&#24341;&#21457;&#20102;&#32593;&#32476;&#30340;&#31232;&#30095;&#34920;&#31034;&#65292;&#36825;&#19982;&#26799;&#24230;&#27969;&#22312;&#25910;&#25947;&#26102;&#30340;&#38544;&#21547;&#20559;&#22909;&#30452;&#25509;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31232;&#30095;&#35825;&#23548;&#30340;&#23545;&#40784;&#26159;&#20197;&#22312;&#26368;&#23567;&#21270;&#35757;&#32451;&#30446;&#26631;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#20026;&#20195;&#20215;&#30340;&#65306;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#25454;&#31034;&#20363;&#65292;&#20854;&#20013;&#36229;&#21442;&#25968;&#32593;&#32476;&#26080;&#27861;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training neural networks with first order optimisation methods is at the core of the empirical success of deep learning. The scale of initialisation is a crucial factor, as small initialisations are generally associated to a feature learning regime, for which gradient descent is implicitly biased towards simple solutions. This work provides a general and quantitative description of the early alignment phase, originally introduced by Maennel et al. (2018) . For small initialisation and one hidden ReLU layer networks, the early stage of the training dynamics leads to an alignment of the neurons towards key directions. This alignment induces a sparse representation of the network, which is directly related to the implicit bias of gradient flow at convergence. This sparsity inducing alignment however comes at the expense of difficulties in minimising the training objective: we also provide a simple data example for which overparameterised networks fail to converge towards global minima and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24322;&#27493;Local-SGD&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#24322;&#27493;&#26356;&#26032;&#26356;&#39057;&#32321;&#65292;&#20294;&#20854;&#25910;&#25947;&#25152;&#38656;&#30340;&#36845;&#20195;&#27425;&#25968;&#22810;&#20110;&#21516;&#27493;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24310;&#36831;&#30340;Nesterov&#21160;&#37327;&#26356;&#26032;&#36827;&#34892;&#35843;&#25972;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#24322;&#27493;&#26356;&#26032;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.09135</link><description>&lt;p&gt;
&#24322;&#27493;Local-SGD&#35757;&#32451;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Local-SGD Training for Language Modeling. (arXiv:2401.09135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24322;&#27493;Local-SGD&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#24322;&#27493;&#26356;&#26032;&#26356;&#39057;&#32321;&#65292;&#20294;&#20854;&#25910;&#25947;&#25152;&#38656;&#30340;&#36845;&#20195;&#27425;&#25968;&#22810;&#20110;&#21516;&#27493;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24310;&#36831;&#30340;Nesterov&#21160;&#37327;&#26356;&#26032;&#36827;&#34892;&#35843;&#25972;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#24322;&#27493;&#26356;&#26032;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Local&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(Local-SGD)&#65292;&#20063;&#31216;&#20026;&#32852;&#37030;&#24179;&#22343;&#65292;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#35774;&#22791;&#22312;&#36890;&#20449;&#20013;&#25191;&#34892;&#22810;&#20010;SGD&#26356;&#26032;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24322;&#27493;Local-SGD&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#39564;&#35777;&#30740;&#31350;&#65307;&#21363;&#65292;&#27599;&#20010;&#24037;&#20316;&#33410;&#28857;&#22312;&#23436;&#25104;&#20854;SGD&#27493;&#39588;&#21518;&#31435;&#21363;&#26356;&#26032;&#20840;&#23616;&#21442;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#23519;&#24037;&#20316;&#33410;&#28857;&#30828;&#20214;&#24322;&#26500;&#24615;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#24037;&#20316;&#33410;&#28857;&#25968;&#37327;&#21644;&#20248;&#21270;&#22120;&#31561;&#22240;&#32032;&#23545;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#26356;&#39057;&#32321;&#22320;&#26356;&#26032;&#65288;&#20840;&#23616;&#65289;&#27169;&#22411;&#21442;&#25968;&#65292;&#20294;&#24322;&#27493;Local-SGD&#27604;&#20854;&#21516;&#27493;&#23545;&#24212;&#29289;&#38656;&#35201;&#26356;&#22810;&#36845;&#20195;&#25165;&#33021;&#25910;&#25947;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#24037;&#20316;&#33410;&#28857;&#26799;&#24230;&#38472;&#26087;&#26102;&#20840;&#23616;&#21442;&#25968;&#30340;&#21160;&#37327;&#21152;&#36895;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24310;&#36831;&#30340;Nesterov&#21160;&#37327;&#26356;&#26032;&#65292;&#26681;&#25454;&#24037;&#20316;&#33410;&#28857;&#30340;&#26412;&#22320;&#35757;&#32451;&#27493;&#39588;&#36827;&#34892;&#35843;&#25972;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of {\it asynchronous} Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based
&lt;/p&gt;</description></item><item><title>SiT&#26159;&#19968;&#31181;&#22522;&#20110;Diffusion Transformers&#39592;&#24178;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25554;&#20540;&#26694;&#26550;&#21644;&#21508;&#31181;&#35774;&#35745;&#36873;&#25321;&#30340;&#27169;&#22359;&#21270;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#22312;&#27169;&#22411;&#22823;&#23567;&#19978;&#36229;&#36807;DiT&#65292;&#22312;&#26465;&#20214;ImageNet&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#36739;&#20302;&#30340;FID-50K&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2401.08740</link><description>&lt;p&gt;
SiT:&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#25554;&#20540;&#20223;&#23556;&#21464;&#25442;&#25506;&#32034;&#22522;&#20110;&#27969;&#21160;&#21644;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers. (arXiv:2401.08740v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08740
&lt;/p&gt;
&lt;p&gt;
SiT&#26159;&#19968;&#31181;&#22522;&#20110;Diffusion Transformers&#39592;&#24178;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25554;&#20540;&#26694;&#26550;&#21644;&#21508;&#31181;&#35774;&#35745;&#36873;&#25321;&#30340;&#27169;&#22359;&#21270;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#22312;&#27169;&#22411;&#22823;&#23567;&#19978;&#36229;&#36807;DiT&#65292;&#22312;&#26465;&#20214;ImageNet&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#36739;&#20302;&#30340;FID-50K&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#21464;&#25442;&#22120;&#65288;DiT&#65289;&#39592;&#24178;&#30340;&#21487;&#25193;&#23637;&#25554;&#20540;&#20223;&#23556;&#21464;&#25442;&#22120;&#65288;SiT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#30340;&#31995;&#21015;&#12290;&#25554;&#20540;&#26694;&#26550;&#20801;&#35768;&#20197;&#27604;&#26631;&#20934;&#25193;&#25955;&#27169;&#22411;&#26356;&#28789;&#27963;&#30340;&#26041;&#24335;&#36830;&#25509;&#20004;&#20010;&#20998;&#24067;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#24314;&#31435;&#22312;&#21160;&#24577;&#20256;&#36755;&#19978;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#21508;&#31181;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#27169;&#22359;&#21270;&#30740;&#31350;&#65306;&#20351;&#29992;&#31163;&#25955;&#26102;&#38388;&#23398;&#20064;&#36824;&#26159;&#36830;&#32493;&#26102;&#38388;&#23398;&#20064;&#65292;&#20915;&#23450;&#27169;&#22411;&#23398;&#20064;&#30340;&#30446;&#26631;&#65292;&#36873;&#25321;&#36830;&#25509;&#20998;&#24067;&#30340;&#25554;&#20540;&#22120;&#65292;&#20197;&#21450;&#37096;&#32626;&#30830;&#23450;&#24615;&#36824;&#26159;&#38543;&#26426;&#37319;&#26679;&#22120;&#12290;&#36890;&#36807;&#31934;&#24515;&#24341;&#20837;&#19978;&#36848;&#20803;&#32032;&#65292;SiT&#22312;&#20855;&#26377;&#30456;&#21516;&#39592;&#24178;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;GFLOPs&#30340;&#26465;&#20214;ImageNet 256x256&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22312;&#27169;&#22411;&#22823;&#23567;&#19978;&#20840;&#38754;&#36229;&#36807;&#20102;DiT&#12290;&#36890;&#36807;&#25506;&#32034;&#21487;&#20197;&#19982;&#23398;&#20064;&#20998;&#24320;&#35843;&#25972;&#30340;&#21508;&#31181;&#25193;&#25955;&#31995;&#25968;&#65292;SiT&#22312;FID-50K&#35780;&#20998;&#19978;&#36798;&#21040;&#20102;2.06&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Scalable Interpolant Transformers (SiT), a family of generative models built on the backbone of Diffusion Transformers (DiT). The interpolant framework, which allows for connecting two distributions in a more flexible way than standard diffusion models, makes possible a modular study of various design choices impacting generative models built on dynamical transport: using discrete vs. continuous time learning, deciding the objective for the model to learn, choosing the interpolant connecting the distributions, and deploying a deterministic or stochastic sampler. By carefully introducing the above ingredients, SiT surpasses DiT uniformly across model sizes on the conditional ImageNet 256x256 benchmark using the exact same backbone, number of parameters, and GFLOPs. By exploring various diffusion coefficients, which can be tuned separately from learning, SiT achieves an FID-50K score of 2.06.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05535</link><description>&lt;p&gt;
&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Accuracy and Interpretability of Random Forests via Forest Pruning. (arXiv:2401.05535v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05535
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25509;&#36817;&#20960;&#21313;&#24180;&#30340;&#21457;&#23637;&#20043;&#21518;&#65292;&#38543;&#26426;&#26862;&#26519;&#20173;&#28982;&#22312;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#20013;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#36825;&#26041;&#38754;&#36229;&#36234;&#20102;&#20915;&#31574;&#26641;&#29978;&#33267;&#31070;&#32463;&#32593;&#32476;&#31561;&#26367;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#38543;&#26426;&#26862;&#26519;&#22312;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#27604;&#20915;&#31574;&#26641;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#21518;&#26041;&#27861;&#65292;&#26088;&#22312;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#22312;&#32473;&#23450;&#30340;&#38543;&#26426;&#26862;&#26519;&#20869;&#25214;&#21040;&#26368;&#20339;&#23376;&#26862;&#26519;&#65292;&#28982;&#21518;&#22312;&#36866;&#29992;&#30340;&#24773;&#20917;&#19979;&#23558;&#36873;&#23450;&#30340;&#26641;&#21512;&#24182;&#20026;&#19968;&#26869;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#32422;&#26463;&#31351;&#20030;&#25628;&#32034;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#22522;&#20110;LASSO&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#20013;&#33267;&#23569;&#26377;&#19968;&#31181;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decades after their inception, random forests continue to provide state-of-the-art accuracy in a variety of learning problems, outperforming in this respect alternative machine learning algorithms such as decision trees or even neural networks. However, being an ensemble method, the one aspect where random forests tend to severely underperform decision trees is interpretability. In the present work, we propose a post-hoc approach that aims to have the best of both worlds: the accuracy of random forests and the interpretability of decision trees. To this end, we present two forest-pruning methods to find an optimal sub-forest within a given random forest, and then, when applicable, combine the selected trees into one. Our first method relies on constrained exhaustive search, while our second method is based on an adaptation of the LASSO methodology. Extensive experiments over synthetic and real world datasets show that, in the majority of scenarios, at least one of the two methods propo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#26080;&#30028;&#26041;&#24046;&#30340;&#26377;&#20559;&#22122;&#22768;&#23545;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#20171;&#32461;&#20102;&#35813;&#31639;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.02828</link><description>&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#30340;&#25910;&#25947;&#36895;&#24230;&#65306;&#24102;&#26377;&#26080;&#30028;&#26041;&#24046;&#30340;&#26377;&#20559;&#22122;&#22768;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Convergence Rates for Stochastic Approximation: Biased Noise with Unbounded Variance, and Applications. (arXiv:2312.02828v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#26080;&#30028;&#26041;&#24046;&#30340;&#26377;&#20559;&#22122;&#22768;&#23545;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#20171;&#32461;&#20102;&#35813;&#31639;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
1951&#24180;&#32599;&#23486;&#26031;&#21644;&#33707;&#27931;&#24341;&#20837;&#30340;&#38543;&#26426;&#36924;&#36817;&#65288;SA&#65289;&#31639;&#27861;&#24050;&#32463;&#25104;&#20026;&#35299;&#26041;&#31243;$\mathbf{f}({\boldsymbol{\theta}}) = \mathbf{0}$&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#24403;&#21482;&#26377;$\mathbf{f}(\cdot)$&#30340;&#24102;&#22122;&#22768;&#27979;&#37327;&#21487;&#29992;&#26102;&#12290;&#22914;&#26524;&#23545;&#20110;&#26576;&#20010;&#20989;&#25968;$J(\cdot)$&#65292;$\mathbf{f}({\boldsymbol{\theta}}) = \nabla J({\boldsymbol{\theta}})$&#65292;&#37027;&#20040;SA&#20063;&#21487;&#20197;&#29992;&#26469;&#23547;&#25214;$J(\cdot)$&#30340;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;$t$&#65292;&#24403;&#21069;&#30340;&#29468;&#27979;${\boldsymbol{\theta}}_t$&#36890;&#36807;&#24418;&#24335;&#20026;$\mathbf{f}({\boldsymbol{\theta}}_t) + {\boldsymbol{\xi}}_{t+1}$&#30340;&#24102;&#22122;&#22768;&#27979;&#37327;&#26356;&#26032;&#20026;${\boldsymbol{\theta}}_{t+1}$&#12290;&#22312;&#35768;&#22810;&#25991;&#29486;&#20013;&#65292;&#20551;&#35774;&#35823;&#24046;&#39033;${\boldsymbol{\xi}}_{t+1}$&#30340;&#26465;&#20214;&#22343;&#20540;&#20026;&#38646;&#65292;&#21644;/&#25110;&#32773;&#23427;&#30340;&#26465;&#20214;&#26041;&#24046;&#38543;$t$&#65288;&#32780;&#19981;&#26159;${\boldsymbol{\theta}}_t$&#65289;&#34987;&#38480;&#21046;&#12290;&#22810;&#24180;&#26469;&#65292;SA&#24050;&#32463;&#24212;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20854;&#20013;&#19968;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Stochastic Approximation (SA) algorithm introduced by Robbins and Monro in 1951 has been a standard method for solving equations of the form $\mathbf{f}({\boldsymbol {\theta}}) = \mathbf{0}$, when only noisy measurements of $\mathbf{f}(\cdot)$ are available. If $\mathbf{f}({\boldsymbol {\theta}}) = \nabla J({\boldsymbol {\theta}})$ for some function $J(\cdot)$, then SA can also be used to find a stationary point of $J(\cdot)$. At each time $t$, the current guess ${\boldsymbol {\theta}}_t$ is updated to ${\boldsymbol {\theta}}_{t+1}$ using a noisy measurement of the form $\mathbf{f}({\boldsymbol {\theta}}_t) + {\boldsymbol {\xi}}_{t+1}$. In much of the literature, it is assumed that the error term ${\boldsymbol {\xi}}_{t+1}$ has zero conditional mean, and/or that its conditional variance is bounded as a function of $t$ (though not necessarily with respect to ${\boldsymbol {\theta}}_t$). Over the years, SA has been applied to a variety of areas, out of which the focus in this paper i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#21028;&#21035;&#22120;&#24341;&#23548;&#65292;&#29992;&#20110;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20248;&#21028;&#21035;&#22120;&#26469;&#32416;&#27491;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#26469;&#24212;&#23545;&#20351;&#29992;&#27425;&#20248;&#21028;&#21035;&#22120;&#30340;&#24773;&#20917;&#12290;&#22312;&#29983;&#25104;&#20998;&#23376;&#22270;&#30340;&#20219;&#21153;&#20013;&#65292;&#21028;&#21035;&#22120;&#24341;&#23548;&#26377;&#21161;&#20110;&#25552;&#39640;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15817</link><description>&lt;p&gt;
&#21028;&#21035;&#22120;&#24341;&#23548;&#19979;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discriminator Guidance for Autoregressive Diffusion Models. (arXiv:2310.15817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#21028;&#21035;&#22120;&#24341;&#23548;&#65292;&#29992;&#20110;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20248;&#21028;&#21035;&#22120;&#26469;&#32416;&#27491;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#26469;&#24212;&#23545;&#20351;&#29992;&#27425;&#20248;&#21028;&#21035;&#22120;&#30340;&#24773;&#20917;&#12290;&#22312;&#29983;&#25104;&#20998;&#23376;&#22270;&#30340;&#20219;&#21153;&#20013;&#65292;&#21028;&#21035;&#22120;&#24341;&#23548;&#26377;&#21161;&#20110;&#25552;&#39640;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#21028;&#21035;&#22120;&#24341;&#23548;&#12290;&#22312;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#21028;&#21035;&#22120;&#24341;&#23548;&#25193;&#25955;&#36807;&#31243;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#20351;&#29992;&#36807;&#65292;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#22312;&#31163;&#25955;&#24773;&#20917;&#19979;&#20351;&#29992;&#21028;&#21035;&#22120;&#21644;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#26368;&#20248;&#21028;&#21035;&#22120;&#23558;&#32416;&#27491;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#33021;&#22815;&#20174;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#20013;&#31934;&#30830;&#37319;&#26679;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#24212;&#23545;&#20351;&#29992;&#27425;&#20248;&#21028;&#21035;&#22120;&#30340;&#23454;&#38469;&#24773;&#20917;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#36845;&#20195;&#22320;&#23558;&#21028;&#21035;&#22120;&#30340;&#39044;&#27979;&#32435;&#20837;&#32771;&#34385;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#25104;&#20998;&#23376;&#22270;&#30340;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#21028;&#21035;&#22120;&#30456;&#36739;&#20110;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26102;&#30340;&#29983;&#25104;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce discriminator guidance in the setting of Autoregressive Diffusion Models. The use of a discriminator to guide a diffusion process has previously been used for continuous diffusion models, and in this work we derive ways of using a discriminator together with a pretrained generative model in the discrete case. First, we show that using an optimal discriminator will correct the pretrained model and enable exact sampling from the underlying data distribution. Second, to account for the realistic scenario of using a sub-optimal discriminator, we derive a sequential Monte Carlo algorithm which iteratively takes the predictions from the discrimiator into account during the generation process. We test these approaches on the task of generating molecular graphs and show how the discriminator improves the generative performance over using only the pretrained model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25968;&#25454;&#20381;&#36182;&#32806;&#21512;&#26469;&#26500;&#24314;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03725</link><description>&lt;p&gt;
&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#32806;&#21512;&#30340;&#38543;&#26426;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic interpolants with data-dependent couplings. (arXiv:2310.03725v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25968;&#25454;&#20381;&#36182;&#32806;&#21512;&#26469;&#26500;&#24314;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21160;&#24577;&#27979;&#24230;&#20256;&#36755;&#21551;&#21457;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#27969;&#21644;&#25193;&#25955;&#65289;&#26500;&#24314;&#20102;&#20004;&#20010;&#27010;&#29575;&#23494;&#24230;&#20043;&#38388;&#30340;&#36830;&#32493;&#26102;&#38388;&#26144;&#23556;&#12290;&#25353;&#29031;&#20256;&#32479;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;&#30446;&#26631;&#23494;&#24230;&#65292;&#21482;&#33021;&#36890;&#36807;&#26679;&#26412;&#35775;&#38382;&#65292;&#32780;&#21478;&#19968;&#20010;&#26159;&#31616;&#21333;&#30340;&#22522;&#30784;&#23494;&#24230;&#65292;&#19982;&#25968;&#25454;&#26080;&#20851;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#25554;&#20540;&#30340;&#26694;&#26550;&#65292;&#35268;&#33539;&#21270;&#20102;&#22914;&#20309;&#8220;&#32806;&#21512;&#8221;&#22522;&#26412;&#23494;&#24230;&#21644;&#30446;&#26631;&#23494;&#24230;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#31867;&#21035;&#26631;&#31614;&#25110;&#36830;&#32493;&#23884;&#20837;&#30340;&#20449;&#24687;&#32435;&#20837;&#21040;&#26500;&#24314;&#21160;&#24577;&#20256;&#36755;&#26144;&#23556;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#35299;&#20915;&#31867;&#20284;&#20110;&#26631;&#20934;&#29420;&#31435;&#35774;&#32622;&#30340;&#31616;&#21333;&#24179;&#26041;&#25439;&#22833;&#22238;&#24402;&#38382;&#39064;&#26469;&#23398;&#20064;&#36825;&#20123;&#20256;&#36755;&#26144;&#23556;&#12290;&#36890;&#36807;&#36229;&#20998;&#36776;&#29575;&#21644;&#20462;&#22797;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26500;&#24314;&#20381;&#36182;&#32806;&#21512;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models inspired by dynamical transport of measure -- such as flows and diffusions -- construct a continuous-time map between two probability densities. Conventionally, one of these is the target density, only accessible through samples, while the other is taken as a simple base density that is data-agnostic. In this work, using the framework of stochastic interpolants, we formalize how to \textit{couple} the base and the target densities. This enables us to incorporate information about class labels or continuous embeddings to construct dynamical transport maps that serve as conditional generative models. We show that these transport maps can be learned by solving a simple square loss regression problem analogous to the standard independent setting. We demonstrate the usefulness of constructing dependent couplings in practice through experiments in super-resolution and in-painting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#31163;&#24515;&#27893;&#22312;&#22810;&#30456;&#27969;&#19979;&#30340;&#29305;&#24615;&#21442;&#25968;&#21644;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2310.03001</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#22810;&#30456;&#27969;&#19979;&#31163;&#24515;&#27893;&#30340;&#29305;&#24615;&#21442;&#25968;&#21644;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Learning characteristic parameters and dynamics of centrifugal pumps under multi-phase flow using physics-informed neural networks. (arXiv:2310.03001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#31163;&#24515;&#27893;&#22312;&#22810;&#30456;&#27969;&#19979;&#30340;&#29305;&#24615;&#21442;&#25968;&#21644;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#28508;&#27893;&#65288;ESP&#65289;&#30001;&#20110;&#20854;&#39640;&#27969;&#37327;&#21644;&#22686;&#21387;&#65292;&#26159;&#27833;&#27668;&#24037;&#19994;&#20013;&#31532;&#20108;&#24120;&#29992;&#30340;&#20154;&#24037;&#25552;&#21319;&#35774;&#22791;&#12290;&#23427;&#20204;&#32463;&#24120;&#38656;&#35201;&#22788;&#29702;&#22810;&#30456;&#27969;&#21160;&#65292;&#36825;&#20123;&#27969;&#20307;&#36890;&#24120;&#21253;&#21547;&#28867;&#31867;&#12289;&#27700;&#21644;/&#25110;&#27785;&#31215;&#29289;&#30340;&#28151;&#21512;&#29289;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36890;&#24120;&#20250;&#24418;&#25104;&#20083;&#28082;&#65292;&#23427;&#26159;&#30001;&#20004;&#31181;&#19981;&#20114;&#28342;&#27969;&#20307;&#32452;&#25104;&#30340;&#28082;&#28082;&#27969;&#21160;&#65292;&#20854;&#26377;&#25928;&#31896;&#24230;&#21644;&#23494;&#24230;&#19982;&#21333;&#29420;&#30340;&#21333;&#30456;&#27969;&#21160;&#26377;&#25152;&#19981;&#21516;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#20934;&#30830;&#24314;&#27169;ESP&#31995;&#32479;&#23545;&#20110;&#20248;&#21270;&#27833;&#30000;&#29983;&#20135;&#21644;&#23454;&#26045;&#25511;&#21046;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#38480;&#21046;&#21644;&#32463;&#27982;&#21407;&#22240;&#65292;&#23454;&#26102;&#21644;&#30452;&#25509;&#27979;&#37327;&#27969;&#20307;&#21644;&#31995;&#32479;&#29305;&#24615;&#36890;&#24120;&#26159;&#19981;&#21487;&#23454;&#29616;&#30340;&#12290;&#22240;&#27492;&#65292;&#19968;&#33324;&#32771;&#34385;&#38388;&#25509;&#26041;&#27861;&#26469;&#20272;&#35745;&#31995;&#32479;&#21442;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#20851;&#38190;&#30340;&#31995;&#32479;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrical submersible pumps (ESP) are the second most used artificial lifting equipment in the oil and gas industry due to their high flow rates and boost pressures. They often have to handle multiphase flows, which usually contain a mixture of hydrocarbons, water, and/or sediments. Given these circumstances, emulsions are commonly formed. It is a liquid-liquid flow composed of two immiscible fluids whose effective viscosity and density differ from the single phase separately. In this context, accurate modeling of ESP systems is crucial for optimizing oil production and implementing control strategies. However, real-time and direct measurement of fluid and system characteristics is often impractical due to time constraints and economy. Hence, indirect methods are generally considered to estimate the system parameters. In this paper, we formulate a machine learning model based on Physics-Informed Neural Networks (PINNs) to estimate crucial system parameters. In order to study the effic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SYNLABEL&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#22522;&#20110;&#30495;&#23454;&#25968;&#25454;&#30340;&#26080;&#22122;&#22768;&#25968;&#25454;&#38598;&#65292;&#24182;&#21487;&#20197;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#20998;&#37197;&#19968;&#20010;&#36719;&#26631;&#31614;&#25110;&#26631;&#31614;&#20998;&#24067;&#65292;&#29992;&#20110;&#25913;&#36827;&#26631;&#31614;&#22122;&#22768;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2309.04318</link><description>&lt;p&gt;
&#29983;&#25104;&#30495;&#23454;&#26631;&#31614;: &#29992;&#20110;&#26631;&#31614;&#22122;&#22768;&#30740;&#31350;&#30340;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generating the Ground Truth: Synthetic Data for Label Noise Research. (arXiv:2309.04318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SYNLABEL&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#22522;&#20110;&#30495;&#23454;&#25968;&#25454;&#30340;&#26080;&#22122;&#22768;&#25968;&#25454;&#38598;&#65292;&#24182;&#21487;&#20197;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#20998;&#37197;&#19968;&#20010;&#36719;&#26631;&#31614;&#25110;&#26631;&#31614;&#20998;&#24067;&#65292;&#29992;&#20110;&#25913;&#36827;&#26631;&#31614;&#22122;&#22768;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#30495;&#23454;&#19990;&#30028;&#30340;&#20998;&#31867;&#20219;&#21153;&#37117;&#23384;&#22312;&#30528;&#19968;&#23450;&#31243;&#24230;&#30340;&#26631;&#31614;&#22122;&#22768;&#12290;&#36825;&#31181;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#23545;&#20110;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#19988;&#20351;&#24471;&#22122;&#22768;&#22788;&#29702;&#26041;&#27861;&#30340;&#35780;&#20272;&#21464;&#24471;&#22797;&#26434;&#65292;&#22240;&#20026;&#27809;&#26377;&#28165;&#26224;&#30340;&#26631;&#31614;&#65292;&#26080;&#27861;&#20934;&#30830;&#34913;&#37327;&#20854;&#24615;&#33021;&#12290;&#22312;&#26631;&#31614;&#22122;&#22768;&#30740;&#31350;&#20013;&#65292;&#36890;&#24120;&#25509;&#21463;&#26377;&#22122;&#22768;&#25110;&#31616;&#21333;&#30340;&#27169;&#25311;&#25968;&#25454;&#20316;&#20026;&#22522;&#32447;&#65292;&#28982;&#21518;&#27880;&#20837;&#20855;&#26377;&#24050;&#30693;&#23646;&#24615;&#30340;&#39069;&#22806;&#22122;&#22768;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SYNLABEL&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#25913;&#36827;&#19978;&#36848;&#26041;&#27861;&#30340;&#26694;&#26550;&#12290;&#23427;&#20801;&#35768;&#36890;&#36807;&#39044;&#20808;&#25351;&#23450;&#25110;&#23398;&#20064;&#19968;&#20010;&#20989;&#25968;&#65292;&#24182;&#23558;&#20854;&#23450;&#20041;&#20026;&#29983;&#25104;&#26631;&#31614;&#30340;&#22522;&#26412;&#30495;&#20540;&#20989;&#25968;&#65292;&#20174;&#32780;&#21019;&#24314;&#19968;&#20010;&#26080;&#22122;&#22768;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#20989;&#25968;&#22495;&#20013;&#30340;&#36873;&#23450;&#29305;&#24449;&#19978;&#37325;&#26032;&#37319;&#26679;&#19968;&#20123;&#20540;&#65292;&#35780;&#20272;&#20989;&#25968;&#24182;&#27719;&#24635;&#32467;&#26524;&#26631;&#31614;&#65292;&#21487;&#20197;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#20998;&#37197;&#19968;&#20010;&#36719;&#26631;&#31614;&#25110;&#26631;&#31614;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most real-world classification tasks suffer from label noise to some extent. Such noise in the data adversely affects the generalization error of learned models and complicates the evaluation of noise-handling methods, as their performance cannot be accurately measured without clean labels. In label noise research, typically either noisy or incomplex simulated data are accepted as a baseline, into which additional noise with known properties is injected. In this paper, we propose SYNLABEL, a framework that aims to improve upon the aforementioned methodologies. It allows for creating a noiseless dataset informed by real data, by either pre-specifying or learning a function and defining it as the ground truth function from which labels are generated. Furthermore, by resampling a number of values for selected features in the function domain, evaluating the function and aggregating the resulting labels, each data point can be assigned a soft label or label distribution. Such distributions 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HePCo&#30340;&#36731;&#37327;&#32423;&#25552;&#31034;&#21512;&#24182;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#21644;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#20849;&#20139;&#25110;&#23384;&#20648;&#20219;&#20309;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#20102;&#36890;&#20449;&#24320;&#38144;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2306.09970</link><description>&lt;p&gt;
HePCo&#65306;&#29992;&#20110;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#30340;&#26080;&#25968;&#25454;&#24322;&#26500;&#25552;&#31034;&#21512;&#24182;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HePCo: Data-Free Heterogeneous Prompt Consolidation for Continual Federated Learning. (arXiv:2306.09970v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HePCo&#30340;&#36731;&#37327;&#32423;&#25552;&#31034;&#21512;&#24182;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#21644;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#20849;&#20139;&#25110;&#23384;&#20648;&#20219;&#20309;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#20102;&#36890;&#20449;&#24320;&#38144;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#30340;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26381;&#21153;&#22120;&#19982;&#19968;&#32452;&#23458;&#25143;&#31471;&#36890;&#20449;&#65292;&#20197;&#36880;&#27493;&#23398;&#20064;&#26032;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#19981;&#20849;&#20139;&#25110;&#23384;&#20648;&#20219;&#20309;&#25968;&#25454;&#12290;&#30001;&#20110;&#26469;&#33258;&#36830;&#32493;&#21644;&#32852;&#37030;&#23398;&#20064;&#35282;&#24230;&#30340;&#25361;&#25112;&#65292;&#27492;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21463;&#21040;&#20102;&#21152;&#21095;&#12290;&#26412;&#25991;&#23581;&#35797;&#22312;&#19981;&#38656;&#35201;&#35775;&#38382;&#20219;&#20309;&#23384;&#20648;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36951;&#24536;&#21644;&#24322;&#26500;&#38382;&#39064;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#24320;&#38144;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#24182;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;HePCo&#30340;&#26032;&#39062;&#36731;&#37327;&#32423;&#25552;&#31034;&#21512;&#24182;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#27492;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#22343;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#24182;&#20445;&#25345;&#20302;&#36890;&#20449;&#24320;&#38144;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on the important yet understudied problem of Continual Federated Learning (CFL), where a server communicates with a set of clients to incrementally learn new concepts over time without sharing or storing any data. The complexity of this problem is compounded by challenges from both the Continual and Federated Learning perspectives. Specifically, models trained in a CFL setup suffer from catastrophic forgetting which is exacerbated by data heterogeneity across clients. Existing attempts at this problem tend to impose large overheads on clients and communication channels or require access to stored data which renders them unsuitable for real-world use due to privacy. In this paper, we attempt to tackle forgetting and heterogeneity while minimizing overhead costs and without requiring access to any stored data. We achieve this by leveraging a prompting based approach (such that only prompts and classifier heads have to be communicated) and proposing a novel and lig
&lt;/p&gt;</description></item><item><title>OpenOOD v1.5 &#26159;&#23545;&#21069;&#36523;&#30340;&#37325;&#22823;&#25913;&#36827;&#65292;&#23558;OCC&#26816;&#27979;&#26041;&#27861;&#30340;&#35780;&#20272;&#33021;&#21147;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#35843;&#26597;&#20102;&#20840;&#20809;&#35889;OCC&#26816;&#27979;&#65292;&#24341;&#20837;&#20102;&#22312;&#32447;&#25490;&#34892;&#27036;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#35780;&#20272;&#22120;&#31561;&#26032;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.09301</link><description>&lt;p&gt;
OpenOOD v1.5&#65306;&#22686;&#24378;&#30340;OCC&#65288;Out-of-Distribution Detection&#65289;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection. (arXiv:2306.09301v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09301
&lt;/p&gt;
&lt;p&gt;
OpenOOD v1.5 &#26159;&#23545;&#21069;&#36523;&#30340;&#37325;&#22823;&#25913;&#36827;&#65292;&#23558;OCC&#26816;&#27979;&#26041;&#27861;&#30340;&#35780;&#20272;&#33021;&#21147;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#35843;&#26597;&#20102;&#20840;&#20809;&#35889;OCC&#26816;&#27979;&#65292;&#24341;&#20837;&#20102;&#22312;&#32447;&#25490;&#34892;&#27036;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#35780;&#20272;&#22120;&#31561;&#26032;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OCC&#26816;&#27979;&#23545;&#20110;&#24320;&#25918;&#19990;&#30028;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#38752;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;OCC&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#35780;&#20272;&#19981;&#19968;&#33268;&#24615;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#38590;&#20197;&#36319;&#36394;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;OpenOOD v1.5&#65292;&#36825;&#26159;&#23545;&#21069;&#36523;&#30340;&#37325;&#22823;&#25913;&#36827;&#65292;&#30830;&#20445;OCC&#26816;&#27979;&#26041;&#27861;&#30340;&#20934;&#30830;&#12289;&#26631;&#20934;&#21270;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#35780;&#20272;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;OpenOOD v1.5&#23558;&#20854;&#35780;&#20272;&#33021;&#21147;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#22914;ImageNet&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#35843;&#26597;&#20102;&#20840;&#20809;&#35889;OCC&#26816;&#27979;&#65292;&#24341;&#20837;&#20102;&#22312;&#32447;&#25490;&#34892;&#27036;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#35780;&#20272;&#22120;&#31561;&#26032;&#21151;&#33021;&#12290;&#35813;&#24037;&#20316;&#36824;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#32508;&#21512;&#23454;&#39564;&#32467;&#26524;&#30340;&#35265;&#35299;&#65292;&#20174;&#32780;&#20016;&#23500;&#20102;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution (OOD) detection is critical for the reliable operation of open-world intelligent systems. Despite the emergence of an increasing number of OOD detection methods, the evaluation inconsistencies present challenges for tracking the progress in this field. OpenOOD v1 initiated the unification of the OOD detection evaluation but faced limitations in scalability and usability. In response, this paper presents OpenOOD v1.5, a significant improvement from its predecessor that ensures accurate, standardized, and user-friendly evaluation of OOD detection methodologies. Notably, OpenOOD v1.5 extends its evaluation capabilities to large-scale datasets such as ImageNet, investigates full-spectrum OOD detection which is important yet underexplored, and introduces new features including an online leaderboard and an easy-to-use evaluator. This work also contributes in-depth analysis and insights derived from comprehensive experimental results, thereby enriching the knowledge pool o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;SGD-like&#31639;&#27861;&#65292;&#27880;&#20837;&#38543;&#26426;&#22122;&#22768;&#24182;&#21033;&#29992;&#20998;&#24067;&#23545;&#31216;&#24615;&#26469;&#20943;&#23569;&#26041;&#24046;&#65292;&#20197;&#23547;&#25214;&#20855;&#26377;&#20302;&#28023;&#26862;&#30697;&#38453;&#36857;&#30340;&#24179;&#22374;&#26497;&#23567;&#20540;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#29575;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.08553</link><description>&lt;p&gt;
&#22122;&#22768;&#31283;&#23450;&#20248;&#21270;&#23545;&#20110;&#20855;&#26377;&#26368;&#20248;&#25910;&#25947;&#29575;&#30340;&#24179;&#22374;&#26497;&#23567;&#20540;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Noise Stability Optimization for Flat Minima with Optimal Convergence Rates. (arXiv:2306.08553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;SGD-like&#31639;&#27861;&#65292;&#27880;&#20837;&#38543;&#26426;&#22122;&#22768;&#24182;&#21033;&#29992;&#20998;&#24067;&#23545;&#31216;&#24615;&#26469;&#20943;&#23569;&#26041;&#24046;&#65292;&#20197;&#23547;&#25214;&#20855;&#26377;&#20302;&#28023;&#26862;&#30697;&#38453;&#36857;&#30340;&#24179;&#22374;&#26497;&#23567;&#20540;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#29575;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#21152;&#20837;&#21152;&#26435;&#25200;&#21160;&#26469;&#25214;&#21040;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#12290;&#32473;&#23450;&#19968;&#20010;&#38750;&#20984;&#20989;&#25968;$f:\mathbb{R}^d\rightarrow \mathbb{R}$&#21644;&#19968;&#20010;$d$&#32500;&#20998;&#24067;$\mathcal{P}$&#65292;&#25105;&#20204;&#25200;&#21160;$f$&#30340;&#26435;&#37325;&#65292;&#24182;&#23450;&#20041;$F(W)=\mathbb{E}[f({W+U})]$&#65292;&#20854;&#20013;$U$&#26159;&#19968;&#20010;&#20174;$\mathcal{P}$&#20013;&#38543;&#26426;&#25277;&#21462;&#30340;&#26679;&#26412;&#12290;&#36825;&#20010;&#36807;&#31243;&#36890;&#36807;$f$&#30340;&#28023;&#26862;&#30697;&#38453;&#30340;&#36857;&#26469;&#35825;&#23548;&#27491;&#21017;&#21270;&#65292;&#20197;&#36866;&#24212;&#20110;&#23567;&#30340;&#12289;&#21508;&#21521;&#21516;&#24615;&#30340;&#39640;&#26031;&#25200;&#21160;&#12290;&#22240;&#27492;&#65292;&#21152;&#26435;&#25200;&#21160;&#30340;&#20989;&#25968;&#20559;&#21521;&#20110;&#24102;&#26377;&#20302;&#28023;&#26862;&#30697;&#38453;&#36857;&#30340;&#26497;&#23567;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;SGD&#30340;&#31639;&#27861;&#65292;&#22312;&#35745;&#31639;&#26799;&#24230;&#20043;&#21069;&#27880;&#20837;&#38543;&#26426;&#22122;&#22768;&#65292;&#21516;&#26102;&#21033;&#29992;$\mathcal{P}$&#30340;&#23545;&#31216;&#24615;&#26469;&#20943;&#23569;&#26041;&#24046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We consider finding flat, local minimizers by adding average weight perturbations. Given a nonconvex function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ and a $d$-dimensional distribution $\mathcal{P}$ which is symmetric at zero, we perturb the weight of $f$ and define $F(W) = \mathbb{E}[f({W + U})]$, where $U$ is a random sample from $\mathcal{P}$. This injection induces regularization through the Hessian trace of $f$ for small, isotropic Gaussian perturbations. Thus, the weight-perturbed function biases to minimizers with low Hessian trace. Several prior works have studied settings related to this weight-perturbed function by designing algorithms to improve generalization. Still, convergence rates are not known for finding minima under the average perturbations of the function $F$. This paper considers an SGD-like algorithm that injects random noise before computing gradients while leveraging the symmetry of $\mathcal{P}$ to reduce variance. We then provide a rigorous analysis, showing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#25968;&#25454;&#20998;&#25104;&#25209;&#27425;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#39640;&#32500;&#36229;&#21442;&#25968;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25209;&#37327;&#22823;&#23567;&#36873;&#25321;&#65292;&#31283;&#23450;&#20102;&#39118;&#38505;&#34892;&#20026;&#65292;&#28040;&#38500;&#20102;&#25554;&#20540;&#28857;&#22788;&#30340;&#33192;&#32960;&#21644;&#21452;&#23792;&#29616;&#35937;</title><link>http://arxiv.org/abs/2306.08432</link><description>&lt;p&gt;
&#25209;&#27425;&#20351;&#39640;&#32500;&#36229;&#21442;&#25968;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#23567;&#35268;&#33539;&#39118;&#38505;&#31283;&#23450;
&lt;/p&gt;
&lt;p&gt;
Batches Stabilize the Minimum Norm Risk in High Dimensional Overparameterized Linear Regression. (arXiv:2306.08432v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#25968;&#25454;&#20998;&#25104;&#25209;&#27425;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#39640;&#32500;&#36229;&#21442;&#25968;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25209;&#37327;&#22823;&#23567;&#36873;&#25321;&#65292;&#31283;&#23450;&#20102;&#39118;&#38505;&#34892;&#20026;&#65292;&#28040;&#38500;&#20102;&#25554;&#20540;&#28857;&#22788;&#30340;&#33192;&#32960;&#21644;&#21452;&#23792;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#25968;&#25454;&#20998;&#25104;&#25209;&#27425;&#30340;&#23398;&#20064;&#31639;&#27861;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#36890;&#24120;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#20043;&#38388;&#25552;&#20379;&#26377;&#29992;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#36890;&#36807;&#20855;&#26377;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#29305;&#24449;&#30340;&#26368;&#23567;&#35268;&#33539;&#36229;&#21442;&#25968;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#35270;&#35282;&#26469;&#30740;&#31350;&#25209;&#37327;&#20998;&#21306;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#24314;&#35758;&#26368;&#23567;&#35268;&#33539;&#20272;&#35745;&#37327;&#30340;&#33258;&#28982;&#23567;&#25209;&#37327;&#29256;&#26412;&#65292;&#24182;&#25512;&#23548;&#20986;&#20854;&#20108;&#27425;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#34920;&#26126;&#20854;&#19982;&#22122;&#22768;&#27700;&#24179;&#20197;&#21450;&#36807;&#24230;&#21442;&#25968;&#21270;&#27604;&#20363;&#25104;&#21453;&#27604;&#65292;&#23545;&#20110;&#26368;&#20339;&#25209;&#37327;&#22823;&#23567;&#30340;&#36873;&#25321;&#12290;&#19982;&#26368;&#23567;&#35268;&#33539;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#20855;&#26377;&#31283;&#23450;&#30340;&#39118;&#38505;&#34892;&#20026;&#65292;&#20854;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#27604;&#20363;&#19978;&#21333;&#35843;&#36882;&#22686;&#65292;&#28040;&#38500;&#20102;&#25554;&#20540;&#28857;&#22788;&#30340;&#33192;&#32960;&#21644;&#21452;&#23792;&#29616;&#35937;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25209;&#22788;&#29702;&#25152;&#25552;&#20379;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#20197;&#36890;&#36807;&#29305;&#24449;&#37325;&#21472;&#26469;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning algorithms that divide the data into batches are prevalent in many machine-learning applications, typically offering useful trade-offs between computational efficiency and performance. In this paper, we examine the benefits of batch-partitioning through the lens of a minimum-norm overparameterized linear regression model with isotropic Gaussian features. We suggest a natural small-batch version of the minimum-norm estimator, and derive an upper bound on its quadratic risk, showing it is inversely proportional to the noise level as well as to the overparameterization ratio, for the optimal choice of batch size. In contrast to minimum-norm, our estimator admits a stable risk behavior that is monotonically increasing in the overparameterization ratio, eliminating both the blowup at the interpolation point and the double-descent phenomenon. Interestingly, we observe that this implicit regularization offered by the batch partition is partially explained by feature overlap between t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31561;&#24335;&#23884;&#20837;&#28145;&#24230;Lagrange&#23545;&#20598;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#24102;&#26631;&#31614;&#30340;&#36924;&#36817;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#26041;&#27861;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23884;&#20837;&#31561;&#24335;&#32422;&#26463;&#26469;&#30830;&#20445;&#21487;&#34892;&#35299;&#65292;&#24182;&#20351;&#29992;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;DeepLDE&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#20248;&#21270;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.06674</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#31561;&#24335;&#23884;&#20837;&#28145;&#24230;Lagrange&#23545;&#20598;&#31639;&#27861;&#20248;&#21270;&#36924;&#36817;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Equality Embedded Deep Lagrange Dual for Approximate Constrained Optimization. (arXiv:2306.06674v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06674
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31561;&#24335;&#23884;&#20837;&#28145;&#24230;Lagrange&#23545;&#20598;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#24102;&#26631;&#31614;&#30340;&#36924;&#36817;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#26041;&#27861;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23884;&#20837;&#31561;&#24335;&#32422;&#26463;&#26469;&#30830;&#20445;&#21487;&#34892;&#35299;&#65292;&#24182;&#20351;&#29992;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;DeepLDE&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#20248;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#20256;&#32479;&#27714;&#35299;&#26041;&#27861;&#36890;&#24120;&#35745;&#31639;&#37327;&#36739;&#22823;&#65292;&#29305;&#21035;&#26159;&#22312;&#35268;&#27169;&#36739;&#22823;&#12289;&#26102;&#38388;&#25935;&#24863;&#30340;&#38382;&#39064;&#19978;&#26356;&#26159;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#24555;&#36895;&#26368;&#20248;&#35299;&#36924;&#36817;&#22120;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#36234;&#26469;&#36234;&#22823;&#20852;&#36259;&#65292;&#20294;&#26159;&#23558;&#32422;&#26463;&#26465;&#20214;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DeepLDE&#30340;&#28145;&#24230;Lagrange&#23545;&#20598;&#31639;&#27861;&#65292;&#35813;&#26694;&#26550;&#23398;&#20064;&#22312;&#19981;&#20351;&#29992;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23547;&#25214;&#26368;&#20248;&#35299;&#65292;&#36890;&#36807;&#23558;&#31561;&#24335;&#32422;&#26463;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#26469;&#30830;&#20445;&#21487;&#34892;&#35299;&#65292;&#24182;&#20351;&#29992;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#23545;&#19981;&#31561;&#24335;&#32422;&#26463;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DeepLDE&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#34920;&#26126;&#20165;&#38752;&#21407;&#22987;-&#23545;&#20598;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#30830;&#20445;&#31561;&#24335;&#32422;&#26463;&#65292;&#38656;&#35201;&#31561;&#24335;&#23884;&#20837;&#30340;&#24110;&#21161;&#12290;&#22312;&#20984;&#12289;&#38750;&#20984;&#21644;&#20132;&#27969;&#26368;&#20248;&#28526;&#27969;&#65288;AC-OPF&#65289;&#38382;&#39064;&#30340;&#27169;&#25311;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DeepLDE&#30340;&#26368;&#20248;&#24615;&#33021;&#32780;&#19988;&#22987;&#32456;&#20445;&#35777;&#21487;&#34892;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional solvers are often computationally expensive for constrained optimization, particularly in large-scale and time-critical problems. While this leads to a growing interest in using neural networks (NNs) as fast optimal solution approximators, incorporating the constraints with NNs is challenging. In this regard, we propose deep Lagrange dual with equality embedding (DeepLDE), a framework that learns to find an optimal solution without using labels. To ensure feasible solutions, we embed equality constraints into the NNs and train the NNs using the primal-dual method to impose inequality constraints. Furthermore, we prove the convergence of DeepLDE and show that the primal-dual learning method alone cannot ensure equality constraints without the help of equality embedding. Simulation results on convex, non-convex, and AC optimal power flow (AC-OPF) problems show that the proposed DeepLDE achieves the smallest optimality gap among all the NN-based approaches while always ensuri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.16326</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#22522;&#20934;&#12289;&#22522;&#32447;&#21644;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#25163;&#21160;&#31579;&#36873;&#21644;&#25552;&#21462;&#30693;&#35782;&#21464;&#24471;&#22256;&#38590;&#12290;&#33258;&#21160;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;BioNLP&#65289;&#25216;&#26415;&#26377;&#21161;&#20110;&#20943;&#36731;&#36825;&#31181;&#36127;&#25285;&#12290;&#36817;&#24180;&#26469;&#65292;&#22914;GPT-3&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;BioNLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#26041;&#27861;&#24320;&#21457;&#21644;&#19979;&#28216;&#29992;&#25143;&#30340;&#24433;&#21709;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#65288;1&#65289;&#22312;&#22235;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#22312;&#20843;&#20010;BioNLP&#25968;&#25454;&#38598;&#20013;&#24314;&#31435;&#20102;GPT-3&#21644;GPT-4&#22312;&#38646;-shot&#21644;&#19968;-shot&#35774;&#32622;&#19979;&#30340;&#22522;&#20934;&#34920;&#29616;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20851;&#31995;&#25552;&#21462;&#65292;&#22810;&#26631;&#31614;&#25991;&#26723;&#20998;&#31867;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#25512;&#29702;&#65307;&#65288;2&#65289;&#23457;&#26597;&#20102;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#38169;&#35823;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#32570;&#22833;&#65292;&#19981;&#19968;&#33268;&#21644;&#19981;&#38656;&#35201;&#30340;&#20154;&#24037;&#20869;&#23481;&#65307;&#65288;3&#65289;&#25552;&#20986;&#20102;&#20351;&#29992;LLMs&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;BraTS 2023&#20462;&#22797;&#25361;&#25112;&#65292;&#35201;&#27714;&#21442;&#19982;&#32773;&#20351;&#29992;&#20462;&#34917;&#25216;&#26415;&#20174;&#26377;&#30149;&#21464;&#30340;&#33041;&#37096;&#25195;&#25551;&#20013;&#21512;&#25104;&#20581;&#24247;&#33041;&#25195;&#25551;&#65292;&#20197;&#35299;&#20915;&#35768;&#22810;&#31639;&#27861;&#26080;&#27861;&#20998;&#26512;&#30149;&#21464;&#22270;&#20687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08992</link><description>&lt;p&gt;
&#33041;&#32959;&#30244;&#20998;&#21106;&#65288;BraTS&#65289;&#25361;&#25112;&#36187;2023&#65306;&#36890;&#36807;&#20462;&#22797;&#29983;&#25104;&#20581;&#24247;&#33041;&#32452;&#32455;&#30340;&#23616;&#37096;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Brain Tumor Segmentation (BraTS) Challenge 2023: Local Synthesis of Healthy Brain Tissue via Inpainting. (arXiv:2305.08992v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08992
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;BraTS 2023&#20462;&#22797;&#25361;&#25112;&#65292;&#35201;&#27714;&#21442;&#19982;&#32773;&#20351;&#29992;&#20462;&#34917;&#25216;&#26415;&#20174;&#26377;&#30149;&#21464;&#30340;&#33041;&#37096;&#25195;&#25551;&#20013;&#21512;&#25104;&#20581;&#24247;&#33041;&#25195;&#25551;&#65292;&#20197;&#35299;&#20915;&#35768;&#22810;&#31639;&#27861;&#26080;&#27861;&#20998;&#26512;&#30149;&#21464;&#22270;&#20687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25903;&#25345;&#20020;&#24202;&#21307;&#29983;&#30340;&#20915;&#31574;&#65292;&#25552;&#20379;&#20102;&#35768;&#22810;&#33258;&#21160;&#20998;&#26512;&#33041;&#37096;MR&#22270;&#20687;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;&#33041;&#32959;&#30244;&#24739;&#32773;&#65292;&#22270;&#20687;&#37319;&#38598;&#26102;&#38388;&#24207;&#21015;&#36890;&#24120;&#22987;&#20110;&#24050;&#32463;&#30149;&#29702;&#24615;&#30340;&#25195;&#25551;&#12290;&#36825;&#20250;&#24102;&#26469;&#38382;&#39064;&#65292;&#22240;&#20026;&#35768;&#22810;&#31639;&#27861;&#26159;&#35774;&#35745;&#29992;&#20110;&#20998;&#26512;&#20581;&#24247;&#30340;&#22823;&#33041;&#22270;&#20687;&#65292;&#24182;&#19988;&#27809;&#26377;&#20026;&#21253;&#21547;&#30149;&#21464;&#30340;&#22270;&#20687;&#25552;&#20379;&#20445;&#35777;&#12290;&#20363;&#22914;&#65292;&#36827;&#34892;&#33041;&#37096;&#35299;&#21078;&#20998;&#21106;&#12289;&#32452;&#32455;&#20998;&#21106;&#21644;&#33041;&#37096;&#25552;&#21462;&#30340;&#31639;&#27861;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BraTS 2023&#20462;&#22797;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#21442;&#19982;&#32773;&#38656;&#35201;&#25506;&#32034;&#20462;&#22797;&#25216;&#26415;&#65292;&#20174;&#26377;&#30149;&#21464;&#30340;&#25195;&#25551;&#20013;&#21512;&#25104;&#20581;&#24247;&#30340;&#33041;&#37096;&#25195;&#25551;&#12290;&#19979;&#38754;&#30340;&#25163;&#31295;&#21253;&#21547;&#20102;&#20219;&#21153;&#20844;&#24335;&#12289;&#25968;&#25454;&#38598;&#21644;&#25552;&#20132;&#31243;&#24207;&#12290;&#20043;&#21518;&#20250;&#26356;&#26032;&#20197;&#24635;&#32467;&#25361;&#25112;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#25361;&#25112;&#26159;&#20316;&#20026;BraTS 2023&#25361;&#25112;&#30340;&#19968;&#37096;&#20998;&#65292;&#30001;&#21152;&#25343;&#22823;&#28201;&#21733;&#21326;MICCAI 2023&#20250;&#35758;&#20027;&#21150;&#12290;
&lt;/p&gt;
&lt;p&gt;
A myriad of algorithms for the automatic analysis of brain MR images is available to support clinicians in their decision-making. For brain tumor patients, the image acquisition time series typically starts with a scan that is already pathological. This poses problems, as many algorithms are designed to analyze healthy brains and provide no guarantees for images featuring lesions. Examples include but are not limited to algorithms for brain anatomy parcellation, tissue segmentation, and brain extraction. To solve this dilemma, we introduce the BraTS 2023 inpainting challenge. Here, the participants' task is to explore inpainting techniques to synthesize healthy brain scans from lesioned ones. The following manuscript contains the task formulation, dataset, and submission procedure. Later it will be updated to summarize the findings of the challenge. The challenge is organized as part of the BraTS 2023 challenge hosted at the MICCAI 2023 conference in Vancouver, Canada.
&lt;/p&gt;</description></item><item><title>FastServe&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#26381;&#21153;&#31995;&#32479;&#65292;&#21033;&#29992;&#25250;&#21344;&#24335;&#35843;&#24230;&#21644;&#36339;&#36807;-&#36830;&#25509;&#22810;&#32423;&#21453;&#39304;&#38431;&#21015;&#65292;&#26368;&#23567;&#21270;&#27169;&#22411;&#25512;&#26029;&#30340;&#20316;&#19994;&#23436;&#25104;&#26102;&#38388;(JCT)&#12290;</title><link>http://arxiv.org/abs/2305.05920</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#20998;&#24067;&#24335;&#25512;&#26029;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Fast Distributed Inference Serving for Large Language Models. (arXiv:2305.05920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05920
&lt;/p&gt;
&lt;p&gt;
FastServe&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#26381;&#21153;&#31995;&#32479;&#65292;&#21033;&#29992;&#25250;&#21344;&#24335;&#35843;&#24230;&#21644;&#36339;&#36807;-&#36830;&#25509;&#22810;&#32423;&#21453;&#39304;&#38431;&#21015;&#65292;&#26368;&#23567;&#21270;&#27169;&#22411;&#25512;&#26029;&#30340;&#20316;&#19994;&#23436;&#25104;&#26102;&#38388;(JCT)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#25512;&#21160;&#20102;&#20197;ChatGPT&#20026;&#20195;&#34920;&#30340;&#26032;&#19968;&#20195;&#20114;&#21160;AI&#24212;&#29992;&#31243;&#24207;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#20132;&#20114;&#24615;&#35201;&#27714;&#27169;&#22411;&#25512;&#26029;&#30340;&#20302;&#20316;&#19994;&#23436;&#25104;&#26102;&#38388;(JCT)&#12290;&#29616;&#26377;&#30340;LLM&#26381;&#21153;&#31995;&#32479;&#20351;&#29992;&#30340;&#26159;&#36816;&#34892;&#21040;&#23436;&#25104;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#23384;&#22312;&#22836;&#37096;&#38459;&#22622;&#21644;&#38271;JCT&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FastServe&#65292;&#19968;&#31181;&#38024;&#23545;LLMs&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#26381;&#21153;&#31995;&#32479;&#12290;FastServe&#21033;&#29992;LLM&#25512;&#29702;&#30340;&#33258;&#22238;&#24402;&#27169;&#24335;&#65292;&#20197;&#27599;&#20010;&#36755;&#20986;&#26631;&#35760;&#30340;&#31890;&#24230;&#23454;&#29616;&#25250;&#21344;&#24335;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#36339;&#36807;-&#36830;&#25509;&#22810;&#32423;&#21453;&#39304;&#38431;&#21015;&#35843;&#24230;&#22120;&#26368;&#23567;&#21270;JCT&#12290;&#22522;&#20110;LLM&#25512;&#29702;&#30340;&#26032;&#21322;&#20449;&#24687;&#19981;&#21487;&#30693;&#35774;&#32622;&#65292;&#35843;&#24230;&#31243;&#24207;&#21033;&#29992;&#36755;&#20837;&#38271;&#24230;&#20449;&#24687;&#26469;&#20026;&#27599;&#20010;&#21040;&#36798;&#20316;&#19994;&#20998;&#37197;&#36866;&#24403;&#30340;&#21021;&#22987;&#38431;&#21015;&#26469;&#36830;&#25509;&#12290;&#39640;&#20110;&#25152;&#36830;&#25509;&#38431;&#21015;&#30340;&#20248;&#20808;&#32423;&#38431;&#21015;&#34987;&#36339;&#36807;&#20197;&#20943;&#23569;&#38477;&#32423;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;GPU&#20869;&#23384;&#31649;&#29702;&#26426;&#21046;&#65292;&#20197;&#25552;&#21069;&#28165;&#38500;&#19981;&#20877;&#20351;&#29992;&#30340;GPU&#32531;&#23384;&#65292;&#24182;&#23545;&#24120;&#29992;&#27169;&#22411;&#36827;&#34892;&#32531;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) power a new generation of interactive AI applications exemplified by ChatGPT. The interactive nature of these applications demand low job completion time (JCT) for model inference. Existing LLM serving systems use run-to-completion processing for inference jobs, which suffers from head-of-line blocking and long JCT. We present FastServe, a distributed inference serving system for LLMs. FastServe exploits the autoregressive pattern of LLM inference to enable preemption at the granularity of each output token. FastServe uses preemptive scheduling to minimize JCT with a novel skip-join Multi-Level Feedback Queue scheduler. Based on the new semi information-agnostic setting of LLM inference, the scheduler leverages the input length information to assign an appropriate initial queue for each arrival job to join. The higher priority queues than the joined queue are skipped to reduce demotions. We design an efficient GPU memory management mechanism that proactivel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#37326;&#22806;&#35270;&#39057;&#20013;&#33258;&#21160;&#25552;&#21462;&#19977;&#32500;&#30417;&#30563;&#26469;&#25193;&#23637;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#25163;&#37096;&#23039;&#21183;&#20316;&#20026;&#29289;&#20307;&#23039;&#21183;&#30340;&#20195;&#29702;&#21644;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#31561;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26410;&#30693;&#30456;&#26426;&#23039;&#21183;&#21644;&#36974;&#25377;&#31561;&#38382;&#39064;&#65292;&#20174;&#32780;&#36890;&#36807;&#20174;&#21333;&#20010;RGB&#22270;&#20687;&#39044;&#27979;&#29289;&#20307;&#19977;&#32500;&#24418;&#29366;&#30340;&#21344;&#25454;&#32593;&#32476;&#24471;&#21040;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03036</link><description>&lt;p&gt;
&#20174;&#37326;&#22806;&#35270;&#39057;&#20013;&#23398;&#20064;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Learning Hand-Held Object Reconstruction from In-The-Wild Videos. (arXiv:2305.03036v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#37326;&#22806;&#35270;&#39057;&#20013;&#33258;&#21160;&#25552;&#21462;&#19977;&#32500;&#30417;&#30563;&#26469;&#25193;&#23637;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#25163;&#37096;&#23039;&#21183;&#20316;&#20026;&#29289;&#20307;&#23039;&#21183;&#30340;&#20195;&#29702;&#21644;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#31561;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26410;&#30693;&#30456;&#26426;&#23039;&#21183;&#21644;&#36974;&#25377;&#31561;&#38382;&#39064;&#65292;&#20174;&#32780;&#36890;&#36807;&#20174;&#21333;&#20010;RGB&#22270;&#20687;&#39044;&#27979;&#29289;&#20307;&#19977;&#32500;&#24418;&#29366;&#30340;&#21344;&#25454;&#32593;&#32476;&#24471;&#21040;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#21333;&#24433;&#20687;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#26041;&#27861;&#20381;&#36182;&#20110;&#38590;&#20197;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#35268;&#27169;&#21270;&#25910;&#38598;&#30340;&#30452;&#25509;3D&#24418;&#29366;&#30417;&#30563;&#65292;&#22240;&#27492;&#36825;&#20123;&#26041;&#27861;&#22312;&#37326;&#22806;&#29615;&#22659;&#19979;&#38754;&#23545;&#26032;&#39062;&#29289;&#20307;&#26102;&#38590;&#20197;&#25512;&#24191;&#12290;&#26412;&#25991;&#20174;&#29983;&#21160;&#30340;&#37326;&#22806;&#21407;&#22987;&#35270;&#39057;&#25968;&#25454;&#20013;&#33258;&#21160;&#25552;&#21462;&#19977;&#32500;&#30417;&#30563;&#65292;&#24182;&#36890;&#36807;&#22810;&#35270;&#35282;&#20108;&#32500;&#30417;&#30563;&#26469;&#25193;&#23637;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#36825;&#38656;&#35201;&#24212;&#23545;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#26410;&#30693;&#30340;&#30456;&#26426;&#23039;&#21183;&#21644;&#36974;&#25377;&#12290;&#23545;&#20110;&#21069;&#32773;&#65292;&#25105;&#20204;&#20351;&#29992;&#25163;&#37096;&#23039;&#21183;&#20316;&#20026;&#29289;&#20307;&#23039;&#21183;&#30340;&#20195;&#29702;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#20351;&#29992;ObMan&#25968;&#25454;&#38598;&#20013;&#21512;&#25104;&#30340;&#29289;&#20307;&#26469;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#38388;&#25509;&#30340;&#19977;&#32500;&#32447;&#32034;&#26469;&#35757;&#32451;&#21344;&#25454;&#32593;&#32476;&#65292;&#20174;&#21333;&#20010;RGB&#22270;&#20687;&#39044;&#27979;&#29289;&#20307;&#30340;&#19977;&#32500;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior works for reconstructing hand-held objects from a single image rely on direct 3D shape supervision which is challenging to gather in real world at scale. Consequently, these approaches do not generalize well when presented with novel objects in in-the-wild settings. While 3D supervision is a major bottleneck, there is an abundance of in-the-wild raw video data showing hand-object interactions. In this paper, we automatically extract 3D supervision (via multiview 2D supervision) from such raw video data to scale up the learning of models for hand-held object reconstruction. This requires tackling two key challenges: unknown camera pose and occlusion. For the former, we use hand pose (predicted from existing techniques, e.g. FrankMocap) as a proxy for object pose. For the latter, we learn data-driven 3D shape priors using synthetic objects from the ObMan dataset. We use these indirect 3D cues to train occupancy networks that predict the 3D shape of objects from a single RGB image. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;MDP&#20013;&#22522;&#20110;&#27010;&#29575;&#20195;&#29702;&#25481;&#32447;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#33021;&#22815;&#28040;&#38500;&#25481;&#32447;&#24773;&#20917;&#38656;&#35201;&#26522;&#20030;&#35745;&#31639;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#35745;&#31639;&#21518;&#25481;&#32447;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.12458</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;MDP&#20013;&#22522;&#20110;&#27010;&#29575;&#20195;&#29702;&#25481;&#32447;&#30340;&#26080;&#27169;&#22411;&#23398;&#20064;&#21644;&#26368;&#20248;&#31574;&#30053;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Model-Free Learning and Optimal Policy Design in Multi-Agent MDPs Under Probabilistic Agent Dropout. (arXiv:2304.12458v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;MDP&#20013;&#22522;&#20110;&#27010;&#29575;&#20195;&#29702;&#25481;&#32447;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#33021;&#22815;&#28040;&#38500;&#25481;&#32447;&#24773;&#20917;&#38656;&#35201;&#26522;&#20030;&#35745;&#31639;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#35745;&#31639;&#21518;&#25481;&#32447;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#35813;&#36807;&#31243;&#21487;&#20197;&#32463;&#21382;&#20195;&#29702;&#25481;&#32447;&#65292;&#24182;&#22522;&#20110;&#23545;&#20110;&#31574;&#30053;&#30340;&#25511;&#21046;&#21644;&#39044;&#20195;&#29702;&#36807;&#31243;&#30340;&#37319;&#26679;&#26469;&#35745;&#31639;&#21518;&#25481;&#32447;&#31995;&#32479;&#30340;&#31574;&#30053;&#12290;&#25511;&#21046;&#22120;&#30340;&#30446;&#26631;&#26159;&#23547;&#25214;&#19968;&#20010;&#26368;&#20248;&#31574;&#30053;&#65292;&#20351;&#24471;&#22312;&#24050;&#30693;&#20195;&#29702;&#25481;&#20986;&#27010;&#29575;&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#26399;&#26395;&#31995;&#32479;&#30340;&#20215;&#20540;&#26368;&#22823;&#21270;&#12290;&#23545;&#20110;&#20219;&#20309;&#29305;&#23450;&#30340;&#25481;&#32447;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#31574;&#30053;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#29305;&#20363;&#12290;&#23545;&#20110;&#20855;&#26377;&#29305;&#23450;&#36716;&#25442;&#29420;&#31435;&#24615;&#21644;&#22870;&#21169;&#21487;&#20998;&#24615;&#32467;&#26500;&#30340;MDPs&#65292;&#25105;&#20204;&#20551;&#35774;&#20174;&#31995;&#32479;&#20013;&#31227;&#38500;&#20195;&#29702;&#32452;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;MDP&#65292;&#30001;&#21097;&#20313;&#20195;&#29702;&#32452;&#25104;&#20855;&#26377;&#26032;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;MDP&#65292;&#36716;&#25442;&#21160;&#24577;&#28040;&#38500;&#24050;&#21024;&#38500;&#30340;&#20195;&#29702;&#65292;&#22870;&#21169;&#19982;&#24050;&#21024;&#38500;&#30340;&#20195;&#29702;&#26080;&#20851;&#12290;&#39318;&#20808;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#20551;&#35774;&#19979;&#65292;&#23545;&#20110;&#39044;&#25481;&#20986;&#31995;&#32479;&#26399;&#26395;&#20540;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#21333;&#19968;&#30340;MDP&#26469;&#34920;&#31034;&#65307;&#36825;&#20010;&#8220;&#40065;&#26834;MDP&#8221;&#33021;&#22815;&#28040;&#38500;&#22312;&#35745;&#31639;&#26368;&#20248;&#31574;&#30053;&#26102;&#35201;&#35780;&#20272;&#25152;&#26377;$2^N$&#31181;&#20195;&#29702;&#25481;&#32447;&#24773;&#20917;&#30340;&#38656;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#21644;&#37325;&#35201;&#24615;&#37319;&#26679;&#26469;&#23398;&#20064;&#40065;&#26834;MDP&#65292;&#20174;&#32780;&#33021;&#22815;&#35745;&#31639;&#21518;&#25481;&#32447;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#20223;&#30495;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies a multi-agent Markov decision process (MDP) that can undergo agent dropout and the computation of policies for the post-dropout system based on control and sampling of the pre-dropout system. The controller's objective is to find an optimal policy that maximizes the value of the expected system given a priori knowledge of the agents' dropout probabilities. Finding an optimal policy for any specific dropout realization is a special case of this problem. For MDPs with a certain transition independence and reward separability structure, we assume that removing agents from the system forms a new MDP comprised of the remaining agents with new state and action spaces, transition dynamics that marginalize the removed agents, and rewards that are independent of the removed agents. We first show that under these assumptions, the value of the expected post-dropout system can be represented by a single MDP; this "robust MDP" eliminates the need to evaluate all $2^N$ realizations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#35821;&#21644;&#22810;&#35821;&#26041;&#27861;&#26469;&#26816;&#27979;&#22312;&#32447;&#26032;&#38395;&#30340;&#31867;&#22411;&#12289;&#26694;&#26550;&#21644;&#35828;&#26381;&#25216;&#24039;&#65292;&#24182;&#21457;&#29616;&#22810;&#35821;&#26041;&#27861;&#27604;&#21333;&#35821;&#26041;&#27861;&#26356;&#22909;&#65292;&#20351;&#29992;&#31867;&#26435;&#37325;&#21644;&#26679;&#26412;&#26435;&#37325;&#30340;&#32452;&#21512;&#23545;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#29992;&#20110;&#24212;&#23545;&#22810;&#25968;&#31867;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#22312;SemEval2023&#20219;&#21153;3&#20013;&#25552;&#20132;&#30340;&#31995;&#32479;&#22312;&#24847;&#22823;&#21033;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65288;&#38646;&#26679;&#26412;&#65289;&#30340;&#23376;&#20219;&#21153;1&#20013;&#25490;&#21517;&#31532;&#20108;&#12290;</title><link>http://arxiv.org/abs/2304.04190</link><description>&lt;p&gt;
QUST&#38431;&#22312;SemEval-2023&#20219;&#21153;3&#20013;&#30340;&#32508;&#21512;&#30740;&#31350;&#65306;&#26816;&#27979;&#22312;&#32447;&#26032;&#38395;&#30340;&#31867;&#22411;&#12289;&#26694;&#26550;&#21644;&#35828;&#26381;&#25216;&#24039;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Team QUST at SemEval-2023 Task 3: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting Online News Genre, Framing and Persuasion Techniques. (arXiv:2304.04190v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#35821;&#21644;&#22810;&#35821;&#26041;&#27861;&#26469;&#26816;&#27979;&#22312;&#32447;&#26032;&#38395;&#30340;&#31867;&#22411;&#12289;&#26694;&#26550;&#21644;&#35828;&#26381;&#25216;&#24039;&#65292;&#24182;&#21457;&#29616;&#22810;&#35821;&#26041;&#27861;&#27604;&#21333;&#35821;&#26041;&#27861;&#26356;&#22909;&#65292;&#20351;&#29992;&#31867;&#26435;&#37325;&#21644;&#26679;&#26412;&#26435;&#37325;&#30340;&#32452;&#21512;&#23545;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#29992;&#20110;&#24212;&#23545;&#22810;&#25968;&#31867;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#22312;SemEval2023&#20219;&#21153;3&#20013;&#25552;&#20132;&#30340;&#31995;&#32479;&#22312;&#24847;&#22823;&#21033;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65288;&#38646;&#26679;&#26412;&#65289;&#30340;&#23376;&#20219;&#21153;1&#20013;&#25490;&#21517;&#31532;&#20108;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;QUST&#22242;&#38431;&#21442;&#21152;SemEval2023&#20219;&#21153;3&#30340;&#24773;&#20917;&#12290;&#39318;&#20808;&#65292;&#21333;&#35821;&#27169;&#22411;&#22312;&#20219;&#21153;&#26089;&#26399;&#23545;&#22810;&#25968;&#31867;&#36827;&#34892;&#20102;&#27424;&#37319;&#26679;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#31867;&#26435;&#37325;&#21644;&#26679;&#26412;&#26435;&#37325;&#30340;&#32452;&#21512;&#23545;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#36827;&#19968;&#27493;&#30740;&#31350;&#20004;&#31181;&#19981;&#21516;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#20998;&#21035;&#20026;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#30456;&#20851;&#30340;&#12290;&#25152;&#26377;&#23454;&#39564;&#37117;&#22312;10&#25240;&#20132;&#21449;&#39564;&#35777;&#19979;&#36827;&#34892;&#65292;&#22810;&#35821;&#26041;&#27861;&#27604;&#21333;&#35821;&#26041;&#27861;&#26356;&#20855;&#20248;&#21183;&#12290;&#25552;&#20132;&#30340;&#31995;&#32479;&#22312;&#24847;&#22823;&#21033;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65288;&#38646;&#26679;&#26412;&#65289;&#30340;&#23376;&#20219;&#21153;1&#20013;&#21462;&#24471;&#20102;&#31532;&#20108;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the participation of team QUST in the SemEval2023 task 3. The monolingual models are first evaluated with the under-sampling of the majority classes in the early stage of the task. Then, the pre-trained multilingual model is fine-tuned with a combination of the class weights and the sample weights. Two different fine-tuning strategies, the task-agnostic and the task-dependent, are further investigated. All experiments are conducted under the 10-fold cross-validation, the multilingual approaches are superior to the monolingual ones. The submitted system achieves the second best in Italian and Spanish (zero-shot) in subtask-1.
&lt;/p&gt;</description></item><item><title>repliclust &#26159;&#19968;&#20010; Python &#21253;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#32858;&#31867;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#25968;&#25454;&#38598;&#30340;&#21407;&#22411;&#65292;&#25552;&#20379;&#20102;&#25918;&#32622;&#38598;&#32676;&#20013;&#24515;&#12289;&#37319;&#26679;&#38598;&#32676;&#24418;&#29366;&#12289;&#36873;&#25321;&#27599;&#20010;&#38598;&#32676;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#20197;&#21450;&#20026;&#38598;&#32676;&#20998;&#37197;&#27010;&#29575;&#20998;&#24067;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.14301</link><description>&lt;p&gt;
repliclust&#65306;&#32858;&#31867;&#20998;&#26512;&#30340;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
repliclust: Synthetic Data for Cluster Analysis. (arXiv:2303.14301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14301
&lt;/p&gt;
&lt;p&gt;
repliclust &#26159;&#19968;&#20010; Python &#21253;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#32858;&#31867;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#25968;&#25454;&#38598;&#30340;&#21407;&#22411;&#65292;&#25552;&#20379;&#20102;&#25918;&#32622;&#38598;&#32676;&#20013;&#24515;&#12289;&#37319;&#26679;&#38598;&#32676;&#24418;&#29366;&#12289;&#36873;&#25321;&#27599;&#20010;&#38598;&#32676;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#20197;&#21450;&#20026;&#38598;&#32676;&#20998;&#37197;&#27010;&#29575;&#20998;&#24067;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; repliclust&#65288;&#26469;&#33258;&#20110; repli-cate &#21644; clust-er&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#32858;&#31867;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340; Python &#21253;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#25968;&#25454;&#38598;&#30340;&#21407;&#22411;&#65292;&#21363;&#39640;&#32423;&#20960;&#20309;&#25551;&#36848;&#65292;&#29992;&#25143;&#21487;&#20197;&#20174;&#20013;&#21019;&#24314;&#35768;&#22810;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20855;&#26377;&#25152;&#38656;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;&#25105;&#20204;&#36719;&#20214;&#30340;&#26550;&#26500;&#26159;&#27169;&#22359;&#21270;&#21644;&#38754;&#21521;&#23545;&#35937;&#30340;&#65292;&#23558;&#25968;&#25454;&#29983;&#25104;&#20998;&#35299;&#25104;&#25918;&#32622;&#38598;&#32676;&#20013;&#24515;&#30340;&#31639;&#27861;&#12289;&#37319;&#26679;&#38598;&#32676;&#24418;&#29366;&#30340;&#31639;&#27861;&#12289;&#36873;&#25321;&#27599;&#20010;&#38598;&#32676;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#30340;&#31639;&#27861;&#20197;&#21450;&#20026;&#38598;&#32676;&#20998;&#37197;&#27010;&#29575;&#20998;&#24067;&#30340;&#31639;&#27861;&#12290;repliclust.org &#39033;&#30446;&#32593;&#39029;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#29992;&#25143;&#25351;&#21335;&#21644;&#20840;&#38754;&#30340;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present repliclust (from repli-cate and clust-er), a Python package for generating synthetic data sets with clusters. Our approach is based on data set archetypes, high-level geometric descriptions from which the user can create many different data sets, each possessing the desired geometric characteristics. The architecture of our software is modular and object-oriented, decomposing data generation into algorithms for placing cluster centers, sampling cluster shapes, selecting the number of data points for each cluster, and assigning probability distributions to clusters. The project webpage, repliclust.org, provides a concise user guide and thorough documentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24352;&#37327;&#20998;&#35299;&#30340;&#30693;&#35782;&#22270;&#32534;&#30721;&#22120;&#65292;&#23558;&#37051;&#23621;&#23454;&#20307;&#20351;&#29992;&#30001;&#20851;&#31995;&#31867;&#22411;&#23450;&#20041;&#30340;&#20302;&#31209;&#24352;&#37327;&#30340;&#25237;&#24433;&#30697;&#38453;&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#20135;&#29983;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#20851;&#31995;&#24863;&#30693;&#24615;&#30340;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.05581</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#39640;&#25928;&#30340;&#20851;&#31995;&#24863;&#30693;&#37051;&#22495;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Efficient Relation-aware Neighborhood Aggregation in Graph Neural Networks via Tensor Decomposition. (arXiv:2212.05581v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24352;&#37327;&#20998;&#35299;&#30340;&#30693;&#35782;&#22270;&#32534;&#30721;&#22120;&#65292;&#23558;&#37051;&#23621;&#23454;&#20307;&#20351;&#29992;&#30001;&#20851;&#31995;&#31867;&#22411;&#23450;&#20041;&#30340;&#20302;&#31209;&#24352;&#37327;&#30340;&#25237;&#24433;&#30697;&#38453;&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#20135;&#29983;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#20851;&#31995;&#24863;&#30693;&#24615;&#30340;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#38754;&#21521;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#36825;&#31181;&#26041;&#27861;&#24573;&#30053;&#20102;&#20851;&#31995;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#23558;&#20854;&#19982;&#23454;&#20307;&#20449;&#24687;&#32452;&#21512;&#20351;&#29992;&#25928;&#29575;&#20302;&#19979;&#65292;&#23548;&#33268;&#34920;&#36798;&#33021;&#21147;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#20851;&#31995;&#22270;&#21367;&#31215;&#32593;&#32476;(R-GCN)&#30340;&#32858;&#21512;&#20989;&#25968;&#20013;&#24341;&#20837;&#20102;&#24352;&#37327;&#20998;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#30693;&#35782;&#22270;&#32534;&#30721;&#22120;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#30001;&#20851;&#31995;&#31867;&#22411;&#23450;&#20041;&#30340;&#20302;&#31209;&#24352;&#37327;&#30340;&#25237;&#24433;&#30697;&#38453;&#65292;&#23558;&#37051;&#23621;&#23454;&#20307;&#36827;&#34892;&#36716;&#25442;&#65292;&#20197;&#33719;&#24471;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#22909;&#22788;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#30340;&#20851;&#31995;&#24863;&#30693;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;CP&#20998;&#35299;&#26469;&#20272;&#35745;&#26680;&#24515;&#24352;&#37327;&#30340;&#20302;&#31209;&#20272;&#35745;&#65292;&#20174;&#32780;&#21387;&#32553;&#21644;&#35268;&#33539;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#22522;&#20110;1-N&#26041;&#27861;&#22312;&#22823;&#22411;&#22270;&#19978;&#30340;&#35757;&#32451;&#38480;&#21046;&#12290;&#25105;&#20204;&#20351;&#29992;&#20302;&#32500;&#24230;&#23884;&#20837;&#65292;&#22312;FB15k-237&#21644;WN18RR&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many Graph Neural Networks (GNNs) are proposed for Knowledge Graph Embedding (KGE). However, lots of these methods neglect the importance of the information of relations and combine it with the information of entities inefficiently, leading to low expressiveness. To address this issue, we introduce a general knowledge graph encoder incorporating tensor decomposition in the aggregation function of Relational Graph Convolutional Network (R-GCN). In our model, neighbor entities are transformed using projection matrices of a low-rank tensor which are defined by relation types to benefit from multi-task learning and produce expressive relation-aware representations. Besides, we propose a low-rank estimation of the core tensor using CP decomposition to compress and regularize our model. We use a training method inspired by contrastive learning, which relieves the training limitation of the 1-N method on huge graphs. We achieve favorably competitive results on FB15k-237 and WN18RR with embedd
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;&#65288;CURO&#65289;&#30340;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270; (RO) &#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#21512;&#20316;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.02733</link><description>&lt;p&gt;
&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Curriculum Learning for Relative Overgeneralization. (arXiv:2212.02733v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;&#65288;CURO&#65289;&#30340;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270; (RO) &#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#21512;&#20316;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064; (MARL) &#20013;&#65292;&#35768;&#22810;&#27969;&#34892;&#26041;&#27861;&#22914; VDN &#21644; QMIX&#65292;&#37117;&#23481;&#26131;&#21463;&#21040;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270; (RO) &#36825;&#19968;&#20851;&#38190;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#30149;&#29702;&#30340;&#24433;&#21709;&#12290;&#24403;&#21512;&#20316;&#20219;&#21153;&#20013;&#26368;&#20339;&#32852;&#21512;&#34892;&#21160;&#30340;&#25928;&#29992;&#20302;&#20110;&#27425;&#20248;&#32852;&#21512;&#34892;&#21160;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;RO&#12290;RO&#21487;&#33021;&#23548;&#33268;&#26234;&#33021;&#20307;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#25110;&#26080;&#27861;&#35299;&#20915;&#38656;&#35201;&#26234;&#33021;&#20307;&#20043;&#38388;&#22312;&#32473;&#23450;&#26102;&#38388;&#27493;&#38271;&#20869;&#36827;&#34892;&#22823;&#37327;&#21327;&#35843;&#30340;&#21512;&#20316;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#20215;&#20540;&#30340;MARL&#31639;&#27861;&#65292;&#22914;QPLEX&#21644;WQMIX&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20811;&#26381;RO&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#20204;&#20173;&#28982;&#26080;&#27861;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#21512;&#20316;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;&#65288;CURO&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#20811;&#26381;RO&#12290;&#22312;CURO&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#30340;&#22870;&#21169;&#20989;&#25968;&#20197;&#29983;&#25104;&#36866;&#21512;&#24403;&#21069;&#33021;&#21147;&#30340;&#28304;&#20219;&#21153;&#26469;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#30446;&#26631;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-agent reinforcement learning (MARL), many popular methods, such as VDN and QMIX, are susceptible to a critical multi-agent pathology known as relative overgeneralization (RO), which arises when the optimal joint action's utility falls below that of a sub-optimal joint action in cooperative tasks. RO can cause the agents to get stuck into local optima or fail to solve cooperative tasks that require significant coordination between agents within a given timestep. Recent value-based MARL algorithms such as QPLEX and WQMIX can overcome RO to some extent. However, our experimental results show that they can still fail to solve cooperative tasks that exhibit strong RO. In this work, we propose a novel approach called curriculum learning for relative overgeneralization (CURO) to better overcome RO. To solve a target task that exhibits strong RO, in CURO, we first fine-tune the reward function of the target task to generate source tasks that are tailored to the current ability of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;D3G&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#36712;&#36857;&#19982;&#28436;&#31034;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#27599;&#20010;&#26426;&#22120;&#20154;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#20854;&#20010;&#20307;&#21160;&#24577;&#21644;&#30446;&#26631;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.08892</link><description>&lt;p&gt;
D3G: &#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
D3G: Learning Multi-robot Coordination from Demonstrations. (arXiv:2207.08892v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;D3G&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#36712;&#36857;&#19982;&#28436;&#31034;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#27599;&#20010;&#26426;&#22120;&#20154;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#20854;&#20010;&#20307;&#21160;&#24577;&#21644;&#30446;&#26631;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#21487;&#24494;&#21160;&#24577;&#28216;&#25103;&#65288;D3G&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#12290;&#25105;&#20204;&#23558;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#34920;&#31034;&#20026;&#19968;&#20010;&#21160;&#24577;&#28216;&#25103;&#65292;&#20854;&#20013;&#19968;&#20010;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#21463;&#20854;&#33258;&#36523;&#21160;&#24577;&#21644;&#30446;&#26631;&#30340;&#25511;&#21046;&#65292;&#21516;&#26102;&#20063;&#21462;&#20915;&#20110;&#20854;&#20182;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#35843;&#25972;&#27599;&#20010;&#26426;&#22120;&#20154;&#30340;&#30446;&#26631;&#21644;&#21160;&#24577;&#65292;&#21487;&#20197;&#36866;&#24212;&#21327;&#35843;&#12290;&#25152;&#25552;&#20986;&#30340;D3G&#20351;&#27599;&#20010;&#26426;&#22120;&#20154;&#36890;&#36807;&#26368;&#23567;&#21270;&#20854;&#36712;&#36857;&#19982;&#28436;&#31034;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#22312;&#20998;&#24067;&#24335;&#26041;&#24335;&#19979;&#33258;&#21160;&#35843;&#25972;&#20854;&#20010;&#20307;&#21160;&#24577;&#21644;&#30446;&#26631;&#12290;&#35813;&#23398;&#20064;&#26694;&#26550;&#20855;&#26377;&#26032;&#30340;&#35774;&#35745;&#65292;&#21253;&#25324;&#19968;&#20010;&#21069;&#21521;&#20256;&#36882;&#65292;&#25152;&#26377;&#26426;&#22120;&#20154;&#21512;&#20316;&#23547;&#25214;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#20197;&#21450;&#19968;&#20010;&#21453;&#21521;&#20256;&#36882;&#65292;&#22312;&#36890;&#20449;&#22270;&#20013;&#20256;&#25773;&#26799;&#24230;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#20013;&#27979;&#35797;&#20102;D3G&#65292;&#24182;&#32473;&#20986;&#20102;&#19981;&#21516;&#20219;&#21153;&#37197;&#32622;&#30340;&#20004;&#31181;&#26426;&#22120;&#20154;&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;D3G&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper develops a Distributed Differentiable Dynamic Game (D3G) framework, which enables learning multi-robot coordination from demonstrations. We represent multi-robot coordination as a dynamic game, where the behavior of a robot is dictated by its own dynamics and objective that also depends on others' behavior. The coordination thus can be adapted by tuning the objective and dynamics of each robot. The proposed D3G enables each robot to automatically tune its individual dynamics and objectives in a distributed manner by minimizing the mismatch between its trajectory and demonstrations. This learning framework features a new design, including a forward-pass, where all robots collaboratively seek Nash equilibrium of a game, and a backward-pass, where gradients are propagated via the communication graph. We test the D3G in simulation with two types of robots given different task configurations. The results validate the capability of D3G for learning multi-robot coordination from de
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphMLP&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#23427;&#23558;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2206.06420</link><description>&lt;p&gt;
GraphMLP&#65306;&#19968;&#31181;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#22270;&#24418;MLP&#24335;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
GraphMLP: A Graph MLP-Like Architecture for 3D Human Pose Estimation. (arXiv:2206.06420v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06420
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphMLP&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#23427;&#23558;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#27809;&#26377;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#30340;&#31454;&#20105;&#24615;&#32467;&#26524;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MLP&#27169;&#22411;&#24182;&#19981;&#25797;&#38271;&#25429;&#25417;&#23616;&#37096;&#32454;&#33410;&#65292;&#20063;&#32570;&#20047;&#26377;&#20851;&#20154;&#20307;&#26500;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#39592;&#39612;&#34920;&#31034;&#23398;&#20064;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#31216;&#20026;GraphMLP&#65292;&#23427;&#32467;&#21512;&#20102;MLP&#21644;&#22270;&#24418;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#20840;&#23616;-&#23616;&#37096;-&#22270;&#24418;&#32479;&#19968;&#26550;&#26500;&#20013;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#12290;GraphMLP&#23558;&#20154;&#20307;&#30340;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#23637;&#31034;&#20102;&#21487;&#20197;&#20197;&#21487;&#24573;&#30053;&#30340;&#35745;&#31639;&#20195;&#20215;&#26469;&#26377;&#25928;&#22320;&#24314;&#27169;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern multi-layer perceptron (MLP) models have shown competitive results in learning visual representations without self-attention. However, existing MLP models are not good at capturing local details and lack prior knowledge of human body configurations, which limits their modeling power for skeletal representation learning. To address these issues, we propose a simple yet effective graph-reinforced MLP-Like architecture, named GraphMLP, that combines MLPs and graph convolutional networks (GCNs) in a global-local-graphical unified architecture for 3D human pose estimation. GraphMLP incorporates the graph structure of human bodies into an MLP model to meet the domain-specific demand of the 3D human pose, while allowing for both local and global spatial interactions. Furthermore, we propose to flexibly and efficiently extend the GraphMLP to the video domain and show that complex temporal dynamics can be effectively modeled in a simple way with negligible computational cost gains in the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#24207;&#21015;&#23454;&#39564;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#65292;&#20351;&#29992;&#38750;&#21442;&#25968;&#26041;&#27861;&#23545;&#21453;&#20107;&#23454;&#22343;&#20540;&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#24314;&#31435;&#20102;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2202.06891</link><description>&lt;p&gt;
&#24207;&#21015;&#23454;&#39564;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Counterfactual inference for sequential experiments. (arXiv:2202.06891v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24207;&#21015;&#23454;&#39564;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#65292;&#20351;&#29992;&#38750;&#21442;&#25968;&#26041;&#27861;&#23545;&#21453;&#20107;&#23454;&#22343;&#20540;&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#24314;&#31435;&#20102;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#38024;&#23545;&#36830;&#32493;&#35774;&#35745;&#23454;&#39564;&#36827;&#34892;&#30340;&#20107;&#21518;&#32479;&#35745;&#25512;&#26029;&#65292;&#22312;&#27492;&#23454;&#39564;&#20013;&#65292;&#22810;&#20010;&#21333;&#20301;&#22312;&#22810;&#20010;&#26102;&#38388;&#28857;&#19978;&#20998;&#37197;&#27835;&#30103;&#65292;&#24182;&#20351;&#29992;&#38543;&#26102;&#38388;&#32780;&#36866;&#24212;&#30340;&#27835;&#30103;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#23545;&#36866;&#24212;&#24615;&#27835;&#30103;&#31574;&#30053;&#20570;&#20986;&#26368;&#23569;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#26368;&#23567;&#21487;&#33021;&#35268;&#27169;&#30340;&#21453;&#20107;&#23454;&#22343;&#20540;&#25552;&#20379;&#25512;&#26029;&#20445;&#35777;&#65292;&#21363;&#22312;&#27599;&#20010;&#21333;&#20301;&#21644;&#27599;&#20010;&#26102;&#38388;&#19979;&#65292;&#38024;&#23545;&#19981;&#21516;&#27835;&#30103;&#30340;&#24179;&#22343;&#32467;&#26524;&#12290;&#22312;&#27809;&#26377;&#23545;&#21453;&#20107;&#23454;&#22343;&#20540;&#36827;&#34892;&#20219;&#20309;&#32467;&#26500;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#26410;&#30693;&#21464;&#37327;&#27604;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#28857;&#36824;&#22810;&#12290;&#20026;&#20102;&#21462;&#24471;&#36827;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#29992;&#20110;&#21453;&#20107;&#23454;&#22343;&#20540;&#19978;&#65292;&#35813;&#27169;&#22411;&#20316;&#20026;&#38750;&#21442;&#25968;&#24418;&#24335;&#30340;&#38750;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#21644;&#20197;&#21069;&#24037;&#20316;&#20013;&#32771;&#34385;&#30340;&#21452;&#32447;&#24615;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#20351;&#29992;&#38750;&#21442;&#25968;&#26041;&#27861;&#36827;&#34892;&#20272;&#35745;&#65292;&#21363;&#26368;&#36817;&#37051;&#30340;&#21464;&#20307;&#65292;&#24182;&#20026;&#27599;&#20010;&#21333;&#20301;&#21644;&#27599;&#20010;&#26102;&#38388;&#30340;&#21453;&#20107;&#23454;&#22343;&#20540;&#24314;&#31435;&#20102;&#38750;&#28176;&#36827;&#39640;&#27010;&#29575;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider after-study statistical inference for sequentially designed experiments wherein multiple units are assigned treatments for multiple time points using treatment policies that adapt over time. Our goal is to provide inference guarantees for the counterfactual mean at the smallest possible scale -- mean outcome under different treatments for each unit and each time -- with minimal assumptions on the adaptive treatment policy. Without any structural assumptions on the counterfactual means, this challenging task is infeasible due to more unknowns than observed data points. To make progress, we introduce a latent factor model over the counterfactual means that serves as a non-parametric generalization of the non-linear mixed effects model and the bilinear latent factor model considered in prior works. For estimation, we use a non-parametric method, namely a variant of nearest neighbors, and establish a non-asymptotic high probability error bound for the counterfactual mean for ea
&lt;/p&gt;</description></item></channel></rss>