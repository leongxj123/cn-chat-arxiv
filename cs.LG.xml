<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>BAdam&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#20840;&#21442;&#25968;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#25910;&#25947;&#34892;&#20026;&#20197;&#21450;&#22312;&#24615;&#33021;&#35780;&#20272;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2404.02827</link><description>&lt;p&gt;
BAdam&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#39640;&#25928;&#20840;&#21442;&#25968;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02827
&lt;/p&gt;
&lt;p&gt;
BAdam&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#20840;&#21442;&#25968;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#25910;&#25947;&#34892;&#20026;&#20197;&#21450;&#22312;&#24615;&#33021;&#35780;&#20272;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;BAdam&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;Adam&#20316;&#20026;&#20869;&#37096;&#27714;&#35299;&#22120;&#30340;&#22359;&#22352;&#26631;&#20248;&#21270;&#26694;&#26550;&#30340;&#20248;&#21270;&#22120;&#12290;BAdam&#25552;&#20379;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#21442;&#25968;&#24494;&#35843;&#65292;&#24182;&#19988;&#30001;&#20110;&#38142;&#24335;&#35268;&#21017;&#23646;&#24615;&#20943;&#23569;&#20102;&#21453;&#21521;&#36807;&#31243;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;BAdam&#24212;&#29992;&#20110;&#22312;Alpaca-GPT4&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#21333;&#20010;RTX3090-24GB GPU&#36827;&#34892;&#25351;&#23548;&#24494;&#35843;&#30340;Llama 2-7B&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;LoRA&#21644;LOMO&#30456;&#27604;&#65292;BAdam&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;MT-bench&#23545;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;&#19979;&#28216;&#24615;&#33021;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;BAdam&#22312;&#36866;&#24230;&#36229;&#36234;LoRA&#30340;&#22522;&#30784;&#19978;&#26356;&#26174;&#33879;&#22320;&#20248;&#20110;LOMO&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;BAdam&#19982;Adam&#22312;&#20013;&#31561;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21363;&#22312;SuperGLUE&#22522;&#20934;&#19978;&#23545;RoBERTa-large&#36827;&#34892;&#24494;&#35843;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;BAdam&#33021;&#22815;&#32553;&#23567;&#19982;Adam&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02827v1 Announce Type: new  Abstract: This work presents BAdam, an optimizer that leverages the block coordinate optimization framework with Adam as the inner solver. BAdam offers a memory efficient approach to the full parameter finetuning of large language models and reduces running time of the backward process thanks to the chain rule property. Experimentally, we apply BAdam to instruction-tune the Llama 2-7B model on the Alpaca-GPT4 dataset using a single RTX3090-24GB GPU. The results indicate that BAdam exhibits superior convergence behavior in comparison to LoRA and LOMO. Furthermore, our downstream performance evaluation of the instruction-tuned models using the MT-bench shows that BAdam modestly surpasses LoRA and more substantially outperforms LOMO. Finally, we compare BAdam with Adam on a medium-sized task, i.e., finetuning RoBERTa-large on the SuperGLUE benchmark. The results demonstrate that BAdam is capable of narrowing the performance gap with Adam. Our code is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;NN TSC&#65289;&#39044;&#27979;&#20891;&#20107;&#32972;&#26223;&#19979;&#32676;&#20307;&#33258;&#20027;&#20307;&#30340;&#20851;&#38190;&#23646;&#24615;&#21644;&#25112;&#26415;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;NN TSC&#22312;&#24555;&#36895;&#25512;&#26029;&#25915;&#20987;&#32676;&#20307;&#24773;&#25253;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19572</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#32676;&#20307;&#29305;&#24615;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Swarm Characteristics Classification Using Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;NN TSC&#65289;&#39044;&#27979;&#20891;&#20107;&#32972;&#26223;&#19979;&#32676;&#20307;&#33258;&#20027;&#20307;&#30340;&#20851;&#38190;&#23646;&#24615;&#21644;&#25112;&#26415;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;NN TSC&#22312;&#24555;&#36895;&#25512;&#26029;&#25915;&#20987;&#32676;&#20307;&#24773;&#25253;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#32676;&#20307;&#33258;&#20027;&#20307;&#30340;&#29305;&#24615;&#23545;&#20110;&#22269;&#38450;&#21644;&#23433;&#20840;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;NN TSC&#65289;&#26469;&#39044;&#27979;&#20891;&#20107;&#29615;&#22659;&#20013;&#32676;&#20307;&#33258;&#20027;&#20307;&#30340;&#20851;&#38190;&#23646;&#24615;&#21644;&#25112;&#26415;&#30340;&#30740;&#31350;&#12290;&#20855;&#20307;&#22320;&#65292;NN TSC&#34987;&#24212;&#29992;&#20110;&#25512;&#26029;&#20004;&#20010;&#20108;&#36827;&#21046;&#23646;&#24615; - &#36890;&#20449;&#21644;&#27604;&#20363;&#23548;&#33322; - &#36825;&#20004;&#32773;&#32467;&#21512;&#23450;&#20041;&#20102;&#22235;&#31181;&#20114;&#26021;&#30340;&#32676;&#20307;&#25112;&#26415;&#12290;&#25105;&#20204;&#21457;&#29616;&#25991;&#29486;&#20013;&#23545;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32676;&#20307;&#20998;&#31867;&#23384;&#22312;&#19968;&#23450;&#30340;&#31354;&#30333;&#65292;&#24182;&#23637;&#31034;&#20102;NN TSC&#22312;&#24555;&#36895;&#25512;&#26029;&#26377;&#20851;&#25915;&#20987;&#32676;&#20307;&#24773;&#25253;&#20197;&#25351;&#23548;&#21453;&#21046;&#21160;&#20316;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#27169;&#25311;&#30340;&#32676;&#20307;&#23545;&#25112;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;NN TSC&#22312;&#35266;&#23519;&#31383;&#21475;&#35201;&#27714;&#12289;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#23545;&#32676;&#20307;&#35268;&#27169;&#30340;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#20851;&#38190;&#21457;&#29616;&#26174;&#31034;NN&#33021;&#22815;&#20351;&#29992;&#36739;&#30701;&#30340;&#35266;&#23519;&#31383;&#21475;&#20197;97%&#30340;&#20934;&#30830;&#29575;&#39044;&#27979;&#32676;&#20307;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19572v1 Announce Type: new  Abstract: Understanding the characteristics of swarming autonomous agents is critical for defense and security applications. This article presents a study on using supervised neural network time series classification (NN TSC) to predict key attributes and tactics of swarming autonomous agents for military contexts. Specifically, NN TSC is applied to infer two binary attributes - communication and proportional navigation - which combine to define four mutually exclusive swarm tactics. We identify a gap in literature on using NNs for swarm classification and demonstrate the effectiveness of NN TSC in rapidly deducing intelligence about attacking swarms to inform counter-maneuvers. Through simulated swarm-vs-swarm engagements, we evaluate NN TSC performance in terms of observation window requirements, noise robustness, and scalability to swarm size. Key findings show NNs can predict swarm behaviors with 97% accuracy using short observation windows of
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#26159;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#30340;&#26368;&#26032;&#21457;&#23637;</title><link>https://arxiv.org/abs/2403.17561</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21450;&#20854;&#26368;&#26032;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning and State-of-the-arts Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17561
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#30340;&#26368;&#26032;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;, &#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#23618;&#20114;&#36830;&#21333;&#20803;&#65288;&#31070;&#32463;&#20803;&#65289;&#20174;&#21407;&#22987;&#36755;&#20837;&#25968;&#25454;&#20013;&#30452;&#25509;&#23398;&#20064;&#22797;&#26434;&#27169;&#24335;&#21644;&#34920;&#31034;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#21463;&#21040;&#36825;&#31181;&#23398;&#20064;&#33021;&#21147;&#30340;&#36171;&#33021;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26159;&#35768;&#22810;&#31361;&#30772;&#24615;&#25216;&#26415;&#21644;&#21019;&#26032;&#30340;&#26680;&#24515;&#39537;&#21160;&#21147;&#12290;&#26500;&#24314;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#29616;&#23454;&#38382;&#39064;&#30340;&#21160;&#24577;&#24615;&#12290;&#26377;&#20960;&#39033;&#30740;&#31350;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#27010;&#24565;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31867;&#22411;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26368;&#26032;&#21457;&#23637;&#30340;&#35206;&#30422;&#38754;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#21463;&#21040;&#36825;&#20123;&#38480;&#21046;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#23457;&#35270;th
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17561v1 Announce Type: new  Abstract: Deep learning, a branch of artificial intelligence, is a computational model that uses multiple layers of interconnected units (neurons) to learn intricate patterns and representations directly from raw input data. Empowered by this learning capability, it has become a powerful tool for solving complex problems and is the core driver of many groundbreaking technologies and innovations. Building a deep learning model is a challenging task due to the algorithm`s complexity and the dynamic nature of real-world problems. Several studies have reviewed deep learning concepts and applications. However, the studies mostly focused on the types of deep learning models and convolutional neural network architectures, offering limited coverage of the state-of-the-art of deep learning models and their applications in solving complex problems across different domains. Therefore, motivated by the limitations, this study aims to comprehensively review th
&lt;/p&gt;</description></item><item><title>ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09871</link><description>&lt;p&gt;
ThermoHands&#65306;&#19968;&#31181;&#29992;&#20110;&#20174;&#20027;&#35266;&#35270;&#35282;&#28909;&#22270;&#20013;&#20272;&#35745;3D&#25163;&#37096;&#23039;&#21183;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09871
&lt;/p&gt;
&lt;p&gt;
ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ThermoHands&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#22522;&#20110;&#28909;&#22270;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#20811;&#26381;&#35832;&#22914;&#20809;&#29031;&#21464;&#21270;&#21644;&#36974;&#25377;&#65288;&#20363;&#22914;&#25163;&#37096;&#31359;&#25140;&#29289;&#65289;&#31561;&#25361;&#25112;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;28&#21517;&#20027;&#20307;&#36827;&#34892;&#25163;-&#29289;&#20307;&#21644;&#25163;-&#34394;&#25311;&#20132;&#20114;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#65292;&#32463;&#36807;&#33258;&#21160;&#21270;&#36807;&#31243;&#20934;&#30830;&#26631;&#27880;&#20102;3D&#25163;&#37096;&#23039;&#21183;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#21033;&#29992;&#21452;transformer&#27169;&#22359;&#22312;&#28909;&#22270;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20102;TheFormer&#30340;&#39046;&#20808;&#24615;&#33021;&#65292;&#24182;&#30830;&#35748;&#20102;&#28909;&#25104;&#20687;&#22312;&#23454;&#29616;&#24694;&#21155;&#26465;&#20214;&#19979;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09871v1 Announce Type: cross  Abstract: In this work, we present ThermoHands, a new benchmark for thermal image-based egocentric 3D hand pose estimation, aimed at overcoming challenges like varying lighting and obstructions (e.g., handwear). The benchmark includes a diverse dataset from 28 subjects performing hand-object and hand-virtual interactions, accurately annotated with 3D hand poses through an automated process. We introduce a bespoken baseline method, TheFormer, utilizing dual transformer modules for effective egocentric 3D hand pose estimation in thermal imagery. Our experimental results highlight TheFormer's leading performance and affirm thermal imaging's effectiveness in enabling robust 3D hand pose estimation in adverse conditions.
&lt;/p&gt;</description></item><item><title>DEEP-IoT&#36890;&#36807;&#8220;&#26356;&#22810;&#30417;&#21548;&#65292;&#26356;&#23569;&#20256;&#36755;&#8221;&#30340;&#31574;&#30053;&#65292;&#25361;&#25112;&#21644;&#36716;&#21464;&#20102;&#20256;&#32479;&#30340;&#29289;&#32852;&#32593;&#36890;&#20449;&#27169;&#22411;&#65292;&#22823;&#24133;&#38477;&#20302;&#33021;&#32791;&#24182;&#25552;&#39640;&#35774;&#22791;&#23551;&#21629;&#12290;</title><link>https://arxiv.org/abs/2403.00321</link><description>&lt;p&gt;
DEEP-IoT: &#19979;&#34892;&#22686;&#24378;&#22411;&#39640;&#25928;&#33021;&#29289;&#32852;&#32593;
&lt;/p&gt;
&lt;p&gt;
DEEP-IoT: Downlink-Enhanced Efficient-Power Internet of Things
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00321
&lt;/p&gt;
&lt;p&gt;
DEEP-IoT&#36890;&#36807;&#8220;&#26356;&#22810;&#30417;&#21548;&#65292;&#26356;&#23569;&#20256;&#36755;&#8221;&#30340;&#31574;&#30053;&#65292;&#25361;&#25112;&#21644;&#36716;&#21464;&#20102;&#20256;&#32479;&#30340;&#29289;&#32852;&#32593;&#36890;&#20449;&#27169;&#22411;&#65292;&#22823;&#24133;&#38477;&#20302;&#33021;&#32791;&#24182;&#25552;&#39640;&#35774;&#22791;&#23551;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DEEP-IoT&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#38761;&#21629;&#24847;&#20041;&#30340;&#36890;&#20449;&#33539;&#20363;&#65292;&#26088;&#22312;&#37325;&#26032;&#23450;&#20041;&#29289;&#32852;&#32593;&#35774;&#22791;&#20043;&#38388;&#30340;&#36890;&#20449;&#26041;&#24335;&#12290;&#36890;&#36807;&#24320;&#21019;&#24615;&#30340;&#8220;&#26356;&#22810;&#30417;&#21548;&#65292;&#26356;&#23569;&#20256;&#36755;&#8221;&#30340;&#31574;&#30053;&#65292;DEEP-IoT&#25361;&#25112;&#21644;&#36716;&#21464;&#20102;&#20256;&#32479;&#30340;&#21457;&#36865;&#26041;&#65288;&#29289;&#32852;&#32593;&#35774;&#22791;&#65289;&#20026;&#20013;&#24515;&#30340;&#36890;&#20449;&#27169;&#22411;&#65292;&#23558;&#25509;&#25910;&#26041;&#65288;&#25509;&#20837;&#28857;&#65289;&#20316;&#20026;&#20851;&#38190;&#35282;&#33394;&#65292;&#20174;&#32780;&#38477;&#20302;&#33021;&#32791;&#24182;&#24310;&#38271;&#35774;&#22791;&#23551;&#21629;&#12290;&#25105;&#20204;&#19981;&#20165;&#27010;&#24565;&#21270;&#20102;DEEP-IoT&#65292;&#36824;&#36890;&#36807;&#22312;&#31364;&#24102;&#31995;&#32479;&#20013;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#30340;&#21453;&#39304;&#20449;&#36947;&#32534;&#30721;&#26469;&#23454;&#29616;&#23427;&#12290;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#65292;IoT&#21333;&#20803;&#30340;&#36816;&#34892;&#23551;&#21629;&#26174;&#33879;&#25552;&#39640;&#65292;&#27604;&#20351;&#29992;Turbo&#21644;Polar&#32534;&#30721;&#30340;&#20256;&#32479;&#31995;&#32479;&#25552;&#39640;&#20102;&#26368;&#22810;52.71%&#12290;&#36825;&#19968;&#36827;&#23637;&#26631;&#24535;&#30528;&#19968;&#31181;&#21464;&#38761;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00321v1 Announce Type: cross  Abstract: At the heart of the Internet of Things (IoT) -- a domain witnessing explosive growth -- the imperative for energy efficiency and the extension of device lifespans has never been more pressing. This paper presents DEEP-IoT, a revolutionary communication paradigm poised to redefine how IoT devices communicate. Through a pioneering "listen more, transmit less" strategy, DEEP-IoT challenges and transforms the traditional transmitter (IoT devices)-centric communication model to one where the receiver (the access point) play a pivotal role, thereby cutting down energy use and boosting device longevity. We not only conceptualize DEEP-IoT but also actualize it by integrating deep learning-enhanced feedback channel codes within a narrow-band system. Simulation results show a significant enhancement in the operational lifespan of IoT cells -- surpassing traditional systems using Turbo and Polar codes by up to 52.71%. This leap signifies a paradi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#30340;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#20272;&#35745;&#21327;&#26041;&#24046;&#32467;&#26500;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#20195;&#29702;&#26041;&#24046;&#30340;&#31639;&#27861;&#33719;&#24471;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#29305;&#21035;&#22312;&#21327;&#26041;&#24046;&#31995;&#25968;&#20840;&#20026;&#38750;&#36127;&#26102;&#65292;&#33021;&#26377;&#25928;&#22320;&#21033;&#29992;&#21322;&#33218;&#21453;&#39304;&#65292;&#24182;&#22312;&#21508;&#31181;&#21442;&#25968;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.15171</link><description>&lt;p&gt;
&#29992;&#20110;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#32769;&#34382;&#26426;&#30340;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Covariance-Adaptive Least-Squares Algorithm for Stochastic Combinatorial Semi-Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15171
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#30340;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#20272;&#35745;&#21327;&#26041;&#24046;&#32467;&#26500;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#20195;&#29702;&#26041;&#24046;&#30340;&#31639;&#27861;&#33719;&#24471;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#29305;&#21035;&#22312;&#21327;&#26041;&#24046;&#31995;&#25968;&#20840;&#20026;&#38750;&#36127;&#26102;&#65292;&#33021;&#26377;&#25928;&#22320;&#21033;&#29992;&#21322;&#33218;&#21453;&#39304;&#65292;&#24182;&#22312;&#21508;&#31181;&#21442;&#25968;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#29609;&#23478;&#21487;&#20197;&#20174;&#21253;&#21547;d&#20010;&#22522;&#26412;&#39033;&#30340;P&#20010;&#23376;&#38598;&#20013;&#36827;&#34892;&#36873;&#25321;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#65288;&#22914;CUCB&#12289;ESCB&#12289;OLS-UCB&#65289;&#38656;&#35201;&#23545;&#22870;&#21169;&#20998;&#24067;&#26377;&#20808;&#39564;&#30693;&#35782;&#65292;&#27604;&#22914;&#23376;&#39640;&#26031;&#20195;&#29702;-&#26041;&#24046;&#30340;&#19978;&#30028;&#65292;&#36825;&#24456;&#38590;&#20934;&#30830;&#20272;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;OLS-UCB&#30340;&#26041;&#24046;&#33258;&#36866;&#24212;&#29256;&#26412;&#65292;&#20381;&#36182;&#20110;&#21327;&#26041;&#24046;&#32467;&#26500;&#30340;&#22312;&#32447;&#20272;&#35745;&#12290;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#65292;&#20272;&#35745;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#31995;&#25968;&#35201;&#23481;&#26131;&#24471;&#22810;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#22522;&#20110;&#20195;&#29702;&#26041;&#24046;&#30340;&#31639;&#27861;&#65292;&#23548;&#33268;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#24403;&#21327;&#26041;&#24046;&#31995;&#25968;&#20840;&#20026;&#38750;&#36127;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#21322;&#33218;&#21453;&#39304;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#32769;&#34382;&#26426;&#21453;&#39304;&#26041;&#27861;&#65292;&#22312;&#25351;&#25968;&#32423;&#21035;P&#8811;d&#20197;&#21450;P&#8804;d&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#19968;&#28857;&#24182;&#19981;&#26469;&#33258;&#22823;&#22810;&#25968;&#29616;&#26377;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15171v1 Announce Type: new  Abstract: We address the problem of stochastic combinatorial semi-bandits, where a player can select from P subsets of a set containing d base items. Most existing algorithms (e.g. CUCB, ESCB, OLS-UCB) require prior knowledge on the reward distribution, like an upper bound on a sub-Gaussian proxy-variance, which is hard to estimate tightly. In this work, we design a variance-adaptive version of OLS-UCB, relying on an online estimation of the covariance structure. Estimating the coefficients of a covariance matrix is much more manageable in practical settings and results in improved regret upper bounds compared to proxy variance-based algorithms. When covariance coefficients are all non-negative, we show that our approach efficiently leverages the semi-bandit feedback and provably outperforms bandit feedback approaches, not only in exponential regimes where P $\gg$ d but also when P $\le$ d, which is not straightforward from most existing analyses.
&lt;/p&gt;</description></item><item><title>CLCE&#26041;&#27861;&#32467;&#21512;&#20102;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#36890;&#36807;&#21327;&#21516;&#21033;&#29992;&#38590;&#20363;&#25366;&#25496;&#25552;&#39640;&#20102;&#24615;&#33021;&#34920;&#29616;</title><link>https://arxiv.org/abs/2402.14551</link><description>&lt;p&gt;
CLCE&#65306;&#19968;&#31181;&#20248;&#21270;&#23398;&#20064;&#34701;&#21512;&#30340;&#25913;&#36827;&#20132;&#21449;&#29109;&#21644;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for Optimized Learning Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14551
&lt;/p&gt;
&lt;p&gt;
CLCE&#26041;&#27861;&#32467;&#21512;&#20102;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#36890;&#36807;&#21327;&#21516;&#21033;&#29992;&#38590;&#20363;&#25366;&#25496;&#25552;&#39640;&#20102;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;&#27169;&#22411;&#20027;&#35201;&#37319;&#29992;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#21021;&#22987;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#65288;CE&#65289;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#35777;&#26126;CE&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CLCE&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#19982;CE&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#20445;&#25345;&#20102;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#21183;&#65292;&#32780;&#19988;&#20197;&#21327;&#21516;&#26041;&#24335;&#21033;&#29992;&#38590;&#20363;&#25366;&#25496;&#26469;&#22686;&#24378;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14551v1 Announce Type: cross  Abstract: State-of-the-art pre-trained image models predominantly adopt a two-stage approach: initial unsupervised pre-training on large-scale datasets followed by task-specific fine-tuning using Cross-Entropy loss~(CE). However, it has been demonstrated that CE can compromise model generalization and stability. While recent works employing contrastive learning address some of these limitations by enhancing the quality of embeddings and producing better decision boundaries, they often overlook the importance of hard negative mining and rely on resource intensive and slow training using large sample batches. To counter these issues, we introduce a novel approach named CLCE, which integrates Label-Aware Contrastive Learning with CE. Our approach not only maintains the strengths of both loss functions but also leverages hard negative mining in a synergistic way to enhance performance. Experimental results demonstrate that CLCE significantly outperf
&lt;/p&gt;</description></item><item><title>ConSmax&#26159;&#19968;&#31181;&#30828;&#20214;&#21451;&#22909;&#22411;Softmax&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#23545;&#21407;Softmax&#20851;&#38190;&#20219;&#21153;&#30340;&#39640;&#25928;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.10930</link><description>&lt;p&gt;
ConSmax: &#20855;&#26377;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#30828;&#20214;&#21451;&#22909;&#22411;Softmax&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10930
&lt;/p&gt;
&lt;p&gt;
ConSmax&#26159;&#19968;&#31181;&#30828;&#20214;&#21451;&#22909;&#22411;Softmax&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#23545;&#21407;Softmax&#20851;&#38190;&#20219;&#21153;&#30340;&#39640;&#25928;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#26426;&#21046;&#23558;&#22522;&#20110;transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#21367;&#31215;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21306;&#20998;&#24320;&#26469;&#12290;&#23613;&#31649;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#65292;&#20294;&#30001;&#20110;&#33258;&#27880;&#24847;&#20013;&#24191;&#27867;&#20351;&#29992;Softmax&#65292;&#22312;&#30789;&#19978;&#23454;&#29616;&#23454;&#26102;LLM&#25512;&#26029;&#20173;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Constant Softmax&#65288;ConSmax&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;Softmax&#26367;&#20195;&#26041;&#26696;&#65292;&#37319;&#29992;&#21487;&#24494;&#30340;&#35268;&#33539;&#21270;&#21442;&#25968;&#26469;&#28040;&#38500;Softmax&#20013;&#30340;&#26368;&#22823;&#25628;&#32034;&#21644;&#20998;&#27597;&#27714;&#21644;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10930v1 Announce Type: cross  Abstract: The self-attention mechanism sets transformer-based large language model (LLM) apart from the convolutional and recurrent neural networks. Despite the performance improvement, achieving real-time LLM inference on silicon is challenging due to the extensively used Softmax in self-attention. Apart from the non-linearity, the low arithmetic intensity greatly reduces the processing parallelism, which becomes the bottleneck especially when dealing with a longer context. To address this challenge, we propose Constant Softmax (ConSmax), a software-hardware co-design as an efficient Softmax alternative. ConSmax employs differentiable normalization parameters to remove the maximum searching and denominator summation in Softmax. It allows for massive parallelization while performing the critical tasks of Softmax. In addition, a scalable ConSmax hardware utilizing a bitwidth-split look-up table (LUT) can produce lossless non-linear operation and 
&lt;/p&gt;</description></item><item><title>&#22810;&#35270;&#35282;&#31526;&#21495;&#22238;&#24402;(MvSR)&#26159;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#21442;&#25968;&#21270;&#35299;&#26469;&#20934;&#30830;&#25311;&#21512;&#25152;&#26377;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#19981;&#21516;&#23454;&#39564;&#35774;&#32622;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04298</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Multi-View Symbolic Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04298
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#31526;&#21495;&#22238;&#24402;(MvSR)&#26159;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#21442;&#25968;&#21270;&#35299;&#26469;&#20934;&#30830;&#25311;&#21512;&#25152;&#26377;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#19981;&#21516;&#23454;&#39564;&#35774;&#32622;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;(SR)&#25628;&#32034;&#34920;&#31034;&#35299;&#37322;&#21464;&#37327;&#21644;&#21709;&#24212;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#12290;&#30446;&#21069;&#30340;SR&#26041;&#27861;&#20551;&#35774;&#20174;&#21333;&#20010;&#23454;&#39564;&#20013;&#25552;&#21462;&#30340;&#21333;&#20010;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#38754;&#20020;&#26469;&#33258;&#19981;&#21516;&#35774;&#32622;&#30340;&#22810;&#20010;&#23454;&#39564;&#32467;&#26524;&#38598;&#12290;&#20256;&#32479;&#30340;SR&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#25214;&#21040;&#28508;&#22312;&#30340;&#34920;&#36798;&#24335;&#65292;&#22240;&#20026;&#27599;&#20010;&#23454;&#39564;&#30340;&#21442;&#25968;&#21487;&#33021;&#19981;&#21516;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#35270;&#35282;&#31526;&#21495;&#22238;&#24402;(MvSR)&#65292;&#23427;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#25968;&#25454;&#38598;&#65292;&#27169;&#25311;&#23454;&#39564;&#29615;&#22659;&#65292;&#24182;&#36755;&#20986;&#19968;&#20010;&#36890;&#29992;&#30340;&#21442;&#25968;&#21270;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#35780;&#20272;&#30340;&#34920;&#36798;&#24335;&#36866;&#24212;&#27599;&#20010;&#29420;&#31435;&#25968;&#25454;&#38598;&#65292;&#24182;&#21516;&#26102;&#36820;&#22238;&#33021;&#22815;&#20934;&#30830;&#25311;&#21512;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#21442;&#25968;&#20989;&#25968;&#26063;f(x; \theta)&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#24050;&#30693;&#34920;&#36798;&#24335;&#29983;&#25104;&#30340;&#25968;&#25454;&#20197;&#21450;&#26469;&#33258;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#26469;&#23637;&#31034;MvSR&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression (SR) searches for analytical expressions representing the relationship between a set of explanatory and response variables. Current SR methods assume a single dataset extracted from a single experiment. Nevertheless, frequently, the researcher is confronted with multiple sets of results obtained from experiments conducted with different setups. Traditional SR methods may fail to find the underlying expression since the parameters of each experiment can be different. In this work we present Multi-View Symbolic Regression (MvSR), which takes into account multiple datasets simultaneously, mimicking experimental environments, and outputs a general parametric solution. This approach fits the evaluated expression to each independent dataset and returns a parametric family of functions f(x; \theta) simultaneously capable of accurately fitting all datasets. We demonstrate the effectiveness of MvSR using data generated from known expressions, as well as real-world data from 
&lt;/p&gt;</description></item></channel></rss>