<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>LLM-ABR&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#35774;&#35745;&#36866;&#29992;&#20110;&#21508;&#31181;&#32593;&#32476;&#29305;&#24615;&#30340;&#33258;&#36866;&#24212;&#30721;&#29575;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#32593;&#32476;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2404.01617</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#33258;&#36866;&#24212;&#30721;&#29575;&#31639;&#27861;&#30340;LLM-ABR
&lt;/p&gt;
&lt;p&gt;
LLM-ABR: Designing Adaptive Bitrate Algorithms via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01617
&lt;/p&gt;
&lt;p&gt;
LLM-ABR&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#35774;&#35745;&#36866;&#29992;&#20110;&#21508;&#31181;&#32593;&#32476;&#29305;&#24615;&#30340;&#33258;&#36866;&#24212;&#30721;&#29575;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#32593;&#32476;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;LLM-ABR&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29983;&#25104;&#33021;&#21147;&#26469;&#33258;&#21160;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#32593;&#32476;&#29305;&#24615;&#30340;&#33258;&#36866;&#24212;&#30721;&#29575;&#65288;ABR&#65289;&#31639;&#27861;&#30340;&#31995;&#32479;&#12290;LLM-ABR&#22312;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20869;&#36816;&#34892;&#65292;&#36171;&#20104;LLMs&#35774;&#35745;&#20851;&#38190;&#32452;&#20214;&#22914;&#29366;&#24577;&#21644;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#23485;&#24102;&#12289;&#21355;&#26143;&#12289;4G&#21644;5G&#22312;&#20869;&#30340;&#19981;&#21516;&#32593;&#32476;&#35774;&#32622;&#20013;&#35780;&#20272;&#20102;LLM-ABR&#12290;LLM-ABR&#22987;&#32456;&#20248;&#20110;&#40664;&#35748;ABR&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01617v1 Announce Type: cross  Abstract: We present LLM-ABR, the first system that utilizes the generative capabilities of large language models (LLMs) to autonomously design adaptive bitrate (ABR) algorithms tailored for diverse network characteristics. Operating within a reinforcement learning framework, LLM-ABR empowers LLMs to design key components such as states and neural network architectures. We evaluate LLM-ABR across diverse network settings, including broadband, satellite, 4G, and 5G. LLM-ABR consistently outperforms default ABR algorithms.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#36716;&#31227;&#23398;&#20064;&#30340;&#20195;&#29702;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#20248;&#21270;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#26469;&#20943;&#23569;&#23454;&#39564;&#30340;&#24635;&#25968;&#65292;&#24182;&#19988;&#28436;&#31034;&#20102;&#22312;&#35774;&#35745;&#29992;&#20110;&#25193;&#22686;&#22522;&#22240;&#35786;&#26029;&#27979;&#23450;&#30340;DNA&#31454;&#20105;&#23545;&#25163;&#26102;&#23454;&#39564;&#25968;&#37327;&#30340;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.17704</link><description>&lt;p&gt;
&#23558;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20110;&#36716;&#31227;&#23398;&#20064;&#20197;&#35774;&#35745;&#29992;&#20110;&#35786;&#26029;&#27979;&#23450;&#30340;&#31454;&#20105;&#23545;&#25163;DNA&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning Bayesian Optimization to Design Competitor DNA Molecules for Use in Diagnostic Assays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17704
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#36716;&#31227;&#23398;&#20064;&#30340;&#20195;&#29702;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#20248;&#21270;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#26469;&#20943;&#23569;&#23454;&#39564;&#30340;&#24635;&#25968;&#65292;&#24182;&#19988;&#28436;&#31034;&#20102;&#22312;&#35774;&#35745;&#29992;&#20110;&#25193;&#22686;&#22522;&#22240;&#35786;&#26029;&#27979;&#23450;&#30340;DNA&#31454;&#20105;&#23545;&#25163;&#26102;&#23454;&#39564;&#25968;&#37327;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24037;&#31243;&#29983;&#29289;&#20998;&#23376;&#35774;&#22791;&#30340;&#20852;&#36215;&#65292;&#23450;&#21046;&#29983;&#29289;&#24207;&#21015;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#36890;&#24120;&#65292;&#20026;&#20102;&#29305;&#23450;&#24212;&#29992;&#38656;&#35201;&#21046;&#20316;&#35768;&#22810;&#31867;&#20284;&#30340;&#29983;&#29289;&#24207;&#21015;&#65292;&#36825;&#24847;&#21619;&#30528;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#29978;&#33267;&#26114;&#36149;&#30340;&#23454;&#39564;&#26469;&#20248;&#21270;&#36825;&#20123;&#24207;&#21015;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36716;&#31227;&#23398;&#20064;&#35774;&#35745;&#23454;&#39564;&#24037;&#20316;&#27969;&#31243;&#65292;&#20351;&#36825;&#31181;&#24320;&#21457;&#21464;&#24471;&#21487;&#34892;&#12290;&#36890;&#36807;&#23558;&#36716;&#31227;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#20248;&#21270;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#26469;&#20943;&#23569;&#23454;&#39564;&#30340;&#24635;&#25968;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;&#29992;&#20110;&#25193;&#22686;&#22522;&#22240;&#35786;&#26029;&#27979;&#23450;&#20013;&#20351;&#29992;&#30340;DNA&#31454;&#20105;&#23545;&#25163;&#24320;&#21457;&#25968;&#25454;&#26469;&#20943;&#23569;&#23454;&#39564;&#25968;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#20132;&#21449;&#39564;&#35777;&#26469;&#27604;&#36739;&#19981;&#21516;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#28982;&#21518;&#27604;&#36739;&#36825;&#20123;&#27169;&#22411;&#22312;&#21333;&#19968;&#30446;&#26631;&#21644;&#24809;&#32602;&#20248;&#21270;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17704v1 Announce Type: cross  Abstract: With the rise in engineered biomolecular devices, there is an increased need for tailor-made biological sequences. Often, many similar biological sequences need to be made for a specific application meaning numerous, sometimes prohibitively expensive, lab experiments are necessary for their optimization. This paper presents a transfer learning design of experiments workflow to make this development feasible. By combining a transfer learning surrogate model with Bayesian optimization, we show how the total number of experiments can be reduced by sharing information between optimization tasks. We demonstrate the reduction in the number of experiments using data from the development of DNA competitors for use in an amplification-based diagnostic assay. We use cross-validation to compare the predictive accuracy of different transfer learning models, and then compare the performance of the models for both single objective and penalized opti
&lt;/p&gt;</description></item><item><title>&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#34920;&#29616;&#20986;&#36807;&#24230;&#27867;&#21270;&#29616;&#35937;&#65292;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#35774;&#35745;&#20102;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#65292;&#36890;&#36807;&#36882;&#24402;&#26144;&#23556;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768;&#29983;&#25104;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.11793</link><description>&lt;p&gt;
&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generative Kaleidoscopic Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11793
&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#34920;&#29616;&#20986;&#36807;&#24230;&#27867;&#21270;&#29616;&#35937;&#65292;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#35774;&#35745;&#20102;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#65292;&#36890;&#36807;&#36882;&#24402;&#26144;&#23556;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768;&#29983;&#25104;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#65288;&#25110;&#22810;&#23618;&#24863;&#30693;&#22120;&#26550;&#26500;&#65289;&#34920;&#29616;&#20986;&#8220;&#36807;&#24230;&#27867;&#21270;&#8221;&#29616;&#35937;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#37027;&#20123;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#30475;&#21040;&#30340;&#36755;&#20837;&#30340;&#36755;&#20986;&#20540;&#34987;&#26144;&#23556;&#21040;&#20102;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#36755;&#20986;&#33539;&#22260;&#38468;&#36817;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#23398;&#20064;&#20102;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#65292;&#36825;&#31181;&#25928;&#24212;&#22312;&#22686;&#21152;&#23618;&#25968;&#25110;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#28145;&#24230;&#26102;&#26356;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#28145;&#23618;ReLU&#32593;&#32476;&#30340;&#36825;&#19968;&#29305;&#24615;&#26469;&#35774;&#35745;&#19968;&#20010;&#25968;&#25454;&#38598;&#19975;&#33457;&#31570;&#65292;&#31216;&#20026;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#22914;&#26524;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#22810;&#23618;&#24863;&#30693;&#22120;&#23558;&#36755;&#20837; $x\in\mathbb{R}^D$ &#26144;&#23556;&#21040;&#33258;&#36523; $f_\mathcal{N}(x)\rightarrow x$&#65292;&#37027;&#20040;&#8220;&#19975;&#33457;&#31570;&#37319;&#26679;&#8221;&#36807;&#31243;&#23558;&#20174;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768; $z\in\mathbb{R}^D$ &#24320;&#22987;&#65292;&#24182;&#36882;&#24402;&#22320;&#24212;&#29992; $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$&#12290;&#32463;&#36807;&#29123;&#28903;&#26399;&#21518;&#65292;&#25105;&#20204;&#24320;&#22987;&#35266;&#23519;&#26469;&#33258;&#36755;&#20837;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#21457;&#29616;&#26356;&#28145;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11793v1 Announce Type: cross  Abstract: We discovered that the Deep ReLU networks (or Multilayer Perceptron architecture) demonstrate an 'over-generalization' phenomenon. That is, the output values for the inputs that were not seen during training are mapped close to the output range that were observed during the learning process. In other words, the MLP learns a many-to-one mapping and this effect is more prominent as we increase the number of layers or depth of the MLP. We utilize this property of Deep ReLU networks to design a dataset kaleidoscope, termed as 'Generative Kaleidoscopic Networks'. Briefly, if we learn a MLP to map from input $x\in\mathbb{R}^D$ to itself $f_\mathcal{N}(x)\rightarrow x$, the 'Kaleidoscopic sampling' procedure starts with a random input noise $z\in\mathbb{R}^D$ and recursively applies $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$. After a burn-in period duration, we start observing samples from the input distribution and we found that deeper 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25193;&#23637;&#20102;&#22312;&#32447;&#32467;&#26500;&#21270;&#39044;&#27979;&#30340;&#26367;&#20195;&#21518;&#24724;&#24230;&#30028;&#38480;&#65292;&#36890;&#36807;&#24341;&#20837;Fenchel-Young&#25439;&#22833;&#21644;&#38543;&#26426;&#35299;&#30721;&#26041;&#26696;&#65292;&#20351;&#24471;&#22312;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#21644;&#36923;&#36753;&#25439;&#22833;&#19979;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08180</link><description>&lt;p&gt;
&#22312;&#32447;&#32467;&#26500;&#21270;&#39044;&#27979;&#19982;Fenchel-Young&#25439;&#22833;&#21644;&#25913;&#36827;&#30340;&#26367;&#20195;&#21518;&#24724;&#24230;&#29992;&#20110;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#19982;&#36923;&#36753;&#25439;&#22833;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Online Structured Prediction with Fenchel--Young Losses and Improved Surrogate Regret for Online Multiclass Classification with Logistic Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08180
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25193;&#23637;&#20102;&#22312;&#32447;&#32467;&#26500;&#21270;&#39044;&#27979;&#30340;&#26367;&#20195;&#21518;&#24724;&#24230;&#30028;&#38480;&#65292;&#36890;&#36807;&#24341;&#20837;Fenchel-Young&#25439;&#22833;&#21644;&#38543;&#26426;&#35299;&#30721;&#26041;&#26696;&#65292;&#20351;&#24471;&#22312;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#21644;&#36923;&#36753;&#25439;&#22833;&#19979;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#23436;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#22312;&#32447;&#32467;&#26500;&#21270;&#39044;&#27979;&#12290;&#23545;&#20110;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#65292;van der Hoeven(2020)&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20248;&#32654;&#30340;&#8220;&#21033;&#29992;&#26367;&#20195;&#38388;&#38553;&#8221;&#30340;&#26694;&#26550;&#65292;&#33719;&#24471;&#20102;&#19982;&#26102;&#38388;&#33539;&#22260;&#26080;&#20851;&#30340;&#26367;&#20195;&#21518;&#24724;&#24230;&#30028;&#38480;&#65292;&#21363;&#26377;&#38480;&#30340;&#30028;&#38480;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#26694;&#26550;&#20027;&#35201;&#38480;&#20110;&#22810;&#31867;&#20998;&#31867;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#19968;&#31181;&#29305;&#23450;&#20110;&#20998;&#31867;&#30340;&#36807;&#31243;&#65292;&#23558;&#20272;&#35745;&#24471;&#20998;&#36716;&#21270;&#20026;&#36755;&#20986;&#12290;&#25105;&#20204;&#23558;&#8220;&#21033;&#29992;&#26367;&#20195;&#38388;&#38553;&#8221;&#26694;&#26550;&#25193;&#23637;&#21040;&#20855;&#26377;&#8220;Fenchel-Young&#25439;&#22833;&#8221;&#30340;&#22312;&#32447;&#32467;&#26500;&#21270;&#39044;&#27979;&#20013;&#65292;&#36825;&#26159;&#19968;&#22823;&#31867;&#21253;&#25324;&#22810;&#31867;&#20998;&#31867;&#30340;&#36923;&#36753;&#25439;&#22833;&#22312;&#20869;&#30340;&#26367;&#20195;&#25439;&#22833;&#65292;&#33719;&#24471;&#20102;&#22312;&#21508;&#31181;&#32467;&#26500;&#21270;&#39044;&#27979;&#38382;&#39064;&#19978;&#30340;&#26377;&#38480;&#26367;&#20195;&#21518;&#24724;&#24230;&#30028;&#38480;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#38543;&#26426;&#35299;&#30721;&#65292;&#23558;&#20272;&#35745;&#24471;&#20998;&#36716;&#21270;&#20026;&#19968;&#33324;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#35299;&#30721;&#24212;&#29992;&#20110;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#19982;&#36923;&#36753;&#25439;&#22833;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#25913;&#36827;&#30340;&#26367;&#20195;&#21518;&#24724;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies online structured prediction with full-information feedback. For online multiclass classification, van der Hoeven (2020) has obtained surrogate regret bounds independent of the time horizon, or \emph{finite}, by introducing an elegant \emph{exploit-the-surrogate-gap} framework. However, this framework has been limited to multiclass classification primarily because it relies on a classification-specific procedure for converting estimated scores to outputs. We extend the exploit-the-surrogate-gap framework to online structured prediction with \emph{Fenchel--Young losses}, a large family of surrogate losses including the logistic loss for multiclass classification, obtaining finite surrogate regret bounds in various structured prediction problems. To this end, we propose and analyze \emph{randomized decoding}, which converts estimated scores to general structured outputs. Moreover, by applying our decoding to online multiclass classification with the logistic loss, we o
&lt;/p&gt;</description></item><item><title>TopoNav&#26159;&#19968;&#31181;&#25299;&#25169;&#23548;&#33322;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20027;&#21160;&#25299;&#25169;&#26144;&#23556;&#12289;&#20869;&#37096;&#22870;&#21169;&#26426;&#21046;&#21644;&#23618;&#27425;&#21270;&#30446;&#26631;&#20248;&#20808;&#32423;&#30340;&#32452;&#21512;&#26469;&#23454;&#29616;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#39640;&#25928;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.04061</link><description>&lt;p&gt;
TopoNav&#65306;&#33410;&#32422;&#22870;&#21169;&#29615;&#22659;&#20013;&#39640;&#25928;&#25506;&#32034;&#30340;&#25299;&#25169;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
TopoNav: Topological Navigation for Efficient Exploration in Sparse Reward Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04061
&lt;/p&gt;
&lt;p&gt;
TopoNav&#26159;&#19968;&#31181;&#25299;&#25169;&#23548;&#33322;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20027;&#21160;&#25299;&#25169;&#26144;&#23556;&#12289;&#20869;&#37096;&#22870;&#21169;&#26426;&#21046;&#21644;&#23618;&#27425;&#21270;&#30446;&#26631;&#20248;&#20808;&#32423;&#30340;&#32452;&#21512;&#26469;&#23454;&#29616;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#39640;&#25928;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#21306;&#22495;&#30340;&#25506;&#32034;&#38754;&#20020;&#30528;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#8212;&#8212;&#22312;&#27809;&#26377;&#20808;&#21069;&#22320;&#22270;&#21644;&#26377;&#38480;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#23548;&#33322;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#65292;&#36825;&#20010;&#25361;&#25112;&#26356;&#21152;&#20005;&#23803;&#65292;&#20256;&#32479;&#30340;&#25506;&#32034;&#25216;&#26415;&#24448;&#24448;&#22833;&#36133;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TopoNav&#65292;&#19968;&#31181;&#20840;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#23454;&#29616;&#39640;&#25928;&#12289;&#36866;&#24212;&#24615;&#24378;&#19988;&#30446;&#26631;&#23548;&#21521;&#30340;&#25506;&#32034;&#12290;TopoNav&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#26159;&#20027;&#21160;&#25299;&#25169;&#26144;&#23556;&#12289;&#20869;&#37096;&#22870;&#21169;&#26426;&#21046;&#21644;&#23618;&#27425;&#21270;&#30446;&#26631;&#20248;&#20808;&#32423;&#12290;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#65292;TopoNav&#26500;&#24314;&#20102;&#21160;&#24577;&#25299;&#25169;&#22320;&#22270;&#65292;&#25429;&#33719;&#20851;&#38190;&#20301;&#32622;&#21644;&#36335;&#24452;&#12290;&#23427;&#21033;&#29992;&#20869;&#37096;&#22870;&#21169;&#26469;&#25351;&#23548;&#26426;&#22120;&#20154;&#26397;&#30528;&#22320;&#22270;&#20013;&#25351;&#23450;&#30340;&#23376;&#30446;&#26631;&#21069;&#36827;&#65292;&#20419;&#36827;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#32467;&#26500;&#21270;&#25506;&#32034;&#12290;&#20026;&#20102;&#30830;&#20445;&#39640;&#25928;&#23548;&#33322;&#65292;TopoNav&#37319;&#29992;&#20102;&#20998;&#23618;&#30446;&#26631;&#39537;&#21160;&#30340;&#20027;&#21160;&#25299;&#25169;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20248;&#20808;&#32771;&#34385;&#26368;&#32039;&#24613;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous robots exploring unknown areas face a significant challenge -- navigating effectively without prior maps and with limited external feedback. This challenge intensifies in sparse reward environments, where traditional exploration techniques often fail. In this paper, we introduce TopoNav, a novel framework that empowers robots to overcome these constraints and achieve efficient, adaptable, and goal-oriented exploration. TopoNav's fundamental building blocks are active topological mapping, intrinsic reward mechanisms, and hierarchical objective prioritization. Throughout its exploration, TopoNav constructs a dynamic topological map that captures key locations and pathways. It utilizes intrinsic rewards to guide the robot towards designated sub-goals within this map, fostering structured exploration even in sparse reward settings. To ensure efficient navigation, TopoNav employs the Hierarchical Objective-Driven Active Topologies framework, enabling the robot to prioritize immed
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#31181;&#39044;&#35757;&#32451;&#21644;&#27867;&#21270;&#20219;&#21153;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#27169;&#24577;&#23545;&#20110;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.02500</link><description>&lt;p&gt;
&#28857;&#20113;&#38382;&#39064;:&#37325;&#26032;&#24605;&#32771;&#19981;&#21516;&#35266;&#27979;&#31354;&#38388;&#23545;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02500
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#31181;&#39044;&#35757;&#32451;&#21644;&#27867;&#21270;&#20219;&#21153;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#27169;&#24577;&#23545;&#20110;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#35266;&#27979;&#31354;&#38388;&#23545;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#19977;&#31181;&#20027;&#35201;&#27169;&#24577;&#65306;RGB&#65292;RGB-D&#21644;&#28857;&#20113;&#12290;&#36890;&#36807;&#22312;&#36229;&#36807;17&#20010;&#19981;&#21516;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#28041;&#21450;&#20004;&#20010;&#22522;&#20934;&#21644;&#20223;&#30495;&#22120;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#36235;&#21183;&#65306;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#35774;&#35745;&#65292;&#36890;&#24120;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;RGB&#21644;RGB-D&#30340;&#23545;&#24212;&#29289;&#12290;&#36825;&#22312;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20004;&#31181;&#24773;&#20917;&#19979;&#37117;&#26159;&#19968;&#33268;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#22312;&#30456;&#26426;&#35270;&#35282;&#12289;&#29031;&#26126;&#26465;&#20214;&#12289;&#22122;&#22768;&#27700;&#24179;&#21644;&#32972;&#26223;&#22806;&#35266;&#31561;&#21508;&#31181;&#20960;&#20309;&#21644;&#35270;&#35273;&#32447;&#32034;&#26041;&#38754;&#65292;&#37117;&#33021;&#25552;&#39640;&#31574;&#30053;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19977;&#32500;&#28857;&#20113;&#26159;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#26377;&#20215;&#20540;&#30340;&#35266;&#27979;&#27169;&#24577;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#25152;&#26377;&#30340;&#20195;&#30721;&#21644;&#26816;&#26597;&#28857;&#65292;&#24076;&#26395;&#25105;&#20204;&#30340;&#35266;&#28857;&#33021;&#24110;&#21161;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the influence of different observation spaces on robot learning, focusing on three predominant modalities: RGB, RGB-D, and point cloud. Through extensive experimentation on over 17 varied contact-rich manipulation tasks, conducted across two benchmarks and simulators, we have observed a notable trend: point cloud-based methods, even those with the simplest designs, frequently surpass their RGB and RGB-D counterparts in performance. This remains consistent in both scenarios: training from scratch and utilizing pretraining. Furthermore, our findings indicate that point cloud observations lead to improved policy zero-shot generalization in relation to various geometry and visual clues, including camera viewpoints, lighting conditions, noise levels and background appearance. The outcomes suggest that 3D point cloud is a valuable observation modality for intricate robotic tasks. We will open-source all our codes and checkpoints, hoping that our insights can help de
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20215;&#20540;&#22686;&#24378;&#30340;&#26465;&#20214;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;RCSL&#30340;&#31283;&#23450;&#24615;&#19982;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;&#30340;&#36830;&#25509;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#21160;&#24577;&#22320;&#26681;&#25454;&#36712;&#36857;&#22238;&#25253;&#23558;&#20215;&#20540;&#24110;&#21161;&#27880;&#20837;&#25439;&#22833;&#20989;&#25968;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#21508;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#36712;&#36857;&#22238;&#25253;&#65292;&#25512;&#21160;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.02017</link><description>&lt;p&gt;
&#26080;&#38656;&#22870;&#21169;&#30340;&#26465;&#20214;&#30417;&#30563;&#23398;&#20064;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Value-Aided Conditional Supervised Learning for Offline RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02017
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20215;&#20540;&#22686;&#24378;&#30340;&#26465;&#20214;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;RCSL&#30340;&#31283;&#23450;&#24615;&#19982;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;&#30340;&#36830;&#25509;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#21160;&#24577;&#22320;&#26681;&#25454;&#36712;&#36857;&#22238;&#25253;&#23558;&#20215;&#20540;&#24110;&#21161;&#27880;&#20837;&#25439;&#22833;&#20989;&#25968;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#21508;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#36712;&#36857;&#22238;&#25253;&#65292;&#25512;&#21160;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#22522;&#20110;&#22238;&#25253;&#30340;&#26465;&#20214;&#30417;&#30563;&#23398;&#20064;&#65288;RCSL&#65289;&#21644;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#27599;&#31181;&#26041;&#27861;&#37117;&#23384;&#22312;&#19968;&#20123;&#23454;&#38469;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20215;&#20540;&#22686;&#24378;&#30340;&#26465;&#20214;&#30417;&#30563;&#23398;&#20064;&#65288;VCS&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;RCSL&#30340;&#31283;&#23450;&#24615;&#19982;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;&#30340;&#36830;&#25509;&#33021;&#21147;&#26377;&#25928;&#22320;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#36890;&#36807;&#31070;&#32463;&#20999;&#32447;&#26680;&#20998;&#26512;&#65292;VCS&#21487;&#20197;&#21160;&#24577;&#22320;&#26681;&#25454;&#36712;&#36857;&#22238;&#25253;&#23558;&#20215;&#20540;&#24110;&#21161;&#27880;&#20837;RCSL&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#20197;&#21306;&#20998;&#20215;&#20540;&#20989;&#25968;&#21487;&#33021;&#26080;&#27861;&#23454;&#29616;&#31283;&#23450;&#36830;&#25509;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;VCS&#19981;&#20165;&#26174;&#33879;&#20248;&#20110;RCSL&#21644;&#22522;&#20110;&#20215;&#20540;&#30340;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#21508;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#23454;&#29616;&#20102;&#25110;&#32463;&#24120;&#36229;&#36807;&#26368;&#39640;&#30340;&#36712;&#36857;&#22238;&#25253;&#12290;&#36825;&#19968;&#31361;&#30772;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#24320;&#36767;&#20102;&#26032;&#30340;&#36947;&#36335;&#65292;&#25512;&#21160;&#20102;&#21487;&#23454;&#29616;&#30340;&#26497;&#38480;&#65292;&#24182;&#20419;&#36827;&#20102;&#36827;&#19968;&#27493;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) has seen notable advancements through return-conditioned supervised learning (RCSL) and value-based methods, yet each approach comes with its own set of practical challenges. Addressing these, we propose Value-Aided Conditional Supervised Learning (VCS), a method that effectively synergizes the stability of RCSL with the stitching ability of value-based methods. Based on the Neural Tangent Kernel analysis to discern instances where value function may not lead to stable stitching, VCS injects the value aid into the RCSL's loss function dynamically according to the trajectory return. Our empirical studies reveal that VCS not only significantly outperforms both RCSL and value-based methods but also consistently achieves, or often surpasses, the highest trajectory returns across diverse offline RL benchmarks. This breakthrough in VCS paves new paths in offline RL, pushing the limits of what can be achieved and fostering further innovations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21152;&#24615;&#27169;&#22411;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#20869;&#23384;&#23384;&#20648;&#21644;&#35745;&#31639;&#35201;&#27714;&#12290;&#22312;&#35268;&#33539;&#24456;&#22909;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20180;&#32454;&#36873;&#25321;&#23398;&#20064;&#29575;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#21644;&#26368;&#20248;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2401.00691</link><description>&lt;p&gt;
&#28155;&#21152;&#38750;&#21442;&#25968;&#22238;&#24402;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent for Additive Nonparametric Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21152;&#24615;&#27169;&#22411;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#20869;&#23384;&#23384;&#20648;&#21644;&#35745;&#31639;&#35201;&#27714;&#12290;&#22312;&#35268;&#33539;&#24456;&#22909;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20180;&#32454;&#36873;&#25321;&#23398;&#20064;&#29575;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#21644;&#26368;&#20248;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21152;&#24615;&#27169;&#22411;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#20869;&#23384;&#23384;&#20648;&#21644;&#35745;&#31639;&#35201;&#27714;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#23545;&#32452;&#20214;&#20989;&#25968;&#30340;&#25130;&#26029;&#22522;&#25193;&#23637;&#30340;&#31995;&#25968;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#20989;&#25968;&#23545;&#24212;&#29289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24471;&#21040;&#30340;&#20272;&#35745;&#37327;&#28385;&#36275;&#19968;&#20010;&#22885;&#25289;&#20811;&#19981;&#31561;&#24335;&#65292;&#20801;&#35768;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#12290;&#22312;&#35268;&#33539;&#24456;&#22909;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#30340;&#19977;&#20010;&#19981;&#21516;&#38454;&#27573;&#20180;&#32454;&#36873;&#25321;&#23398;&#20064;&#29575;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#39118;&#38505;&#22312;&#25968;&#25454;&#32500;&#24230;&#21644;&#35757;&#32451;&#26679;&#26412;&#22823;&#23567;&#30340;&#20381;&#36182;&#26041;&#38754;&#26159;&#26368;&#23567;&#21644;&#26368;&#20248;&#30340;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23558;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#21453;&#21521;&#25311;&#21512;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;&#35745;&#31639;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an iterative algorithm for training additive models that enjoys favorable memory storage and computational requirements. The algorithm can be viewed as the functional counterpart of stochastic gradient descent, applied to the coefficients of a truncated basis expansion of the component functions. We show that the resulting estimator satisfies an oracle inequality that allows for model mis-specification. In the well-specified setting, by choosing the learning rate carefully across three distinct stages of training, we demonstrate that its risk is minimax optimal in terms of the dependence on the dimensionality of the data and the size of the training sample. We further illustrate the computational benefits by comparing the approach with traditional backfitting on two real-world datasets.
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22240;&#26524;&#20844;&#24179;&#24615;&#23545;&#26410;&#35266;&#23519;&#21040;&#28151;&#26434;&#30340;&#25935;&#24863;&#24615;&#65292;&#25512;&#23548;&#20986;&#22240;&#26524;&#20844;&#24179;&#24615;&#25351;&#26631;&#30340;&#30028;&#38480;&#65292;&#25552;&#20986;&#31070;&#32463;&#26694;&#26550;&#29992;&#20110;&#23398;&#20064;&#20844;&#24179;&#39044;&#27979;&#65292;&#23637;&#31034;&#20102;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2311.18460</link><description>&lt;p&gt;
&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#19979;&#30340;&#22240;&#26524;&#20844;&#24179;&#24615;&#65306;&#19968;&#31181;&#31070;&#32463;&#25935;&#24863;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18460
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22240;&#26524;&#20844;&#24179;&#24615;&#23545;&#26410;&#35266;&#23519;&#21040;&#28151;&#26434;&#30340;&#25935;&#24863;&#24615;&#65292;&#25512;&#23548;&#20986;&#22240;&#26524;&#20844;&#24179;&#24615;&#25351;&#26631;&#30340;&#30028;&#38480;&#65292;&#25552;&#20986;&#31070;&#32463;&#26694;&#26550;&#29992;&#20110;&#23398;&#20064;&#20844;&#24179;&#39044;&#27979;&#65292;&#23637;&#31034;&#20102;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20013;&#30340;&#20844;&#24179;&#24615;&#30001;&#20110;&#27861;&#24459;&#12289;&#36947;&#24503;&#21644;&#31038;&#20250;&#21407;&#22240;&#22312;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#35201;&#27714;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#24120;&#38598;&#20013;&#22312;&#27809;&#26377;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#30340;&#35774;&#32622;&#19978;&#65292;&#23613;&#31649;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#36829;&#21453;&#22240;&#26524;&#20844;&#24179;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22240;&#26524;&#20844;&#24179;&#24615;&#23545;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19981;&#21516;&#26469;&#28304;&#30340;&#26410;&#35266;&#23519;&#21040;&#28151;&#26434;&#19979;&#22240;&#26524;&#20844;&#24179;&#24615;&#25351;&#26631;&#30340;&#30028;&#38480;&#12290;&#36825;&#20351;&#20174;&#19994;&#32773;&#33021;&#22815;&#26816;&#26597;&#20854;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#22312;&#20844;&#24179;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#30340;&#25935;&#24863;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#20844;&#24179;&#39044;&#27979;&#30340;&#26032;&#22411;&#31070;&#32463;&#26694;&#26550;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#23545;&#22240;&#26524;&#20844;&#24179;&#24615;&#21487;&#33021;&#30001;&#20110;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#32780;&#21463;&#21040;&#36829;&#21453;&#30340;&#31243;&#24230;&#30340;&#26368;&#22351;&#24773;&#20917;&#20445;&#35777;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18460v2 Announce Type: replace-cross  Abstract: Fairness for machine learning predictions is widely required in practice for legal, ethical, and societal reasons. Existing work typically focuses on settings without unobserved confounding, even though unobserved confounding can lead to severe violations of causal fairness and, thus, unfair predictions. In this work, we analyze the sensitivity of causal fairness to unobserved confounding. Our contributions are three-fold. First, we derive bounds for causal fairness metrics under different sources of unobserved confounding. This enables practitioners to examine the sensitivity of their machine learning models to unobserved confounding in fairness-critical applications. Second, we propose a novel neural framework for learning fair predictions, which allows us to offer worst-case guarantees of the extent to which causal fairness can be violated due to unobserved confounding. Third, we demonstrate the effectiveness of our framewor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#26469;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#32780;&#23548;&#33268;RLHF&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16635</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Reinforcement Learning from Human Feedback with Efficient Reward Model Ensemble. (arXiv:2401.16635v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#26469;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#32780;&#23548;&#33268;RLHF&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;RLHF&#20381;&#36182;&#20110;&#36890;&#36807;&#26377;&#38480;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;RLHF&#21487;&#33021;&#20135;&#29983;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#22870;&#21169;&#27169;&#22411;&#20570;&#20986;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#32771;&#34385;&#21040;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#21487;&#33021;&#20855;&#26377;&#35745;&#31639;&#21644;&#36164;&#28304;&#26114;&#36149;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21253;&#25324;&#32447;&#24615;&#23618;&#38598;&#25104;&#21644;&#22522;&#20110;LoRA&#30340;&#38598;&#25104;&#22312;&#20869;&#30340;&#39640;&#25928;&#38598;&#25104;&#26041;&#27861;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#38598;&#25104;&#22870;&#21169;&#27169;&#22411;&#36816;&#34892;Best-of-$n$&#21644;Proximal Policy Optimization&#65292;&#24182;&#39564;&#35777;&#25105;&#20204;&#30340;&#38598;&#25104;&#26041;&#27861;&#26377;&#21161;&#20110;&#25913;&#21892;RLHF&#36755;&#20986;&#30340;&#23545;&#40784;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is a widely adopted approach for aligning large language models with human values. However, RLHF relies on a reward model that is trained with a limited amount of human preference data, which could lead to inaccurate predictions. As a result, RLHF may produce outputs that are misaligned with human values. To mitigate this issue, we contribute a reward ensemble method that allows the reward model to make more accurate predictions. As using an ensemble of large language model-based reward models can be computationally and resource-expensive, we explore efficient ensemble methods including linear-layer ensemble and LoRA-based ensemble. Empirically, we run Best-of-$n$ and Proximal Policy Optimization with our ensembled reward models, and verify that our ensemble methods help improve the alignment performance of RLHF outputs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#36890;&#36807;&#26500;&#24314;&#25968;&#25454;&#38598;&#21644;&#30830;&#23450;&#32534;&#36753;&#21160;&#20316;&#21644;&#26377;&#29992;&#20449;&#24687;&#65292;&#36825;&#33021;&#22815;&#35299;&#20915;&#30446;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03069</link><description>&lt;p&gt;
&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;Bug&#21487;&#22797;&#29616;&#24615;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Enhancing the Reproducibility of Deep Learning Bugs: An Empirical Study. (arXiv:2401.03069v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#36890;&#36807;&#26500;&#24314;&#25968;&#25454;&#38598;&#21644;&#30830;&#23450;&#32534;&#36753;&#21160;&#20316;&#21644;&#26377;&#29992;&#20449;&#24687;&#65292;&#36825;&#33021;&#22815;&#35299;&#20915;&#30446;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#36719;&#20214;&#31995;&#32479;&#19968;&#26679;&#65292;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20063;&#23384;&#22312;Bug&#65292;&#36825;&#21487;&#33021;&#23545;&#33258;&#21160;&#39550;&#39542;&#31561;&#39046;&#22495;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#36825;&#38459;&#30861;&#20102;Bug&#30340;&#35299;&#20915;&#12290;&#29616;&#26377;&#25991;&#29486;&#25351;&#20986;&#65292;&#20165;&#26377;3%&#30340;&#28145;&#24230;&#23398;&#20064;Bug&#26159;&#21487;&#22797;&#29616;&#30340;&#65292;&#36825;&#20984;&#26174;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#30446;&#26631;&#65306;&#26412;&#25991;&#32771;&#23519;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#35782;&#21035;&#21487;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;Bug&#21487;&#22797;&#29616;&#24615;&#30340;&#32534;&#36753;&#21160;&#20316;&#21644;&#26377;&#29992;&#20449;&#24687;&#12290;&#26041;&#27861;&#65306;&#39318;&#20808;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;Stack Overflow&#21644;Defects4ML&#30340;3&#20010;&#26694;&#26550;&#21644;22&#20010;&#26550;&#26500;&#30340;668&#20010;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#20998;&#23618;&#25277;&#26679;&#36873;&#25321;&#20102;102&#20010;Bug&#65292;&#24182;&#23581;&#35797;&#30830;&#23450;&#23427;&#20204;&#30340;&#21487;&#22797;&#29616;&#24615;&#12290;&#22312;&#22797;&#29616;&#36825;&#20123;Bug&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;&#32534;&#36753;&#21160;&#20316;&#21644;&#26377;&#29992;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Deep learning has achieved remarkable progress in various domains. However, like traditional software systems, deep learning systems contain bugs, which can have severe impacts, as evidenced by crashes involving autonomous vehicles. Despite substantial advancements in deep learning techniques, little research has focused on reproducing deep learning bugs, which hinders resolving them. Existing literature suggests that only 3% of deep learning bugs are reproducible, underscoring the need for further research.  Objective: This paper examines the reproducibility of deep learning bugs. We identify edit actions and useful information that could improve deep learning bug reproducibility.  Method: First, we construct a dataset of 668 deep learning bugs from Stack Overflow and Defects4ML across 3 frameworks and 22 architectures. Second, we select 102 bugs using stratified sampling and try to determine their reproducibility. While reproducing these bugs, we identify edit actions and us
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;Node2Vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#65288;&#32463;&#36807;&#24230;&#20462;&#27491;&#30340;&#65289;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;k-means&#32858;&#31867;&#26041;&#27861;&#23545;&#36825;&#20123;&#23884;&#20837;&#36827;&#34892;&#31038;&#21306;&#24674;&#22797;&#26159;&#24369;&#19968;&#33268;&#30340;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#23884;&#20837;&#22312;&#33410;&#28857;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.17712</link><description>&lt;p&gt;
&#20351;&#29992;Node2Vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#36827;&#34892;&#31038;&#21306;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Community Detection and Classification Guarantees Using Embeddings Learned by Node2Vec. (arXiv:2310.17712v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;Node2Vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#65288;&#32463;&#36807;&#24230;&#20462;&#27491;&#30340;&#65289;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;k-means&#32858;&#31867;&#26041;&#27861;&#23545;&#36825;&#20123;&#23884;&#20837;&#36827;&#34892;&#31038;&#21306;&#24674;&#22797;&#26159;&#24369;&#19968;&#33268;&#30340;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#65292;&#24182;&#25506;&#35752;&#20102;&#23884;&#20837;&#22312;&#33410;&#28857;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#32593;&#32476;&#30340;&#33410;&#28857;&#23884;&#20837;&#21040;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#30446;&#26631;&#65292;&#26377;&#21508;&#31181;&#24037;&#20855;&#21487;&#29992;&#12290;&#36825;&#20123;&#23884;&#20837;&#21487;&#20197;&#29992;&#20316;&#31038;&#21306;&#26816;&#27979;/&#33410;&#28857;&#32858;&#31867;&#25110;&#38142;&#25509;&#39044;&#27979;&#31561;&#20219;&#21153;&#30340;&#29305;&#24449;&#65292;&#20854;&#24615;&#33021;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#38500;&#20102;&#35889;&#32858;&#31867;&#26041;&#27861;&#20043;&#22806;&#65292;&#23545;&#20110;&#20854;&#20182;&#24120;&#29992;&#30340;&#23398;&#20064;&#23884;&#20837;&#26041;&#27861;&#65292;&#32570;&#20047;&#29702;&#35770;&#19978;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#30001;node2vec&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#30340;&#29702;&#35770;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;node2vec&#29983;&#25104;&#30340;&#23884;&#20837;&#21521;&#37327;&#24212;&#29992;k-means&#32858;&#31867;&#21487;&#20197;&#23545;&#65288;&#32463;&#36807;&#24230;&#20462;&#27491;&#30340;&#65289;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#24369;&#19968;&#33268;&#30340;&#31038;&#21306;&#24674;&#22797;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36825;&#20123;&#23884;&#20837;&#22312;&#33410;&#28857;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20010;&#32467;&#26524;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#19982;&#32593;&#32476;&#25968;&#25454;&#30340;&#20854;&#20182;&#23884;&#20837;&#24037;&#20855;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding the nodes of a large network into an Euclidean space is a common objective in modern machine learning, with a variety of tools available. These embeddings can then be used as features for tasks such as community detection/node clustering or link prediction, where they achieve state of the art performance. With the exception of spectral clustering methods, there is little theoretical understanding for other commonly used approaches to learning embeddings. In this work we examine the theoretical properties of the embeddings learned by node2vec. Our main result shows that the use of k-means clustering on the embedding vectors produced by node2vec gives weakly consistent community recovery for the nodes in (degree corrected) stochastic block models. We also discuss the use of these embeddings for node and link prediction tasks. We demonstrate this result empirically, and examine how this relates to other embedding tools for network data.
&lt;/p&gt;</description></item><item><title>StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.17042</link><description>&lt;p&gt;
StochGradAdam: &#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling. (arXiv:2310.17042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17042
&lt;/p&gt;
&lt;p&gt;
StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#39046;&#22495;&#20013;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;StochGradAdam&#20248;&#21270;&#22120;&#65292;&#36825;&#26159;&#23545;&#24191;&#21463;&#36190;&#35465;&#30340;Adam&#31639;&#27861;&#30340;&#26032;&#39062;&#25913;&#36827;&#12290;StochGradAdam&#30340;&#26680;&#24515;&#26159;&#20854;&#26799;&#24230;&#25277;&#26679;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#30830;&#20445;&#31283;&#23450;&#25910;&#25947;&#65292;&#32780;&#19988;&#21033;&#29992;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#20943;&#36731;&#22122;&#22768;&#25110;&#24322;&#24120;&#25968;&#25454;&#30340;&#24433;&#21709;&#21644;&#22686;&#24378;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#25552;&#21319;&#20102;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;StochGradAdam&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;Adam&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#31934;&#24515;&#36873;&#25321;&#19968;&#37096;&#20998;&#26799;&#24230;&#36827;&#34892;&#25277;&#26679;&#65292;&#35813;&#20248;&#21270;&#22120;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22797;&#26434;&#27169;&#22411;&#30340;&#31649;&#29702;&#12290;&#26412;&#25991;&#20174;&#25968;&#23398;&#22522;&#30784;&#21040;&#20559;&#24046;&#26657;&#27491;&#31574;&#30053;&#20840;&#38754;&#25506;&#35752;&#20102;StochGradAdam&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#25216;&#26415;&#30340;&#21487;&#26399;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing domain of deep learning optimization, this paper unveils the StochGradAdam optimizer, a novel adaptation of the well-regarded Adam algorithm. Central to StochGradAdam is its gradient sampling technique. This method not only ensures stable convergence but also leverages the advantages of selective gradient consideration, fostering robust training by potentially mitigating the effects of noisy or outlier data and enhancing the exploration of the loss landscape for more dependable convergence. In both image classification and segmentation tasks, StochGradAdam has demonstrated superior performance compared to the traditional Adam optimizer. By judiciously sampling a subset of gradients at each iteration, the optimizer is optimized for managing intricate models. The paper provides a comprehensive exploration of StochGradAdam's methodology, from its mathematical foundations to bias correction strategies, heralding a promising advancement in deep learning training tec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36719;&#21270;&#30340;&#36880;&#28857;&#26426;&#21046;&#65288;SoftAD&#65289;&#26469;&#23454;&#29616;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#20943;&#23569;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#24182;&#20445;&#30041;&#19978;&#21319;-&#19979;&#38477;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.10006</link><description>&lt;p&gt;
&#36890;&#36807;&#36719;&#19978;&#21319;-&#19979;&#38477;&#23454;&#29616;&#38544;&#24335;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Implicit regularization via soft ascent-descent. (arXiv:2310.10006v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36719;&#21270;&#30340;&#36880;&#28857;&#26426;&#21046;&#65288;SoftAD&#65289;&#26469;&#23454;&#29616;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#20943;&#23569;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#24182;&#20445;&#30041;&#19978;&#21319;-&#19979;&#38477;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#21644;&#22797;&#26434;&#65292;&#36890;&#36807;&#26368;&#23567;&#30340;&#35797;&#38169;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#31163;&#32447;&#27867;&#21270;&#23545;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#30340;&#21487;&#38752;&#24615;&#21644;&#32463;&#27982;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#23547;&#27714;&#8220;&#24179;&#22374;&#8221;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#20195;&#29702;&#65292;&#26799;&#24230;&#27491;&#21017;&#21270;&#26159;&#19968;&#26465;&#33258;&#28982;&#30340;&#36884;&#24452;&#65292;&#19968;&#38454;&#36817;&#20284;&#26041;&#27861;&#22914;Floding&#21644;Sharpness-Aware Minimization (SAM) &#24050;&#32463;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#36229;&#21442;&#25968;&#65288;&#27946;&#27700;&#38408;&#20540;&#21644;&#37051;&#22495;&#21322;&#24452;&#65289;&#65292;&#36825;&#20123;&#36229;&#21442;&#25968;&#19981;&#23481;&#26131;&#20107;&#20808;&#30830;&#23450;&#12290;&#20026;&#20102;&#24320;&#21457;&#19968;&#20010;&#23545;&#38169;&#35823;&#36229;&#21442;&#25968;&#26356;&#20855;&#38887;&#24615;&#30340;&#36807;&#31243;&#65292;&#21463;Flooding&#20013;&#20351;&#29992;&#30340;&#30828;&#38408;&#20540;&#8220;&#19978;&#21319;-&#19979;&#38477;&#8221;&#24320;&#20851;&#35013;&#32622;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#21270;&#30340;&#36880;&#28857;&#26426;&#21046;&#65292;&#31216;&#20026;SoftAD&#65292;&#23427;&#23545;&#36793;&#30028;&#19978;&#30340;&#28857;&#36827;&#34892;&#38477;&#26435;&#65292;&#38480;&#21046;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#65292;&#24182;&#20445;&#30041;&#19978;&#21319;-&#19979;&#38477;&#25928;&#24212;&#12290;&#25105;&#20204;&#23558;&#24418;&#24335;&#30340;&#24179;&#31283;&#24615;&#20445;&#35777;&#19982;Flooding&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
As models grow larger and more complex, achieving better off-sample generalization with minimal trial-and-error is critical to the reliability and economy of machine learning workflows. As a proxy for the well-studied heuristic of seeking "flat" local minima, gradient regularization is a natural avenue, and first-order approximations such as Flooding and sharpness-aware minimization (SAM) have received significant attention, but their performance depends critically on hyperparameters (flood threshold and neighborhood radius, respectively) that are non-trivial to specify in advance. In order to develop a procedure which is more resilient to misspecified hyperparameters, with the hard-threshold "ascent-descent" switching device used in Flooding as motivation, we propose a softened, pointwise mechanism called SoftAD that downweights points on the borderline, limits the effects of outliers, and retains the ascent-descent effect. We contrast formal stationarity guarantees with those for Flo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedNGMs&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#27010;&#29575;&#31070;&#32463;&#22270;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#31169;&#23494;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11680</link><description>&lt;p&gt;
&#20855;&#26377;&#31070;&#32463;&#22270;&#27169;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Neural Graphical Models. (arXiv:2309.11680v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedNGMs&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#27010;&#29575;&#31070;&#32463;&#22270;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#31169;&#23494;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#35299;&#20915;&#20102;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#20445;&#30041;&#23545;&#25968;&#25454;&#30340;&#29420;&#21344;&#25511;&#21046;&#30340;&#21516;&#26102;&#65292;&#22522;&#20110;&#19987;&#26377;&#25968;&#25454;&#21019;&#24314;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#36817;&#26399;&#25552;&#20986;&#30340;&#31070;&#32463;&#22270;&#27169;&#22411;&#65288;NGMs&#65289;&#26159;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#23398;&#20064;&#36755;&#20837;&#29305;&#24449;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#12290;&#23427;&#20204;&#23398;&#20250;&#25429;&#25417;&#24213;&#23618;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#30340;&#25512;&#29702;&#21644;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;FL&#26694;&#26550;&#65292;&#23427;&#32500;&#25252;&#19968;&#20010;&#20840;&#23616;&#30340;NGM&#27169;&#22411;&#65292;&#20174;&#26412;&#22320;NGM&#27169;&#22411;&#20013;&#23398;&#20064;&#21040;&#24179;&#22343;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#22312;&#23458;&#25143;&#31471;&#30340;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;FedNGMs&#36991;&#20813;&#20102;&#31070;&#32463;&#20803;&#21305;&#37197;&#26694;&#26550;&#65288;&#22914;&#32852;&#37030;&#21305;&#37197;&#24179;&#22343;&#65289;&#20013;&#27169;&#22411;&#21442;&#25968;&#29190;&#28856;&#30340;&#32570;&#28857;&#21644;&#19981;&#36275;&#12290;&#25105;&#20204;&#30340;&#20840;&#23616;&#27169;&#22411;&#22823;&#23567;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) addresses the need to create models based on proprietary data in such a way that multiple clients retain exclusive control over their data, while all benefit from improved model accuracy due to pooled resources. Recently proposed Neural Graphical Models (NGMs) are Probabilistic Graphical models that utilize the expressive power of neural networks to learn complex non-linear dependencies between the input features. They learn to capture the underlying data distribution and have efficient algorithms for inference and sampling. We develop a FL framework which maintains a global NGM model that learns the averaged information from the local NGM models while keeping the training data within the client's environment. Our design, FedNGMs, avoids the pitfalls and shortcomings of neuron matching frameworks like Federated Matched Averaging that suffers from model parameter explosion. Our global model size remains constant throughout the process. In the cases where clients 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22312;&#26465;&#20214;&#29420;&#31435;&#22270;&#19978;&#36827;&#34892;&#30693;&#35782;&#20256;&#25773;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#22312;Cora&#21644;PubMed&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.05857</link><description>&lt;p&gt;
&#22312;&#26465;&#20214;&#29420;&#31435;&#22270;&#19978;&#30340;&#30693;&#35782;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Knowledge Propagation over Conditional Independence Graphs. (arXiv:2308.05857v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05857
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22312;&#26465;&#20214;&#29420;&#31435;&#22270;&#19978;&#36827;&#34892;&#30693;&#35782;&#20256;&#25773;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#22312;Cora&#21644;PubMed&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29420;&#31435;&#65288;CI&#65289;&#22270;&#26159;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGM&#65289;&#65292;&#20854;&#20013;&#29305;&#24449;&#36830;&#25509;&#20351;&#29992;&#26080;&#21521;&#22270;&#24314;&#27169;&#65292;&#36793;&#26435;&#37325;&#34920;&#31034;&#29305;&#24449;&#20043;&#38388;&#30340;&#37096;&#20998;&#30456;&#20851;&#24615;&#24378;&#24230;&#12290;&#30001;&#20110;CI&#22270;&#25429;&#25417;&#20102;&#29305;&#24449;&#20043;&#38388;&#30340;&#30452;&#25509;&#20381;&#36182;&#20851;&#31995;&#65292;&#23427;&#20204;&#22312;&#30740;&#31350;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#21457;&#29616;&#39046;&#22495;&#25299;&#25169;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;CI&#22270;&#19978;&#25191;&#34892;&#30693;&#35782;&#20256;&#25773;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#20844;&#24320;&#30340;Cora&#21644;PubMed&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional Independence (CI) graph is a special type of a Probabilistic Graphical Model (PGM) where the feature connections are modeled using an undirected graph and the edge weights show the partial correlation strength between the features. Since the CI graphs capture direct dependence between features, they have been garnering increasing interest within the research community for gaining insights into the systems from various domains, in particular discovering the domain topology. In this work, we propose algorithms for performing knowledge propagation over the CI graphs. Our experiments demonstrate that our techniques improve upon the state-of-the-art on the publicly available Cora and PubMed datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;SMARLA&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;&#65292;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#23545;&#26234;&#33021;&#20307;&#29366;&#24577;&#30340;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#12290;&#32463;&#39564;&#35777;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21487;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.02594</link><description>&lt;p&gt;
SMARLA&#65306;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents. (arXiv:2308.02594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;SMARLA&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;&#65292;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#23545;&#26234;&#33021;&#20307;&#29366;&#24577;&#30340;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#12290;&#32463;&#39564;&#35777;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21487;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;(DRL)&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#12290;&#30830;&#20445;DRL&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#24615;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#38752;&#27979;&#35797;&#26159;&#19981;&#36275;&#20197;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#33021;&#25552;&#20379;&#20445;&#35777;&#12290;&#26500;&#24314;&#23433;&#20840;&#30417;&#27979;&#22120;&#26159;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SMARLA&#65292;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;&#65292;&#19987;&#20026;DRL&#26234;&#33021;&#20307;&#35774;&#35745;&#12290;&#20986;&#20110;&#23454;&#38469;&#21407;&#22240;&#65292;SMARLA&#34987;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;(&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#35775;&#38382;&#26234;&#33021;&#20307;&#30340;&#20869;&#37096;)&#65292;&#24182;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#26469;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#20174;&#32780;&#20419;&#36827;&#20174;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#23398;&#20064;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30693;&#21517;&#30340;RL&#26696;&#20363;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;SMARLA&#12290;&#32463;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#35823;&#25253;&#29575;&#20302;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#19968;&#21322;&#24038;&#21491;&#30340;&#26089;&#26399;&#38454;&#27573;&#39044;&#27979;&#23433;&#20840;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning algorithms (DRL) are increasingly being used in safety-critical systems. Ensuring the safety of DRL agents is a critical concern in such contexts. However, relying solely on testing is not sufficient to ensure safety as it does not offer guarantees. Building safety monitors is one solution to alleviate this challenge. This paper proposes SMARLA, a machine learning-based safety monitoring approach designed for DRL agents. For practical reasons, SMARLA is designed to be black-box (as it does not require access to the internals of the agent) and leverages state abstraction to reduce the state space and thus facilitate the learning of safety violation prediction models from agent's states. We validated SMARLA on two well-known RL case studies. Empirical analysis reveals that SMARLA achieves accurate violation prediction with a low false positive rate, and can predict safety violations at an early stage, approximately halfway through the agent's execution before 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#23545;&#22810;&#65288;OvO&#65289;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#36229;&#36807;&#19977;&#20010;&#27169;&#24577;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.05435</link><description>&lt;p&gt;
One-Versus-Others Attention: &#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
One-Versus-Others Attention: Scalable Multimodal Integration. (arXiv:2307.05435v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05435
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#23545;&#22810;&#65288;OvO&#65289;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#36229;&#36807;&#19977;&#20010;&#27169;&#24577;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#23398;&#20064;&#27169;&#22411;&#22312;&#38382;&#39064;&#22238;&#31572;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#21508;&#31181;&#20219;&#21153;&#19978;&#36229;&#36234;&#21333;&#27169;&#24577;&#26041;&#27861;&#65292;&#22810;&#27169;&#24577;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20165;&#20851;&#27880;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#65292;&#20854;&#20013;&#27169;&#24577;&#25968;&#36890;&#24120;&#23569;&#20110;&#22235;&#20010;&#65288;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#20854;&#20182;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#39046;&#22495;&#65292;&#25968;&#25454;&#36755;&#20837;&#21487;&#33021;&#21253;&#25324;X&#23556;&#32447;&#12289;PET&#25195;&#25551;&#12289;MRI&#12289;&#36951;&#20256;&#31579;&#26597;&#12289;&#20020;&#24202;&#31508;&#35760;&#31561;&#65292;&#36825;&#23601;&#38656;&#35201;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#20449;&#24687;&#34701;&#21512;&#12290;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20381;&#36182;&#20110;&#20004;&#20004;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#65292;&#20294;&#23545;&#20110;&#36229;&#36807;&#19977;&#20010;&#27169;&#24577;&#30340;&#24212;&#29992;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20250;&#24456;&#22909;&#22320;&#25193;&#23637;&#12290;&#23545;&#20110;$n$&#20010;&#27169;&#24577;&#65292;&#35745;&#31639;&#27880;&#24847;&#21147;&#23558;&#23548;&#33268;$n \choose 2$&#30340;&#22797;&#26434;&#24230;&#65292;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#20013;&#31435;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21363;&#19968;&#23545;&#22810;&#65288;OvO&#65289;&#27880;&#24847;&#21147;&#65292;&#35813;&#26426;&#21046;&#38543;&#30528;&#27169;&#24577;&#25968;&#37327;&#32447;&#24615;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning models have become increasingly important as they surpass single-modality approaches on diverse tasks ranging from question-answering to autonomous driving. Despite the importance of multimodal learning, existing efforts focus on NLP applications, where the number of modalities is typically less than four (audio, video, text, images). However, data inputs in other domains, such as the medical field, may include X-rays, PET scans, MRIs, genetic screening, clinical notes, and more, creating a need for both efficient and accurate information fusion. Many state-of-the-art models rely on pairwise cross-modal attention, which does not scale well for applications with more than three modalities. For $n$ modalities, computing attention will result in $n \choose 2$ operations, potentially requiring considerable amounts of computational resources. To address this, we propose a new domain-neutral attention mechanism, One-Versus-Others (OvO) attention, that scales linearly with
&lt;/p&gt;</description></item><item><title>FDINet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21033;&#29992;&#29305;&#24449;&#22833;&#30495;&#25351;&#25968;&#26469;&#20445;&#25252;DNN&#27169;&#22411;&#20813;&#21463;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#65292;&#24182;&#21033;&#29992;FDI&#30456;&#20284;&#24615;&#26469;&#35782;&#21035;&#20998;&#24067;&#24335;&#25552;&#21462;&#25915;&#20987;&#20013;&#30340;&#21246;&#32467;&#25932;&#20154;&#12290;</title><link>http://arxiv.org/abs/2306.11338</link><description>&lt;p&gt;
FDINet&#65306;&#21033;&#29992;&#29305;&#24449;&#22833;&#30495;&#25351;&#25968;&#20445;&#25252; DNN &#27169;&#22411;&#20813;&#21463;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
FDINet: Protecting against DNN Model Extraction via Feature Distortion Index. (arXiv:2306.11338v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11338
&lt;/p&gt;
&lt;p&gt;
FDINet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21033;&#29992;&#29305;&#24449;&#22833;&#30495;&#25351;&#25968;&#26469;&#20445;&#25252;DNN&#27169;&#22411;&#20813;&#21463;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#65292;&#24182;&#21033;&#29992;FDI&#30456;&#20284;&#24615;&#26469;&#35782;&#21035;&#20998;&#24067;&#24335;&#25552;&#21462;&#25915;&#20987;&#20013;&#30340;&#21246;&#32467;&#25932;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21363;&#26381;&#21153;&#65288;MLaaS&#65289;&#24179;&#21488;&#30001;&#20110;&#20854;&#26131;&#29992;&#24615;&#12289;&#25104;&#26412;&#25928;&#30410;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#24555;&#36895;&#24320;&#21457;&#33021;&#21147;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102; MLaaS &#20013;&#22522;&#20110;&#20113;&#30340;&#27169;&#22411;&#23545;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; FDINET&#65292;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#29305;&#24449;&#20998;&#24067;&#30340;&#26032;&#39062;&#38450;&#24481;&#26426;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#25163;&#30340;&#26597;&#35810;&#30340;&#29305;&#24449;&#20998;&#24067;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#20123;&#26597;&#35810;&#30340;&#29305;&#24449;&#20998;&#24067;&#19982;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#19981;&#21516;&#12290;&#22522;&#20110;&#36825;&#20010;&#20851;&#38190;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#22833;&#30495;&#25351;&#25968;&#65288;FDI&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24230;&#37327;&#35774;&#35745;&#65292;&#29992;&#20110;&#23450;&#37327;&#27979;&#37327;&#25509;&#25910;&#21040;&#30340;&#26597;&#35810;&#30340;&#29305;&#24449;&#20998;&#24067;&#20559;&#24046;&#12290;&#25152;&#25552;&#20986;&#30340; FDINET &#21033;&#29992; FDI &#35757;&#32451;&#19968;&#20010;&#20108;&#36827;&#21046;&#26816;&#27979;&#22120;&#65292;&#24182;&#21033;&#29992; FDI &#30456;&#20284;&#24615;&#35782;&#21035;&#20998;&#24067;&#24335;&#25552;&#21462;&#25915;&#20987;&#20013;&#30340;&#21246;&#32467;&#25932;&#20154;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#26469;&#35780;&#20272; FDINET &#23545;&#25239;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning as a Service (MLaaS) platforms have gained popularity due to their accessibility, cost-efficiency, scalability, and rapid development capabilities. However, recent research has highlighted the vulnerability of cloud-based models in MLaaS to model extraction attacks. In this paper, we introduce FDINET, a novel defense mechanism that leverages the feature distribution of deep neural network (DNN) models. Concretely, by analyzing the feature distribution from the adversary's queries, we reveal that the feature distribution of these queries deviates from that of the model's training set. Based on this key observation, we propose Feature Distortion Index (FDI), a metric designed to quantitatively measure the feature distribution deviation of received queries. The proposed FDINET utilizes FDI to train a binary detector and exploits FDI similarity to identify colluding adversaries from distributed extraction attacks. We conduct extensive experiments to evaluate FDINET against
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SEILO&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#26631;&#20934;&#30340;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#21644;&#36870;&#21160;&#21147;&#23398;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#20174;&#26080;&#19987;&#23478;&#25968;&#25454;&#30340;&#35266;&#27979;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#31574;&#30053;&#27169;&#20223;&#23398;&#20064;&#65292;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#24182;&#23454;&#29616;&#20102;&#19987;&#23478;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.09805</link><description>&lt;p&gt;
&#35266;&#27979;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#31574;&#30053;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient On-Policy Imitation Learning from Observations. (arXiv:2306.09805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SEILO&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#26631;&#20934;&#30340;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#21644;&#36870;&#21160;&#21147;&#23398;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#20174;&#26080;&#19987;&#23478;&#25968;&#25454;&#30340;&#35266;&#27979;&#20013;&#30340;&#26679;&#26412;&#39640;&#25928;&#31574;&#30053;&#27169;&#20223;&#23398;&#20064;&#65292;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#24182;&#23454;&#29616;&#20102;&#19987;&#23478;&#27700;&#24179;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#65292;&#27169;&#20223;&#23398;&#20064; (ILD) &#26088;&#22312;&#36890;&#36807;&#28040;&#38500;&#24378;&#21270;&#23398;&#20064;&#30340;&#35768;&#22810;&#32570;&#28857;&#26469;&#24110;&#21161;&#23398;&#20064;&#36755;&#20986;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22810;&#25968;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#32570;&#20047;&#19987;&#23478;&#34892;&#21160;&#25351;&#23548;&#65292;&#22240;&#27492;&#26080;&#27861;&#20351;&#29992;ILD&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#32771;&#34385;&#35266;&#27979;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064; (ILO)&#65292;&#20854;&#20013;&#27809;&#26377;&#25552;&#20379;&#19987;&#23478;&#21160;&#20316;&#65292;&#20351;&#20854;&#25104;&#20026;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#31574;&#30053;&#23398;&#20064;&#65292;&#36825;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#25104;&#26412;&#26114;&#36149;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; SEILO&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26679;&#26412;&#39640;&#25928;&#31574;&#30053;&#31639;&#27861;&#65292;&#29992;&#20110; ILO&#65292;&#23558;&#26631;&#20934;&#30340;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#19982;&#36870;&#21160;&#21147;&#23398;&#24314;&#27169;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#23545;&#25239;&#31243;&#24207;&#21644;&#34892;&#20026;&#20811;&#38534;&#25439;&#22833;&#20013;&#33719;&#24471;&#21453;&#39304;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31574;&#30053; ILO &#21644; ILD &#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#38656;&#35201;&#36739;&#23569;&#30340;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#23454;&#29616;&#19987;&#23478;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning from demonstrations (ILD) aims to alleviate numerous shortcomings of reinforcement learning through the use of demonstrations. However, in most real-world applications, expert action guidance is absent, making the use of ILD impossible. Instead, we consider imitation learning from observations (ILO), where no expert actions are provided, making it a significantly more challenging problem to address. Existing methods often employ on-policy learning, which is known to be sample-costly. This paper presents SEILO, a novel sample-efficient on-policy algorithm for ILO, that combines standard adversarial imitation learning with inverse dynamics modeling. This approach enables the agent to receive feedback from both the adversarial procedure and a behavior cloning loss. We empirically demonstrate that our proposed algorithm requires fewer interactions with the environment to achieve expert performance compared to other state-of-the-art on-policy ILO and ILD methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#20272;&#35745;&#28508;&#22312;&#30340;&#20302;&#31209;&#24352;&#37327;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#22312;&#22788;&#29702;&#36830;&#32493;&#25110;&#20998;&#31867;&#21464;&#37327;&#26102;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#23581;&#35797;&#20102;&#20004;&#20010;&#20855;&#20307;&#30340;&#24212;&#29992;&#65292;&#21363;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#21644;&#22312;&#32447;&#20108;&#20803;&#24352;&#37327;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#36880;&#20010;&#26465;&#30446;&#30340;&#31934;&#30830;&#38169;&#35823;&#30028;&#38480;&#65292;&#36825;&#26159;&#22312;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#20013;&#39318;&#27425;&#32435;&#20837;&#22122;&#22768;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#21644;&#32479;&#35745;&#26041;&#38754;&#23384;&#22312;&#30528;&#20196;&#20154;&#24778;&#35766;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.03372</link><description>&lt;p&gt;
&#22312;&#32447;&#24352;&#37327;&#23398;&#20064;&#65306;&#35745;&#31639;&#21644;&#32479;&#35745;&#26435;&#34913;&#65292;&#36866;&#24212;&#24615;&#21644;&#26368;&#20248;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Online Tensor Learning: Computational and Statistical Trade-offs, Adaptivity and Optimal Regret. (arXiv:2306.03372v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#20272;&#35745;&#28508;&#22312;&#30340;&#20302;&#31209;&#24352;&#37327;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#22312;&#22788;&#29702;&#36830;&#32493;&#25110;&#20998;&#31867;&#21464;&#37327;&#26102;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22312;&#32447;&#24773;&#20917;&#19979;&#23581;&#35797;&#20102;&#20004;&#20010;&#20855;&#20307;&#30340;&#24212;&#29992;&#65292;&#21363;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#21644;&#22312;&#32447;&#20108;&#20803;&#24352;&#37327;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#36880;&#20010;&#26465;&#30446;&#30340;&#31934;&#30830;&#38169;&#35823;&#30028;&#38480;&#65292;&#36825;&#26159;&#22312;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#20013;&#39318;&#27425;&#32435;&#20837;&#22122;&#22768;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#21644;&#32479;&#35745;&#26041;&#38754;&#23384;&#22312;&#30528;&#20196;&#20154;&#24778;&#35766;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32447;&#24773;&#20917;&#19979;&#20272;&#35745;&#28508;&#22312;&#30340;&#20302;&#31209;&#24352;&#37327;&#65292;&#21253;&#25324;&#32447;&#24615;&#21644;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#22788;&#29702;&#36830;&#32493;&#25110;&#20998;&#31867;&#21464;&#37327;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#20855;&#20307;&#30340;&#24212;&#29992;&#65306;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#21644;&#22312;&#32447;&#20108;&#20803;&#24352;&#37327;&#23398;&#20064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32447;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#22312;&#25152;&#26377;&#24212;&#29992;&#31243;&#24207;&#20013;&#37117;&#21487;&#20197;&#26681;&#25454;&#36866;&#24403;&#30340;&#26465;&#20214;&#32447;&#24615;&#25910;&#25947;&#24182;&#24674;&#22797;&#20302;&#31209;&#32452;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#22312;&#32447;&#24352;&#37327;&#34917;&#20840;&#24314;&#31435;&#20102;&#31934;&#30830;&#30340;&#36880;&#20010;&#26465;&#30446;&#38169;&#35823;&#30028;&#38480;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20195;&#34920;&#20102;&#39318;&#27425;&#23581;&#35797;&#22312;&#22312;&#32447;&#20302;&#31209;&#24352;&#37327;&#24674;&#22797;&#20219;&#21153;&#20013;&#32435;&#20837;&#22122;&#22768;&#30340;&#21162;&#21147;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#35745;&#31639;&#21644;&#32479;&#35745;&#26041;&#38754;&#23384;&#22312;&#30528;&#20196;&#20154;&#24778;&#35766;&#30340;&#26435;&#34913;&#12290;&#22686;&#21152;&#27493;&#38271;&#21487;&#20197;&#21152;&#24555;&#25910;&#25947;&#65292;&#20294;&#20250;&#23548;&#33268;&#26356;&#39640;&#30340;&#32479;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate a generalized framework for estimating latent low-rank tensors in an online setting, encompassing both linear and generalized linear models. This framework offers a flexible approach for handling continuous or categorical variables. Additionally, we investigate two specific applications: online tensor completion and online binary tensor learning. To address these challenges, we propose the online Riemannian gradient descent algorithm, which demonstrates linear convergence and the ability to recover the low-rank component under appropriate conditions in all applications. Furthermore, we establish a precise entry-wise error bound for online tensor completion. Notably, our work represents the first attempt to incorporate noise in the online low-rank tensor recovery task. Intriguingly, we observe a surprising trade-off between computational and statistical aspects in the presence of noise. Increasing the step size accelerates convergence but leads to higher statistical error
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;PROVEXPLAINER&#26694;&#26550;&#65292;&#36890;&#36807;&#22797;&#21046;GNN-based security models&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#21033;&#29992;&#20915;&#31574;&#26641;&#21644;&#22270;&#32467;&#26500;&#29305;&#24449;&#23558;&#25277;&#35937;GNN&#20915;&#31574;&#36793;&#30028;&#25237;&#24433;&#21040;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20197;&#22686;&#24378;GNN&#23433;&#20840;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#35810;&#38382;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.00934</link><description>&lt;p&gt;
&#22522;&#20110;&#26435;&#23041;&#22270;&#32467;&#26500;&#29305;&#24449;&#23545;&#22522;&#20110;GNN&#30340;IDS&#26816;&#27979;&#36827;&#34892;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Interpreting GNN-based IDS Detections Using Provenance Graph Structural Features. (arXiv:2306.00934v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00934
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;PROVEXPLAINER&#26694;&#26550;&#65292;&#36890;&#36807;&#22797;&#21046;GNN-based security models&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#21033;&#29992;&#20915;&#31574;&#26641;&#21644;&#22270;&#32467;&#26500;&#29305;&#24449;&#23558;&#25277;&#35937;GNN&#20915;&#31574;&#36793;&#30028;&#25237;&#24433;&#21040;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20197;&#22686;&#24378;GNN&#23433;&#20840;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#35810;&#38382;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#26412;&#36136;&#22952;&#30861;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#36923;&#36753;&#35299;&#37322;&#21644;&#21487;&#25191;&#34892;&#21518;&#32493;&#34892;&#21160;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#22686;&#24378;&#22312;&#31995;&#32479;&#26469;&#28304;&#20998;&#26512;&#20013;&#20351;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23433;&#20840;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PROVEXPLAINER&#65292;&#19968;&#31181;&#23558;&#25277;&#35937;GNN&#20915;&#31574;&#36793;&#30028;&#25237;&#24433;&#21040;&#21487;&#35299;&#37322;&#29305;&#24449;&#31354;&#38388;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#31616;&#21333;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#22914;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#65292;&#22797;&#21046;&#22522;&#20110;GNN&#30340;&#23433;&#20840;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#26367;&#20195;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#20856;&#22270;&#35770;&#30340;&#22270;&#32467;&#26500;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23433;&#20840;&#39046;&#22495;&#30693;&#35782;&#30340;&#24191;&#27867;&#25968;&#25454;&#30740;&#31350;&#23545;&#20854;&#36827;&#34892;&#20102;&#22686;&#24378;&#12290;&#25105;&#20204;&#30340;&#22270;&#32467;&#26500;&#29305;&#24449;&#19982;&#31995;&#32479;&#26469;&#28304;&#39046;&#22495;&#20013;&#30340;&#38382;&#39064;&#31354;&#38388;&#34892;&#21160;&#23494;&#20999;&#30456;&#20851;&#65292;&#36825;&#20351;&#26816;&#27979;&#32467;&#26524;&#21487;&#29992;&#20154;&#31867;&#35821;&#35328;&#25551;&#36848;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The black-box nature of complex Neural Network (NN)-based models has hindered their widespread adoption in security domains due to the lack of logical explanations and actionable follow-ups for their predictions. To enhance the transparency and accountability of Graph Neural Network (GNN) security models used in system provenance analysis, we propose PROVEXPLAINER, a framework for projecting abstract GNN decision boundaries onto interpretable feature spaces.  We first replicate the decision-making process of GNNbased security models using simpler and explainable models such as Decision Trees (DTs). To maximize the accuracy and fidelity of the surrogate models, we propose novel graph structural features founded on classical graph theory and enhanced by extensive data study with security domain knowledge. Our graph structural features are closely tied to problem-space actions in the system provenance domain, which allows the detection results to be explained in descriptive, human languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38416;&#36848;&#20102;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.07063</link><description>&lt;p&gt;
&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Existential First Order Queries Inference on Knowledge Graphs. (arXiv:2304.07063v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#20102;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#35266;&#23519;&#21040;&#30340;&#20449;&#24687;&#26469;&#39044;&#27979;&#32570;&#22833;&#30340;&#20449;&#24687;&#12290;&#29305;&#21035;&#22320;&#65292;&#22238;&#31572;&#19968;&#38454;&#36923;&#36753;&#20844;&#24335;&#26159;&#29305;&#21035;&#24863;&#20852;&#36259;&#30340;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#28165;&#26224;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#32452;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#23558;&#36923;&#36753;&#36816;&#31639;&#35270;&#20026;&#38598;&#21512;&#36816;&#31639;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#30740;&#31350;&#36981;&#24490;&#30456;&#21516;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#32570;&#20047;&#20174;&#36923;&#36753;&#35282;&#24230;&#36827;&#34892;&#31995;&#32479;&#26816;&#26597;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20808;&#21069;&#30740;&#31350;&#35843;&#26597;&#30340;&#26597;&#35810;&#33539;&#22260;&#65292;&#24182;&#20934;&#30830;&#22320;&#30830;&#23450;&#20102;&#23427;&#19982;&#25972;&#20010;&#23384;&#22312;&#24615;&#20844;&#24335;&#23478;&#26063;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#26032;&#20844;&#24335;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#35752;&#35770;&#20102;&#21516;&#26102;&#20986;&#29616;&#30340;&#26032;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning on knowledge graphs is a challenging task because it utilizes observed information to predict the missing one. Specifically, answering first-order logic formulas is of particular interest because of its clear syntax and semantics. Recently, the query embedding method has been proposed which learns the embedding of a set of entities and treats logic operations as set operations. Though there has been much research following the same methodology, it lacks a systematic inspection from the standpoint of logic. In this paper, we characterize the scope of queries investigated previously and precisely identify the gap between it and the whole family of existential formulas. Moreover, we develop a new dataset containing ten new formulas and discuss the new challenges coming simultaneously. Finally, we propose a new search algorithm from fuzzy logic theory which is capable of solving new formulas and outperforming the previous methods in existing formulas.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29420;&#31435;&#22270;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#26041;&#27861;$\texttt{tGLAD}$&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26631;&#35782;&#26102;&#38388;&#24207;&#21015;&#30340;&#26377;&#24847;&#20041;&#30340;&#27573;&#12290;</title><link>http://arxiv.org/abs/2303.11647</link><description>&lt;p&gt;
&#20320;&#29992;$\texttt{tGLAD}$&#20102;&#21527;&#65311;&#26102;&#38388;&#20250;&#21578;&#35785;&#20320;&#65281;
&lt;/p&gt;
&lt;p&gt;
Are uGLAD? Time will tell!. (arXiv:2303.11647v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29420;&#31435;&#22270;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#26041;&#27861;$\texttt{tGLAD}$&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26631;&#35782;&#26102;&#38388;&#24207;&#21015;&#30340;&#26377;&#24847;&#20041;&#30340;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32463;&#24120;&#36935;&#21040;&#21608;&#22260;&#23384;&#22312;&#22810;&#20010;&#30456;&#20851;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#20363;&#22914;&#29992;&#20110;&#26816;&#26597;&#33041;&#27963;&#21160;&#21464;&#21270;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#25110;&#29992;&#20110;&#30417;&#27979;&#36523;&#20307;&#36816;&#21160;&#30340;&#20256;&#24863;&#22120;&#12290;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20998;&#21106;&#26159;&#19968;&#31181;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#20013;&#21547;&#20041;&#30340;&#27169;&#24335;&#25110;&#21464;&#21270;&#30340;&#25216;&#26415;&#65292;&#36825;&#21487;&#20197;&#26631;&#24535;&#31995;&#32479;&#34892;&#20026;&#30340;&#21464;&#21270;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#20998;&#21106;&#31639;&#27861;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#23427;&#20204;&#22312;&#22810;&#20803;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#20173;&#28982;&#19981;&#20196;&#20154;&#28385;&#24847;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29420;&#31435;&#65288;CI&#65289;&#22270;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#12290;CI&#22270;&#26159;&#34920;&#31034;&#33410;&#28857;&#20043;&#38388;&#30340;&#20559;&#30456;&#20851;&#20851;&#31995;&#30340;&#27010;&#29575;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#22810;&#20803;&#20998;&#21106;&#26694;&#26550;&#8220;$\texttt{tGLAD}$&#8221;&#65292;&#23427;&#23558;CI&#22270;&#33410;&#28857;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#21464;&#37327;&#36827;&#34892;&#24182;&#34892;&#12290;&#22312;CI&#22270;&#19978;&#24212;&#29992;&#22270;&#24418;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#26631;&#35782;&#26102;&#38388;&#24207;&#21015;&#30340;&#26377;&#24847;&#20041;&#30340;&#27573;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We frequently encounter multiple series that are temporally correlated in our surroundings, such as EEG data to examine alterations in brain activity or sensors to monitor body movements. Segmentation of multivariate time series data is a technique for identifying meaningful patterns or changes in the time series that can signal a shift in the system's behavior. However, most segmentation algorithms have been designed primarily for univariate time series, and their performance on multivariate data remains largely unsatisfactory, making this a challenging problem. In this work, we introduce a novel approach for multivariate time series segmentation using conditional independence (CI) graphs. CI graphs are probabilistic graphical models that represents the partial correlations between the nodes. We propose a domain agnostic multivariate segmentation framework `$\texttt{tGLAD}$' which draws a parallel between the CI graph nodes and the variables of the time series. Consider applying a gra
&lt;/p&gt;</description></item><item><title>&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#8212;&#8212;&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(IBNNs)&#12290;&#36825;&#31181;&#31639;&#27861;&#20351;&#29992;&#21487;&#20449;&#21306;&#38388;&#20808;&#39564;&#20998;&#24067;&#38598;&#21512;&#21644;&#20284;&#28982;&#20998;&#24067;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;BNNs&#65292;&#21487;&#20197;&#21306;&#20998;&#20808;&#39564;&#21644;&#21518;&#39564;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;IBNNs&#22312;&#36125;&#21494;&#26031;&#28789;&#25935;&#24230;&#20998;&#26512;&#26041;&#38754;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#23545;&#20998;&#24067;&#21464;&#21270;&#20063;&#26356;&#21152;&#40065;&#26834;&#12290;</title><link>http://arxiv.org/abs/2302.09656</link><description>&lt;p&gt;
&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Imprecise Bayesian Neural Networks. (arXiv:2302.09656v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09656
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#8212;&#8212;&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(IBNNs)&#12290;&#36825;&#31181;&#31639;&#27861;&#20351;&#29992;&#21487;&#20449;&#21306;&#38388;&#20808;&#39564;&#20998;&#24067;&#38598;&#21512;&#21644;&#20284;&#28982;&#20998;&#24067;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;BNNs&#65292;&#21487;&#20197;&#21306;&#20998;&#20808;&#39564;&#21644;&#21518;&#39564;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;IBNNs&#22312;&#36125;&#21494;&#26031;&#28789;&#25935;&#24230;&#20998;&#26512;&#26041;&#38754;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#23545;&#20998;&#24067;&#21464;&#21270;&#20063;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;, &#30830;&#23450;&#19981;&#30830;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#26159;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20351;&#24471;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#33021;&#22815;&#34987;&#35780;&#20272;&#65292;&#19981;&#21516;&#26469;&#28304;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#26080;&#27861;&#21306;&#20998;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;IBNNs&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#27010;&#25324;&#21644;&#20811;&#26381;&#26631;&#20934;BNNs&#30340;&#26576;&#20123;&#32570;&#28857;&#12290;&#26631;&#20934;BNNs&#20351;&#29992;&#21333;&#19968;&#30340;&#20808;&#39564;&#20998;&#24067;&#21644;&#20284;&#28982;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;IBNNs&#20351;&#29992;&#21487;&#20449;&#21306;&#38388;&#20808;&#39564;&#20998;&#24067;&#21644;&#20284;&#28982;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#20204;&#20801;&#35768;&#21306;&#20998;&#20808;&#39564;&#21644;&#21518;&#39564;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;IBNNs&#22312;&#36125;&#21494;&#26031;&#28789;&#25935;&#24230;&#20998;&#26512;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#23545;&#20998;&#24067;&#21464;&#21270;&#27604;&#26631;&#20934;BNNs&#26356;&#21152;&#40065;&#26834;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;PAC&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#32467;&#26524;&#38598;&#12290;&#25105;&#20204;&#23558;IBNNs&#24212;&#29992;&#20110;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65306;&#19968;&#20010;&#26159;&#20026;&#20102;&#20154;&#24037;&#33008;&#33146;&#25511;&#21046;&#27169;&#25311;&#34880;&#31958;&#21644;&#33008;&#23707;&#32032;&#21160;&#21147;&#23398;&#65292;&#21478;&#19968;&#20010;&#26159;&#36816;&#21160;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification and robustness to distribution shifts are important goals in machine learning and artificial intelligence. Although Bayesian neural networks (BNNs) allow for uncertainty in the predictions to be assessed, different sources of uncertainty are indistinguishable. We present imprecise Bayesian neural networks (IBNNs); they generalize and overcome some of the drawbacks of standard BNNs. These latter are trained using a single prior and likelihood distributions, whereas IBNNs are trained using credal prior and likelihood sets. They allow to distinguish between aleatoric and epistemic uncertainties, and to quantify them. In addition, IBNNs are robust in the sense of Bayesian sensitivity analysis, and are more robust than BNNs to distribution shift. They can also be used to compute sets of outcomes that enjoy PAC-like properties. We apply IBNNs to two case studies. One, to model blood glucose and insulin dynamics for artificial pancreas control, and two, for motion p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#33539;&#30068;&#35770;&#25506;&#31350;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#26368;&#23567;&#25152;&#38656;&#33021;&#21147;&#30340;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21644;&#36275;&#22815;&#30340;&#36164;&#28304;&#26469;&#35299;&#20915;&#21069;&#32622;&#20219;&#21153;&#25152;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#65292;&#21482;&#35201;&#20801;&#35768;&#24494;&#35843;&#19988;&#19979;&#28216;&#20219;&#21153;&#21487;&#22312;&#21069;&#32622;&#20219;&#21153;&#23450;&#20041;&#30340;&#33539;&#30068;&#20013;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2211.16327</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Power of Foundation Models. (arXiv:2211.16327v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#33539;&#30068;&#35770;&#25506;&#31350;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#26368;&#23567;&#25152;&#38656;&#33021;&#21147;&#30340;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21644;&#36275;&#22815;&#30340;&#36164;&#28304;&#26469;&#35299;&#20915;&#21069;&#32622;&#20219;&#21153;&#25152;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#65292;&#21482;&#35201;&#20801;&#35768;&#24494;&#35843;&#19988;&#19979;&#28216;&#20219;&#21153;&#21487;&#22312;&#21069;&#32622;&#20219;&#21153;&#23450;&#20041;&#30340;&#33539;&#30068;&#20013;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#22522;&#30784;&#27169;&#22411;&#20855;&#26377;&#26080;&#38480;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#28857;&#12289;&#26080;&#38480;&#35745;&#31639;&#33021;&#21147;&#12289;&#19968;&#20010;&#26080;&#38480;&#22823;&#30340;&#23436;&#32654;&#35757;&#32451;&#31639;&#27861;&#12289;&#20197;&#21450;&#22312;&#39044;&#35774;&#20219;&#21153;&#19978;&#20445;&#35777;&#38646;&#27867;&#21270;&#35823;&#24046;&#65292;&#37027;&#20040;&#23427;&#21487;&#20197;&#29992;&#20110;&#19968;&#20999;&#21527;&#65311;&#20256;&#32479;&#30340;&#34920;&#31034;&#12289;&#20248;&#21270;&#25110;&#27867;&#21270;&#29702;&#35770;&#26080;&#27861;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#25506;&#35752;&#30340;&#38382;&#39064;&#22312;&#36825;&#37324;&#37117;&#26159;&#19981;&#23384;&#22312;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#33539;&#30068;&#35770;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#20197;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19977;&#20010;&#32467;&#26524;&#65292;&#31532;&#19968;&#20010;&#38480;&#21046;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#21363;&#20165;&#24403;&#20219;&#21153;&#21487;&#34920;&#31034;&#26102;&#65292;&#27169;&#22411;&#25165;&#33021;&#29992;&#25552;&#31034;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65307;&#31532;&#20108;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;&#24494;&#35843;&#19981;&#21463;&#36825;&#20010;&#38480;&#21046;&#65292;&#22240;&#20026;&#19968;&#20010;&#20855;&#26377;&#26368;&#23567;&#25152;&#38656;&#33021;&#21147;&#65288;&#23545;&#31216;&#24615;&#65289;&#30340;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21644;&#36275;&#22815;&#30340;&#36164;&#28304;&#26469;&#29702;&#35770;&#19978;&#35299;&#20915;&#21069;&#32622;&#20219;&#21153;&#25152;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#32467;&#26524;&#21487;&#20197;&#30475;&#20316;&#26159;&#31532;&#20108;&#20010;&#32467;&#26524;&#30340;&#19968;&#33324;&#21270;&#65292;&#34920;&#26126;&#22914;&#26524;&#20801;&#35768;&#24494;&#35843;&#24182;&#19988;&#19979;&#28216;&#20219;&#21153;&#21487;&#22312;&#21069;&#32622;&#20219;&#21153;&#23450;&#20041;&#30340;&#33539;&#30068;&#20013;&#34920;&#31034;&#65292;&#21017;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#23567;&#33021;&#21147;&#20063;&#36275;&#20197;&#35299;&#20915;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
With infinitely many high-quality data points, infinite computational power, an infinitely large foundation model with a perfect training algorithm and guaranteed zero generalization error on the pretext task, can the model be used for everything? This question cannot be answered by the existing theory of representation, optimization or generalization, because the issues they mainly investigate are assumed to be nonexistent here. In this paper, we show that category theory provides powerful machinery to answer this question. We have proved three results. The first one limits the power of prompt-based learning, saying that the model can solve a downstream task with prompts if and only if the task is representable. The second one says fine tuning does not have this limit, as a foundation model with the minimum required power (up to symmetry) can theoretically solve downstream tasks for the category defined by pretext task, with fine tuning and enough resources. Our final result can be se
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26680;&#24046;&#24322;&#24230;&#37327;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#23558;&#30446;&#26631;&#20998;&#31163;&#20986;&#26469;&#65292;&#20197;&#21450;&#25511;&#21046;&#23545;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;$\mathbb{R}^d$&#19978;&#20351;&#29992;&#20102;&#36825;&#20123;&#32467;&#26524;&#26469;&#25193;&#23637;&#20102;&#26680;Stein&#24046;&#24322;&#20998;&#31163;&#21644;&#25910;&#25947;&#25511;&#21046;&#30340;&#24050;&#30693;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#33021;&#22815;&#31934;&#30830;&#24230;&#37327;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#30340;&#26680;&#24046;&#24322;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.12835</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#24046;&#24322;&#23454;&#29616;&#26377;&#38024;&#23545;&#24615;&#30340;&#20998;&#31163;&#19982;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Targeted Separation and Convergence with Kernel Discrepancies. (arXiv:2209.12835v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12835
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#24046;&#24322;&#24230;&#37327;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#23558;&#30446;&#26631;&#20998;&#31163;&#20986;&#26469;&#65292;&#20197;&#21450;&#25511;&#21046;&#23545;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;$\mathbb{R}^d$&#19978;&#20351;&#29992;&#20102;&#36825;&#20123;&#32467;&#26524;&#26469;&#25193;&#23637;&#20102;&#26680;Stein&#24046;&#24322;&#20998;&#31163;&#21644;&#25910;&#25947;&#25511;&#21046;&#30340;&#24050;&#30693;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#33021;&#22815;&#31934;&#30830;&#24230;&#37327;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#30340;&#26680;&#24046;&#24322;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMDs&#65289;&#22914;&#26680;Stein&#24046;&#24322;&#65288;KSD&#65289;&#24050;&#32463;&#25104;&#20026;&#24191;&#27867;&#24212;&#29992;&#30340;&#20013;&#24515;&#65292;&#21253;&#25324;&#20551;&#35774;&#26816;&#39564;&#12289;&#37319;&#26679;&#22120;&#36873;&#25321;&#12289;&#20998;&#24067;&#36817;&#20284;&#21644;&#21464;&#20998;&#25512;&#26029;&#12290;&#22312;&#27599;&#20010;&#35774;&#32622;&#20013;&#65292;&#36825;&#20123;&#22522;&#20110;&#26680;&#30340;&#24046;&#24322;&#24230;&#37327;&#38656;&#35201;&#23454;&#29616;&#65288;i&#65289;&#23558;&#30446;&#26631;P&#19982;&#20854;&#20182;&#27010;&#29575;&#27979;&#24230;&#20998;&#31163;&#65292;&#29978;&#33267;&#65288;ii&#65289;&#25511;&#21046;&#23545;P&#30340;&#24369;&#25910;&#25947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#30830;&#20445;&#65288;i&#65289;&#21644;&#65288;ii&#65289;&#30340;&#26032;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#12290;&#23545;&#20110;&#21487;&#20998;&#30340;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;MMDs&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20998;&#31163;Bochner&#21487;&#23884;&#20837;&#27979;&#24230;&#30340;&#26680;&#65292;&#24182;&#24341;&#20837;&#31616;&#21333;&#30340;&#26465;&#20214;&#26469;&#20998;&#31163;&#25152;&#26377;&#20855;&#26377;&#26080;&#30028;&#26680;&#30340;&#27979;&#24230;&#21644;&#29992;&#26377;&#30028;&#26680;&#26469;&#25511;&#21046;&#25910;&#25947;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#22312;$\mathbb{R}^d$&#19978;&#22823;&#22823;&#25193;&#23637;&#20102;KSD&#20998;&#31163;&#21644;&#25910;&#25947;&#25511;&#21046;&#30340;&#24050;&#30693;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#39318;&#20010;&#33021;&#22815;&#31934;&#30830;&#24230;&#37327;&#23545;P&#30340;&#24369;&#25910;&#25947;&#30340;KSDs&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum mean discrepancies (MMDs) like the kernel Stein discrepancy (KSD) have grown central to a wide range of applications, including hypothesis testing, sampler selection, distribution approximation, and variational inference. In each setting, these kernel-based discrepancy measures are required to (i) separate a target P from other probability measures or even (ii) control weak convergence to P. In this article we derive new sufficient and necessary conditions to ensure (i) and (ii). For MMDs on separable metric spaces, we characterize those kernels that separate Bochner embeddable measures and introduce simple conditions for separating all measures with unbounded kernels and for controlling convergence with bounded kernels. We use these results on $\mathbb{R}^d$ to substantially broaden the known conditions for KSD separation and convergence control and to develop the first KSDs known to exactly metrize weak convergence to P. Along the way, we highlight the implications of our res
&lt;/p&gt;</description></item></channel></rss>