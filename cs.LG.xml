<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;3D&#22320;&#38663;&#25968;&#25454;&#30340;&#26032;&#22411;&#25193;&#25955;&#27169;&#22411;&#37325;&#24314;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#22797;&#26434;&#32570;&#22833;&#27169;&#24335;&#26102;&#25552;&#39640;&#37325;&#24314;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.11482</link><description>&lt;p&gt;
SeisFusion: &#24102;&#26377;&#36755;&#20837;&#25351;&#23548;&#30340;&#21463;&#38480;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;3D&#22320;&#38663;&#25968;&#25454;&#25554;&#20540;&#21644;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
SeisFusion: Constrained Diffusion Model with Input Guidance for 3D Seismic Data Interpolation and Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11482
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;3D&#22320;&#38663;&#25968;&#25454;&#30340;&#26032;&#22411;&#25193;&#25955;&#27169;&#22411;&#37325;&#24314;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#22797;&#26434;&#32570;&#22833;&#27169;&#24335;&#26102;&#25552;&#39640;&#37325;&#24314;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#12289;&#29289;&#29702;&#25110;&#32463;&#27982;&#32422;&#26463;&#36890;&#24120;&#23548;&#33268;&#22320;&#38663;&#25968;&#25454;&#20013;&#23384;&#22312;&#32570;&#22833;&#30340;&#30165;&#36857;&#65292;&#20351;&#24471;&#37325;&#24314;&#23436;&#25972;&#30340;&#22320;&#38663;&#25968;&#25454;&#25104;&#20026;&#22320;&#38663;&#25968;&#25454;&#22788;&#29702;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20256;&#32479;&#30340;&#22320;&#38663;&#25968;&#25454;&#37325;&#24314;&#26041;&#27861;&#38656;&#35201;&#36873;&#25321;&#22810;&#20010;&#32463;&#39564;&#21442;&#25968;&#65292;&#24182;&#19988;&#38590;&#20197;&#22788;&#29702;&#22823;&#35268;&#27169;&#36830;&#32493;&#32570;&#22833;&#25968;&#25454;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#23637;&#31034;&#20986;&#24378;&#22823;&#30340;&#37325;&#24314;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20195;&#34920;&#20102;&#19968;&#31181;&#28857;&#23545;&#28857;&#30340;&#37325;&#24314;&#26041;&#27861;&#65292;&#21487;&#33021;&#26080;&#27861;&#35206;&#30422;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#24403;&#22788;&#29702;&#20855;&#26377;&#22797;&#26434;&#32570;&#22833;&#27169;&#24335;&#30340;&#22320;&#38663;&#25968;&#25454;&#26102;&#65292;&#36825;&#20123;&#32593;&#32476;&#21487;&#33021;&#20250;&#32463;&#21382;&#19981;&#21516;&#31243;&#24230;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;3D&#22320;&#38663;&#25968;&#25454;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#37325;&#24314;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11482v1 Announce Type: new  Abstract: Geographical, physical, or economic constraints often result in missing traces within seismic data, making the reconstruction of complete seismic data a crucial step in seismic data processing. Traditional methods for seismic data reconstruction require the selection of multiple empirical parameters and struggle to handle large-scale continuous missing data. With the development of deep learning, various neural networks have demonstrated powerful reconstruction capabilities. However, these convolutional neural networks represent a point-to-point reconstruction approach that may not cover the entire distribution of the dataset. Consequently, when dealing with seismic data featuring complex missing patterns, such networks may experience varying degrees of performance degradation. In response to this challenge, we propose a novel diffusion model reconstruction framework tailored for 3D seismic data. To constrain the results generated by the
&lt;/p&gt;</description></item><item><title>FedQNN&#26694;&#26550;&#34701;&#21512;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#19982;&#32463;&#20856;&#32852;&#37030;&#23398;&#20064;&#21407;&#21017;&#65292;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#21512;&#20316;&#23398;&#20064;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.10861</link><description>&lt;p&gt;
FedQNN: &#20351;&#29992;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedQNN: Federated Learning using Quantum Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10861
&lt;/p&gt;
&lt;p&gt;
FedQNN&#26694;&#26550;&#34701;&#21512;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#19982;&#32463;&#20856;&#32852;&#37030;&#23398;&#20064;&#21407;&#21017;&#65292;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#21512;&#20316;&#23398;&#20064;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#65288;QFL&#65289;&#30340;&#21019;&#26032;&#39046;&#22495;&#65292;&#20316;&#20026;&#36890;&#36807;&#20998;&#24067;&#24335;&#32593;&#32476;&#35757;&#32451;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#32852;&#37030;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;FedQNN&#65289;&#26694;&#26550;&#26159;&#19968;&#31181;&#23574;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;QML&#30340;&#29420;&#29305;&#29305;&#24449;&#19982;&#32463;&#20856;&#32852;&#37030;&#23398;&#20064;&#30340;&#21407;&#21017;&#30456;&#32467;&#21512;&#12290;&#36825;&#39033;&#24037;&#20316;&#24443;&#24213;&#35843;&#26597;&#20102;QFL&#65292;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#20445;&#25252;&#25968;&#25454;&#22788;&#29702;&#30340;&#33021;&#21147;&#65292;&#24182;&#20419;&#36827;&#21512;&#20316;&#23398;&#20064;&#32780;&#26080;&#38656;&#30452;&#25509;&#20849;&#20139;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#22522;&#22240;&#32452;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#65289;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#27010;&#24565;&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;FedQNN&#26694;&#26550;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;&#32467;&#26524;&#22312;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20445;&#25345;&#22312;86%&#20197;&#19978;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10861v1 Announce Type: cross  Abstract: In this study, we explore the innovative domain of Quantum Federated Learning (QFL) as a framework for training Quantum Machine Learning (QML) models via distributed networks. Conventional machine learning models frequently grapple with issues about data privacy and the exposure of sensitive information. Our proposed Federated Quantum Neural Network (FedQNN) framework emerges as a cutting-edge solution, integrating the singular characteristics of QML with the principles of classical federated learning. This work thoroughly investigates QFL, underscoring its capability to secure data handling in a distributed environment and facilitate cooperative learning without direct data sharing. Our research corroborates the concept through experiments across varied datasets, including genomics and healthcare, thereby validating the versatility and efficacy of our FedQNN framework. The results consistently exceed 86% accuracy across three distinct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#25506;&#35752;&#20102;&#22312;&#19968;&#26041;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#21518;&#25552;&#20379;&#32473;&#21478;&#19968;&#26041;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.18607</link><description>&lt;p&gt;
&#22312;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#20013;&#25506;&#35752;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#65306;&#19968;&#31181;&#23545;&#25239;&#24615;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An Adversarial Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#25506;&#35752;&#20102;&#22312;&#19968;&#26041;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#21518;&#25552;&#20379;&#32473;&#21478;&#19968;&#26041;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36817;&#24180;&#26469;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#20854;&#22312;&#37319;&#26679;&#36136;&#37327;&#21644;&#20998;&#24067;&#35206;&#30422;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#36328;&#19981;&#21516;&#32452;&#32455;&#20998;&#20139;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#24314;&#35758;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#21033;&#29992;&#29575;&#21516;&#26102;&#36890;&#36807;&#36991;&#20813;&#30452;&#25509;&#20998;&#20139;&#31169;&#20154;&#25968;&#25454;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#19982;&#36825;&#31181;&#26041;&#27861;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#35843;&#26597;&#12290;&#26412;&#25991;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#19982;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#30456;&#20851;&#30340;&#28508;&#22312;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#26041;&#65288;&#20998;&#20139;&#32773;&#65289;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#24182;&#21521;&#21478;&#19968;&#26041;&#65288;&#25509;&#25910;&#32773;&#65289;&#25552;&#20379;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#20139;&#32773;&#21487;&#20197;&#23454;&#34892;&#30340;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18607v1 Announce Type: cross  Abstract: Diffusion models have recently gained significant attention in both academia and industry due to their impressive generative performance in terms of both sampling quality and distribution coverage. Accordingly, proposals are made for sharing pre-trained diffusion models across different organizations, as a way of improving data utilization while enhancing privacy protection by avoiding sharing private data directly. However, the potential risks associated with such an approach have not been comprehensively examined.   In this paper, we take an adversarial perspective to investigate the potential privacy and fairness risks associated with the sharing of diffusion models. Specifically, we investigate the circumstances in which one party (the sharer) trains a diffusion model using private data and provides another party (the receiver) black-box access to the pre-trained model for downstream tasks. We demonstrate that the sharer can execut
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18603</link><description>&lt;p&gt;
MMSR&#65306;&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
MMSR: Symbolic Regression is a Multimodal Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18603
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#20844;&#24335;&#26159;&#25506;&#32034;&#33258;&#28982;&#35268;&#24459;&#20960;&#21315;&#24180;&#26469;&#20154;&#31867;&#26234;&#24935;&#30340;&#32467;&#26230;&#12290;&#29992;&#31616;&#27905;&#30340;&#25968;&#23398;&#20844;&#24335;&#25551;&#36848;&#22797;&#26434;&#30340;&#33258;&#28982;&#35268;&#24459;&#26159;&#31185;&#23398;&#23478;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#65292;&#20063;&#26159;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#19968;&#39046;&#22495;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#20174;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cobweb4V&#30340;&#26032;&#39062;&#35270;&#35273;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#31867;&#31867;&#20284;&#23398;&#20064;&#31995;&#32479;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#25928;&#24212;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#25104;&#26524;&#65292;&#24182;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16933</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#27010;&#24565;&#24418;&#25104;&#36991;&#20813;&#35270;&#35273;&#20998;&#31867;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Avoiding Catastrophic Forgetting in Visual Classification Using Human Concept Formation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16933
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cobweb4V&#30340;&#26032;&#39062;&#35270;&#35273;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#31867;&#31867;&#20284;&#23398;&#20064;&#31995;&#32479;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#25928;&#24212;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#25104;&#26524;&#65292;&#24182;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#28982;&#32780;&#65292;&#24403;&#25353;&#39034;&#24207;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#23427;&#20204;&#32463;&#24120;&#38754;&#20020;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Cobweb4V&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;&#20998;&#31867;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;Cobweb&#65292;&#36825;&#26159;&#19968;&#31181;&#20154;&#31867;&#31867;&#20284;&#30340;&#23398;&#20064;&#31995;&#32479;&#65292;&#21463;&#21040;&#20154;&#31867;&#38543;&#26102;&#38388;&#36880;&#28176;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;Cobweb4V&#22312;&#23398;&#20064;&#35270;&#35273;&#27010;&#24565;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#23398;&#20064;&#25104;&#26524;&#65292;&#38543;&#26102;&#38388;&#20445;&#25345;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#25928;&#24212;&#12290;&#36825;&#20123;&#29305;&#24449;&#19982;&#20154;&#31867;&#35748;&#30693;&#20013;&#30340;&#23398;&#20064;&#31574;&#30053;&#19968;&#33268;&#65292;&#23558;Cobweb4V&#23450;&#20301;&#20026;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16933v1 Announce Type: cross  Abstract: Deep neural networks have excelled in machine learning, particularly in vision tasks, however, they often suffer from catastrophic forgetting when learning new tasks sequentially. In this work, we propose Cobweb4V, a novel visual classification approach that builds on Cobweb, a human like learning system that is inspired by the way humans incrementally learn new concepts over time. In this research, we conduct a comprehensive evaluation, showcasing the proficiency of Cobweb4V in learning visual concepts, requiring less data to achieve effective learning outcomes compared to traditional methods, maintaining stable performance over time, and achieving commendable asymptotic behavior, without catastrophic forgetting effects. These characteristics align with learning strategies in human cognition, positioning Cobweb4V as a promising alternative to neural network approaches.
&lt;/p&gt;</description></item><item><title /><link>https://arxiv.org/abs/2402.07127</link><description>&lt;p&gt;
&#35266;&#23519;&#23398;&#20064;&#65306;&#22522;&#20110;&#35270;&#39057;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#23398;&#20064;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07127
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#25805;&#20316;&#25216;&#33021;&#21463;&#21040;&#22810;&#26679;&#21270;&#12289;&#26080;&#20559;&#30340;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#38382;&#39064;&#65292;&#20294;&#22312;&#27867;&#21270;&#24615;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#8220;&#37326;&#22806;&#8221;&#35270;&#39057;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#23384;&#22312;&#36890;&#36807;&#33258;&#30417;&#30563;&#25216;&#26415;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#36827;&#23637;&#12290;&#23558;&#36825;&#19968;&#28857;&#24212;&#29992;&#21040;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#34987;&#21160;&#35266;&#23519;&#26469;&#23398;&#20064;&#20016;&#23500;&#30340;&#22312;&#32447;&#35270;&#39057;&#20013;&#30340;&#25805;&#20316;&#25216;&#33021;&#12290;&#36825;&#31181;&#22522;&#20110;&#35270;&#39057;&#30340;&#23398;&#20064;&#33539;&#24335;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#23427;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#12290;&#26412;&#32508;&#36848;&#22238;&#39038;&#20102;&#35270;&#39057;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#12289;&#29289;&#20307;&#21487;&#34892;&#24615;&#29702;&#35299;&#12289;&#19977;&#32500;&#25163;&#37096;/&#36523;&#20307;&#24314;&#27169;&#21644;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#36164;&#28304;&#31561;&#22522;&#30784;&#30693;&#35782;&#65292;&#20197;&#21450;&#20174;&#19981;&#21463;&#25511;&#21046;&#30340;&#35270;&#39057;&#28436;&#31034;&#20013;&#33719;&#21462;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30340;&#26032;&#20852;&#25216;&#26415;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20165;&#20174;&#35266;&#23519;&#22823;&#35268;&#27169;&#20154;&#31867;&#35270;&#39057;&#20013;&#23398;&#20064;&#22914;&#20309;&#22686;&#24378;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot learning of manipulation skills is hindered by the scarcity of diverse, unbiased datasets. While curated datasets can help, challenges remain in generalizability and real-world transfer. Meanwhile, large-scale "in-the-wild" video datasets have driven progress in computer vision through self-supervised techniques. Translating this to robotics, recent works have explored learning manipulation skills by passively watching abundant videos sourced online. Showing promising results, such video-based learning paradigms provide scalable supervision while reducing dataset bias. This survey reviews foundations such as video feature representation learning techniques, object affordance understanding, 3D hand/body modeling, and large-scale robot resources, as well as emerging techniques for acquiring robot manipulation skills from uncontrolled video demonstrations. We discuss how learning only from observing large-scale human videos can enhance generalization and sample efficiency for roboti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#20013;&#30340;&#24635;&#26799;&#24230;&#26041;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#25209;&#27425;&#22823;&#23567;&#26377;&#21161;&#20110;&#20943;&#23567;&#21063;&#37319;&#27171;&#24341;&#36215;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20248;&#21270;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03990</link><description>&lt;p&gt;
&#21063;&#37319;&#27171;&#24182;&#19981;&#26159;&#39764;&#27861;: &#22823;&#25209;&#37327;&#22823;&#23567;&#28858;&#20160;&#40636;&#36969;&#29992;&#26044;&#24046;&#20998;&#38577;&#31169;&#38568;&#27231;&#20778;&#21270;
&lt;/p&gt;
&lt;p&gt;
Subsampling is not Magic: Why Large Batch Sizes Work for Differentially Private Stochastic Optimisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03990
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#20013;&#30340;&#24635;&#26799;&#24230;&#26041;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#25209;&#27425;&#22823;&#23567;&#26377;&#21161;&#20110;&#20943;&#23567;&#21063;&#37319;&#27171;&#24341;&#36215;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20248;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20497;&#30740;&#31350;&#20102;&#25209;&#27425;&#22823;&#23567;&#23565;&#24046;&#20998;&#38577;&#31169;&#38568;&#27231;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#20013;&#32317;&#26799;&#24230;&#26041;&#24046;&#30340;&#24433;&#38911;&#65292;&#23563;&#27714;&#23565;&#22823;&#25209;&#27425;&#22823;&#23567;&#26377;&#29992;&#24615;&#30340;&#29702;&#35542;&#35299;&#37323;&#12290;&#30001;&#26044;DP-SGD&#26159;&#29694;&#20195;&#24046;&#20998;&#38577;&#31169;&#28145;&#24230;&#23416;&#32722;&#30340;&#22522;&#30990;&#65292;&#20854;&#24615;&#36074;&#24050;&#34987;&#24291;&#27867;&#30740;&#31350;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#22312;&#23526;&#36368;&#20013;&#30332;&#29694;&#22823;&#25209;&#27425;&#22823;&#23567;&#26377;&#30410;&#12290;&#28982;&#32780;&#65292;&#23565;&#26044;&#36889;&#31278;&#22909;&#34389;&#30340;&#29702;&#35542;&#35299;&#37323;&#30446;&#21069;&#26368;&#22810;&#21482;&#33021;&#35498;&#26159;&#21855;&#30332;&#24335;&#30340;&#12290;&#25105;&#20497;&#39318;&#20808;&#35264;&#23519;&#21040;&#65292;&#22312;DP-SGD&#20013;&#65292;&#32317;&#26799;&#24230;&#26041;&#24046;&#21487;&#20197;&#20998;&#35299;&#28858;&#30001;&#21063;&#37319;&#27171;&#21644;&#22122;&#32882;&#24341;&#36215;&#30340;&#26041;&#24046;&#12290;&#28982;&#24460;&#65292;&#25105;&#20497;&#35657;&#26126;&#22312;&#28961;&#38480;&#27425;&#36845;&#20195;&#30340;&#26997;&#38480;&#24773;&#27841;&#19979;&#65292;&#26377;&#25928;&#30340;&#22122;&#32882;&#24341;&#36215;&#30340;&#26041;&#24046;&#23565;&#25209;&#27425;&#22823;&#23567;&#26159;&#19981;&#35722;&#30340;&#12290;&#21097;&#19979;&#30340;&#21063;&#37319;&#27171;&#24341;&#36215;&#30340;&#26041;&#24046;&#38568;&#33879;&#25209;&#27425;&#22823;&#23567;&#30340;&#22686;&#22823;&#32780;&#28187;&#23567;&#65292;&#22240;&#27492;&#22823;&#25209;&#27425;&#22823;&#23567;&#28187;&#23567;&#20102;&#26377;&#25928;&#30340;&#32317;&#26799;&#24230;&#26041;&#24046;&#12290;&#25105;&#20497;&#22312;&#25976;&#20540;&#19978;&#30906;&#35469;&#36889;&#31278;&#28472;&#36914;&#30340;&#24773;&#27841;&#22312;&#23526;&#38555;&#29872;&#22659;&#20013;&#26159;&#30456;&#38364;&#30340;&#65292;&#30070;&#25209;&#27425;&#22823;&#23567;&#19981;&#23567;&#30340;&#26178;&#20505;&#26371;&#36215;&#20316;&#29992;&#65292;&#20006;&#19988;&#30332;&#29694;
&lt;/p&gt;
&lt;p&gt;
We study the effect of the batch size to the total gradient variance in differentially private stochastic gradient descent (DP-SGD), seeking a theoretical explanation for the usefulness of large batch sizes. As DP-SGD is the basis of modern DP deep learning, its properties have been widely studied, and recent works have empirically found large batch sizes to be beneficial. However, theoretical explanations of this benefit are currently heuristic at best. We first observe that the total gradient variance in DP-SGD can be decomposed into subsampling-induced and noise-induced variances. We then prove that in the limit of an infinite number of iterations, the effective noise-induced variance is invariant to the batch size. The remaining subsampling-induced variance decreases with larger batch sizes, so large batches reduce the effective total gradient variance. We confirm numerically that the asymptotic regime is relevant in practical settings when the batch size is not small, and find tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#30340;&#26032;&#22411;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#24378;&#21270;&#23398;&#20064;&#21407;&#29702;&#30340;&#22810;&#21442;&#25968;&#22870;&#21169;&#20989;&#25968;&#65292;&#25104;&#21151;&#20248;&#21270;&#20102;&#23567;&#20998;&#23376;&#29983;&#25104;&#30340;&#33647;&#29289;&#26679;&#24615;&#20272;&#35745;&#65292;&#26368;&#39640;&#25552;&#21319;&#20102;30%&#12290;</title><link>https://arxiv.org/abs/2402.00014</link><description>&lt;p&gt;
&#28151;&#21512;&#37327;&#23376;&#24490;&#29615;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29992;&#20110;&#23567;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hybrid quantum cycle generative adversarial network for small molecule generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#30340;&#26032;&#22411;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#24378;&#21270;&#23398;&#20064;&#21407;&#29702;&#30340;&#22810;&#21442;&#25968;&#22870;&#21169;&#20989;&#25968;&#65292;&#25104;&#21151;&#20248;&#21270;&#20102;&#23567;&#20998;&#23376;&#29983;&#25104;&#30340;&#33647;&#29289;&#26679;&#24615;&#20272;&#35745;&#65292;&#26368;&#39640;&#25552;&#21319;&#20102;30%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#33647;&#29289;&#35774;&#35745;&#36807;&#31243;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#26469;&#24320;&#21457;&#39318;&#27425;&#36827;&#20837;&#24066;&#22330;&#30340;&#27599;&#20010;&#26032;&#21270;&#21512;&#29289;&#12290;&#29983;&#25104;&#23567;&#20998;&#23376;&#26159;&#33647;&#29289;&#21457;&#29616;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#23545;&#20110;&#24320;&#21457;&#21019;&#26032;&#30340;&#21046;&#33647;&#20135;&#21697;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#20805;&#20998;&#21457;&#25381;&#29420;&#29305;&#24615;&#12289;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#33647;&#29289;&#26679;&#24615;&#12289;&#21487;&#21512;&#25104;&#24615;&#21644;&#28342;&#35299;&#24230;&#20998;&#23376;&#33647;&#20195;&#21160;&#21147;&#23398;&#24615;&#36136;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20960;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#24037;&#31243;&#25972;&#21512;&#21040;&#24050;&#30693;&#20998;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#26032;&#22411;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#12290;&#24341;&#20837;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#20102;&#20197;&#24378;&#21270;&#23398;&#20064;&#21407;&#29702;&#20026;&#22522;&#30784;&#30340;&#26032;&#22411;&#22810;&#21442;&#25968;&#22870;&#21169;&#20989;&#25968;&#12290;&#36890;&#36807;&#22312;&#22522;&#20934;&#33647;&#29289;&#35774;&#35745;&#25968;&#25454;&#38598;QM9&#21644;PC9&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#26174;&#31034;&#20986;&#24341;&#20837;&#30340;&#27169;&#22411;&#20248;&#20110;&#20197;&#21069;&#30340;&#35780;&#20998;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#26032;&#30340;&#35780;&#20998;&#34920;&#26126;&#33647;&#29289;&#26679;&#24615;&#23450;&#37327;&#20272;&#35745;&#22686;&#21152;&#20102;&#26368;&#22810;30%&#12290;
&lt;/p&gt;
&lt;p&gt;
The contemporary drug design process demands considerable time and resources to develop each new compound entering the market. Generating small molecules is a pivotal aspect of drug discovery, essential for developing innovative pharmaceuticals. Uniqueness, validity, diversity, druglikeliness, synthesizability, and solubility molecular pharmacokinetic properties, however, are yet to be maximized. This work introduces several new generative adversarial network models based on engineering integration of parametrized quantum circuits into known molecular generative adversarial networks. The introduced machine learning models incorporate a new multi-parameter reward function grounded in reinforcement learning principles. Through extensive experimentation on benchmark drug design datasets, QM9 and PC9, the introduced models are shown to outperform scores achieved previously. Most prominently, the new scores indicate an increase of up to 30% in the druglikeness quantitative estimation. The n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35770;&#36848;&#20102;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#36755;&#20986;&#32423;&#21035;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#27604;&#36739;&#19968;&#33268;&#24615;&#27979;&#35797;&#24211;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2311.07466</link><description>&lt;p&gt;
&#20851;&#20110;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Measuring Faithfulness or Self-consistency of Natural Language Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35770;&#36848;&#20102;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#36755;&#20986;&#32423;&#21035;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#27604;&#36739;&#19968;&#33268;&#24615;&#27979;&#35797;&#24211;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#20107;&#21518;&#25110;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#35299;&#37322;&#20854;&#39044;&#27979;&#12290;&#20294;&#26159;&#65292;LLM&#21487;&#33021;&#20250;&#32534;&#36896;&#21548;&#36215;&#26469;&#21512;&#29702;&#20294;&#19981;&#24544;&#23454;&#20110;&#20854;&#22522;&#26412;&#25512;&#29702;&#30340;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#35774;&#35745;&#20102;&#26088;&#22312;&#21028;&#26029;&#20107;&#21518;&#25110;CoT&#35299;&#37322;&#24544;&#23454;&#24230;&#30340;&#27979;&#35797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#24544;&#23454;&#24230;&#27979;&#35797;&#19981;&#26159;&#34913;&#37327;&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#30340;&#24544;&#23454;&#24230;&#65292;&#32780;&#26159;&#34913;&#37327;&#20854;&#36755;&#20986;&#32423;&#21035;&#30340;&#33258;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;i&#65289;&#25105;&#20204;&#22312;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#32972;&#26223;&#19979;&#28548;&#28165;&#20102;&#24544;&#23454;&#24230;&#27979;&#35797;&#30340;&#22320;&#20301;&#65292;&#23558;&#20854;&#25551;&#36848;&#20026;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;ii&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#27604;&#36739;&#19968;&#33268;&#24615;&#30340;&#27979;&#35797;&#24211;&#65292;&#39318;&#27425;&#22312;11&#20010;&#24320;&#25918;&#24335;LLMs&#21644;5&#20010;&#20219;&#21153;&#30340;&#36890;&#29992;&#22871;&#20214;&#19978;&#27604;&#36739;&#20102;&#29616;&#26377;&#27979;&#35797;&#65292;&#21253;&#25324;iii&#65289;&#25105;&#20204;&#30340;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#12290;CC-SHAP&#26159;LLM&#33258;&#19968;&#33268;&#24615;&#30340;&#32454;&#31890;&#24230;&#24230;&#37327;&#65288;&#32780;&#19981;&#26159;&#27979;&#35797;&#65289;&#12290;&#23427;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36335;&#24452;&#37325;&#21152;&#26435;&#30340;&#23450;&#21046;&#22270;&#31070;&#32463;&#32593;&#32476;CustomGNN&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#39640;&#23618;&#35821;&#20041;&#65292;&#31361;&#20986;&#19982;&#35821;&#20041;&#30456;&#20851;&#30340;&#36335;&#24452;&#65292;&#24182;&#36807;&#28388;&#25481;&#20219;&#21153;&#26080;&#20851;&#30340;&#22122;&#38899;&#65292;&#36991;&#20813;&#20256;&#32479;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#12289;&#40065;&#26834;&#24615;&#24046;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2106.10866</link><description>&lt;p&gt;
&#20351;&#29992;&#36335;&#24452;&#37325;&#21152;&#26435;&#23450;&#21046;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Customizing Graph Neural Networks using Path Reweighting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2106.10866
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36335;&#24452;&#37325;&#21152;&#26435;&#30340;&#23450;&#21046;&#22270;&#31070;&#32463;&#32593;&#32476;CustomGNN&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#39640;&#23618;&#35821;&#20041;&#65292;&#31361;&#20986;&#19982;&#35821;&#20041;&#30456;&#20851;&#30340;&#36335;&#24452;&#65292;&#24182;&#36807;&#28388;&#25481;&#20219;&#21153;&#26080;&#20851;&#30340;&#22122;&#38899;&#65292;&#36991;&#20813;&#20256;&#32479;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#12289;&#40065;&#26834;&#24615;&#24046;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#24191;&#27867;&#29992;&#20110;&#25366;&#25496;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;GNNs&#24182;&#26410;&#21306;&#20998;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#65292;&#22240;&#27492;&#23427;&#20204;&#23884;&#20837;&#30340;&#23884;&#20837;&#21521;&#37327;&#24182;&#38750;&#24635;&#26159;&#26377;&#25928;&#12290;&#26412;&#25991;&#20197;&#22270;&#20013;&#30340;&#36335;&#24452;&#20026;&#28789;&#24863;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#24102;&#26377;&#36335;&#24452;&#37325;&#21152;&#26435;&#30340;&#23450;&#21046;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;&#31616;&#31216;CustomGNN&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;CustomGNN&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#39640;&#23618;&#35821;&#20041;&#65292;&#31361;&#20986;&#19982;&#35821;&#20041;&#30456;&#20851;&#30340;&#36335;&#24452;&#65292;&#24182;&#36807;&#28388;&#25481;&#22270;&#20013;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#22122;&#38899;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;CustomGNN&#23398;&#20064;&#30340;&#35821;&#20041;&#20197;&#21450;&#20854;&#36991;&#20813;&#20256;&#32479;GNN&#20013;&#23384;&#22312;&#30340;&#36807;&#24230;&#24179;&#28369;&#12289;&#40065;&#26834;&#24615;&#24046;&#21644;&#36807;&#25311;&#21512;&#31561;&#19977;&#20010;&#22266;&#26377;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2106.10866v3 Announce Type: replace  Abstract: Graph Neural Networks (GNNs) have been extensively used for mining graph-structured data with impressive performance. However, because these traditional GNNs do not distinguish among various downstream tasks, embeddings embedded by them are not always effective. Intuitively, paths in a graph imply different semantics for different downstream tasks. Inspired by this, we design a novel GNN solution, namely Customized Graph Neural Network with Path Reweighting (CustomGNN for short). Specifically, the proposed CustomGNN can automatically learn the high-level semantics for specific downstream tasks to highlight semantically relevant paths as well to filter out task-irrelevant noises in a graph. Furthermore, we empirically analyze the semantics learned by CustomGNN and demonstrate its ability to avoid the three inherent problems in traditional GNNs, i.e., over-smoothing, poor robustness, and overfitting. In experiments with the node classi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#21644;&#25351;&#21335;&#26469;&#35299;&#20915;&#39640;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27835;&#29702;&#21644;&#21033;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10745</link><description>&lt;p&gt;
&#23545;&#39640;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27835;&#29702;&#21644;&#21033;&#29992;&#30340;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#21644;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Ethical Artificial Intelligence Principles and Guidelines for the Governance and Utilization of Highly Advanced Large Language Models. (arXiv:2401.10745v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#21644;&#25351;&#21335;&#26469;&#35299;&#20915;&#39640;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27835;&#29702;&#21644;&#21033;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;ChatGPT&#12289;LaMDA&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#65292;&#25216;&#26415;&#34892;&#19994;&#21644;&#20854;&#20182;&#34892;&#19994;&#23545;LLMs&#30340;&#24320;&#21457;&#21644;&#20351;&#29992;&#26377;&#25152;&#22686;&#21152;&#12290;&#34429;&#28982;LLMs&#30340;&#27700;&#24179;&#23578;&#26410;&#36229;&#36807;&#20154;&#31867;&#26234;&#33021;&#65292;&#20294;&#24635;&#26377;&#19968;&#22825;&#20250;&#36798;&#21040;&#36825;&#19968;&#28857;&#12290;&#36825;&#31181;LLMs&#21487;&#20197;&#31216;&#20026;&#39640;&#32423;LLMs&#12290;&#30446;&#21069;&#65292;&#30001;&#20110;&#23578;&#26410;&#36798;&#21040;&#36825;&#19968;&#28857;&#65292;&#20351;&#29992;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21407;&#21017;&#21644;&#25351;&#21335;&#26469;&#35299;&#20915;&#39640;&#32423;LLMs&#30340;&#38382;&#39064;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#19968;&#26086;&#36798;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23558;&#26080;&#27861;&#20805;&#20998;&#20934;&#22791;&#22909;&#20197;&#36947;&#24503;&#21644;&#26368;&#20339;&#26041;&#24335;&#22788;&#29702;&#20854;&#20135;&#29983;&#30340;&#21518;&#26524;&#65292;&#36825;&#23558;&#23548;&#33268;&#19981;&#21487;&#39044;&#26399;&#30340;&#21518;&#26524;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#21644;&#25351;&#21335;&#26469;&#35299;&#20915;&#39640;&#32423;LLMs&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the success of ChatGPT, LaMDA and other large language models (LLMs), there has been an increase in development and usage of LLMs within the technology sector and other sectors. While the level in which LLMs has not reached a level where it has surpassed human intelligence, there will be a time when it will. Such LLMs can be referred to as advanced LLMs. Currently, there are limited usage of ethical artificial intelligence (AI) principles and guidelines addressing advanced LLMs due to the fact that we have not reached that point yet. However, this is a problem as once we do reach that point, we will not be adequately prepared to deal with the aftermath of it in an ethical and optimal way, which will lead to undesired and unexpected consequences. This paper addresses this issue by discussing what ethical AI principles and guidelines can be used to address highly advanced LLMs.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36895;&#29575;&#38480;&#21046;&#36890;&#36947;&#19978;&#23454;&#29616;&#27169;&#22411;&#26080;&#20851;&#30340;LQR&#25511;&#21046;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#37327;&#21270;&#26799;&#24230;&#19979;&#38477;&#65288;AQGD&#65289;&#31639;&#27861;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#22312;&#22122;&#22768;&#30005;&#36335;&#20013;&#21487;&#20197;&#23454;&#29616;&#25511;&#21046;&#38382;&#39064;&#30340;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2401.01258</link><description>&lt;p&gt;
&#23454;&#29616;&#27169;&#22411;&#26080;&#20851;&#30340;&#36890;&#36807;&#36895;&#29575;&#38480;&#21046;&#36890;&#36947;&#30340;LQR&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Model-Free LQR Control over Rate-Limited Channels. (arXiv:2401.01258v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01258
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36895;&#29575;&#38480;&#21046;&#36890;&#36947;&#19978;&#23454;&#29616;&#27169;&#22411;&#26080;&#20851;&#30340;LQR&#25511;&#21046;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#37327;&#21270;&#26799;&#24230;&#19979;&#38477;&#65288;AQGD&#65289;&#31639;&#27861;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#22312;&#22122;&#22768;&#30005;&#36335;&#20013;&#21487;&#20197;&#23454;&#29616;&#25511;&#21046;&#38382;&#39064;&#30340;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#22312;&#35768;&#22810;&#38382;&#39064;&#35774;&#32622;&#20013;&#30340;&#25511;&#21046;&#35774;&#35745;&#26041;&#38754;&#21462;&#24471;&#30340;&#25104;&#21151;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#20250;&#38382;&#65292;&#22914;&#26524;&#21033;&#29992;&#23454;&#38469;&#30340;&#36890;&#20449;&#36890;&#36947;&#26469;&#20256;&#36755;&#26799;&#24230;&#25110;&#31574;&#30053;&#65292;&#24773;&#20917;&#20250;&#22914;&#20309;&#25913;&#21464;&#12290;&#23613;&#31649;&#30001;&#27492;&#20135;&#29983;&#30340;&#38382;&#39064;&#19982;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#20013;&#30740;&#31350;&#30340;&#20844;&#24335;&#26377;&#31867;&#20284;&#20043;&#22788;&#65292;&#20294;&#37027;&#20010;&#39046;&#22495;&#30340;&#20016;&#23500;&#25991;&#29486;&#36890;&#24120;&#20551;&#23450;&#31995;&#32479;&#30340;&#27169;&#22411;&#26159;&#24050;&#30693;&#30340;&#12290;&#20026;&#20102;&#22312;&#27169;&#22411;&#26080;&#20851;&#25511;&#21046;&#35774;&#35745;&#21644;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#39046;&#22495;&#20043;&#38388;&#24314;&#31435;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;\textit{&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36895;&#29575;&#38480;&#21046;&#30340;&#36890;&#36947;&#20197;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#24335;&#35299;&#20915;&#22522;&#26412;&#30340;&#25511;&#21046;&#38382;&#39064;-&#20363;&#22914;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#38382;&#39064;&#65311;}&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#35774;&#32622;&#65292;&#20854;&#20013;&#19968;&#20010;&#24037;&#20316;&#20195;&#29702;&#36890;&#36807;&#19968;&#20010;&#26080;&#22122;&#22768;&#20449;&#36947;&#20197;&#26377;&#38480;&#30340;&#20301;&#36895;&#29575;&#20256;&#36755;&#37327;&#21270;&#31574;&#30053;&#26799;&#24230;&#65288;LQR&#25104;&#26412;&#65289;&#21040;&#19968;&#20010;&#26381;&#21153;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#37327;&#21270;&#26799;&#24230;&#19979;&#38477;&#65288;AQGD&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
Given the success of model-free methods for control design in many problem settings, it is natural to ask how things will change if realistic communication channels are utilized for the transmission of gradients or policies. While the resulting problem has analogies with the formulations studied under the rubric of networked control systems, the rich literature in that area has typically assumed that the model of the system is known. As a step towards bridging the fields of model-free control design and networked control systems, we ask: \textit{Is it possible to solve basic control problems - such as the linear quadratic regulator (LQR) problem - in a model-free manner over a rate-limited channel?} Toward answering this question, we study a setting where a worker agent transmits quantized policy gradients (of the LQR cost) to a server over a noiseless channel with a finite bit-rate. We propose a new algorithm titled Adaptively Quantized Gradient Descent (\texttt{AQGD}), and prove that
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#20854;&#24378;&#22823;&#30340;&#22270;&#24314;&#27169;&#33021;&#21147;&#65292;&#22312;&#20132;&#36890;&#39046;&#22495;&#34920;&#29616;&#20986;&#20248;&#31168;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20132;&#36890;&#39044;&#27979;&#26041;&#38754;&#65292;&#20854;&#20182;&#39046;&#22495;&#20173;&#38656;&#26356;&#22810;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2401.00713</link><description>&lt;p&gt;
&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Graph Neural Networks in Intelligent Transportation Systems. (arXiv:2401.00713v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#20854;&#24378;&#22823;&#30340;&#22270;&#24314;&#27169;&#33021;&#21147;&#65292;&#22312;&#20132;&#36890;&#39046;&#22495;&#34920;&#29616;&#20986;&#20248;&#31168;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20132;&#36890;&#39044;&#27979;&#26041;&#38754;&#65292;&#20854;&#20182;&#39046;&#22495;&#20173;&#38656;&#26356;&#22810;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#23545;&#20110;&#25913;&#21892;&#20132;&#36890;&#25317;&#22581;&#12289;&#20943;&#23569;&#20132;&#36890;&#20107;&#25925;&#12289;&#20248;&#21270;&#22478;&#24066;&#35268;&#21010;&#31561;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20132;&#36890;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#26041;&#27861;&#21464;&#24471;&#19981;&#21463;&#37325;&#35270;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#19988;&#34987;&#35748;&#20026;&#26159;&#35768;&#22810;&#39046;&#22495;&#20013;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#20316;&#20026;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30001;&#20110;&#20854;&#22312;&#24314;&#27169;&#19982;&#22270;&#30456;&#20851;&#30340;&#38382;&#39064;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#33258;2019&#24180;&#20197;&#26469;&#24050;&#32463;&#22312;ITS&#39046;&#22495;&#20013;&#23853;&#38706;&#22836;&#35282;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#23398;&#32773;&#20851;&#27880;GNN&#22312;&#20132;&#36890;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35813;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#20173;&#38598;&#20013;&#22312;&#20132;&#36890;&#39044;&#27979;&#26041;&#38754;&#65292;&#32780;&#20854;&#20182;ITS&#39046;&#22495;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#22478;&#24066;&#35268;&#21010;&#65292;&#20173;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent Transportation System (ITS) is vital in improving traffic congestion, reducing traffic accidents, optimizing urban planning, etc. However, due to the complexity of the traffic network, traditional machine learning and statistical methods are relegated to the background. With the advent of the artificial intelligence era, many deep learning frameworks have made remarkable progress in various fields and are now considered effective methods in many areas. As a deep learning method, Graph Neural Networks (GNNs) have emerged as a highly competitive method in the ITS field since 2019 due to their strong ability to model graph-related problems. As a result, more and more scholars pay attention to the applications of GNNs in transportation domains, which have shown excellent performance. However, most of the research in this area is still concentrated on traffic forecasting, while other ITS domains, such as autonomous vehicles and urban planning, still require more attention. This 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20272;&#35745;&#21644;&#25512;&#26029;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#31561;&#20215;&#30830;&#23450;&#27861;&#65292;&#22312;&#25552;&#20379;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.17262</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20272;&#35745;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Estimation and Inference in Distributional Reinforcement Learning. (arXiv:2309.17262v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20272;&#35745;&#21644;&#25512;&#26029;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#31561;&#20215;&#30830;&#23450;&#27861;&#65292;&#22312;&#25552;&#20379;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#32479;&#35745;&#25928;&#29575;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#31574;&#30053;&#35780;&#20272;&#65292;&#26088;&#22312;&#20272;&#35745;&#30001;&#32473;&#23450;&#31574;&#30053;&#960;&#33719;&#24471;&#30340;&#38543;&#26426;&#22238;&#25253;&#30340;&#23436;&#25972;&#20998;&#24067;&#65288;&#34920;&#31034;&#20026;&#951;^&#960;&#65289;&#12290;&#22312;&#25552;&#20379;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#31561;&#20215;&#30830;&#23450;&#27861;&#26500;&#36896;&#20102;&#20272;&#35745;&#22120;&#951;^&#960;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20855;&#26377;&#22823;&#23567;&#20026;O(|S||A|/(&#949;^(2p)(1-&#947;)^(2p+2)))&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#20445;&#35777;&#20272;&#35745;&#22120;&#951;^&#960;&#21644;&#30495;&#23454;&#20998;&#24067;&#951;^&#960;&#20043;&#38388;&#30340;p-Wasserstein&#36317;&#31163;&#23567;&#20110;&#949;&#30340;&#27010;&#29575;&#24456;&#39640;&#12290;&#36825;&#24847;&#21619;&#30528;&#20998;&#24067;&#24335;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#21487;&#20197;&#20197;&#39640;&#25928;&#21033;&#29992;&#26679;&#26412;&#30340;&#26041;&#24335;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#22312;&#19981;&#21516;&#30340;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#36890;&#36807;&#20855;&#26377;&#22823;&#23567;&#20026;O(|S||A|/(&#949;^2(1-&#947;)^4))&#30340;&#25968;&#25454;&#38598;&#23601;&#36275;&#20197;&#30830;&#20445;Kolmogorov&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study distributional reinforcement learning from the perspective of statistical efficiency.  We investigate distributional policy evaluation, aiming to estimate the complete distribution of the random return (denoted $\eta^\pi$) attained by a given policy $\pi$.  We use the certainty-equivalence method to construct our estimator $\hat\eta^\pi$, given a generative model is available.  We show that in this circumstance we need a dataset of size $\widetilde O\left(\frac{|\mathcal{S}||\mathcal{A}|}{\epsilon^{2p}(1-\gamma)^{2p+2}}\right)$ to guarantee a $p$-Wasserstein metric between $\hat\eta^\pi$ and $\eta^\pi$ is less than $\epsilon$ with high probability.  This implies the distributional policy evaluation problem can be solved with sample efficiency.  Also, we show that under different mild assumptions a dataset of size $\widetilde O\left(\frac{|\mathcal{S}||\mathcal{A}|}{\epsilon^{2}(1-\gamma)^{4}}\right)$ suffices to ensure the Kolmogorov metric and total variation m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20462;&#25913;&#22240;&#26524;&#26862;&#26519;&#26041;&#27861;&#65292;&#20197;&#30456;&#23545;&#39118;&#38505;&#20026;&#30446;&#26631;&#65292;&#20174;&#32780;&#25429;&#25417;&#21040;&#27835;&#30103;&#25928;&#24212;&#24322;&#36136;&#24615;&#30340;&#28508;&#22312;&#26469;&#28304;&#12290;</title><link>http://arxiv.org/abs/2309.15793</link><description>&lt;p&gt;
&#29992;&#22240;&#26524;&#26862;&#26519;&#38024;&#23545;&#30456;&#23545;&#39118;&#38505;&#24322;&#36136;&#24615;&#36827;&#34892;&#30446;&#26631;&#21270;
&lt;/p&gt;
&lt;p&gt;
Targeting Relative Risk Heterogeneity with Causal Forests. (arXiv:2309.15793v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20462;&#25913;&#22240;&#26524;&#26862;&#26519;&#26041;&#27861;&#65292;&#20197;&#30456;&#23545;&#39118;&#38505;&#20026;&#30446;&#26631;&#65292;&#20174;&#32780;&#25429;&#25417;&#21040;&#27835;&#30103;&#25928;&#24212;&#24322;&#36136;&#24615;&#30340;&#28508;&#22312;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#35797;&#39564;&#20998;&#26512;&#20013;&#65292;&#27835;&#30103;&#25928;&#24212;&#24322;&#36136;&#24615;&#65288;TEH&#65289;&#21363;&#31181;&#32676;&#20013;&#19981;&#21516;&#20122;&#32676;&#30340;&#27835;&#30103;&#25928;&#24212;&#30340;&#21464;&#24322;&#24615;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#22240;&#26524;&#26862;&#26519;&#65288;Wager&#21644;Athey&#65292;2018&#65289;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#38750;&#24120;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#20294;&#20687;&#35768;&#22810;&#20854;&#20182;&#21457;&#29616;TEH&#30340;&#26041;&#27861;&#19968;&#26679;&#65292;&#23427;&#29992;&#20110;&#20998;&#31163;&#20122;&#32676;&#30340;&#26631;&#20934;&#20391;&#37325;&#20110;&#32477;&#23545;&#39118;&#38505;&#30340;&#24046;&#24322;&#12290;&#36825;&#21487;&#33021;&#20250;&#21066;&#24369;&#32479;&#35745;&#21151;&#25928;&#65292;&#25513;&#30422;&#20102;&#30456;&#23545;&#39118;&#38505;&#20013;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#32780;&#30456;&#23545;&#39118;&#38505;&#36890;&#24120;&#26159;&#20020;&#24202;&#20851;&#27880;&#30340;&#26356;&#21512;&#36866;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#20462;&#25913;&#22240;&#26524;&#26862;&#26519;&#20197;&#38024;&#23545;&#30456;&#23545;&#39118;&#38505;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#27604;&#36739;&#30340;&#26032;&#39062;&#33410;&#28857;&#20998;&#21106;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#32467;&#26524;&#65292;&#34920;&#26126;&#30456;&#23545;&#39118;&#38505;&#30340;&#22240;&#26524;&#26862;&#26519;&#21487;&#20197;&#25429;&#25417;&#21040;&#20854;&#20182;&#26410;&#35266;&#23519;&#21040;&#30340;&#24322;&#36136;&#24615;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Treatment effect heterogeneity (TEH), or variability in treatment effect for different subgroups within a population, is of significant interest in clinical trial analysis. Causal forests (Wager and Athey, 2018) is a highly popular method for this problem, but like many other methods for detecting TEH, its criterion for separating subgroups focuses on differences in absolute risk. This can dilute statistical power by masking nuance in the relative risk, which is often a more appropriate quantity of clinical interest. In this work, we propose and implement a methodology for modifying causal forests to target relative risk using a novel node-splitting procedure based on generalized linear model (GLM) comparison. We present results on simulated and real-world data that suggest relative risk causal forests can capture otherwise unobserved sources of heterogeneity.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25511;&#21046;&#20855;&#26377;&#22810;&#36793;&#24418;&#21333;&#20803;&#39063;&#31890;&#30340;&#21160;&#24577;&#33258;&#32452;&#35013;&#36807;&#31243;&#65292;&#24418;&#25104;&#21313;&#20108;&#36793;&#20934;&#26230;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#26368;&#20339;&#30340;&#28201;&#24230;&#25511;&#21046;&#31574;&#30053;&#65292;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;&#20960;&#20046;&#27809;&#26377;&#32570;&#38519;&#30340;&#32467;&#26500;&#12290;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#30340;&#28201;&#24230;&#35843;&#24230;&#27604;&#20256;&#32479;&#30340;&#39044;&#35774;&#28201;&#24230;&#35843;&#24230;&#26356;&#26377;&#25928;&#22320;&#37325;&#29616;&#20102;&#26399;&#26395;&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.06869</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21160;&#24577;&#25511;&#21046;&#25311;&#26230;&#32467;&#26500;&#30340;&#33258;&#32452;&#35013;
&lt;/p&gt;
&lt;p&gt;
Dynamic control of self-assembly of quasicrystalline structures through reinforcement learning. (arXiv:2309.06869v1 [cond-mat.soft])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25511;&#21046;&#20855;&#26377;&#22810;&#36793;&#24418;&#21333;&#20803;&#39063;&#31890;&#30340;&#21160;&#24577;&#33258;&#32452;&#35013;&#36807;&#31243;&#65292;&#24418;&#25104;&#21313;&#20108;&#36793;&#20934;&#26230;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#26368;&#20339;&#30340;&#28201;&#24230;&#25511;&#21046;&#31574;&#30053;&#65292;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;&#20960;&#20046;&#27809;&#26377;&#32570;&#38519;&#30340;&#32467;&#26500;&#12290;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#30340;&#28201;&#24230;&#35843;&#24230;&#27604;&#20256;&#32479;&#30340;&#39044;&#35774;&#28201;&#24230;&#35843;&#24230;&#26356;&#26377;&#25928;&#22320;&#37325;&#29616;&#20102;&#26399;&#26395;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25511;&#21046;&#20855;&#26377;&#22810;&#36793;&#24418;&#21333;&#20803;&#30340;&#39063;&#31890;&#30340;&#21160;&#24577;&#33258;&#32452;&#35013;&#36807;&#31243;&#65292;&#24418;&#25104;&#21313;&#20108;&#36793;&#20934;&#26230;&#20307;&#65288;DDQC&#65289;&#12290;&#36825;&#20123;&#20855;&#26377;&#22810;&#36793;&#24418;&#21333;&#20803;&#30340;&#39063;&#31890;&#19982;&#20854;&#20182;&#39063;&#31890;&#20855;&#26377;&#21508;&#21521;&#24322;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#24418;&#25104;DDQC&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#31283;&#24577;&#19979;&#30340;&#32467;&#26500;&#21463;&#20854;&#32467;&#26500;&#24418;&#25104;&#30340;&#21160;&#21147;&#23398;&#36335;&#24452;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;Q&#23398;&#20064;&#26041;&#27861;&#20272;&#35745;&#20102;&#26368;&#20339;&#30340;&#28201;&#24230;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#20272;&#35745;&#30340;&#31574;&#30053;&#29983;&#25104;&#20960;&#20046;&#27809;&#26377;&#32570;&#38519;&#30340;DDQC&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#30340;&#28201;&#24230;&#35843;&#24230;&#27604;&#20256;&#32479;&#30340;&#39044;&#35774;&#28201;&#24230;&#35843;&#24230;&#65288;&#22914;&#36864;&#28779;&#65289;&#26356;&#26377;&#25928;&#22320;&#37325;&#29616;&#20102;&#26399;&#26395;&#30340;&#32467;&#26500;&#12290;&#20026;&#20102;&#38416;&#26126;&#23398;&#20064;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#19968;&#20010;&#25551;&#36848;&#32467;&#26500;&#21464;&#21270;&#21160;&#21147;&#23398;&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#20854;&#20013;&#30340;&#36816;&#21160;&#26159;&#22312;&#19977;&#20117;&#21183;&#33021;&#20013;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#33258;&#20027;&#22320;&#21457;&#29616;&#22686;&#24378;&#32467;&#26500;&#27874;&#21160;&#30340;&#20020;&#30028;&#28201;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose reinforcement learning to control the dynamical self-assembly of the dodecagonal quasicrystal (DDQC) from patchy particles. The patchy particles have anisotropic interactions with other particles and form DDQC. However, their structures at steady states are significantly influenced by the kinetic pathways of their structural formation. We estimate the best policy of temperature control trained by the Q-learning method and demonstrate that we can generate DDQC with few defects using the estimated policy. The temperature schedule obtained by reinforcement learning can reproduce the desired structure more efficiently than the conventional pre-fixed temperature schedule, such as annealing. To clarify the success of the learning, we also analyse a simple model describing the kinetics of structural changes through the motion in a triple-well potential. We have found that reinforcement learning autonomously discovers the critical temperature at which structural fluctuations enhance
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#19968;&#27454;&#25915;&#38450;&#28216;&#25103;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#24207;&#21015;&#37329;&#34701;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#29616;&#29366;&#21644;&#21160;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#27604;&#36187;&#21160;&#24577;&#65292;&#22238;&#31572;&#20102;&#38544;&#34255;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#29992;&#25143;&#25915;&#20987;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#38656;&#35201;&#22810;&#38271;&#26102;&#38388;&#25165;&#33021;&#30772;&#35299;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11406</link><description>&lt;p&gt;
&#35774;&#35745;&#19968;&#27454;&#25915;&#38450;&#28216;&#25103;&#65306;&#36890;&#36807;&#31454;&#20105;&#26469;&#22686;&#21152;&#37329;&#34701;&#20132;&#26131;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Designing an attack-defense game: how to increase robustness of financial transaction models via a competition. (arXiv:2308.11406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11406
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#19968;&#27454;&#25915;&#38450;&#28216;&#25103;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#24207;&#21015;&#37329;&#34701;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#29616;&#29366;&#21644;&#21160;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#27604;&#36187;&#21160;&#24577;&#65292;&#22238;&#31572;&#20102;&#38544;&#34255;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#29992;&#25143;&#25915;&#20987;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#38656;&#35201;&#22810;&#38271;&#26102;&#38388;&#25165;&#33021;&#30772;&#35299;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#37329;&#34701;&#39046;&#22495;&#24694;&#24847;&#25915;&#20987;&#39118;&#38505;&#19981;&#26029;&#21319;&#32423;&#21644;&#30001;&#27492;&#24341;&#21457;&#30340;&#20005;&#37325;&#25439;&#23475;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#31574;&#30053;&#21644;&#40065;&#26834;&#30340;&#38450;&#24481;&#26426;&#21046;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#38134;&#34892;&#26085;&#30410;&#24191;&#27867;&#37319;&#29992;&#26356;&#31934;&#30830;&#20294;&#28508;&#22312;&#33030;&#24369;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#19968;&#23041;&#32961;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#20351;&#29992;&#24207;&#21015;&#37329;&#34701;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#24403;&#21069;&#29366;&#24577;&#21644;&#21160;&#24577;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#27604;&#36187;&#65292;&#20801;&#35768;&#23545;&#29616;&#20195;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#20013;&#30340;&#38382;&#39064;&#36827;&#34892;&#36924;&#30495;&#32780;&#35814;&#32454;&#30340;&#30740;&#31350;&#12290;&#21442;&#19982;&#32773;&#30452;&#25509;&#31454;&#20105;&#65292;&#22240;&#27492;&#21487;&#33021;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#22312;&#25509;&#36817;&#30495;&#23454;&#26465;&#20214;&#19979;&#36827;&#34892;&#20102;&#26816;&#39564;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20998;&#26512;&#27604;&#36187;&#21160;&#24577;&#65292;&#22238;&#31572;&#20102;&#38544;&#34255;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#29992;&#25143;&#25915;&#20987;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#38656;&#35201;&#22810;&#38271;&#26102;&#38388;&#25165;&#33021;&#30772;&#35299;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the escalating risks of malicious attacks in the finance sector and the consequential severe damage, a thorough understanding of adversarial strategies and robust defense mechanisms for machine learning models is critical. The threat becomes even more severe with the increased adoption in banks more accurate, but potentially fragile neural networks. We aim to investigate the current state and dynamics of adversarial attacks and defenses for neural network models that use sequential financial data as the input.  To achieve this goal, we have designed a competition that allows realistic and detailed investigation of problems in modern financial transaction data. The participants compete directly against each other, so possible attacks and defenses are examined in close-to-real-life conditions. Our main contributions are the analysis of the competition dynamics that answers the questions on how important it is to conceal a model from malicious users, how long does it take to break i
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#24341;&#23548;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31574;&#30053;&#20135;&#29983;&#36879;&#26126;&#21644;&#26080;&#20559;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#30830;&#20445;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.09494</link><description>&lt;p&gt;
&#36879;&#26126;&#30340;6G RAN&#20999;&#29255;&#20013;&#22522;&#20110;&#35299;&#37322;&#30340;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Explanation-Guided Fair Federated Learning for Transparent 6G RAN Slicing. (arXiv:2307.09494v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09494
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#24341;&#23548;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31574;&#30053;&#20135;&#29983;&#36879;&#26126;&#21644;&#26080;&#20559;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#30830;&#20445;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#38646;&#35302;&#25720;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;6G&#32593;&#32476;&#33258;&#21160;&#21270;&#38656;&#35201;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#24314;&#31435;&#23545;AI&#40657;&#30418;&#23376;&#30340;&#20449;&#20219;&#65292;&#39044;&#35745;AI&#30340;&#21487;&#20449;&#24230;&#23558;&#19982;&#36890;&#20449;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#19968;&#36215;&#20316;&#20026;&#21487;&#37327;&#21270;&#30340;&#26381;&#21153;&#32423;&#21035;&#21327;&#35758;&#25351;&#26631;&#12290;&#36825;&#38656;&#35201;&#21033;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#36755;&#20986;&#26469;&#29983;&#25104;&#36879;&#26126;&#21644;&#26080;&#20559;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35299;&#37322;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;(EGFL)&#26469;&#30830;&#20445;&#22312;&#35757;&#32451;&#36816;&#34892;&#26102;&#36890;&#36807;Jensen-Shannon (JS)&#25955;&#24230;&#21033;&#29992;XAI&#31574;&#30053;&#30340;&#27169;&#22411;&#35299;&#37322;&#20197;&#30830;&#20445;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22238;&#24518;&#24230;&#25351;&#26631;&#20316;&#20026;&#20248;&#21270;&#20219;&#21153;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#39044;&#27979;&#27599;&#20010;&#20999;&#29255;RAN&#30340;&#20002;&#21253;&#27010;&#29575;&#26469;&#35828;&#26126;&#25152;&#25552;&#20986;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future zero-touch artificial intelligence (AI)-driven 6G network automation requires building trust in the AI black boxes via explainable artificial intelligence (XAI), where it is expected that AI faithfulness would be a quantifiable service-level agreement (SLA) metric along with telecommunications key performance indicators (KPIs). This entails exploiting the XAI outputs to generate transparent and unbiased deep neural networks (DNNs). Motivated by closed-loop (CL) automation and explanation-guided learning (EGL), we design an explanation-guided federated learning (EGFL) scheme to ensure trustworthy predictions by exploiting the model explanation emanating from XAI strategies during the training run time via Jensen-Shannon (JS) divergence. Specifically, we predict per-slice RAN dropped traffic probability to exemplify the proposed concept while respecting fairness goals formulated in terms of the recall metric which is included as a constraint in the optimization task. Finally, the 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#24182;&#34892;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26469;&#39044;&#27979;&#21457;&#30005;&#21378;&#20013;&#30340;&#33976;&#27773;&#36136;&#37327;&#27969;&#37327;&#65292;&#30456;&#27604;&#32431;&#32463;&#20856;&#21644;&#32431;&#37327;&#23376;&#27169;&#22411;&#65292;&#35813;&#28151;&#21512;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#24179;&#26041;&#35823;&#24046;&#38477;&#20302;&#20102;5.7&#20493;&#21644;4.9&#20493;&#65292;&#24182;&#19988;&#30456;&#23545;&#35823;&#24046;&#36739;&#23567;&#65292;&#26368;&#22810;&#25552;&#21319;&#20102;2&#20493;&#12290;</title><link>http://arxiv.org/abs/2307.09483</link><description>&lt;p&gt;
&#20351;&#29992;&#24182;&#34892;&#28151;&#21512;&#32593;&#32476;&#39044;&#27979;&#21457;&#30005;&#21378;&#20013;&#30340;&#33976;&#27773;&#36136;&#37327;&#27969;&#37327;
&lt;/p&gt;
&lt;p&gt;
Forecasting the steam mass flow in a powerplant using the parallel hybrid network. (arXiv:2307.09483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09483
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#24182;&#34892;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26469;&#39044;&#27979;&#21457;&#30005;&#21378;&#20013;&#30340;&#33976;&#27773;&#36136;&#37327;&#27969;&#37327;&#65292;&#30456;&#27604;&#32431;&#32463;&#20856;&#21644;&#32431;&#37327;&#23376;&#27169;&#22411;&#65292;&#35813;&#28151;&#21512;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#24179;&#26041;&#35823;&#24046;&#38477;&#20302;&#20102;5.7&#20493;&#21644;4.9&#20493;&#65292;&#24182;&#19988;&#30456;&#23545;&#35823;&#24046;&#36739;&#23567;&#65292;&#26368;&#22810;&#25552;&#21319;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#21487;&#25345;&#32493;&#30340;&#21457;&#30005;&#26159;&#33021;&#28304;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#23588;&#20854;&#26159;&#28909;&#30005;&#21378;&#22312;&#20934;&#30830;&#39044;&#27979;&#33976;&#27773;&#36136;&#37327;&#27969;&#37327;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#65292;&#36825;&#23545;&#20110;&#36816;&#33829;&#25928;&#29575;&#21644;&#25104;&#26412;&#38477;&#20302;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24182;&#34892;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#35813;&#32467;&#26500;&#23558;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#21644;&#20256;&#32479;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#24037;&#19994;&#29615;&#22659;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#20197;&#25552;&#39640;&#23545;&#26410;&#26469;15&#20998;&#38047;&#20869;&#33976;&#27773;&#36136;&#37327;&#27969;&#37327;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24182;&#34892;&#28151;&#21512;&#27169;&#22411;&#20248;&#20110;&#29420;&#31435;&#30340;&#32463;&#20856;&#21644;&#37327;&#23376;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#21518;&#30340;&#27979;&#35797;&#38598;&#19978;&#30456;&#23545;&#20110;&#32431;&#32463;&#20856;&#27169;&#22411;&#21644;&#32431;&#37327;&#23376;&#32593;&#32476;&#65292;&#24179;&#22343;&#24179;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#25439;&#22833;&#20998;&#21035;&#38477;&#20302;&#20102;5.7&#20493;&#21644;4.9&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#28151;&#21512;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#30456;&#23545;&#35823;&#24046;&#36739;&#23567;&#65292;&#27604;&#32431;&#32463;&#20856;&#27169;&#22411;&#26356;&#22909;&#65292;&#26368;&#22810;&#25552;&#21319;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient and sustainable power generation is a crucial concern in the energy sector. In particular, thermal power plants grapple with accurately predicting steam mass flow, which is crucial for operational efficiency and cost reduction. In this study, we use a parallel hybrid neural network architecture that combines a parametrized quantum circuit and a conventional feed-forward neural network specifically designed for time-series prediction in industrial settings to enhance predictions of steam mass flow 15 minutes into the future. Our results show that the parallel hybrid model outperforms standalone classical and quantum models, achieving more than 5.7 and 4.9 times lower mean squared error (MSE) loss on the test set after training compared to pure classical and pure quantum networks, respectively. Furthermore, the hybrid model demonstrates smaller relative errors between the ground truth and the model predictions on the test set, up to 2 times better than the pure classical model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30772;&#20135;&#38382;&#39064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#28608;&#21169;&#20998;&#37197;&#30340;&#25361;&#25112;&#65292;&#20197;&#30830;&#20445;&#20844;&#24179;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03515</link><description>&lt;p&gt;
&#22522;&#20110;&#30772;&#20135;&#38382;&#39064;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28608;&#21169;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Incentive Allocation in Vertical Federated Learning Based on Bankruptcy Problem. (arXiv:2307.03515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30772;&#20135;&#38382;&#39064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#28608;&#21169;&#20998;&#37197;&#30340;&#25361;&#25112;&#65292;&#20197;&#30830;&#20445;&#20844;&#24179;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#20316;&#35757;&#32451;&#22312;&#19981;&#21516;&#21442;&#19982;&#26041;&#20043;&#38388;&#22402;&#30452;&#21010;&#20998;&#30340;&#31169;&#26377;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;VFL&#35774;&#32622;&#20013;&#65292;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#26041;&#65288;&#25317;&#26377;&#24102;&#26631;&#31614;&#26679;&#26412;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#65289;&#36890;&#36807;&#19982;&#26576;&#20123;&#34987;&#21160;&#26041;&#65288;&#25317;&#26377;&#30456;&#21516;&#26679;&#26412;&#20294;&#27809;&#26377;&#26631;&#31614;&#30340;&#39069;&#22806;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#65289;&#21512;&#20316;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#20854;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#28608;&#21169;&#34987;&#21160;&#26041;&#21442;&#19982;VFL&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#22522;&#20110;&#34987;&#21160;&#26041;&#22312;VFL&#36807;&#31243;&#20013;&#30340;&#36129;&#29486;&#26469;&#20026;&#20182;&#20204;&#20998;&#37197;&#28608;&#21169;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#23450;&#20041;&#20026;&#26680;&#24515;&#28216;&#25103;&#35770;&#27010;&#24565;&#30340;&#19968;&#31181;&#21464;&#20307;&#8212;&#8212;&#30772;&#20135;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22612;&#26408;&#24503;&#21010;&#20998;&#35268;&#21017;&#26469;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#23427;&#30830;&#20445;&#20102;&#28608;&#21169;&#30340;&#20844;&#24179;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical federated learning (VFL) is a promising approach for collaboratively training machine learning models using private data partitioned vertically across different parties. Ideally in a VFL setting, the active party (party possessing features of samples with labels) benefits by improving its machine learning model through collaboration with some passive parties (parties possessing additional features of the same samples without labels) in a privacy preserving manner. However, motivating passive parties to participate in VFL can be challenging. In this paper, we focus on the problem of allocating incentives to the passive parties by the active party based on their contributions to the VFL process. We formulate this problem as a variant of the Nucleolus game theory concept, known as the Bankruptcy Problem, and solve it using the Talmud's division rule. We evaluate our proposed method on synthetic and real-world datasets and show that it ensures fairness and stability in incentive a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;MOLAR&#65292;&#23427;&#21033;&#29992;&#21327;&#21516;&#32447;&#24615;&#22238;&#24402;&#21644;&#19978;&#19979;&#25991;&#33218;&#38382;&#39064;&#20013;&#30340;&#31232;&#30095;&#24322;&#36136;&#24615;&#26469;&#25552;&#39640;&#20272;&#35745;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#27604;&#29420;&#31435;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.06291</link><description>&lt;p&gt;
&#26368;&#20248;&#24322;&#26500;&#21327;&#21516;&#32447;&#24615;&#22238;&#24402;&#21644;&#19978;&#19979;&#25991;&#33218;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal Heterogeneous Collaborative Linear Regression and Contextual Bandits. (arXiv:2306.06291v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;MOLAR&#65292;&#23427;&#21033;&#29992;&#21327;&#21516;&#32447;&#24615;&#22238;&#24402;&#21644;&#19978;&#19979;&#25991;&#33218;&#38382;&#39064;&#20013;&#30340;&#31232;&#30095;&#24322;&#36136;&#24615;&#26469;&#25552;&#39640;&#20272;&#35745;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#27604;&#29420;&#31435;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#21644;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#26469;&#33258;&#20110;&#20960;&#20010;&#21487;&#33021;&#26159;&#24322;&#26500;&#30340;&#26469;&#28304;&#12290;&#21327;&#21516;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20849;&#24615;&#25552;&#39640;&#25928;&#29575;&#65292;&#21516;&#26102;&#32771;&#34385;&#21487;&#33021;&#20986;&#29616;&#30340;&#24046;&#24322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#21327;&#21516;&#32447;&#24615;&#22238;&#24402;&#21644;&#19978;&#19979;&#25991;&#33218;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#23454;&#20363;&#30340;&#30456;&#20851;&#21442;&#25968;&#31561;&#20110;&#20840;&#23616;&#21442;&#25968;&#21152;&#19978;&#19968;&#20010;&#31232;&#30095;&#30340;&#23454;&#20363;&#29305;&#23450;&#26415;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MOLAR&#30340;&#26032;&#22411;&#20108;&#38454;&#27573;&#20272;&#35745;&#22120;&#65292;&#23427;&#36890;&#36807;&#39318;&#20808;&#26500;&#24314;&#23454;&#20363;&#32447;&#24615;&#22238;&#24402;&#20272;&#35745;&#30340;&#36880;&#39033;&#20013;&#20301;&#25968;&#65292;&#28982;&#21518;&#23558;&#23454;&#20363;&#29305;&#23450;&#20272;&#35745;&#20540;&#25910;&#32553;&#21040;&#20013;&#20301;&#25968;&#38468;&#36817;&#26469;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#12290;&#19982;&#29420;&#31435;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#30456;&#27604;&#65292;MOLAR&#25552;&#39640;&#20102;&#20272;&#35745;&#35823;&#24046;&#23545;&#25968;&#25454;&#32500;&#24230;&#30340;&#20381;&#36182;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;MOLAR&#24212;&#29992;&#20110;&#24320;&#21457;&#29992;&#20110;&#31232;&#30095;&#24322;&#26500;&#21327;&#21516;&#19978;&#19979;&#25991;&#33218;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#29420;&#31435;&#33218;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#36129;&#29486;&#20248;&#20110;&#20808;&#21069;&#22312;&#25991;&#29486;&#20013;&#25253;&#36947;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large and complex datasets are often collected from several, possibly heterogeneous sources. Collaborative learning methods improve efficiency by leveraging commonalities across datasets while accounting for possible differences among them. Here we study collaborative linear regression and contextual bandits, where each instance's associated parameters are equal to a global parameter plus a sparse instance-specific term. We propose a novel two-stage estimator called MOLAR that leverages this structure by first constructing an entry-wise median of the instances' linear regression estimates, and then shrinking the instance-specific estimates towards the median. MOLAR improves the dependence of the estimation error on the data dimension, compared to independent least squares estimates. We then apply MOLAR to develop methods for sparsely heterogeneous collaborative contextual bandits, which lead to improved regret guarantees compared to independent bandit methods. We further show that our 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#37319;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36866;&#29992;&#20110;&#24322;&#36136;&#24615;&#22270;&#21644;&#21516;&#36136;&#24615;&#22270;&#65292;&#24182;&#22312;&#35768;&#22810;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18385</link><description>&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#21452;&#37325;&#23884;&#20837;&#65306;&#36866;&#29992;&#20110;&#24322;&#36136;&#24615;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Self-attention Dual Embedding for Graphs with Heterophily. (arXiv:2305.18385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#37319;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36866;&#29992;&#20110;&#24322;&#36136;&#24615;&#22270;&#21644;&#21516;&#36136;&#24615;&#22270;&#65292;&#24182;&#22312;&#35768;&#22810;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#21151;&#12290;GNNs&#36890;&#24120;&#20551;&#35774;&#22270;&#26159;&#21516;&#36136;&#30340;&#65292;&#21363;&#30456;&#37051;&#33410;&#28857;&#24456;&#21487;&#33021;&#23646;&#20110;&#30456;&#21516;&#30340;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#37117;&#26159;&#24322;&#36136;&#30340;&#65292;&#36825;&#23548;&#33268;&#20351;&#29992;&#26631;&#20934;&#30340;GNNs&#26102;&#20998;&#31867;&#31934;&#24230;&#35201;&#20302;&#24471;&#22810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#65292;&#23427;&#23545;&#24322;&#36136;&#24615;&#21644;&#21516;&#36136;&#24615;&#22270;&#37117;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#19977;&#20010;&#20027;&#35201;&#35266;&#23519;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#30340;&#22270;&#20013;&#65292;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#25299;&#25169;&#25552;&#20379;&#19981;&#21516;&#25968;&#37327;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#24212;&#35813;&#29420;&#31435;&#32534;&#30721;&#24182;&#20197;&#33258;&#36866;&#24212;&#26041;&#24335;&#20248;&#20808;&#32423;&#21270;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20256;&#25773;&#22270;&#25299;&#25169;&#20449;&#24687;&#26102;&#20801;&#35768;&#36127;&#30340;&#27880;&#24847;&#26435;&#37325;&#21487;&#20197;&#25552;&#39640;&#31934;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#33410;&#28857;&#20043;&#38388;&#19981;&#23545;&#31216;&#30340;&#27880;&#24847;&#26435;&#37325;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;GNN&#65292;&#21033;&#29992;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#36890;&#36807;&#26032;&#39062;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#20123;&#26631;&#20934;&#30340;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22270;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have been highly successful for the node classification task. GNNs typically assume graphs are homophilic, i.e. neighboring nodes are likely to belong to the same class. However, a number of real-world graphs are heterophilic, and this leads to much lower classification accuracy using standard GNNs. In this work, we design a novel GNN which is effective for both heterophilic and homophilic graphs. Our work is based on three main observations. First, we show that node features and graph topology provide different amounts of informativeness in different graphs, and therefore they should be encoded independently and prioritized in an adaptive manner. Second, we show that allowing negative attention weights when propagating graph topology information improves accuracy. Finally, we show that asymmetric attention weights between nodes are helpful. We design a GNN which makes use of these observations through a novel self-attention mechanism. We evaluate our algor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Grad-Align+&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#33410;&#28857;&#29305;&#24449;&#26469;&#25191;&#34892;NA&#20219;&#21153;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#22686;&#24378;&#30340;&#33410;&#28857;&#29305;&#24449;&#26469;&#35774;&#35745;NA&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;NA&#26041;&#27861;&#32570;&#20047;&#39069;&#22806;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.12751</link><description>&lt;p&gt;
&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#25913;&#36827;&#32593;&#32476;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Node Feature Augmentation Vitaminizes Network Alignment. (arXiv:2304.12751v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Grad-Align+&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#33410;&#28857;&#29305;&#24449;&#26469;&#25191;&#34892;NA&#20219;&#21153;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#22686;&#24378;&#30340;&#33410;&#28857;&#29305;&#24449;&#26469;&#35774;&#35745;NA&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;NA&#26041;&#27861;&#32570;&#20047;&#39069;&#22806;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23545;&#40784;&#65288;NA&#65289;&#26159;&#36890;&#36807;&#32473;&#23450;&#32593;&#32476;&#30340;&#25299;&#25169;&#21644;/&#25110;&#29305;&#24449;&#20449;&#24687;&#26469;&#21457;&#29616;&#22810;&#20010;&#32593;&#32476;&#20043;&#38388;&#30340;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;NA&#26041;&#27861;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#24182;&#19981;&#24635;&#26159;&#26377;&#39069;&#22806;&#20449;&#24687;&#65292;&#22914;&#20808;&#21069;&#30340;&#38170;&#28857;&#38142;&#25509;&#21644;/&#25110;&#33410;&#28857;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23454;&#38469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Grad-Align+&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;NA&#26041;&#27861;&#65292;&#24314;&#31435;&#22312;&#26368;&#36817;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;NA&#26041;&#27861;Grad-Align&#20043;&#19978;&#65292;Grad-Align+&#20165;&#36880;&#27493;&#21457;&#29616;&#37096;&#20998;&#33410;&#28857;&#23545;&#65292;&#30452;&#21040;&#25214;&#21040;&#25152;&#26377;&#33410;&#28857;&#23545;&#12290;&#22312;&#35774;&#35745;Grad-Align+&#26102;&#65292;&#25105;&#20204;&#32771;&#34385;&#22914;&#20309;&#36890;&#36807;&#22686;&#24378;&#33410;&#28857;&#29305;&#24449;&#26469;&#25191;&#34892;NA&#20219;&#21153;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#22686;&#24378;&#30340;&#33410;&#28857;&#29305;&#24449;&#26469;&#35774;&#35745;NA&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#30340;Grad-Align+&#65306;&#22522;&#20110;&#20013;&#24515;&#24615;&#30340;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#65288;CNFA&#65289;&#12289;&#22270;&#20999;&#29255;&#29983;&#25104;&#21644;&#20248;&#21270;&#33410;&#28857;&#23884;&#20837;&#29305;&#24449;&#65288;ONIFE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network alignment (NA) is the task of discovering node correspondences across multiple networks using topological and/or feature information of given networks. Although NA methods have achieved remarkable success in a myriad of scenarios, their effectiveness is not without additional information such as prior anchor links and/or node features, which may not always be available due to privacy concerns or access restrictions. To tackle this practical challenge, we propose Grad-Align+, a novel NA method built upon a recent state-of-the-art NA method, the so-called Grad-Align, that gradually discovers only a part of node pairs until all node pairs are found. In designing Grad-Align+, we account for how to augment node features in the sense of performing the NA task and how to design our NA method by maximally exploiting the augmented node features. To achieve this goal, we develop Grad-Align+ consisting of three key components: 1) centrality-based node feature augmentation (CNFA), 2) graph
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#32467;&#21512;Copula&#29702;&#35770;&#26469;&#35299;&#20915;&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.17448</link><description>&lt;p&gt;
NN-Copula-CD&#65306;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#21464;&#21270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
NN-Copula-CD: A Copula-Guided Interpretable Neural Network for Change Detection in Heterogeneous Remote Sensing Images. (arXiv:2303.17448v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17448
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#32467;&#21512;Copula&#29702;&#35770;&#26469;&#35299;&#20915;&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#26159;&#19968;&#20010;&#23454;&#38469;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36807;&#21435;&#21313;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#21457;&#23637;&#35753;&#24322;&#26500;&#21464;&#21270;&#26816;&#27979;&#38382;&#39064;&#21463;&#30410;&#21290;&#27973;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;DNN&#22987;&#32456;&#20687;&#40657;&#21283;&#23376;&#19968;&#26679;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;DNN&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#21464;&#21270;&#26816;&#27979;&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#24322;&#26500;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;(NN-Copula-CD)&#12290;&#22312;NN-Copula-CD&#20013;&#65292;Copula&#30340;&#25968;&#23398;&#29305;&#24449;&#34987;&#35774;&#35745;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#30417;&#30563;&#19968;&#20010;&#31616;&#21333;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Change detection (CD) in heterogeneous remote sensing images is a practical and challenging issue for real-life emergencies. In the past decade, the heterogeneous CD problem has significantly benefited from the development of deep neural networks (DNN). However, the data-driven DNNs always perform like a black box where the lack of interpretability limits the trustworthiness and controllability of DNNs in most practical CD applications. As a strong knowledge-driven tool to measure correlation between random variables, Copula theory has been introduced into CD, yet it suffers from non-robust CD performance without manual prior selection for Copula functions. To address the above issues, we propose a knowledge-data-driven heterogeneous CD method (NN-Copula-CD) based on the Copula-guided interpretable neural network. In our NN-Copula-CD, the mathematical characteristics of Copula are designed as the losses to supervise a simple fully connected neural network to learn the correlation betwe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.16199</link><description>&lt;p&gt;
LLaMA-Adapter: &#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#31934;&#32454;&#35843;&#25972;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. (arXiv:2303.16199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Adapter&#36825;&#19968;&#36731;&#37327;&#32423;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;LLaMA&#39640;&#25928;&#22320;&#24494;&#35843;&#20026;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;&#21033;&#29992;52K&#20010;&#33258;&#25105;&#25351;&#23548;&#31034;&#33539;&#65292;LLaMA-Adapter&#20165;&#22312;&#20923;&#32467;&#30340;LLaMA 7B&#27169;&#22411;&#19978;&#24341;&#20837;&#20102;1.2M&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;8&#20010;A100 GPU&#19978;&#20165;&#32791;&#26102;&#19981;&#21040;&#19968;&#20010;&#23567;&#26102;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#36866;&#24212;&#25552;&#31034;&#65292;&#24182;&#22312;&#36739;&#39640;&#30340;&#21464;&#21387;&#22120;&#23618;&#20013;&#23558;&#23427;&#20204;&#39044;&#32622;&#20110;&#36755;&#20837;&#25991;&#26412;&#20196;&#29260;&#20043;&#21069;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#38646;&#38376;&#25511;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#26032;&#30340;&#25351;&#20196;&#25552;&#31034;&#27880;&#20837;LLaMA&#65292;&#24182;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#20854;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#39640;&#25928;&#35757;&#32451;&#65292;LLaMA-Adapter&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;7B&#21442;&#25968;&#30340;Alpaca&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#31616;&#21333;&#22320;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20363;&#22914;&#22270;&#20687;&#65292;&#29992;&#20110;&#22270;&#20687;&#30456;&#20851;&#30340;LLaMA&#65292;&#22312;ScienceQA&#19978;&#23454;&#29616;&#20102;&#26356;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/ZrrSkywalker/LLaMA-Adapt&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapt
&lt;/p&gt;</description></item><item><title>repliclust &#26159;&#19968;&#20010; Python &#21253;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#32858;&#31867;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#25968;&#25454;&#38598;&#30340;&#21407;&#22411;&#65292;&#25552;&#20379;&#20102;&#25918;&#32622;&#38598;&#32676;&#20013;&#24515;&#12289;&#37319;&#26679;&#38598;&#32676;&#24418;&#29366;&#12289;&#36873;&#25321;&#27599;&#20010;&#38598;&#32676;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#20197;&#21450;&#20026;&#38598;&#32676;&#20998;&#37197;&#27010;&#29575;&#20998;&#24067;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.14301</link><description>&lt;p&gt;
repliclust&#65306;&#32858;&#31867;&#20998;&#26512;&#30340;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
repliclust: Synthetic Data for Cluster Analysis. (arXiv:2303.14301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14301
&lt;/p&gt;
&lt;p&gt;
repliclust &#26159;&#19968;&#20010; Python &#21253;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#32858;&#31867;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#25968;&#25454;&#38598;&#30340;&#21407;&#22411;&#65292;&#25552;&#20379;&#20102;&#25918;&#32622;&#38598;&#32676;&#20013;&#24515;&#12289;&#37319;&#26679;&#38598;&#32676;&#24418;&#29366;&#12289;&#36873;&#25321;&#27599;&#20010;&#38598;&#32676;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#20197;&#21450;&#20026;&#38598;&#32676;&#20998;&#37197;&#27010;&#29575;&#20998;&#24067;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; repliclust&#65288;&#26469;&#33258;&#20110; repli-cate &#21644; clust-er&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#32858;&#31867;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340; Python &#21253;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#25968;&#25454;&#38598;&#30340;&#21407;&#22411;&#65292;&#21363;&#39640;&#32423;&#20960;&#20309;&#25551;&#36848;&#65292;&#29992;&#25143;&#21487;&#20197;&#20174;&#20013;&#21019;&#24314;&#35768;&#22810;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20855;&#26377;&#25152;&#38656;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;&#25105;&#20204;&#36719;&#20214;&#30340;&#26550;&#26500;&#26159;&#27169;&#22359;&#21270;&#21644;&#38754;&#21521;&#23545;&#35937;&#30340;&#65292;&#23558;&#25968;&#25454;&#29983;&#25104;&#20998;&#35299;&#25104;&#25918;&#32622;&#38598;&#32676;&#20013;&#24515;&#30340;&#31639;&#27861;&#12289;&#37319;&#26679;&#38598;&#32676;&#24418;&#29366;&#30340;&#31639;&#27861;&#12289;&#36873;&#25321;&#27599;&#20010;&#38598;&#32676;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#30340;&#31639;&#27861;&#20197;&#21450;&#20026;&#38598;&#32676;&#20998;&#37197;&#27010;&#29575;&#20998;&#24067;&#30340;&#31639;&#27861;&#12290;repliclust.org &#39033;&#30446;&#32593;&#39029;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#29992;&#25143;&#25351;&#21335;&#21644;&#20840;&#38754;&#30340;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present repliclust (from repli-cate and clust-er), a Python package for generating synthetic data sets with clusters. Our approach is based on data set archetypes, high-level geometric descriptions from which the user can create many different data sets, each possessing the desired geometric characteristics. The architecture of our software is modular and object-oriented, decomposing data generation into algorithms for placing cluster centers, sampling cluster shapes, selecting the number of data points for each cluster, and assigning probability distributions to clusters. The project webpage, repliclust.org, provides a concise user guide and thorough documentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23614;&#24179;&#22343;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23545;&#26102;&#24207;&#24046;&#24322;(TD)&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#34892;&#20026;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#23614;&#24179;&#22343;TD&#33021;&#20197;&#26368;&#20248;&#36895;&#29575; $O(1/t)$ &#25910;&#25947;&#65292;&#24182;&#19988;&#21021;&#22987;&#35823;&#24046;&#34928;&#20943;&#36895;&#29575;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#27491;&#21017;&#21270;&#30340;TD&#29256;&#26412;&#22312;&#20855;&#26377;&#30149;&#24577;&#29305;&#24449;&#30340;&#38382;&#39064;&#19978;&#24456;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.05918</link><description>&lt;p&gt;
&#26377;&#38480;&#26102;&#38388;&#20869;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#26102;&#24207;&#24046;&#24322;&#23398;&#20064;&#30340;&#20998;&#26512;&#65306;&#23614;&#24179;&#22343;&#21644;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Finite time analysis of temporal difference learning with linear function approximation: Tail averaging and regularisation. (arXiv:2210.05918v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23614;&#24179;&#22343;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23545;&#26102;&#24207;&#24046;&#24322;(TD)&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#34892;&#20026;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#23614;&#24179;&#22343;TD&#33021;&#20197;&#26368;&#20248;&#36895;&#29575; $O(1/t)$ &#25910;&#25947;&#65292;&#24182;&#19988;&#21021;&#22987;&#35823;&#24046;&#34928;&#20943;&#36895;&#29575;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#27491;&#21017;&#21270;&#30340;TD&#29256;&#26412;&#22312;&#20855;&#26377;&#30149;&#24577;&#29305;&#24449;&#30340;&#38382;&#39064;&#19978;&#24456;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#27969;&#34892;&#30340;&#26102;&#24207;&#24046;&#24322;(TD)&#23398;&#20064;&#31639;&#27861;&#19982;&#23614;&#24179;&#22343;&#30456;&#32467;&#21512;&#26102;&#30340;&#26377;&#38480;&#26102;&#38388;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#19981;&#38656;&#35201;&#20851;&#20110;&#24213;&#23618;&#25237;&#24433;TD&#19981;&#21160;&#28857;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#20449;&#24687;&#30340;&#27493;&#38271;&#36873;&#25321;&#19979;&#65292;&#25512;&#23548;&#20102;&#23614;&#24179;&#22343;TD&#36845;&#20195;&#30340;&#21442;&#25968;&#35823;&#24046;&#30340;&#26377;&#38480;&#26102;&#38388;&#30028;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23614;&#24179;&#22343;TD&#20197;&#26399;&#26395;&#36895;&#29575;&#21644;&#39640;&#27010;&#29575;&#25910;&#25947;&#20110;&#26368;&#20248;&#30340; $O(1/t)$ &#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#23637;&#31034;&#20102;&#21021;&#22987;&#35823;&#24046;(&#20559;&#24046;)&#30340;&#26356;&#24555;&#34928;&#20943;&#36895;&#29575;&#65292;&#36825;&#26159;&#23545;&#25152;&#26377;&#36845;&#20195;&#30340;&#24179;&#22343;&#20540;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21017;&#21270;&#30340;TD&#21464;&#20307;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#35748;&#20026;&#27491;&#21017;&#21270;&#30340;TD&#29256;&#26412;&#22312;&#20855;&#26377;&#30149;&#24577;&#29305;&#24449;&#30340;&#38382;&#39064;&#19978;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the finite-time behaviour of the popular temporal difference (TD) learning algorithm when combined with tail-averaging. We derive finite time bounds on the parameter error of the tail-averaged TD iterate under a step-size choice that does not require information about the eigenvalues of the matrix underlying the projected TD fixed point. Our analysis shows that tail-averaged TD converges at the optimal $O\left(1/t\right)$ rate, both in expectation and with high probability. In addition, our bounds exhibit a sharper rate of decay for the initial error (bias), which is an improvement over averaging all iterates. We also propose and analyse a variant of TD that incorporates regularisation. From analysis, we conclude that the regularised version of TD is useful for problems with ill-conditioned features.
&lt;/p&gt;</description></item></channel></rss>