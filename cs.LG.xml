<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>SugarcaneNet2024&#26159;&#36890;&#36807;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24555;&#36895;&#20934;&#30830;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.18870</link><description>&lt;p&gt;
SugarcaneNet2024: LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
SugarcaneNet2024: An Optimized Weighted Average Ensemble Approach of LASSO Regularized Pre-trained Models for Sugarcane Disease Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18870
&lt;/p&gt;
&lt;p&gt;
SugarcaneNet2024&#26159;&#36890;&#36807;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24555;&#36895;&#20934;&#30830;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29976;&#34071;&#20316;&#20026;&#19990;&#30028;&#31958;&#19994;&#30340;&#20851;&#38190;&#20316;&#29289;&#65292;&#23481;&#26131;&#21463;&#22810;&#31181;&#30149;&#23475;&#20405;&#23475;&#65292;&#36825;&#20123;&#30149;&#23475;&#23545;&#20854;&#20135;&#37327;&#21644;&#36136;&#37327;&#37117;&#26377;&#37325;&#22823;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#26377;&#25928;&#31649;&#29702;&#21644;&#23454;&#26045;&#39044;&#38450;&#25514;&#26045;&#65292;&#24517;&#39035;&#21450;&#26102;&#20934;&#30830;&#22320;&#26816;&#27979;&#30149;&#23475;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SugarcaneNet2024&#30340;&#29420;&#29305;&#27169;&#22411;&#65292;&#36890;&#36807;&#21494;&#29255;&#22270;&#20687;&#22788;&#29702;&#65292;&#33021;&#22815;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#33258;&#21160;&#24555;&#36895;&#26816;&#27979;&#29976;&#34071;&#30149;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#27719;&#24635;&#20102;&#19971;&#20010;&#23450;&#21046;&#30340;&#12289;&#32463;&#36807;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;&#65292;&#29305;&#21035;&#26159;InceptionV3&#12289;InceptionResNetV2&#12289;DenseNet201&#12289;DenseNet169&#12289;Xception&#21644;ResNet152V2&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#22312;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#24213;&#37096;&#28155;&#21152;&#20102;&#19977;&#23618;&#26356;&#23494;&#38598;&#23618;&#65292;&#20855;&#26377;0.0001&#30340;LASSO&#27491;&#21017;&#21270;&#65292;&#19977;&#20010;30%&#30340;dropout&#23618;&#21644;&#19977;&#20010;&#21551;&#29992;renorm&#30340;&#25209;&#37327;&#24402;&#19968;&#21270;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18870v1 Announce Type: cross  Abstract: Sugarcane, a key crop for the world's sugar industry, is prone to several diseases that have a substantial negative influence on both its yield and quality. To effectively manage and implement preventative initiatives, diseases must be detected promptly and accurately. In this study, we present a unique model called sugarcaneNet2024 that outperforms previous methods for automatically and quickly detecting sugarcane disease through leaf image processing. Our proposed model consolidates an optimized weighted average ensemble of seven customized and LASSO-regularized pre-trained models, particularly InceptionV3, InceptionResNetV2, DenseNet201, DenseNet169, Xception, and ResNet152V2. Initially, we added three more dense layers with 0.0001 LASSO regularization, three 30% dropout layers, and three batch normalizations with renorm enabled at the bottom of these pre-trained models to improve the performance. The accuracy of sugarcane leaf dise
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#33041;&#21330;&#20013;&#20998;&#21106;&#19978;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#26159;&#21542;&#38656;&#35201;&#39640;&#32423;&#21035;&#35774;&#35745;&#26469;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.17177</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#33041;&#21330;&#20013;&#20998;&#21106;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Brain Stroke Segmentation Using Deep Learning Models: A Comparative Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#33041;&#21330;&#20013;&#20998;&#21106;&#19978;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#26159;&#21542;&#38656;&#35201;&#39640;&#32423;&#21035;&#35774;&#35745;&#26469;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#21330;&#20013;&#20998;&#21106;&#22312;&#33041;&#21330;&#20013;&#24739;&#32773;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36890;&#36807;&#25552;&#20379;&#21463;&#24433;&#21709;&#33041;&#21306;&#22495;&#30340;&#31354;&#38388;&#20449;&#24687;&#21644;&#21463;&#25439;&#31243;&#24230;&#12290;&#20934;&#30830;&#20998;&#21106;&#33041;&#21330;&#20013;&#30149;&#21464;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#25163;&#24037;&#25216;&#26415;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#26368;&#36817;&#65292;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#22411;&#24050;&#34987;&#24341;&#20837;&#29992;&#20110;&#19968;&#33324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23637;&#31034;&#20986;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#26102;&#36229;&#36234;&#35768;&#22810;&#26368;&#20808;&#36827;&#32593;&#32476;&#30340;&#26377;&#21069;&#26223;&#32467;&#26524;&#12290;&#38543;&#30528;&#35270;&#35273;Transformer&#30340;&#20986;&#29616;&#65292;&#24050;&#32463;&#22522;&#20110;&#23427;&#20204;&#24341;&#20837;&#20102;&#20960;&#31181;&#27169;&#22411;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21017;&#26088;&#22312;&#35774;&#35745;&#22522;&#20110;&#20256;&#32479;&#21367;&#31215;&#23618;&#26469;&#25552;&#21462;&#20687;Transformer&#36825;&#26679;&#30340;&#38271;&#31243;&#20381;&#36182;&#30340;&#26356;&#22909;&#27169;&#22359;&#12290;&#26159;&#21542;&#23545;&#25152;&#26377;&#20998;&#21106;&#26696;&#20363;&#37117;&#38656;&#35201;&#36825;&#26679;&#39640;&#32423;&#21035;&#30340;&#35774;&#35745;&#26469;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#35299;&#31572;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#22235;&#31181;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17177v1 Announce Type: cross  Abstract: Stroke segmentation plays a crucial role in the diagnosis and treatment of stroke patients by providing spatial information about affected brain regions and the extent of damage. Segmenting stroke lesions accurately is a challenging task, given that conventional manual techniques are time consuming and prone to errors. Recently, advanced deep models have been introduced for general medical image segmentation, demonstrating promising results that surpass many state of the art networks when evaluated on specific datasets. With the advent of the vision Transformers, several models have been introduced based on them, while others have aimed to design better modules based on traditional convolutional layers to extract long-range dependencies like Transformers. The question of whether such high-level designs are necessary for all segmentation cases to achieve the best results remains unanswered. In this study, we selected four types of deep 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#22238;&#24402;&#30340;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#21487;&#36801;&#31227;&#32467;&#26500;&#33258;&#36866;&#24212;&#26816;&#27979;&#21644;&#32858;&#21512;&#29305;&#24449;&#21644;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.13565</link><description>&lt;p&gt;
AdaTrans&#65306;&#38024;&#23545;&#39640;&#32500;&#22238;&#24402;&#30340;&#29305;&#24449;&#33258;&#36866;&#24212;&#19982;&#26679;&#26412;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for High-dimensional Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13565
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#22238;&#24402;&#30340;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#21487;&#36801;&#31227;&#32467;&#26500;&#33258;&#36866;&#24212;&#26816;&#27979;&#21644;&#32858;&#21512;&#29305;&#24449;&#21644;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#39640;&#32500;&#32972;&#26223;&#19979;&#30340;&#36801;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#29305;&#24449;&#32500;&#24230;&#22823;&#20110;&#26679;&#26412;&#22823;&#23567;&#12290;&#20026;&#20102;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20449;&#24687;&#65292;&#35813;&#20449;&#24687;&#21487;&#33021;&#22312;&#29305;&#24449;&#25110;&#28304;&#26679;&#26412;&#20043;&#38388;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#21644;&#32858;&#21512;&#29305;&#24449;-wise (F-AdaTrans)&#25110;&#26679;&#26412;-wise (S-AdaTrans)&#21487;&#36801;&#31227;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#34701;&#21512;&#24809;&#32602;&#26041;&#27861;&#65292;&#32467;&#21512;&#26435;&#37325;&#65292;&#21487;&#20197;&#26681;&#25454;&#21487;&#36801;&#31227;&#32467;&#26500;&#36827;&#34892;&#35843;&#25972;&#12290;&#20026;&#20102;&#36873;&#25321;&#26435;&#37325;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#29702;&#35770;&#19978;&#24314;&#31435;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#36807;&#31243;&#65292;&#20351;&#24471; F-AdaTrans &#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#23558;&#21487;&#36801;&#31227;&#30340;&#20449;&#21495;&#19982;&#30446;&#26631;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#21516;&#26102;&#28388;&#38500;&#38750;&#21487;&#36801;&#31227;&#30340;&#20449;&#21495;&#65292;S-AdaTrans&#21017;&#21487;&#20197;&#33719;&#24471;&#27599;&#20010;&#28304;&#26679;&#26412;&#20256;&#36882;&#30340;&#20449;&#24687;&#30340;&#26368;&#20339;&#32452;&#21512;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#38750;&#28176;&#36817;&#36895;&#29575;&#65292;&#21487;&#20197;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#24674;&#22797;&#29616;&#26377;&#30340;&#36817;&#26368;&#23567;&#20284;&#20046;&#26368;&#20248;&#36895;&#29575;&#12290;&#25928;&#26524;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13565v1 Announce Type: cross  Abstract: We consider the transfer learning problem in the high dimensional setting, where the feature dimension is larger than the sample size. To learn transferable information, which may vary across features or the source samples, we propose an adaptive transfer learning method that can detect and aggregate the feature-wise (F-AdaTrans) or sample-wise (S-AdaTrans) transferable structures. We achieve this by employing a novel fused-penalty, coupled with weights that can adapt according to the transferable structure. To choose the weight, we propose a theoretically informed, data-driven procedure, enabling F-AdaTrans to selectively fuse the transferable signals with the target while filtering out non-transferable signals, and S-AdaTrans to obtain the optimal combination of information transferred from each source sample. The non-asymptotic rates are established, which recover existing near-minimax optimal rates in special cases. The effectivene
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#23450;&#20215;&#21644;&#21305;&#37197;&#31639;&#27861;&#20197;&#26368;&#22823;&#21270;&#24179;&#21488;&#21033;&#28070;&#65292;&#22312;&#26410;&#30693;&#38656;&#27714;&#21644;&#20379;&#24212;&#20989;&#25968;&#19979;&#65292;&#20445;&#25345;&#39038;&#23458;&#21644;&#26381;&#21153;&#22120;&#38431;&#21015;&#38271;&#24230;&#20302;&#20110;&#38408;&#20540;</title><link>https://arxiv.org/abs/2403.11093</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#21452;&#36793;&#38431;&#21015;&#23450;&#20215;&#21644;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Pricing and Matching for Two-Sided Queues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11093
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#23450;&#20215;&#21644;&#21305;&#37197;&#31639;&#27861;&#20197;&#26368;&#22823;&#21270;&#24179;&#21488;&#21033;&#28070;&#65292;&#22312;&#26410;&#30693;&#38656;&#27714;&#21644;&#20379;&#24212;&#20989;&#25968;&#19979;&#65292;&#20445;&#25345;&#39038;&#23458;&#21644;&#26381;&#21153;&#22120;&#38431;&#21015;&#38271;&#24230;&#20302;&#20110;&#38408;&#20540;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#20855;&#26377;&#22810;&#31181;&#31867;&#22411;&#39038;&#23458;&#21644;&#26381;&#21153;&#22120;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;&#27599;&#31181;&#31561;&#24453;&#30340;&#39038;&#23458;&#25110;&#26381;&#21153;&#22120;&#21152;&#20837;&#19968;&#20010;&#21333;&#29420;&#30340;&#38431;&#21015;&#65292;&#24418;&#25104;&#19968;&#20010;&#20855;&#26377;&#39038;&#23458;&#38431;&#21015;&#21644;&#26381;&#21153;&#22120;&#38431;&#21015;&#30340;&#20108;&#37096;&#22270;&#12290;&#24179;&#21488;&#21487;&#20197;&#21305;&#37197;&#26381;&#21153;&#22120;&#21644;&#39038;&#23458;&#65292;&#22914;&#26524;&#23427;&#20204;&#30340;&#31867;&#22411;&#26159;&#20860;&#23481;&#30340;&#12290;&#21305;&#37197;&#30340;&#23545;&#23558;&#31163;&#24320;&#31995;&#32479;&#12290;&#24179;&#21488;&#23558;&#26681;&#25454;&#39038;&#23458;&#30340;&#31867;&#22411;&#25910;&#21462;&#19968;&#20010;&#20215;&#26684;&#65292;&#24403;&#23427;&#20204;&#21040;&#36798;&#26102;&#65292;&#24182;&#26681;&#25454;&#20854;&#31867;&#22411;&#21521;&#26381;&#21153;&#22120;&#25903;&#20184;&#19968;&#20010;&#20215;&#26684;&#12290;&#27599;&#20010;&#38431;&#21015;&#30340;&#21040;&#36798;&#29575;&#21462;&#20915;&#20110;&#26576;&#20123;&#26410;&#30693;&#30340;&#38656;&#27714;&#25110;&#20379;&#24212;&#20989;&#25968;&#25353;&#20215;&#26684;&#30830;&#23450;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#23450;&#20215;&#21644;&#21305;&#37197;&#31639;&#27861;&#65292;&#20197;&#26368;&#22823;&#21270;&#24179;&#21488;&#22312;&#26410;&#30693;&#38656;&#27714;&#21644;&#20379;&#24212;&#20989;&#25968;&#19979;&#30340;&#21033;&#28070;&#65292;&#21516;&#26102;&#20445;&#25345;&#39038;&#23458;&#21644;&#26381;&#21153;&#22120;&#30340;&#38431;&#21015;&#38271;&#24230;&#20302;&#20110;&#39044;&#23450;&#38408;&#20540;&#12290;&#36825;&#20010;&#31995;&#32479;&#21487;&#20197;&#29992;&#26469;&#24314;&#27169;&#20687;&#20056;&#36710;&#20849;&#20139;&#24066;&#22330;&#36825;&#26679;&#30340;&#21452;&#36793;&#24066;&#22330;&#65292;&#26377;&#20056;&#23458;&#21644;&#21496;&#26426;&#12290;&#25361;&#25112;&#22312;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11093v1 Announce Type: cross  Abstract: We consider a dynamic system with multiple types of customers and servers. Each type of waiting customer or server joins a separate queue, forming a bipartite graph with customer-side queues and server-side queues. The platform can match the servers and customers if their types are compatible. The matched pairs then leave the system. The platform will charge a customer a price according to their type when they arrive and will pay a server a price according to their type. The arrival rate of each queue is determined by the price according to some unknown demand or supply functions. Our goal is to design pricing and matching algorithms to maximize the profit of the platform with unknown demand and supply functions, while keeping queue lengths of both customers and servers below a predetermined threshold. This system can be used to model two-sided markets such as ride-sharing markets with passengers and drivers. The difficulties of the pr
&lt;/p&gt;</description></item><item><title>&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#29983;&#23384;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#20419;&#36827;&#20102;&#36879;&#26126;&#24230;&#21644;&#20844;&#24179;&#24615;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#28508;&#22312;&#20559;&#35265;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#31526;&#21512;&#25968;&#23398;&#21407;&#29702;&#30340;&#29305;&#24449;&#24433;&#21709;&#21644;&#39118;&#38505;&#22240;&#32032;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10250</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Interpretable Machine Learning for Survival Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10250
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#29983;&#23384;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#20419;&#36827;&#20102;&#36879;&#26126;&#24230;&#21644;&#20844;&#24179;&#24615;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#28508;&#22312;&#20559;&#35265;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#31526;&#21512;&#25968;&#23398;&#21407;&#29702;&#30340;&#29305;&#24449;&#24433;&#21709;&#21644;&#39118;&#38505;&#22240;&#32032;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20256;&#25773;&#21644;&#24555;&#36895;&#36827;&#27493;&#65292;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;IML&#65289;&#39046;&#22495;&#25110;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290; &#36825;&#22312;&#29983;&#23384;&#20998;&#26512;&#39046;&#22495;&#23588;&#20026;&#37325;&#35201;&#65292;&#20854;&#20013;&#37319;&#29992;IML&#25216;&#26415;&#20419;&#36827;&#20102;&#36879;&#26126;&#24230;&#12289;&#38382;&#36131;&#21046;&#21644;&#20844;&#24179;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#12289;&#26377;&#38024;&#23545;&#24615;&#30103;&#27861;&#30340;&#24320;&#21457;&#12289;&#24178;&#39044;&#25110;&#20854;&#20182;&#21307;&#23398;&#25110;&#19982;&#21307;&#30103;&#20445;&#20581;&#30456;&#20851;&#30340;&#29615;&#22659;&#20013;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#21487;&#35299;&#37322;&#24615;&#21487;&#20197;&#25581;&#31034;&#29983;&#23384;&#27169;&#22411;&#30340;&#28508;&#22312;&#20559;&#35265;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#26356;&#31526;&#21512;&#25968;&#23398;&#21407;&#29702;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#21738;&#20123;&#29305;&#24449;&#23545;&#39044;&#27979;&#26377;&#24433;&#21709;&#25110;&#26500;&#25104;&#39118;&#38505;&#22240;&#32032;&#12290; &#28982;&#32780;&#65292;&#32570;&#20047;&#21363;&#26102;&#21487;&#29992;&#30340;IML&#26041;&#27861;&#21487;&#33021;&#24050;&#32463;&#38459;&#30861;&#20102;&#21307;&#23398;&#20174;&#19994;&#32773;&#21644;&#20844;&#20849;&#21355;&#29983;&#25919;&#31574;&#21046;&#23450;&#32773;&#20805;&#20998;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10250v1 Announce Type: cross  Abstract: With the spread and rapid advancement of black box machine learning models, the field of interpretable machine learning (IML) or explainable artificial intelligence (XAI) has become increasingly important over the last decade. This is particularly relevant for survival analysis, where the adoption of IML techniques promotes transparency, accountability and fairness in sensitive areas, such as clinical decision making processes, the development of targeted therapies, interventions or in other medical or healthcare related contexts. More specifically, explainability can uncover a survival model's potential biases and limitations and provide more mathematically sound ways to understand how and which features are influential for prediction or constitute risk factors. However, the lack of readily available IML methods may have deterred medical practitioners and policy makers in public health from leveraging the full potential of machine lea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SHERD&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#30417;&#25511;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#26089;&#26399;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#65292;&#21033;&#29992;&#26631;&#20934;&#36317;&#31163;&#24230;&#37327;&#26816;&#27979;&#26131;&#21463;&#25915;&#20987;&#33410;&#28857;&#65292;&#20174;&#32780;&#22312;&#22270;&#36755;&#20837;&#20013;&#23454;&#29616;&#24615;&#33021;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09901</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#25511;&#26089;&#26399;&#35757;&#32451;&#34920;&#31034;&#26469;&#23454;&#29616;&#40065;&#26834;&#30340;&#23376;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Subgraph Learning by Monitoring Early Training Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SHERD&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#30417;&#25511;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#26089;&#26399;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#65292;&#21033;&#29992;&#26631;&#20934;&#36317;&#31163;&#24230;&#37327;&#26816;&#27979;&#26131;&#21463;&#25915;&#20987;&#33410;&#28857;&#65292;&#20174;&#32780;&#22312;&#22270;&#36755;&#20837;&#20013;&#23454;&#29616;&#24615;&#33021;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#25991;:2403.09901v1 &#20844;&#21578;&#31867;&#22411;:&#26032;&#25688;&#35201;:&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22240;&#22312;&#22270;&#23398;&#20064;&#21644;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#26131;&#21463;&#25915;&#20987;&#30340;&#33410;&#28857;&#65292;&#32473;&#20915;&#31574;&#21046;&#23450;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#40065;&#26834;&#30340;&#22270;&#25688;&#35201;&#38656;&#27714;&#22312;&#20110;&#23545;&#25239;&#24615;&#25361;&#25112;&#20250;&#23548;&#33268;&#25915;&#20987;&#22312;&#25972;&#20010;&#22270;&#20013;&#20256;&#25773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#25216;&#26415;SHERD (&#36890;&#36807;&#26089;&#26399;&#35757;&#32451;&#34920;&#31034;&#36317;&#31163;&#36827;&#34892;&#23376;&#22270;&#23398;&#20064;)&#26469;&#35299;&#20915;&#22270;&#36755;&#20837;&#20013;&#30340;&#24615;&#33021;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;SHERD&#21033;&#29992;&#37096;&#20998;&#35757;&#32451;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#30340;&#23618;&#20449;&#24687;&#65292;&#36890;&#36807;&#26631;&#20934;&#36317;&#31163;&#24230;&#37327;&#26469;&#26816;&#27979;&#23545;&#25239;&#25915;&#20987;&#26399;&#38388;&#26131;&#21463;&#25915;&#20987;&#30340;&#33410;&#28857;&#12290;&#35813;&#26041;&#27861;&#35782;&#21035;&#20986;"&#26131;&#21463;&#25915;&#20987;&#30340;(&#22351;)"&#33410;&#28857;&#24182;&#31227;&#38500;&#36825;&#20123;&#33410;&#28857;&#65292;&#24418;&#25104;&#19968;&#20010;&#40065;&#26834;&#30340;&#23376;&#22270;&#65292;&#21516;&#26102;&#20445;&#25345;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09901v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have attracted significant attention for their outstanding performance in graph learning and node classification tasks. However, their vulnerability to adversarial attacks, particularly through susceptible nodes, poses a challenge in decision-making. The need for robust graph summarization is evident in adversarial challenges resulting from the propagation of attacks throughout the entire graph. In this paper, we address both performance and adversarial robustness in graph input by introducing the novel technique SHERD (Subgraph Learning Hale through Early Training Representation Distances). SHERD leverages information from layers of a partially trained graph convolutional network (GCN) to detect susceptible nodes during adversarial attacks using standard distance metrics. The method identifies "vulnerable (bad)" nodes and removes such nodes to form a robust subgraph while maintaining node classification perf
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#26032;&#25351;&#26631;&#23545;&#25239;&#36229;&#20307;&#31215;&#26469;&#20840;&#38754;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#31181;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#37319;&#29992;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05100</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#25239;&#30028;&#38480;&#65306;&#36890;&#36807;&#23545;&#25239;&#36229;&#20307;&#31215;&#37327;&#21270;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Adversarial Frontier: Quantifying Robustness via Adversarial Hypervolume
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05100
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#26032;&#25351;&#26631;&#23545;&#25239;&#36229;&#20307;&#31215;&#26469;&#20840;&#38754;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#31181;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#37319;&#29992;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38754;&#20020;&#26085;&#30410;&#20005;&#37325;&#30340;&#23545;&#25239;&#25915;&#20987;&#23041;&#32961;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#23545;&#40065;&#26834;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#20256;&#32479;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#20381;&#36182;&#20110;&#23545;&#25239;&#20934;&#30830;&#24615;&#65292;&#35813;&#25351;&#26631;&#34913;&#37327;&#27169;&#22411;&#22312;&#29305;&#23450;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#21333;&#19968;&#25351;&#26631;&#24182;&#19981;&#33021;&#23436;&#20840;&#27010;&#25324;&#27169;&#22411;&#23545;&#19981;&#21516;&#31243;&#24230;&#25200;&#21160;&#30340;&#25972;&#20307;&#38887;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#23545;&#25239;&#36229;&#20307;&#31215;&#65292;&#20174;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#35282;&#24230;&#32508;&#21512;&#35780;&#20272;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#25351;&#26631;&#20801;&#35768;&#28145;&#20837;&#27604;&#36739;&#38450;&#24481;&#26426;&#21046;&#65292;&#24182;&#25215;&#35748;&#20102;&#36739;&#24369;&#30340;&#38450;&#24481;&#31574;&#30053;&#25152;&#24102;&#26469;&#30340;&#40065;&#26834;&#24615;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#22343;&#21248;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05100v1 Announce Type: cross  Abstract: The escalating threat of adversarial attacks on deep learning models, particularly in security-critical fields, has underscored the need for robust deep learning systems. Conventional robustness evaluations have relied on adversarial accuracy, which measures a model's performance under a specific perturbation intensity. However, this singular metric does not fully encapsulate the overall resilience of a model against varying degrees of perturbation. To address this gap, we propose a new metric termed adversarial hypervolume, assessing the robustness of deep learning models comprehensively over a range of perturbation intensities from a multi-objective optimization standpoint. This metric allows for an in-depth comparison of defense mechanisms and recognizes the trivial improvements in robustness afforded by less potent defensive strategies. Additionally, we adopt a novel training algorithm that enhances adversarial robustness uniformly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#36827;&#21270;&#33539;&#24335;&#30340;&#26032;&#39062;&#36951;&#20256;&#27169;&#25311;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#21322;&#30417;&#30563;&#32858;&#31867;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#27425;&#22312;&#36825;&#20010;&#39046;&#22495;&#23581;&#35797;&#23450;&#20041;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.04322</link><description>&lt;p&gt;
&#22522;&#20110;&#36951;&#20256;&#27169;&#25311;&#30340;&#24046;&#20998;&#36827;&#21270;&#26041;&#27861;&#29992;&#20110;&#21322;&#30417;&#30563;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Memetic Differential Evolution Methods for Semi-Supervised Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#36827;&#21270;&#33539;&#24335;&#30340;&#26032;&#39062;&#36951;&#20256;&#27169;&#25311;&#31574;&#30053;&#65292;&#29992;&#20110;&#35299;&#20915;&#21322;&#30417;&#30563;&#32858;&#31867;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#27425;&#22312;&#36825;&#20010;&#39046;&#22495;&#23581;&#35797;&#23450;&#20041;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22788;&#29702;&#21322;&#30417;&#30563;&#26368;&#23567;&#24179;&#26041;&#21644;&#32858;&#31867;(MSSC)&#38382;&#39064;&#65292;&#20854;&#20013;&#32972;&#26223;&#30693;&#35782;&#20197;&#23454;&#20363;&#32423;&#32422;&#26463;&#30340;&#24418;&#24335;&#32473;&#23450;&#12290;&#25105;&#20204;&#29305;&#21035;&#32771;&#34385;&#8220;&#24517;&#36830;&#25509;&#8221;&#21644;&#8220;&#38750;&#36830;&#25509;&#8221;&#32422;&#26463;&#65292;&#27599;&#20010;&#32422;&#26463;&#25351;&#31034;&#20004;&#20010;&#25968;&#25454;&#38598;&#28857;&#26159;&#21542;&#24212;&#35813;&#20851;&#32852;&#21040;&#21516;&#19968;&#20010;&#25110;&#19981;&#21516;&#30340;&#31751;&#20013;&#12290;&#36825;&#20123;&#32422;&#26463;&#30340;&#23384;&#22312;&#20351;&#24471;&#38382;&#39064;&#33267;&#23569;&#19982;&#20854;&#26080;&#30417;&#30563;&#29256;&#26412;&#19968;&#26679;&#22256;&#38590;&#65306;&#19981;&#20877;&#27599;&#20010;&#28857;&#37117;&#20851;&#32852;&#21040;&#20854;&#26368;&#36817;&#30340;&#31751;&#20013;&#24515;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#20851;&#38190;&#25805;&#20316;&#65288;&#22914;&#20998;&#37197;&#27493;&#39588;&#65289;&#20013;&#36827;&#34892;&#19968;&#20123;&#20462;&#25913;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#36827;&#21270;&#33539;&#24335;&#30340;&#26032;&#39062;&#36951;&#20256;&#27169;&#25311;&#31574;&#30053;&#65292;&#30452;&#25509;&#25193;&#23637;&#20102;&#26368;&#36817;&#22312;&#26080;&#30417;&#30563;&#32858;&#31867;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#26368;&#26032;&#26694;&#26550;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#20195;&#34920;&#20102;&#31532;&#19968;&#27425;&#23581;&#35797;&#23450;&#20041;&#19968;&#20010;&#26088;&#22312;&#29983;&#25104;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04322v1 Announce Type: cross  Abstract: In this paper, we deal with semi-supervised Minimum Sum-of-Squares Clustering (MSSC) problems where background knowledge is given in the form of instance-level constraints. In particular, we take into account "must-link" and "cannot-link" constraints, each of which indicates if two dataset points should be associated to the same or to a different cluster. The presence of such constraints makes the problem at least as hard as its unsupervised version: it is no more true that each point is associated to its nearest cluster center, thus requiring some modifications in crucial operations, such as the assignment step. In this scenario, we propose a novel memetic strategy based on the Differential Evolution paradigm, directly extending a state-of-the-art framework recently proposed in the unsupervised clustering literature. As far as we know, our contribution represents the first attempt to define a memetic methodology designed to generate a
&lt;/p&gt;</description></item><item><title>ARNN&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#21644;&#24182;&#34892;&#35745;&#31639;&#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#21644;LSTM gate&#30340;&#20248;&#21183;&#65292;&#24182;&#36991;&#20813;&#20102;&#23427;&#20204;&#30340;&#32570;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.03276</link><description>&lt;p&gt;
ARNN: &#29992;&#20110;&#35782;&#21035;&#30315;&#30187;&#21457;&#20316;&#30340;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ARNN: Attentive Recurrent Neural Network for Multi-channel EEG Signals to Identify Epileptic Seizures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03276
&lt;/p&gt;
&lt;p&gt;
ARNN&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#21644;&#24182;&#34892;&#35745;&#31639;&#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#21644;LSTM gate&#30340;&#20248;&#21183;&#65292;&#24182;&#36991;&#20813;&#20102;&#23427;&#20204;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;ARNN&#65289;&#65292;&#20854;&#27839;&#30528;&#24207;&#21015;&#24490;&#29615;&#24212;&#29992;&#27880;&#24847;&#21147;&#23618;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#24207;&#21015;&#38271;&#24230;&#30456;&#20851;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#19978;&#36816;&#34892;&#65292;&#32780;&#19981;&#26159;&#21333;&#36890;&#36947;&#20449;&#21495;&#65292;&#24182;&#21033;&#29992;&#24182;&#34892;&#35745;&#31639;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#27880;&#24847;&#21147;&#23618;&#26159;&#19968;&#31181;&#35745;&#31639;&#21333;&#20803;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35745;&#31639;&#19968;&#32452;&#24191;&#27867;&#25968;&#37327;&#30340;&#29366;&#24577;&#21521;&#37327;&#21644;&#36755;&#20837;&#20449;&#21495;&#30340;&#36882;&#24402;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21463;&#21040;&#20102;&#27880;&#24847;&#21147;&#23618;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21333;&#20803;&#30340;&#21551;&#21457;&#65292;&#24182;&#20351;&#29992;&#38271;&#30701;&#39118;&#26684;&#38376;&#65292;&#20294;&#36890;&#36807;&#22810;&#20010;&#38454;&#27573;&#23558;&#36825;&#31181;&#20856;&#22411;&#21333;&#20803;&#25193;&#23637;&#21040;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#24182;&#34892;&#21270;&#12290;&#23427;&#32487;&#25215;&#20102;&#27880;&#24847;&#21147;&#23618;&#21644;LSTM&#38376;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#32570;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#24322;&#36136;&#23454;&#39564;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#22411;&#26377;&#25928;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03276v1 Announce Type: cross  Abstract: We proposed an Attentive Recurrent Neural Network (ARNN), which recurrently applies attention layers along a sequence and has linear complexity with respect to the sequence length. The proposed model operates on multi-channel EEG signals rather than single channel signals and leverages parallel computation. In this cell, the attention layer is a computational unit that efficiently applies self-attention and cross-attention mechanisms to compute a recurrent function over a wide number of state vectors and input signals. Our architecture is inspired in part by the attention layer and long short-term memory (LSTM) cells, and it uses long-short style gates, but it scales this typical cell up by several orders to parallelize for multi-channel EEG signals. It inherits the advantages of attention layers and LSTM gate while avoiding their respective drawbacks. We evaluated the model effectiveness through extensive experiments with heterogeneou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;Polyak&#21160;&#37327;&#30340;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#38750;&#20984;&#22797;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#65292;&#26080;&#35770;&#25209;&#37327;&#22823;&#23567;&#22914;&#20309;&#12290;</title><link>https://arxiv.org/abs/2403.02967</link><description>&lt;p&gt;
&#20855;&#26377;Polyak&#21160;&#37327;&#30340;&#38750;&#20984;&#38543;&#26426;&#22797;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-Convex Stochastic Composite Optimization with Polyak Momentum
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;Polyak&#21160;&#37327;&#30340;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#38750;&#20984;&#22797;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#65292;&#26080;&#35770;&#25209;&#37327;&#22823;&#23567;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#27861;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#27861;&#30340;&#19968;&#20010;&#24378;&#22823;&#27867;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#24403;&#38543;&#26426;&#22122;&#22768;&#26174;&#33879;&#26102;&#65288;&#21363;&#20165;&#20351;&#29992;&#23567;&#22411;&#25110;&#26377;&#30028;&#25209;&#37327;&#22823;&#23567;&#26102;&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#20984;&#29615;&#22659;&#20013;&#26080;&#27861;&#25910;&#25947;&#12290;&#26412;&#25991;&#20851;&#27880;&#20855;&#26377;Polyak&#21160;&#37327;&#30340;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#38750;&#20984;&#22797;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#29616;&#20102;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#25209;&#37327;&#22823;&#23567;&#22823;&#23567;&#26080;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;Polyak&#21160;&#37327;&#22312;&#22797;&#21512;&#20248;&#21270;&#29615;&#22659;&#20013;&#30340;&#26041;&#24046;&#20943;&#23569;&#25928;&#24212;&#36827;&#34892;&#20102;&#20005;&#26684;&#20998;&#26512;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#36817;&#31471;&#27493;&#39588;&#21482;&#33021;&#36890;&#36807;&#36817;&#20284;&#35299;&#26469;&#27714;&#35299;&#26102;&#65292;&#35813;&#26041;&#27861;&#20063;&#20250;&#25910;&#25947;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02967v1 Announce Type: cross  Abstract: The stochastic proximal gradient method is a powerful generalization of the widely used stochastic gradient descent (SGD) method and has found numerous applications in Machine Learning. However, it is notoriously known that this method fails to converge in non-convex settings where the stochastic noise is significant (i.e. when only small or bounded batch sizes are used). In this paper, we focus on the stochastic proximal gradient method with Polyak momentum. We prove this method attains an optimal convergence rate for non-convex composite optimization problems, regardless of batch size. Additionally, we rigorously analyze the variance reduction effect of the Polyak momentum in the composite optimization setting and we show the method also converges when the proximal step can only be solved inexactly. Finally, we provide numerical experiments to validate our theoretical results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#33539;&#30068;&#35770;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23436;&#20840;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#22810;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.02598</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#20010;&#21327;&#21464;&#37327;&#36716;&#31227;&#21644;&#19981;&#24179;&#34913;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Pooling Image Datasets With Multiple Covariate Shift and Imbalance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#33539;&#30068;&#35770;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23436;&#20840;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#22810;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23398;&#31185;&#20013;&#24120;&#35265;&#23567;&#26679;&#26412;&#22823;&#23567;&#65292;&#36825;&#38656;&#35201;&#36328;&#22810;&#20010;&#26426;&#26500;&#27719;&#24635;&#22823;&#33268;&#30456;&#20284;&#30340;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#22270;&#20687;&#19982;&#30142;&#30149;&#32467;&#26524;&#20043;&#38388;&#30340;&#24369;&#20294;&#30456;&#20851;&#20851;&#32852;&#12290;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#20307;&#29616;&#20986;&#21327;&#21464;&#37327;&#65288;&#21363;&#27425;&#35201;&#30340;&#38750;&#25104;&#20687;&#25968;&#25454;&#65289;&#30340;&#36716;&#31227;/&#19981;&#24179;&#34913;&#12290;&#22312;&#26631;&#20934;&#32479;&#35745;&#20998;&#26512;&#20013;&#25511;&#21046;&#36825;&#20123;&#26080;&#29992;&#21464;&#37327;&#26159;&#24120;&#35265;&#30340;&#65292;&#20294;&#36825;&#20123;&#24605;&#24819;&#24182;&#19981;&#30452;&#25509;&#36866;&#29992;&#20110;&#21442;&#25968;&#36807;&#22810;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#20174;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24847;&#20041;&#30340;&#36215;&#28857;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#24211;&#20165;&#38480;&#20110;&#19968;&#27425;&#32771;&#34385;&#20960;&#20010;&#21327;&#21464;&#37327;&#30340;&#36716;&#31227;/&#19981;&#24179;&#34913;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#33539;&#30068;&#35770;&#30340;&#35282;&#24230;&#30475;&#24453;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23436;&#20840;&#36991;&#20813;&#20102;&#21407;&#26412;&#38656;&#35201;&#22797;&#26434;&#30340;&#22810;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02598v1 Announce Type: new  Abstract: Small sample sizes are common in many disciplines, which necessitates pooling roughly similar datasets across multiple institutions to study weak but relevant associations between images and disease outcomes. Such data often manifest shift/imbalance in covariates (i.e., secondary non-imaging data). Controlling for such nuisance variables is common within standard statistical analysis, but the ideas do not directly apply to overparameterized models. Consequently, recent work has shown how strategies from invariant representation learning provides a meaningful starting point, but the current repertoire of methods is limited to accounting for shifts/imbalances in just a couple of covariates at a time. In this paper, we show how viewing this problem from the perspective of Category theory provides a simple and effective solution that completely avoids elaborate multi-stage training pipelines that would otherwise be needed. We show the effect
&lt;/p&gt;</description></item><item><title>&#24322;&#36136;&#24615;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#20013;&#20986;&#29616;&#22240;&#26524;&#24615;&#30340;&#36129;&#29486;&#35299;&#37322;&#20102;&#20026;&#20309;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#20851;&#32852;&#24615;&#35757;&#32451;&#20013;&#25581;&#31034;&#22240;&#26524;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2403.01420</link><description>&lt;p&gt;
&#24322;&#36136;&#24615;&#23545;&#19981;&#21464;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;&#38544;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
The Implicit Bias of Heterogeneity towards Invariance and Causality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01420
&lt;/p&gt;
&lt;p&gt;
&#24322;&#36136;&#24615;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#20013;&#20986;&#29616;&#22240;&#26524;&#24615;&#30340;&#36129;&#29486;&#35299;&#37322;&#20102;&#20026;&#20309;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#20851;&#32852;&#24615;&#35757;&#32451;&#20013;&#25581;&#31034;&#22240;&#26524;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32463;&#39564;&#19978;&#35266;&#23519;&#21040;&#65292;&#20351;&#29992;&#26469;&#33258;&#20114;&#32852;&#32593;&#30340;&#22823;&#37327;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20351;&#29992;&#19968;&#31181;&#21464;&#20307;&#22238;&#24402;&#25439;&#22833;&#65292;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25581;&#31034;&#22240;&#26524;&#20851;&#32852;&#12290;&#36825;&#19982;&#20256;&#32479;&#26234;&#24935;&#8220;&#20851;&#32852;&#19981;&#26159;&#22240;&#26524;&#8221;&#20197;&#21450;&#20256;&#32479;&#22240;&#26524;&#25512;&#26029;&#33539;&#24335;&#30456;&#21453;&#65292;&#20256;&#32479;&#22240;&#26524;&#25512;&#26029;&#33539;&#24335;&#35748;&#20026;&#20808;&#21069;&#30340;&#22240;&#26524;&#30693;&#35782;&#24212;&#35880;&#24910;&#22320;&#32435;&#20837;&#21040;&#26041;&#27861;&#35774;&#35745;&#20013;&#12290;&#20196;&#20154;&#22256;&#24785;&#30340;&#26159;&#65292;&#20026;&#20309;&#22312;&#36861;&#27714;&#20851;&#32852;&#30340;&#22238;&#24402;&#20219;&#21153;&#20013;&#33021;&#22815;&#20174;&#26356;&#39640;&#23618;&#27425;&#30340;&#29702;&#35299;&#20013;&#20986;&#29616;&#22240;&#26524;&#24615;&#12290;&#26412;&#25991;&#22768;&#31216;&#20174;&#38754;&#21521;&#20851;&#32852;&#30340;&#35757;&#32451;&#20013;&#20986;&#29616;&#22240;&#26524;&#24615;&#21487;&#20197;&#24402;&#22240;&#20110;&#28304;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12289;&#35757;&#32451;&#31639;&#27861;&#30340;&#38543;&#26426;&#24615;&#21644;&#23398;&#20064;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#21270;&#30340;&#32806;&#21512;&#25928;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#35265;&#22320;&#30340;&#27169;&#22411;&#26469;&#38416;&#37322;&#36825;&#26679;&#30340;&#30452;&#35273;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#22238;&#24402;&#25439;&#22833;&#23398;&#20064;&#19981;&#21464;&#24615;&#65292;&#19968;&#31181;&#20934;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01420v1 Announce Type: new  Abstract: It is observed empirically that the large language models (LLM), trained with a variant of regression loss using numerous corpus from the Internet, can unveil causal associations to some extent. This is contrary to the traditional wisdom that ``association is not causation'' and the paradigm of traditional causal inference in which prior causal knowledge should be carefully incorporated into the design of methods. It is a mystery why causality, in a higher layer of understanding, can emerge from the regression task that pursues associations. In this paper, we claim the emergence of causality from association-oriented training can be attributed to the coupling effects from the heterogeneity of the source data, stochasticity of training algorithms, and over-parameterization of the learning models. We illustrate such an intuition using a simple but insightful model that learns invariance, a quasi-causality, using regression loss. To be spec
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20195;&#29702;&#36827;&#34892;&#29305;&#24449;&#23545;&#40784;&#65292;&#20197;&#35299;&#20915;&#39044;&#20808;&#35745;&#31639;&#29305;&#24449;&#26080;&#27861;&#21306;&#20998;&#26631;&#35760;&#26679;&#26412;&#31867;&#21035;&#21644;&#36991;&#20813;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#36873;&#25321;&#26679;&#26412;&#26102;&#29306;&#29298;&#23453;&#36149;&#39044;&#35757;&#32451;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01101</link><description>&lt;p&gt;
&#29305;&#24449;&#23545;&#40784;&#65306;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#32972;&#26223;&#19979;&#36890;&#36807;&#20195;&#29702;&#24605;&#32771;&#39640;&#25928;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Feature Alignment: Rethinking Efficient Active Learning via Proxy in the Context of Pre-trained Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01101
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20195;&#29702;&#36827;&#34892;&#29305;&#24449;&#23545;&#40784;&#65292;&#20197;&#35299;&#20915;&#39044;&#20808;&#35745;&#31639;&#29305;&#24449;&#26080;&#27861;&#21306;&#20998;&#26631;&#35760;&#26679;&#26412;&#31867;&#21035;&#21644;&#36991;&#20813;&#36890;&#36807;&#20195;&#29702;&#27169;&#22411;&#36873;&#25321;&#26679;&#26412;&#26102;&#29306;&#29298;&#23453;&#36149;&#39044;&#35757;&#32451;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26377;&#26395;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32452;&#21512;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#38271;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#20195;&#29702;&#30340;&#20027;&#21160;&#23398;&#20064;&#65292;&#23427;&#39044;&#20808;&#35745;&#31639;&#29305;&#24449;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#20250;&#22312;&#20027;&#21160;&#23398;&#20064;&#24615;&#33021;&#19978;&#36896;&#25104;&#37325;&#22823;&#25439;&#22833;&#65292;&#29978;&#33267;&#21487;&#33021;&#36229;&#36807;&#35745;&#31639;&#25104;&#26412;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01101v1 Announce Type: cross  Abstract: Fine-tuning the pre-trained model with active learning holds promise for reducing annotation costs. However, this combination introduces significant computational costs, particularly with the growing scale of pre-trained models. Recent research has proposed proxy-based active learning, which pre-computes features to reduce computational costs. Yet, this approach often incurs a significant loss in active learning performance, which may even outweigh the computational cost savings. In this paper, we argue the performance drop stems not only from pre-computed features' inability to distinguish between categories of labeled samples, resulting in the selection of redundant samples but also from the tendency to compromise valuable pre-trained information when fine-tuning with samples selected through the proxy model. To address this issue, we propose a novel method called aligned selection via proxy to update pre-computed features while sele
&lt;/p&gt;</description></item><item><title>RLHF&#22312;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#24615;&#33021;&#25110;&#36807;&#24230;&#36777;&#25252;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23398;&#26465;&#20214;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2402.17747</link><description>&lt;p&gt;
&#24403;&#20320;&#30340;AI&#27450;&#39575;&#20320;&#65306;&#22312;&#22870;&#21169;&#23398;&#20064;&#20013;&#20154;&#31867;&#35780;&#20272;&#32773;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17747
&lt;/p&gt;
&lt;p&gt;
RLHF&#22312;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#24615;&#33021;&#25110;&#36807;&#24230;&#36777;&#25252;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23398;&#26465;&#20214;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#36807;&#21435;&#20998;&#26512;&#20551;&#35774;&#20154;&#31867;&#23436;&#20840;&#35266;&#23519;&#21040;&#29615;&#22659;&#12290;&#24403;&#20154;&#31867;&#21453;&#39304;&#20165;&#22522;&#20110;&#37096;&#20998;&#35266;&#23519;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#25105;&#20204;&#23545;&#20004;&#31181;&#22833;&#36133;&#24773;&#20917;&#36827;&#34892;&#20102;&#27491;&#24335;&#23450;&#20041;&#65306;&#27450;&#39575;&#21644;&#36807;&#24230;&#36777;&#25252;&#12290;&#36890;&#36807;&#23558;&#20154;&#31867;&#24314;&#27169;&#20026;&#23545;&#36712;&#36857;&#20449;&#24565;&#30340;Boltzmann-&#29702;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RLHF&#20445;&#35777;&#20250;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#20854;&#24615;&#33021;&#12289;&#20026;&#20102;&#30041;&#19979;&#21360;&#35937;&#32780;&#36807;&#24230;&#36777;&#25252;&#25110;&#32773;&#20004;&#32773;&#20860;&#32780;&#26377;&#20043;&#30340;&#26465;&#20214;&#12290;&#20026;&#20102;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25968;&#23398;&#22320;&#21051;&#30011;&#20102;&#29615;&#22659;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#22914;&#20309;&#36716;&#21270;&#20026;&#65288;&#32570;&#20047;&#65289;&#23398;&#21040;&#30340;&#22238;&#25253;&#20989;&#25968;&#20013;&#30340;&#27169;&#31946;&#24615;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#32771;&#34385;&#29615;&#22659;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#20351;&#24471;&#22312;&#29702;&#35770;&#19978;&#21487;&#33021;&#24674;&#22797;&#22238;&#25253;&#20989;&#25968;&#21644;&#26368;&#20248;&#31574;&#30053;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19981;&#21487;&#20943;&#23569;&#30340;&#27169;&#31946;&#24615;&#12290;&#25105;&#20204;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17747v1 Announce Type: cross  Abstract: Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deception and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function. In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity. We caution against blindly applying RLHF in partially observa
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#26399;&#21002;&#25991;&#31456;&#65292;&#24635;&#32467;&#20102;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#22312;&#24494;&#29983;&#29289;&#32452;&#23398;&#20013;&#30340;&#29616;&#26377;&#23454;&#36341;&#65292;&#25506;&#35752;&#20102;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36991;&#20813;&#24120;&#35265;&#23454;&#39564;&#35774;&#35745;&#32570;&#38519;&#30340;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2402.17621</link><description>&lt;p&gt;
&#29992;&#20110;&#24494;&#29983;&#29289;&#32452;&#23398;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#65306;&#24357;&#21512;&#24403;&#21069;&#21644;&#26368;&#20339;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Supervised machine learning for microbiomics: bridging the gap between current and best practices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17621
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#26399;&#21002;&#25991;&#31456;&#65292;&#24635;&#32467;&#20102;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#22312;&#24494;&#29983;&#29289;&#32452;&#23398;&#20013;&#30340;&#29616;&#26377;&#23454;&#36341;&#65292;&#25506;&#35752;&#20102;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36991;&#20813;&#24120;&#35265;&#23454;&#39564;&#35774;&#35745;&#32570;&#38519;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#23558;&#21152;&#36895;&#20020;&#24202;&#24494;&#29983;&#29289;&#32452;&#23398;&#21019;&#26032;&#65292;&#22914;&#30142;&#30149;&#35786;&#26029;&#21644;&#39044;&#21518;&#12290;&#36825;&#23558;&#38656;&#35201;&#39640;&#36136;&#37327;&#12289;&#21487;&#37325;&#29616;&#12289;&#21487;&#35299;&#37322;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#20854;&#39044;&#27979;&#33021;&#21147;&#36798;&#21040;&#25110;&#36229;&#36807;&#30417;&#31649;&#26426;&#26500;&#23545;&#20020;&#24202;&#24037;&#20855;&#35774;&#23450;&#30340;&#39640;&#38376;&#27099;&#12290;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;2021-2022&#24180;&#21457;&#34920;&#30340;100&#31687;&#21516;&#34892;&#35780;&#35758;&#30340;&#26399;&#21002;&#25991;&#31456;&#65292;&#25429;&#25417;&#20102;&#24403;&#21069;&#23558;&#30417;&#30563;ML&#24212;&#29992;&#20110;&#24494;&#29983;&#29289;&#32452;&#23398;&#25968;&#25454;&#30340;&#23454;&#36341;&#30340;&#19968;&#20010;&#24555;&#29031;&#12290;&#25105;&#20204;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#24341;&#23548;&#35752;&#35770;&#21508;&#31181;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#21253;&#25324;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#22914;&#20309;&#20943;&#36731;&#23567;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#21516;&#26102;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20851;&#20110;&#22914;&#20309;&#36991;&#20813;&#21487;&#33021;&#25439;&#23475;&#27169;&#22411;&#24615;&#33021;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#30340;&#24120;&#35265;&#23454;&#39564;&#35774;&#35745;&#32570;&#38519;&#30340;&#25351;&#21335;&#12290;&#35752;&#35770;&#38468;&#26377;&#19968;&#20010;&#20114;&#21160;&#22312;&#32447;&#25945;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17621v1 Announce Type: cross  Abstract: Machine learning (ML) is set to accelerate innovations in clinical microbiomics, such as in disease diagnostics and prognostics. This will require high-quality, reproducible, interpretable workflows whose predictive capabilities meet or exceed the high thresholds set for clinical tools by regulatory agencies. Here, we capture a snapshot of current practices in the application of supervised ML to microbiomics data, through an in-depth analysis of 100 peer-reviewed journal articles published in 2021-2022. We apply a data-driven approach to steer discussion of the merits of varied approaches to experimental design, including key considerations such as how to mitigate the effects of small dataset size while avoiding data leakage. We further provide guidance on how to avoid common experimental design pitfalls that can hurt model performance, trustworthiness, and reproducibility. Discussion is accompanied by an interactive online tutorial th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36870;&#21521;&#24037;&#31243;&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#35266;&#23519;&#20102;Transformer&#20869;&#37096;&#30005;&#36335;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#20943;&#27861;&#22312;Transformer&#19978;&#36896;&#25104;&#20102;&#24378;&#28872;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#20056;&#27861;&#38656;&#35201;&#20313;&#24358;&#20559;&#32622;&#20998;&#37327;&#65292;&#22810;&#39033;&#24335;&#21472;&#21152;&#20102;&#22522;&#26412;&#31639;&#26415;&#27169;&#24335;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#24182;&#19981;&#28165;&#26224;&#65292;Grokking&#29978;&#33267;&#21487;&#20197;&#22312;&#20855;&#26377;&#22522;&#26412;&#23545;&#31216;&#21644;&#20132;&#26367;&#34920;&#36798;&#24335;&#30340;&#39640;&#27425;&#20844;&#24335;&#20013;&#36731;&#26494;&#21457;&#29983;&#12290;</title><link>https://arxiv.org/abs/2402.16726</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#35299;&#37322;&#29702;&#35299;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Interpreting Grokked Transformers in Complex Modular Arithmetic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36870;&#21521;&#24037;&#31243;&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#35266;&#23519;&#20102;Transformer&#20869;&#37096;&#30005;&#36335;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#20943;&#27861;&#22312;Transformer&#19978;&#36896;&#25104;&#20102;&#24378;&#28872;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#20056;&#27861;&#38656;&#35201;&#20313;&#24358;&#20559;&#32622;&#20998;&#37327;&#65292;&#22810;&#39033;&#24335;&#21472;&#21152;&#20102;&#22522;&#26412;&#31639;&#26415;&#27169;&#24335;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#24182;&#19981;&#28165;&#26224;&#65292;Grokking&#29978;&#33267;&#21487;&#20197;&#22312;&#20855;&#26377;&#22522;&#26412;&#23545;&#31216;&#21644;&#20132;&#26367;&#34920;&#36798;&#24335;&#30340;&#39640;&#27425;&#20844;&#24335;&#20013;&#36731;&#26494;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grokking&#19968;&#30452;&#26159;&#35299;&#24320;&#24310;&#36831;&#27867;&#21270;&#20043;&#35868;&#30340;&#31215;&#26497;&#25506;&#32034;&#12290;&#22312;&#24050;&#35299;&#23494;&#27169;&#22411;&#20013;&#35782;&#21035;&#21487;&#35299;&#37322;&#30340;&#31639;&#27861;&#26159;&#29702;&#35299;&#20854;&#26426;&#21046;&#30340;&#26263;&#31034;&#24615;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#38500;&#20102;&#26368;&#31616;&#21333;&#21644;&#24191;&#20026;&#30740;&#31350;&#30340;&#27169;&#22359;&#21270;&#21152;&#27861;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36870;&#21521;&#24037;&#31243;&#35266;&#23519;&#20102;&#36890;&#36807;Grokking&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#23398;&#21040;&#30340;&#20869;&#37096;&#30005;&#36335;&#65292;&#31361;&#20986;&#26174;&#31034;&#20102;&#23427;&#20204;&#21160;&#21147;&#23398;&#19978;&#30340;&#37325;&#22823;&#24046;&#24322;&#65306;&#20943;&#27861;&#23545;Transformer&#20135;&#29983;&#24378;&#28872;&#30340;&#19981;&#23545;&#31216;&#24615;&#65307;&#20056;&#27861;&#22312;&#20613;&#31435;&#21494;&#22495;&#30340;&#25152;&#26377;&#39057;&#29575;&#19978;&#38656;&#35201;&#20313;&#24358;&#20559;&#32622;&#20998;&#37327;&#65307;&#22810;&#39033;&#24335;&#36890;&#24120;&#23548;&#33268;&#22522;&#26412;&#31639;&#26415;&#27169;&#24335;&#30340;&#21472;&#21152;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#28165;&#26224;&#30340;&#27169;&#24335;&#24182;&#19981;&#26174;&#29616;&#65307;&#21363;&#20351;&#22312;&#20855;&#26377;&#22522;&#26412;&#23545;&#31216;&#21644;&#20132;&#26367;&#34920;&#36798;&#24335;&#30340;&#39640;&#27425;&#20844;&#24335;&#20013;&#65292;Grokking&#20063;&#24456;&#23481;&#26131;&#21457;&#29983;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#27169;&#22359;&#21270;&#31639;&#26415;&#30340;&#26032;&#39062;&#36827;&#23637;&#24230;&#37327;&#65307;&#20613;&#31435;&#21494;&#39057;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16726v2 Announce Type: replace-cross  Abstract: Grokking has been actively explored to reveal the mystery of delayed generalization. Identifying interpretable algorithms inside the grokked models is a suggestive hint to understanding its mechanism. In this work, beyond the simplest and well-studied modular addition, we observe the internal circuits learned through grokking in complex modular arithmetic via interpretable reverse engineering, which highlights the significant difference in their dynamics: subtraction poses a strong asymmetry on Transformer; multiplication requires cosine-biased components at all the frequencies in a Fourier domain; polynomials often result in the superposition of the patterns from elementary arithmetic, but clear patterns do not emerge in challenging cases; grokking can easily occur even in higher-degree formulas with basic symmetric and alternating expressions. We also introduce the novel progress measure for modular arithmetic; Fourier Freque
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#21333;&#35789;&#24207;&#21015;&#29109;&#65288;WSE&#65289;&#65292;&#29992;&#20110;&#22312;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#37327;&#21270;&#31572;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2402.14259</link><description>&lt;p&gt;
&#21333;&#35789;&#24207;&#21015;&#29109;&#65306;&#36208;&#21521;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#24212;&#29992;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#21333;&#35789;&#24207;&#21015;&#29109;&#65288;WSE&#65289;&#65292;&#29992;&#20110;&#22312;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#37327;&#21270;&#31572;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22312;&#30830;&#20445;&#23433;&#20840;&#20851;&#38190;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#21487;&#38752;&#24615;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#23588;&#20854;&#22312;&#21307;&#30103;&#39046;&#22495;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#30001;&#24418;&#24335;&#30340;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#23578;&#26410;&#24314;&#31435;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#37327;&#21270;&#31572;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20854;&#20013;&#26080;&#20851;&#30340;&#35789;&#27719;&#21644;&#35821;&#24207;&#21547;&#26377;&#26377;&#38480;&#30340;&#35821;&#20041;&#20449;&#24687;&#21487;&#33021;&#26159;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#36825;&#26159;&#30001;&#20110;&#29983;&#25104;&#19981;&#24179;&#31561;&#30340;&#23384;&#22312;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21333;&#35789;&#24207;&#21015;&#29109;&#65288;WSE&#65289;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#35821;&#20041;&#30456;&#20851;&#24615;&#22312;&#21333;&#35789;&#21644;&#24207;&#21015;&#32423;&#21035;&#19978;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#27604;&#20363;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26102;&#26356;&#21152;&#24378;&#35843;&#20851;&#38190;&#35789;&#21644;&#26356;&#30456;&#20851;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#22312;5&#20010;&#33258;&#30001;&#24418;&#24335;&#21307;&#23398;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#65292;&#21033;&#29992;7&#31181;&#8220;&#29616;&#25104;&#30340;&#8221;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;WSE&#19982;6&#31181;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;WSE&#22312;&#24615;&#33021;&#19978;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14259v1 Announce Type: cross  Abstract: Uncertainty estimation plays a pivotal role in ensuring the reliability of safety-critical human-AI interaction systems, particularly in the medical domain. However, a general method for quantifying the uncertainty of free-form answers has yet to be established in open-ended medical question-answering (QA) tasks, where irrelevant words and sequences with limited semantic information can be the primary source of uncertainty due to the presence of generative inequality. In this paper, we propose the Word-Sequence Entropy (WSE), which calibrates the uncertainty proportion at both the word and sequence levels according to the semantic relevance, with greater emphasis placed on keywords and more relevant sequences when performing uncertainty quantification. We compare WSE with 6 baseline methods on 5 free-form medical QA datasets, utilizing 7 "off-the-shelf" large language models (LLMs), and show that WSE exhibits superior performance on ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65292;&#36890;&#36807;&#25972;&#21512;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#21644;&#20445;&#23432;Q&#23398;&#20064;&#26469;&#35299;&#20915;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#26377;&#38480;&#25968;&#25454;&#24102;&#26469;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08421</link><description>&lt;p&gt;
&#20445;&#23432;&#21644;&#39118;&#38505;&#24847;&#35782;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conservative and Risk-Aware Offline Multi-Agent Reinforcement Learning for Digital Twins
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65292;&#36890;&#36807;&#25972;&#21512;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#21644;&#20445;&#23432;Q&#23398;&#20064;&#26469;&#35299;&#20915;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#26377;&#38480;&#25968;&#25454;&#24102;&#26469;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#24179;&#21488;&#34987;&#36234;&#26469;&#36234;&#35748;&#20026;&#26159;&#25511;&#21046;&#12289;&#20248;&#21270;&#21644;&#30417;&#25511;&#35832;&#22914;&#19979;&#19968;&#20195;&#26080;&#32447;&#32593;&#32476;&#20043;&#31867;&#30340;&#22797;&#26434;&#24037;&#31243;&#31995;&#32479;&#30340;&#26377;&#24076;&#26395;&#25216;&#26415;&#12290;&#37319;&#29992;DT&#35299;&#20915;&#26041;&#26696;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#31163;&#32447;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#32570;&#20047;&#23545;&#29289;&#29702;&#29615;&#22659;&#30340;&#30452;&#25509;&#35775;&#38382;&#12290;&#36825;&#19968;&#38480;&#21046;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23588;&#20026;&#20005;&#37325;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#22312;&#32447;&#20114;&#21160;&#12290;&#23558;&#22312;&#32447;MARL&#26041;&#26696;&#30452;&#25509;&#24212;&#29992;&#20110;&#31163;&#32447;&#29615;&#22659;&#36890;&#24120;&#20250;&#22240;&#26377;&#38480;&#25968;&#25454;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#32780;&#22833;&#36133;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;DT&#30340;&#26080;&#32447;&#32593;&#32476;&#30340;&#31163;&#32447;MARL&#26041;&#26696;&#65292;&#23427;&#25972;&#21512;&#20102;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;distributional RL&#65289;&#21644;&#20445;&#23432;Q&#23398;&#20064;&#65292;&#20197;&#24212;&#23545;&#29615;&#22659;&#22266;&#26377;&#30340;&#26696;&#20363;&#24615;&#19981;&#30830;&#23450;&#24615;&#21644;&#26377;&#38480;&#25968;&#25454;&#24341;&#36215;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#65292;&#25105;&#20204;&#25913;&#32534;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital twin (DT) platforms are increasingly regarded as a promising technology for controlling, optimizing, and monitoring complex engineering systems such as next-generation wireless networks. An important challenge in adopting DT solutions is their reliance on data collected offline, lacking direct access to the physical environment. This limitation is particularly severe in multi-agent systems, for which conventional multi-agent reinforcement (MARL) requires online interactions with the environment. A direct application of online MARL schemes to an offline setting would generally fail due to the epistemic uncertainty entailed by the limited availability of data. In this work, we propose an offline MARL scheme for DT-based wireless networks that integrates distributional RL and conservative Q-learning to address the environment's inherent aleatoric uncertainty and the epistemic uncertainty arising from limited data. To further exploit the offline data, we adapt the proposed scheme t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;LoCoV1&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#24615;&#33021;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;M2-BERT&#26816;&#32034;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#35780;&#20272;&#24615;&#33021;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07440</link><description>&lt;p&gt;
&#20351;&#29992;LoCo&#21644;M2-BERT&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#26500;&#24314;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;LoCoV1&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#24615;&#33021;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;M2-BERT&#26816;&#32034;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#35780;&#20272;&#24615;&#33021;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#31649;&#36947;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#25991;&#26723;&#24456;&#38271;&#65288;&#20363;&#22914;10K&#20010;&#26631;&#35760;&#25110;&#26356;&#22810;&#65289;&#19988;&#38656;&#35201;&#22312;&#25972;&#20010;&#25991;&#26412;&#20013;&#21512;&#25104;&#20449;&#24687;&#26469;&#30830;&#23450;&#30456;&#20851;&#25991;&#26723;&#30340;&#39046;&#22495;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#24320;&#21457;&#36866;&#29992;&#20110;&#36825;&#20123;&#39046;&#22495;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#32534;&#30721;&#22120;&#38754;&#20020;&#19977;&#20010;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22914;&#20309;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#24615;&#33021;&#65292;&#65288;2&#65289;&#22914;&#20309;&#39044;&#35757;&#32451;&#22522;&#26412;&#35821;&#35328;&#27169;&#22411;&#20197;&#34920;&#31034;&#30701;&#19978;&#19979;&#25991;&#65288;&#23545;&#24212;&#26597;&#35810;&#65289;&#21644;&#38271;&#19978;&#19979;&#25991;&#65288;&#23545;&#24212;&#25991;&#26723;&#65289;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22914;&#20309;&#26681;&#25454;GPU&#20869;&#23384;&#38480;&#21046;&#19979;&#30340;&#25209;&#37327;&#22823;&#23567;&#38480;&#21046;&#23545;&#35813;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;LoCoV1&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;12&#20010;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#27979;&#37327;&#22312;&#19981;&#21487;&#20998;&#22359;&#25110;&#19981;&#26377;&#25928;&#30340;&#24773;&#20917;&#19979;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M2-BERT&#26816;&#32034;&#32534;&#30721;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;80M&#21442;&#25968;&#29366;&#24577;&#31354;&#38388;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#37319;&#29992;Monarch Mixer&#26550;&#26500;&#26500;&#24314;&#65292;&#33021;&#22815;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval pipelines-an integral component of many machine learning systems-perform poorly in domains where documents are long (e.g., 10K tokens or more) and where identifying the relevant document requires synthesizing information across the entire text. Developing long-context retrieval encoders suitable for these domains raises three challenges: (1) how to evaluate long-context retrieval performance, (2) how to pretrain a base language model to represent both short contexts (corresponding to queries) and long contexts (corresponding to documents), and (3) how to fine-tune this model for retrieval under the batch size limitations imposed by GPU memory constraints. To address these challenges, we first introduce LoCoV1, a novel 12 task benchmark constructed to measure long-context retrieval where chunking is not possible or not effective. We next present the M2-BERT retrieval encoder, an 80M parameter state-space encoder model built from the Monarch Mixer architecture, capable of scali
&lt;/p&gt;</description></item><item><title>&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#65292;&#24182;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.05271</link><description>&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#24341;&#21457;&#20102;&#28145;&#24230;&#38750;&#32447;&#24615;&#32593;&#32476;&#26435;&#37325;&#19982;&#32463;&#39564;NTK&#20043;&#38388;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05271
&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#65292;&#24182;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#65292;&#22312;&#19968;&#33324;&#32467;&#26500;&#30340;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#20010;&#35828;&#27861;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#37327;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22914;&#20309;&#30456;&#20851;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#24341;&#20837;&#30340;NFA&#26159;&#30001;&#38548;&#31163;&#36825;&#31181;&#23545;&#40784;&#30340;&#20013;&#24515;&#21270;NFA&#39537;&#21160;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the mechanisms through which neural networks extract statistics from input-label pairs is one of the most important unsolved problems in supervised learning. Prior works have identified that the gram matrices of the weights in trained neural networks of general architectures are proportional to the average gradient outer product of the model, in a statement known as the Neural Feature Ansatz (NFA). However, the reason these quantities become correlated during training is poorly understood. In this work, we explain the emergence of this correlation. We identify that the NFA is equivalent to alignment between the left singular structure of the weight matrices and a significant component of the empirical neural tangent kernels associated with those weights. We establish that the NFA introduced in prior works is driven by a centered NFA that isolates this alignment. We show that the speed of NFA development can be predicted analytically at early training times in terms of sim
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#30340;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#25968;&#25454;&#28304;&#20043;&#38388;&#36136;&#37327;&#21644;&#20840;&#38754;&#24615;&#24046;&#24322;&#32473;&#31995;&#32479;&#20248;&#21270;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04146</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#36890;&#36807;&#28508;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04146
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#30340;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#25968;&#25454;&#28304;&#20043;&#38388;&#36136;&#37327;&#21644;&#20840;&#38754;&#24615;&#24046;&#24322;&#32473;&#31995;&#32479;&#20248;&#21270;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20986;&#29616;&#65292;&#21508;&#20010;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#24050;&#32463;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26367;&#20195;&#27169;&#22411;&#26469;&#24314;&#27169;&#26469;&#33258;&#22823;&#37327;&#20449;&#24687;&#28304;&#65288;&#25968;&#25454;&#65289;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#36825;&#31181;&#22686;&#21152;&#23548;&#33268;&#20102;&#24320;&#21457;&#20986;&#29992;&#20110;&#25191;&#34892;&#29305;&#23450;&#21151;&#33021;&#30340;&#20248;&#36234;&#31995;&#32479;&#25152;&#38656;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#30340;&#26174;&#33879;&#38477;&#20302;&#12290;&#36825;&#26679;&#30340;&#26367;&#20195;&#27169;&#22411;&#24448;&#24448;&#24191;&#27867;&#22320;&#34701;&#21512;&#22810;&#20010;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#33021;&#26159;&#21457;&#34920;&#30340;&#35770;&#25991;&#12289;&#19987;&#21033;&#12289;&#24320;&#25918;&#36164;&#28304;&#24211;&#25110;&#20854;&#20182;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#20449;&#24687;&#26469;&#28304;&#30340;&#22522;&#30784;&#29289;&#29702;&#21442;&#25968;&#30340;&#36136;&#37327;&#21644;&#20840;&#38754;&#24615;&#30340;&#24046;&#24322;&#65292;&#21487;&#33021;&#23545;&#31995;&#32479;&#20248;&#21270;&#36807;&#31243;&#20135;&#29983;&#21518;&#32493;&#24433;&#21709;&#65292;&#21364;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#65288;LVGP&#65289;&#30340;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of artificial intelligence (AI) and machine learning (ML), various domains of science and engineering communites has leveraged data-driven surrogates to model complex systems from numerous sources of information (data). The proliferation has led to significant reduction in cost and time involved in development of superior systems designed to perform specific functionalities. A high proposition of such surrogates are built extensively fusing multiple sources of data, may it be published papers, patents, open repositories, or other resources. However, not much attention has been paid to the differences in quality and comprehensiveness of the known and unknown underlying physical parameters of the information sources that could have downstream implications during system optimization. Towards resolving this issue, a multi-source data fusion framework based on Latent Variable Gaussian Process (LVGP) is proposed. The individual data sources are tagged as a characteristic cate
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20581;&#24247;&#29366;&#24577;&#20272;&#35745;&#12290;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00068</link><description>&lt;p&gt;
GPT4Battery: &#19968;&#31181;&#22522;&#20110;LLM&#39537;&#21160;&#30340;&#33258;&#36866;&#24212;&#38146;&#31163;&#23376;&#30005;&#27744;&#20581;&#24247;&#29366;&#24577;&#20272;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GPT4Battery: An LLM-driven Framework for Adaptive State of Health Estimation of Raw Li-ion Batteries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20581;&#24247;&#29366;&#24577;&#20272;&#35745;&#12290;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#29366;&#24577;&#65288;SOH&#65289;&#26159;&#35780;&#20272;&#30005;&#27744;&#36864;&#21270;&#27700;&#24179;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#26080;&#27861;&#30452;&#25509;&#27979;&#37327;&#20294;&#38656;&#35201;&#20272;&#35745;&#12290;&#20934;&#30830;&#30340;SOH&#20272;&#35745;&#25552;&#21319;&#20102;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#26816;&#27979;&#12289;&#25511;&#21046;&#21644;&#21453;&#39304;&#33021;&#21147;&#65292;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33021;&#28304;&#31649;&#29702;&#65292;&#24182;&#25351;&#23548;&#26032;&#19968;&#20195;&#30005;&#27744;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;SOH&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20026;&#29983;&#25104;&#23551;&#21629;&#38271;&#26399;&#35757;&#32451;&#25968;&#25454;&#32780;&#36827;&#34892;&#30340;&#32791;&#26102;&#19988;&#36164;&#28304;&#23494;&#38598;&#30340;&#36864;&#21270;&#23454;&#39564;&#22312;&#24314;&#31435;&#19968;&#20010;&#33021;&#22788;&#29702;&#22810;&#26679;&#21270;&#38146;&#31163;&#23376;&#30005;&#27744;&#65288;&#20363;&#22914;&#65292;&#36328;&#21270;&#23398;&#12289;&#36328;&#21046;&#36896;&#21830;&#21644;&#36328;&#23481;&#37327;&#65289;&#30340;&#22823;&#22411;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19981;&#21516;&#30005;&#27744;&#30340;&#21487;&#35843;&#25972;SOH&#20272;&#35745;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;&#20026;&#20102;&#36866;&#24212;&#23454;&#38469;&#24773;&#26223;&#65292;&#20854;&#20013;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#25353;&#39034;&#24207;&#20197;&#21450;&#20998;&#24067;&#21464;&#21270;&#30340;&#26041;&#24335;&#21040;&#36798;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#36827;&#34892;&#20102;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
State of health (SOH) is a crucial indicator for assessing the degradation level of batteries that cannot be measured directly but requires estimation. Accurate SOH estimation enhances detection, control, and feedback for Li-ion batteries, allowing for safe and efficient energy management and guiding the development of new-generation batteries. Despite the significant progress in data-driven SOH estimation, the time and resource-consuming degradation experiments for generating lifelong training data pose a challenge in establishing one large model capable of handling diverse types of Li-ion batteries, e.g., cross-chemistry, cross-manufacturer, and cross-capacity. Hence, this paper utilizes the strong generalization capability of large language model (LLM) to proposes a novel framework for adaptable SOH estimation across diverse batteries. To match the real scenario where unlabeled data sequentially arrives in use with distribution shifts, the proposed model is modified by a test-time t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38646;&#23556;&#20987;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#26469;&#25351;&#23548;&#27169;&#22411;&#36827;&#34892;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#65292;&#21253;&#25324;&#38405;&#35835;&#29702;&#35299;&#12289;&#31639;&#26415;&#25512;&#29702;&#21644;&#38381;&#21367;&#38382;&#31572;&#65292;&#27169;&#22411;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#36825;&#20123;&#32467;&#26524;&#20063;&#26174;&#31034;&#20986;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2401.08273</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#38646;&#23556;&#20987;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Null-Shot Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38646;&#23556;&#20987;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#26469;&#25351;&#23548;&#27169;&#22411;&#36827;&#34892;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#65292;&#21253;&#25324;&#38405;&#35835;&#29702;&#35299;&#12289;&#31639;&#26415;&#25512;&#29702;&#21644;&#38381;&#21367;&#38382;&#31572;&#65292;&#27169;&#22411;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#36825;&#20123;&#32467;&#26524;&#20063;&#26174;&#31034;&#20986;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38646;&#23556;&#20987;&#25552;&#31034;&#26041;&#27861;&#12290;&#38646;&#23556;&#20987;&#25552;&#31034;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#36890;&#36807;&#25351;&#31034;LLMs&#21033;&#29992;&#20174;&#8220;&#31034;&#20363;&#8221;&#37096;&#20998;&#20013;&#33719;&#21462;&#30340;&#20449;&#24687;&#65288;&#35813;&#20449;&#24687;&#22312;&#25152;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#20013;&#19981;&#23384;&#22312;&#65289;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#34429;&#28982;&#20943;&#23569;&#38169;&#35823;&#20449;&#24687;&#23545;&#20110;LLMs&#30340;&#26085;&#24120;&#21644;&#37325;&#35201;&#29992;&#36884;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#22312;&#30446;&#21069;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;LLMs&#20173;&#28982;&#20855;&#26377;&#38169;&#35823;&#20449;&#24687;&#65292;&#23454;&#38469;&#19978;&#21487;&#20197;&#21033;&#29992;&#38169;&#35823;&#20449;&#24687;&#26469;&#25552;&#39640;&#19982;&#26631;&#20934;&#38646;&#23556;&#20987;&#25552;&#31034;&#30456;&#27604;&#30340;&#20219;&#21153;&#34920;&#29616;&#12290;&#23545;&#20843;&#20010;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#20843;&#20010;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#38405;&#35835;&#29702;&#35299;&#12289;&#31639;&#26415;&#25512;&#29702;&#21644;&#38381;&#21367;&#38382;&#31572;&#65289;&#20013;&#65292;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#35266;&#23519;&#21040;&#30340;&#19981;&#19968;&#33268;&#24615;&#22686;&#21152;&#30456;&#23545;&#24615;&#33021;&#22312;LLMs&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20063;&#21487;&#33021;&#34920;&#31034;&#27599;&#20010;&#27169;&#22411;&#20013;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08273v2 Announce Type: replace-cross Abstract: This paper presents null-shot prompting. Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the "Examples" section that never exists within the provided context to perform a task. While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, we propose that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting. Experiments with eight LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering. The observed inconsistency in increased relative performance across the LLMs also potentially indicates a different degree of inherent hallucination in each model. These differences show 
&lt;/p&gt;</description></item><item><title>SupplyGraph&#26159;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20379;&#24212;&#38142;&#35268;&#21010;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;&#23391;&#21152;&#25289;&#22269;&#19968;&#23478;&#39046;&#20808;&#24555;&#36895;&#28040;&#36153;&#21697;&#20844;&#21496;&#30340;&#23454;&#38469;&#25968;&#25454;&#65292;&#29992;&#20110;&#20248;&#21270;&#12289;&#39044;&#27979;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#25968;&#25454;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#38144;&#21806;&#39044;&#27979;&#12289;&#29983;&#20135;&#35745;&#21010;&#21644;&#25925;&#38556;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2401.15299</link><description>&lt;p&gt;
SupplyGraph: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20379;&#24212;&#38142;&#35268;&#21010;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SupplyGraph: A Benchmark Dataset for Supply Chain Planning using Graph Neural Networks. (arXiv:2401.15299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15299
&lt;/p&gt;
&lt;p&gt;
SupplyGraph&#26159;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20379;&#24212;&#38142;&#35268;&#21010;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;&#23391;&#21152;&#25289;&#22269;&#19968;&#23478;&#39046;&#20808;&#24555;&#36895;&#28040;&#36153;&#21697;&#20844;&#21496;&#30340;&#23454;&#38469;&#25968;&#25454;&#65292;&#29992;&#20110;&#20248;&#21270;&#12289;&#39044;&#27979;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#25968;&#25454;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#65292;&#21487;&#29992;&#20110;&#38144;&#21806;&#39044;&#27979;&#12289;&#29983;&#20135;&#35745;&#21010;&#21644;&#25925;&#38556;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#22914;&#36816;&#36755;&#12289;&#29983;&#29289;&#20449;&#24687;&#23398;&#12289;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;GNNs&#24212;&#29992;&#20110;&#20379;&#24212;&#38142;&#32593;&#32476;&#26041;&#38754;&#65292;&#30446;&#21069;&#23578;&#32570;&#20047;&#30740;&#31350;&#12290;&#20379;&#24212;&#38142;&#32593;&#32476;&#22312;&#32467;&#26500;&#19978;&#31867;&#20284;&#20110;&#22270;&#24418;&#65292;&#20351;&#20854;&#25104;&#20026;&#24212;&#29992;GNN&#26041;&#27861;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#36825;&#20026;&#20248;&#21270;&#12289;&#39044;&#27979;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#24320;&#36767;&#20102;&#26080;&#38480;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#27492;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#22312;&#20110;&#32570;&#20047;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#20419;&#36827;&#20351;&#29992;GNN&#26469;&#30740;&#31350;&#21644;&#35299;&#20915;&#20379;&#24212;&#38142;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26469;&#33258;&#23391;&#21152;&#25289;&#22269;&#19968;&#23478;&#39046;&#20808;&#30340;&#24555;&#36895;&#28040;&#36153;&#21697;&#20844;&#21496;&#30340;&#23454;&#38469;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#29992;&#20110;&#29983;&#20135;&#30446;&#30340;&#30340;&#20379;&#24212;&#38142;&#35268;&#21010;&#30340;&#26102;&#38388;&#20219;&#21153;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26102;&#38388;&#25968;&#25454;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#38144;&#21806;&#39044;&#27979;&#12289;&#29983;&#20135;&#35745;&#21010;&#21644;&#25925;&#38556;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have gained traction across different domains such as transportation, bio-informatics, language processing, and computer vision. However, there is a noticeable absence of research on applying GNNs to supply chain networks. Supply chain networks are inherently graph-like in structure, making them prime candidates for applying GNN methodologies. This opens up a world of possibilities for optimizing, predicting, and solving even the most complex supply chain problems. A major setback in this approach lies in the absence of real-world benchmark datasets to facilitate the research and resolution of supply chain problems using GNNs. To address the issue, we present a real-world benchmark dataset for temporal tasks, obtained from one of the leading FMCG companies in Bangladesh, focusing on supply chain planning for production purposes. The dataset includes temporal data as node features to enable sales predictions, production planning, and the identification of fa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LadaGAN&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#65292;&#36890;&#36807;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09596</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#21147;Transformer&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Efficient generative adversarial networks using linear additive-attention Transformers. (arXiv:2401.09596v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LadaGAN&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#65292;&#36890;&#36807;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#31561;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#35745;&#31639;&#22797;&#26434;&#30340;&#26550;&#26500;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30740;&#31350;&#23454;&#39564;&#23460;&#21644;&#36164;&#28304;&#20805;&#36275;&#30340;&#20844;&#21496;&#20013;&#30340;&#37319;&#29992;&#21644;&#20351;&#29992;&#65292;&#21516;&#26102;&#20063;&#26497;&#22823;&#22320;&#22686;&#21152;&#20102;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#30340;&#30899;&#36275;&#36857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LadaGAN&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#24314;&#31435;&#22312;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#19978;&#12290;&#35813;&#22359;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#19968;&#20010;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#65292;&#23427;&#27599;&#20010;&#22836;&#37096;&#35745;&#31639;&#19968;&#20010;&#27880;&#24847;&#21521;&#37327;&#65292;&#32780;&#19981;&#26159;&#20108;&#27425;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#22312;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#20013;&#37117;&#37319;&#29992;&#20102;Ladaformer&#65292;&#36825;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#20811;&#26381;&#20102;Transformer GAN&#32463;&#24120;&#20986;&#29616;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#12290;LadaGAN&#19968;&#30452;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;GANs&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the capacity of deep generative models for image generation, such as Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has dramatically improved in recent years, much of their success can be attributed to computationally expensive architectures. This has limited their adoption and use to research laboratories and companies with large resources, while significantly raising the carbon footprint for training, fine-tuning, and inference. In this work, we present LadaGAN, an efficient generative adversarial network that is built upon a novel Transformer block named Ladaformer. The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention. We employ Ladaformer in both the generator and discriminator, which reduces the computational complexity and overcomes the training instabilities often associated with Transformer GANs. LadaGAN consistently outperforms exist
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01286</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#19982;&#20154;&#31867;&#20132;&#27969;&#32039;&#23494;&#30456;&#20284;&#30340;&#25991;&#26412;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26174;&#33879;&#35745;&#31639;&#38656;&#27714;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#21442;&#25968;&#21270;&#36896;&#25104;&#30340;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#20110;&#19990;&#30028;&#30340;&#21160;&#24577;&#24615;&#65292;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;LLM&#20197;&#20462;&#27491;&#36807;&#26102;&#30340;&#20449;&#24687;&#25110;&#38598;&#25104;&#26032;&#30693;&#35782;&#65292;&#20174;&#32780;&#30830;&#20445;&#20854;&#25345;&#32493;&#30340;&#30456;&#20851;&#24615;&#12290;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#22312;&#35757;&#32451;&#21518;&#36827;&#34892;&#25345;&#32493;&#30340;&#27169;&#22411;&#35843;&#25972;&#65292;&#20197;&#35299;&#20915;&#32570;&#38519;&#25110;&#19981;&#33391;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;LLM&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#39640;&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#26377;&#25928;&#22320;&#20462;&#25913;LLM&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#22312;&#21508;&#31181;&#36755;&#20837;&#20013;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#39318;&#20808;&#23450;&#20041;&#20102;&#30693;&#35782;&#32534;&#36753;&#30340;&#30446;&#26631;&#21644;&#25361;&#25112;&#65292;&#28982;&#21518;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24212;&#29992;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#38544;&#31169;&#21644;&#29256;&#26435;&#20445;&#25252;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#25216;&#26415;&#21019;&#26032;&#19982;&#20262;&#29702;&#21069;&#30651;&#30456;&#32467;&#21512;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#20840;&#38754;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.18252</link><description>&lt;p&gt;
&#36328;&#36234;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#25968;&#25454;&#29983;&#21629;&#21608;&#26399;&#30340;&#38544;&#31169;&#21644;&#29256;&#26435;&#25361;&#25112;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Navigating Privacy and Copyright Challenges Across the Data Lifecycle of Generative AI. (arXiv:2311.18252v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18252
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#38544;&#31169;&#21644;&#29256;&#26435;&#20445;&#25252;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#25216;&#26415;&#21019;&#26032;&#19982;&#20262;&#29702;&#21069;&#30651;&#30456;&#32467;&#21512;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#20840;&#38754;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#37325;&#35201;&#37324;&#31243;&#30865;&#65292;&#23637;&#31034;&#20986;&#22312;&#29983;&#25104;&#30495;&#23454;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#25968;&#25454;&#27169;&#24335;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36827;&#23637;&#20063;&#24102;&#26469;&#20102;&#23545;&#25968;&#25454;&#38544;&#31169;&#21644;&#29256;&#26435;&#20405;&#29359;&#30340;&#26356;&#39640;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#27169;&#22411;&#35757;&#32451;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#24046;&#20998;&#38544;&#31169;&#12289;&#26426;&#22120;&#36951;&#24536;&#21644;&#25968;&#25454;&#20013;&#27602;&#21482;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#22797;&#26434;&#38382;&#39064;&#30340;&#29255;&#38754;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#29983;&#21629;&#21608;&#26399;&#20869;&#38544;&#31169;&#21644;&#29256;&#26435;&#20445;&#25252;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#12290;&#25105;&#20204;&#20027;&#24352;&#37319;&#29992;&#23558;&#25216;&#26415;&#21019;&#26032;&#19982;&#20262;&#29702;&#21069;&#30651;&#30456;&#32467;&#21512;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#21644;&#21046;&#23450;&#22312;&#29983;&#21629;&#21608;&#26399;&#35270;&#35282;&#19979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20840;&#38754;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25512;&#21160;&#26356;&#24191;&#27867;&#30340;&#35752;&#35770;&#65292;&#24182;&#28608;&#21169;&#23545;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#38544;&#31169;&#21644;&#29256;&#26435;&#23436;&#25972;&#24615;&#30340;&#21327;&#21516;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of Generative AI has marked a significant milestone in artificial intelligence, demonstrating remarkable capabilities in generating realistic images, texts, and data patterns. However, these advancements come with heightened concerns over data privacy and copyright infringement, primarily due to the reliance on vast datasets for model training. Traditional approaches like differential privacy, machine unlearning, and data poisoning only offer fragmented solutions to these complex issues. Our paper delves into the multifaceted challenges of privacy and copyright protection within the data lifecycle. We advocate for integrated approaches that combines technical innovation with ethical foresight, holistically addressing these concerns by investigating and devising solutions that are informed by the lifecycle perspective. This work aims to catalyze a broader discussion and inspire concerted efforts towards data privacy and copyright integrity in Generative AI.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#37319;&#26679;/&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;&#21435;&#22122;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36890;&#29992;&#21435;&#22122;&#32593;&#32476;&#22312;&#19981;&#21516;&#22122;&#22768;&#20998;&#24067;&#19979;&#34920;&#29616;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.20064</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#35757;&#32451;&#31574;&#30053;&#29992;&#20110;&#30450;&#30446;&#30340;&#22810;&#20998;&#24067;&#22122;&#22768;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
A Scalable Training Strategy for Blind Multi-Distribution Noise Removal. (arXiv:2310.20064v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20064
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#37319;&#26679;/&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;&#21435;&#22122;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36890;&#29992;&#21435;&#22122;&#32593;&#32476;&#22312;&#19981;&#21516;&#22122;&#22768;&#20998;&#24067;&#19979;&#34920;&#29616;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#26159;&#24320;&#21457;&#36890;&#29992;&#30340;&#21435;&#22122;&#21644;&#21435;&#20266;&#24433;&#32593;&#32476;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#22266;&#23450;&#30340;&#32593;&#32476;&#26435;&#37325;&#65292;&#19968;&#20010;&#20219;&#21153;&#65288;&#20363;&#22914;&#21435;&#38500;&#27850;&#26494;&#22122;&#22768;&#65289;&#30340;&#19987;&#38376;&#21270;&#19982;&#21478;&#19968;&#20010;&#20219;&#21153;&#65288;&#20363;&#22914;&#21435;&#38500;&#26001;&#28857;&#22122;&#22768;&#65289;&#30340;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#22825;&#28982;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#32500;&#24230;&#30340;&#35781;&#21650;&#65292;&#35757;&#32451;&#36825;&#26679;&#30340;&#32593;&#32476;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65306;&#38543;&#30528;&#35268;&#26684;&#31354;&#38388;&#30340;&#32500;&#24230;&#22686;&#21152;&#65288;&#21363;&#38656;&#35201;&#25551;&#36848;&#22122;&#22768;&#20998;&#24067;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#65289;&#65292;&#38656;&#35201;&#35757;&#32451;&#30340;&#21807;&#19968;&#35268;&#26684;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#22343;&#21248;&#37319;&#26679;&#36825;&#20010;&#31354;&#38388;&#20250;&#23548;&#33268;&#32593;&#32476;&#22312;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#35268;&#26684;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#31616;&#21333;&#30340;&#38382;&#39064;&#35268;&#26684;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#21363;&#20351;&#22823;&#35823;&#24046;&#20063;&#23545;&#24635;&#20307;&#22343;&#26041;&#35823;&#24046;&#30340;&#24433;&#21709;&#24456;&#23567;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#36866;&#24212;&#37319;&#26679;/&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;&#21435;&#22122;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25913;&#36827;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances, developing general-purpose universal denoising and artifact-removal networks remains largely an open problem: Given fixed network weights, one inherently trades-off specialization at one task (e.g.,~removing Poisson noise) for performance at another (e.g.,~removing speckle noise). In addition, training such a network is challenging due to the curse of dimensionality: As one increases the dimensions of the specification-space (i.e.,~the number of parameters needed to describe the noise distribution) the number of unique specifications one needs to train for grows exponentially. Uniformly sampling this space will result in a network that does well at very challenging problem specifications but poorly at easy problem specifications, where even large errors will have a small effect on the overall mean squared error.  In this work we propose training denoising networks using an adaptive-sampling/active-learning strategy. Our work improves upon a recently proposed un
&lt;/p&gt;</description></item><item><title>Clover&#26159;&#19968;&#31181;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17807</link><description>&lt;p&gt;
Clover: &#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Clover: Closed-Loop Verifiable Code Generation. (arXiv:2310.17807v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17807
&lt;/p&gt;
&lt;p&gt;
Clover&#26159;&#19968;&#31181;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;&#26159;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#36825;&#20010;&#36235;&#21183;&#21487;&#33021;&#20250;&#23548;&#33268;&#35768;&#22810;&#19981;&#33391;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#24895;&#26223;&#65306;Clover&#33539;&#24335;&#65292;&#21363;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#65292;&#23427;&#23558;&#27491;&#30830;&#24615;&#26816;&#26597;&#31616;&#21270;&#20026;&#26356;&#21487;&#35775;&#38382;&#30340;&#19968;&#33268;&#24615;&#26816;&#26597;&#38382;&#39064;&#12290;&#22312;Clover&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26816;&#26597;&#22120;&#65292;&#23427;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#12290;&#35813;&#26816;&#26597;&#22120;&#20351;&#29992;&#20102;&#24418;&#24335;&#39564;&#35777;&#24037;&#20855;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#39062;&#38598;&#25104;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35770;&#28857;&#65292;&#21363;Clover&#22312;&#19968;&#33268;&#24615;&#26816;&#26597;&#26041;&#38754;&#24212;&#35813;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#30001;&#25163;&#24037;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65288;CloverBench&#65289;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35843;&#26597;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#27880;&#37322;&#30340;Dafny&#31243;&#24207;&#65292;&#38590;&#24230;&#27700;&#24179;&#19982;&#25945;&#31185;&#20070;&#30456;&#24403;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
The use of large language models for code generation is a rapidly growing trend in software development. However, without effective methods for ensuring the correctness of generated code, this trend could lead to any number of undesirable outcomes. In this paper, we lay out a vision for addressing this challenge: the Clover paradigm, short for Closed-Loop Verifiable Code Generation, which reduces correctness checking to the more accessible problem of consistency checking. At the core of Clover lies a checker that performs consistency checks among code, docstrings, and formal annotations. The checker is implemented using a novel integration of formal verification tools and large language models. We provide a theoretical analysis to support our thesis that Clover should be effective at consistency checking. We also empirically investigate its feasibility on a hand-designed dataset (CloverBench) featuring annotated Dafny programs at a textbook level of difficulty. Experimental results sho
&lt;/p&gt;</description></item><item><title>Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#29616;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2310.17086</link><description>&lt;p&gt;
Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;&#19968;&#39033;&#19982;&#32447;&#24615;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models. (arXiv:2310.17086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17086
&lt;/p&gt;
&lt;p&gt;
Transformers&#23398;&#20250;&#20102;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#29616;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#23427;&#20204;&#26159;&#22914;&#20309;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Transformers&#21487;&#33021;&#36890;&#36807;&#20869;&#37096;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21363;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Transformers&#23398;&#20250;&#20102;&#23454;&#29616;&#39640;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#20197;&#19978;&#19979;&#25991;&#32447;&#24615;&#22238;&#24402;&#20026;&#37325;&#28857;&#65292;&#23637;&#31034;&#20102;Transformers&#23398;&#20250;&#20102;&#23454;&#29616;&#19968;&#20010;&#38750;&#24120;&#31867;&#20284;&#20110;&#36845;&#20195;&#29275;&#39039;&#27861;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26799;&#24230;&#19979;&#38477;&#12290;&#20174;&#23454;&#35777;&#19978;&#26469;&#30475;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36830;&#32493;&#30340;Transformer&#23618;&#30340;&#39044;&#27979;&#19982;&#29275;&#39039;&#27861;&#30340;&#19981;&#21516;&#36845;&#20195;&#38750;&#24120;&#25509;&#36817;&#65292;&#27599;&#20010;&#20013;&#38388;&#23618;&#22823;&#33268;&#35745;&#31639;&#20102;3&#27425;&#36845;&#20195;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#25165;&#33021;&#21305;&#37197;&#39069;&#22806;&#30340;Transformer&#23618;&#65307;&#36825;&#34920;&#26126;Transformers&#20855;&#26377;&#30456;&#24403;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are remarkably good at in-context learning (ICL) -- learning from demonstrations without parameter updates -- but how they perform ICL remains a mystery. Recent work suggests that Transformers may learn in-context by internally running Gradient Descent, a first-order optimization method. In this paper, we instead demonstrate that Transformers learn to implement higher-order optimization methods to perform ICL. Focusing on in-context linear regression, we show that Transformers learn to implement an algorithm very similar to Iterative Newton's Method, a higher-order optimization method, rather than Gradient Descent. Empirically, we show that predictions from successive Transformer layers closely match different iterations of Newton's Method linearly, with each middle layer roughly computing 3 iterations. In contrast, exponentially more Gradient Descent steps are needed to match an additional Transformers layer; this suggests that Transformers have an comparable rate of conv
&lt;/p&gt;</description></item><item><title>&#22270;&#21435;&#23398;&#20064;&#26159;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#21024;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#25935;&#24863;&#25968;&#25454;&#30165;&#36857;&#26469;&#32500;&#25252;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#12290;&#36825;&#31687;&#32508;&#36848;&#24615;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#22238;&#39038;&#20102;&#22270;&#21435;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20102;&#21508;&#31181;&#26041;&#27861;&#23398;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#20998;&#31867;&#21644;&#26368;&#26032;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#20197;&#24110;&#21161;&#26032;&#36827;&#20837;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;&#12290;&#19982;&#24046;&#20998;&#38544;&#31169;&#30340;&#20851;&#31995;&#21152;&#28145;&#20102;&#23545;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.02164</link><description>&lt;p&gt;
&#22270;&#21435;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Graph Unlearning. (arXiv:2310.02164v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02164
&lt;/p&gt;
&lt;p&gt;
&#22270;&#21435;&#23398;&#20064;&#26159;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#21024;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#25935;&#24863;&#25968;&#25454;&#30165;&#36857;&#26469;&#32500;&#25252;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#12290;&#36825;&#31687;&#32508;&#36848;&#24615;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#22238;&#39038;&#20102;&#22270;&#21435;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20102;&#21508;&#31181;&#26041;&#27861;&#23398;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#20998;&#31867;&#21644;&#26368;&#26032;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#20197;&#24110;&#21161;&#26032;&#36827;&#20837;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;&#12290;&#19982;&#24046;&#20998;&#38544;&#31169;&#30340;&#20851;&#31995;&#21152;&#28145;&#20102;&#23545;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21435;&#23398;&#20064;&#26159;&#22312;&#36861;&#27714;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#23427;&#25552;&#20379;&#20102;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#25935;&#24863;&#25968;&#25454;&#30165;&#36857;&#30340;&#26041;&#27861;&#65292;&#20197;&#32500;&#25252;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#12290;&#26174;&#28982;&#65292;&#22270;&#26426;&#22120;&#23398;&#20064;&#23545;&#25968;&#25454;&#38544;&#31169;&#21644;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#25935;&#24863;&#24615;&#65292;&#22240;&#27492;&#38656;&#35201;&#24212;&#29992;&#22270;&#21435;&#23398;&#20064;&#25216;&#26415;&#26469;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#24615;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#22270;&#21435;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26041;&#27861;&#23398;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#20998;&#31867;&#21644;&#26368;&#26032;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#20197;&#24110;&#21161;&#26032;&#36827;&#20837;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#22270;&#21435;&#23398;&#20064;&#19982;&#24046;&#20998;&#38544;&#31169;&#20043;&#38388;&#30340;&#37325;&#35201;&#32852;&#31995;&#65292;&#22686;&#24378;&#20102;&#25105;&#20204;&#23545;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#30456;&#20851;&#24615;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#20445;&#35777;&#28165;&#26224;&#24230;&#65292;&#25105;&#20204;&#23545;&#22270;&#21435;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#20102;&#31616;&#26126;&#25212;&#35201;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph unlearning emerges as a crucial advancement in the pursuit of responsible AI, providing the means to remove sensitive data traces from trained models, thereby upholding the right to be forgotten. It is evident that graph machine learning exhibits sensitivity to data privacy and adversarial attacks, necessitating the application of graph unlearning techniques to address these concerns effectively. In this comprehensive survey paper, we present the first systematic review of graph unlearning approaches, encompassing a diverse array of methodologies and offering a detailed taxonomy and up-to-date literature overview to facilitate the understanding of researchers new to this field. Additionally, we establish the vital connections between graph unlearning and differential privacy, augmenting our understanding of the relevance of privacy-preserving techniques in this context. To ensure clarity, we provide lucid explanations of the fundamental concepts and evaluation measures used in gr
&lt;/p&gt;</description></item><item><title>MaGNet&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#39034;&#24207;&#22320;&#25972;&#21512;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#32039;&#20945;&#22270;&#32467;&#26500;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13459</link><description>&lt;p&gt;
&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25972;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Model-Agnostic Graph Neural Network for Integrating Local and Global Information. (arXiv:2309.13459v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13459
&lt;/p&gt;
&lt;p&gt;
MaGNet&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#39034;&#24207;&#22320;&#25972;&#21512;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#32039;&#20945;&#22270;&#32467;&#26500;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#20197;&#22270;&#20026;&#37325;&#28857;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;GNN&#23384;&#22312;&#20004;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#30001;&#20110;&#40657;&#30418;&#29305;&#24615;&#65292;&#32467;&#26524;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65307;&#26080;&#27861;&#23398;&#20064;&#19981;&#21516;&#39034;&#24207;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MaGNet&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#39034;&#24207;&#22320;&#25972;&#21512;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#20174;&#39640;&#38454;&#37051;&#23621;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#32039;&#20945;&#22270;&#32467;&#26500;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;MaGNet&#30001;&#20004;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#22270;&#25299;&#25169;&#19979;&#22797;&#26434;&#20851;&#31995;&#30340;&#28508;&#22312;&#34920;&#31034;&#30340;&#20272;&#35745;&#27169;&#22411;&#21644;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#12289;&#36793;&#21644;&#37325;&#35201;&#33410;&#28857;&#29305;&#24449;&#30340;&#35299;&#37322;&#27169;&#22411;&#12290;&#20174;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;Rademacher&#22797;&#26434;&#24230;&#24314;&#31435;&#20102;MaGNet&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have achieved promising performance in a variety of graph-focused tasks. Despite their success, existing GNNs suffer from two significant limitations: a lack of interpretability in results due to their black-box nature, and an inability to learn representations of varying orders. To tackle these issues, we propose a novel Model-agnostic Graph Neural Network (MaGNet) framework, which is able to sequentially integrate information of various orders, extract knowledge from high-order neighbors, and provide meaningful and interpretable results by identifying influential compact graph structures. In particular, MaGNet consists of two components: an estimation model for the latent representation of complex relationships under graph topology, and an interpretation model that identifies influential nodes, edges, and important node features. Theoretically, we establish the generalization error bound for MaGNet via empirical Rademacher complexity, and showcase its pow
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20851;&#38190;&#39046;&#22495;&#20013;&#38024;&#23545;&#40723;&#21169;&#25919;&#31574;&#30340;&#26368;&#20248;&#21644;&#20844;&#24179;&#35780;&#20272;&#20197;&#21450;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#20154;&#31867;&#19981;&#36981;&#24490;&#27835;&#30103;&#24314;&#35758;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#31574;&#30053;&#35268;&#21017;&#21482;&#26159;&#24314;&#35758;&#12290;&#21516;&#26102;&#65292;&#38024;&#23545;&#27835;&#30103;&#30340;&#24322;&#36136;&#24615;&#21644;&#20844;&#24179;&#32771;&#34385;&#22240;&#32032;&#65292;&#20915;&#31574;&#32773;&#30340;&#26435;&#34913;&#21644;&#20915;&#31574;&#35268;&#21017;&#20063;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#22312;&#31038;&#20250;&#26381;&#21153;&#39046;&#22495;&#65292;&#30740;&#31350;&#26174;&#31034;&#23384;&#22312;&#19968;&#20010;&#20351;&#29992;&#24046;&#36317;&#38382;&#39064;&#65292;&#37027;&#20123;&#26368;&#26377;&#21487;&#33021;&#21463;&#30410;&#30340;&#20154;&#21364;&#26080;&#27861;&#33719;&#24471;&#36825;&#20123;&#30410;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.07176</link><description>&lt;p&gt;
&#26368;&#20248;&#21644;&#20844;&#24179;&#30340;&#40723;&#21169;&#25919;&#31574;&#35780;&#20272;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal and Fair Encouragement Policy Evaluation and Learning. (arXiv:2309.07176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20851;&#38190;&#39046;&#22495;&#20013;&#38024;&#23545;&#40723;&#21169;&#25919;&#31574;&#30340;&#26368;&#20248;&#21644;&#20844;&#24179;&#35780;&#20272;&#20197;&#21450;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#20154;&#31867;&#19981;&#36981;&#24490;&#27835;&#30103;&#24314;&#35758;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#31574;&#30053;&#35268;&#21017;&#21482;&#26159;&#24314;&#35758;&#12290;&#21516;&#26102;&#65292;&#38024;&#23545;&#27835;&#30103;&#30340;&#24322;&#36136;&#24615;&#21644;&#20844;&#24179;&#32771;&#34385;&#22240;&#32032;&#65292;&#20915;&#31574;&#32773;&#30340;&#26435;&#34913;&#21644;&#20915;&#31574;&#35268;&#21017;&#20063;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#22312;&#31038;&#20250;&#26381;&#21153;&#39046;&#22495;&#65292;&#30740;&#31350;&#26174;&#31034;&#23384;&#22312;&#19968;&#20010;&#20351;&#29992;&#24046;&#36317;&#38382;&#39064;&#65292;&#37027;&#20123;&#26368;&#26377;&#21487;&#33021;&#21463;&#30410;&#30340;&#20154;&#21364;&#26080;&#27861;&#33719;&#24471;&#36825;&#20123;&#30410;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#38190;&#39046;&#22495;&#20013;&#65292;&#24378;&#21046;&#20010;&#20307;&#25509;&#21463;&#27835;&#30103;&#36890;&#24120;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#22240;&#27492;&#22312;&#20154;&#31867;&#19981;&#36981;&#24490;&#27835;&#30103;&#24314;&#35758;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#31574;&#30053;&#35268;&#21017;&#21482;&#26159;&#24314;&#35758;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#25509;&#21463;&#27835;&#30103;&#30340;&#20010;&#20307;&#21487;&#33021;&#23384;&#22312;&#24322;&#36136;&#24615;&#65292;&#27835;&#30103;&#25928;&#26524;&#20063;&#21487;&#33021;&#23384;&#22312;&#24322;&#36136;&#24615;&#12290;&#34429;&#28982;&#26368;&#20248;&#27835;&#30103;&#35268;&#21017;&#21487;&#20197;&#26368;&#22823;&#21270;&#25972;&#20010;&#20154;&#32676;&#30340;&#22240;&#26524;&#32467;&#26524;&#65292;&#20294;&#22312;&#40723;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#35775;&#38382;&#24179;&#31561;&#38480;&#21046;&#25110;&#20854;&#20182;&#20844;&#24179;&#32771;&#34385;&#22240;&#32032;&#21487;&#33021;&#26159;&#30456;&#20851;&#30340;&#12290;&#20363;&#22914;&#65292;&#22312;&#31038;&#20250;&#26381;&#21153;&#39046;&#22495;&#65292;&#19968;&#20010;&#25345;&#20037;&#30340;&#38590;&#39064;&#26159;&#37027;&#20123;&#26368;&#26377;&#21487;&#33021;&#20174;&#20013;&#21463;&#30410;&#30340;&#20154;&#20013;&#37027;&#20123;&#33719;&#30410;&#26381;&#21153;&#30340;&#20351;&#29992;&#24046;&#36317;&#12290;&#24403;&#20915;&#31574;&#32773;&#23545;&#35775;&#38382;&#21644;&#24179;&#22343;&#32467;&#26524;&#37117;&#26377;&#20998;&#37197;&#20559;&#22909;&#26102;&#65292;&#26368;&#20248;&#20915;&#31574;&#35268;&#21017;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22240;&#26524;&#35782;&#21035;&#12289;&#32479;&#35745;&#26041;&#24046;&#20943;&#23569;&#20272;&#35745;&#21644;&#31283;&#20581;&#20272;&#35745;&#30340;&#26368;&#20248;&#27835;&#30103;&#35268;&#21017;&#65292;&#21253;&#25324;&#22312;&#36829;&#21453;&#38451;&#24615;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In consequential domains, it is often impossible to compel individuals to take treatment, so that optimal policy rules are merely suggestions in the presence of human non-adherence to treatment recommendations. In these same domains, there may be heterogeneity both in who responds in taking-up treatment, and heterogeneity in treatment efficacy. While optimal treatment rules can maximize causal outcomes across the population, access parity constraints or other fairness considerations can be relevant in the case of encouragement. For example, in social services, a persistent puzzle is the gap in take-up of beneficial services among those who may benefit from them the most. When in addition the decision-maker has distributional preferences over both access and average outcomes, the optimal decision rule changes. We study causal identification, statistical variance-reduced estimation, and robust estimation of optimal treatment rules, including under potential violations of positivity. We c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#37329;&#34701;&#25351;&#25968;&#36319;&#36394;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#20855;&#26377;&#21160;&#24577;&#24615;&#30340;&#31163;&#25955;&#26102;&#38388;&#26080;&#31351;&#26399;&#27169;&#22411;&#65292;&#23427;&#20811;&#26381;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#19968;&#20123;&#23616;&#38480;&#65292;&#21487;&#20197;&#31934;&#30830;&#35745;&#31639;&#20132;&#26131;&#25104;&#26412;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#36319;&#36394;&#35823;&#24046;&#21644;&#20132;&#26131;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#33021;&#26377;&#25928;&#21033;&#29992;&#38271;&#26102;&#38388;&#27573;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#35813;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#25968;&#25454;&#38480;&#21046;&#23548;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02820</link><description>&lt;p&gt;
&#38024;&#23545;&#37329;&#34701;&#25351;&#25968;&#36319;&#36394;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Financial Index Tracking. (arXiv:2308.02820v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#37329;&#34701;&#25351;&#25968;&#36319;&#36394;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#20855;&#26377;&#21160;&#24577;&#24615;&#30340;&#31163;&#25955;&#26102;&#38388;&#26080;&#31351;&#26399;&#27169;&#22411;&#65292;&#23427;&#20811;&#26381;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#19968;&#20123;&#23616;&#38480;&#65292;&#21487;&#20197;&#31934;&#30830;&#35745;&#31639;&#20132;&#26131;&#25104;&#26412;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#36319;&#36394;&#35823;&#24046;&#21644;&#20132;&#26131;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#33021;&#26377;&#25928;&#21033;&#29992;&#38271;&#26102;&#38388;&#27573;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#35813;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#25968;&#25454;&#38480;&#21046;&#23548;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31163;&#25955;&#26102;&#38388;&#26080;&#31351;&#26399;&#21160;&#24577;&#24418;&#24335;&#30340;&#37329;&#34701;&#25351;&#25968;&#36319;&#36394;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#22522;&#20110;&#25910;&#30410;&#30340;&#36319;&#36394;&#35823;&#24046;&#21644;&#22522;&#20110;&#20215;&#20540;&#30340;&#36319;&#36394;&#35823;&#24046;&#12290;&#35813;&#27169;&#22411;&#20811;&#26381;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#19981;&#20165;&#38480;&#20110;&#20215;&#26684;&#30340;&#24066;&#22330;&#20449;&#24687;&#21464;&#37327;&#30340;&#26102;&#38388;&#21160;&#24577;&#24615;&#65292;&#21487;&#20197;&#31934;&#30830;&#35745;&#31639;&#20132;&#26131;&#25104;&#26412;&#65292;&#32771;&#34385;&#36319;&#36394;&#35823;&#24046;&#21644;&#20132;&#26131;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#38271;&#26102;&#38388;&#27573;&#30340;&#25968;&#25454;&#31561;&#12290;&#35813;&#27169;&#22411;&#36824;&#24341;&#20837;&#20102;&#29616;&#37329;&#27880;&#20837;&#25110;&#25552;&#21462;&#30340;&#26032;&#30340;&#20915;&#31574;&#21464;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;Banach&#19981;&#21160;&#28857;&#36845;&#20195;&#27714;&#35299;&#25237;&#36164;&#32452;&#21512;&#20877;&#24179;&#34913;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#35745;&#31639;&#23454;&#36341;&#20013;&#25351;&#23450;&#20026;&#20132;&#26131;&#37327;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#20132;&#26131;&#25104;&#26412;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#25193;&#23637;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#21160;&#24577;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;RL&#26041;&#27861;&#35299;&#20915;&#20102;&#30001;&#25968;&#25454;&#38480;&#21046;&#24341;&#36215;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the first discrete-time infinite-horizon dynamic formulation of the financial index tracking problem under both return-based tracking error and value-based tracking error. The formulation overcomes the limitations of existing models by incorporating the intertemporal dynamics of market information variables not limited to prices, allowing exact calculation of transaction costs, accounting for the tradeoff between overall tracking error and transaction costs, allowing effective use of data in a long time period, etc. The formulation also allows novel decision variables of cash injection or withdraw. We propose to solve the portfolio rebalancing equation using a Banach fixed point iteration, which allows to accurately calculate the transaction costs specified as nonlinear functions of trading volumes in practice. We propose an extension of deep reinforcement learning (RL) method to solve the dynamic formulation. Our RL method resolves the issue of data limitation resulting fro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24494;&#34920;&#24773;&#35270;&#39057;&#25968;&#25454;&#38598;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24085;&#37329;&#26862;&#30149;&#31579;&#26597;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24494;&#31505;&#35270;&#39057;&#20013;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;89.7%&#30340;&#20934;&#30830;&#24615;&#21644;89.3%&#30340;AUROC&#20540;&#65292;&#21516;&#26102;&#22312;&#20154;&#32676;&#23376;&#32452;&#19978;&#27809;&#26377;&#26816;&#27979;&#21040;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2308.02588</link><description>&lt;p&gt;
&#29992;&#24494;&#31505;&#25581;&#31034;&#24085;&#37329;&#26862;&#30149;&#65306;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31579;&#26597;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Unmasking Parkinson's Disease with Smile: An AI-enabled Screening Framework. (arXiv:2308.02588v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24494;&#34920;&#24773;&#35270;&#39057;&#25968;&#25454;&#38598;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24085;&#37329;&#26862;&#30149;&#31579;&#26597;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24494;&#31505;&#35270;&#39057;&#20013;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;89.7%&#30340;&#20934;&#30830;&#24615;&#21644;89.3%&#30340;AUROC&#20540;&#65292;&#21516;&#26102;&#22312;&#20154;&#32676;&#23376;&#32452;&#19978;&#27809;&#26377;&#26816;&#27979;&#21040;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#30446;&#21069;&#32570;&#20047;&#21487;&#38752;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#26377;&#38480;&#30340;&#20020;&#24202;&#25252;&#29702;&#36164;&#28304;&#65292;&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#30340;&#35786;&#26029;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;&#24494;&#34920;&#24773;&#30340;&#26368;&#22823;&#35270;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;PD&#31579;&#26597;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;1,059&#21517;&#29420;&#31435;&#21442;&#19982;&#32773;&#30340;3,871&#20010;&#35270;&#39057;&#65292;&#20854;&#20013;&#21253;&#25324;256&#21517;&#33258;&#25253;PD&#24739;&#32773;&#12290;&#36825;&#20123;&#24405;&#20687;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#65292;&#21253;&#25324;&#22810;&#20010;&#22269;&#23478;&#30340;&#21442;&#19982;&#32773;&#23478;&#20013;&#12289;&#19968;&#23478;&#35786;&#25152;&#21644;&#19968;&#20010;&#32654;&#22269;&#30340;PD&#25252;&#29702;&#26426;&#26500;&#12290;&#36890;&#36807;&#21033;&#29992;&#38754;&#37096;&#26631;&#24535;&#21644;&#34892;&#21160;&#21333;&#20301;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#19982;PD&#30340;&#19968;&#20010;&#20027;&#35201;&#30151;&#29366;Hypomimia&#65288;&#38754;&#37096;&#34920;&#24773;&#20943;&#23569;&#65289;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#22312;&#36825;&#20123;&#29305;&#24449;&#19978;&#35757;&#32451;&#30340;&#19968;&#32452;AI&#27169;&#22411;&#22312;&#20445;&#30041;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;89.7%&#30340;&#20934;&#30830;&#24615;&#21644;89.3%&#30340;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24615;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUROC&#65289;&#65292;&#24182;&#19988;&#22312;&#24615;&#21035;&#21644;&#31181;&#26063;&#31561;&#20154;&#32676;&#23376;&#32452;&#19978;&#26080;&#21487;&#26816;&#27979;&#30340;&#20559;&#35265;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#65292;&#20165;&#36890;&#36807;&#24494;&#31505;&#35270;&#39057;&#20013;&#30340;&#29305;&#24449;&#23601;&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#21644;AUROC&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkinson's disease (PD) diagnosis remains challenging due to lacking a reliable biomarker and limited access to clinical care. In this study, we present an analysis of the largest video dataset containing micro-expressions to screen for PD. We collected 3,871 videos from 1,059 unique participants, including 256 self-reported PD patients. The recordings are from diverse sources encompassing participants' homes across multiple countries, a clinic, and a PD care facility in the US. Leveraging facial landmarks and action units, we extracted features relevant to Hypomimia, a prominent symptom of PD characterized by reduced facial expressions. An ensemble of AI models trained on these features achieved an accuracy of 89.7% and an Area Under the Receiver Operating Characteristic (AUROC) of 89.3% while being free from detectable bias across population subgroups based on sex and ethnicity on held-out data. Further analysis reveals that features from the smiling videos alone lead to comparable 
&lt;/p&gt;</description></item><item><title>&#36951;&#24536;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#19981;&#20165;&#38480;&#20110;&#36830;&#32493;&#23398;&#20064;&#39046;&#22495;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#19982;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#36951;&#24536;&#19981;&#24635;&#26159;&#26377;&#23475;&#30340;&#65292;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#20013;&#12290;</title><link>http://arxiv.org/abs/2307.09218</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#36951;&#24536;&#29616;&#35937;&#30340;&#20840;&#38754;&#35843;&#26597;&#65306;&#36229;&#36234;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning. (arXiv:2307.09218v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09218
&lt;/p&gt;
&lt;p&gt;
&#36951;&#24536;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#19981;&#20165;&#38480;&#20110;&#36830;&#32493;&#23398;&#20064;&#39046;&#22495;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#19982;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#36951;&#24536;&#19981;&#24635;&#26159;&#26377;&#23475;&#30340;&#65292;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36951;&#24536;&#25351;&#30340;&#26159;&#20808;&#21069;&#33719;&#21462;&#30340;&#20449;&#24687;&#25110;&#30693;&#35782;&#30340;&#20007;&#22833;&#25110;&#24694;&#21270;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20851;&#20110;&#36951;&#24536;&#30340;&#35843;&#26597;&#20027;&#35201;&#38598;&#20013;&#22312;&#36830;&#32493;&#23398;&#20064;&#26041;&#38754;&#65292;&#20294;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#36951;&#24536;&#26159;&#19968;&#31181;&#26222;&#36941;&#29616;&#35937;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#20013;&#35266;&#23519;&#21040;&#12290;&#36951;&#24536;&#22312;&#30740;&#31350;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26469;&#65292;&#20363;&#22914;&#30001;&#20110;&#29983;&#25104;&#22120;&#28418;&#31227;&#32780;&#22312;&#29983;&#25104;&#27169;&#22411;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26469;&#65292;&#20197;&#21450;&#30001;&#20110;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#32780;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26469;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#28041;&#21450;&#21040;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#22312;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#21516;&#26102;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36830;&#32493;&#23398;&#20064;&#35843;&#26597;&#37117;&#40664;&#35748;&#35748;&#20026;&#36951;&#24536;&#24635;&#26159;&#26377;&#23475;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#35748;&#20026;&#36951;&#24536;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#20363;&#22914;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#12290;&#36890;&#36807;&#22312;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#25506;&#35752;&#36951;&#24536;&#29616;&#35937;&#65292;
&lt;/p&gt;
&lt;p&gt;
Forgetting refers to the loss or deterioration of previously acquired information or knowledge. While the existing surveys on forgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research domains within deep learning. Forgetting manifests in research fields such as generative models due to generator shifts, and federated learning due to heterogeneous data distributions across clients. Addressing forgetting encompasses several challenges, including balancing the retention of old task knowledge with fast learning of new tasks, managing task interference with conflicting goals, and preventing privacy leakage, etc. Moreover, most existing surveys on continual learning implicitly assume that forgetting is always harmful. In contrast, our survey argues that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios. By exploring forgetting in a broader context
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#26426;&#22120;&#32763;&#35793;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#25552;&#20379;&#32508;&#21512;&#32508;&#36848;&#21644;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#36129;&#29486;&#19979;&#19968;&#20195;&#26041;&#27861;&#30340;&#24895;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.13041</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable Evaluation Metrics for Machine Translation. (arXiv:2306.13041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#26426;&#22120;&#32763;&#35793;&#21487;&#35299;&#37322;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#25552;&#20379;&#32508;&#21512;&#32508;&#36848;&#21644;&#26368;&#26032;&#26041;&#27861;&#65292;&#24182;&#36129;&#29486;&#19979;&#19968;&#20195;&#26041;&#27861;&#30340;&#24895;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#35789;&#27719;&#37325;&#21472;&#24230;&#37327;&#65288;&#22914;BLEU&#65289;&#19981;&#21516;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#30340;&#25351;&#26631;&#65288;&#20363;&#22914;COMET&#25110;BERTScore&#65289;&#22522;&#20110;&#40657;&#30418;&#23376;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#20204;&#36890;&#24120;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#24378;&#30456;&#20851;&#24615;&#65292;&#20294;&#26159;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36739;&#20302;&#36136;&#37327;&#30340;&#20256;&#32479;&#25351;&#26631;&#20173;&#28982;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#20854;&#20013;&#19968;&#20010;&#28508;&#22312;&#21407;&#22240;&#26159;&#23427;&#20204;&#30340;&#20915;&#31574;&#36807;&#31243;&#26356;&#36879;&#26126;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20419;&#36827;&#26032;&#30340;&#39640;&#36136;&#37327;&#25351;&#26631;&#30340;&#26356;&#24191;&#27867;&#25509;&#21463;&#65292;&#35299;&#37322;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#27010;&#24565;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21487;&#35299;&#37322;&#26426;&#22120;&#32763;&#35793;&#25351;&#26631;&#30340;&#20851;&#38190;&#23646;&#24615;&#21644;&#30446;&#26631;&#65292;&#24182;&#25552;&#20379;&#20102;&#26368;&#36817;&#25216;&#26415;&#30340;&#32508;&#21512;&#32508;&#36848;&#65292;&#23558;&#23427;&#20204;&#19982;&#25105;&#20204;&#30830;&#31435;&#30340;&#30446;&#26631;&#21644;&#23646;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;ChatGPT&#21644;GPT4&#65289;&#30340;&#21487;&#35299;&#37322;&#25351;&#26631;&#30340;&#26368;&#26032;&#20808;&#36827;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;&#19979;&#19968;&#20195;&#26041;&#27861;&#30340;&#24895;&#26223;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;e&#12290;
&lt;/p&gt;
&lt;p&gt;
Unlike classical lexical overlap metrics such as BLEU, most current evaluation metrics for machine translation (for example, COMET or BERTScore) are based on black-box large language models. They often achieve strong correlations with human judgments, but recent research indicates that the lower-quality classical metrics remain dominant, one of the potential reasons being that their decision processes are more transparent. To foster more widespread acceptance of novel high-quality metrics, explainability thus becomes crucial. In this concept paper, we identify key properties as well as key goals of explainable machine translation metrics and provide a comprehensive synthesis of recent techniques, relating them to our established goals and properties. In this context, we also discuss the latest state-of-the-art approaches to explainable metrics based on generative models such as ChatGPT and GPT4. Finally, we contribute a vision of next-generation approaches, including natural language e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#32500;&#29305;&#24449;&#36924;&#36817;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#35889;&#23884;&#20837;&#25511;&#21046;&#31639;&#27861;&#65288;SDEC&#65289;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2304.03907</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#38480;&#32500;&#35889;&#21160;&#24577;&#23884;&#20837;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Stochastic Nonlinear Control via Finite-dimensional Spectral Dynamic Embedding. (arXiv:2304.03907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#32500;&#29305;&#24449;&#36924;&#36817;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#35889;&#23884;&#20837;&#25511;&#21046;&#31639;&#27861;&#65288;SDEC&#65289;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#19968;&#30452;&#26159;&#19968;&#20010;&#26840;&#25163;&#30340;&#38382;&#39064;&#12290;Ren&#31561;&#20154;&#24341;&#20837;&#20102;&#35889;&#21160;&#24577;&#23884;&#20837;&#26469;&#24320;&#21457;&#25511;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#26080;&#31351;&#32500;&#29305;&#24449;&#26469;&#32447;&#24615;&#34920;&#31034;&#29366;&#24577;&#20540;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#26377;&#38480;&#32500;&#30340;&#25130;&#26029;&#36924;&#36817;&#36827;&#34892;&#23454;&#38469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#24050;&#30693;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25511;&#21046;&#20013;&#30340;&#26377;&#38480;&#32500;&#36924;&#36817;&#24615;&#36136;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#25511;&#21046;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#26377;&#38480;&#32500;&#29305;&#24449;&#36924;&#36817;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#35889;&#23884;&#20837;&#25511;&#21046;&#65288;SDEC&#65289;&#65292;&#24182;&#36827;&#34892;&#28145;&#20837;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#34920;&#24449;&#30001;&#26377;&#38480;&#32500;&#25130;&#26029;&#24341;&#36215;&#30340;&#36924;&#36817;&#35823;&#24046;&#21644;&#30001;&#26377;&#38480;&#26679;&#26412;&#36924;&#36817;&#24341;&#36215;&#30340;&#32479;&#35745;&#35823;&#24046;&#65292;&#21516;&#26102;&#36827;&#34892;&#25919;&#31574;&#35780;&#20272;&#21644;&#25919;&#31574;&#20248;&#21270;&#30340;&#23454;&#39564;&#27979;&#35797;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal control is notoriously difficult for stochastic nonlinear systems. Ren et al. introduced Spectral Dynamics Embedding for developing reinforcement learning methods for controlling an unknown system. It uses an infinite-dimensional feature to linearly represent the state-value function and exploits finite-dimensional truncation approximation for practical implementation. However, the finite-dimensional approximation properties in control have not been investigated even when the model is known. In this paper, we provide a tractable stochastic nonlinear control algorithm that exploits the nonlinear dynamics upon the finite-dimensional feature approximation, Spectral Dynamics Embedding Control (SDEC), with an in-depth theoretical analysis to characterize the approximation error induced by the finite-dimension truncation and statistical error induced by finite-sample approximation in both policy evaluation and policy optimization. We also empirically test the algorithm and compare th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24179;&#31283;&#20004;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#22788;&#29702;&#24179;&#28369;&#21464;&#21270;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31574;&#30053;&#22312;&#20108;&#27425;Lipschitz&#36830;&#32493;&#30340;&#24773;&#20917;&#19979;&#30340;&#36951;&#25022;&#20026; $\tilde O(T^{3/5})$&#12290;</title><link>http://arxiv.org/abs/2301.12366</link><description>&lt;p&gt;
&#24179;&#28369;&#30340;&#38750;&#24179;&#31283;&#36830;&#32493;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Smooth Non-Stationary Bandits. (arXiv:2301.12366v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24179;&#31283;&#20004;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#22788;&#29702;&#24179;&#28369;&#21464;&#21270;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31574;&#30053;&#22312;&#20108;&#27425;Lipschitz&#36830;&#32493;&#30340;&#24773;&#20917;&#19979;&#30340;&#36951;&#25022;&#20026; $\tilde O(T^{3/5})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22312;&#32447;&#20915;&#31574;&#24212;&#29992;&#20013;&#65292;&#29615;&#22659;&#37117;&#26159;&#38750;&#24179;&#31283;&#30340;&#65292;&#22240;&#27492;&#20351;&#29992;&#33021;&#22815;&#22788;&#29702;&#21464;&#21270;&#30340;&#36172;&#21338;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26159;&#20026;&#20102;&#20445;&#25252;&#38750;&#24179;&#28369;&#21464;&#21270;&#32780;&#35774;&#35745;&#30340;&#65292;&#20165;&#21463;&#21040;&#24635;&#21464;&#24046;&#25110;&#26102;&#38388;&#19978;&#30340;Lipschitz&#24615;&#30340;&#38480;&#21046;&#65292;&#20854;&#20013;&#23427;&#20204;&#20445;&#35777;$\tilde \Theta(T^{2/3})$&#30340;&#36951;&#25022;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#29615;&#22659;&#32463;&#24120;&#20197;&#24179;&#31283;&#30340;&#26041;&#24335;&#25913;&#21464;&#65292;&#22240;&#27492;&#36825;&#31181;&#31639;&#27861;&#21487;&#33021;&#20250;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#20135;&#29983;&#27604;&#24517;&#35201;&#26356;&#39640;&#30340;&#36951;&#25022;&#65292;&#24182;&#19988;&#19981;&#21033;&#29992;&#21464;&#21270;&#29575;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#38750;&#24179;&#31283;&#30340;&#20004;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20551;&#35774;&#33218;&#30340;&#24179;&#22343;&#22238;&#25253;&#26159;&#19968;&#20010;$\beta$-H\''older&#20989;&#25968;&#65292;&#21363;&#23427;&#26159;$(\beta-1)$&#27425;Lipschitz&#36830;&#32493;&#21487;&#24494;&#20998;&#30340;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31574;&#30053;&#65292;&#23545;&#20110;$\beta=2$&#65292;&#23427;&#30340;&#36951;&#25022;&#20026;$\tilde O(T^{3/5})$&#65292;&#20174;&#32780;&#39318;&#27425;&#22312;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#20043;&#38388;&#36827;&#34892;&#20102;&#21306;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20219;&#24847;$\Omg(T^{(\beta+1)/(2\beta+1)})$&#30340;&#19979;&#30028;&#26469;&#34917;&#20805;&#36825;&#20010;&#32467;&#26524;&#65292;&#35828;&#26126;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#22256;&#38590;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many applications of online decision making, the environment is non-stationary and it is therefore crucial to use bandit algorithms that handle changes. Most existing approaches are designed to protect against non-smooth changes, constrained only by total variation or Lipschitzness over time, where they guarantee $\tilde \Theta(T^{2/3})$ regret. However, in practice environments are often changing {\bf smoothly}, so such algorithms may incur higher-than-necessary regret in these settings and do not leverage information on the rate of change. We study a non-stationary two-armed bandits problem where we assume that an arm's mean reward is a $\beta$-H\"older function over (normalized) time, meaning it is $(\beta-1)$-times Lipschitz-continuously differentiable. We show the first separation between the smooth and non-smooth regimes by presenting a policy with $\tilde O(T^{3/5})$ regret for $\beta=2$. We complement this result by an $\Omg(T^{(\beta+1)/(2\beta+1)})$ lower bound for any int
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;(SOM)&#20998;&#26512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#19982;&#20915;&#31574;&#30456;&#20851;&#30340;&#20869;&#37096;&#32534;&#30721;&#65292;&#21457;&#29616;&#27973;&#23618;&#23558;&#29305;&#24449;&#21387;&#32553;&#21040;&#32039;&#20945;&#31354;&#38388;&#20013;&#65292;&#32780;&#28145;&#23618;&#23558;&#29305;&#24449;&#31354;&#38388;&#25193;&#23637;&#65292;&#24182;&#25351;&#20986;&#21387;&#32553;&#29305;&#24449;&#21487;&#33021;&#23548;&#33268;&#23545;&#25932;&#23545;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.10952</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21151;&#33021;&#24615;&#31070;&#32463;&#32534;&#30721;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of functional neural codes of deep learning models. (arXiv:2205.10952v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;(SOM)&#20998;&#26512;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#19982;&#20915;&#31574;&#30456;&#20851;&#30340;&#20869;&#37096;&#32534;&#30721;&#65292;&#21457;&#29616;&#27973;&#23618;&#23558;&#29305;&#24449;&#21387;&#32553;&#21040;&#32039;&#20945;&#31354;&#38388;&#20013;&#65292;&#32780;&#28145;&#23618;&#23558;&#29305;&#24449;&#31354;&#38388;&#25193;&#23637;&#65292;&#24182;&#25351;&#20986;&#21387;&#32553;&#29305;&#24449;&#21487;&#33021;&#23548;&#33268;&#23545;&#25932;&#23545;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;(DL)&#30340;&#20195;&#29702;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#24182;&#34892;/&#39034;&#24207;&#25805;&#20316;&#12290;&#36825;&#20351;&#24471;&#29702;&#35299;DNNs&#30340;&#25805;&#20316;&#21464;&#24471;&#22256;&#38590;&#65292;&#38459;&#30861;&#20102;&#36866;&#24403;&#30340;&#35786;&#26029;&#12290;&#22312;&#27809;&#26377;&#23545;&#20854;&#20869;&#37096;&#36807;&#31243;&#26377;&#26356;&#22909;&#30340;&#20102;&#35299;&#20043;&#21069;&#65292;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#37096;&#32626;DNNs&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#25925;&#38556;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26500;&#24314;&#26356;&#21487;&#38752;&#30340;DNNs/DL&#26469;&#35299;&#20915;&#39640;&#39118;&#38505;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#65292;&#25105;&#20204;&#24517;&#39035;&#28145;&#20837;&#20102;&#35299;DNNs&#20915;&#31574;&#32972;&#21518;&#30340;&#20869;&#37096;&#25805;&#20316;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;(SOM)&#20998;&#26512;&#19982;DNNs&#20915;&#31574;&#30456;&#20851;&#30340;DL&#27169;&#22411;&#30340;&#20869;&#37096;&#32534;&#30721;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#38752;&#36817;&#36755;&#20837;&#23618;&#30340;&#27973;&#23618;&#23558;&#29305;&#24449;&#21387;&#32553;&#21040;&#32039;&#20945;&#31354;&#38388;&#20013;&#65292;&#32780;&#38752;&#36817;&#36755;&#20986;&#23618;&#30340;&#28145;&#23618;&#23558;&#29305;&#24449;&#31354;&#38388;&#25193;&#23637;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#21387;&#32553;&#29305;&#24449;&#21487;&#33021;&#23548;&#33268;DNNs&#23545;&#25932;&#23545;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs), the agents of deep learning (DL), require a massive number of parallel/sequential operations. This makes it difficult to comprehend DNNs' operations and impedes proper diagnosis. Without better knowledge of their internal process, deploying DNNs in high-stakes domains can lead to catastrophic failures. Therefore, to build more reliable DNNs/DL to be deployed in high-stakes real-world problems, it is imperative that we gain insights into DNNs' internal operations underlying their decision-making. Here, we use the self-organizing map (SOM) to analyze DL models' internal codes associated with DNNs' decision-making. Our analyses suggest that shallow layers close to the input layer compress features into condensed space and that deep layers close to the output layer expand feature space. We also found evidence indicating that compressed features may underlie DNNs' vulnerabilities to adversarial perturbations.
&lt;/p&gt;</description></item></channel></rss>