<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#30740;&#31350;&#23545;&#31227;&#21160;&#35774;&#22791;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25191;&#34892;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#24182;&#21019;&#24314;&#20102;&#33258;&#21160;&#21270;&#22522;&#30784;&#26550;&#26500;MELT&#26469;&#25903;&#25345;&#20854;&#35780;&#20272;&#21644;&#24615;&#33021;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2403.12844</link><description>&lt;p&gt;
MELTing point: &#31227;&#21160;&#35821;&#35328;&#36716;&#25442;&#22120;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MELTing point: Mobile Evaluation of Language Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12844
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#31227;&#21160;&#35774;&#22791;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25191;&#34892;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#24182;&#21019;&#24314;&#20102;&#33258;&#21160;&#21270;&#22522;&#30784;&#26550;&#26500;MELT&#26469;&#25903;&#25345;&#20854;&#35780;&#20272;&#21644;&#24615;&#33021;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#36880;&#28176;&#24212;&#29992;&#20110;&#26085;&#24120;&#20219;&#21153;&#65292;&#36171;&#20104;&#25105;&#20204;&#30340;&#35745;&#31639;&#26426;&#8220;&#26234;&#33021;&#30340;&#28779;&#33457;&#8221;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#36816;&#34892;&#26102;&#38656;&#27714;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#24191;&#27867;&#37096;&#32626;&#12290;&#22312;&#20010;&#20154;&#35774;&#22791;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#20197;&#21450;&#36805;&#36895;&#38544;&#31169;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#32039;&#36843;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#25191;&#34892;&#30340;&#29616;&#29366;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#33258;&#24049;&#30340;&#33258;&#21160;&#21270;&#22522;&#30784;&#26550;&#26500;MELT&#65292;&#25903;&#25345;&#22312;&#35774;&#22791;&#19978;&#26080;&#30028;&#38754;&#25191;&#34892;&#21644;&#35780;&#20272;LLMs&#65292;&#24182;&#25903;&#25345;&#19981;&#21516;&#30340;&#27169;&#22411;&#12289;&#35774;&#22791;&#21644;&#26694;&#26550;&#65292;&#21253;&#25324;Android&#12289;iOS&#21644;Nvidia Jetson&#35774;&#22791;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#27969;&#34892;&#30340;&#25351;&#20196;&#24494;&#35843;&#30340;LLMs&#65292;&#24182;&#21033;&#29992;&#19981;&#21516;&#30340;&#26694;&#26550;&#26469;&#27979;&#37327;&#23427;&#20204;&#30340;&#31471;&#21040;&#31471;&#21644;&#32454;&#31890;&#24230;&#24615;&#33021;&#65292;&#36319;&#36394;&#23427;&#20204;&#30340;&#20869;&#23384;&#21644;&#33021;&#32791;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12844v1 Announce Type: new  Abstract: Transformers have revolutionized the machine learning landscape, gradually making their way into everyday tasks and equipping our computers with ``sparks of intelligence''. However, their runtime requirements have prevented them from being broadly deployed on mobile. As personal devices become increasingly powerful and prompt privacy becomes an ever more pressing issue, we explore the current state of mobile execution of Large Language Models (LLMs). To achieve this, we have created our own automation infrastructure, MELT, which supports the headless execution and benchmarking of LLMs on device, supporting different models, devices and frameworks, including Android, iOS and Nvidia Jetson devices. We evaluate popular instruction fine-tuned LLMs and leverage different frameworks to measure their end-to-end and granular performance, tracing their memory and energy requirements along the way.   Our analysis is the first systematic study of o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#30340;&#20114;&#21160;&#20351;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#33258;&#36866;&#24212;&#65292;&#24182;&#20174;&#20854;&#20182;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20013;&#21306;&#20998;&#20986;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#21644;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#12290;</title><link>https://arxiv.org/abs/2403.09793</link><description>&lt;p&gt;
&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#65306;&#20855;&#26377;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31038;&#20132;&#34892;&#21160;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Socially Integrated Navigation: A Social Acting Robot with Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09793
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#30340;&#20114;&#21160;&#20351;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#33258;&#36866;&#24212;&#65292;&#24182;&#20174;&#20854;&#20182;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20013;&#21306;&#20998;&#20986;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#21644;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#27491;&#22312;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25317;&#25380;&#22330;&#26223;&#65292;&#24182;&#25104;&#20026;&#25105;&#20204;&#31038;&#20250;&#30340;&#19968;&#37096;&#20998;&#12290;&#19968;&#20010;&#20855;&#26377;&#20010;&#20307;&#20154;&#31867;&#32771;&#34385;&#30340;&#31038;&#20250;&#21487;&#25509;&#21463;&#30340;&#23548;&#33322;&#34892;&#20026;&#23545;&#20110;&#21487;&#25193;&#23637;&#30340;&#24212;&#29992;&#21644;&#20154;&#31867;&#25509;&#21463;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#26469;&#23398;&#20064;&#26426;&#22120;&#20154;&#30340;&#23548;&#33322;&#31574;&#30053;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#24314;&#35758;&#26681;&#25454;&#26426;&#22120;&#20154;&#23637;&#31034;&#30340;&#31038;&#20132;&#34892;&#20026;&#23558;&#29616;&#26377;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20998;&#20026;&#20855;&#26377;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#21644;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#26159;&#33258;&#36866;&#24212;&#30340;&#65292;&#24182;&#19988;&#26159;&#36890;&#36807;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#32780;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26500;&#24335;&#28304;&#33258;&#31038;&#20250;&#23398;&#23450;&#20041;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09793v1 Announce Type: cross  Abstract: Mobile robots are being used on a large scale in various crowded situations and become part of our society. The socially acceptable navigation behavior of a mobile robot with individual human consideration is an essential requirement for scalable applications and human acceptance. Deep Reinforcement Learning (DRL) approaches are recently used to learn a robot's navigation policy and to model the complex interactions between robots and humans. We propose to divide existing DRL-based navigation approaches based on the robot's exhibited social behavior and distinguish between social collision avoidance with a lack of social behavior and socially aware approaches with explicit predefined social behavior. In addition, we propose a novel socially integrated navigation approach where the robot's social behavior is adaptive and emerges from the interaction with humans. The formulation of our approach is derived from a sociological definition, 
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04202</link><description>&lt;p&gt;
&#24322;&#36136;&#23398;&#20064;&#20195;&#29702;&#32676;&#20307;&#20013;&#36947;&#24503;&#34892;&#20026;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04202
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#26085;&#30410;&#20851;&#27880;AI&#31995;&#32479;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#30340;&#38382;&#39064;&#31361;&#26174;&#20102;&#22312;&#20154;&#24037;&#20195;&#29702;&#20013;&#23884;&#20837;&#36947;&#24503;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#32463;&#39564;&#23398;&#20064;&#65292;&#21363;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#22810;&#20195;&#29702;&#65288;&#31038;&#20250;&#65289;&#29615;&#22659;&#20013;&#65292;&#20010;&#20307;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#21487;&#33021;&#20135;&#29983;&#22797;&#26434;&#30340;&#32676;&#20307;&#23618;&#38754;&#29616;&#35937;&#12290;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#20381;&#36182;&#20110;&#27169;&#25311;&#30340;&#31038;&#20250;&#22256;&#22659;&#29615;&#22659;&#26469;&#30740;&#31350;&#29420;&#31435;&#23398;&#20064;&#20195;&#29702;&#30340;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#23454;&#36341;&#20013;&#20195;&#29702;&#31038;&#20250;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36947;&#24503;&#24322;&#36136;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#65292;&#21333;&#20010;&#23398;&#20064;&#20195;&#29702;&#21487;&#33021;&#38754;&#23545;&#21518;&#26524;&#20027;&#20041;&#32773;&#65288;&#21363;&#20851;&#24515;&#38543;&#26102;&#38388;&#26368;&#22823;&#21270;&#26576;&#31181;&#32467;&#26524;&#65289;&#25110;&#22522;&#20110;&#35268;&#33539;&#30340;&#23545;&#25163;&#65288;&#21363;&#19987;&#27880;&#20110;&#31435;&#21363;&#36981;&#23432;&#29305;&#23450;&#35268;&#33539;&#65289; &#12290;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#33021;&#21463;&#21040;&#36825;&#31181;&#36947;&#24503;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 Announce Type: cross  Abstract: Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents. A promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents. However, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., caring about maximizing some outcome over time) or norm-based (i.e., focusing on conforming to a specific norm here and now). The extent to which agents' co-development may be impacted by such moral heterogeneity in 
&lt;/p&gt;</description></item><item><title>CGGM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#65292;&#35299;&#20915;&#20102;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#33410;&#28857;&#24322;&#24120;&#26816;&#27979;&#20013;&#33410;&#28857;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.17363</link><description>&lt;p&gt;
CGGM&#65306;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#30340;&#26465;&#20214;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#33410;&#28857;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CGGM: A conditional graph generation model with adaptive sparsity for node anomaly detection in IoT networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17363
&lt;/p&gt;
&lt;p&gt;
CGGM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#65292;&#35299;&#20915;&#20102;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#33410;&#28857;&#24322;&#24120;&#26816;&#27979;&#20013;&#33410;&#28857;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#34987;&#24191;&#27867;&#29992;&#20110;&#26816;&#27979;&#29289;&#32852;&#32593;&#20013;&#33410;&#28857;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#22270;&#20013;&#33410;&#28857;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#30340;&#32422;&#26463;&#21253;&#25324;&#37051;&#25509;&#20851;&#31995;&#30340;&#21333;&#35843;&#24615;&#65292;&#20026;&#33410;&#28857;&#26500;&#24314;&#22810;&#32500;&#29305;&#24449;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#32570;&#20047;&#31471;&#21040;&#31471;&#29983;&#25104;&#22810;&#31867;&#33410;&#28857;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CGGM&#30340;&#26032;&#39062;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#23569;&#25968;&#31867;&#21035;&#20013;&#26356;&#22810;&#33410;&#28857;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#30340;&#26426;&#21046;&#22686;&#24378;&#20102;&#20854;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;&#29305;&#24449;&#29983;&#25104;&#27169;&#22359;&#21517;&#20026;&#22810;&#32500;&#29305;&#24449;&#29983;&#25104;&#22120;&#65288;MFG&#65289;&#65292;&#21487;&#29983;&#25104;&#21253;&#25324;&#25299;&#25169;&#20449;&#24687;&#22312;&#20869;&#30340;&#33410;&#28857;&#29305;&#24449;&#12290;&#26631;&#31614;&#34987;&#36716;&#25442;&#20026;&#23884;&#20837;&#21521;&#37327;&#65292;&#29992;&#20316;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17363v1 Announce Type: cross  Abstract: Dynamic graphs are extensively employed for detecting anomalous behavior in nodes within the Internet of Things (IoT). Generative models are often used to address the issue of imbalanced node categories in dynamic graphs. Nevertheless, the constraints it faces include the monotonicity of adjacency relationships, the difficulty in constructing multi-dimensional features for nodes, and the lack of a method for end-to-end generation of multiple categories of nodes. This paper presents a novel graph generation model, called CGGM, designed specifically to generate a larger number of nodes belonging to the minority class. The mechanism for generating an adjacency matrix, through adaptive sparsity, enhances flexibility in its structure. The feature generation module, called multidimensional features generator (MFG) to generate node features along with topological information. Labels are transformed into embedding vectors, serving as condition
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#36127;&#23545;&#35937;&#24615;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;K+1&#20010;logits&#24182;&#22312;&#23494;&#38598;&#39044;&#27979;&#32467;&#26500;&#20013;&#23884;&#20837;&#65292;&#21487;&#29420;&#31435;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.15374</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#36127;&#23545;&#35937;&#24615;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Outlier detection by ensembling uncertainty with negative objectness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15374
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#36127;&#23545;&#35937;&#24615;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;K+1&#20010;logits&#24182;&#22312;&#23494;&#38598;&#39044;&#27979;&#32467;&#26500;&#20013;&#23884;&#20837;&#65292;&#21487;&#29420;&#31435;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#30417;&#30563;&#24335;&#35270;&#35273;&#35782;&#21035;&#20013;&#20851;&#38190;&#30340;&#21151;&#33021;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#36807;&#40723;&#21169;&#26631;&#20934;&#23553;&#38381;&#38598;&#27169;&#22411;&#22312;&#36127;&#35757;&#32451;&#25968;&#25454;&#20013;&#20135;&#29983;&#20302;&#32622;&#20449;&#24230;&#39044;&#27979;&#26469;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#28151;&#28102;&#20102;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#36127;&#31867;&#21035;&#30340;&#35782;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#30452;&#25509;&#39044;&#27979;K+1&#20010;logits&#65292;&#36825;&#20123;logits&#23545;&#24212;&#20110;K&#20010;&#22522;&#26412;&#30495;&#23454;&#31867;&#21035;&#21644;&#19968;&#20010;&#24322;&#24120;&#31867;&#21035;&#12290;&#36825;&#31181;&#35774;&#32622;&#20801;&#35768;&#25105;&#20204;&#21046;&#23450;&#19968;&#31181;&#26032;&#22855;&#30340;&#24322;&#24120;&#24471;&#20998;&#65292;&#20316;&#20026;&#20998;&#24067;&#20869;&#19981;&#30830;&#23450;&#24615;&#21644;&#24322;&#24120;&#31867;&#21035;&#30340;&#21518;&#39564;&#30340;&#38598;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#36127;&#23545;&#35937;&#24615;&#12290;&#29616;&#22312;&#65292;&#24322;&#24120;&#20540;&#21487;&#20197;&#36890;&#36807;&#39640;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25110;&#19982;&#36127;&#25968;&#25454;&#30456;&#20284;&#20043;&#22788;&#29420;&#31435;&#26816;&#27979;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#23884;&#20837;&#21040;&#19968;&#20010;&#23494;&#38598;&#39044;&#27979;&#32467;&#26500;&#20013;&#65292;&#35813;&#32467;&#26500;&#20855;&#26377;K+2&#20010;&#31867;&#21035;&#30340;&#25513;&#30721;&#32423;&#21035;&#35782;&#21035;&#12290;&#35757;&#32451;&#36807;&#31243;&#40723;&#21169;&#26032;&#39062;&#30340;K+2-th&#31867;&#21035;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15374v1 Announce Type: cross  Abstract: Outlier detection is an essential capability in safety-critical applications of supervised visual recognition. Most of the existing methods deliver best results by encouraging standard closed-set models to produce low-confidence predictions in negative training data. However, that approach conflates prediction uncertainty with recognition of the negative class. We therefore reconsider direct prediction of K+1 logits that correspond to K groundtruth classes and one outlier class. This setup allows us to formulate a novel anomaly score as an ensemble of in-distribution uncertainty and the posterior of the outlier class which we term negative objectness. Now outliers can be independently detected due to i) high prediction uncertainty or ii) similarity with negative data. We embed our method into a dense prediction architecture with mask-level recognition over K+2 classes. The training procedure encourages the novel K+2-th class to learn n
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;</title><link>https://arxiv.org/abs/2402.10962</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#27979;&#37327;&#21644;&#25511;&#21046;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Measuring and Controlling Persona Drift in Language Model Dialogs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10962
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#26159;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26631;&#20934;&#24037;&#20855;&#65292;&#20351;&#20854;&#33021;&#22815;&#25215;&#25285;&#29305;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#12290;&#22312;&#20351;&#29992;&#25552;&#31034;&#26102;&#30340;&#19968;&#20010;&#38544;&#21547;&#20551;&#35774;&#26159;&#65292;&#23427;&#20204;&#23558;&#26159;&#31283;&#23450;&#30340;&#65292;&#22240;&#27492;&#32842;&#22825;&#26426;&#22120;&#20154;&#23558;&#22312;&#25972;&#20010;&#23545;&#35805;&#36807;&#31243;&#20013;&#32487;&#32493;&#26681;&#25454;&#35268;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#29983;&#25104;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#36890;&#36807;&#20004;&#20010;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#33258;&#25105;&#23545;&#35805;&#26469;&#35780;&#20272;&#8220;&#20154;&#35774;&#8221;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#27169;&#22411;&#22914;LLaMA2-chat-70B&#36827;&#34892;&#27979;&#35797;&#65292;&#21457;&#29616;&#22312;&#20843;&#36718;&#23545;&#35805;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#12290;&#23545;&#36825;&#19968;&#29616;&#35937;&#30340;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#30001;&#20110;&#38271;&#23545;&#35805;&#20013;&#30340;&#27880;&#24847;&#21147;&#34928;&#20943;&#65292;&#21464;&#21387;&#22120;&#27880;&#24847;&#21147;&#26426;&#21046;&#36215;&#21040;&#20102;&#19968;&#23450;&#20316;&#29992;&#12290;&#20026;&#20102;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#19982;&#20004;&#20010;&#24378;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10962v1 Announce Type: cross  Abstract: Prompting is a standard tool for customizing language-model chatbots, enabling them to take on a specific "persona". An implicit assumption in the use of prompts is that they will be stable, so the chatbot will continue to generate text according to the stipulated persona for the duration of a conversation. We propose a quantitative benchmark to test this assumption, evaluating persona stability via self-chats between two personalized chatbots. Testing popular models like LLaMA2-chat-70B, we reveal a significant persona drift within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to attention decay over long exchanges. To combat attention decay and persona drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#22312;&#36229;&#31435;&#26041;&#20307;&#25968;&#25454;&#27969;&#19978;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#31934;&#24230;&#30028;&#38480;&#65292;&#20063;&#25512;&#24191;&#20102;&#20043;&#21069;&#20851;&#20110;&#35745;&#25968;&#26597;&#35810;&#30340;&#36830;&#32493;&#21457;&#24067;&#27169;&#22411;&#30340;&#24037;&#20316;&#65292;&#20165;&#38656;&#35201;&#39069;&#22806;&#30340;&#22810;&#39033;&#24335;&#23545;&#25968;&#22240;&#23376;&#12290;</title><link>https://arxiv.org/abs/2402.08012</link><description>&lt;p&gt;
&#22312;&#32447;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Online Differentially Private Synthetic Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#22312;&#36229;&#31435;&#26041;&#20307;&#25968;&#25454;&#27969;&#19978;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#31934;&#24230;&#30028;&#38480;&#65292;&#20063;&#25512;&#24191;&#20102;&#20043;&#21069;&#20851;&#20110;&#35745;&#25968;&#26597;&#35810;&#30340;&#36830;&#32493;&#21457;&#24067;&#27169;&#22411;&#30340;&#24037;&#20316;&#65292;&#20165;&#38656;&#35201;&#39069;&#22806;&#30340;&#22810;&#39033;&#24335;&#23545;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;&#23545;&#20110;&#22312;&#36229;&#31435;&#26041;&#20307;$[0,1]^d$&#20869;&#30340;&#25968;&#25454;&#27969;&#21644;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#27599;&#20010;&#26102;&#38388;$t$&#37117;&#29983;&#25104;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#35813;&#31639;&#27861;&#22312;1-Wasserstein&#36317;&#31163;&#19978;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#31934;&#24230;&#30028;&#38480;&#65306;&#24403;$d\geq 2$&#26102;&#20026;$O(t^{-1/d}\log(t)$&#65292;&#24403;$d=1$&#26102;&#20026;$O(t^{-1}\log^{4.5}(t)$&#12290;&#36825;&#20010;&#32467;&#26524;&#23558;&#20043;&#21069;&#20851;&#20110;&#35745;&#25968;&#26597;&#35810;&#30340;&#36830;&#32493;&#21457;&#24067;&#27169;&#22411;&#30340;&#24037;&#20316;&#25512;&#24191;&#21040;&#21253;&#25324;Lipschitz&#26597;&#35810;&#12290;&#19982;&#31163;&#32447;&#24773;&#20917;&#19981;&#21516;&#65292;&#31163;&#32447;&#24773;&#20917;&#19979;&#25972;&#20010;&#25968;&#25454;&#38598;&#19968;&#27425;&#24615;&#21487;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#38656;&#35201;&#22312;&#31934;&#24230;&#30028;&#38480;&#20013;&#39069;&#22806;&#30340;&#22810;&#39033;&#24335;&#23545;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a polynomial-time algorithm for online differentially private synthetic data generation. For a data stream within the hypercube $[0,1]^d$ and an infinite time horizon, we develop an online algorithm that generates a differentially private synthetic dataset at each time $t$. This algorithm achieves a near-optimal accuracy bound of $O(t^{-1/d}\log(t))$ for $d\geq 2$ and $O(t^{-1}\log^{4.5}(t))$ for $d=1$ in the 1-Wasserstein distance. This result generalizes the previous work on the continual release model for counting queries to include Lipschitz queries. Compared to the offline case, where the entire dataset is available at once, our approach requires only an extra polylog factor in the accuracy bound.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;AIR-Bench&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#21644;&#29983;&#25104;&#38899;&#39057;&#20449;&#21495;&#30340;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.07729</link><description>&lt;p&gt;
AIR-Bench: &#36890;&#36807;&#29983;&#25104;&#24615;&#29702;&#35299;&#35780;&#20272;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07729
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;AIR-Bench&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#21644;&#29983;&#25104;&#38899;&#39057;&#20449;&#21495;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25351;&#23548;&#24615;&#30340;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#23545;&#20154;&#19982;&#38899;&#39057;&#30340;&#20114;&#21160;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#33021;&#22815;&#35780;&#20272;&#20197;&#38899;&#39057;&#20026;&#20013;&#24515;&#30340;&#20114;&#21160;&#33021;&#21147;&#30340;&#22522;&#20934;&#24050;&#32463;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#20197;&#24448;&#30340;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#35780;&#20272;&#19981;&#21516;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#22914;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;&#32570;&#20047;&#23545;&#22260;&#32469;&#38899;&#39057;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#33021;&#21147;&#30340;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#36861;&#36394;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#65288;LALMs&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#24182;&#20026;&#26410;&#26469;&#30340;&#25913;&#36827;&#25552;&#20379;&#25351;&#23548;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AIR-Bench&#65288;&#38899;&#39057;&#25351;&#23548;&#22522;&#20934;&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LALMs&#29702;&#35299;&#21508;&#31181;&#31867;&#22411;&#38899;&#39057;&#20449;&#21495;&#65288;&#21253;&#25324;&#20154;&#31867;&#35821;&#38899;&#12289;&#33258;&#28982;&#22768;&#38899;&#21644;&#38899;&#20048;&#65289;&#20197;&#21450;&#19982;&#20154;&#20197;&#25991;&#26412;&#24418;&#24335;&#36827;&#34892;&#20132;&#20114;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;AIR-Bench&#21253;&#21547;&#20004;&#20010;&#32500;&#24230;&#65306;&#22522;&#30784;&#21644;&#29983;&#25104;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, instruction-following audio-language models have received broad attention for human-audio interaction. However, the absence of benchmarks capable of evaluating audio-centric interaction capabilities has impeded advancements in this field. Previous models primarily focus on assessing different fundamental tasks, such as Automatic Speech Recognition (ASR), and lack an assessment of the open-ended generative capabilities centered around audio. Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement. In this paper, we introduce AIR-Bench (\textbf{A}udio \textbf{I}nst\textbf{R}uction \textbf{Bench}mark), the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format. AIR-Bench encompasses two dimensions: \textit{foundation} and \textit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SpongeNet &#30340;&#26032;&#22411;&#28023;&#32501;&#25915;&#20987;&#65292;&#36890;&#36807;&#30452;&#25509;&#20316;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#65292;&#25104;&#21151;&#22686;&#21152;&#20102;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#32791;&#65292;&#32780;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.06357</link><description>&lt;p&gt;
SpongeNet &#25915;&#20987;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28023;&#32501;&#26435;&#37325;&#20013;&#27602;
&lt;/p&gt;
&lt;p&gt;
The SpongeNet Attack: Sponge Weight Poisoning of Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SpongeNet &#30340;&#26032;&#22411;&#28023;&#32501;&#25915;&#20987;&#65292;&#36890;&#36807;&#30452;&#25509;&#20316;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#65292;&#25104;&#21151;&#22686;&#21152;&#20102;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#32791;&#65292;&#32780;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#32501;&#25915;&#20987;&#26088;&#22312;&#22686;&#21152;&#22312;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#37096;&#32626;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#32791;&#21644;&#35745;&#31639;&#26102;&#38388;&#12290;&#29616;&#26377;&#30340;&#28023;&#32501;&#25915;&#20987;&#21487;&#20197;&#36890;&#36807;&#28023;&#32501;&#31034;&#20363;&#36827;&#34892;&#25512;&#29702;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#28023;&#32501;&#20013;&#27602;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#12290;&#28023;&#32501;&#31034;&#20363;&#21033;&#29992;&#28155;&#21152;&#21040;&#27169;&#22411;&#36755;&#20837;&#30340;&#25200;&#21160;&#26469;&#22686;&#21152;&#33021;&#37327;&#21644;&#24310;&#36831;&#65292;&#32780;&#28023;&#32501;&#20013;&#27602;&#21017;&#25913;&#21464;&#27169;&#22411;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#24341;&#21457;&#25512;&#29702;&#26102;&#30340;&#33021;&#37327;/&#24310;&#36831;&#25928;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28023;&#32501;&#25915;&#20987;&#65292;&#31216;&#20026; SpongeNet&#12290;SpongeNet &#26159;&#31532;&#19968;&#20010;&#30452;&#25509;&#20316;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#30340;&#28023;&#32501;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#28023;&#32501;&#20013;&#27602;&#65292;SpongeNet &#21487;&#20197;&#25104;&#21151;&#22686;&#21152;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#32791;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#19981;&#19987;&#38376;&#38024;&#23545;&#28023;&#32501;&#20013;&#27602;&#36827;&#34892;&#35843;&#25972;&#65288;&#21363;&#20943;&#23567;&#25209;&#24402;&#19968;&#21270;&#20559;&#24046;&#20540;&#65289;&#65292;&#21017;&#27602;&#23475;&#38450;&#24481;&#20250;&#22833;&#25928;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26174;&#31034;&#20986;&#28023;&#32501;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sponge attacks aim to increase the energy consumption and computation time of neural networks deployed on hardware accelerators. Existing sponge attacks can be performed during inference via sponge examples or during training via Sponge Poisoning. Sponge examples leverage perturbations added to the model's input to increase energy and latency, while Sponge Poisoning alters the objective function of a model to induce inference-time energy/latency effects.   In this work, we propose a novel sponge attack called SpongeNet. SpongeNet is the first sponge attack that is performed directly on the parameters of a pre-trained model. Our experiments show that SpongeNet can successfully increase the energy consumption of vision models with fewer samples required than Sponge Poisoning. Our experiments indicate that poisoning defenses are ineffective if not adjusted specifically for the defense against Sponge Poisoning (i.e., they decrease batch normalization bias values). Our work shows that Spong
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#30340;LLMs&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#26469;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05140</link><description>&lt;p&gt;
Tag-LLM: &#23558;&#36890;&#29992;&#30340;LLM&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#20877;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#30340;LLMs&#24212;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#26469;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#19987;&#38376;&#39046;&#22495;&#20013;&#65292;&#22914;&#29289;&#29702;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#26410;&#20805;&#20998;&#28085;&#30422;&#30340;&#39046;&#22495;&#65292;&#23427;&#20204;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#36890;&#29992;LLMs&#37325;&#26032;&#29992;&#20110;&#19987;&#19994;&#39046;&#22495;&#30340;&#26377;&#25928;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#23450;&#20041;&#30340;&#36755;&#20837;&#26631;&#31614;&#65292;&#36825;&#20123;&#26631;&#31614;&#34987;&#21442;&#25968;&#21270;&#20026;&#36830;&#32493;&#21521;&#37327;&#24182;&#38468;&#21152;&#21040;LLMs&#30340;&#23884;&#20837;&#23618;&#65292;&#20197;&#23545;LLMs&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#26631;&#31614;&#65306;&#39046;&#22495;&#26631;&#31614;&#29992;&#20110;&#38480;&#23450;&#19987;&#19994;&#34920;&#31034;&#65288;&#20363;&#22914;&#21270;&#23398;&#24335;&#65289;&#24182;&#25552;&#20379;&#39046;&#22495;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65307;&#21151;&#33021;&#26631;&#31614;&#29992;&#20110;&#34920;&#31034;&#29305;&#23450;&#30340;&#21151;&#33021;&#65288;&#20363;&#22914;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#65289;&#24182;&#21387;&#32553;&#21151;&#33021;&#35299;&#20915;&#25351;&#20196;&#12290;&#25105;&#20204;&#20351;&#29992;&#36741;&#21161;&#25968;&#25454;&#21644;&#39046;&#22495;&#30693;&#35782;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#36825;&#20123;&#26631;&#31614;&#30340;&#21327;&#35758;&#12290;&#36890;&#36807;&#26126;&#30830;&#23558;&#20219;&#21153;&#39046;&#22495;&#19982;&#20219;&#21153;&#21151;&#33021;&#20998;&#31163;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable proficiency in understanding and generating natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into effective task solvers for specialized domains. We introduce a novel, model-agnostic framework for learning custom input tags, which are parameterized as continuous vectors appended to the LLM's embedding layer, to condition the LLM. We design two types of input tags: domain tags are used to delimit specialized representations (e.g., chemical formulas) and provide domain-relevant context; function tags are used to represent specific functions (e.g., predicting molecular properties) and compress function-solving instructions. We develop a three-stage protocol to learn these tags using auxiliary data and domain knowledge. By explicitly disentangling task domains from tas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#20998;&#25955;&#21306;&#22359;&#38142;&#25216;&#26415;&#19982;&#26032;&#39062;&#26426;&#21046;&#32467;&#21512;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#30830;&#20445;&#35802;&#23454;&#21442;&#19982;&#32773;&#33719;&#24471;&#30340;&#32047;&#31215;&#22870;&#21169;&#65292;&#24182;&#24212;&#23545;&#24694;&#24847;&#34892;&#20026;&#21644;&#20445;&#25252;&#21442;&#19982;&#32773;&#38544;&#31169;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.04417</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25955;&#21306;&#22359;&#38142;&#30340;&#31283;&#20581;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Decentralized Blockchain-based Robust Multi-agent Multi-armed Bandit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#20998;&#25955;&#21306;&#22359;&#38142;&#25216;&#26415;&#19982;&#26032;&#39062;&#26426;&#21046;&#32467;&#21512;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#30830;&#20445;&#35802;&#23454;&#21442;&#19982;&#32773;&#33719;&#24471;&#30340;&#32047;&#31215;&#22870;&#21169;&#65292;&#24182;&#24212;&#23545;&#24694;&#24847;&#34892;&#20026;&#21644;&#20445;&#25252;&#21442;&#19982;&#32773;&#38544;&#31169;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#22810;&#20010;&#23458;&#25143;&#25110;&#21442;&#19982;&#32773;&#20998;&#24067;&#22312;&#19968;&#20010;&#23436;&#20840;&#20998;&#25955;&#30340;&#21306;&#22359;&#38142;&#19978;&#65292;&#20854;&#20013;&#19968;&#20123;&#21487;&#33021;&#26159;&#24694;&#24847;&#30340;&#12290;&#33218;&#30340;&#22870;&#21169;&#22312;&#21442;&#19982;&#32773;&#20043;&#38388;&#26159;&#22343;&#21248;&#30340;&#65292;&#36981;&#24490;&#26102;&#38388;&#19981;&#21464;&#30340;&#38543;&#26426;&#20998;&#24067;&#65292;&#21482;&#26377;&#22312;&#31995;&#32479;&#36275;&#22815;&#23433;&#20840;&#26102;&#25165;&#21521;&#21442;&#19982;&#32773;&#36879;&#38706;&#12290;&#31995;&#32479;&#30340;&#30446;&#26631;&#26159;&#26377;&#25928;&#22320;&#30830;&#20445;&#35802;&#23454;&#21442;&#19982;&#32773;&#33719;&#24471;&#30340;&#32047;&#31215;&#22870;&#21169;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23558;&#21306;&#22359;&#38142;&#30340;&#20808;&#36827;&#25216;&#26415;&#21644;&#26032;&#39062;&#30340;&#26426;&#21046;&#32467;&#21512;&#21040;&#31995;&#32479;&#20013;&#65292;&#20026;&#35802;&#23454;&#21442;&#19982;&#32773;&#35774;&#35745;&#26368;&#20339;&#31574;&#30053;&#12290;&#36825;&#26679;&#21487;&#20197;&#24212;&#23545;&#21508;&#31181;&#24694;&#24847;&#34892;&#20026;&#24182;&#20445;&#25252;&#21442;&#19982;&#32773;&#30340;&#38544;&#31169;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#20102;&#19968;&#32452;&#21487;&#20197;&#35775;&#38382;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#39564;&#35777;&#32773;&#27744;&#65292;&#20026;&#36825;&#20123;&#39564;&#35777;&#32773;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#31614;&#21517;&#30340;&#20840;&#26032;&#20849;&#35782;&#26426;&#21046;&#65292;&#24182;&#21457;&#26126;&#20102;&#19968;&#31181;&#22522;&#20110;UCB&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a robust multi-agent multi-armed bandit problem where multiple clients or participants are distributed on a fully decentralized blockchain, with the possibility of some being malicious. The rewards of arms are homogeneous among the clients, following time-invariant stochastic distributions that are revealed to the participants only when the system is secure enough. The system's objective is to efficiently ensure the cumulative rewards gained by the honest participants. To this end and to the best of our knowledge, we are the first to incorporate advanced techniques from blockchains, as well as novel mechanisms, into the system to design optimal strategies for honest participants. This allows various malicious behaviors and the maintenance of participant privacy. More specifically, we randomly select a pool of validators who have access to all participants, design a brand-new consensus mechanism based on digital signatures for these validators, invent a UCB-based strategy that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#24494;&#32467;&#26500;&#26144;&#23556;&#12289;&#32420;&#32500;&#26463;&#25551;&#35760;&#12289;&#30333;&#36136;&#32420;&#32500;&#26463;&#20998;&#26512;&#20197;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#24635;&#32467;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00019</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion MRI with Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#24494;&#32467;&#26500;&#26144;&#23556;&#12289;&#32420;&#32500;&#26463;&#25551;&#35760;&#12289;&#30333;&#36136;&#32420;&#32500;&#26463;&#20998;&#26512;&#20197;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#24635;&#32467;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24357;&#25955;&#21152;&#26435;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#20855;&#26377;&#38750;&#20405;&#20837;&#24615;&#35780;&#20272;&#22823;&#33041;&#24494;&#32467;&#26500;&#21644;&#32467;&#26500;&#36830;&#25509;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;dMRI&#25968;&#25454;&#20197;&#25552;&#21462;&#20020;&#24202;&#21644;&#31185;&#23398;&#30446;&#30340;&#30340;&#26377;&#29992;&#20449;&#24687;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290; dMRI&#27979;&#37327;&#36890;&#24120;&#21463;&#21040;&#24378;&#22122;&#22768;&#21644;&#20266;&#24433;&#30340;&#24178;&#25200;&#65292;&#25968;&#25454;&#20013;&#36890;&#24120;&#23384;&#22312;&#39640;&#30340;&#20250;&#35805;&#38388;&#21644;&#25195;&#25551;&#32773;&#38388;&#24322;&#36136;&#24615;&#65292;&#20197;&#21450;&#22823;&#33041;&#32467;&#26500;&#30340;&#30456;&#24403;&#22823;&#30340;&#20010;&#20307;&#38388;&#21464;&#24322;&#65292;&#24182;&#19988;&#27979;&#37327;&#21644;&#24863;&#20852;&#36259;&#29616;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#21487;&#33021;&#38750;&#24120;&#22797;&#26434;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;dMRI&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#36825;&#20123;&#23581;&#35797;&#65292;&#37325;&#28857;&#20851;&#27880;&#24050;&#32463;&#35299;&#20915;&#20102;&#24494;&#32467;&#26500;&#26144;&#23556;&#12289;&#32420;&#32500;&#26463;&#25551;&#35760;&#12289;&#30333;&#36136;&#32420;&#32500;&#26463;&#20998;&#26512;&#20197;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20027;&#35201;&#21457;&#29616;&#12289;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-weighted magnetic resonance imaging (dMRI) offers unique capabilities such as noninvasive assessment of brain's micro-structure and structural connectivity. However, analyzing the dMRI data to extract useful information for clinical and scientific purposes is challenging. The dMRI measurements often suffer from strong noise and artifacts, there is usually high inter-session and inter-scanner heterogeneity in the data and considerable inter-subject variability in brain structure, and the relationship between measurements and the phenomena of interest can be highly complex. Recent years have witnessed increasing use of machine learning methods for dMRI analysis. This manuscript aims to assess these efforts, with a focus on methods that have addressed micro-structure mapping, tractography, white matter tract analysis, as well as data preprocessing and harmonization. We summarize the main findings, strengths, and weaknesses of the existing methods and suggest topics for future re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#22810;&#32423;&#22810;&#27169;&#24577;&#23545;&#40784;&#65288;K-M3AID&#65289;&#65292;&#36890;&#36807;&#22312;&#20998;&#23376;&#22270;&#21644;NMR&#20809;&#35889;&#20043;&#38388;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#37319;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;&#23454;&#20363;&#32423;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#20998;&#23376;&#26816;&#32034;&#12289;&#24322;&#26500;&#20307;&#35782;&#21035;&#21644;&#23792;&#24402;&#23646;&#31561;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2311.13817</link><description>&lt;p&gt;
&#20998;&#23376;&#37492;&#23450;&#19982;&#23792;&#24402;&#23646;&#65306;&#22312;NMR&#19978;&#21033;&#29992;&#22810;&#32423;&#22810;&#27169;&#24577;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Molecular Identification and Peak Assignment: Leveraging Multi-Level Multimodal Alignment on NMR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#22810;&#32423;&#22810;&#27169;&#24577;&#23545;&#40784;&#65288;K-M3AID&#65289;&#65292;&#36890;&#36807;&#22312;&#20998;&#23376;&#22270;&#21644;NMR&#20809;&#35889;&#20043;&#38388;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#37319;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;&#23454;&#20363;&#32423;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#20998;&#23376;&#26816;&#32034;&#12289;&#24322;&#26500;&#20307;&#35782;&#21035;&#21644;&#23792;&#24402;&#23646;&#31561;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13817v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#26680;&#30913;&#20849;&#25391;&#65288;NMR&#65289;&#20809;&#35889;&#22312;&#35299;&#35835;&#20998;&#23376;&#32467;&#26500;&#21644;&#21160;&#24577;&#34892;&#20026;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#34429;&#28982;&#22522;&#20110;AI&#30340;NMR&#39044;&#27979;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;&#20998;&#23376;&#26816;&#32034;&#12289;&#24322;&#26500;&#20307;&#35782;&#21035;&#21644;&#23792;&#24402;&#23646;&#31561;&#20219;&#21153;&#20013;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20855;&#26377;&#30693;&#35782;&#24341;&#23548;&#30340;&#23454;&#20363;&#32423;&#23545;&#40784;&#30340;&#22810;&#32423;&#22810;&#27169;&#24577;&#23545;&#40784;&#65288;K-M3AID&#65289;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#22312;&#20004;&#31181;&#24322;&#36136;&#27169;&#24577;&#20043;&#38388;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65306;&#20998;&#23376;&#22270;&#21644;NMR&#20809;&#35889;&#12290;K-M3AID&#37319;&#29992;&#20102;&#19968;&#20010;&#21452;&#21327;&#35843;&#23545;&#27604;&#23398;&#20064;&#26550;&#26500;&#65292;&#21253;&#21547;&#19977;&#20010;&#20851;&#38190;&#27169;&#22359;&#65306;&#22270;&#32423;&#23545;&#40784;&#27169;&#22359;&#12289;&#33410;&#28857;&#32423;&#23545;&#40784;&#27169;&#22359;&#21644;&#36890;&#20449;&#36890;&#36947;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#33410;&#28857;&#32423;&#23545;&#40784;&#27169;&#22359;&#20013;&#65292;K-M3AID&#24341;&#20837;&#20102;&#30693;&#35782;&#24341;&#23548;&#30340;&#23454;&#20363;&#32423;&#23545;&#27604;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;K-M3AID&#34920;&#26126;&#22312;&#33410;&#28857;&#32423;&#23545;&#40784;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13817v2 Announce Type: replace  Abstract: Nuclear magnetic resonance (NMR) spectroscopy plays an essential role in deciphering molecular structure and dynamic behaviors. While AI-enhanced NMR prediction models hold promise, challenges still persist in tasks such as molecular retrieval, isomer recognition, and peak assignment. In response, this paper introduces a novel solution, Multi-Level Multimodal Alignment with Knowledge-Guided Instance-Wise Discrimination (K-M3AID), which establishes correspondences between two heterogeneous modalities: molecular graphs and NMR spectra. K-M3AID employs a dual-coordinated contrastive learning architecture with three key modules: a graph-level alignment module, a node-level alignment module, and a communication channel. Notably, K-M3AID introduces knowledge-guided instance-wise discrimination into contrastive learning within the node-level alignment module. In addition, K-M3AID demonstrates that skills acquired during node-level alignment
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MulCo&#65306;&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#29992;&#20110;&#20840;&#38754;&#25552;&#39640;&#25152;&#26377;&#31867;&#22411;&#26102;&#38388;&#25968;&#25454;&#38598;&#24615;&#33021;</title><link>https://arxiv.org/abs/2209.00568</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#29992;&#20110;&#20107;&#20214;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Contrastive Knowledge Co-Distillation for Event Temporal Relation Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.00568
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MulCo&#65306;&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#29992;&#20110;&#20840;&#38754;&#25552;&#39640;&#25152;&#26377;&#31867;&#22411;&#26102;&#38388;&#25968;&#25454;&#38598;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;&#65288;ETRE&#65289;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20107;&#20214;&#23545;&#20301;&#20110;&#19981;&#21516;&#36317;&#31163;&#30340;&#35805;&#35821;&#20013;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25509;&#36817;&#24615;&#24102;&#12290;&#20851;&#20110;&#20301;&#20110;&#26356;&#36828;&#65288;&#21363;&#8220;&#38271;&#8221;&#65289;&#25110;&#26356;&#36817;&#65288;&#21363;&#8220;&#30701;&#8221;&#65289;&#25509;&#36817;&#24615;&#24102;&#30340;&#20107;&#20214;&#23545;&#30340;&#26102;&#38388;&#39034;&#24207;&#20256;&#36798;&#26041;&#24335;&#19981;&#21516;&#12290;&#30446;&#21069;ETRE&#27169;&#22411;&#24448;&#24448;&#22312;&#20301;&#20110;&#30701;&#25110;&#38271;&#25509;&#36817;&#24615;&#24102;&#30340;&#20107;&#20214;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#19981;&#33021;&#21516;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#28982;&#25991;&#26412;&#21253;&#21547;&#25152;&#26377;&#31867;&#22411;&#30340;&#26102;&#38388;&#20107;&#20214;&#23545;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MulCo&#65306;&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#65292;&#36825;&#26159;&#19968;&#31181;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#22810;&#20010;&#20107;&#20214;&#23545;&#25509;&#36817;&#24615;&#24102;&#20849;&#20139;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#23545;&#25152;&#26377;&#31867;&#22411;&#26102;&#38388;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MulCo&#25104;&#21151;&#22320;&#25972;&#21512;&#20102;&#36328;&#30701;&#21644;&#38271;&#25509;&#36817;&#24615;&#24102;&#30340;&#19982;&#26102;&#38388;&#25512;&#29702;&#30456;&#20851;&#30340;&#35821;&#35328;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.00568v2 Announce Type: replace-cross  Abstract: Event Temporal Relation Extraction (ETRE) is a crucial yet challenging problem. Event pairs are situated within a discourse at different distances, which we refer to as proximity bands. The temporal ordering communicated about event pairs situated at more remote (i.e., ``long'') or less remote (i.e., ``short'') proximity bands is encoded differently. SOTA ETRE models have tended to perform well on events situated at either short or long proximity bands, but not both. Yet, real-world, natural texts contain all types of temporal event-pairs. In this paper, we present MulCo: Multi-Scale Contrastive Knowledge Co-Distillation, a fusion approach that shares knowledge across multiple event pair proximity bands in order to improve performance on all types of temporal datasets. Our experimental results show that MulCo successfully integrates linguistic cues pertaining to temporal reasoning across both short and long proximity bands and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#37325;&#24615;&#25233;&#37057;&#38556;&#30861;(MDD)&#20013;&#30340;&#22870;&#21169;&#22788;&#29702;&#24322;&#24120;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21644;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#32467;&#21512;&#65292;&#25506;&#32034;&#20915;&#31574;&#21046;&#23450;&#36807;&#31243;&#20013;&#30340;&#23398;&#20064;&#31574;&#30053;&#21160;&#24577;&#23545;&#20010;&#20307;&#22870;&#21169;&#23398;&#20064;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13929</link><description>&lt;p&gt;
&#20351;&#29992;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#21457;&#29616;&#20915;&#31574;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Hidden Markov Models for Discovering Decision-Making Dynamics. (arXiv:2401.13929v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#37325;&#24615;&#25233;&#37057;&#38556;&#30861;(MDD)&#20013;&#30340;&#22870;&#21169;&#22788;&#29702;&#24322;&#24120;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21644;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#32467;&#21512;&#65292;&#25506;&#32034;&#20915;&#31574;&#21046;&#23450;&#36807;&#31243;&#20013;&#30340;&#23398;&#20064;&#31574;&#30053;&#21160;&#24577;&#23545;&#20010;&#20307;&#22870;&#21169;&#23398;&#20064;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#24322;&#36136;&#24615;&#65292;&#37325;&#24615;&#25233;&#37057;&#38556;&#30861;(MDD)&#22312;&#35786;&#26029;&#21644;&#27835;&#30103;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#26032;&#30340;&#35777;&#25454;&#34920;&#26126;&#22870;&#21169;&#22788;&#29702;&#24322;&#24120;&#21487;&#33021;&#20316;&#20026;MDD&#30340;&#34892;&#20026;&#26631;&#35760;&#12290;&#20026;&#20102;&#34913;&#37327;&#22870;&#21169;&#22788;&#29702;&#65292;&#24739;&#32773;&#25191;&#34892;&#28041;&#21450;&#20570;&#20986;&#36873;&#25321;&#25110;&#23545;&#19982;&#19981;&#21516;&#32467;&#26524;&#30456;&#20851;&#32852;&#30340;&#21050;&#28608;&#20316;&#20986;&#21453;&#24212;&#30340;&#22522;&#20110;&#35745;&#31639;&#26426;&#30340;&#34892;&#20026;&#20219;&#21153;&#12290;&#24378;&#21270;&#23398;&#20064;(RL)&#27169;&#22411;&#34987;&#25311;&#21512;&#20197;&#25552;&#21462;&#34913;&#37327;&#22870;&#21169;&#22788;&#29702;&#21508;&#20010;&#26041;&#38754;&#30340;&#21442;&#25968;&#65292;&#20197;&#34920;&#24449;&#24739;&#32773;&#22312;&#34892;&#20026;&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#26041;&#24335;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20165;&#22522;&#20110;&#21333;&#20010;RL&#27169;&#22411;&#30340;&#22870;&#21169;&#23398;&#20064;&#34920;&#24449;&#19981;&#36275;; &#30456;&#21453;&#65292;&#20915;&#31574;&#36807;&#31243;&#20013;&#21487;&#33021;&#23384;&#22312;&#22810;&#31181;&#31574;&#30053;&#20043;&#38388;&#30340;&#20999;&#25442;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#31185;&#23398;&#38382;&#39064;&#26159;&#20915;&#31574;&#21046;&#23450;&#20013;&#23398;&#20064;&#31574;&#30053;&#30340;&#21160;&#24577;&#22914;&#20309;&#24433;&#21709;MDD&#24739;&#32773;&#30340;&#22870;&#21169;&#23398;&#20064;&#33021;&#21147;&#12290;&#30001;&#27010;&#29575;&#22870;&#21169;&#20219;&#21153;(PRT)&#25152;&#21551;&#21457;
&lt;/p&gt;
&lt;p&gt;
Major depressive disorder (MDD) presents challenges in diagnosis and treatment due to its complex and heterogeneous nature. Emerging evidence indicates that reward processing abnormalities may serve as a behavioral marker for MDD. To measure reward processing, patients perform computer-based behavioral tasks that involve making choices or responding to stimulants that are associated with different outcomes. Reinforcement learning (RL) models are fitted to extract parameters that measure various aspects of reward processing to characterize how patients make decisions in behavioral tasks. Recent findings suggest the inadequacy of characterizing reward learning solely based on a single RL model; instead, there may be a switching of decision-making processes between multiple strategies. An important scientific question is how the dynamics of learning strategies in decision-making affect the reward learning ability of individuals with MDD. Motivated by the probabilistic reward task (PRT) wi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#30340;&#27010;&#24565;&#65288;CATE&#65289;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#20219;&#21153;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#65292;&#20294;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#35813;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;</title><link>http://arxiv.org/abs/2401.10805</link><description>&lt;p&gt;
&#23398;&#20064;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Learning to Visually Connect Actions and their Effects. (arXiv:2401.10805v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10805
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#30340;&#27010;&#24565;&#65288;CATE&#65289;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#20219;&#21153;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#65292;&#20294;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#35813;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#65288;CATE&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;CATE&#21487;&#20197;&#22312;&#20219;&#21153;&#35268;&#21010;&#21644;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#31561;&#39046;&#22495;&#20013;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#22522;&#20110;CATE&#30340;&#20219;&#21153;&#24418;&#24335;&#65292;&#22914;&#21160;&#20316;&#36873;&#25321;&#21644;&#21160;&#20316;&#25351;&#23450;&#65292;&#20854;&#20013;&#35270;&#39057;&#29702;&#35299;&#27169;&#22411;&#20197;&#35821;&#20041;&#21644;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#36830;&#25509;&#21160;&#20316;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#29992;&#20110;&#21160;&#20316;&#36873;&#25321;&#21644;&#21160;&#20316;&#25351;&#23450;&#12290;&#23613;&#31649;&#20219;&#21153;&#20855;&#26377;&#30452;&#35266;&#24615;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#22256;&#38590;&#37325;&#37325;&#65292;&#20154;&#31867;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#22522;&#30784;&#65292;&#23637;&#31034;&#20102;&#36830;&#25509;&#35270;&#39057;&#29702;&#35299;&#20013;&#21160;&#20316;&#21644;&#25928;&#26524;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce the novel concept of visually Connecting Actions and Their Effects (CATE) in video understanding. CATE can have applications in areas like task planning and learning from demonstration. We propose different CATE-based task formulations, such as action selection and action specification, where video understanding models connect actions and effects at semantic and fine-grained levels. We observe that different formulations produce representations capturing intuitive action properties. We also design various baseline models for action selection and action specification. Despite the intuitive nature of the task, we observe that models struggle, and humans outperform them by a large margin. The study aims to establish a foundation for future efforts, showcasing the flexibility and versatility of connecting actions and effects in video understanding, with the hope of inspiring advanced formulations and models.
&lt;/p&gt;</description></item><item><title>&#26412;&#27425;&#35843;&#26597;&#24635;&#32467;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#22312;&#26234;&#33021;&#21046;&#36896;&#21644;&#20135;&#19994;4.0&#32972;&#26223;&#19979;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#31361;&#20986;&#20102;&#20854;&#22312;&#24037;&#19994;&#29983;&#20135;&#21644;&#21046;&#36896;&#39046;&#22495;&#30340;&#20851;&#38190;&#20248;&#21183;&#21644;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;TDA&#26041;&#27861;&#21644;&#24050;&#35782;&#21035;&#30340;&#24212;&#29992;&#31867;&#22411;&#65292;&#20197;&#25512;&#21160;&#26356;&#22810;&#30340;&#30456;&#20851;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.09319</link><description>&lt;p&gt;
&#26234;&#33021;&#21046;&#36896;&#36807;&#31243;&#20013;&#30340;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;--&#20851;&#20110;&#29616;&#29366;&#30340;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Topological Data Analysis in smart manufacturing processes -- A survey on the state of the art. (arXiv:2310.09319v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#27425;&#35843;&#26597;&#24635;&#32467;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#22312;&#26234;&#33021;&#21046;&#36896;&#21644;&#20135;&#19994;4.0&#32972;&#26223;&#19979;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#31361;&#20986;&#20102;&#20854;&#22312;&#24037;&#19994;&#29983;&#20135;&#21644;&#21046;&#36896;&#39046;&#22495;&#30340;&#20851;&#38190;&#20248;&#21183;&#21644;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;TDA&#26041;&#27861;&#21644;&#24050;&#35782;&#21035;&#30340;&#24212;&#29992;&#31867;&#22411;&#65292;&#20197;&#25512;&#21160;&#26356;&#22810;&#30340;&#30456;&#20851;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#26159;&#19968;&#31181;&#20351;&#29992;&#25299;&#25169;&#23398;&#25216;&#26415;&#23545;&#22797;&#26434;&#30340;&#22810;&#32500;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#30340;&#25968;&#23398;&#26041;&#27861;&#65292;&#24050;&#32463;&#22312;&#21307;&#23398;&#12289;&#26448;&#26009;&#31185;&#23398;&#12289;&#29983;&#29289;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#34987;&#24191;&#27867;&#32780;&#25104;&#21151;&#22320;&#24212;&#29992;&#12290;&#26412;&#35843;&#26597;&#24635;&#32467;&#20102;TDA&#22312;&#21478;&#19968;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65306;&#24037;&#19994;&#21046;&#36896;&#21644;&#20135;&#19994;4.0&#32972;&#26223;&#19979;&#30340;&#29983;&#20135;&#12290;&#25105;&#20204;&#23545;&#24037;&#19994;&#29983;&#20135;&#21644;&#21046;&#36896;&#39046;&#22495;&#20013;TDA&#24212;&#29992;&#36827;&#34892;&#20102;&#20005;&#35880;&#21487;&#37325;&#22797;&#30340;&#25991;&#29486;&#25628;&#32034;&#12290;&#36890;&#36807;&#23545;&#32467;&#26524;&#36827;&#34892;&#32858;&#31867;&#21644;&#20998;&#26512;&#65292;&#22522;&#20110;&#20854;&#22312;&#21046;&#36896;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#39046;&#22495;&#21644;&#36755;&#20837;&#25968;&#25454;&#31867;&#22411;&#36827;&#34892;&#35770;&#36848;&#12290;&#25105;&#20204;&#31361;&#20986;&#20102;TDA&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#20851;&#38190;&#20248;&#21183;&#21450;&#20854;&#24037;&#20855;&#65292;&#24182;&#25551;&#36848;&#20102;&#23427;&#30340;&#25361;&#25112;&#20197;&#21450;&#26410;&#26469;&#30340;&#28508;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#65288;&#29305;&#23450;&#39046;&#22495;&#30340;&#65289;&#24037;&#19994;&#20013;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;TDA&#26041;&#27861;&#21644;&#24050;&#35782;&#21035;&#30340;&#24212;&#29992;&#31867;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#26356;&#22810;&#30340;&#30740;&#31350;&#22312;&#24403;&#21069;&#39046;&#22495;&#20013;&#30340;&#24320;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topological Data Analysis (TDA) is a mathematical method using techniques from topology for the analysis of complex, multi-dimensional data that has been widely and successfully applied in several fields such as medicine, material science, biology, and others. This survey summarizes the state of the art of TDA in yet another application area: industrial manufacturing and production in the context of Industry 4.0. We perform a rigorous and reproducible literature search of applications of TDA on the setting of industrial production and manufacturing. The resulting works are clustered and analyzed based on their application area within the manufacturing process and their input data type. We highlight the key benefits of TDA and their tools in this area and describe its challenges, as well as future potential. Finally, we discuss which TDA methods are underutilized in (the specific area of) industry and the identified types of application, with the goal of prompting more research in this 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;2D&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20165;&#26377;2D&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#20986;3D&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#35813;&#20449;&#24687;&#36827;&#34892;3D&#35270;&#35273;&#20219;&#21153;&#12290;&#36890;&#36807;&#35266;&#28857;&#31070;&#32463;&#25991;&#26412;&#20498;&#32622;&#65288;ViewNeTI&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#20013;&#23545;&#35937;&#30340;3D&#35270;&#28857;&#65292;&#26377;&#25928;&#35299;&#20915;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#35270;&#22270;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#35821;&#20041;&#32454;&#33410;&#21644;&#36924;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.07986</link><description>&lt;p&gt;
&#35266;&#28857;&#25991;&#26412;&#20498;&#32622;&#65306;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;2D&#25193;&#25955;&#27169;&#22411;&#37322;&#25918;&#26032;&#39062;&#30340;&#35270;&#22270;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Viewpoint Textual Inversion: Unleashing Novel View Synthesis with Pretrained 2D Diffusion Models. (arXiv:2309.07986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;2D&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20165;&#26377;2D&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#20986;3D&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#35813;&#20449;&#24687;&#36827;&#34892;3D&#35270;&#35273;&#20219;&#21153;&#12290;&#36890;&#36807;&#35266;&#28857;&#31070;&#32463;&#25991;&#26412;&#20498;&#32622;&#65288;ViewNeTI&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#20013;&#23545;&#35937;&#30340;3D&#35270;&#28857;&#65292;&#26377;&#25928;&#35299;&#20915;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#21333;&#35270;&#22270;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#35821;&#20041;&#32454;&#33410;&#21644;&#36924;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#29702;&#35299;&#23545;&#35937;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#20165;&#36890;&#36807;2D&#30417;&#30563;&#26469;&#34920;&#31034;&#19990;&#30028;&#30340;&#30495;&#23454;3D&#32467;&#26500;&#65311;&#25105;&#20204;&#35777;&#26126;&#65292;&#26159;&#30340;&#65292;3D&#30693;&#35782;&#34987;&#32534;&#30721;&#22312;2D&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;&#22914;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65289;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#32467;&#26500;&#21487;&#20197;&#29992;&#20110;3D&#35270;&#35273;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35266;&#28857;&#31070;&#32463;&#25991;&#26412;&#20498;&#32622;&#65288;ViewNeTI&#65289;&#65292;&#21487;&#20197;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#20013;&#23545;&#35937;&#30340;3D&#35270;&#28857;&#12290;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#23567;&#22411;&#31070;&#32463;&#26144;&#23556;&#22120;&#65292;&#29992;&#20110;&#33719;&#21462;&#30456;&#26426;&#35270;&#28857;&#21442;&#25968;&#24182;&#39044;&#27979;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#21521;&#37327;&#65307;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#28508;&#22312;&#21521;&#37327;&#26469;&#35843;&#25972;&#25193;&#25955;&#29983;&#25104;&#36807;&#31243;&#65292;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#30456;&#26426;&#35270;&#28857;&#30340;&#22270;&#20687;&#12290;ViewNeTI&#33258;&#28982;&#35299;&#20915;&#20102;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#65288;NVS&#65289;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#34987;&#20923;&#32467;&#30340;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#21487;&#20197;&#29992;&#24456;&#23569;&#30340;&#36755;&#20837;&#35270;&#22270;&#26469;&#35299;&#20915;NVS&#38382;&#39064;&#65307;&#25105;&#20204;&#29978;&#33267;&#21487;&#20197;&#36827;&#34892;&#21333;&#35270;&#22270;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#21333;&#35270;&#22270;NVS&#39044;&#27979;&#20855;&#26377;&#33391;&#22909;&#30340;&#35821;&#20041;&#32454;&#33410;&#21644;&#36924;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models understand spatial relationship between objects, but do they represent the true 3D structure of the world from only 2D supervision? We demonstrate that yes, 3D knowledge is encoded in 2D image diffusion models like Stable Diffusion, and we show that this structure can be exploited for 3D vision tasks. Our method, Viewpoint Neural Textual Inversion (ViewNeTI), controls the 3D viewpoint of objects in generated images from frozen diffusion models. We train a small neural mapper to take camera viewpoint parameters and predict text encoder latents; the latents then condition the diffusion generation process to produce images with the desired camera viewpoint.  ViewNeTI naturally addresses Novel View Synthesis (NVS). By leveraging the frozen diffusion model as a prior, we can solve NVS with very few input views; we can even do single-view novel view synthesis. Our single-view NVS predictions have good semantic details and photorealism compared to prior methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CBRNet&#65292;&#19968;&#31181;&#36890;&#36807;&#34920;&#31034;&#24179;&#34913;&#23398;&#20064;&#36830;&#32493;&#20540;&#27835;&#30103;&#25928;&#26524;&#30340;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03731</link><description>&lt;p&gt;
&#36890;&#36807;&#34920;&#31034;&#24179;&#34913;&#23398;&#20064;&#36830;&#32493;&#20540;&#27835;&#30103;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Learning continuous-valued treatment effects through representation balancing. (arXiv:2309.03731v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CBRNet&#65292;&#19968;&#31181;&#36890;&#36807;&#34920;&#31034;&#24179;&#34913;&#23398;&#20064;&#36830;&#32493;&#20540;&#27835;&#30103;&#25928;&#26524;&#30340;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#12289;&#21830;&#19994;&#12289;&#32463;&#27982;&#31561;&#39046;&#22495;&#65292;&#20272;&#35745;&#19982;&#27835;&#30103;&#21058;&#37327;&#30456;&#20851;&#30340;&#27835;&#30103;&#25928;&#26524;&#65288;&#21363;&#8220;&#21058;&#37327;&#21453;&#24212;&#8221;&#65289;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36830;&#32493;&#20540;&#27835;&#30103;&#25928;&#26524;&#36890;&#24120;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#24471;&#21040;&#30340;&#65292;&#32780;&#35266;&#27979;&#25968;&#25454;&#21487;&#33021;&#23384;&#22312;&#21058;&#37327;&#36873;&#25321;&#20559;&#24046;&#65292;&#21363;&#21058;&#37327;&#20998;&#37197;&#21463;&#21040;&#39044;&#22788;&#29702;&#21327;&#21464;&#37327;&#30340;&#24433;&#21709;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#23384;&#22312;&#21058;&#37327;&#36873;&#25321;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#20934;&#30830;&#23398;&#20064;&#21040;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CBRNet&#30340;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#20010;&#20307;&#30340;&#21058;&#37327;&#21453;&#24212;&#12290;CBRNet&#37319;&#29992;&#20102;Neyman-Rubin&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#65292;&#24182;&#25193;&#23637;&#20102;&#24179;&#34913;&#34920;&#31034;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#20197;&#20811;&#26381;&#36830;&#32493;&#20540;&#27835;&#30103;&#20013;&#30340;&#36873;&#25321;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#22312;&#36830;&#32493;&#20540;&#27835;&#30103;&#20013;&#24212;&#29992;&#34920;&#31034;&#24179;&#34913;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the effects of treatments with an associated dose on an instance's outcome, the "dose response", is relevant in a variety of domains, from healthcare to business, economics, and beyond. Such effects, also known as continuous-valued treatment effects, are typically estimated from observational data, which may be subject to dose selection bias. This means that the allocation of doses depends on pre-treatment covariates. Previous studies have shown that conventional machine learning approaches fail to learn accurate individual estimates of dose responses under the presence of dose selection bias. In this work, we propose CBRNet, a causal machine learning approach to estimate an individual dose response from observational data. CBRNet adopts the Neyman-Rubin potential outcome framework and extends the concept of balanced representation learning for overcoming selection bias to continuous-valued treatments. Our work is the first to apply representation balancing in a continuous-v
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#39044;&#27979;&#38899;&#20048;&#36716;&#25442;&#22120;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#36827;&#34892;&#25511;&#21046;&#65292;&#21253;&#25324;&#34917;&#20840;&#25511;&#21046;&#20219;&#21153;&#21644;&#20276;&#22863;&#65292;&#24182;&#19988;&#22312;&#22823;&#22411;&#19988;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2306.08620</link><description>&lt;p&gt;
&#39044;&#27979;&#38899;&#20048;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Anticipatory Music Transformer. (arXiv:2306.08620v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08620
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#39044;&#27979;&#38899;&#20048;&#36716;&#25442;&#22120;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#36827;&#34892;&#25511;&#21046;&#65292;&#21253;&#25324;&#34917;&#20840;&#25511;&#21046;&#20219;&#21153;&#21644;&#20276;&#22863;&#65292;&#24182;&#19988;&#22312;&#22823;&#22411;&#19988;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;anticipation&#65288;&#39044;&#27979;&#65289;&#65306;&#19968;&#31181;&#26500;&#24314;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#20107;&#20214;&#36807;&#31243;&#65288;&#26102;&#38388;&#28857;&#36807;&#31243;&#65289;&#30340;&#23454;&#29616;&#65292;&#20197;&#24322;&#27493;&#22320;&#25511;&#21046;&#19982;&#31532;&#20108;&#20010;&#30456;&#20851;&#36807;&#31243;&#65288;&#25511;&#21046;&#36807;&#31243;&#65289;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20132;&#38169;&#20107;&#20214;&#21644;&#25511;&#20214;&#24207;&#21015;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20351;&#25511;&#20214;&#20986;&#29616;&#22312;&#20107;&#20214;&#24207;&#21015;&#30340;&#20572;&#27490;&#26102;&#38388;&#20043;&#21518;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#21160;&#26426;&#26469;&#33258;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#25511;&#21046;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;infiling&#65288;&#34917;&#20840;&#65289;&#25511;&#21046;&#20219;&#21153;&#65292;&#20854;&#20013;&#25511;&#21046;&#20107;&#20214;&#26159;&#20107;&#20214;&#26412;&#36523;&#30340;&#23376;&#38598;&#65292;&#24182;&#19988;&#26465;&#20214;&#29983;&#25104;&#23436;&#25104;&#32473;&#23450;&#22266;&#23450;&#25511;&#21046;&#20107;&#20214;&#30340;&#20107;&#20214;&#24207;&#21015;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#22810;&#26679;&#30340;Lakh MIDI&#38899;&#20048;&#25968;&#25454;&#38598;&#35757;&#32451;&#39044;&#27979;infiling&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#19982;&#25552;&#31034;&#38899;&#20048;&#29983;&#25104;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#24615;&#33021;&#30456;&#24403;&#65292;&#24182;&#20855;&#26377;&#25191;&#34892;infilling&#25511;&#21046;&#20219;&#21153;&#30340;&#38468;&#21152;&#33021;&#21147;&#65292;&#21253;&#25324;&#20276;&#22863;&#12290;&#20154;&#24037;&#35780;&#20272;&#21592;&#25253;&#21578;&#35828;&#65292;&#39044;&#27979;&#27169;&#22411;&#20135;&#29983;&#30340;&#20276;&#22863;&#20855;&#26377;&#39640;&#21487;&#36776;&#24615;&#21644;&#20248;&#32654;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce anticipation: a method for constructing a controllable generative model of a temporal point process (the event process) conditioned asynchronously on realizations of a second, correlated process (the control process). We achieve this by interleaving sequences of events and controls, such that controls appear following stopping times in the event sequence. This work is motivated by problems arising in the control of symbolic music generation. We focus on infilling control tasks, whereby the controls are a subset of the events themselves, and conditional generation completes a sequence of events given the fixed control events. We train anticipatory infilling models using the large and diverse Lakh MIDI music dataset. These models match the performance of autoregressive models for prompted music generation, with the additional capability to perform infilling control tasks, including accompaniment. Human evaluators report that an anticipatory model produces accompaniments with
&lt;/p&gt;</description></item></channel></rss>