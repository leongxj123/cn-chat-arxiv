<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#33258;&#30417;&#30563;&#27979;&#36317;&#35823;&#24046;&#26657;&#27491;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#25910;&#38598;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#34920;&#26126;&#24615;&#33021;&#19982;&#26377;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;</title><link>https://arxiv.org/abs/2403.19262</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#25910;&#38598;&#65306;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#33258;&#30417;&#30563;&#27979;&#36317;&#35823;&#24046;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Removing the need for ground truth UWB data collection: self-supervised ranging error correction using deep reinforcement learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19262
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#33258;&#30417;&#30563;&#27979;&#36317;&#35823;&#24046;&#26657;&#27491;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#25910;&#38598;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#34920;&#26126;&#24615;&#33021;&#19982;&#26377;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;&#23450;&#20301;&#21033;&#29992;UWB&#25216;&#26415;&#22240;&#20854;&#21400;&#31859;&#32423;&#20934;&#30830;&#24230;&#28508;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22810;&#24452;&#25928;&#24212;&#21644;&#38750;&#30452;&#23556;&#26465;&#20214;&#23548;&#33268;&#20102;&#22522;&#31449;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#27979;&#36317;&#35823;&#24046;&#12290;&#29616;&#26377;&#30340;&#32531;&#35299;&#36825;&#20123;&#27979;&#36317;&#35823;&#24046;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#25910;&#38598;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#36825;&#20351;&#23427;&#20204;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#19981;&#20999;&#23454;&#38469;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#12290;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23558;&#36890;&#36947;&#33033;&#20914;&#21709;&#24212;&#20316;&#20026;&#29366;&#24577;&#65292;&#24182;&#39044;&#27979;&#26657;&#27491;&#20197;&#20943;&#23567;&#26657;&#27491;&#21644;&#20272;&#35745;&#27979;&#36317;&#20043;&#38388;&#30340;&#35823;&#24046;&#12290;&#35813;&#20195;&#29702;&#36890;&#36807;&#32467;&#21512;&#36712;&#36857;&#30340;&#21487;&#39044;&#27979;&#24615;&#19982;&#36807;&#28388;&#21644;&#24179;&#28369;&#22788;&#29702;&#29983;&#25104;&#30340;&#26657;&#27491;&#65292;&#33258;&#30417;&#30563;&#22320;&#23398;&#20064;&#21644;&#36845;&#20195;&#25913;&#36827;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;UWB&#27979;&#37327;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20854;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19262v1 Announce Type: cross  Abstract: Indoor positioning using UWB technology has gained interest due to its centimeter-level accuracy potential. However, multipath effects and non-line-of-sight conditions cause ranging errors between anchors and tags. Existing approaches for mitigating these ranging errors rely on collecting large labeled datasets, making them impractical for real-world deployments. This paper proposes a novel self-supervised deep reinforcement learning approach that does not require labeled ground truth data. A reinforcement learning agent uses the channel impulse response as a state and predicts corrections to minimize the error between corrected and estimated ranges. The agent learns, self-supervised, by iteratively improving corrections that are generated by combining the predictability of trajectories with filtering and smoothening. Experiments on real-world UWB measurements demonstrate comparable performance to state-of-the-art supervised methods, o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#24207;&#21015;&#65292;&#25506;&#35752;&#28385;&#36275;&#29305;&#23450;&#32422;&#26463;&#30340;&#31574;&#30053;&#36335;&#24452;&#65292;&#21363;&#28385;&#36275;&#36335;&#24452;&#65292;&#23545;&#20110;&#26500;&#24314;&#32456;&#27490;&#20110;&#22343;&#34913;&#31574;&#30053;&#30340;&#36335;&#24452;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.18079</link><description>&lt;p&gt;
&#27491;&#24577;&#24418;&#24335;&#21338;&#24328;&#20013;&#30340;&#22343;&#34913;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Paths to Equilibrium in Normal-Form Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#24207;&#21015;&#65292;&#25506;&#35752;&#28385;&#36275;&#29305;&#23450;&#32422;&#26463;&#30340;&#31574;&#30053;&#36335;&#24452;&#65292;&#21363;&#28385;&#36275;&#36335;&#24452;&#65292;&#23545;&#20110;&#26500;&#24314;&#32456;&#27490;&#20110;&#22343;&#34913;&#31574;&#30053;&#30340;&#36335;&#24452;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#65292;&#26234;&#20307;&#20250;&#21453;&#22797;&#22312;&#26102;&#38388;&#19978;&#20132;&#20114;&#65292;&#24182;&#38543;&#30528;&#26032;&#25968;&#25454;&#30340;&#21040;&#26469;&#20462;&#35746;&#20182;&#20204;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#31995;&#21015;&#31574;&#30053;&#27010;&#20917;&#12290;&#26412;&#25991;&#30740;&#31350;&#28385;&#36275;&#19968;&#31181;&#30001;&#24378;&#21270;&#23398;&#20064;&#20013;&#25919;&#31574;&#26356;&#26032;&#21551;&#21457;&#30340;&#25104;&#23545;&#32422;&#26463;&#30340;&#31574;&#30053;&#24207;&#21015;&#65292;&#20854;&#20013;&#22312;&#31532; $t$ &#26399;&#26368;&#20248;&#24212;&#31572;&#30340;&#26234;&#20307;&#22312;&#19979;&#19968;&#26399; $t+1$ &#19981;&#20250;&#25913;&#21464;&#20854;&#31574;&#30053;&#12290;&#36825;&#31181;&#32422;&#26463;&#20165;&#35201;&#27714;&#20248;&#21270;&#26234;&#20307;&#19981;&#26356;&#25913;&#31574;&#30053;&#65292;&#20294;&#24182;&#19981;&#20197;&#20219;&#20309;&#26041;&#24335;&#38480;&#21046;&#20854;&#20182;&#38750;&#26368;&#20248;&#21270;&#26234;&#20307;&#65292;&#22240;&#27492;&#20801;&#35768;&#25506;&#32034;&#12290;&#20855;&#26377;&#27492;&#23646;&#24615;&#30340;&#24207;&#21015;&#34987;&#31216;&#20026;&#28385;&#36275;&#36335;&#24452;&#65292;&#24182;&#22312;&#35768;&#22810; MARL &#31639;&#27861;&#20013;&#33258;&#28982;&#20986;&#29616;&#12290;&#20851;&#20110;&#25112;&#30053;&#21160;&#24577;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#65306;&#23545;&#20110;&#32473;&#23450;&#30340;&#21338;&#24328;&#21644;&#21021;&#22987;&#31574;&#30053;&#27010;&#20917;&#65292;&#26159;&#21542;&#24635;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#32456;&#27490;&#20110;&#22343;&#34913;&#31574;&#30053;&#30340;&#28385;&#36275;&#36335;&#24452;&#65311;&#36825;&#20010;&#38382;&#39064;&#30340;&#35299;&#20915;&#23545;&#24212;&#30528;&#19968;&#20123;&#37325;&#35201;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18079v1 Announce Type: cross  Abstract: In multi-agent reinforcement learning (MARL), agents repeatedly interact across time and revise their strategies as new data arrives, producing a sequence of strategy profiles. This paper studies sequences of strategies satisfying a pairwise constraint inspired by policy updating in reinforcement learning, where an agent who is best responding in period $t$ does not switch its strategy in the next period $t+1$. This constraint merely requires that optimizing agents do not switch strategies, but does not constrain the other non-optimizing agents in any way, and thus allows for exploration. Sequences with this property are called satisficing paths, and arise naturally in many MARL algorithms. A fundamental question about strategic dynamics is such: for a given game and initial strategy profile, is it always possible to construct a satisficing path that terminates at an equilibrium strategy? The resolution of this question has implication
&lt;/p&gt;</description></item><item><title>ParFormer&#25552;&#20986;&#20102;&#24182;&#34892;&#23616;&#37096;&#20840;&#23616;&#26631;&#35760;&#28151;&#21512;&#22120;&#21644;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;&#65292;&#20248;&#21270;&#20102;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;CNN&#21644;&#26368;&#20808;&#36827;&#30340;Transformer&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.15004</link><description>&lt;p&gt;
ParFormer&#65306;&#20855;&#26377;&#24182;&#34892;&#23616;&#37096;&#20840;&#23616;&#26631;&#35760;&#28151;&#21512;&#22120;&#21644;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;&#30340;&#35270;&#35273;Transformer&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
ParFormer: Vision Transformer Baseline with Parallel Local Global Token Mixer and Convolution Attention Patch Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15004
&lt;/p&gt;
&lt;p&gt;
ParFormer&#25552;&#20986;&#20102;&#24182;&#34892;&#23616;&#37096;&#20840;&#23616;&#26631;&#35760;&#28151;&#21512;&#22120;&#21644;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;&#65292;&#20248;&#21270;&#20102;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;CNN&#21644;&#26368;&#20808;&#36827;&#30340;Transformer&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ParFormer&#20316;&#20026;&#19968;&#31181;&#22686;&#24378;&#22411;Transformer&#26550;&#26500;&#65292;&#20801;&#35768;&#23558;&#19981;&#21516;&#30340;&#26631;&#35760;&#28151;&#21512;&#22120;&#25972;&#21512;&#21040;&#21333;&#20010;&#38454;&#27573;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#12290;&#21516;&#26102;&#25972;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#25968;&#25454;&#65292;&#23454;&#29616;&#23545;&#30701;&#31243;&#21644;&#38271;&#31243;&#31354;&#38388;&#20851;&#31995;&#30340;&#31934;&#30830;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#20687;&#24179;&#31227;&#31383;&#21475;&#36825;&#26679;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#38500;&#20102;&#24182;&#34892;&#26631;&#35760;&#28151;&#21512;&#22120;&#32534;&#30721;&#22120;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;(CAPE)&#65292;&#20316;&#20026;&#26631;&#20934;&#34917;&#19969;&#23884;&#20837;&#30340;&#22686;&#24378;&#65292;&#36890;&#36807;&#21367;&#31215;&#27880;&#24847;&#21147;&#27169;&#22359;&#25913;&#36827;&#26631;&#35760;&#28151;&#21512;&#22120;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;ParFormer&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29289;&#20307;&#35782;&#21035;&#31561;&#22810;&#20010;&#22797;&#26434;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20110;CNN&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;CAPE&#24050;&#34987;&#35777;&#26126;&#26377;&#30410;&#20110;&#25972;&#20307;MetaFormer&#26550;&#26500;&#65292;&#21363;&#20351;&#20351;&#29992;Id&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15004v1 Announce Type: cross  Abstract: This work presents ParFormer as an enhanced transformer architecture that allows the incorporation of different token mixers into a single stage, hence improving feature extraction capabilities. Integrating both local and global data allows for precise representation of short- and long-range spatial relationships without the need for computationally intensive methods such as shifting windows. Along with the parallel token mixer encoder, We offer the Convolutional Attention Patch Embedding (CAPE) as an enhancement of standard patch embedding to improve token mixer extraction with a convolutional attention module. Our comprehensive evaluation demonstrates that our ParFormer outperforms CNN-based and state-of-the-art transformer-based architectures in image classification and several complex tasks such as object recognition. The proposed CAPE has been demonstrated to benefit the overall MetaFormer architecture, even while utilizing the Id
&lt;/p&gt;</description></item><item><title>MicroT&#26159;&#19968;&#20010;&#20302;&#33021;&#32791;&#12289;&#22810;&#20219;&#21153;&#33258;&#36866;&#24212;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#30340;&#20998;&#31163;&#12289;&#27169;&#22411;&#20248;&#21270;&#21644;&#26412;&#22320;&#20219;&#21153;&#35757;&#32451;&#65292;&#22312;MCUs&#19978;&#23454;&#29616;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#25552;&#21319;&#21644;&#33021;&#32791;&#30340;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2403.08040</link><description>&lt;p&gt;
MicroT&#65306;&#29992;&#20110;MCUs&#30340;&#20302;&#33021;&#32791;&#21644;&#33258;&#36866;&#24212;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MicroT: Low-Energy and Adaptive Models for MCUs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08040
&lt;/p&gt;
&lt;p&gt;
MicroT&#26159;&#19968;&#20010;&#20302;&#33021;&#32791;&#12289;&#22810;&#20219;&#21153;&#33258;&#36866;&#24212;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#30340;&#20998;&#31163;&#12289;&#27169;&#22411;&#20248;&#21270;&#21644;&#26412;&#22320;&#20219;&#21153;&#35757;&#32451;&#65292;&#22312;MCUs&#19978;&#23454;&#29616;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#25552;&#21319;&#21644;&#33021;&#32791;&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MicroT&#65292;&#36825;&#26159;&#19968;&#20010;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;MCUs&#30340;&#20302;&#33021;&#32791;&#12289;&#22810;&#20219;&#21153;&#33258;&#36866;&#24212;&#27169;&#22411;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;&#21407;&#22987;&#27169;&#22411;&#21010;&#20998;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#12290;&#29305;&#24449;&#25552;&#21462;&#22120;&#36890;&#36807;&#33258;&#30417;&#30563;&#30693;&#35782;&#33976;&#39311;&#33719;&#24471;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#20998;&#21106;&#21644;&#32852;&#21512;&#35757;&#32451;&#36827;&#19968;&#27493;&#20248;&#21270;&#20026;&#37096;&#20998;&#27169;&#22411;&#21644;&#23436;&#25972;&#27169;&#22411;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#22312;MCUs&#19978;&#65292;&#22686;&#21152;&#24182;&#22312;&#26412;&#22320;&#20219;&#21153;&#19978;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#26368;&#32456;&#25191;&#34892;&#20851;&#33410;&#25512;&#29702;&#30340;&#38454;&#27573;&#20915;&#31574;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#37096;&#20998;&#27169;&#22411;&#26368;&#21021;&#22788;&#29702;&#26679;&#26412;&#65292;&#22914;&#26524;&#32622;&#20449;&#24230;&#24471;&#20998;&#20302;&#20110;&#35774;&#23450;&#30340;&#38408;&#20540;&#65292;&#23436;&#25972;&#27169;&#22411;&#23558;&#24674;&#22797;&#24182;&#32487;&#32493;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27169;&#22411;&#12289;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;MCU&#26495;&#19978;&#35780;&#20272;&#20102;MicroT&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#26412;&#22320;&#20219;&#21153;&#26102;&#65292;MicroT&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#33021;&#32791;&#12290;&#19982;&#26410;&#32463;&#20248;&#21270;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#30456;&#27604;&#65292;MicroT
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08040v1 Announce Type: new  Abstract: We propose MicroT, a low-energy, multi-task adaptive model framework for resource-constrained MCUs. We divide the original model into a feature extractor and a classifier. The feature extractor is obtained through self-supervised knowledge distillation and further optimized into part and full models through model splitting and joint training. These models are then deployed on MCUs, with classifiers added and trained on local tasks, ultimately performing stage-decision for joint inference. In this process, the part model initially processes the sample, and if the confidence score falls below the set threshold, the full model will resume and continue the inference. We evaluate MicroT on two models, three datasets, and two MCU boards. Our experimental evaluation shows that MicroT effectively improves model performance and reduces energy consumption when dealing with multiple local tasks. Compared to the unoptimized feature extractor, MicroT
&lt;/p&gt;</description></item><item><title>GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.05527</link><description>&lt;p&gt;
GEAR: &#19968;&#31181;&#29992;&#20110;&#20960;&#20046;&#26080;&#25439;&#29983;&#25104;&#25512;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05527
&lt;/p&gt;
&lt;p&gt;
GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;-&#20540;&#65288;KV&#65289;&#32531;&#23384;&#24050;&#25104;&#20026;&#21152;&#24555;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#26029;&#29983;&#25104;&#36895;&#24230;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#38271;&#30340;&#32531;&#23384;&#38656;&#27714;&#24050;&#23558;LLM&#25512;&#26029;&#36716;&#21464;&#20026;&#19968;&#20010;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#65292;&#26174;&#33879;&#22320;&#38480;&#21046;&#20102;&#31995;&#32479;&#21534;&#21520;&#37327;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#26631;&#35760;&#25110;&#22343;&#21248;&#37327;&#21270;&#25152;&#26377;&#26465;&#30446;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#20250;&#20135;&#29983;&#36739;&#39640;&#30340;&#36817;&#20284;&#35823;&#24046;&#26469;&#34920;&#31034;&#21387;&#32553;&#21518;&#30340;&#30697;&#38453;&#12290;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#27599;&#20010;&#27493;&#39588;&#30340;&#35823;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#37325;&#22823;&#20559;&#24046;&#21644;&#24615;&#33021;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GEAR&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#21387;&#32553;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05527v1 Announce Type: cross  Abstract: Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quant
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#26399;&#21002;&#25991;&#31456;&#65292;&#24635;&#32467;&#20102;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#22312;&#24494;&#29983;&#29289;&#32452;&#23398;&#20013;&#30340;&#29616;&#26377;&#23454;&#36341;&#65292;&#25506;&#35752;&#20102;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36991;&#20813;&#24120;&#35265;&#23454;&#39564;&#35774;&#35745;&#32570;&#38519;&#30340;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2402.17621</link><description>&lt;p&gt;
&#29992;&#20110;&#24494;&#29983;&#29289;&#32452;&#23398;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#65306;&#24357;&#21512;&#24403;&#21069;&#21644;&#26368;&#20339;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Supervised machine learning for microbiomics: bridging the gap between current and best practices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17621
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#26399;&#21002;&#25991;&#31456;&#65292;&#24635;&#32467;&#20102;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#22312;&#24494;&#29983;&#29289;&#32452;&#23398;&#20013;&#30340;&#29616;&#26377;&#23454;&#36341;&#65292;&#25506;&#35752;&#20102;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36991;&#20813;&#24120;&#35265;&#23454;&#39564;&#35774;&#35745;&#32570;&#38519;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#23558;&#21152;&#36895;&#20020;&#24202;&#24494;&#29983;&#29289;&#32452;&#23398;&#21019;&#26032;&#65292;&#22914;&#30142;&#30149;&#35786;&#26029;&#21644;&#39044;&#21518;&#12290;&#36825;&#23558;&#38656;&#35201;&#39640;&#36136;&#37327;&#12289;&#21487;&#37325;&#29616;&#12289;&#21487;&#35299;&#37322;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#20854;&#39044;&#27979;&#33021;&#21147;&#36798;&#21040;&#25110;&#36229;&#36807;&#30417;&#31649;&#26426;&#26500;&#23545;&#20020;&#24202;&#24037;&#20855;&#35774;&#23450;&#30340;&#39640;&#38376;&#27099;&#12290;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;2021-2022&#24180;&#21457;&#34920;&#30340;100&#31687;&#21516;&#34892;&#35780;&#35758;&#30340;&#26399;&#21002;&#25991;&#31456;&#65292;&#25429;&#25417;&#20102;&#24403;&#21069;&#23558;&#30417;&#30563;ML&#24212;&#29992;&#20110;&#24494;&#29983;&#29289;&#32452;&#23398;&#25968;&#25454;&#30340;&#23454;&#36341;&#30340;&#19968;&#20010;&#24555;&#29031;&#12290;&#25105;&#20204;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#24341;&#23548;&#35752;&#35770;&#21508;&#31181;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#21253;&#25324;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#22914;&#20309;&#20943;&#36731;&#23567;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#21516;&#26102;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20851;&#20110;&#22914;&#20309;&#36991;&#20813;&#21487;&#33021;&#25439;&#23475;&#27169;&#22411;&#24615;&#33021;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#30340;&#24120;&#35265;&#23454;&#39564;&#35774;&#35745;&#32570;&#38519;&#30340;&#25351;&#21335;&#12290;&#35752;&#35770;&#38468;&#26377;&#19968;&#20010;&#20114;&#21160;&#22312;&#32447;&#25945;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17621v1 Announce Type: cross  Abstract: Machine learning (ML) is set to accelerate innovations in clinical microbiomics, such as in disease diagnostics and prognostics. This will require high-quality, reproducible, interpretable workflows whose predictive capabilities meet or exceed the high thresholds set for clinical tools by regulatory agencies. Here, we capture a snapshot of current practices in the application of supervised ML to microbiomics data, through an in-depth analysis of 100 peer-reviewed journal articles published in 2021-2022. We apply a data-driven approach to steer discussion of the merits of varied approaches to experimental design, including key considerations such as how to mitigate the effects of small dataset size while avoiding data leakage. We further provide guidance on how to avoid common experimental design pitfalls that can hurt model performance, trustworthiness, and reproducibility. Discussion is accompanied by an interactive online tutorial th
&lt;/p&gt;</description></item><item><title>ITL&#26159;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20013;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.15898</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Information-based Transductive Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15898
&lt;/p&gt;
&lt;p&gt;
ITL&#26159;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20013;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#20027;&#21160;&#23398;&#20064;&#25512;&#24191;&#21040;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#37319;&#26679;&#21463;&#38480;&#20110;&#21487;&#35775;&#38382;&#22495;&#30340;&#24773;&#20917;&#65292;&#32780;&#39044;&#27979;&#30446;&#26631;&#21487;&#33021;&#20301;&#20110;&#36825;&#20010;&#22495;&#20043;&#22806;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ITL&#65292;&#21363;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#12290;&#22312;&#19968;&#33324;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ITL&#25910;&#25947;&#21040;&#21487;&#20174;&#21487;&#35775;&#38382;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#26368;&#23567;&#21487;&#33021;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;ITL&#65306;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;ITL&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15898v1 Announce Type: cross  Abstract: We generalize active learning to address real-world settings where sampling is restricted to an accessible region of the domain, while prediction targets may lie outside this region. To this end, we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified prediction targets. We show, under general regularity assumptions, that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. We demonstrate ITL in two key applications: Few-shot fine-tuning of large neural networks and safe Bayesian optimization, and in both cases, ITL significantly outperforms the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#29983;&#25104;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#22686;&#21152;&#27169;&#22411;&#22810;&#26679;&#24615;&#65292;&#24182;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>https://arxiv.org/abs/2311.16176</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#21270;&#21512;&#25104;&#21644;&#25193;&#25955;&#27169;&#22411;&#20943;&#36731;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Biases with Diverse Ensembles and Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16176
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#29983;&#25104;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#22686;&#21152;&#27169;&#22411;&#22810;&#26679;&#24615;&#65292;&#24182;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#21363;&#22810;&#20010;&#32447;&#32034;&#21487;&#20197;&#39044;&#27979;&#30446;&#26631;&#26631;&#31614;&#65292;&#24120;&#24120;&#23548;&#33268;&#19968;&#31181;&#31216;&#20026;&#25463;&#24452;&#20559;&#35265;&#30340;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#20381;&#36182;&#20110;&#38169;&#35823;&#30340;&#12289;&#26131;&#23398;&#30340;&#32447;&#32034;&#65292;&#32780;&#24573;&#30053;&#21487;&#38752;&#30340;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#30340;&#38598;&#25104;&#22810;&#26679;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#29305;&#23450;&#30340;&#35757;&#32451;&#38388;&#38548;&#20013;&#65292;DPMs&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21363;&#20351;&#22312;&#26174;&#31034;&#30456;&#20851;&#36755;&#20837;&#29305;&#24449;&#30340;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#23646;&#24615;&#36890;&#36807;&#38598;&#25104;&#19981;&#19968;&#33268;&#24615;&#29983;&#25104;&#21512;&#25104;&#21453;&#20107;&#23454;&#26469;&#22686;&#21152;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DPM&#24341;&#23548;&#30340;&#22810;&#26679;&#21270;&#36275;&#20197;&#28040;&#38500;&#23545;&#20027;&#35201;&#25463;&#24452;&#32447;&#32034;&#30340;&#20381;&#36182;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20960;&#20010;&#22810;&#26679;&#21270;&#30446;&#26631;&#19978;&#22312;&#23454;&#35777;&#19978;&#37327;&#21270;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#26368;&#32456;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16176v2 Announce Type: replace-cross  Abstract: Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to a phenomenon known as shortcut bias, where a model relies on erroneous, easy-to-learn cues while ignoring reliable ones. In this work, we propose an ensemble diversification framework exploiting Diffusion Probabilistic Models (DPMs) for shortcut bias mitigation. We show that at particular training intervals, DPMs can generate images with novel feature combinations, even when trained on samples displaying correlated input features. We leverage this crucial property to generate synthetic counterfactuals to increase model diversity via ensemble disagreement. We show that DPM-guided diversification is sufficient to remove dependence on primary shortcut cues, without a need for additional supervised signals. We further empirically quantify its efficacy on several diversification objectives, and finally show improved generalizati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;&#65288;MHSNs&#65289;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#21644;&#21367;&#31215;&#32467;&#26500;&#65292;&#29983;&#25104;&#23545;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2311.10270</link><description>&lt;p&gt;
&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#30340;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multiscale Hodge Scattering Networks for Data Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;&#65288;MHSNs&#65289;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#21644;&#21367;&#31215;&#32467;&#26500;&#65292;&#29983;&#25104;&#23545;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25955;&#23556;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#19978;&#27979;&#37327;&#30340;&#20449;&#21495;&#65292;&#31216;&#20026;\emph{&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;}&#65288;MHSNs&#65289;&#12290;&#25105;&#20204;&#30340;&#26500;&#36896;&#22522;&#20110;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#19978;&#30340;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#65292;&#21363;$\kappa$-GHWT&#21644;$\kappa$-HGLET&#65292;&#25105;&#20204;&#26368;&#36817;&#20026;&#32473;&#23450;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#20013;&#30340;&#32500;&#24230;$\kappa \in \mathbb{N}$&#25512;&#24191;&#20102;&#22522;&#20110;&#33410;&#28857;&#30340;&#24191;&#20041;&#21704;-&#27779;&#20160;&#21464;&#25442;&#65288;GHWT&#65289;&#21644;&#20998;&#23618;&#22270;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21464;&#25442;&#65288;HGLET&#65289;&#12290;$\kappa$-GHWT&#21644;$\kappa$-HGLET&#37117;&#24418;&#25104;&#20887;&#20313;&#38598;&#21512;&#65288;&#21363;&#35789;&#20856;&#65289;&#30340;&#22810;&#23610;&#24230;&#22522;&#30784;&#21521;&#37327;&#21644;&#32473;&#23450;&#20449;&#21495;&#30340;&#30456;&#24212;&#25193;&#23637;&#31995;&#25968;&#12290;&#25105;&#20204;&#30340;MHSNs&#20351;&#29992;&#31867;&#20284;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20998;&#23618;&#32467;&#26500;&#26469;&#32423;&#32852;&#35789;&#20856;&#31995;&#25968;&#27169;&#30340;&#30697;&#12290;&#25152;&#24471;&#29305;&#24449;&#23545;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#30340;&#37325;&#26032;&#25490;&#24207;&#19981;&#21464;&#65288;&#21363;&#33410;&#28857;&#25490;&#21015;&#30340;&#32622;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10270v2 Announce Type: replace  Abstract: We propose new scattering networks for signals measured on simplicial complexes, which we call \emph{Multiscale Hodge Scattering Networks} (MHSNs). Our construction is based on multiscale basis dictionaries on simplicial complexes, i.e., the $\kappa$-GHWT and $\kappa$-HGLET, which we recently developed for simplices of dimension $\kappa \in \mathbb{N}$ in a given simplicial complex by generalizing the node-based Generalized Haar-Walsh Transform (GHWT) and Hierarchical Graph Laplacian Eigen Transform (HGLET). The $\kappa$-GHWT and the $\kappa$-HGLET both form redundant sets (i.e., dictionaries) of multiscale basis vectors and the corresponding expansion coefficients of a given signal. Our MHSNs use a layered structure analogous to a convolutional neural network (CNN) to cascade the moments of the modulus of the dictionary coefficients. The resulting features are invariant to reordering of the simplices (i.e., node permutation of the u
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LadaGAN&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#65292;&#36890;&#36807;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09596</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#21147;Transformer&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Efficient generative adversarial networks using linear additive-attention Transformers. (arXiv:2401.09596v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LadaGAN&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#65292;&#36890;&#36807;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#31561;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#35745;&#31639;&#22797;&#26434;&#30340;&#26550;&#26500;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30740;&#31350;&#23454;&#39564;&#23460;&#21644;&#36164;&#28304;&#20805;&#36275;&#30340;&#20844;&#21496;&#20013;&#30340;&#37319;&#29992;&#21644;&#20351;&#29992;&#65292;&#21516;&#26102;&#20063;&#26497;&#22823;&#22320;&#22686;&#21152;&#20102;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#30340;&#30899;&#36275;&#36857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LadaGAN&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#24314;&#31435;&#22312;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#19978;&#12290;&#35813;&#22359;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#19968;&#20010;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#65292;&#23427;&#27599;&#20010;&#22836;&#37096;&#35745;&#31639;&#19968;&#20010;&#27880;&#24847;&#21521;&#37327;&#65292;&#32780;&#19981;&#26159;&#20108;&#27425;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#22312;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#20013;&#37117;&#37319;&#29992;&#20102;Ladaformer&#65292;&#36825;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#20811;&#26381;&#20102;Transformer GAN&#32463;&#24120;&#20986;&#29616;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#12290;LadaGAN&#19968;&#30452;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;GANs&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the capacity of deep generative models for image generation, such as Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has dramatically improved in recent years, much of their success can be attributed to computationally expensive architectures. This has limited their adoption and use to research laboratories and companies with large resources, while significantly raising the carbon footprint for training, fine-tuning, and inference. In this work, we present LadaGAN, an efficient generative adversarial network that is built upon a novel Transformer block named Ladaformer. The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention. We employ Ladaformer in both the generator and discriminator, which reduces the computational complexity and overcomes the training instabilities often associated with Transformer GANs. LadaGAN consistently outperforms exist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#38381;&#24335;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21040;&#19968;&#32452;&#22522;&#20110;&#30456;&#21516;&#37327;&#30340;&#31561;&#20215;&#31867;&#20013;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#35813;&#31561;&#20215;&#31867;&#19982;&#31526;&#21495;&#22238;&#24402;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#26041;&#31243;&#30340;&#20132;&#38598;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2401.04978</link><description>&lt;p&gt;
&#29992;&#31526;&#21495;&#22238;&#24402;&#26799;&#24230;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#38381;&#24335;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Closed-Form Interpretation of Neural Network Classifiers with Symbolic Regression Gradients. (arXiv:2401.04978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#38381;&#24335;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21040;&#19968;&#32452;&#22522;&#20110;&#30456;&#21516;&#37327;&#30340;&#31561;&#20215;&#31867;&#20013;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#35813;&#31561;&#20215;&#31867;&#19982;&#31526;&#21495;&#22238;&#24402;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#26041;&#31243;&#30340;&#20132;&#38598;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#31185;&#23398;&#21457;&#29616;&#12290;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22238;&#24402;&#19981;&#21516;&#65292;&#23545;&#20110;&#20998;&#31867;&#32780;&#35328;&#65292;&#21363;&#20351;&#31070;&#32463;&#32593;&#32476;&#26412;&#36523;&#30340;&#20998;&#31867;&#22522;&#20110;&#21487;&#20197;&#34920;&#31034;&#20026;&#38381;&#24335;&#26041;&#31243;&#30340;&#37327;&#65292;&#20063;&#19968;&#33324;&#26080;&#27861;&#25214;&#21040;&#20174;&#31070;&#32463;&#32593;&#32476;&#21040;&#31526;&#21495;&#26041;&#31243;&#30340;&#19968;&#23545;&#19968;&#26144;&#23556;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#23558;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21040;&#19968;&#20010;&#31561;&#20215;&#31867;&#20013;&#65292;&#36825;&#20010;&#31561;&#20215;&#31867;&#30340;&#20998;&#31867;&#20989;&#25968;&#30340;&#20915;&#31574;&#37117;&#22522;&#20110;&#30456;&#21516;&#30340;&#37327;&#12290;&#25105;&#36890;&#36807;&#25214;&#21040;&#36825;&#20010;&#31561;&#20215;&#31867;&#19982;&#30001;&#31526;&#21495;&#22238;&#24402;&#25628;&#32034;&#31354;&#38388;&#23450;&#20041;&#30340;&#21487;&#35835;&#30340;&#26041;&#31243;&#30340;&#20132;&#38598;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38480;&#20110;&#20998;&#31867;&#22120;&#25110;&#23436;&#25972;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#38544;&#34255;&#23618;&#25110;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20219;&#24847;&#31070;&#32463;&#20803;&#65292;&#25110;&#31616;&#21270;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#22120;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
I introduce a unified framework for interpreting neural network classifiers tailored toward automated scientific discovery. In contrast to neural network-based regression, for classification, it is in general impossible to find a one-to-one mapping from the neural network to a symbolic equation even if the neural network itself bases its classification on a quantity that can be written as a closed-form equation. In this paper, I embed a trained neural network into an equivalence class of classifying functions that base their decisions on the same quantity. I interpret neural networks by finding an intersection between this equivalence class and human-readable equations defined by the search space of symbolic regression. The approach is not limited to classifiers or full neural networks and can be applied to arbitrary neurons in hidden layers or latent spaces or to simplify the process of interpreting neural network regressors.
&lt;/p&gt;</description></item><item><title>Upcycled-FL&#26159;&#19968;&#31181;&#20943;&#23569;&#20449;&#24687;&#27844;&#28431;&#21644;&#35745;&#31639;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#27599;&#20010;&#20598;&#25968;&#36845;&#20195;&#20013;&#24212;&#29992;&#19968;&#38454;&#36817;&#20284;&#65292;&#20351;&#24471;&#19968;&#21322;&#30340;&#32852;&#37030;&#23398;&#20064;&#26356;&#26032;&#19981;&#20250;&#27844;&#28431;&#20449;&#24687;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2310.06341</link><description>&lt;p&gt;
&#20943;&#23569;&#20449;&#24687;&#27844;&#28431;&#21644;&#35745;&#31639;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Reduced Information Leakage and Computation. (arXiv:2310.06341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06341
&lt;/p&gt;
&lt;p&gt;
Upcycled-FL&#26159;&#19968;&#31181;&#20943;&#23569;&#20449;&#24687;&#27844;&#28431;&#21644;&#35745;&#31639;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#27599;&#20010;&#20598;&#25968;&#36845;&#20195;&#20013;&#24212;&#29992;&#19968;&#38454;&#36817;&#20284;&#65292;&#20351;&#24471;&#19968;&#21322;&#30340;&#32852;&#37030;&#23398;&#20064;&#26356;&#26032;&#19981;&#20250;&#27844;&#28431;&#20449;&#24687;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#20801;&#35768;&#22810;&#20010;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#22312;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#23398;&#20064;&#19968;&#20010;&#20844;&#20849;&#27169;&#22411;&#12290;&#23613;&#31649;&#26412;&#22320;&#25968;&#25454;&#27809;&#26377;&#30452;&#25509;&#26292;&#38706;&#65292;&#20294;&#20173;&#23384;&#22312;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#23458;&#25143;&#31471;&#30340;&#25935;&#24863;&#20449;&#24687;&#21487;&#20197;&#20174;&#20013;&#38388;&#35745;&#31639;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#30456;&#21516;&#25968;&#25454;&#22312;&#36845;&#20195;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#36825;&#31181;&#20449;&#24687;&#27844;&#28431;&#20250;&#19981;&#26029;&#31215;&#32047;&#12290;&#22240;&#27492;&#65292;&#22312;&#35774;&#35745;&#20445;&#25252;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#26102;&#65292;&#24456;&#38590;&#24179;&#34913;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;Upcycled-FL&#65292;&#23427;&#22312;&#27599;&#20010;&#20598;&#25968;&#36845;&#20195;&#20013;&#37117;&#24212;&#29992;&#20102;&#19968;&#38454;&#36817;&#20284;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#19968;&#21322;&#30340;&#32852;&#37030;&#23398;&#20064;&#26356;&#26032;&#19981;&#20250;&#36896;&#25104;&#20449;&#24687;&#27844;&#28431;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;Upcycled-FL&#30340;&#25910;&#25947;&#65288;&#36895;&#29575;&#65289;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#28982;&#21518;&#24212;&#29992;&#25200;&#21160;&#26426;&#21046;&#26469;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed learning paradigm that allows multiple decentralized clients to collaboratively learn a common model without sharing local data. Although local data is not exposed directly, privacy concerns nonetheless exist as clients' sensitive information can be inferred from intermediate computations. Moreover, such information leakage accumulates substantially over time as the same data is repeatedly used during the iterative learning process. As a result, it can be particularly difficult to balance the privacy-accuracy trade-off when designing privacy-preserving FL algorithms. In this paper, we introduce Upcycled-FL, a novel federated learning framework with first-order approximation applied at every even iteration. Under this framework, half of the FL updates incur no information leakage and require much less computation. We first conduct the theoretical analysis on the convergence (rate) of Upcycled-FL, and then apply perturbation mechanisms to preserve
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32858;&#31867;&#30697;&#38453;&#24418;&#24335;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20854;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.05288</link><description>&lt;p&gt;
&#24102;&#26377;&#24322;&#24120;&#20540;&#30340;&#19977;&#20803;&#25968;&#25454;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Clustering Three-Way Data with Outliers. (arXiv:2310.05288v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05288
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32858;&#31867;&#30697;&#38453;&#24418;&#24335;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20854;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30697;&#38453;&#21464;&#37327;&#20998;&#24067;&#26159;&#27169;&#22411;&#32858;&#31867;&#39046;&#22495;&#30340;&#26368;&#26032;&#28155;&#21152;&#65292;&#20174;&#32780;&#21487;&#20197;&#20998;&#26512;&#20855;&#26377;&#22797;&#26434;&#32467;&#26500;&#65288;&#22914;&#22270;&#20687;&#21644;&#26102;&#38388;&#24207;&#21015;&#65289;&#30340;&#30697;&#38453;&#24418;&#24335;&#25968;&#25454;&#12290;&#30001;&#20110;&#20854;&#26368;&#36817;&#30340;&#20986;&#29616;&#65292;&#20851;&#20110;&#30697;&#38453;&#21464;&#37327;&#25968;&#25454;&#30340;&#25991;&#29486;&#26377;&#38480;&#65292;&#23545;&#20110;&#22788;&#29702;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#24322;&#24120;&#20540;&#30340;&#25991;&#29486;&#26356;&#23569;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#29992;&#20110;&#32858;&#31867;&#30697;&#38453;&#21464;&#37327;&#27491;&#24577;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#23376;&#38598;&#23545;&#25968;&#20284;&#28982;&#30340;&#20998;&#24067;&#65292;&#23558;OCLUST&#31639;&#27861;&#25193;&#23637;&#21040;&#30697;&#38453;&#21464;&#37327;&#27491;&#24577;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#36845;&#20195;&#26041;&#27861;&#26816;&#27979;&#21644;&#21098;&#35009;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matrix-variate distributions are a recent addition to the model-based clustering field, thereby making it possible to analyze data in matrix form with complex structure such as images and time series. Due to its recent appearance, there is limited literature on matrix-variate data, with even less on dealing with outliers in these models. An approach for clustering matrix-variate normal data with outliers is discussed. The approach, which uses the distribution of subset log-likelihoods, extends the OCLUST algorithm to matrix-variate normal data and uses an iterative approach to detect and trim outliers.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#26469;&#25506;&#35752;&#38750;&#24179;&#31283;&#26680;&#22312;&#28145;&#23618;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2309.10068</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#26680;&#23545;&#28145;&#23618;&#39640;&#26031;&#36807;&#31243;&#30340;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Unifying Perspective on Non-Stationary Kernels for Deeper Gaussian Processes. (arXiv:2309.10068v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#26469;&#25506;&#35752;&#38750;&#24179;&#31283;&#26680;&#22312;&#28145;&#23618;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#29992;&#20110;&#25968;&#25454;&#30340;&#38543;&#26426;&#20989;&#25968;&#36817;&#20284;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#32479;&#35745;&#25216;&#26415;&#12290;&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#65292;&#30001;&#20110;&#20854;&#20248;&#36234;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#31232;&#30095;&#24773;&#20917;&#19979;&#65292;&#20197;&#21450;&#20854;&#22266;&#26377;&#30340;&#25552;&#20379;&#24378;&#20581;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;GP&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#39640;&#24230;&#20381;&#36182;&#20110;&#26680;&#24515;&#26041;&#27861;&#30340;&#22797;&#26434;&#23450;&#21046;&#65292;&#36825;&#24448;&#24448;&#22312;&#20351;&#29992;&#26631;&#20934;&#35774;&#32622;&#21644;&#29616;&#25104;&#36719;&#20214;&#24037;&#20855;&#26102;&#20351;&#20174;&#19994;&#32773;&#19981;&#28385;&#24847;&#12290;&#21487;&#20197;&#35828;&#65292;GP&#26368;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#26159;&#26680;&#20989;&#25968;&#65292;&#23427;&#25198;&#28436;&#21327;&#26041;&#24046;&#31639;&#23376;&#30340;&#35282;&#33394;&#12290;Mat\'ern&#31867;&#30340;&#24179;&#31283;&#26680;&#22312;&#22823;&#22810;&#25968;&#24212;&#29992;&#30740;&#31350;&#20013;&#34987;&#20351;&#29992;&#65307;&#20302;&#25928;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#19981;&#29616;&#23454;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24448;&#24448;&#26159;&#20854;&#32467;&#26524;&#12290;&#38750;&#24179;&#31283;&#26680;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20854;&#26356;&#21152;&#22797;&#26434;&#30340;&#23646;&#24615;&#65292;&#24456;&#23569;&#34987;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Gaussian process (GP) is a popular statistical technique for stochastic function approximation and uncertainty quantification from data. GPs have been adopted into the realm of machine learning in the last two decades because of their superior prediction abilities, especially in data-sparse scenarios, and their inherent ability to provide robust uncertainty estimates. Even so, their performance highly depends on intricate customizations of the core methodology, which often leads to dissatisfaction among practitioners when standard setups and off-the-shelf software tools are being deployed. Arguably the most important building block of a GP is the kernel function which assumes the role of a covariance operator. Stationary kernels of the Mat\'ern class are used in the vast majority of applied studies; poor prediction performance and unrealistic uncertainty quantification are often the consequences. Non-stationary kernels show improved performance but are rarely used due to their more
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#31995;&#25968;&#30340;&#37327;&#65292;&#29992;&#20110;&#31934;&#30830;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36864;&#21270;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#21442;&#25968;&#21306;&#22495;&#30340;&#36864;&#21270;&#39034;&#24207;&#65292;&#24182;&#25581;&#31034;&#20102;&#38543;&#26426;&#20248;&#21270;&#22120;&#23545;&#20020;&#30028;&#28857;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.12108</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#31995;&#25968;&#37327;&#21270;&#22855;&#24322;&#27169;&#22411;&#20013;&#30340;&#36864;&#21270;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Quantifying degeneracy in singular models via the learning coefficient. (arXiv:2308.12108v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12108
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#31995;&#25968;&#30340;&#37327;&#65292;&#29992;&#20110;&#31934;&#30830;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36864;&#21270;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#21442;&#25968;&#21306;&#22495;&#30340;&#36864;&#21270;&#39034;&#24207;&#65292;&#24182;&#25581;&#31034;&#20102;&#38543;&#26426;&#20248;&#21270;&#22120;&#23545;&#20020;&#30028;&#28857;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26159;&#20855;&#26377;&#22797;&#26434;&#36864;&#21270;&#30340;&#22855;&#24322;&#32479;&#35745;&#27169;&#22411;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#31995;&#25968;&#30340;&#37327;&#65292;&#23427;&#22312;&#22855;&#24322;&#23398;&#20064;&#29702;&#35770;&#20013;&#31934;&#30830;&#22320;&#37327;&#21270;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36864;&#21270;&#31243;&#24230;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#35777;&#26126;DNN&#20013;&#30340;&#36864;&#21270;&#19981;&#33021;&#20165;&#36890;&#36807;&#35745;&#31639;&#8220;&#24179;&#22374;&#8221;&#26041;&#21521;&#30340;&#25968;&#37327;&#26469;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#30340;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;&#30340;&#35745;&#31639;&#21487;&#25193;&#23637;&#36817;&#20284;&#26041;&#27861;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#24050;&#30693;&#29702;&#35770;&#20540;&#30340;&#20302;&#32500;&#27169;&#22411;&#19978;&#28436;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;&#33021;&#22815;&#27491;&#30830;&#24674;&#22797;&#24863;&#20852;&#36259;&#21442;&#25968;&#21306;&#22495;&#20043;&#38388;&#36864;&#21270;&#30340;&#39034;&#24207;&#12290;&#23545;MNIST&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;&#21487;&#20197;&#25581;&#31034;&#38543;&#26426;&#20248;&#21270;&#22120;&#23545;&#26356;&#36864;&#21270;&#25110;&#19981;&#22826;&#36864;&#21270;&#30340;&#20020;&#30028;&#28857;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) are singular statistical models which exhibit complex degeneracies. In this work, we illustrate how a quantity known as the \emph{learning coefficient} introduced in singular learning theory quantifies precisely the degree of degeneracy in deep neural networks. Importantly, we will demonstrate that degeneracy in DNN cannot be accounted for by simply counting the number of "flat" directions. We propose a computationally scalable approximation of a localized version of the learning coefficient using stochastic gradient Langevin dynamics. To validate our approach, we demonstrate its accuracy in low-dimensional models with known theoretical values. Importantly, the local learning coefficient can correctly recover the ordering of degeneracy between various parameter regions of interest. An experiment on MNIST shows the local learning coefficient can reveal the inductive bias of stochastic opitmizers for more or less degenerate critical points.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26816;&#27979;&#21464;&#21387;&#22120;&#23558;&#24494;&#38663;&#20107;&#20214;&#26816;&#27979;&#21644;&#28304;&#23450;&#20301;&#32479;&#19968;&#20026;&#19968;&#20010;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23454;&#26102;&#24494;&#38663;&#30417;&#27979;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2307.09207</link><description>&lt;p&gt;
&#20351;&#29992;&#26816;&#27979;&#21464;&#21387;&#22120;&#36827;&#34892;&#32852;&#21512;&#24494;&#38663;&#20107;&#20214;&#26816;&#27979;&#21644;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Joint Microseismic Event Detection and Location with a Detection Transformer. (arXiv:2307.09207v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26816;&#27979;&#21464;&#21387;&#22120;&#23558;&#24494;&#38663;&#20107;&#20214;&#26816;&#27979;&#21644;&#28304;&#23450;&#20301;&#32479;&#19968;&#20026;&#19968;&#20010;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23454;&#26102;&#24494;&#38663;&#30417;&#27979;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#38663;&#20107;&#20214;&#30340;&#26816;&#27979;&#21644;&#23450;&#20301;&#26159;&#24494;&#38663;&#30417;&#27979;&#30340;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#27833;&#34255;&#21050;&#28608;&#21644;&#28436;&#21270;&#36807;&#31243;&#20013;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#22320;&#19979;&#20449;&#24687;&#12290;&#20256;&#32479;&#30340;&#20107;&#20214;&#26816;&#27979;&#21644;&#23450;&#20301;&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#25163;&#21160;&#24178;&#39044;&#21644;/&#25110;&#22823;&#37327;&#35745;&#31639;&#65292;&#32780;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#26041;&#27861;&#36890;&#24120;&#20998;&#21035;&#35299;&#20915;&#26816;&#27979;&#21644;&#23450;&#20301;&#38382;&#39064;&#65307;&#36825;&#20123;&#38480;&#21046;&#38459;&#30861;&#20102;&#23454;&#26102;&#24494;&#38663;&#30417;&#27979;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20027;&#24178;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#65292;&#23558;&#20107;&#20214;&#26816;&#27979;&#21644;&#28304;&#23450;&#20301;&#32479;&#19968;&#21040;&#19968;&#20010;&#21333;&#19968;&#26694;&#26550;&#20013;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#20110;&#35760;&#24405;&#30340;&#27874;&#24418;&#19978;&#12290;&#35813;&#32593;&#32476;&#20197;&#27169;&#25311;&#22810;&#20010;&#24494;&#38663;&#20107;&#20214;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#20107;&#20214;&#23545;&#24212;&#20110;&#30097;&#20284;&#24494;&#38663;&#27963;&#21160;&#21306;&#22495;&#20013;&#30340;&#38543;&#26426;&#28304;&#20301;&#32622;&#12290;&#22312;SEAM Time Lapse&#27169;&#22411;&#30340;&#20108;&#32500;&#21078;&#38754;&#19978;&#36827;&#34892;&#20102;&#21512;&#25104;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Microseismic event detection and location are two primary components in microseismic monitoring, which offers us invaluable insights into the subsurface during reservoir stimulation and evolution. Conventional approaches for event detection and location often suffer from manual intervention and/or heavy computation, while current machine learning-assisted approaches typically address detection and location separately; such limitations hinder the potential for real-time microseismic monitoring. We propose an approach to unify event detection and source location into a single framework by adapting a Convolutional Neural Network backbone and an encoder-decoder Transformer with a set-based Hungarian loss, which is applied directly to recorded waveforms. The proposed network is trained on synthetic data simulating multiple microseismic events corresponding to random source locations in the area of suspected microseismic activities. A synthetic test on a 2D profile of the SEAM Time Lapse mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#39640;&#23494;&#24230;&#21306;&#22495;&#20013;&#30340;&#32467;&#26500;&#65292;&#20855;&#26377;&#20808;&#21069;&#31639;&#27861;&#25152;&#19981;&#20855;&#22791;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00677</link><description>&lt;p&gt;
SDC-HSDD-NDSA: &#20351;&#29992;&#23618;&#27425;&#27425;&#32423;&#23548;&#21521;&#24046;&#24322;&#21644;&#24402;&#19968;&#21270;&#23494;&#24230;&#33258;&#36866;&#24212;&#30340;&#32467;&#26500;&#26816;&#27979;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption. (arXiv:2307.00677v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#39640;&#23494;&#24230;&#21306;&#22495;&#20013;&#30340;&#32467;&#26500;&#65292;&#20855;&#26377;&#20808;&#21069;&#31639;&#27861;&#25152;&#19981;&#20855;&#22791;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#32858;&#31867;&#31639;&#27861;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#35782;&#21035;&#20219;&#24847;&#24418;&#29366;&#30340;&#32858;&#31867;&#65292;&#21482;&#35201;&#19981;&#21516;&#30340;&#39640;&#23494;&#24230;&#32858;&#31867;&#20043;&#38388;&#26377;&#20302;&#23494;&#24230;&#21306;&#22495;&#20998;&#38548;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20302;&#23494;&#24230;&#21306;&#22495;&#23558;&#32858;&#31867;&#20998;&#38548;&#24320;&#30340;&#35201;&#27714;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#22240;&#20026;&#39640;&#23494;&#24230;&#21306;&#22495;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#32467;&#26500;&#65292;&#24212;&#35813;&#34987;&#32858;&#31867;&#21040;&#19981;&#21516;&#30340;&#32452;&#20013;&#12290;&#36825;&#31181;&#24773;&#20917;&#35828;&#26126;&#20102;&#25105;&#20204;&#24050;&#30693;&#30340;&#25152;&#26377;&#20808;&#21069;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#30340;&#20027;&#35201;&#32570;&#38519;--&#26080;&#27861;&#26816;&#27979;&#39640;&#23494;&#24230;&#32858;&#31867;&#20013;&#30340;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#26041;&#26696;&#65292;&#26082;&#20855;&#26377;&#20808;&#21069;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#21448;&#33021;&#22815;&#26816;&#27979;&#21040;&#39640;&#23494;&#24230;&#21306;&#22495;&#20013;&#26410;&#34987;&#20302;&#23494;&#24230;&#21306;&#20998;&#24320;&#30340;&#32467;&#26500;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#23618;&#27425;&#27425;&#32423;&#23548;&#21521;&#24046;&#24322;&#12289;&#23618;&#27425;&#21270;&#12289;&#24402;&#19968;&#21270;&#23494;&#24230;&#20197;&#21450;&#33258;&#36866;&#24212;&#31995;&#25968;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;&#32467;&#26500;&#26816;&#27979;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Density-based clustering could be the most popular clustering algorithm since it can identify clusters of arbitrary shape as long as different (high-density) clusters are separated by low-density regions. However, the requirement of the separateness of clusters by low-density regions is not trivial since a high-density region might have different structures which should be clustered into different groups. Such a situation demonstrates the main flaw of all previous density-based clustering algorithms we have known--structures in a high-density cluster could not be detected. Therefore, this paper aims to provide a density-based clustering scheme that not only has the ability previous ones have but could also detect structures in a high-density region not separated by low-density ones. The algorithm employs secondary directed differential, hierarchy, normalized density, as well as the self-adaption coefficient, and thus is called Structure Detecting Cluster by Hierarchical Secondary Direc
&lt;/p&gt;</description></item><item><title>HUMAP&#26159;&#19968;&#31181;&#26032;&#30340;&#23618;&#27425;&#38477;&#32500;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#23618;&#27425;&#25506;&#32034;&#20013;&#20445;&#30041;&#24515;&#29702;&#22320;&#22270;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#25968;&#25454;&#31867;&#22411;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.07718</link><description>&lt;p&gt;
HUMAP&#65306;&#23618;&#27425;&#32479;&#19968;&#27969;&#24418;&#36924;&#36817;&#19982;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
HUMAP: Hierarchical Uniform Manifold Approximation and Projection. (arXiv:2106.07718v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07718
&lt;/p&gt;
&lt;p&gt;
HUMAP&#26159;&#19968;&#31181;&#26032;&#30340;&#23618;&#27425;&#38477;&#32500;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#23618;&#27425;&#25506;&#32034;&#20013;&#20445;&#30041;&#24515;&#29702;&#22320;&#22270;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#25968;&#25454;&#31867;&#22411;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38477;&#32500;&#25216;&#26415;&#26377;&#21161;&#20110;&#20998;&#26512;&#20154;&#21592;&#29702;&#35299;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#27169;&#24335;&#12290;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#20197;&#25955;&#28857;&#22270;&#24418;&#24335;&#21576;&#29616;&#65292;&#24212;&#29992;&#20110;&#21508;&#31181;&#31185;&#23398;&#39046;&#22495;&#65292;&#24182;&#20419;&#36827;&#38598;&#32676;&#21644;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20998;&#26512;&#12290;&#38024;&#23545;&#21253;&#21547;&#35768;&#22810;&#31890;&#24230;&#25110;&#36981;&#24490;&#20449;&#24687;&#21487;&#35270;&#21270;&#20934;&#21017;&#30340;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#23618;&#27425;&#38477;&#32500;&#25216;&#26415;&#26159;&#26368;&#36866;&#21512;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#20808;&#21069;&#21576;&#29616;&#20102;&#20027;&#35201;&#32467;&#26500;&#24182;&#21487;&#20197;&#25353;&#38656;&#25552;&#20379;&#35814;&#32454;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#23618;&#27425;&#38477;&#32500;&#25216;&#26415;&#24182;&#19981;&#33021;&#23436;&#20840;&#35299;&#20915;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#33021;&#22312;&#23618;&#27425;&#32423;&#21035;&#20043;&#38388;&#20445;&#25345;&#25237;&#24433;&#24515;&#29702;&#22320;&#22270;&#65292;&#20063;&#19981;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#25968;&#25454;&#31867;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23618;&#27425;&#38477;&#32500;&#25216;&#26415;HUMAP&#65292;&#26088;&#22312;&#28789;&#27963;&#22320;&#20445;&#30041;&#26412;&#22320;&#21644;&#20840;&#23616;&#32467;&#26500;&#20197;&#21450;&#24515;&#29702;&#22320;&#22270;&#65292;&#22312;&#23618;&#27425;&#25506;&#32034;&#20013;&#20855;&#26377;&#20248;&#36234;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#25968;&#25454;&#31867;&#22411;&#19978;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dimensionality reduction (DR) techniques help analysts understand patterns in high-dimensional spaces. These techniques, often represented by scatter plots, are employed in diverse science domains and facilitate similarity analysis among clusters and data samples. For datasets containing many granularities or when analysis follows the information visualization mantra, hierarchical DR techniques are the most suitable approach since they present major structures beforehand and details on demand. However, current hierarchical DR techniques are not fully capable of addressing literature problems because they do not preserve the projection mental map across hierarchical levels or are not suitable for most data types. This work presents HUMAP, a novel hierarchical dimensionality reduction technique designed to be flexible in preserving local and global structures and the mental map throughout hierarchical exploration. We provide empirical evidence of our technique's superiority compared with
&lt;/p&gt;</description></item></channel></rss>